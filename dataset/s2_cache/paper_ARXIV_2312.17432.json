{"paperId":"5f58863dd6474d6f127be995b5871e7c60f2792f","externalIds":{"DBLP":"journals/corr/abs-2312-17432","ArXiv":"2312.17432","DOI":"10.1109/TCSVT.2025.3566695","CorpusId":266690572},"title":"Video Understanding With Large Language Models: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2312.17432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2119309562","name":"Yunlong Tang"},{"authorId":"2066776633","name":"Jing Bi"},{"authorId":"2186266050","name":"Siting Xu"},{"authorId":"2242154602","name":"Luchuan Song"},{"authorId":"2153545235","name":"Susan Liang"},{"authorId":"2269119984","name":"Teng Wang"},{"authorId":"2266412651","name":"Daoan Zhang"},{"authorId":"2277235738","name":"Jie An"},{"authorId":"2277246502","name":"Jingyang Lin"},{"authorId":"2277251955","name":"Rongyi Zhu"},{"authorId":"82946757","name":"A. Vosoughi"},{"authorId":"2161012966","name":"Chao Huang"},{"authorId":"2155280541","name":"Zeliang Zhang"},{"authorId":"2146483847","name":"Feng Zheng"},{"authorId":"2277204434","name":"Jianguo Zhang"},{"authorId":"2277217764","name":"Ping Luo"},{"authorId":"2257204717","name":"Jiebo Luo"},{"authorId":"2242467374","name":"Chenliang Xu"}],"abstract":"With the rapid growth of online video platforms and the escalating volume of video content, the need for proficient video understanding tools has increased significantly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advances in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (abstract, temporal, and spatiotemporal) reasoning combined with common-sense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: <italic>Video Analyzer</italic> <inline-formula> <tex-math notation=\"LaTeX\">$\\times $ </tex-math></inline-formula> <italic>LLM</italic>, <italic>Video Embedder</italic> <inline-formula> <tex-math notation=\"LaTeX\">$\\times $ </tex-math></inline-formula> <italic>LLM</italic>, and <italic>(Analyzer + Embedder)</italic> <inline-formula> <tex-math notation=\"LaTeX\">$\\times $ </tex-math></inline-formula> <italic>LLM</italic>. We identify five subtypes based on the functions of LLMs in Vid-LLMs: <italic>LLM as Summarizer</italic>, <italic>LLM as Manager</italic>, <italic>LLM as Text Decoder</italic>, <italic>LLM as Regressor</italic>, and <italic>LLM as Hidden Layer</italic>. This survey also presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methods for Vid-LLMs. Additionally, it explores the extensive applications of Vid-LLMs in various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Additionally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are encouraged to visit the repository at <uri>https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding</uri>"}