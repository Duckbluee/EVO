{"references":[{"paperId":"14ba624e625a28d38c20e4e0c7803a88138cfa72","externalIds":{"ArXiv":"2407.18454","DBLP":"journals/corr/abs-2407-18454","DOI":"10.1002/widm.70063","CorpusId":271516455},"title":"Fairness Definitions in Language Models Explained"},{"paperId":"306f523c579d544aa01379b2b62588a15c0489aa","externalIds":{"DBLP":"conf/icse/YinW024","DOI":"10.1145/3639478.3643531","CorpusId":269987658},"title":"Improving Fairness in Machine Learning Software via Counterfactual Fairness Thinking"},{"paperId":"796036e087ade0198a43e3a2b5e323adc033f978","externalIds":{"DBLP":"conf/aaai/Zhang24","DOI":"10.1609/aaai.v38i20.30301","CorpusId":268715625},"title":"Fairness with Censorship: Bridging the Gap between Fairness Research and Real-World Deployment"},{"paperId":"6e3eed2b1ec12cf13bcc3a7427113b832193265a","externalIds":{"ArXiv":"2402.06853","DBLP":"journals/aiethics/WangCDNYZ25","DOI":"10.1007/s43681-024-00583-7","CorpusId":267627936},"title":"History, development, and principles of large language models: an introductory survey"},{"paperId":"5da06eb3a746932acfe36b81c7c640c3d969ae70","externalIds":{"DBLP":"journals/corr/abs-2401-15585","ArXiv":"2401.15585","DOI":"10.48550/arXiv.2401.15585","CorpusId":267311383},"title":"Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting"},{"paperId":"de4dfb773ab455081e5fb1862e08f581c58d43bc","externalIds":{"DBLP":"journals/corr/abs-2401-05778","ArXiv":"2401.05778","DOI":"10.48550/arXiv.2401.05778","CorpusId":266933337},"title":"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems"},{"paperId":"fb4dc0178e5d7347b1615c48caf05347b6e5eb48","externalIds":{"DBLP":"journals/corr/abs-2401-05561","ArXiv":"2401.05561","DOI":"10.48550/arXiv.2401.05561","CorpusId":266933236},"title":"TrustLLM: Trustworthiness in Large Language Models"},{"paperId":"4d246e4d83d5cd898a09e74ca2786141c5b5b0ec","externalIds":{"DBLP":"conf/icdm/CaiYZ23","DOI":"10.1109/ICDMW60847.2023.00198","CorpusId":267524276},"title":"Exploring Approaches for Teaching Cybersecurity and AI for K-12"},{"paperId":"af1f8c7a7b9a24c65a10ddbc9ab3f63976f57ab0","externalIds":{"DBLP":"conf/icdm/ChintaFCFYYWWXL23","DOI":"10.1109/ICDMW60847.2023.00199","CorpusId":267525207},"title":"Optimization and Improvement of Fake News Detection using Voting Technique for Societal Benefit"},{"paperId":"3409a29c7287a5e0010f48f8bca42679e3b10c12","externalIds":{"DBLP":"journals/corr/abs-2312-01509","ArXiv":"2312.01509","DOI":"10.48550/arXiv.2312.01509","CorpusId":265609311},"title":"Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies"},{"paperId":"94f9155414e802ae1a5605447f19457f0563db51","externalIds":{"DBLP":"conf/icdm/WangNYZ23","DOI":"10.1109/ICDM58522.2023.00073","CorpusId":267502643},"title":"Mitigating Multisource Biases in Graph Neural Networks via Real Counterfactual Samples"},{"paperId":"14ba788bf3b55ddcb515aad2deb45c6a4422e473","externalIds":{"ArXiv":"2311.18140","DBLP":"conf/emnlp/EsiobuTHUZFDPWS23","DOI":"10.48550/arXiv.2311.18140","CorpusId":265506521},"title":"ROBBIE: Robust Bias Evaluation of Large Generative Language Models"},{"paperId":"ff5f0c5b6905a8c4b361a625b450e9ab417fa854","externalIds":{"ArXiv":"2311.16079","DBLP":"journals/corr/abs-2311-16079","DOI":"10.48550/arXiv.2311.16079","CorpusId":265456229},"title":"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models"},{"paperId":"647d64679501b9c161d39f9872d255b8dec95def","externalIds":{"ArXiv":"2311.16087","DBLP":"conf/emnlp/AkyurekPKW23","DOI":"10.48550/arXiv.2311.16087","CorpusId":265456263},"title":"DUnE: Dataset for Unified Editing"},{"paperId":"b930342321b0e3e63e6ce6f90680130d7b5b4a97","externalIds":{"DBLP":"conf/wmt/SavoldiGNB23","ACL":"2023.wmt-1.25","ArXiv":"2310.19345","DOI":"10.48550/arXiv.2310.19345","CorpusId":264802108},"title":"Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES"},{"paperId":"45a476cb04cccee74b9ddabce4d58d928be99f7d","externalIds":{"ArXiv":"2310.19736","DBLP":"journals/corr/abs-2310-19736","DOI":"10.48550/arXiv.2310.19736","CorpusId":264825354},"title":"Evaluating Large Language Models: A Comprehensive Survey"},{"paperId":"021f74a2d827e0e77b111315b14381c28f82264e","externalIds":{"DBLP":"journals/corr/abs-2310-18913","ArXiv":"2310.18913","DOI":"10.48550/arXiv.2310.18913","CorpusId":264590288},"title":"Debiasing Algorithm through Model Adaptation"},{"paperId":"a5b499835d1781acb58e19b571a95b31e1b3f088","externalIds":{"DBLP":"journals/corr/abs-2310-13132","ArXiv":"2310.13132","DOI":"10.1145/3589334.3645643","CorpusId":264405758},"title":"Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries"},{"paperId":"ce157cea880c9ab64de64f11a531202f5348fa05","externalIds":{"DBLP":"journals/corr/abs-2310-09219","ArXiv":"2310.09219","DOI":"10.48550/arXiv.2310.09219","CorpusId":264128125},"title":"\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters"},{"paperId":"72e05fac8cf01593f70c63e16385a7bf6fd0fe09","externalIds":{"DBLP":"journals/corr/abs-2310-00905","ArXiv":"2310.00905","DOI":"10.48550/arXiv.2310.00905","CorpusId":263605466},"title":"All Languages Matter: On the Multilingual Safety of Large Language Models"},{"paperId":"57207b935fc3484d175f5e9e2980d73ca793f994","externalIds":{"DBLP":"journals/tmlr/0003M0GZ0MC000Z25","ArXiv":"2309.11166","DOI":"10.48550/arXiv.2309.11166","CorpusId":262064288},"title":"Are Large Language Models Really Robust to Word-Level Perturbations?"},{"paperId":"9c7231245715018490593c75381824c591027c4e","externalIds":{"DBLP":"conf/kbse/MoralesCC23","DOI":"10.1109/ASE56229.2023.00018","CorpusId":265053881},"title":"Automating Bias Testing of LLMs"},{"paperId":"bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7","externalIds":{"DBLP":"journals/coling/GallegosRBTKDYZA24","ArXiv":"2309.00770","DOI":"10.1162/coli_a_00524","CorpusId":261530629},"title":"Bias and Fairness in Large Language Models: A Survey"},{"paperId":"cf852c18387cfcdcb3cb3bb5ba6a549b35766c33","externalIds":{"DBLP":"conf/ci2/KotekDS23","ArXiv":"2308.14921","DOI":"10.1145/3582269.3615599","CorpusId":261276445},"title":"Gender bias and stereotypes in Large Language Models"},{"paperId":"03bf28df6e282a7e36e1686edeb9c624e6ffb13b","externalIds":{"DBLP":"journals/corr/abs-2308-10149","ArXiv":"2308.10149","DOI":"10.48550/arXiv.2308.10149","CorpusId":261049466},"title":"A Survey on Fairness in Large Language Models"},{"paperId":"a4d543cc77a0a643a271988f5b4b6699f421c85c","externalIds":{"ArXiv":"2307.10223","DBLP":"conf/aies/DennlerOSSSTAGY23","DOI":"10.1145/3600211.3604682","CorpusId":259991807},"title":"Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms"},{"paperId":"88549b4f48b9709acdfb8b9e41656b6d133c5390","externalIds":{"DBLP":"journals/corr/abs-2307-00101","ArXiv":"2307.00101","DOI":"10.48550/arXiv.2307.00101","CorpusId":259316226},"title":"Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models"},{"paperId":"72c7cb545f7da68efd1014afe3a4f01b590e435b","externalIds":{"ArXiv":"2306.15087","DBLP":"journals/corr/abs-2306-15087","ACL":"2023.acl-long.507","DOI":"10.48550/arXiv.2306.15087","CorpusId":259262064},"title":"WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models"},{"paperId":"cfd2145fa17d2fcd7f1dba27bd713eaa4e798c1f","externalIds":{"ArXiv":"2306.08158","ACL":"2024.gebnlp-1.19","DOI":"10.18653/v1/2024.gebnlp-1.19","CorpusId":259164882},"title":"Sociodemographic Bias in Language Models: A Survey and Forward Path"},{"paperId":"bbd9c17b95c01207623dd80069d55917e9f5133d","externalIds":{"ACL":"2023.bea-1.13","DBLP":"journals/corr/abs-2306-07415","ArXiv":"2306.07415","DOI":"10.48550/arXiv.2306.07415","CorpusId":259145073},"title":"Gender-Inclusive Grammatical Error Correction through Augmentation"},{"paperId":"5aa26e0b2bb27162a4de07bf8c3d5e0e9d3b0853","externalIds":{"ArXiv":"2306.04140","DBLP":"journals/corr/abs-2306-04140","ACL":"2023.acl-long.34","DOI":"10.18653/v1/2023.acl-long.34","CorpusId":259096160},"title":"Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions"},{"paperId":"8d9ca1e2c703e2752a4904c967a65d45d0bef5f6","externalIds":{"DBLP":"journals/corr/abs-2305-18189","ACL":"2023.acl-long.84","ArXiv":"2305.18189","DOI":"10.48550/arXiv.2305.18189","CorpusId":258960243},"title":"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models"},{"paperId":"ce913026f693101e54d3ab9152e107034d81fce1","externalIds":{"DBLP":"journals/tmlr/LiangBLTSYZNWKN23","DOI":"10.1111/nyas.15007","CorpusId":253553585,"PubMed":"37230490"},"title":"Holistic Evaluation of Language Models"},{"paperId":"3052d8475ce4991724bf3c943ffcf24c5db3b68f","externalIds":{"ArXiv":"2305.15389","DBLP":"conf/cogsci/LiorS23","DOI":"10.48550/arXiv.2305.15389","CorpusId":258865922},"title":"Comparing Humans and Models on a Similar Scale: Towards Cognitive Gender Bias Evaluation in Coreference Resolution"},{"paperId":"f5c73d9e6641b018b633690102121f5605d34fb0","externalIds":{"DBLP":"conf/emnlp/YaoWT0LDC023","ArXiv":"2305.13172","DOI":"10.48550/arXiv.2305.13172","CorpusId":258833129},"title":"Editing Large Language Models: Problems, Methods, and Opportunities"},{"paperId":"6927a5b0152433a199ab4974ad85e787454d6a30","externalIds":{"DBLP":"journals/corr/abs-2305-13088","ArXiv":"2305.13088","DOI":"10.48550/arXiv.2305.13088","CorpusId":258832694},"title":"Should We Attend More or Less? Modulating Attention for Fairness"},{"paperId":"744a98cc2736fa71d3984602e10b68319a47c65e","externalIds":{"DBLP":"conf/sigsoft/WanWHGBL23","ArXiv":"2305.12434","DOI":"10.1145/3611643.3616310","CorpusId":258833296},"title":"BiasAsker: Measuring the Bias in Conversational AI System"},{"paperId":"416d95170cd73d00d72847b2a34c0c39f28621fe","externalIds":{"DBLP":"journals/corr/abs-2305-11140","ACL":"2023.acl-long.246","ArXiv":"2305.11140","DOI":"10.48550/arXiv.2305.11140","CorpusId":258762715},"title":"Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model"},{"paperId":"839cc546b58968e2a8cb968337fb2e3a279e2b00","externalIds":{"DBLP":"journals/corr/abs-2305-11262","ACL":"2023.acl-long.757","ArXiv":"2305.11262","DOI":"10.48550/arXiv.2305.11262","CorpusId":258823380},"title":"CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models"},{"paperId":"9b4f7c97c0b83a80c32bc0b93595cbcfb4ecb16d","externalIds":{"DBLP":"journals/corr/abs-2305-10429","ArXiv":"2305.10429","DOI":"10.48550/arXiv.2305.10429","CorpusId":258741043},"title":"DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"},{"paperId":"5471114e37448bea2457b74894b1ecb92bbcfdf6","externalIds":{"DBLP":"conf/acl/FengPLT23","ArXiv":"2305.08283","ACL":"2023.acl-long.656","DOI":"10.48550/arXiv.2305.08283","CorpusId":258686693},"title":"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models"},{"paperId":"d9e8a7e6d24e443dffeeec30e0ee8aea05c032a3","externalIds":{"DBLP":"journals/corr/abs-2305-06626","ArXiv":"2305.06626","DOI":"10.48550/arXiv.2305.06626","CorpusId":258615411},"title":"When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks"},{"paperId":"41839ba1e42b642d048cb7824e492d8822723859","externalIds":{"ArXiv":"2305.04547","DBLP":"journals/corr/abs-2305-04547","DOI":"10.48550/arXiv.2305.04547","CorpusId":258558111},"title":"Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias"},{"paperId":"6ce81d6d0285432c415e10d51cda62956491e18c","externalIds":{"DOI":"10.1038/s41587-023-01788-7","CorpusId":258311346,"PubMed":"37095351"},"title":"Drug discovery companies are customizing ChatGPT: here’s how"},{"paperId":"ad267e9ad515881a013203466424c30c7d87c777","externalIds":{"ArXiv":"2304.10153","DBLP":"conf/fat/CabelloJS23","DOI":"10.1145/3593013.3594004","CorpusId":258236466},"title":"On the Independence of Association Bias and Empirical Fairness in Language Models"},{"paperId":"16d83e930a4dab2d49f5d276838ddce79df3f787","externalIds":{"DBLP":"journals/firstmonday/Ferrara23a","ArXiv":"2304.03738","DOI":"10.5210/fm.v28i11.13346","CorpusId":258041203},"title":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","externalIds":{"DBLP":"conf/icml/BidermanSABOHKP23","ArXiv":"2304.01373","DOI":"10.48550/arXiv.2304.01373","CorpusId":257921893},"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"e4d8de5d5d95c1075050de5bf0b19811141006bd","externalIds":{"DOI":"10.1038/s41586-023-05905-z","CorpusId":258336875,"PubMed":"37100941"},"title":"Computational approaches streamlining drug discovery"},{"paperId":"6cec42c0df61c642229b4bcae86596cbbd20a98e","externalIds":{"DBLP":"journals/cacm/LuoPS24","ArXiv":"2303.16281","DOI":"10.1145/3670241","CorpusId":257804525},"title":"A Perspectival Mirror of the Elephant"},{"paperId":"8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c","externalIds":{"ArXiv":"2303.12712","DBLP":"journals/corr/abs-2303-12712","CorpusId":257663729},"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"b3dcd48b68bdbb304fa53299496539c054638e0c","externalIds":{"DBLP":"journals/corr/abs-2303-05670","ArXiv":"2303.05670","ACL":"2023.eacl-main.89","DOI":"10.48550/arXiv.2303.05670","CorpusId":257482682},"title":"Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"9df799a83dea19d0c46076c490aad81c45573ff7","externalIds":{"DBLP":"journals/kais/ZhangW23","DOI":"10.1007/s10115-023-01842-5","CorpusId":257326848},"title":"Fairness with censorship and group constraints"},{"paperId":"4cc5fbda617a52384702805b9eb632ff613d9b3b","externalIds":{"DBLP":"conf/wsdm/ParkCYK23","DOI":"10.1145/3539597.3570473","CorpusId":257079734},"title":"Never Too Late to Learn: Regularizing Gender Bias in Coreference Resolution"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"705e49afd92130f2bc1e0d4d0b1f6cb14e88803f","externalIds":{"DBLP":"conf/ccs/AbdelnabiGMEHF23","ArXiv":"2302.12173","DOI":"10.1145/3605764.3623985","CorpusId":258546941},"title":"Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection"},{"paperId":"e7319ae38ded230484050b7a0b60f3b4009830bd","externalIds":{"ArXiv":"2302.08018","DBLP":"journals/corr/abs-2302-08018","DOI":"10.48550/arXiv.2302.08018","CorpusId":256900637},"title":"Towards Fair Machine Learning Software: Understanding and Addressing Model Bias Through Counterfactual Thinking"},{"paperId":"83d9a4469537122f91c62de7d848996554bc2e68","externalIds":{"DBLP":"conf/ecai/ZhangW0CORW23","ArXiv":"2302.08015","DOI":"10.3233/FAIA230621","CorpusId":256900766},"title":"Individual Fairness Under Uncertainty"},{"paperId":"dc4c9eacc731c49e8acd0e63931ef799ae01e664","externalIds":{"DBLP":"conf/fat/WangSYKZ0ZKSWB023","ArXiv":"2302.08017","DOI":"10.1145/3593013.3593984","CorpusId":256900738},"title":"Preventing Discriminatory Decision-making in Evolving Data Streams"},{"paperId":"5ef821267fa68d3231ed8135ff8ec09f25bb1398","externalIds":{"DBLP":"journals/corr/abs-2302-07257","ArXiv":"2302.07257","DOI":"10.48550/arXiv.2302.07257","CorpusId":256846858},"title":"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"},{"paperId":"89e184d2bc830af568e439db9476caa0c047e11a","externalIds":{"ArXiv":"2302.06692","DBLP":"journals/corr/abs-2302-06692","DOI":"10.48550/arXiv.2302.06692","CorpusId":256846700},"title":"Guiding Pretraining in Reinforcement Learning with Large Language Models"},{"paperId":"75ddc4fb91332f95222d74449d96b9f7c8f976c7","externalIds":{"DBLP":"journals/corr/abs-2302-02463","ArXiv":"2302.02463","ACL":"2023.eacl-main.9","DOI":"10.48550/arXiv.2302.02463","CorpusId":256616034},"title":"Nationality Bias in Text Generation"},{"paperId":"92999f7a304e866a2be7176e59c745481ed01042","externalIds":{"DBLP":"conf/chi/JakeschBBZN23","ArXiv":"2302.00560","DOI":"10.1145/3544548.3581196","CorpusId":256459801},"title":"Co-Writing with Opinionated Language Models Affects Users’ Views"},{"paperId":"2eb0d00e5675582980245b95a48e40bd8e5f46a0","externalIds":{"ArXiv":"2301.11100","DBLP":"journals/corr/abs-2301-11100","DOI":"10.48550/arXiv.2301.11100","CorpusId":256274734},"title":"Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities"},{"paperId":"db5423d1d5737aa90c48bc121239160d24dccb36","externalIds":{"DBLP":"journals/corr/abs-2301-05327","ArXiv":"2301.05327","DOI":"10.48550/arXiv.2301.05327","CorpusId":255825875},"title":"Blind Judgement: Agent-Based Supreme Court Modelling With GPT"},{"paperId":"6052486bc9144dc1730c12bf35323af3792a1fd0","externalIds":{"ArXiv":"2212.13138","DBLP":"journals/corr/abs-2212-13138","PubMedCentral":"10396962","DOI":"10.1038/s41586-023-06291-2","CorpusId":255124952,"PubMed":"37438534"},"title":"Large language models encode clinical knowledge"},{"paperId":"cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3","externalIds":{"DBLP":"journals/corr/abs-2212-01326","ArXiv":"2212.01326","DOI":"10.48550/arXiv.2212.01326","CorpusId":254221002},"title":"Legal Prompting: Teaching a Language Model to Think Like a Lawyer"},{"paperId":"a45ff2a8b18abc850b267cf0ec6e391dba9138a5","externalIds":{"DBLP":"journals/corr/abs-2211-11109","ArXiv":"2211.11109","DOI":"10.48550/arXiv.2211.11109","CorpusId":253734850},"title":"Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness"},{"paperId":"1abd4fa45ce20175452aa238870db2aebe9c0fe0","externalIds":{"DBLP":"journals/corr/abs-2211-05414","ArXiv":"2211.05414","DOI":"10.48550/arXiv.2211.05414","CorpusId":253446867},"title":"ADEPT: A DEbiasing PrompT Framework"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"c8d594f09413b1555970f43e68847c211235d60f","externalIds":{"DBLP":"journals/corr/abs-2210-09150","ArXiv":"2210.09150","DOI":"10.48550/arXiv.2210.09150","CorpusId":252917981},"title":"Prompting GPT-3 To Be Reliable"},{"paperId":"969f45a3adf5e0bcf741447b1c67a0f3a386801a","externalIds":{"ACL":"2022.emnlp-main.245","DBLP":"conf/emnlp/SunHQH22","ArXiv":"2210.07626","DOI":"10.48550/arXiv.2210.07626","CorpusId":252907549},"title":"BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation"},{"paperId":"4dcef4d040cdbc17eb8e7e39d1456c2a1ab691a0","externalIds":{"ACL":"2023.eacl-main.116","DBLP":"journals/corr/abs-2210-07269","ArXiv":"2210.07269","DOI":"10.48550/arXiv.2210.07269","CorpusId":252907310},"title":"SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","externalIds":{"DBLP":"journals/corr/abs-2209-14375","ArXiv":"2209.14375","DOI":"10.48550/arXiv.2209.14375","CorpusId":252596089},"title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","externalIds":{"ArXiv":"2206.04615","DBLP":"journals/corr/abs-2206-04615","CorpusId":263625818},"title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"52808c881730c907332eb04b7b89133f8145a69e","externalIds":{"ArXiv":"2205.12586","ACL":"2022.emnlp-main.646","DBLP":"journals/corr/abs-2205-12586","DOI":"10.48550/arXiv.2205.12586","CorpusId":249062690},"title":"Perturbation Augmentation for Fairer NLP"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","externalIds":{"DBLP":"journals/corr/abs-2205-11916","ArXiv":"2205.11916","CorpusId":249017743},"title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"e1ec84ee5c64566154f48576ac17968c39413ef0","externalIds":{"ArXiv":"2205.10400","DBLP":"conf/naacl/HungLVPG22","ACL":"2022.naacl-main.270","DOI":"10.48550/arXiv.2205.10400","CorpusId":248987294},"title":"Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog"},{"paperId":"7ef43bacd43393ff116e6fcda6a52a6902e016d7","externalIds":{"ACL":"2022.emnlp-main.625","DBLP":"conf/emnlp/SmithHKPW22","ArXiv":"2205.09209","DOI":"10.18653/v1/2022.emnlp-main.625","CorpusId":253224433},"title":"“I’m sorry to hear that”: Finding New Biases in Language Models with a Holistic Descriptor Dataset"},{"paperId":"8e11c4f14567d1257d4c8d8ac4e14446e1b99c36","externalIds":{"ArXiv":"2204.09888","DBLP":"journals/corr/abs-2204-09888","DOI":"10.1109/TKDE.2023.3265598","CorpusId":248300164},"title":"Fairness in Graph Mining: A Survey"},{"paperId":"cfb35f8c18fbc5baa453280ecd0aa8148bbba659","externalIds":{"ArXiv":"2204.08570","DBLP":"journals/ijautcomp/DaiZZXGLTW24","DOI":"10.1007/s11633-024-1510-8","CorpusId":248239981},"title":"A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability"},{"paperId":"c435ecd0321dcec1f25e458bf930311f9e1d04b6","externalIds":{"ArXiv":"2204.03162","DBLP":"journals/corr/abs-2204-03162","DOI":"10.1109/CVPR52688.2022.00517","CorpusId":248006414},"title":"Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"6bdebf88b07e22f9955f9b06590e8ff030697299","externalIds":{"DBLP":"journals/nca/TianZLZ22","DOI":"10.1007/s00521-022-07136-1","CorpusId":247723355},"title":"Image fairness in deep learning: problems, models, and challenges"},{"paperId":"7335fbc509da851b9d141a40e6463b5e82dea01c","externalIds":{"ArXiv":"2204.09591","DBLP":"journals/corr/abs-2204-09591","DOI":"10.48550/arXiv.2204.09591","CorpusId":248266397},"title":"A Survey on Bias and Fairness in Natural Language Processing"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","externalIds":{"DBLP":"conf/kdd/Lees0TSGMV22","ArXiv":"2202.11176","DOI":"10.1145/3534678.3539147","CorpusId":247058801},"title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"a361b203fb2485bfbc092d65625e25a1df22c4c1","externalIds":{"ArXiv":"2202.04173","DBLP":"journals/corr/abs-2202-04173","CorpusId":246679892},"title":"Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models"},{"paperId":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","externalIds":{"DBLP":"journals/corr/abs-2202-03286","ACL":"2022.emnlp-main.225","ArXiv":"2202.03286","DOI":"10.18653/v1/2022.emnlp-main.225","CorpusId":246634238},"title":"Red Teaming Language Models with Language Models"},{"paperId":"20328647c38282088dc9dddedcb2e5bdaeeeea78","externalIds":{"ACL":"2022.naacl-srw.21","DBLP":"journals/corr/abs-2201-08643","ArXiv":"2201.08643","DOI":"10.18653/v1/2022.naacl-srw.21","CorpusId":246210255},"title":"Text Style Transfer for Bias Mitigation using Masked Language Modeling"},{"paperId":"fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf","externalIds":{"ArXiv":"2112.04359","DBLP":"journals/corr/abs-2112-04359","CorpusId":244954639},"title":"Ethical and social risks of harm from Language Models"},{"paperId":"9286ac6e9b1aacd7d93496eb4615ae7678876d2a","externalIds":{"DBLP":"journals/corr/abs-2110-11309","ArXiv":"2110.11309","CorpusId":239050360},"title":"Fast Model Editing at Scale"},{"paperId":"de6807676d8171472ed6cf421c4e4ed3cbb47699","externalIds":{"ArXiv":"2110.08527","ACL":"2022.acl-long.132","DBLP":"journals/corr/abs-2110-08527","DOI":"10.18653/v1/2022.acl-long.132","CorpusId":239015827},"title":"An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models"},{"paperId":"7d5c661fa9a4255ee087e861f820564ea2e2bd6b","externalIds":{"DBLP":"journals/corr/abs-2110-08193","ArXiv":"2110.08193","ACL":"2022.findings-acl.165","DOI":"10.18653/v1/2022.findings-acl.165","CorpusId":239010011},"title":"BBQ: A hand-built bias benchmark for question answering"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","externalIds":{"ArXiv":"2110.08207","DBLP":"journals/corr/abs-2110-08207","CorpusId":239009562},"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"b15469d0ab3dc3a9dec037d761817b3fe546bed6","externalIds":{"DBLP":"journals/csur/WangXPCTLF24","ArXiv":"2110.05006","DOI":"10.1145/3611651","CorpusId":238634270},"title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey"},{"paperId":"01c39795715404593230cb0f75007b48f156039f","externalIds":{"ACL":"2023.acl-short.108","DBLP":"conf/acl/FatemiXLX23","ArXiv":"2110.05367","DOI":"10.18653/v1/2023.acl-short.108","CorpusId":238582879},"title":"Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting"},{"paperId":"d9f561c3cf90c8c175be4178cc32c543a09fcb82","externalIds":{"ACL":"2021.cinlp-1.3","ArXiv":"2110.00911","DBLP":"journals/corr/abs-2110-00911","DOI":"10.18653/v1/2021.cinlp-1.3","CorpusId":237630643},"title":"Enhancing Model Robustness and Fairness with Causality: A Regularization Approach"},{"paperId":"ebb0df8229e7dcf47f79c96e77b37e6fa3dae485","externalIds":{"DBLP":"journals/corr/abs-2110-00530","ArXiv":"2110.00530","DOI":"10.1002/widm.1452","CorpusId":238252945},"title":"A survey on datasets for fairness‐aware machine learning"},{"paperId":"95ec171964b16e7c14739b1ab039df2cca6abb73","externalIds":{"ArXiv":"2109.06105","DBLP":"journals/corr/abs-2109-06105","ACL":"2021.emnlp-main.704","DOI":"10.18653/v1/2021.emnlp-main.704","CorpusId":237494677},"title":"NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives"},{"paperId":"130ab5c480860e330b65280a3410f17bb2d50fe1","externalIds":{"DBLP":"conf/emnlp/LauscherLG21","ArXiv":"2109.03646","DOI":"10.18653/v1/2021.findings-emnlp.411","CorpusId":237440429},"title":"Sustainable Modular Debiasing of Language Models"},{"paperId":"d48d1e80b6ea9708fa3a09d1556a7ced3b147da2","externalIds":{"DBLP":"journals/corr/abs-2109-03858","ArXiv":"2109.03858","DOI":"10.18653/v1/2021.findings-emnlp.211","CorpusId":237452751},"title":"Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"e6a237ab883e503b10b73b3a411c0078c47c9830","externalIds":{"DBLP":"journals/corr/abs-2108-12084","ArXiv":"2108.12084","ACL":"2021.emnlp-main.150","DOI":"10.18653/v1/2021.emnlp-main.150","CorpusId":237347097},"title":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies"},{"paperId":"2ee460b329c0987b52793b84f7ac35cb4a547475","externalIds":{"PubMedCentral":"8874824","DOI":"10.2196/32875","CorpusId":246700751,"PubMed":"35142635"},"title":"Operationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook of Generative Pretrained Transformer 3 (GPT-3) as a Service Model"},{"paperId":"beb4f0ef465212c5eae59e85dc838d3ba47dbacc","externalIds":{"ArXiv":"2108.03362","DBLP":"conf/ijcnlp/DevSZASHSKNPC22","DOI":"10.18653/v1/2022.findings-aacl.24","CorpusId":252907216},"title":"On Measures of Biases and Harms in NLP"},{"paperId":"48df243cd6d09b224eddb45b4e49574e9e726ded","externalIds":{"DBLP":"journals/corr/abs-2108-02924","ArXiv":"2108.02924","DOI":"10.1007/978-3-030-86362-3_45","CorpusId":236950469,"PubMed":"35072174"},"title":"Interpretable Visual Understanding with Cognitive Attention Network"},{"paperId":"1a21ed83b4c06d96db34e35df8b2db24fbdc4c26","externalIds":{"ArXiv":"2108.07790","DBLP":"journals/corr/abs-2108-07790","CorpusId":237142520},"title":"Mitigating harm in language models with conditional-likelihood filtration"},{"paperId":"8b430ae5af9d7991cb3e698b2b30296fdf43dd15","externalIds":{"DBLP":"journals/llc/HovyP21","PubMedCentral":"9285808","DOI":"10.1111/lnc3.12432","CorpusId":237298625,"PubMed":"35864931"},"title":"Five sources of bias in natural language processing"},{"paperId":"79cd76d93f75a7921f977c2fe08e52e87d9bc0a3","externalIds":{"DBLP":"journals/fgcs/LiuWJTZZ21","DOI":"10.1016/j.future.2021.02.015","CorpusId":233375153},"title":"Research on unsupervised feature learning for Android malware detection based on Restricted Boltzmann Machines"},{"paperId":"2add974973ab45e46f1f8d3b932d24ba88cbb0b4","externalIds":{"DBLP":"journals/corr/abs-2106-03521","ArXiv":"2106.03521","ACL":"2021.acl-long.151","DOI":"10.18653/v1/2021.acl-long.151","CorpusId":235358955},"title":"RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models"},{"paperId":"d4f6ef636e16b001986b541aa2afc76eed42ae34","externalIds":{"DBLP":"journals/npjdm/KorngiebelM21","PubMedCentral":"8175735","DOI":"10.1038/s41746-021-00464-x","CorpusId":235307275,"PubMed":"34083689"},"title":"Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery"},{"paperId":"80e015b4edbe72cb10af0bd2cb065bba163d6e0d","externalIds":{"ACL":"2021.naacl-main.60","DBLP":"conf/naacl/ShengCNP21","MAG":"3166704354","DOI":"10.18653/V1/2021.NAACL-MAIN.60","CorpusId":235097342},"title":"“Nice Try, Kiddo”: Investigating Ad Hominems in Dialogue Responses"},{"paperId":"4f76d26369ee573cb03db4b9f6e4ab5d61806095","externalIds":{"MAG":"3168584517","ACL":"2021.naacl-main.191","DBLP":"conf/naacl/NozzaBH21","DOI":"10.18653/V1/2021.NAACL-MAIN.191","CorpusId":235097294},"title":"HONEST: Measuring Hurtful Sentence Completion in Language Models"},{"paperId":"1e3e65e7773b7869d9bd7f5394b54199e48195e6","externalIds":{"ArXiv":"2105.03887","DBLP":"journals/aiopen/XiaoHLTS21","MAG":"3163842580","DOI":"10.1016/j.aiopen.2021.06.003","CorpusId":234342706},"title":"Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"23b1a29a65f71f76fe97ffcb063de22d2a239b10","externalIds":{"ACL":"2022.findings-acl.301","DBLP":"conf/acl/SaundersSB22","ArXiv":"2104.07429","DOI":"10.18653/v1/2022.findings-acl.301","CorpusId":233240748},"title":"First the Worst: Finding Better Gender Translations During Beam Search"},{"paperId":"b41e07349b87a178d904e6b5d05a2f90b16f8e1e","externalIds":{"ArXiv":"2102.04130","DBLP":"conf/nips/KirkJVIBDSA21","CorpusId":236950797},"title":"Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models"},{"paperId":"ce3b364b7e6358940ce97d8d5887a65e5024ca21","externalIds":{"DBLP":"conf/fat/DhamalaSKKPCG21","ArXiv":"2101.11718","DOI":"10.1145/3442188.3445924","CorpusId":231719337},"title":"BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation"},{"paperId":"4c2733d191e347753bb28afa46a1c55c65e085be","externalIds":{"DBLP":"conf/aies/AbidF021","ArXiv":"2101.05783","DOI":"10.1145/3461702.3462624","CorpusId":231603388},"title":"Persistent Anti-Muslim Bias in Large Language Models"},{"paperId":"497d29459a894ac38a48ed58753976ccbf2aa433","externalIds":{"DBLP":"journals/corr/abs-2012-15859","ACL":"2021.acl-long.150","ArXiv":"2012.15859","DOI":"10.18653/v1/2021.acl-long.150","CorpusId":229923772},"title":"Intrinsic Bias Metrics Do Not Correlate with Application Bias"},{"paperId":"d198f847288a2664755da96464501c429c378c75","externalIds":{"DBLP":"conf/ictai/ZhangZZLCWRM20","DOI":"10.1109/ICTAI50040.2020.00069","CorpusId":229703513},"title":"Flexible and Adaptive Fairness-aware Learning in Non-stationary Data Streams"},{"paperId":"10391eed628dfece8a9136f76c5df53b5704422d","externalIds":{"MAG":"3104041537","DBLP":"conf/emnlp/ForbesHSSC20","ACL":"2020.emnlp-main.48","ArXiv":"2011.00620","DOI":"10.18653/v1/2020.emnlp-main.48","CorpusId":226226666},"title":"Social Chemistry 101: Learning to Reason about Social and Moral Norms"},{"paperId":"0712334d1109248e52706f13aeff5281834727f8","externalIds":{"DBLP":"journals/corr/abs-2010-14534","ACL":"2020.gebnlp-1.1","ArXiv":"2010.14534","MAG":"3095105395","CorpusId":225094152},"title":"Unmasking Contextual Stereotypes: Measuring and Mitigating BERT’s Gender Bias"},{"paperId":"839a91751f5032a8ab627ad129eb5eb814dafefc","externalIds":{"DBLP":"journals/kais/Zhang25","ArXiv":"2010.08146","DOI":"10.1007/s10115-025-02568-2","CorpusId":223953329},"title":"Online and Customizable Fairness-aware Learning"},{"paperId":"3d864a8bc5a55ccab9993aa66203d8e70b88148c","externalIds":{"ArXiv":"2010.06032","MAG":"3093211917","DBLP":"journals/corr/abs-2010-06032","CorpusId":222310622},"title":"Measuring and Reducing Gendered Correlations in Pre-trained Models"},{"paperId":"f72983cef733670d6915e37383257f548b5a3365","externalIds":{"DBLP":"journals/corr/abs-2010-02428","ACL":"2020.findings-emnlp.311","MAG":"3105042180","ArXiv":"2010.02428","DOI":"10.18653/v1/2020.findings-emnlp.311","CorpusId":222141056},"title":"UNQOVERing Stereotypical Biases via Underspecified Questions"},{"paperId":"645bd6eadc247989abc5e0b0aa0be79ec8b11ea6","externalIds":{"MAG":"3089430725","DBLP":"journals/corr/abs-2010-00133","ArXiv":"2010.00133","ACL":"2020.emnlp-main.154","DOI":"10.18653/v1/2020.emnlp-main.154","CorpusId":222090785},"title":"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","externalIds":{"MAG":"3088599783","ACL":"2020.findings-emnlp.301","DBLP":"journals/corr/abs-2009-11462","ArXiv":"2009.11462","DOI":"10.18653/v1/2020.findings-emnlp.301","CorpusId":221878771},"title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"f15c5e191d2f5cc640fd51bd8aa32840ea8f0bd0","externalIds":{"DBLP":"conf/dexa/ZhangZZCEP20","MAG":"3084909377","DOI":"10.1007/978-3-030-59003-1_29","CorpusId":221667054},"title":"Deep Discriminative Learning for Autism Spectrum Disorder Classification"},{"paperId":"e960beaf0e84bc20cbcfd67fb7aefe37db15e1a1","externalIds":{"DBLP":"conf/um/DeshpandePF20","MAG":"3043760925","DOI":"10.1145/3386392.3399569","CorpusId":220040093},"title":"Mitigating Demographic Bias in AI-based Resume Filtering"},{"paperId":"aa64d955454464ef5d921cc9df6682ff4921b2e3","externalIds":{"MAG":"3029881688","DBLP":"journals/npjdm/CirilloCMGSMGVR20","PubMedCentral":"7264169","DOI":"10.1038/s41746-020-0288-5","CorpusId":219156874,"PubMed":"33597695"},"title":"Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare"},{"paperId":"69d9d8f0a76996a0d8731ccff241daf703c28c05","externalIds":{"DOI":"10.35940/ijrte.a2951.059120","CorpusId":241193808},"title":"FinAID, A Financial Advisor Application using AI"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"e3e9d2bdcc3fefab7c294196c8b2e149727376ed","externalIds":{"ACL":"2020.acl-main.260","MAG":"3035379020","DBLP":"journals/corr/abs-2005-00699","ArXiv":"2005.00699","DOI":"10.18653/v1/2020.acl-main.260","CorpusId":218487087},"title":"Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer"},{"paperId":"e969aa3422a49152c22f3faf734e4561a2a3cf42","externalIds":{"DBLP":"conf/acl/RavfogelEGTG20","ArXiv":"2004.07667","ACL":"2020.acl-main.647","MAG":"3035241006","DOI":"10.18653/v1/2020.acl-main.647","CorpusId":215786522},"title":"Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection"},{"paperId":"eef4df3a5232c7ce70123aaebb326ff9169a3c8c","externalIds":{"MAG":"3101004475","ArXiv":"1912.11078","ACL":"2020.acl-main.468","DBLP":"conf/acl/ShahSH20","DOI":"10.18653/v1/2020.acl-main.468","CorpusId":209461005},"title":"Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview"},{"paperId":"5d22b241836e30d5b0d852b463951ab7e3245ea4","externalIds":{"ArXiv":"1911.03064","ACL":"2020.findings-emnlp.7","MAG":"3101767999","DBLP":"journals/corr/abs-1911-03064","DOI":"10.18653/v1/2020.findings-emnlp.7","CorpusId":207847197},"title":"Reducing Sentiment Bias in Language Models via Counterfactual Evaluation"},{"paperId":"dfe35fc088b4d7d6b6297ebd3ec830ef7c03d5fd","externalIds":{"DBLP":"conf/icdm/ZhangTW19","MAG":"2999584418","DOI":"10.1109/ICDMW.2019.00157","CorpusId":210698032},"title":"On Fairness-Aware Learning for Non-discriminative Decision-Making"},{"paperId":"c0ba20d689e3d09f35ff038358e1a1b4a10a82f9","externalIds":{"MAG":"2997607995","DBLP":"conf/emnlp/ChangPO19","CorpusId":209325068},"title":"Bias and Fairness in Natural Language Processing"},{"paperId":"4099c4d272c12081b562392606e6d567e4ae7031","externalIds":{"DBLP":"conf/acl/SalazarLNK20","ACL":"2020.acl-main.240","MAG":"3034775979","DOI":"10.18653/v1/2020.acl-main.240","CorpusId":218628872},"title":"Masked Language Model Scoring"},{"paperId":"62d19e1e5c7dad3ba79cebd2f7579d249f37fcda","externalIds":{"MAG":"2987794593","DOI":"10.17485/ijst/2019/v12i33/146130","CorpusId":209057837},"title":"Classification of Sindhi Headline News Documents based on TF-IDF Text Analysis Scheme"},{"paperId":"0e9b89f034b9a8c2828fe7daaee3894d6bfe3e50","externalIds":{"DBLP":"conf/aaai/DevLPS20","ArXiv":"1908.09369","MAG":"2969426522","DOI":"10.1609/AAAI.V34I05.6267","CorpusId":201670701},"title":"On Measuring and Mitigating Biased Inferences of Word Embeddings"},{"paperId":"0090023afc66cd2741568599057f4e82b566137c","externalIds":{"ArXiv":"1908.09635","MAG":"2969896603","DBLP":"journals/csur/MehrabiMSLG21","DOI":"10.1145/3457607","CorpusId":201666566},"title":"A Survey on Bias and Fairness in Machine Learning"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"0bee924fb75f3db3d9ecf6e384cc751c1784ab1e","externalIds":{"DBLP":"journals/corr/abs-1907-07237","MAG":"2965366403","ArXiv":"1907.07237","DOI":"10.24963/ijcai.2019/205","CorpusId":197431421},"title":"FAHT: An Adaptive Fairness-aware Decision Tree Classifier"},{"paperId":"cea323953067cda0b89af253d39507c755b55a24","externalIds":{"CorpusId":267881571},"title":"Measures of"},{"paperId":"333671a5fbbf726f8819138f3670524ec0405726","externalIds":{"MAG":"2974817986","DOI":"10.1147/jrd.2019.2942287","CorpusId":203897430},"title":"AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias"},{"paperId":"8963317176fa81e185fd7a8f8cd001d7e11a4868","externalIds":{"ACL":"P19-1163","DBLP":"conf/acl/SapCGCS19","MAG":"2949678053","DOI":"10.18653/v1/P19-1163","CorpusId":196211238},"title":"The Risk of Racial Bias in Hate Speech Detection"},{"paperId":"a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd","externalIds":{"MAG":"2950437211","ArXiv":"1906.07337","DBLP":"journals/corr/abs-1906-07337","ACL":"W19-3823","DOI":"10.18653/v1/W19-3823","CorpusId":190000105},"title":"Measuring Bias in Contextualized Word Representations"},{"paperId":"835ac3cbb41f2ec47718c5491211dd33b64f382b","externalIds":{"DBLP":"conf/acl/ZmigrodMWC19","MAG":"2949682057","ArXiv":"1906.04571","ACL":"P19-1161","DOI":"10.18653/v1/P19-1161","CorpusId":184486914},"title":"Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology"},{"paperId":"5e312f79d2a7286cb2b9e48b71b5d47990966879","externalIds":{"MAG":"2978279988","DOI":"10.2139/ssrn.3395476","CorpusId":209095787},"title":"Deep Learning in Computer Vision: Methods, Interpretation, Causation, and Fairness"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","externalIds":{"MAG":"2943552823","ArXiv":"1905.00537","DBLP":"conf/nips/WangPNSMHLB19","CorpusId":143424870},"title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"a4e67bcbf912e13cebbb1241d05d1ca0a1df9df8","externalIds":{"ArXiv":"1904.03035","DBLP":"conf/naacl/BordiaB19","MAG":"2955635700","ACL":"N19-3002","DOI":"10.18653/v1/N19-3002","CorpusId":102352788},"title":"Identifying and Reducing Gender Bias in Word-Level Language Models"},{"paperId":"e235ad7dcf6e97cd372f09724dc947c5b1efac79","externalIds":{"MAG":"2926555354","ArXiv":"1904.03310","DBLP":"conf/naacl/ZhaoWYCOC19","ACL":"N19-1064","DOI":"10.18653/v1/N19-1064","CorpusId":102352962},"title":"Gender Bias in Contextualized Word Embeddings"},{"paperId":"5e9c85235210b59a16bdd84b444a904ae271f7e7","externalIds":{"MAG":"2963078909","ACL":"N19-1063","DBLP":"conf/naacl/MayWBBR19","ArXiv":"1903.10561","DOI":"10.18653/v1/N19-1063","CorpusId":85518027},"title":"On Measuring Social Biases in Sentence Encoders"},{"paperId":"c4afa2b3eda95a1194313394901e0e96e24cefaa","externalIds":{"DBLP":"conf/fat/De-ArteagaRWCBC19","MAG":"3105536512","ArXiv":"1901.09451","DOI":"10.1145/3287560.3287572","CorpusId":58006082},"title":"Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting"},{"paperId":"97bfa89addc6e5d76361e4c1e296949cad887b86","externalIds":{"DBLP":"journals/tacl/BenderF18","ACL":"Q18-1041","MAG":"2911227954","DOI":"10.1162/tacl_a_00041","CorpusId":52255687},"title":"Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science"},{"paperId":"3e04302e6e45e801a2e7a43bbe5925ed7b14e1be","externalIds":{"MAG":"2914131329","DBLP":"conf/bibm/ZhangW18","DOI":"10.1109/BIBM.2018.8621180","CorpusId":59231101},"title":"Content-bootstrapped Collaborative Filtering for Medical Article Recommendations"},{"paperId":"36845e5d13e6d8698561d416b694122cfa754332","externalIds":{"ArXiv":"1811.05577","DBLP":"journals/corr/abs-1811-05577","MAG":"2900572965","CorpusId":53305131},"title":"Aequitas: A Bias and Fairness Audit Toolkit"},{"paperId":"4beb2beef0a44a1c0abe236bfb0e0ff800ac52bd","externalIds":{"DBLP":"conf/bigdataconf/ZhangWJOZ18","ArXiv":"1808.08315","MAG":"2953034721","DOI":"10.1109/BigData.2018.8622558","CorpusId":52096421},"title":"A Deterministic Self-Organizing Map Approach and its Application on Satellite Data based Cloud Type Classification"},{"paperId":"fef9d9eb2d527174ac5b329b0a044e98a1808971","externalIds":{"DBLP":"journals/corr/abs-1807-11714","ArXiv":"1807.11714","MAG":"3128232076","DOI":"10.1007/978-3-030-62077-6_14","CorpusId":51888520},"title":"Gender Bias in Neural Natural Language Processing"},{"paperId":"a78f9467070992fc8742641ec97f9972597d869a","externalIds":{"DBLP":"conf/icse/VermaR18","MAG":"2809878087","DOI":"10.1145/3194770.3194776","CorpusId":49561627},"title":"Fairness Definitions Explained"},{"paperId":"5d4af8c9321168f9ba7a501f33fb019fa2deaa22","externalIds":{"MAG":"2949534740","DBLP":"journals/corr/abs-1805-04508","ACL":"S18-2005","ArXiv":"1805.04508","DOI":"10.18653/v1/S18-2005","CorpusId":21670658},"title":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems"},{"paperId":"8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6","externalIds":{"ArXiv":"1805.01042","ACL":"S18-2023","DBLP":"conf/starsem/PoliakNHRD18","MAG":"2952914353","DOI":"10.18653/v1/S18-2023","CorpusId":21382535},"title":"Hypothesis Only Baselines in Natural Language Inference"},{"paperId":"9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","externalIds":{"MAG":"2963457723","DBLP":"conf/naacl/RudingerNLD18","ArXiv":"1804.09301","ACL":"N18-2002","DOI":"10.18653/v1/N18-2002","CorpusId":13756572},"title":"Gender Bias in Coreference Resolution"},{"paperId":"0be19fd9896e5d40222c690cc3ff553adc7c0e27","externalIds":{"MAG":"2963526187","DBLP":"conf/naacl/ZhaoWYOC18","ACL":"N18-2003","ArXiv":"1804.06876","DOI":"10.18653/v1/N18-2003","CorpusId":4952494},"title":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","externalIds":{"MAG":"2962736243","ArXiv":"1803.02324","DBLP":"journals/corr/abs-1803-02324","ACL":"N18-2017","DOI":"10.18653/v1/N18-2017","CorpusId":4537113},"title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"18858cc936947fc96b5c06bbe3c6c2faa5614540","externalIds":{"MAG":"2788481061","DBLP":"conf/fat/BuolamwiniG18","CorpusId":3298854},"title":"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"},{"paperId":"b5d7a19bd0bae10917a8e294960fdacf224d64fe","externalIds":{"DBLP":"journals/corr/abs-1711-08412","ArXiv":"1711.08412","MAG":"2769358515","DOI":"10.1073/pnas.1720347115","CorpusId":4930886,"PubMed":"29615513"},"title":"Word embeddings quantify 100 years of gender and ethnic stereotypes"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"2592aa0de9955a5b5bfc0039387dacb5874a1107","externalIds":{"MAG":"2964341672","ArXiv":"1907.09013","DBLP":"journals/bigdata/dAlessandroOL17","DOI":"10.1089/big.2016.0048","CorpusId":4414223,"PubMed":"28632437"},"title":"Conscientious Classification: A Data Scientist's Guide to Discrimination-Aware Classification"},{"paperId":"442e10a3c6640ded9408622005e3c2a8906ce4c2","externalIds":{"MAG":"2618851150","DBLP":"journals/corr/LundbergL17","ArXiv":"1705.07874","CorpusId":21889700},"title":"A Unified Approach to Interpreting Model Predictions"},{"paperId":"7bdbaf9b8f8ae53a9929643f1fd721593071be5b","externalIds":{"MAG":"2626255148","DBLP":"conf/smartcomp/Zhang17","DOI":"10.1109/SMARTCOMP.2017.7947035","CorpusId":30849030},"title":"PhD Forum: Recognizing Human Posture from Time-Changing Wearable Sensor Data Streams"},{"paperId":"237f2fdbaa553b452ede55ccf08002504473096c","externalIds":{"MAG":"2571597935","DBLP":"conf/bibm/ZhangTW16","DOI":"10.1109/BIBM.2016.7822695","CorpusId":1823395},"title":"Using the machine learning approach to predict patient survival from high-dimensional survival data"},{"paperId":"d42b11ce90c9c69a20ed015b73dc33e0e4100a7b","externalIds":{"DBLP":"journals/corr/HardtPS16","ArXiv":"1610.02413","MAG":"2530395818","CorpusId":7567061},"title":"Equality of Opportunity in Supervised Learning"},{"paperId":"5966d7c7f60898d610812e24c64d4d57855ad86a","externalIds":{"MAG":"3105700579","ArXiv":"1608.07187","DBLP":"journals/corr/IslamBN16","DOI":"10.1126/science.aal4230","CorpusId":23163324,"PubMed":"28408601"},"title":"Semantics derived automatically from language corpora contain human-like biases"},{"paperId":"ccf6a69a7f33bcf052aa7def176d3b9de495beb7","externalIds":{"DBLP":"conf/nips/BolukbasiCZSK16","MAG":"2950018712","ArXiv":"1607.06520","CorpusId":1704893},"title":"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","externalIds":{"DBLP":"conf/emnlp/PenningtonSM14","ACL":"D14-1162","MAG":"2250539671","DOI":"10.3115/v1/D14-1162","CorpusId":1957433},"title":"GloVe: Global Vectors for Word Representation"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","externalIds":{"MAG":"2950577311","DBLP":"journals/corr/abs-1301-3781","ArXiv":"1301.3781","CorpusId":5959482},"title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"e7d4ee33d7d4a79204389ea8815d44bdc6fc2b11","externalIds":{"MAG":"746832161","ACL":"W11-3715","CorpusId":18314974},"title":"Towards Enhanced Opinion Classification using NLP Techniques."},{"paperId":"adaa0523a5c9d5f92aa2009a51226391d8e62380","externalIds":{"MAG":"2949135123","DBLP":"journals/corr/abs-1104-3913","ArXiv":"1104.3913","DOI":"10.1145/2090236.2090255","CorpusId":13496699},"title":"Fairness through awareness"},{"paperId":"ababc1999b5f31409c78c39d1842219821e37a6d","externalIds":{"MAG":"2148506018","DBLP":"conf/kcap/NasukawaY03","DOI":"10.1145/945645.945658","CorpusId":3333256},"title":"Sentiment analysis: capturing favorability using natural language processing"},{"paperId":"c6586e7c73cc1c9e9a251947425c54c5051be626","externalIds":{"DBLP":"journals/pieee/Rosenfeld00","MAG":"2100506586","DOI":"10.1109/5.880083","CorpusId":10959945},"title":"Two decades of statistical language modeling: where do we go from here?"},{"paperId":"431047d198a842f085331db674f13775951a01fa","externalIds":{"MAG":"15592790","DBLP":"conf/interspeech/SteinbissTN94","DOI":"10.21437/ICSLP.1994-538","CorpusId":21336309},"title":"Improvements in beam search"},{"paperId":"b54c53ad5b8dc0af62395d0873fef6f12178d5c2","externalIds":{"MAG":"1841959837","CorpusId":57947977},"title":"Speech and Language Processing"},{"paperId":"347192591d37037d19b59f98931f420d4a12c9b7","externalIds":{"ACL":"2024.eacl-long.111","DBLP":"conf/eacl/ShrawgiRSD24","CorpusId":268417107},"title":"Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach"},{"paperId":"c988808bad22408274d511cffd77d902db398788","externalIds":{"DBLP":"journals/corr/abs-2404-08221","DOI":"10.48550/arXiv.2404.08221","CorpusId":269137391},"title":"Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI"},{"paperId":"f6c4b6e734ebdc1e378c49587ac1c345f0d25ca5","externalIds":{"DBLP":"conf/pkdd/WangCBCCZ24","DOI":"10.1007/978-3-031-70368-3_3","CorpusId":272554629},"title":"Advancing Graph Counterfactual Fairness Through Fair Representation Learning"},{"paperId":"04a84b5adedcdd73b40c21b4bb0d52a644e24163","externalIds":{"DBLP":"conf/pkdd/WangDYCWYZ24","DOI":"10.1007/978-3-031-70362-1_6","CorpusId":272427288},"title":"Individual Fairness with Group Awareness Under Uncertainty"},{"paperId":"c373c792bcf5d0add8de812425d384ff101ef070","externalIds":{"DBLP":"conf/acl/YuJKYJ23","DOI":"10.18653/v1/2023.findings-acl.375","CorpusId":259859034},"title":"Unlearning Bias in Language Models by Partitioning Gradients"},{"paperId":"f7f4fabd2e6cfd76106975d4e479f03dbe53a3c2","externalIds":{"ACL":"2023.ranlp-1.119","DBLP":"conf/ranlp/SobhaniSD23","DOI":"10.26615/978-954-452-092-2_119","CorpusId":265068329},"title":"Measuring Gender Bias in Natural Language Processing: Incorporating Gender-Neutral Linguistic Forms for Non-Binary Gender Identities in Abusive Speech Detection"},{"paperId":"60999ddb99aea5a93bd2ba16fb7671dc76bf3ba5","externalIds":{"ACL":"2023.acl-long.232","DBLP":"conf/acl/ZhouMYYZ23","DOI":"10.18653/v1/2023.acl-long.232","CorpusId":259370743},"title":"Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning"},{"paperId":"13b8060acc3db1fc555f6e55368f6d02899a1698","externalIds":{"DBLP":"conf/acl/FleisigAABDOSVW23","ACL":"2023.acl-long.343","DOI":"10.18653/v1/2023.acl-long.343","CorpusId":259092939},"title":"FairPrism: Evaluating Fairness-Related Harms in Text Generation"},{"paperId":"0995fc83865e03a592c21bd5f21ef15f9b3012d1","externalIds":{"DOI":"10.2139/ssrn.4339839","CorpusId":256571867},"title":"ChatGPT by OpenAI: The End of Litigation Lawyers?"},{"paperId":"47c2aa5ceaf40cce9a778ca80cbb49c0cd3f7552","externalIds":{"DBLP":"journals/corr/abs-2308-11596","CorpusId":267200805},"title":"SeamlessM4T-Massively Multilingual & Multimodal Machine Translation"},{"paperId":"d5c2947cab82c44e3cca8e90486da10a81e1f697","externalIds":{"DBLP":"journals/corr/abs-2309-14345","DOI":"10.48550/arXiv.2309.14345","CorpusId":271269343},"title":"Bias Assessment and Mitigation in LLM-based Code Generation"},{"paperId":"e81466aab95dfba46b27f5d24dd3d2860cad45cd","externalIds":{"DBLP":"journals/corr/abs-2305-14328","DOI":"10.48550/arXiv.2305.14328","CorpusId":275357621},"title":"Empowering LLM-based Machine Translation with Cultural Awareness"},{"paperId":"b79bb5e86b0836cb1d305bf7d0481383e39b37b4","externalIds":{"ACL":"2022.acl-long.72","DBLP":"conf/acl/GuoYA22","DOI":"10.18653/v1/2022.acl-long.72","CorpusId":248780440},"title":"Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts"},{"paperId":"058dee85d522f6565fe1502cafcf9a5e3f6a6f0e","externalIds":{"ACL":"2022.naacl-main.122","DBLP":"conf/naacl/DelobelleTCB22","DOI":"10.18653/v1/2022.naacl-main.122","CorpusId":250390561},"title":"Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models"},{"paperId":"72128b2da0ffb784861889462070570b21017b9f","externalIds":{"ACL":"2022.acl-long.583","DBLP":"conf/acl/NeveolDBF22","DOI":"10.18653/v1/2022.acl-long.583","CorpusId":248780290},"title":"French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English"},{"paperId":"6dc9b109c995873c33f13722d0541a87f3362ac0","externalIds":{"DBLP":"journals/corr/abs-2205-12689","CorpusId":263891555},"title":"Large Language Models are Zero-Shot Clinical Information Extractors"},{"paperId":"5aa69bbe1a00e3edadb4bb10c9c14beeaebf2690","externalIds":{"ACL":"2022.gebnlp-1.28","DOI":"10.18653/v1/2022.gebnlp-1.28","CorpusId":250390445},"title":"Incorporating Subjectivity into Gendered Ambiguous Pronoun (GAP) Resolution using Style Transfer"},{"paperId":"2972ad9cd2f5a8efaffce15fc527e1a8644b081a","externalIds":{"ACL":"2022.bigscience-1.6","DOI":"10.18653/v1/2022.bigscience-1.6","CorpusId":248780490},"title":"Pipelines for Social Bias Testing of Large Language Models"},{"paperId":"8d863cafea3493fb033fcdcf9f272a1a4912628b","externalIds":{"DBLP":"conf/acl/SteedPKW22","ACL":"2022.acl-long.247","DOI":"10.18653/v1/2022.acl-long.247","CorpusId":248780439},"title":"Upstream Mitigation Is \n Not\n All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models"},{"paperId":"ed5ebed7ff668fd7362d531a40b49b3aea33b3a9","externalIds":{"DBLP":"journals/corr/abs-2212-10678","DOI":"10.48550/arXiv.2212.10678","CorpusId":273994217},"title":"Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing"},{"paperId":"42cc067475c05a9801b8a0952a28e7b8dadc35a4","externalIds":{"DBLP":"conf/spaca/Wang0JW21","CorpusId":235468748},"title":"Harmonic-Mean Cox Models: A Ruler for Equal Attention to Risk"},{"paperId":"0fc78f61aafd8bb791eecdbdf747814aea950a06","externalIds":{"DBLP":"conf/dexa/ZhangZZCGZZE21","DOI":"10.1007/978-3-030-86475-0_9","CorpusId":237403859},"title":"AutoEncoder for Neuroimage"},{"paperId":"080df61ee1c15ff3c8e5d0d82d60bfd80e372e38","externalIds":{"ACL":"2021.acl-long.329","DBLP":"conf/acl/OusidhoumZFSY20","DOI":"10.18653/v1/2021.acl-long.329","CorpusId":236460108},"title":"Probing Toxic Content in Large Pre-Trained Language Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9819b600a828a57e1cde047bbe710d3446b30da5","externalIds":{"MAG":"179875071","DBLP":"conf/interspeech/MikolovKBCK10","DOI":"10.21437/Interspeech.2010-343","CorpusId":17048224},"title":"Recurrent neural network based language model"},{"paperId":"231f6de83cfa4d641da1681e97a11b689a48e3aa","externalIds":{"MAG":"2569383081","DOI":"10.2307/2670189","CorpusId":12495425},"title":"Statistical methods for speech recognition"}]}