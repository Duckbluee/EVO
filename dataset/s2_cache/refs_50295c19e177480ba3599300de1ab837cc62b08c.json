{"references":[{"paperId":"f96898d15a1bf1fa8925b1280d0e07a7a8e72194","externalIds":{"ArXiv":"1603.01417","MAG":"2963579811","DBLP":"journals/corr/XiongMS16","CorpusId":14294589},"title":"Dynamic Memory Networks for Visual and Textual Question Answering"},{"paperId":"45b47d87736cf45924abe9bc4761ca0894de54cd","externalIds":{"MAG":"2255045308","ArXiv":"1602.02672","DBLP":"journals/corr/FoersterAFW16","CorpusId":17473440},"title":"Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks"},{"paperId":"846aedd869a00c09b40f1f1f35673cb22bc87490","externalIds":{"DBLP":"journals/nature/SilverHMGSDSAPL16","MAG":"2257979135","DOI":"10.1038/nature16961","CorpusId":515925,"PubMed":"26819042"},"title":"Mastering the game of Go with deep neural networks and tree search"},{"paperId":"83bf91012997019f432179aad798e6d3fbb95c36","externalIds":{"DBLP":"journals/corr/TampuuMKKKAAV15","PubMedCentral":"5381785","ArXiv":"1511.08779","MAG":"2951120231","DOI":"10.1371/journal.pone.0172395","CorpusId":12046082,"PubMed":"28380078"},"title":"Multiagent cooperation and competition with deep reinforcement learning"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","externalIds":{"MAG":"2173051530","DBLP":"journals/corr/KaiserS15","ArXiv":"1511.08228","CorpusId":2009318},"title":"Neural GPUs Learn Algorithms"},{"paperId":"b90e5b8cb0357a412770a8f3aa820ee292686d5e","externalIds":{"ArXiv":"1511.07401","MAG":"2174196774","DBLP":"journals/corr/SukhbaatarSSCF15","CorpusId":14542167},"title":"MazeBase: A Sandbox for Learning from Games"},{"paperId":"492f57ee9ceb61fb5a47ad7aebfec1121887a175","externalIds":{"MAG":"2244807774","ArXiv":"1511.05493","DBLP":"journals/corr/LiTBZ15","CorpusId":8393918},"title":"Gated Graph Sequence Neural Networks"},{"paperId":"4d05ab884a6c1645b80ce5d02b09c7e5ff499790","externalIds":{"ArXiv":"1508.05508","DBLP":"journals/corr/PengLLW15","MAG":"2133585753","CorpusId":402276},"title":"Towards Neural Network-based Reasoning"},{"paperId":"b6b8a1b80891c96c28cc6340267b58186157e536","externalIds":{"MAG":"2952125531","DBLP":"journals/jmlr/LevineFDA16","ArXiv":"1504.00702","DOI":"10.5555/2946645.2946684","CorpusId":7242892},"title":"End-to-End Training of Deep Visuomotor Policies"},{"paperId":"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","externalIds":{"DBLP":"conf/nips/SukhbaatarSWF15","MAG":"2951008357","CorpusId":1399322},"title":"End-To-End Memory Networks"},{"paperId":"340f48901f72278f6bf78a04ee5b01df208cc508","externalIds":{"DBLP":"journals/nature/MnihKSRVBGRFOPB15","MAG":"2145339207","DOI":"10.1038/nature14236","CorpusId":205242740,"PubMed":"25719670"},"title":"Human-level control through deep reinforcement learning"},{"paperId":"abb33d75dc297993fcc3fb75e0f4498f413eb4f6","externalIds":{"MAG":"2949122016","ArXiv":"1502.05698","DBLP":"journals/corr/WestonBCM15","CorpusId":3178759},"title":"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","externalIds":{"MAG":"2964121744","DBLP":"journals/corr/KingmaB14","ArXiv":"1412.6980","CorpusId":6628106},"title":"Adam: A Method for Stochastic Optimization"},{"paperId":"127f464c2dc8d85b7612a6924495f79e5458710f","externalIds":{"ArXiv":"1412.6564","MAG":"1947291763","DBLP":"journals/corr/MaddisonHSS14","CorpusId":7355762},"title":"Move Evaluation in Go Using Deep Convolutional Neural Networks"},{"paperId":"b6cc21b30912bdaecd9f178d700a4c545b1d0838","externalIds":{"MAG":"2151210636","DBLP":"conf/nips/GuoSLLW14","CorpusId":2187487},"title":"Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning"},{"paperId":"dbb9845f405152fd6748b17d54fc6984626618fe","externalIds":{"DBLP":"conf/atal/ZhangL13","MAG":"6043852","DOI":"10.65109/zash3647","CorpusId":7085652},"title":"Coordinating multi-agent reinforcement learning with limited communication"},{"paperId":"b4eceb166de3f0c96ceae657975b8394c890a4f4","externalIds":{"MAG":"2739551265","DBLP":"journals/tii/CaoY0C13","ArXiv":"1207.3231","DOI":"10.1109/TII.2012.2219061","CorpusId":9588126},"title":"An Overview of Recent Progress in the Study of Distributed Multi-Agent Coordination"},{"paperId":"e8fb7221ceae17a19dd4acfc49da8f0133916fb1","externalIds":{"MAG":"1450304033","DBLP":"conf/eumas/MeloSW11","DOI":"10.1007/978-3-642-34799-3_13","CorpusId":16879675},"title":"QueryPOMDP: POMDP-Based Communication in Multiagent Systems"},{"paperId":"6d34117fe36737b875115625d0d616a51389da3a","externalIds":{"DBLP":"journals/ras/MaravallAD13","MAG":"2043931021","DOI":"10.1016/j.robot.2012.09.016","CorpusId":6523309},"title":"Coordination of communication in robot teams by reinforcement learning"},{"paperId":"8de174ab5419b9d3127695405efd079808e956e8","externalIds":{"MAG":"2296073425","DBLP":"conf/icml/BengioLCW09","DOI":"10.1145/1553374.1553380","CorpusId":873046},"title":"Curriculum learning"},{"paperId":"8b95b36c5729d73f48e47563986a9ddb0a251898","externalIds":{"MAG":"2126222949","DOI":"10.1109/SMCIA.2008.5045926","CorpusId":15768446},"title":"Learning of communication codes in multi-agent reinforcement learning problem"},{"paperId":"4aece8df7bd59e2fbfedbf5729bba41abc56d870","externalIds":{"DBLP":"journals/tsmc/BusoniuBS08","MAG":"2099618002","DOI":"10.1109/TSMCC.2007.913919","CorpusId":206794869},"title":"A Comprehensive Survey of Multiagent Reinforcement Learning"},{"paperId":"aa6be519b394b44ab24c6ad964f8a2c6a9b23571","externalIds":{"MAG":"2160643434","DBLP":"journals/pieee/Olfati-SaberFM07","DOI":"10.1109/JPROC.2006.887293","CorpusId":6533249},"title":"Consensus and Cooperation in Networked Multi-Agent Systems"},{"paperId":"f789a35704db4f90516de7d469f1b32e87e128e0","externalIds":{"MAG":"1607316222","DBLP":"conf/wrac/GilesJ02","DOI":"10.1007/978-3-540-45173-0_29","CorpusId":6439950},"title":"Learning Communication for Multi-agent Systems"},{"paperId":"ba34fd2fd9628d92d2f4dcff73277d15c35bd6cb","externalIds":{"MAG":"1973039793","DBLP":"journals/cogsr/Littman01","DOI":"10.1016/S1389-0417(01)00015-8","CorpusId":2943343},"title":"Value-function reinforcement learning in Markov games"},{"paperId":"c0fda2ab14245ca3775fdfc9b0fef1d309dfdb04","externalIds":{"DBLP":"conf/nips/GuestrinKP01","MAG":"2134779831","CorpusId":6487585},"title":"Multiagent Planning with Factored MDPs"},{"paperId":"9d94165d22ae17b7d933dafffabb16ad2b8e147a","externalIds":{"MAG":"1560074431","DBLP":"conf/icml/LauerR00","CorpusId":28535139},"title":"An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems"},{"paperId":"ec350223de4dbc7ddb1e8fd1d2fbeb2288a992e9","externalIds":{"DBLP":"journals/arobots/FoxBKT00","MAG":"2128453677","DOI":"10.1023/A:1008937911390","CorpusId":11568446},"title":"A Probabilistic Approach to Collaborative Multi-Robot Localization"},{"paperId":"db3dd93699cb1c27edfaa430a93c7ab1d8a14275","externalIds":{"MAG":"1574700590","DBLP":"journals/ml/CritesB98","DOI":"10.1023/A:1007518724497","CorpusId":2306465},"title":"Elevator Group Control Using Multiple Reinforcement Learning Agents"},{"paperId":"eb6412a2fa74513609ab64c09970ae8fde747a1c","externalIds":{"MAG":"1507591516","DBLP":"journals/arobots/Matari97","DOI":"10.1023/A:1008819414322","CorpusId":14610547},"title":"Reinforcement Learning in the Multi-Robot Domain"},{"paperId":"99485775cce4c1afa7361a7fcbd3c9d362309554","externalIds":{"DBLP":"conf/aaai/Pearl82","MAG":"2156094048","DOI":"10.1145/3501714.3501727","CorpusId":14936636},"title":"Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach"},{"paperId":"3efd851140aa28e95221b55fcc5659eea97b172d","externalIds":{"DBLP":"journals/tnn/ScarselliGTHM09","MAG":"2116341502","DOI":"10.1109/TNN.2008.2005605","CorpusId":206756462,"PubMed":"19068426"},"title":"The Graph Neural Network Model"},{"paperId":"116de409c4d61fb47d275081f19a0270bfaaf1fd","externalIds":{"DBLP":"conf/dars/VarshavskayaKR08","MAG":"1863534978","DOI":"10.1007/978-3-642-00644-9_33","CorpusId":12526694},"title":"Efficient Distributed Reinforcement Learning through Agreement"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","externalIds":{"DOI":"10.1023/A:1022672621406","CorpusId":2332513},"title":"Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"},{"paperId":"b3fc56876ad1cdf35ad4af13b991bbb24d219bd9","externalIds":{"CorpusId":260435822},"title":"Multi Agent Reinforcement Learning Independent vs Cooperative Agents"},{"paperId":"424f13775159c69d8dd5bfa4f4c662a81b526a81","externalIds":{"DBLP":"conf/nips/WangS02","MAG":"2145067550","CorpusId":2908243},"title":"Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games"},{"paperId":"de3342a80c684940a1a98128e1fc15dcd05d2719","externalIds":{"MAG":"2148831627","DBLP":"journals/ijmms/StoneV98","DOI":"10.1006/ijhc.1997.0162","CorpusId":171919},"title":"Towards collaborative and adversarial learning: a case study in robotic soccer"},{"paperId":"e29f7b2aae8db749e90c10f6d763ab76c1b78646","externalIds":{"DBLP":"conf/dars/1996","DOI":"10.1007/978-4-431-66942-5","CorpusId":86543265},"title":"Distributed Autonomous Robotic Systems 2, Proceedings of the 3rd International Symposium on Distributed Autonomous Robotic Systems, DARS 1996, Wako-shi, Saitama, Japan, October 1996"}]}