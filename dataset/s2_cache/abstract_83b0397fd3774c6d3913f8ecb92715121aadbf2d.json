{"abstract":"Federated learning (FL) has emerged as a privacy-aware collaborative learning paradigm where participants jointly train a powerful model without sharing their private data. One desirable property for FL is the implementation of the <italic>right to be forgotten (RTBF)</italic>, i.e., a leaving participant has the right to request the deletion of its private data from the global model. However, <italic>unlearning itself may not be enough to implement RTBF unless the unlearning effect can be independently verified</italic>, an important aspect that has been overlooked in the current literature. Unlearning verification is particularly challenging in FL as the unlearning effect on one participant's data could be canceled by the contribution of other participants. In this work, we prompt the concept of <italic>verifiable federated unlearning</italic> and propose <sc>VeriFi</sc>, a unified framework that allows systematic analysis of federated unlearning and quantification of its effect, with different combinations of various unlearning and verification methods. In <sc>VeriFi</sc>, the leaving participant is granted the <italic>right to verify (RTV)</italic> to actively verify the unlearning effect in the next few rounds immediately after notifying the server of its intention to leave, along with local verification done through two steps: 1) <italic>marking</italic> that fingerprints the leaving participant by specially-designed <italic>markers</italic> and 2) <italic>checking</italic> that examines the global model's performance change on the markers. Based on <sc>VeriFi</sc>, we have conducted so far the most systematic study on verifiable federated unlearning, covering six unlearning methods and five verification methods. Our study sheds light on the existing drawbacks and potential alternatives for both unlearning and verification methods. During the study, we also propose a more efficient and FL-friendly unlearning method <inline-formula><tex-math notation=\"LaTeX\">$^{u}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mi>u</mml:mi></mml:msup></mml:math><inline-graphic xlink:href=\"chen-ieq1-3382321.gif\"/></alternatives></inline-formula>S2U, and two more effective and robust non-invasive (without training controllability, external data, white-box model access nor introducing new security risks) verification methods <inline-formula><tex-math notation=\"LaTeX\">$^{v}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mi>v</mml:mi></mml:msup></mml:math><inline-graphic xlink:href=\"chen-ieq2-3382321.gif\"/></alternatives></inline-formula>FM and <inline-formula><tex-math notation=\"LaTeX\">$^{v}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mi>v</mml:mi></mml:msup></mml:math><inline-graphic xlink:href=\"chen-ieq3-3382321.gif\"/></alternatives></inline-formula>EM. While the proposed methods may not be a panacea for all the challenges, they address several key drawbacks of existing methods and represent a promising step toward effective, efficient, robust, and more importantly, non-invasive federated unlearning and verification. We extensively evaluate <sc>VeriFi</sc> on seven datasets, including natural/facial/medical images and audios, and four types of deep learning models, including both Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We hope, such an extensive and holistic experimental evaluation, although admittedly complex and challenging, could help establish important empirical understandings, evidence, and insights for trustworthy federated unlearning."}