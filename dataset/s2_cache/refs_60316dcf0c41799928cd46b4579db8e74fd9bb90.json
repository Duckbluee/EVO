{"references":[{"paperId":"8036a1061c7da7ec696293320648fbe7c6856b01","externalIds":{"DBLP":"journals/corr/abs-2506-21547","ArXiv":"2506.21547","DOI":"10.48550/arXiv.2506.21547","CorpusId":280011997},"title":"SAM4D: Segment Anything in Camera and LiDAR Streams"},{"paperId":"b84b2cc5f9c4e4d8b24bd51ece3666affb94a8fe","externalIds":{"DBLP":"conf/aaai/YangLZPG0CG0GZ25","DOI":"10.1609/aaai.v39i9.33001","CorpusId":277768115},"title":"LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding"},{"paperId":"4d9e66789ea1fe8930a482194a4ae36679506359","externalIds":{"DBLP":"journals/corr/abs-2410-23262","ArXiv":"2410.23262","DOI":"10.48550/arXiv.2410.23262","CorpusId":273695673},"title":"EMMA: End-to-End Multimodal Model for Autonomous Driving"},{"paperId":"2e7328eddcf0f300888c9bf8ab300a92be04b7d7","externalIds":{"ArXiv":"2410.22313","DBLP":"journals/corr/abs-2410-22313","DOI":"10.48550/arXiv.2410.22313","CorpusId":273661831},"title":"Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving"},{"paperId":"a22b23b65cdbc0335b872ed7c7a7206c710f8997","externalIds":{"ArXiv":"2409.12191","DBLP":"journals/corr/abs-2409-12191","DOI":"10.48550/arXiv.2409.12191","CorpusId":272704132},"title":"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"},{"paperId":"ee6c8c9a8874a57a778c1ccc6ef41346162e0750","externalIds":{"ArXiv":"2409.10228","DBLP":"journals/corr/abs-2409-10228","DOI":"10.48550/arXiv.2409.10228","CorpusId":272690282},"title":"Robust Bird's Eye View Segmentation by Adapting DINOv2"},{"paperId":"1e4685a84f7b9c9dadff0fd1c0e7a5cb9fcf0090","externalIds":{"ArXiv":"2403.08032","DBLP":"journals/corr/abs-2403-08032","DOI":"10.48550/arXiv.2403.08032","CorpusId":268379606},"title":"LG-Traj: LLM Guided Pedestrian Trajectory Prediction"},{"paperId":"758c2dc290c037a6f211ec503beee70abe2d1197","externalIds":{"DBLP":"conf/corl/TianGLLWZZJLZ24","ArXiv":"2402.12289","DOI":"10.48550/arXiv.2402.12289","CorpusId":267750682},"title":"DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models"},{"paperId":"867c82da010e0cb2c69e7d8fe12f94ba6a49ee74","externalIds":{"ArXiv":"2312.14925","DBLP":"journals/corr/abs-2312-14925","DOI":"10.48550/arXiv.2312.14925","CorpusId":266521540},"title":"A Survey of Reinforcement Learning from Human Feedback"},{"paperId":"c708931122d29d9a14df1640555c6eb6775ed95b","externalIds":{"DBLP":"journals/corr/abs-2312-03408","ArXiv":"2312.03408","DOI":"10.48550/arXiv.2312.03408","CorpusId":265697020},"title":"Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future"},{"paperId":"91e3906550821c4624146e6e87db36c3296e773a","externalIds":{"ArXiv":"2311.12144","DBLP":"journals/corr/abs-2311-12144","DOI":"10.48550/arXiv.2311.12144","CorpusId":265308588},"title":"Applications of Large Scale Foundation Models for Autonomous Driving"},{"paperId":"357e182a38219625dd37cba526befe5f8429aa4b","externalIds":{"ArXiv":"2311.10813","DBLP":"journals/corr/abs-2311-10813","DOI":"10.48550/arXiv.2311.10813","CorpusId":265294541},"title":"A Language Agent for Autonomous Driving"},{"paperId":"0333cf45841955d19de5096d5fd0ee55fdcd15ad","externalIds":{"ArXiv":"2311.08206","DBLP":"journals/corr/abs-2311-08206","DOI":"10.1109/WACVW60836.2024.00108","CorpusId":265157758},"title":"Human-Centric Autonomous Systems With LLMs for User Command Reasoning"},{"paperId":"c85845796afd368853cb9ac51c308f2f9356bab1","externalIds":{"DBLP":"journals/corr/abs-2311-05332","ArXiv":"2311.05332","DOI":"10.48550/arXiv.2311.05332","CorpusId":265067382},"title":"On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving"},{"paperId":"2313afae52d98e569da2dedbf14daf9efc74e7cf","externalIds":{"DBLP":"journals/corr/abs-2311-03079","ArXiv":"2311.03079","DOI":"10.48550/arXiv.2311.03079","CorpusId":265034288},"title":"CogVLM: Visual Expert for Pretrained Language Models"},{"paperId":"5b7c820dec29f38438b89bc6d483a63de0084402","externalIds":{"DBLP":"conf/iclr/ZhangX000U24","ArXiv":"2311.01017","CorpusId":264935456},"title":"Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion"},{"paperId":"84d99893ee24fc825e359598d44d602c45c4865e","externalIds":{"ArXiv":"2311.01043","DBLP":"journals/corr/abs-2311-01043","DOI":"10.48550/arXiv.2311.01043","CorpusId":264935408},"title":"LLM4Drive: A Survey of Large Language Models for Autonomous Driving"},{"paperId":"e3b67ed3e4de785b27e7ebacc0c274e0e7097219","externalIds":{"DBLP":"conf/eccv/PengCQKLSWZM24","ArXiv":"2310.08820","DOI":"10.1007/978-3-031-72775-7_4","CorpusId":264128251},"title":"Learning to Adapt SAM for Segmenting Cross-Domain Point Clouds"},{"paperId":"10158879cdb64ce7d3f7bb5572c4617ea808602e","externalIds":{"ArXiv":"2310.08034","DBLP":"journals/corr/abs-2310-08034","DOI":"10.1109/MITS.2024.3381793","CorpusId":263908840},"title":"Receive, Reason, and React: Drive as You Say, With Large Language Models in Autonomous Vehicles"},{"paperId":"19933dd9e03058e686ef412262eef7696cce3e8f","externalIds":{"ArXiv":"2310.03026","DBLP":"journals/corr/abs-2310-03026","DOI":"10.48550/arXiv.2310.03026","CorpusId":263620279},"title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving"},{"paperId":"f01ff5acf9e086030c01beda6f433f99013ebbd4","externalIds":{"DBLP":"conf/icra/ChenSHKWBMS24","ArXiv":"2310.01957","DOI":"10.1109/ICRA57147.2024.10611018","CorpusId":263608168},"title":"Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"},{"paperId":"77693ca00a8ef775af96b5c450aa0afdb0e10a51","externalIds":{"DBLP":"journals/corr/abs-2310-02251","ArXiv":"2310.02251","DOI":"10.1109/ICRA57147.2024.10611485","CorpusId":263608641},"title":"Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving"},{"paperId":"ccd6f8b6544f112de632e49bfbe592a0a654537d","externalIds":{"ArXiv":"2310.01412","DBLP":"journals/ral/XuZXZGWLZ24","DOI":"10.1109/LRA.2024.3440097","CorpusId":263605524},"title":"DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model"},{"paperId":"958ed4830ae80a189ecb9b93ab75a6ce2e3926fc","externalIds":{"DBLP":"journals/corr/abs-2310-01415","ArXiv":"2310.01415","DOI":"10.48550/arXiv.2310.01415","CorpusId":263605637},"title":"GPT-Driver: Learning to Drive with GPT"},{"paperId":"98478ac589e5b40a20630ff54bb4eec4ab4c5f6b","externalIds":{"ArXiv":"2309.17080","DBLP":"journals/corr/abs-2309-17080","DOI":"10.48550/arXiv.2309.17080","CorpusId":263310665},"title":"GAIA-1: A Generative World Model for Autonomous Driving"},{"paperId":"3cbfe152220de84ecf8059fa50c47587a3134c86","externalIds":{"DBLP":"conf/iclr/WenF0C0CDS0024","ArXiv":"2309.16292","DOI":"10.48550/arXiv.2309.16292","CorpusId":263136146},"title":"DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models"},{"paperId":"0ef8bbccbfab7e6d9c8bd09bcadfe9c5bbbff512","externalIds":{"ArXiv":"2309.09777","DBLP":"journals/corr/abs-2309-09777","DOI":"10.48550/arXiv.2309.09777","CorpusId":262044078},"title":"DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"7679dc8534cb1dd65c63c50b38f56386228d32d1","externalIds":{"ArXiv":"2309.05282","DBLP":"journals/corr/abs-2309-05282","DOI":"10.48550/arXiv.2309.05282","CorpusId":261681760},"title":"Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","externalIds":{"DBLP":"journals/corr/abs-2308-00692","ArXiv":"2308.00692","DOI":"10.1109/CVPR52733.2024.00915","CorpusId":260351258},"title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"38939304bb760473141c2aca0305e44fbe04e6e8","externalIds":{"ArXiv":"2307.15818","DBLP":"conf/corl/ZitkovichYXXXXW23","DOI":"10.48550/arXiv.2307.15818","CorpusId":260293142},"title":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"},{"paperId":"7726e8bd28be2c5580d9ce511d16382164fc4a30","externalIds":{"DBLP":"conf/itsc/AtakishiyevSBG23","ArXiv":"2307.10408","DOI":"10.1109/ITSC57777.2023.10421901","CorpusId":259991336},"title":"Explaining Autonomous Driving Actions with Visual Question Answering"},{"paperId":"c651cfd518de20c7c15450ab4ebd5bd843072160","externalIds":{"DBLP":"conf/corl/TanIW0K23","ArXiv":"2307.07947","DOI":"10.48550/arXiv.2307.07947","CorpusId":259937160},"title":"Language Conditioned Traffic Generation"},{"paperId":"5fb40cfc02fecaaeb2a381a979c3cb7c736a608d","externalIds":{"ArXiv":"2306.13290","DBLP":"journals/corr/abs-2306-13290","DOI":"10.48550/arXiv.2306.13290","CorpusId":259243807},"title":"Robustness of Segment Anything Model (SAM) for Autonomous Driving in Adverse Weather Conditions"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"1ed36ffa0555efa22542a0af26d04fb809dceb33","externalIds":{"ArXiv":"2306.02245","DBLP":"journals/chinaf/ZhangLYZYLB24","DOI":"10.1007/s11432-023-3943-6","CorpusId":259075893},"title":"SAM3D: zero-shot 3D object detection via the segment anything model"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"231a434f8fac0b01cbc05890b283f4d9da4cb100","externalIds":{"DBLP":"journals/corr/abs-2305-09972","ArXiv":"2305.09972","DOI":"10.48550/arXiv.2305.09972","CorpusId":258741093},"title":"Real-Time Flying Object Detection with YOLOv8"},{"paperId":"bbdc4118df106d4ba7af9d7d94d7f0a1144c11e2","externalIds":{"ArXiv":"2305.06558","DBLP":"journals/corr/abs-2305-06558","DOI":"10.48550/arXiv.2305.06558","CorpusId":258615204},"title":"Segment and Track Anything"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0","externalIds":{"DBLP":"conf/eccv/LiuZRLZYJLYSZZ24","ArXiv":"2303.05499","DOI":"10.48550/arXiv.2303.05499","CorpusId":257427307},"title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"4cc6db9253bcc970151cfad636ca31677120ba35","externalIds":{"DBLP":"journals/corr/abs-2301-05709","ArXiv":"2301.05709","DOI":"10.1109/CVPR52729.2023.00686","CorpusId":255942155},"title":"Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss"},{"paperId":"fdd7d5b0f6b8641c356e170fd264cd11f70ba657","externalIds":{"DBLP":"conf/cvpr/HuYCLSZCDLWLJLD23","ArXiv":"2212.10156","DOI":"10.1109/CVPR52729.2023.01712","CorpusId":257687420},"title":"Planning-oriented Autonomous Driving"},{"paperId":"7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6","externalIds":{"DBLP":"journals/corr/abs-2210-17323","ArXiv":"2210.17323","CorpusId":253237200},"title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"},{"paperId":"4f005e799bbc7a608f9c37da02a3a9ab14726d43","externalIds":{"DBLP":"conf/kbse/WangS0CZZ0M22","DOI":"10.1145/3551349.3559528","CorpusId":254019913},"title":"ADEPT: A Testing Platform for Simulated Autonomous Driving"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","externalIds":{"DBLP":"journals/corr/abs-2209-00796","ArXiv":"2209.00796","DOI":"10.1145/3626235","CorpusId":252070859},"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"6caa7cce613702a2b204642e2324c61598921e56","externalIds":{"DBLP":"journals/corr/abs-2207-07601","ArXiv":"2207.07601","DOI":"10.48550/arXiv.2207.07601","CorpusId":250607597},"title":"ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning"},{"paperId":"8f6143b53c89ceefbb177810242901d024997368","externalIds":{"DBLP":"journals/corr/abs-2206-10249","ArXiv":"2206.10249","DOI":"10.48550/arXiv.2206.10249","CorpusId":249889458},"title":"Incorporating Voice Instructions in Model-Based Reinforcement Learning for Self-Driving Cars"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","externalIds":{"DBLP":"conf/nips/HoSGC0F22","ArXiv":"2204.03458","DOI":"10.48550/arXiv.2204.03458","CorpusId":248006185},"title":"Video Diffusion Models"},{"paperId":"6550076df01275d6e9efcdbbc970e04cb6b761da","externalIds":{"DBLP":"journals/corr/abs-2203-16258","ArXiv":"2203.16258","DOI":"10.1109/CVPR52688.2022.00966","CorpusId":247793124},"title":"Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","externalIds":{"MAG":"3033156098","CorpusId":226096901},"title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"26051e0070a59444eefe5bed36627703235405b7","externalIds":{"DOI":"10.1007/1-4020-0613-6_16833","CorpusId":240640357},"title":"Segment"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"9d0907770cd4619aa6a36139a859e8f09bc9f0ef","externalIds":{"MAG":"2885138528","ArXiv":"1807.11546","DBLP":"conf/eccv/KimRDCA18","DOI":"10.1007/978-3-030-01216-8_35","CorpusId":51887402},"title":"Textual Explanations for Self-Driving Vehicles"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"2dcef55a07f8607a819c21fe84131ea269cc2e3c","externalIds":{"MAG":"2129069237","DBLP":"journals/corr/Sohl-DicksteinW15","ArXiv":"1503.03585","CorpusId":14888175},"title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"},{"paperId":"558738465079bc9cf51d5b64f27b6cd8f5efaf0b","externalIds":{"DBLP":"conf/icip/VannierH98","MAG":"2152591411","DOI":"10.1109/ICIP.1998.723309","CorpusId":38648025},"title":"Biomedical image segmentation"},{"paperId":"fdfe5d147b43560003c345485dcd6afb59cd648e","externalIds":{"DBLP":"journals/corr/abs-2406-06211","DOI":"10.48550/arXiv.2406.06211","CorpusId":270371911},"title":"iMotion-LLM: Motion Prediction Instruction Tuning"},{"paperId":"825298ea1933f01f1cfbff0c5e9bca4c60fdb5cd","externalIds":{"DBLP":"journals/corr/abs-2309-13193","DOI":"10.48550/arXiv.2309.13193","CorpusId":262460657},"title":"SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model"},{"paperId":"3f68deda3b463794991699cd53d315c32c2ffe97","externalIds":{"CorpusId":258588387},"title":"TARGET: Automated Scenario Generation from Traffic Rules for Testing Autonomous Vehicles"},{"paperId":"2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c","externalIds":{"DBLP":"journals/corr/abs-2306-00978","DOI":"10.48550/arXiv.2306.00978","CorpusId":271271084},"title":"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"},{"paperId":"38b99dbe4f49c6b265abb8f2b703bd53e1b53459","externalIds":{"DOI":"10.4135/9788132108009.n20","CorpusId":221394872},"title":"Diffusion"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"089254dfb8fb0fbfac55696d1f9eba02e4079e97","externalIds":{"CorpusId":220302183},"title":"Visual Question Answering"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"90b70f84d81622a699a7b07ab2fe49a9c375d870","externalIds":{"DOI":"10.4324/9780203842737-7","CorpusId":18801023},"title":"IN REAL-TIME"}]}