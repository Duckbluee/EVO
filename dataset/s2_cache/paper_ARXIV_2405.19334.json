{"paperId":"6da7c160dc6343718771e1e29749359479b071d0","externalIds":{"ArXiv":"2405.19334","DBLP":"journals/corr/abs-2405-19334","DOI":"10.48550/arXiv.2405.19334","CorpusId":270095319},"title":"LLMs Meet Multimodal Generation and Editing: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2405.19334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2154314501","name":"Yin-Yin He"},{"authorId":"46270766","name":"Zhaoyang Liu"},{"authorId":"2244136105","name":"Jingye Chen"},{"authorId":"2287997074","name":"Zeyue Tian"},{"authorId":"2303802159","name":"Hongyu Liu"},{"authorId":"2192825554","name":"Xiaowei Chi"},{"authorId":"9326827","name":"Runtao Liu"},{"authorId":"2303654661","name":"Ruibin Yuan"},{"authorId":"2072727444","name":"Yazhou Xing"},{"authorId":"2257133501","name":"Wenhai Wang"},{"authorId":"2292283383","name":"Jifeng Dai"},{"authorId":"2400660051","name":"Yong Zhang"},{"authorId":"2239201089","name":"Wei Xue"},{"authorId":"2157104830","name":"Qi-fei Liu"},{"authorId":"2118270918","name":"Yi-Ting Guo"},{"authorId":"2296665036","name":"Qifeng Chen"}],"abstract":"With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLIP/T5-based methods. Then, we summarize the various roles of LLMs in multimodal generation and exhaustively investigate the critical technical components behind these methods and the multimodal datasets utilized in these studies. Additionally, we dig into tool-augmented multimodal agents that can leverage existing generative models for human-computer interaction. Lastly, we discuss the advancements in the generative AI safety field, investigate emerging applications, and discuss future prospects. Our work provides a systematic and insightful overview of multimodal generation and processing, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation"}