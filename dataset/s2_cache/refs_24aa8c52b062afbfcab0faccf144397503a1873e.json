{"references":[{"paperId":"f21b933c83110f222ba6ea655cd5df10398acfcb","externalIds":{"DBLP":"conf/iclr/0006Z00LL025","ArXiv":"2502.14572","DOI":"10.48550/arXiv.2502.14572","CorpusId":276482517},"title":"Factor Graph-based Interpretable Neural Networks"},{"paperId":"5f8430d64d217f2434137aee41e4a1a5dec64d69","externalIds":{"DBLP":"conf/aaai/HeZZZCL25","ArXiv":"2501.04975","DOI":"10.1609/aaai.v39i3.32352","CorpusId":275405697},"title":"V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer"},{"paperId":"b0b131c9be1d9b2c54b06a5c0df1db55c296e15b","externalIds":{"DBLP":"journals/corr/abs-2412-07992","ArXiv":"2412.07992","DOI":"10.48550/arXiv.2412.07992","CorpusId":274638138},"title":"Concept Bottleneck Large Language Models"},{"paperId":"d88f96a4c66bc40af0e963ed33bccd87ad840cc6","externalIds":{"DBLP":"journals/corr/abs-2409-17663","ArXiv":"2409.17663","DOI":"10.48550/arXiv.2409.17663","CorpusId":272911273},"title":"Explanation Bottleneck Models"},{"paperId":"50977500f2144a07a354f28c4e1ec1f818bdc4c4","externalIds":{"DBLP":"journals/corr/abs-2407-14499","ArXiv":"2407.14499","DOI":"10.48550/arXiv.2407.14499","CorpusId":271310078},"title":"Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery"},{"paperId":"0d8e3d42a2b9cd6e5d94ee4ecc3d1d50bc1ebb29","externalIds":{"DBLP":"conf/nips/SrivastavaYW24","ArXiv":"2408.01432","DOI":"10.48550/arXiv.2408.01432","CorpusId":271709803},"title":"VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance"},{"paperId":"b8f280d8bf685f8da7c83068e73f000528072d6b","externalIds":{"ArXiv":"2403.19647","DBLP":"conf/iclr/MarksRMBBM25","DOI":"10.48550/arXiv.2403.19647","CorpusId":268732732},"title":"Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models"},{"paperId":"1c8b87bd6f97642702e22d8a1227b504dfd269af","externalIds":{"DBLP":"journals/corr/abs-2402-18956","ArXiv":"2402.18956","DOI":"10.1109/CVPR52733.2024.01043","CorpusId":268063605},"title":"WWW: A Unified Framework for Explaining what, Where and why of Neural Networks by Interpretation of Neuron Concepts"},{"paperId":"8362c45885738f5246e163a9763e0270d229ca6b","externalIds":{"ArXiv":"2402.10376","DBLP":"conf/nips/BhallaOSCL24","DOI":"10.48550/arXiv.2402.10376","CorpusId":267740469},"title":"Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)"},{"paperId":"6b63268fbedb135bfc08a23d1402b753dab414ed","externalIds":{"DBLP":"journals/corr/abs-2401-13544","ArXiv":"2401.13544","DOI":"10.48550/arXiv.2401.13544","CorpusId":267199978},"title":"Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?"},{"paperId":"75e6176204eb6cd241410e07d3ab04d303b77b76","externalIds":{"DBLP":"journals/corr/abs-2312-15033","ArXiv":"2312.15033","DOI":"10.48550/arXiv.2312.15033","CorpusId":266550821},"title":"Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention"},{"paperId":"078e311f88009cedcbb44a4852f93a2c48cbf5d8","externalIds":{"DBLP":"conf/xai/PocheHB23","ArXiv":"2309.03234","DOI":"10.48550/arXiv.2309.03234","CorpusId":261582519},"title":"Natural Example-Based Explainability: a Survey"},{"paperId":"3e1553f28d9db1f964b245b97be58589090d88e8","externalIds":{"DBLP":"journals/corr/abs-2306-01574","ArXiv":"2306.01574","DOI":"10.48550/arXiv.2306.01574","CorpusId":259063823},"title":"Probabilistic Concept Bottleneck Models"},{"paperId":"70a540c7ae331cf41ee32ab31e63717fc9d69e1b","externalIds":{"DBLP":"conf/xai4cv/MadeiraCGRSS23","DOI":"10.1109/CVPRW59228.2023.00392","CorpusId":260743863},"title":"ZEBRA: Explaining rare cases through outlying interpretable concepts"},{"paperId":"5bed1d459566df5ffc52690477ca83d6d79f8719","externalIds":{"DBLP":"conf/cvpr/MoayeriRSF23","DOI":"10.1109/CVPRW59228.2023.00386","CorpusId":260743999},"title":"Text2Concept: Concept Activation Vectors Directly from Text"},{"paperId":"de09c01dab8795b907643ea27c913779526628b1","externalIds":{"DBLP":"conf/nesy/BarbieroCGZMTLP23","ArXiv":"2304.14068","DOI":"10.5555/3618408.3618484","CorpusId":258352760},"title":"Interpretable Neural-Symbolic Concept Reasoning"},{"paperId":"1d603b03bb083a2c3e2e3b6f116cf196d637e6b6","externalIds":{"DBLP":"conf/iclr/OikarinenDNW23","ArXiv":"2304.06129","DOI":"10.48550/arXiv.2304.06129","CorpusId":258107969},"title":"Label-Free Concept Bottleneck Models"},{"paperId":"c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0","externalIds":{"DBLP":"conf/eccv/LiuZRLZYJLYSZZ24","ArXiv":"2303.05499","DOI":"10.48550/arXiv.2303.05499","CorpusId":257427307},"title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"},{"paperId":"6f16dddf797fd17f48e8b77f93f69f239a44e441","externalIds":{"DBLP":"journals/corr/abs-2303-00885","ArXiv":"2303.00885","DOI":"10.1109/CVPR52729.2023.01113","CorpusId":257279770},"title":"Towards Trustable Skin Cancer Diagnosis via Rewriting Model's Decision"},{"paperId":"07c99228a99fcaa08e338ab909a5fe92546843e0","externalIds":{"DBLP":"conf/icml/ShinJAL23","ArXiv":"2302.14260","DOI":"10.48550/arXiv.2302.14260","CorpusId":257233107},"title":"A Closer Look at the Intervention Procedure of Concept Bottleneck Models"},{"paperId":"e695a6a6d8a677f528add0118effc7736da35709","externalIds":{"ArXiv":"2301.11911","DBLP":"journals/corr/abs-2301-11911","DOI":"10.48550/arXiv.2301.11911","CorpusId":256358599},"title":"Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees"},{"paperId":"d3fb854e4e97cab40d1c076cd6e88439a0227249","externalIds":{"DBLP":"conf/iclr/AdebayoMAK22","ArXiv":"2212.04629","DOI":"10.48550/arXiv.2212.04629","CorpusId":251648657},"title":"Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation"},{"paperId":"ec30aa46ab9b93150e4d70b896931fbf4d9bbd21","externalIds":{"DBLP":"conf/aaai/AsadiSFVMK23","ArXiv":"2212.01133","DOI":"10.48550/arXiv.2212.01133","CorpusId":254220866},"title":"Ripple: Concept-Based Interpretation for Raw Time Series Models in Education"},{"paperId":"e63b3f2ac5eee86c10e2265b483957dc27d970ee","externalIds":{"DBLP":"journals/corr/abs-2211-16080","ArXiv":"2211.16080","DOI":"10.48550/arXiv.2211.16080","CorpusId":254069697},"title":"Understanding and Enhancing Robustness of Concept-based Models"},{"paperId":"6d1badc223cfca6ae23fcce811b9dbcb8645cdef","externalIds":{"DBLP":"conf/cvpr/YangPZJCY23","ArXiv":"2211.11158","DOI":"10.1109/CVPR52729.2023.01839","CorpusId":253735286},"title":"Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification"},{"paperId":"8185c2ac7a4ea46ccccaa76a3b8c02fcf7354b47","externalIds":{"ArXiv":"2211.10154","DBLP":"journals/corr/abs-2211-10154","DOI":"10.1109/CVPR52729.2023.00266","CorpusId":253708233,"PubMed":"38463608"},"title":"CRAFT: Concept Recursive Activation FacTorization for Explainability"},{"paperId":"26bb261580f6c01f49e52db99907bfc0a9ea4c3d","externalIds":{"ArXiv":"2211.09732","DBLP":"conf/emnlp/JainCBGBL22","ACL":"2022.emnlp-main.604","DOI":"10.18653/v1/2022.emnlp-main.604","CorpusId":253581381},"title":"Extending Logic Explained Networks to Text Classification"},{"paperId":"b4227655b5a6a9080613a0cbed7666948423a597","externalIds":{"ArXiv":"2210.03735","DBLP":"journals/corr/abs-2210-03735","DOI":"10.1145/3544548.3581001","CorpusId":252780815},"title":"\"Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction"},{"paperId":"e638b9369e9b588b1c4fcfeee6409e51b4625f9b","externalIds":{"ArXiv":"2209.14279","DBLP":"journals/corr/abs-2209-14279","DOI":"10.48550/arXiv.2209.14279","CorpusId":252568071},"title":"Causal Proxy Models for Concept-Based Model Explanations"},{"paperId":"3aea0c72be64f42f014bc2bbbbd261e524680d19","externalIds":{"DBLP":"journals/corr/abs-2209-11222","ArXiv":"2209.11222","CorpusId":252438631},"title":"Concept Activation Regions: A Generalized Framework For Concept-Based Explanations"},{"paperId":"3b8abd466697998c6e17df2cd30f48a7594d795b","externalIds":{"DBLP":"journals/csur/DwivediDNSRPQWS23","DOI":"10.1145/3561048","CorpusId":252051155},"title":"Explainable AI (XAI): Core Ideas, Techniques, and Solutions"},{"paperId":"1566fedfe843ac88fb36803368fa84bed6db2af3","externalIds":{"DBLP":"conf/aaai/XuanyuanBGML23","ArXiv":"2208.10609","DOI":"10.48550/arXiv.2208.10609","CorpusId":251741343},"title":"Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis"},{"paperId":"aa5c2c87d5a24ffd54c5aa6481720a047952c8e7","externalIds":{"ArXiv":"2207.09615","DBLP":"conf/cvpr/RamaswamyKFR23","DOI":"10.1109/CVPR52729.2023.01052","CorpusId":258676658},"title":"Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability"},{"paperId":"806fa5f68362741ef30f8bb1cbd54d393caa2540","externalIds":{"ArXiv":"2206.13872","DBLP":"conf/uai/LeemannKRKK23","CorpusId":257050427},"title":"When are post-hoc conceptual explanations identifiable?"},{"paperId":"4f9e122682617710546ce5beaba02778c098bbc3","externalIds":{"DBLP":"journals/corr/abs-2206-05275","ArXiv":"2206.05275","DOI":"10.1109/CVPR52729.2023.01482","CorpusId":249625746},"title":"Spatial-temporal Concept based Explanation of 3D ConvNets"},{"paperId":"c2fbc8ad4e2403c795529c2606e4686f5a571215","externalIds":{"DBLP":"journals/natmi/AchtibatDEBWSL23","ArXiv":"2206.03208","DOI":"10.1038/s42256-023-00711-8","CorpusId":249431627},"title":"From attribution maps to human-understandable explanations through Concept Relevance Propagation"},{"paperId":"8545e249ab7a49f4a5abcfade395b90ffadb687a","externalIds":{"ArXiv":"2205.15480","DBLP":"conf/iclr/YuksekgonulW023","DOI":"10.48550/arXiv.2205.15480","CorpusId":249209990},"title":"Post-hoc Concept Bottleneck Models"},{"paperId":"1557669e5b43c7dbffe7a9f23b80264098cdc7ea","externalIds":{"DBLP":"conf/iclr/BontempelliTTGP23","ArXiv":"2205.15769","DOI":"10.48550/arXiv.2205.15769","CorpusId":249209899},"title":"Concept-level Debugging of Part-Prototype Networks"},{"paperId":"c0c98210454aba32628e3dbca031bde364914725","externalIds":{"ArXiv":"2205.15612","DBLP":"conf/nips/MarconatoPT22","DOI":"10.48550/arXiv.2205.15612","CorpusId":249209924},"title":"GlanceNets: Interpretabile, Leak-proof Concept-based Models"},{"paperId":"508ff1a8d87011ee35ffde1b8c37301b777a6e20","externalIds":{"DBLP":"journals/corr/abs-2205-14140","ArXiv":"2205.14140","DOI":"10.48550/arXiv.2205.14140","CorpusId":249152172},"title":"CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior"},{"paperId":"57b064db445c8c1e6b08bc7a8499e6f2c8b67dbc","externalIds":{"ArXiv":"2204.10965","DBLP":"journals/corr/abs-2204-10965","DOI":"10.48550/arXiv.2204.10965","CorpusId":248376976},"title":"CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks"},{"paperId":"3462bca75d285c6ce14cc7d29fe38a3d302edaf2","externalIds":{"DBLP":"journals/corr/abs-2203-02586","ArXiv":"2203.02586","DOI":"10.48550/arXiv.2203.02586","CorpusId":247292348},"title":"Concept-based Explanations for Out-Of-Distribution Detectors"},{"paperId":"383276f9fdc753b4fe1df2edbeed191558e1ddbe","externalIds":{"DBLP":"journals/corr/abs-2202-04178","ArXiv":"2202.04178","CorpusId":246679982},"title":"VAEL: Bridging Variational Autoencoders and Probabilistic Logic Programming"},{"paperId":"192a2fcafab8dd0197902729f41303f991f85893","externalIds":{"DBLP":"journals/access/SawadaN22","ArXiv":"2202.01459","DOI":"10.1109/ACCESS.2022.3167702","CorpusId":246485723},"title":"Concept Bottleneck Model With Additional Unsupervised Concepts"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"daeb8a9063c788c3a190484cd79d7bba78c9a47c","externalIds":{"DBLP":"journals/corr/abs-2112-02902","ArXiv":"2112.02902","DOI":"10.1007/978-3-031-19775-8_21","CorpusId":244909006},"title":"Interpretable Image Classification with Differentiable Prototypes Assignment"},{"paperId":"dca709088a2d429941fc86de00f5434c46a8e9b1","externalIds":{"ArXiv":"2111.15000","DBLP":"conf/cvpr/DonnellyBC22","DOI":"10.1109/CVPR52688.2022.01002","CorpusId":244729769},"title":"Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes"},{"paperId":"138fc658c0c8ee82e703e33f8583a2d9afe62e52","externalIds":{"DBLP":"conf/cvpr/BrownK23","ArXiv":"2110.07120","DOI":"10.1109/CVPRW59228.2023.00069","CorpusId":251066592},"title":"Making Corgis Important for Honeycomb Classification: Adversarial Attacks on Concept-based Explainability Tools"},{"paperId":"73fce3aa3611186c3fcd3e7f8da62c1eb3dcf0db","externalIds":{"ArXiv":"2108.11761","DBLP":"conf/cvpr/0001VSB22","DOI":"10.1109/CVPR52688.2022.01004","CorpusId":244772958},"title":"A Framework for Learning Ante-hoc Explainable Models via Concepts"},{"paperId":"f5b1d8ea0ab5dbaa87bc43a109430251a01a139b","externalIds":{"ArXiv":"2108.05149","DBLP":"conf/nesy/CiravegnaBGGLMM23","DOI":"10.1016/j.artint.2022.103822","CorpusId":236976281},"title":"Logic Explained Networks"},{"paperId":"61598046fe8dbdd58b40989f2eb1822e7a160c16","externalIds":{"DBLP":"journals/corr/abs-2107-11889","ArXiv":"2107.11889","CorpusId":236428348},"title":"GCExplainer: Human-in-the-Loop Concept-based Explanations for Graph Neural Networks"},{"paperId":"57db5ffda93877ae809c875637ff4f7043249e3d","externalIds":{"ArXiv":"2106.13314","DBLP":"journals/corr/abs-2106-13314","CorpusId":235652059},"title":"Promises and Pitfalls of Black-Box Concept Learning Models"},{"paperId":"7b7120161cfc91e0388938772602e6ffc9760360","externalIds":{"ArXiv":"2106.06804","DBLP":"journals/corr/abs-2106-06804","DOI":"10.1609/aaai.v36i6.20551","CorpusId":235422550},"title":"Entropy-based Logic Explanations of Neural Networks"},{"paperId":"de072b055ad741729e712ef94de4688997ddc578","externalIds":{"DBLP":"journals/corr/abs-2105-04289","ArXiv":"2105.04289","CorpusId":234339919},"title":"Do Concept Bottleneck Models Learn as Intended?"},{"paperId":"3aba582b62d1abfcd95264e6c7b32aab4c9db4b8","externalIds":{"ArXiv":"2103.12279","DBLP":"conf/emnlp/RajagopalBHT21","ACL":"2021.emnlp-main.64","DOI":"10.18653/v1/2021.emnlp-main.64","CorpusId":232320343},"title":"SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93a55b04045ff7fff78de473f5ff52cbcfb9a948","externalIds":{"DBLP":"journals/datamine/BodriaGGNPR23","ArXiv":"2102.13076","DOI":"10.1007/s10618-023-00933-9","CorpusId":232046272},"title":"Benchmarking and survey of explanation methods for black box models"},{"paperId":"cb6948b0f10f3de7b8d83e1733b6f6aab13dcb44","externalIds":{"MAG":"3107606619","DBLP":"conf/kdd/RymarczykST021","ArXiv":"2011.14340","DOI":"10.1145/3447548.3467245","CorpusId":236980158},"title":"ProtoPShare: Prototypical Parts Sharing for Similarity Discovery in Interpretable Image Classification"},{"paperId":"b103e87c7727134927d3ffb06934a95c10c02fc0","externalIds":{"MAG":"3095319910","DBLP":"journals/mima/FloridiC20","DOI":"10.1007/s11023-020-09548-1","CorpusId":228954221},"title":"GPT-3: Its Nature, Scope, Limits, and Consequences"},{"paperId":"ab9f91fe93b59bf44811e6fc1fcdf1b61cf9a5c9","externalIds":{"MAG":"3093551005","ArXiv":"2010.13233","DBLP":"journals/corr/abs-2010-13233","CorpusId":224817690},"title":"Now You See Me (CME): Concept-based Model Extraction"},{"paperId":"a56d030489511da56119b4dab06a14d22b141b64","externalIds":{"MAG":"3085389796","DBLP":"conf/iccv/LiWVNKN21","ArXiv":"2009.06138","DOI":"10.1109/ICCV48922.2021.00108","CorpusId":221655236},"title":"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition"},{"paperId":"3a24bfb77ed271fef948058e414850f89b0955a7","externalIds":{"DBLP":"conf/icml/KohNTMPKL20","ArXiv":"2007.04612","MAG":"3041871339","CorpusId":220424448},"title":"Concept Bottleneck Models"},{"paperId":"92902e212cd1408f2bfbefbbb0157abe1b05a18e","externalIds":{"DBLP":"conf/aaai/ZhangM0ER21","ArXiv":"2006.15417","DOI":"10.1609/aaai.v35i13.17389","CorpusId":235349029},"title":"Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors"},{"paperId":"56d1003fd02346e93354ab55cd204485c268512a","externalIds":{"ArXiv":"2006.14032","DBLP":"conf/nips/MuA20","MAG":"3098680936","CorpusId":220055965},"title":"Compositional Explanations of Neurons"},{"paperId":"c483beec0afae8d08f011182460095049025b8d1","externalIds":{"ArXiv":"2006.11371","DBLP":"journals/corr/abs-2006-11371","MAG":"3036453007","CorpusId":219965893},"title":"Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"ecdc039b0784d05681cf0195977d9e47bb672740","externalIds":{"MAG":"3089992641","ArXiv":"2005.02000","DBLP":"conf/ijcnn/LucieriBBM0A20","DOI":"10.1109/IJCNN48605.2020.9206946","CorpusId":218502252},"title":"On Interpretability of Deep Learning based Skin Lesion Classifiers using Concept Activation Vectors"},{"paperId":"598658595e887f677967769ed11ba28158f6ea8a","externalIds":{"DBLP":"conf/nips/AgarwalMFZLCH21","ArXiv":"2004.13912","MAG":"3022120663","CorpusId":216641712},"title":"Neural Additive Models: Interpretable Machine Learning with Neural Nets"},{"paperId":"a46c2964c584889297c731407ad9e15909e0d687","externalIds":{"MAG":"3035292170","ArXiv":"2003.09405","DBLP":"journals/corr/abs-2003-09405","DOI":"10.1109/CVPR42600.2020.00954","CorpusId":214605757},"title":"Explainable Object-Induced Action Decision for Autonomous Vehicles"},{"paperId":"18c6547b1af98bec9ce8df3f80cea8e63a57f47d","externalIds":{"DBLP":"journals/corr/abs-2002-01650","ArXiv":"2002.01650","MAG":"3004725381","DOI":"10.1038/s42256-020-00265-z","CorpusId":211031886},"title":"Concept whitening for interpretable image recognition"},{"paperId":"b0c34618ffd1154f35863e2ce7250ac6b6f2c424","externalIds":{"MAG":"2999362542","DOI":"10.1201/9780367816377-16","CorpusId":209379623},"title":"Interpretable Machine Learning"},{"paperId":"653864b10564ab4712c07a3d4043a1d794b13c46","externalIds":{"DBLP":"conf/aies/SlackHJSL20","MAG":"3005086430","ArXiv":"1911.02508","DOI":"10.1145/3375627.3375830","CorpusId":211041098},"title":"Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods"},{"paperId":"12ec6647794bd97e5093d7868c4635ae52abc614","externalIds":{"DOI":"10.2139/ssrn.3615731","CorpusId":233765885},"title":"An Examination of the Algorithmic Accountability Act of 2019"},{"paperId":"530a059cb48477ad1e3d4f8f4b153274c8997332","externalIds":{"ArXiv":"1910.10045","MAG":"2997428643","DBLP":"journals/inffus/ArrietaRSBTBGGM20","DOI":"10.1016/j.inffus.2019.12.012","CorpusId":204824113},"title":"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI"},{"paperId":"c12dda55bdff2f4cebf0a274331de8d117c2b7aa","externalIds":{"MAG":"3007960301","DBLP":"conf/nips/YehKALPR20","ArXiv":"1910.07969","CorpusId":213097139},"title":"On Completeness-aware Concept-Based Explanations in Deep Neural Networks"},{"paperId":"5809135c6ad6d0392be20a3143f2f74ab6f64ef4","externalIds":{"ArXiv":"1907.07165","DBLP":"journals/corr/abs-1907-07165","MAG":"2959325723","CorpusId":196831528},"title":"Explaining Classifiers with Causal Concept Effect (CaCE)"},{"paperId":"84c0528cb2aa4bdacad989b5b43441161dd4ecda","externalIds":{"ArXiv":"1907.06987","DBLP":"journals/corr/abs-1907-06987","MAG":"2961193895","CorpusId":196831809},"title":"A Short Note on the Kinetics-700 Human Action Dataset"},{"paperId":"ac46dde3b19a43229b91f2c1ffa95e69487ad14e","externalIds":{"DBLP":"conf/hcomp/HaseCLR19","ArXiv":"1906.10651","MAG":"2996367332","DOI":"10.1609/hcomp.v7i1.5265","CorpusId":195584480},"title":"Interpretable Image Recognition with Hierarchical Prototypes"},{"paperId":"a3da5fa82d316513ade2dc355ee058af58487751","externalIds":{"DBLP":"journals/corr/abs-1906-03292","MAG":"2990376402","ArXiv":"1906.03292","CorpusId":182952649},"title":"On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset"},{"paperId":"26808fd03f7d553c5380cf82e139cc8cb45cce60","externalIds":{"DBLP":"conf/sac/PastorB19","MAG":"2943031807","DOI":"10.1145/3297280.3297328","CorpusId":142503719},"title":"Explaining black box models by means of local rules"},{"paperId":"4ecfa481430b56bc3fcd735114e1954c4b46cff5","externalIds":{"MAG":"2970030610","DBLP":"conf/nips/GhorbaniWZK19","CorpusId":184487319},"title":"Towards Automatic Concept-based Explanations"},{"paperId":"b79fe48ae523dc66185aa04df2dac7041afa8683","externalIds":{"MAG":"2963350032","DBLP":"journals/corr/abs-1812-10352","ArXiv":"1812.10352","DOI":"10.1109/CVPR.2019.00922","CorpusId":56895575},"title":"Learning Not to Learn: Training Deep Neural Networks With Biased Data"},{"paperId":"bc00ff34ec7772080c7039b17f7069a2f7df0889","externalIds":{"DBLP":"journals/natmi/Rudin19","MAG":"2974440810","DOI":"10.1038/s42256-019-0048-x","CorpusId":182656421,"PubMed":"35603010"},"title":"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead"},{"paperId":"8dc8f3e0127adc6985d4695e9b69d04717b2fde8","externalIds":{"MAG":"2891612330","ArXiv":"1810.03292","DBLP":"journals/corr/abs-1810-03292","CorpusId":52938797},"title":"Sanity Checks for Saliency Maps"},{"paperId":"21dff47a4142445f83016da0819ffe6dd2947f66","externalIds":{"MAG":"2891503716","DBLP":"journals/access/AdadiB18","DOI":"10.1109/ACCESS.2018.2870052","CorpusId":52965836},"title":"Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)"},{"paperId":"aa2ddae22760249729ac2c2c4e24c8b665bcd40e","externalIds":{"MAG":"2895739182","DBLP":"conf/eccv/ZhouSBT18","DOI":"10.1007/978-3-030-01237-3_8","CorpusId":51903952},"title":"Interpretable Basis Decomposition for Visual Explanation"},{"paperId":"5f614777d25efd14b7426e99cb2544f2d6be133e","externalIds":{"MAG":"2970447476","DBLP":"conf/nips/HookerEKK19","ArXiv":"1806.10758","CorpusId":202782699},"title":"A Benchmark for Interpretability Methods in Deep Neural Networks"},{"paperId":"cc145f046788029322835979a14459652da7247e","externalIds":{"DBLP":"journals/corr/abs-1806-10574","MAG":"2971048680","ArXiv":"1806.10574","CorpusId":49482223},"title":"This looks like that: deep learning for interpretable image recognition"},{"paperId":"0cf102da6dd4276115c63cbb6797f24ed450fea1","externalIds":{"MAG":"2809671526","DBLP":"journals/corr/abs-1806-07538","ArXiv":"1806.07538","CorpusId":49324194},"title":"Towards Robust Interpretability with Self-Explaining Neural Networks"},{"paperId":"f3b24d51a81bfe08b2dbebd7f736cb4dd33d64db","externalIds":{"MAG":"2807828695","DOI":"10.2139/SSRN.3196985","CorpusId":149466923},"title":"The right to explanation, explained"},{"paperId":"f986968735459e789890f24b6b277b0920a9725d","externalIds":{"DBLP":"journals/pami/ZhouLKO018","MAG":"2732026016","DOI":"10.1109/TPAMI.2017.2723009","CorpusId":2608922,"PubMed":"28692961"},"title":"Places: A 10 Million Image Database for Scene Recognition"},{"paperId":"fd44d398b2945b4c20da8ec3cc32becd5e08100e","externalIds":{"ArXiv":"1805.10820","DBLP":"journals/corr/abs-1805-10820","MAG":"2803532212","CorpusId":44063479},"title":"Local Rule-Based Explanations of Black Box Decision Systems"},{"paperId":"eec1d7dd61e4f3e720e8948d88733b2b642d8d11","externalIds":{"ArXiv":"1805.04755","DBLP":"journals/corr/abs-1805-04755","MAG":"2799613771","CorpusId":46890050},"title":"A Simple and Effective Model-Based Variable Importance Measure"},{"paperId":"2f4e42ab1368b83e38e3011fcec4edb57267b5d2","externalIds":{"MAG":"2788362053","DBLP":"journals/corr/abs-1802-07810","ArXiv":"1802.07810","DOI":"10.1145/3411764.3445315","CorpusId":262417267},"title":"Manipulating and Measuring Model Interpretability"},{"paperId":"f7325d232c7ac7d2daaf6605377058db5b5b83cc","externalIds":{"MAG":"2951278035","DBLP":"journals/csur/GuidottiMRTGP19","ArXiv":"1802.01933","DOI":"10.1145/3236009","CorpusId":3342225},"title":"A Survey of Methods for Explaining Black Box Models"},{"paperId":"de99fbe728dfd3d337ab13ae27512ab028444c6a","externalIds":{"MAG":"2952038846","DBLP":"conf/cvpr/FongV18","ArXiv":"1801.03454","DOI":"10.1109/CVPR.2018.00910","CorpusId":2738204},"title":"Net2Vec: Quantifying and Explaining How Concepts are Encoded by Filters in Deep Neural Networks"},{"paperId":"682b9d2212258fd5edbfca589c86390c31a956b0","externalIds":{"ArXiv":"1711.11279","MAG":"2796885425","DBLP":"conf/icml/KimWGCWVS18","CorpusId":51737170},"title":"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"},{"paperId":"62e39c6dbcead1fe4d29017914591d929aa6ac4c","externalIds":{"MAG":"2766047647","ArXiv":"1711.00867","DBLP":"series/lncs/KindermansHAASDEK19","DOI":"10.1007/978-3-030-28954-6_14","CorpusId":28562869},"title":"The (Un)reliability of saliency methods"},{"paperId":"96cf326c91adc96aa59382bf71fbead29ea34c52","externalIds":{"ArXiv":"1710.10547","MAG":"2766912318","DBLP":"journals/corr/abs-1710-10547","DOI":"10.1609/aaai.v33i01.33013681","CorpusId":22172746},"title":"Interpretation of Neural Networks is Fragile"},{"paperId":"bf9af274a17eea6ca3721acffab14e5c8e9f097a","externalIds":{"MAG":"2964180856","ArXiv":"1710.04806","DBLP":"journals/corr/abs-1710-04806","DOI":"10.1609/aaai.v32i1.11771","CorpusId":19106556},"title":"Deep Learning for Case-based Reasoning through Prototypes: A Neural Network that Explains its Predictions"},{"paperId":"773ca5c76da50cf6f21553b0f8eee391ac65f9c8","externalIds":{"DBLP":"conf/cvpr/ZhangWZ18a","MAG":"2951308125","ArXiv":"1710.00935","DOI":"10.1109/CVPR.2018.00920","CorpusId":4562004},"title":"Interpretable Convolutional Neural Networks"},{"paperId":"f9c602cc436a9ea2f9e7db48c77d924e09ce3c32","externalIds":{"MAG":"2750384547","DBLP":"journals/corr/abs-1708-07747","ArXiv":"1708.07747","CorpusId":702279},"title":"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms"},{"paperId":"442e10a3c6640ded9408622005e3c2a8906ce4c2","externalIds":{"MAG":"2618851150","DBLP":"journals/corr/LundbergL17","ArXiv":"1705.07874","CorpusId":21889700},"title":"A Unified Approach to Interpreting Model Predictions"},{"paperId":"744464cd6fa8341633cd3b5d378faab18a3b543a","externalIds":{"MAG":"2952857044","ArXiv":"1704.05796","DBLP":"journals/corr/BauZKOT17","DOI":"10.1109/CVPR.2017.354","CorpusId":378410},"title":"Network Dissection: Quantifying Interpretability of Deep Visual Representations"},{"paperId":"4a73a1840945e87583d89ca0216a2c449d50a4a3","externalIds":{"ArXiv":"1703.06211","DBLP":"conf/iccv/DaiQXLZHW17","MAG":"2950477723","DOI":"10.1109/ICCV.2017.89","CorpusId":4028864},"title":"Deformable Convolutional Networks"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","externalIds":{"MAG":"2561529111","DBLP":"journals/corr/SpeerCH16","ArXiv":"1612.03975","DOI":"10.1609/aaai.v31i1.11164","CorpusId":15206880},"title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","externalIds":{"MAG":"2962858109","DBLP":"conf/iccv/SelvarajuCDVPB17","ArXiv":"1610.02391","DOI":"10.1007/s11263-019-01228-7","CorpusId":15019293},"title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"36eff562f65125511b5dfab68ce7f7a943c27478","externalIds":{"ArXiv":"1609.02907","MAG":"2519887557","DBLP":"journals/corr/KipfW16","CorpusId":3144218},"title":"Semi-Supervised Classification with Graph Convolutional Networks"},{"paperId":"cc5afe344cc7ed7acd68a28b9774ea8023a162dc","externalIds":{"MAG":"2493343568","DBLP":"journals/aim/GoodmanF17","ArXiv":"1606.08813","DOI":"10.1609/aimag.v38i3.2741","CorpusId":7373959},"title":"European Union Regulations on Algorithmic Decision-Making and a \"Right to Explanation\""},{"paperId":"c0883f5930a232a9c1ad601c978caede29155979","externalIds":{"DBLP":"conf/naacl/Ribeiro0G16","MAG":"2516809705","ArXiv":"1602.04938","ACL":"N16-3020","DOI":"10.1145/2939672.2939778","CorpusId":13029170},"title":"“Why Should I Trust You?”: Explaining the Predictions of Any Classifier"},{"paperId":"31f9eb39d840821979e5df9f34a6e92dd9c879f2","externalIds":{"ArXiv":"1512.04150","MAG":"2950328304","DBLP":"journals/corr/ZhouKLOT15","DOI":"10.1109/CVPR.2016.319","CorpusId":6789015},"title":"Learning Deep Features for Discriminative Localization"},{"paperId":"f6ce14f91b4641942947882062682125369847f7","externalIds":{"MAG":"2214801105","DOI":"10.2139/ssrn.3799967","CorpusId":146138497},"title":"The V–Dem Measurement Model: Latent Variable Analysis for Cross-National and Cross-Temporal Expert-Coded Data"},{"paperId":"00d736c540f80582279093cfc5ffe454a3226da9","externalIds":{"DBLP":"conf/kdd/YanardagV15","MAG":"2008857988","DOI":"10.1145/2783258.2783417","CorpusId":207227372},"title":"Deep Graph Kernels"},{"paperId":"401192b00b650adfa5ac49de59b720e1c81f1410","externalIds":{"MAG":"1899185266","ArXiv":"1412.6856","DBLP":"journals/corr/ZhouKLOT14","CorpusId":8217340},"title":"Object Detectors Emerge in Deep Scene CNNs"},{"paperId":"6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","externalIds":{"MAG":"1834627138","ArXiv":"1411.7766","DBLP":"journals/corr/LiuLWT14","DOI":"10.1109/ICCV.2015.425","CorpusId":459456},"title":"Deep Learning Face Attributes in the Wild"},{"paperId":"dcbf587642c39f495117552ca453a4f955ffa76a","externalIds":{"DBLP":"conf/eccv/AgrawalGM14","MAG":"2160921898","ArXiv":"1407.1610","DOI":"10.1007/978-3-319-10584-0_22","CorpusId":1811598},"title":"Analyzing the Performance of Multilayer Neural Networks for Object Recognition"},{"paperId":"dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71","externalIds":{"MAG":"2962851944","ArXiv":"1312.6034","DBLP":"journals/corr/SimonyanVZ13","CorpusId":1450294},"title":"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"},{"paperId":"1a2a770d23b4a171fa81de62a78a3deb0588f238","externalIds":{"DBLP":"journals/corr/ZeilerF13","ArXiv":"1311.2901","MAG":"1849277567","DOI":"10.1007/978-3-319-10590-1_53","CorpusId":3960646},"title":"Visualizing and Understanding Convolutional Networks"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","externalIds":{"ACL":"D13-1170","DBLP":"conf/emnlp/SocherPWCMNP13","MAG":"2251939518","DOI":"10.18653/v1/d13-1170","CorpusId":990233},"title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"184ac0766262312ba76bbdece4e7ffad0aa8180b","externalIds":{"DBLP":"journals/pami/BengioCV13","MAG":"2952111767","ArXiv":"1206.5538","DOI":"10.1109/TPAMI.2013.50","CorpusId":393948,"PubMed":"23787338"},"title":"Representation Learning: A Review and New Perspectives"},{"paperId":"6af58c061f2e4f130c3b795c21ff0c7e3903278f","externalIds":{"MAG":"2163455955","DBLP":"conf/acl/PangL05","ArXiv":"cs/0506075","ACL":"P05-1015","DOI":"10.3115/1219840.1219855","CorpusId":3264224},"title":"Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales"},{"paperId":"b480f6a3750b4cebaf1db205692c8321d45926a2","externalIds":{"MAG":"2034328688","DBLP":"conf/icpr/SchuldtLC04","DOI":"10.1109/ICPR.2004.747","CorpusId":8777811},"title":"Recognizing human actions: a local SVM approach"},{"paperId":"2c8ac3e1f0edeed1fbd76813e61efdc384c319c7","externalIds":{"MAG":"2070246124","DBLP":"conf/coling/LiR02","ACL":"C02-1150","DOI":"10.3115/1072228.1072378","CorpusId":11039301},"title":"Learning Question Classifiers"},{"paperId":"072a0db716fb6f8332323f076b71554716a7271c","externalIds":{"MAG":"2095409369","DOI":"10.1109/51.932724","CorpusId":18619383,"PubMed":"11446209"},"title":"The impact of the MIT-BIH Arrhythmia Database"},{"paperId":"ccaf829bffd0b1a55a67a6958dcfb7af4cd16641","externalIds":{"MAG":"1503729935","CorpusId":261528859},"title":"Formal Concept Analysis: Mathematical Foundations"},{"paperId":"ef77a8b0db638c675d032bd05dc3a012a2899da4","externalIds":{"MAG":"2056562706","DOI":"10.1021/JM00106A046","CorpusId":19990980,"PubMed":"1995902"},"title":"Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity."},{"paperId":"bd62ca16d44caed1e5c95775d4ad4b4e9c229740","externalIds":{"DBLP":"conf/iclr/LaiHWB024","CorpusId":271745833},"title":"Faithful Vision-Language Interpretation via Concept Bottleneck Models"},{"paperId":"68875bb9aeddd814624ab6216fc8e783fb520695","externalIds":{"DBLP":"conf/iclr/IsmailABRC24","CorpusId":271745630},"title":"Concept Bottleneck Generative Models"},{"paperId":"399898f6cfd942b26126594e0adb0d66c614d6c5","externalIds":{"DBLP":"conf/xai/MikriukovSMB24","DOI":"10.1007/978-3-031-63787-2_6","CorpusId":273496230},"title":"Unveiling the Anatomy of Adversarial Attacks: Concept-Based XAI Dissection of CNNs"},{"paperId":"65dbea1bfb792872e0966ef1a361f33d3f110250","externalIds":{"DBLP":"conf/nips/HavasiPD22","CorpusId":258509699},"title":"Addressing Leakage in Concept Bottleneck Models"},{"paperId":"2f47a4c37c01d3ad4e6c4b074ff61468f1e976b8","externalIds":{"DBLP":"conf/iclr/RigottiMGGS22","CorpusId":251648020},"title":"Attention-based Interpretability with Concept Transformers"},{"paperId":"4f8d648c52edf74e41b0996128aa536e13cc7e82","externalIds":{"DBLP":"journals/ijsc/HaoZM16","DOI":"10.1142/S1793351X16500045","CorpusId":1779661},"title":"Deep Learning"},{"paperId":"15a618962f27608d62347cdfc8aa06025e31792c","externalIds":{"CorpusId":264744871},"title":"Multiparameter Intelligent Monitoring in Intensive Care Ii (Mimic-Ii): A Public-Access Intensive Care Unit Database"},{"paperId":"65d994fb778a8d9e0f632659fb33a082949a50d3","externalIds":{"MAG":"2901978908","CorpusId":15127402},"title":"Visualizing Higher-Layer Features of a Deep Network"},{"paperId":"ba640c0682b242cb480bb4eb5b934ee6db949269","externalIds":{"MAG":"2099438806","DBLP":"conf/ismb/BorgwardtOSVSK05","DOI":"10.1093/bioinformatics/bti1007","CorpusId":8174592,"PubMed":"15961493"},"title":"Protein function prediction via graph kernels"},{"paperId":"dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2","externalIds":{"MAG":"200806003","CorpusId":60282629},"title":"The mnist database of handwritten digits"},{"paperId":"299ea120f6b64abfc37c6d7ff2c8d42895781192","externalIds":{"DOI":"10.1111/j.1468-0017.1989.tb00233.x","CorpusId":14876285},"title":"WHAT IS A CONCEPT ?"}]}