{"abstract":"Recently, machine learning (ML) has shown its effectiveness in improving communication efficiency by reinstating the semantics of bits. To understand its underlying principles, we propose a novel stochastic model of semantic communication, dubbed semantics-native communication (SNC). Inspired from human communication, we consider a point-to-point SNC scenario where a speaker has an intention of referring to an entity, extracts its semantic concepts, and maps these concepts into symbols that are communicated to a target listener through the traditional Shannon communication channel. Next, we recall rational humans who can communicate more efficiently by reasoning about the others’ contexts before communication, referred to as contextual reasoning. This motivates us to propose a novel SNC framework harnessing agent states as side information in a way that the speaker locally and iteratively communicates with a virtual agent having the listener’s state, and vice versa. Theoretically, we prove the convergence of contextual reasoning, at which it minimizes the bit length while guaranteeing a target reliability. Simulation results corroborate that contextual reasoning based SNC can significantly reduce bit lengths, and be a robust solution to imperfect agent states by quantizing the entity-concept-symbol mapping before contextual reasoning."}