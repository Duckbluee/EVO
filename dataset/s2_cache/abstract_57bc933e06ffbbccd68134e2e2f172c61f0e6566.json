{"abstract":"Graph clustering, aiming to partition nodes of a graph into various groups via an unsupervised approach, is an attractive topic in recent years. To improve the representative ability, several graph auto-encoder (GAE) models, which are based on semisupervised graph convolution networks (GCN), have been developed and they have achieved impressive results compared with traditional clustering methods. However, all existing methods either fail to utilize the orthogonal property of the representations generated by GAE or separate the clustering and the training of neural networks. We first prove that the relaxed <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>-means will obtain an optimal partition in the inner-product distance used space. Driven by theoretical analysis about relaxed <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>-means, we design a specific GAE-based model for graph clustering to be consistent with the theory, namely Embedding GAE (EGAE). The learned representations are well explainable so that the representations can be also used for other tasks. To induce the neural network to produce deep features that are appropriate for the specific clustering model, the relaxed <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>-means and GAE are learned simultaneously. Meanwhile, the relaxed <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>-means can be equivalently regarded as a decoder that attempts to learn representations that can be linearly constructed by some centroid vectors. Accordingly, EGAE consists of one encoder and dual decoders. Extensive experiments are conducted to prove the superiority of EGAE and the corresponding theoretical analyses."}