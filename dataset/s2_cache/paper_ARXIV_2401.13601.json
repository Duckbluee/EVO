{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","externalIds":{"DBLP":"journals/corr/abs-2401-13601","ArXiv":"2401.13601","DOI":"10.48550/arXiv.2401.13601","CorpusId":267199815},"title":"MM-LLMs: Recent Advances in MultiModal Large Language Models","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2401.13601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2261355090","name":"Duzhen Zhang"},{"authorId":"2223948196","name":"Yahan Yu"},{"authorId":"2280939789","name":"Chenxing Li"},{"authorId":"2261382558","name":"Jiahua Dong"},{"authorId":"2280914208","name":"Dan Su"},{"authorId":"2280997095","name":"Chenhui Chu"},{"authorId":"2281062687","name":"Dong Yu"}],"abstract":"In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain."}