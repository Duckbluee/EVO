{"references":[{"paperId":"558ea6a6b4292627e3a00fb5eb9d58a0ba2185a5","externalIds":{"DBLP":"journals/tcss/QianWCYXJYZ25","DOI":"10.1109/TCSS.2024.3521445","CorpusId":266818071},"title":"MusicAOG: An Energy-Based Model for Learning and Sampling a Hierarchical Representation of Symbolic Music"},{"paperId":"824d6b91f22296637b4efff1387fa73b73346b42","externalIds":{"ArXiv":"2407.08428","DBLP":"journals/corr/abs-2407-08428","DOI":"10.48550/arXiv.2407.08428","CorpusId":271097523},"title":"A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights"},{"paperId":"6cf5697aa3be5328e6d2dc83c58eef610cbbf4fe","externalIds":{"ArXiv":"2405.09901","DBLP":"conf/iclr/WangMX24","DOI":"10.48550/arXiv.2405.09901","CorpusId":269790760},"title":"Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models"},{"paperId":"0cecc17d774ab3fcb0bd8090288d58da6c422a3f","externalIds":{"DOI":"10.34306/ijcitsm.v4i1.149","CorpusId":274491867},"title":"Exploration of Artificial Intelligence in Creative Fields: Generative Art, Music, and Design"},{"paperId":"d7b71afea8a94ab772249bbb19202ca59c38740b","externalIds":{"DBLP":"conf/asru/HuangVLST23","ArXiv":"2306.14422","DOI":"10.1109/ASRU57964.2023.10389671","CorpusId":259251521},"title":"The Singing Voice Conversion Challenge 2023"},{"paperId":"a559acac0e84319d62cefd564a5eecbf9d566ec4","externalIds":{"ArXiv":"2306.00110","DBLP":"journals/corr/abs-2306-00110","DOI":"10.48550/arXiv.2306.00110","CorpusId":258999849},"title":"MuseCoco: Generating Symbolic Music from Text"},{"paperId":"f0bfec5397cd46cb0c192f798c6789ff259a5528","externalIds":{"DBLP":"journals/csur/JiYL24","DOI":"10.1145/3597493","CorpusId":258788155},"title":"A Survey on Deep Learning for Symbolic Music Generation: Representations, Algorithms, Evaluations, and Challenges"},{"paperId":"a4aa5d7802f97d52d9060d3582503e28a1eee396","externalIds":{"DBLP":"journals/ais/Cross23","DOI":"10.1007/s00146-023-01670-9","CorpusId":258410562},"title":"Music in the digital age: commodity, community, communion"},{"paperId":"02540ae926814f4b7972d3fa4dd33932fdc4b58b","externalIds":{"ArXiv":"2302.03917","DBLP":"journals/corr/abs-2302-03917","DOI":"10.48550/arXiv.2302.03917","CorpusId":256662408},"title":"Noise2Music: Text-conditioned Music Generation with Diffusion Models"},{"paperId":"195fe438ffd51a351073e8a20076ae0172b4f349","externalIds":{"DBLP":"journals/corr/abs-2211-09124","ArXiv":"2211.09124","DOI":"10.1007/s00521-024-09418-2","CorpusId":253581404},"title":"A review of intelligent music generation systems"},{"paperId":"6406212f78990744352780e090a97d51e118b853","externalIds":{"DBLP":"conf/cikm/ChuKKLLJLKK22","DOI":"10.1145/3511808.3557235","CorpusId":252904882},"title":"An Empirical Study on How People Perceive AI-generated Music"},{"paperId":"ef2c984acbae0e8c82c24ae51f2a3abe6e174501","externalIds":{"DOI":"10.1080/02560046.2022.2112725","CorpusId":251755563},"title":"A New Harmonisation of Art and Technology: Philosophic Interpretations of Artificial Intelligence Art"},{"paperId":"cc240ec1d7dbeb54e7fbbc7e9caf866a90571200","externalIds":{"DBLP":"journals/tismir/DerutyGLNA22","DOI":"10.5334/tismir.100","CorpusId":246655025},"title":"On the Development and Practice of AI Technology for Contemporary Popular Music Production"},{"paperId":"95a35473fd1936927dd4a53fe0a5d2d6762d99b3","externalIds":{"DBLP":"journals/corr/abs-2112-09312","ArXiv":"2112.09312","CorpusId":245216646},"title":"MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling"},{"paperId":"5a828683659b8d773c394fbbd3bd5e34fe524d34","externalIds":{"DBLP":"conf/mm/TanL21","DOI":"10.1145/3474085.3478875","CorpusId":239012134},"title":"A Tutorial on AI Music Composition"},{"paperId":"b998b6d89da22deec4fbeccb1e59e86b5bc68a57","externalIds":{"DOI":"10.1080/03007766.2021.1972701","CorpusId":242779244},"title":"Who let the DAWs Out? The Digital in a New Generation of the Digital Audio Workstation"},{"paperId":"8385f2da0e7fa0549d82499d16f5db7ec8900096","externalIds":{"DBLP":"journals/iet-spr/AgarwalO21","MAG":"3138774025","DOI":"10.1049/SIL2.12015","CorpusId":233590808},"title":"An efficient supervised framework for music mood recognition using autoencoder-based optimised support vector regression model"},{"paperId":"415ce76382607e7253cdf5085945bbf37a932142","externalIds":{"DBLP":"journals/corr/abs-2010-07061","ArXiv":"2010.07061","MAG":"3093121331","DOI":"10.5334/tismir.80","CorpusId":222341932},"title":"GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music"},{"paperId":"34bf13e58c7226d615afead0c0f679432502940e","externalIds":{"MAG":"3087665158","DBLP":"conf/iclr/KongPHZC21","ArXiv":"2009.09761","CorpusId":221818900},"title":"DiffWave: A Versatile Diffusion Model for Audio Synthesis"},{"paperId":"5e6e88267d7e6f2f15b778f3d2bba09ab7a6bdde","externalIds":{"ArXiv":"2009.01776","MAG":"3082910224","DBLP":"journals/corr/abs-2009-01776","CorpusId":221470340},"title":"HiFiSinger: Towards High-Fidelity Neural Singing Voice Synthesis"},{"paperId":"685af6d2bcdff7170574643b2c5ab4fbcc36f597","externalIds":{"DBLP":"conf/iclr/ChenZZWNC21","MAG":"3082563516","ArXiv":"2009.00713","CorpusId":221447287},"title":"WaveGrad: Estimating Gradients for Waveform Generation"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"67dea28495cab71703993d0d52ca4733b9a66077","externalIds":{"DBLP":"journals/corr/abs-2005-00341","ArXiv":"2005.00341","MAG":"3021164770","CorpusId":218470180},"title":"Jukebox: A Generative Model for Music"},{"paperId":"ce510c6cfeac703706460680e977c54554840830","externalIds":{"MAG":"3092879656","DBLP":"conf/mm/HuangY20","DOI":"10.1145/3394171.3413671","CorpusId":220919638},"title":"Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions"},{"paperId":"468aa95cfdf66da9fc3dc6a1b9042a52a6ec99c6","externalIds":{"DBLP":"journals/corr/abs-2001-04643","MAG":"3000389243","ArXiv":"2001.04643","CorpusId":210473083},"title":"DDSP: Differentiable Digital Signal Processing"},{"paperId":"63a71de0dafc90910e37a2b07169ff486d9b5fe5","externalIds":{"MAG":"3030437843","DBLP":"conf/lrec/ArdilaBDKMHMSTW20","ArXiv":"1912.06670","ACL":"2020.lrec-1.520","CorpusId":209376338},"title":"Common Voice: A Massively-Multilingual Speech Corpus"},{"paperId":"37e52ff4714c7a08900b518127e438a195b84611","externalIds":{"DBLP":"journals/corr/abs-1910-06711","ArXiv":"1910.06711","MAG":"2980709326","CorpusId":202777813},"title":"MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis"},{"paperId":"8414354e89d8f213c1c8cf690d7271d451c69451","externalIds":{"DBLP":"journals/corr/abs-1909-13287","ArXiv":"1909.13287","MAG":"2975076716","DOI":"10.1007/978-981-15-2756-2_8","CorpusId":203593193},"title":"MG-VAE: Deep Chinese Folk Songs Generation with Specific Regional Style"},{"paperId":"6235de880364a1c4dedd6b446a7dd55f925a95fb","externalIds":{"MAG":"2998490864","DBLP":"journals/corr/abs-1909-08494","ArXiv":"1909.08494","DOI":"10.1109/WASPAA.2019.8937170","CorpusId":202660940},"title":"Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity"},{"paperId":"24a70db0bbb5f486126477e32a6a44ab917a4b11","externalIds":{"DBLP":"journals/corr/abs-1907-04868","MAG":"2959020461","ArXiv":"1907.04868","CorpusId":195886341},"title":"LakhNES: Improving Multi-instrumental Music Generation with Cross-domain Pre-training"},{"paperId":"82340f38baf7a8adf6b70b1cd9696fe270e35724","externalIds":{"MAG":"2946521317","DBLP":"journals/corr/abs-1905-06118","ArXiv":"1905.06118","CorpusId":155092724},"title":"Learning to Groove with Inverse Sequence Transformations"},{"paperId":"986ac33cdc5bee93e1e995757056969de86c641a","externalIds":{"MAG":"2982647297","DBLP":"journals/corr/abs-1904-08842","ArXiv":"1904.08842","DOI":"10.5281/zenodo.3672974","CorpusId":120395725},"title":"Inspecting and Interacting with Meaningful Music Representations using VAE"},{"paperId":"2603a68b4503ba949c91c7e00cd342624b4aae2f","externalIds":{"MAG":"2898148140","ArXiv":"1810.12247","DBLP":"conf/iclr/HawthorneSRSHDE19","CorpusId":53094405},"title":"Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset"},{"paperId":"f15f208a0421d330c50270ed10b716f1c01d38a3","externalIds":{"MAG":"2892104732","ArXiv":"1809.07600","DBLP":"journals/corr/abs-1809-07600","DOI":"10.3929/ETHZ-B-000292318","CorpusId":49317433},"title":"MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer"},{"paperId":"48ba025acf306b0b422859a2e523838fe4c0c4bc","externalIds":{"MAG":"2951155817","DBLP":"journals/corr/abs-1804-06267","ArXiv":"1804.06267","DOI":"10.1007/978-3-319-93764-9_28","CorpusId":4903311},"title":"The 2018 Signal Separation Evaluation Campaign"},{"paperId":"0d3bbe47fe67e5a125a2547913ac0e4a30f18c8d","externalIds":{"ArXiv":"1802.04208","MAG":"2950299304","DBLP":"conf/iclr/DonahueMP19","CorpusId":52890982},"title":"Adversarial Audio Synthesis"},{"paperId":"eba908be67198eda5d8e1ec4ce739aad4fe36a97","externalIds":{"MAG":"2758804652","DBLP":"journals/corr/abs-1812-04186","ArXiv":"1812.04186","DOI":"10.1145/3108242","CorpusId":3483927},"title":"A Functional Taxonomy of Music Generation Systems"},{"paperId":"6a70f88b77a88db4102af25aadd66952944ac5d7","externalIds":{"MAG":"2752134738","DBLP":"journals/corr/abs-1709-01620","ArXiv":"1709.01620","CorpusId":12211154},"title":"Deep Learning Techniques for Music Generation - A Survey"},{"paperId":"f83ef3250ba1166d7c1c7585da7dd78e0641fae7","externalIds":{"DBLP":"conf/aaai/DongHYY18","MAG":"2952928149","ArXiv":"1709.06298","DOI":"10.1609/aaai.v32i1.11312","CorpusId":19098155},"title":"MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"97c01b6cef7d7d88ec7eda488bfdc46fd601e76a","externalIds":{"DBLP":"conf/icml/EngelRRDNES17","ArXiv":"1704.01279","MAG":"2606176153","CorpusId":3697399},"title":"Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders"},{"paperId":"5cca155f40276323e9f66c04cf193f51413b5f21","externalIds":{"DBLP":"journals/taffco/HuY17","MAG":"2343847964","DOI":"10.1109/TAFFC.2016.2523503","CorpusId":25257939},"title":"Cross-Dataset and Cross-Cultural Music Mood Prediction: A Case on Western and Chinese Pop Songs"},{"paperId":"1fa6ba95b8383fad600bcbd6033c6eec73296381","externalIds":{"MAG":"2746068898","DBLP":"journals/corr/YangCY17","ArXiv":"1703.10847","CorpusId":2002865},"title":"MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation"},{"paperId":"4185286ec9d65086803c3ddf1cae1b27a9d6b5bb","externalIds":{"MAG":"2604184139","DBLP":"journals/corr/WangSSWWJYXCBLA17","CorpusId":6995778},"title":"Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model"},{"paperId":"659337507ff6d7817318edf7633634f7a0c4d5c3","externalIds":{"MAG":"2592535880","PubMedCentral":"5345802","DOI":"10.1371/journal.pone.0173392","CorpusId":11510950,"PubMed":"28282400"},"title":"Developing a benchmark for emotional analysis of music"},{"paperId":"5ba2218b708ca64ab556e39d5997202e012717d5","externalIds":{"MAG":"2593116425","DBLP":"conf/icassp/GemmekeEFJLMPR17","DOI":"10.1109/ICASSP.2017.7952261","CorpusId":21519176},"title":"Audio Set: An ontology and human-labeled dataset for audio events"},{"paperId":"3af5e203368fa2c7959d035493571d181a8682af","externalIds":{"MAG":"3101943858","ArXiv":"1612.08727","DBLP":"journals/tmm/LiLDDS19","DOI":"10.1109/TMM.2018.2856090","CorpusId":23731269},"title":"Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications"},{"paperId":"9a82095be10926f0a52f8f9939deadfe39be2184","externalIds":{"DBLP":"journals/corr/BenziDVB16","MAG":"2963358591","ArXiv":"1612.01840","CorpusId":1768692},"title":"FMA: A Dataset for Music Analysis"},{"paperId":"a066233a0b52a0e07197540ff2c73f42daf198f4","externalIds":{"MAG":"2560316200","ArXiv":"1612.01010","DBLP":"journals/corr/HadjeresP16","CorpusId":1215640},"title":"DeepBach: a Steerable Model for Bach Chorales Generation"},{"paperId":"d4903c15a7aba8e2c2386b2fe95edf0905144d6a","externalIds":{"MAG":"2527729766","DOI":"10.7488/DS/1495","CorpusId":64303572},"title":"SUPERSEDED - CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit"},{"paperId":"df0402517a7338ae28bc54acaac400de6b456a46","externalIds":{"MAG":"2519091744","ArXiv":"1609.03499","DBLP":"journals/corr/OordDZSVGKSK16","CorpusId":6254678},"title":"WaveNet: A Generative Model for Raw Audio"},{"paperId":"f0000d2e0b0510651a40d853ad5930c4c59652cd","externalIds":{"MAG":"2514141612","DBLP":"conf/cp/PapadopoulosRP16","DOI":"10.1007/978-3-319-44953-1_48","CorpusId":540471},"title":"Assisted Lead Sheet Composition Using FlowComposer"},{"paperId":"a5003309c446371a3e541d08484f2bb1d95ae40c","externalIds":{"MAG":"2536271585","CorpusId":3450397},"title":"Tension ribbons: Quantifying and visualising tonal tension."},{"paperId":"8388f1be26329fa45e5807e968a641ce170ea078","externalIds":{"MAG":"2949811265","ArXiv":"1511.06434","DBLP":"journals/corr/RadfordMC15","CorpusId":11758569},"title":"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"},{"paperId":"16a21a57c85bae0ada26454300dc5c5891f1c0e2","externalIds":{"MAG":"1539620591","DBLP":"conf/icassp/ChenYWC15","DOI":"10.1109/ICASSP.2015.7178058","CorpusId":5344924},"title":"The AMG1608 dataset for music emotion recognition"},{"paperId":"22ce875f4c614bb8cd9639e45181e8c3b577f580","externalIds":{"DBLP":"conf/enic/TopirceanuBU14","MAG":"2003419296","DOI":"10.1109/ENIC.2014.10","CorpusId":11742686},"title":"MuSeNet: Collaboration in the Music Artists Industry"},{"paperId":"bf8865380e642811621274aec480dfe495b44302","externalIds":{"MAG":"581915825","CorpusId":106630414},"title":"Chasing Sound: Technology, Culture, and the Art of Studio Recording from Edison to the LP"},{"paperId":"19158a205e3dedb6a19cc5ddbc568788d131d26e","externalIds":{"MAG":"2136129419","DBLP":"conf/mm/SoleymaniCSSY13","DOI":"10.1145/2506364.2506365","CorpusId":15438733},"title":"1000 songs for emotional analysis of music"},{"paperId":"e38e2bb8bce6f05610b62b293025559cc270c0aa","externalIds":{"MAG":"112722302","DBLP":"conf/ictinnovations/TanevB13","DOI":"10.1007/978-3-319-01466-1_22","CorpusId":71715517},"title":"Virtual Studio Technology inside Music Production"},{"paperId":"07c43a3ff15f2104022f2b1ca8ec4128a930b414","externalIds":{"MAG":"1819710477","DBLP":"conf/icml/Boulanger-LewandowskiBV12","ArXiv":"1206.6392","CorpusId":175089},"title":"Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription"},{"paperId":"6002ca403762b56b0f349a54ff1d285b6fc56af7","externalIds":{"DBLP":"journals/tist/YangC12","MAG":"1983507146","DOI":"10.1145/2168752.2168754","CorpusId":15995620},"title":"Machine Recognition of Music Emotion: A Review"},{"paperId":"432ef0d9be154b842ba0072a8213a66061abbbe4","externalIds":{"MAG":"1607743512","DOI":"10.1093/OXFORDHB/9780195388947.001.0001","CorpusId":108418071},"title":"The Oxford Handbook of Sound Studies"},{"paperId":"841354e2be279e84fcbdc5f018676e2ab9848fb3","externalIds":{"MAG":"1591470100","DOI":"10.1093/acprof:oso/9780199230143.001.0001","CorpusId":142736613},"title":"Handbook of Music and Emotion: Theory, Research, Applications"},{"paperId":"d1bacc1a26df8a3f78c78ba39193eac398c590de","externalIds":{"MAG":"2129192849","DBLP":"conf/ismir/CuthbertA10","CorpusId":6411706},"title":"Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data"},{"paperId":"4f488bb845638395212c77840f3e40dfc67e8166","externalIds":{"MAG":"1569085508","CorpusId":190599837},"title":"Musicophilia: Tales of Music and the Brain"},{"paperId":"52227cd6e3b3ffe71d2445b3d95db33457215cde","externalIds":{"MAG":"2133999183","DBLP":"conf/sigir/TurnbullBTL07","DOI":"10.1145/1277741.1277817","CorpusId":7162292},"title":"Towards musical query-by-semantic-description using the CAL500 data set"},{"paperId":"82bbb440a5093498ed3e88f2e4134cc17cf2dbe8","externalIds":{"MAG":"1655784964","DOI":"10.7551/mitpress/6575.001.0001","CorpusId":263150143},"title":"Sweet Anticipation: Music and the Psychology of Expectation"},{"paperId":"fa25610fb8586c2b50a3654edc5bb42fa7fc4729","externalIds":{"MAG":"1984514442","DOI":"10.1198/jasa.2004.s339","CorpusId":118901444},"title":"The Elements of Statistical Learning: Data Mining, Inference, and Prediction"},{"paperId":"ce03eab90a7ce28f3f51db004d2521498c89b240","externalIds":{"MAG":"2169264582","DBLP":"conf/icmc/Pachet02","DOI":"10.1076/jnmr.32.3.333.16861","CorpusId":5709063},"title":"The Continuator: Musical Interaction With Style"},{"paperId":"3b90fcf2572c3695ea2eee8297054237af5c3692","externalIds":{"MAG":"2483845588","DOI":"10.1525/CALIFORNIA/9780520218093.001.0001","CorpusId":107654268},"title":"The Poetics of Rock: Cutting Tracks, Making Records"},{"paperId":"d3718c5e9abb74c0baaa015a9390529139a4b3cc","externalIds":{"MAG":"1868379447","DOI":"10.1177/10298649020050S105","CorpusId":142992733},"title":"Emotion perceived and emotion felt: Same or different?"},{"paperId":"f74ba123df84672502af6b5bf869e7f58c985f7e","externalIds":{"MAG":"2476292100","DOI":"10.2307/900082","CorpusId":56210224},"title":"Any Sound You Can Imagine: Making Music/Consuming Technology"},{"paperId":"76499cfeac3733a7947737d2e3e2e644728eb8f6","externalIds":{"MAG":"2144840087","DOI":"10.1177/0305735691192002","CorpusId":145263317},"title":"Music Structure and Emotional Response: Some Empirical Findings"},{"paperId":"db004fc79653bbcf10252c83daeebe2ce2f94c5a","externalIds":{"MAG":"2169630934","DOI":"10.1017/S0261143000006589","CorpusId":163092664},"title":"Melody: A Popular Perspective"},{"paperId":"f6608509ff51a5d90d57bbc13bae678cfe46c38d","externalIds":{"MAG":"2133727920","DOI":"10.1093/JMT/23.2.56","CorpusId":22456172,"PubMed":"10301218"},"title":"Music research in medical/dental treatment: meta-analysis and clinical applications."},{"paperId":"490fcfac8e94dd1aae500fc5d4ba61671651208b","externalIds":{"MAG":"1979251464","DBLP":"journals/csur/LoyA85","DOI":"10.1145/4468.4485","CorpusId":2207680},"title":"Programming languages for computer music synthesis, performance, and composition"},{"paperId":"230efd8aefec1bb77facdd25926b4c045648837b","externalIds":{"MAG":"1513866359","DOI":"10.5860/choice.46-1385","CorpusId":191225654},"title":"Electronic and Experimental Music: Technology, Music, and Culture"},{"paperId":"18fdc9227f804a4e8ee54e3647d03cb035084ec9","externalIds":{"MAG":"2117804589","DOI":"10.2307/3343831","CorpusId":145794780},"title":"A Philosophy of Music Education"},{"paperId":"225ed03ab91432cc7d64509c1c90e8786c332390","externalIds":{"DBLP":"conf/evoW/PrivatoRN22","DOI":"10.1007/978-3-031-03789-4_15","CorpusId":248326337},"title":"A Creative Tool for the Musician Combining LSTM and Markov Chains in Max/MSP"},{"paperId":"9a456c9b0df438b9bea379deb022096131f17b4c","externalIds":{"MAG":"3127255509","DOI":"10.11517/JJSAI.36.1_2","CorpusId":236662030},"title":"特集「Affective Computing」にあたって"},{"paperId":"ced6ad20f2f8479193749243323616fd635b575c","externalIds":{"DBLP":"conf/aimusic/LuD21","DOI":"10.5281/zenodo.5137924","CorpusId":236328860},"title":"ChordGAN: Symbolic Music Style Transfer with Chroma Feature Extraction"},{"paperId":"c0537c56eadf74252b27aef3b424d2b21d2526fe","externalIds":{"MAG":"3080857286","DBLP":"journals/taslp/CifkaSR20","DOI":"10.1109/TASLP.2020.3019642","CorpusId":221848174},"title":"Groove2Groove: One-Shot Music Style Transfer With Supervision From Synthetic Data"},{"paperId":"d97a25dbb5a318d72989a5efc94a4449fd3f4963","externalIds":{"MAG":"2793284161","CorpusId":56407529},"title":"Music, media and technological creativity in the digital age"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"ccc4b1a0ff1d85e5cd6d7247ddd1c16ab10e8145","externalIds":{"MAG":"2770119437","DBLP":"conf/ismir/FonsecaPFFBFOPS17","CorpusId":40501028},"title":"Freesound Datasets: A Platform for the Creation of Open Audio Datasets"},{"paperId":"1cc934800ada5a3d8ed0015154a9b22f807039b7","externalIds":{"CorpusId":255094733},"title":"The Design and Study of Virtual Sound Field in Music Production"},{"paperId":"da766ce5c3c44376663e93df84650d6d2059bc3c","externalIds":{"DBLP":"phd/us/Raffel16","MAG":"2475687244","DOI":"10.7916/D8N58MHV","CorpusId":63439223},"title":"Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching"},{"paperId":"f5494d2cc1048226793470052f707acc4653f102","externalIds":{"MAG":"2494956341","DOI":"10.5860/choice.42-4553","CorpusId":63819322},"title":"Capturing Sound How Technology Has Changed Music"},{"paperId":"1d30b7de056b8b3c541f1164c9a8db17958061ab","externalIds":{"MAG":"2465188654","CorpusId":114213740},"title":"Interface Aesthetics: Sound, Software, and the Ecology of Digital Audio Production"},{"paperId":"5aaf311172b9778d78f6904fbe40124c63463b57","externalIds":{"MAG":"1556219185","DBLP":"conf/ismir/Bertin-MahieuxEWL11","DOI":"10.7916/D8NZ8J07","CorpusId":9770532},"title":"The Million Song Dataset"},{"paperId":"8a1384e041cc6ea2735b01c734aeef666dc92884","externalIds":{"DBLP":"conf/ismir/LawWMBD09","MAG":"2127870748","CorpusId":9788545},"title":"Evaluation of Algorithms Using Games: The Case of Music Tagging"},{"paperId":"6a3b58cbad80a7c9039ea388829ee8b093ad1eba","externalIds":{"DOI":"10.1002/14651858.CD004517.pub2","CorpusId":27781262,"PubMed":"18254052"},"title":"Music therapy for depression."},{"paperId":"178d2cc5628113cd2702e1f6773334272fb94d0e","externalIds":{"MAG":"2904306201","CorpusId":14074976},"title":"Music Generation from Statistical Models"},{"paperId":"a99f26f722bc562565ff4b43993c69d9d6845218","externalIds":{"MAG":"1502260477","CorpusId":107190688},"title":"Analog Days: The Invention and Impact of the Moog Synthesizer"},{"paperId":"a05d9b0d6929a07bf3556356e787c1c2b6339c22","externalIds":{"MAG":"1977444015","DOI":"10.2307/2003155","CorpusId":119884301},"title":"Experimental music: Composition with an electronic computer"},{"paperId":"70690725c7973492978dd27a6aa9051c6c97cbab","externalIds":{"MAG":"2087137043","DOI":"10.5860/choice.30-0194","CorpusId":145500709},"title":"Formalized Music: Thought and Mathematics in Composition"},{"paperId":"6038d62f22be3162324d3cb5214512966fc6ddb0","externalIds":{"CorpusId":266690063},"title":"Music Transformer 기반 음악"}]}