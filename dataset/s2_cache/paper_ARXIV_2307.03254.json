{"paperId":"420f03d44a3bee8c1c51216e1045d0ff92e8fefc","externalIds":{"ArXiv":"2307.03254","DBLP":"journals/corr/abs-2307-03254","DOI":"10.48550/arXiv.2307.03254","CorpusId":259375902},"title":"Vision Language Transformers: A Survey","openAccessPdf":{"url":"https://arxiv.org/pdf/2307.03254","status":"CLOSED","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03254, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2221315806","name":"Clayton Fields"},{"authorId":"2061232","name":"C. Kennington"}],"abstract":"Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \\citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strengths, limitations and some open questions that remain."}