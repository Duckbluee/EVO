{"references":[{"paperId":"48cbad923c60ff4cfab54a52de491fa62268cf43","externalIds":{"DBLP":"journals/tmlr/MelnikLLYRR24","ArXiv":"2405.03150","DOI":"10.48550/arXiv.2405.03150","CorpusId":269606007},"title":"Video Diffusion Models: A Survey"},{"paperId":"036e51a8f83d1fb95b0b5d2b85c8add36cccd741","externalIds":{"ArXiv":"2312.16274","CorpusId":266573593},"title":"Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face Synthesis"},{"paperId":"5c4b5ecef0a174260a09b8b5b2f27a3e9c870932","externalIds":{"ArXiv":"2312.15636","DBLP":"journals/corr/abs-2312-15636","DOI":"10.48550/arXiv.2312.15636","CorpusId":266551316},"title":"Lifting by Image - Leveraging Image Cues for Accurate 3D Human Pose Estimation"},{"paperId":"90cf73f5a7f98891be62d67ec0ef70e8b2e8a80f","externalIds":{"DBLP":"conf/icassp/FuWC025","ArXiv":"2312.14871","DOI":"10.1109/ICASSP49660.2025.10889805","CorpusId":266521368},"title":"BrainVis: Exploring the Bridge between Brain and Visual Signals via Image Reconstruction"},{"paperId":"18b91948fe1aa1ccacbaff4ee110dbdd5b790fb1","externalIds":{"DBLP":"conf/iclr/ZhaoXXJ00IVG25","ArXiv":"2312.14216","CorpusId":266521085},"title":"DreamDistribution: Learning Prompt Distribution for Diverse In-distribution Generation"},{"paperId":"b2e650f7f3348ee4fa12798c0d8ecfcf479dc328","externalIds":{"ArXiv":"2312.13691","DBLP":"journals/corr/abs-2312-13691","DOI":"10.48550/arXiv.2312.13691","CorpusId":266436022},"title":"DreamTuner: Single Image is Enough for Subject-Driven Generation"},{"paperId":"4b1b5e219fb41a7413599c3b2ca6a7fdf045d1a5","externalIds":{"DBLP":"conf/cvpr/SunCZZYWRL0W24","ArXiv":"2312.13286","DOI":"10.1109/CVPR52733.2024.01365","CorpusId":266374640},"title":"Generative Multimodal Models are In-Context Learners"},{"paperId":"301d7c64a085af324acd9ab729be57d3466e7ff5","externalIds":{"ArXiv":"2312.12232","DBLP":"conf/aaai/ZhangC00Q24","DOI":"10.48550/arXiv.2312.12232","CorpusId":266362590},"title":"Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model"},{"paperId":"f2832c8404f9dd823667e2cc6ffd39076c473369","externalIds":{"DBLP":"conf/aaai/Cai0JB0Z24","ArXiv":"2312.11826","DOI":"10.48550/arXiv.2312.11826","CorpusId":266362424},"title":"Decoupled Textual Embeddings for Customized Image Generation"},{"paperId":"b3fe9a43686211016ae9a5a28b3fe253578f7fb2","externalIds":{"DBLP":"journals/corr/abs-2312-12030","ArXiv":"2312.12030","DOI":"10.48550/arXiv.2312.12030","CorpusId":266362385},"title":"Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method"},{"paperId":"2da62ed1a45489e0f3adcb3bef375ebbb4a6d529","externalIds":{"DBLP":"conf/cvpr/JiangMPHZ24","ArXiv":"2312.11392","DOI":"10.1109/CVPR52733.2024.00859","CorpusId":266359162},"title":"SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing"},{"paperId":"9ad5bd881c0990ef740c03d74ff68b18572f11c3","externalIds":{"ArXiv":"2312.08768","DBLP":"journals/corr/abs-2312-08768","DOI":"10.48550/arXiv.2312.08768","CorpusId":266209765},"title":"Local Conditional Controlling for Text-to-Image Diffusion Models"},{"paperId":"715e39ebf8d0287bb78bad7148086c11e4b05933","externalIds":{"DBLP":"conf/cvpr/CaoZ0H025","ArXiv":"2312.08195","DOI":"10.1109/CVPR52734.2025.01711","CorpusId":266191061},"title":"Image is All You Need to Empower Large-scale Diffusion Models for In-Domain Generation"},{"paperId":"831425c9cd30c61a333528e6678d73370aff84b1","externalIds":{"DBLP":"journals/corr/abs-2312-07536","ArXiv":"2312.07536","DOI":"10.1109/CVPR52733.2024.00713","CorpusId":266174339},"title":"FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition"},{"paperId":"2ce676332cc027524319f72a487c1d999ede13cf","externalIds":{"ArXiv":"2312.06971","DBLP":"journals/corr/abs-2312-06971","DOI":"10.48550/arXiv.2312.06971","CorpusId":266174705},"title":"CCM: Adding Conditional Controls to Text-to-Image Consistency Models"},{"paperId":"c15380dcda5a010827e3b014dcebe95b1218c680","externalIds":{"DBLP":"journals/corr/abs-2312-06439","ArXiv":"2312.06439","DOI":"10.1109/CVPR52733.2024.00513","CorpusId":266162414},"title":"DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior"},{"paperId":"9732d02e9fbfc953cbe9c6332919b13610859a13","externalIds":{"ArXiv":"2312.06354","DBLP":"conf/cvpr/PengZJTLZLJWJ24","DOI":"10.1109/CVPR52733.2024.02557","CorpusId":266163204},"title":"PortraitBooth: A Versatile Portrait Model for Fast Identity-Preserved Personalization"},{"paperId":"cb01408aed1a7ee32d7556af2c6949ca7f5374ca","externalIds":{"DBLP":"journals/corr/abs-2312-06116","ArXiv":"2312.06116","DOI":"10.48550/arXiv.2312.06116","CorpusId":266163420},"title":"Stellar: Systematic Evaluation of Human-Centric Personalized Text-to-Image Methods"},{"paperId":"a09a43b60ed529f405196d4100156b432bb443cd","externalIds":{"DBLP":"conf/cvpr/HoeJCTH24","ArXiv":"2312.05849","DOI":"10.1109/CVPR52733.2024.00591","CorpusId":266162685},"title":"InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models"},{"paperId":"7845058565eb71e3e423ec72641abe9d7ba2ed0a","externalIds":{"DBLP":"journals/corr/abs-2312-04884","ArXiv":"2312.04884","DOI":"10.48550/arXiv.2312.04884","CorpusId":266149725},"title":"UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models"},{"paperId":"089f11a3e677996cbc62832d0c6987b75989c5fd","externalIds":{"ArXiv":"2312.05107","DBLP":"journals/corr/abs-2312-05107","DOI":"10.48550/arXiv.2312.05107","CorpusId":266149456},"title":"DreaMoving: A Human Video Generation Framework based on Diffusion Models"},{"paperId":"9a0fd42bc41e1c4b7cc36690378b9bacad4ef178","externalIds":{"DBLP":"journals/corr/abs-2312-05288","ArXiv":"2312.05288","DOI":"10.48550/arXiv.2312.05288","CorpusId":266162348},"title":"MotionCrafter: One-Shot Motion Customization of Diffusion Models"},{"paperId":"50dfd272a4a7da2b643f80064eec6192d0e7279c","externalIds":{"DBLP":"journals/corr/abs-2312-04461","ArXiv":"2312.04461","DOI":"10.1109/CVPR52733.2024.00825","CorpusId":266052536},"title":"PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding"},{"paperId":"ed2aaf4878e2c3a3b43981aa1d7aa37a21972f07","externalIds":{"DBLP":"journals/corr/abs-2312-04364","ArXiv":"2312.04364","DOI":"10.1109/CVPR52733.2024.00824","CorpusId":266051307},"title":"DemoCaricature: Democratising Caricature Generation with a Rough Sketch"},{"paperId":"5a0f5d1e90c1f509a79432a400e02916d5a8dff0","externalIds":{"ArXiv":"2312.04966","DBLP":"conf/accv/MaterzynskaSSTZR24","DOI":"10.1007/978-981-96-0917-8_7","CorpusId":266149624},"title":"NewMove: Customizing Text-to-Video Models with Novel Motions"},{"paperId":"8a14b0a3633de9d9b3bed552c5d3b9d7dbd57bc4","externalIds":{"ArXiv":"2312.03584","DBLP":"conf/eccv/NajdenkoskaSDMRR24","DOI":"10.48550/arXiv.2312.03584","CorpusId":265722277},"title":"Context Diffusion: In-Context Aware Image Generation"},{"paperId":"0cfdcbc1429c43ad38c9ba8b8a0b9b9b71e8b6ce","externalIds":{"DBLP":"conf/cvpr/PoYAW24","ArXiv":"2312.02432","DOI":"10.1109/CVPR52733.2024.00761","CorpusId":265659333},"title":"Orthogonal Adaptation for Modular Customization of Diffusion Models"},{"paperId":"257c2adecba9cfba7eaec8463f8d378f6886f550","externalIds":{"DBLP":"conf/eccv/CheongMG24","ArXiv":"2312.03154","DOI":"10.48550/arXiv.2312.03154","CorpusId":265722853},"title":"ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet"},{"paperId":"f30bb09dbd95845d792bdac217a9a652635ee8a5","externalIds":{"DBLP":"conf/cvpr/ZhouZGS24","ArXiv":"2312.03045","DOI":"10.1109/CVPR52733.2024.00877","CorpusId":265695221},"title":"Customization Assistant for Text-to-image Generation"},{"paperId":"9af04f2ef5c798655c987aaf76e35a103d2efc30","externalIds":{"DBLP":"journals/corr/abs-2312-03079","ArXiv":"2312.03079","DOI":"10.1145/3641519.3657525","CorpusId":265693942},"title":"LOOSECONTROL: Lifting ControlNet for Generalized Depth Conditioning"},{"paperId":"28315d5dbafab7c5603d1365aea06a5c94506532","externalIds":{"DBLP":"journals/corr/abs-2312-02503","ArXiv":"2312.02503","DOI":"10.48550/arXiv.2312.02503","CorpusId":265659524},"title":"SAVE: Protagonist Diversification with Structure Agnostic Video Editing"},{"paperId":"29f1eb77b135770b4887df6a5c20980ea7d19769","externalIds":{"ArXiv":"2312.02663","DBLP":"journals/corr/abs-2312-02663","DOI":"10.48550/arXiv.2312.02663","CorpusId":265659109},"title":"FaceStudio: Put Your Face Everywhere in Seconds"},{"paperId":"5322ba0056794e0d4b7bc519b92061492d3cecb4","externalIds":{"ArXiv":"2312.03771","DBLP":"journals/corr/abs-2312-03771","DOI":"10.48550/arXiv.2312.03771","CorpusId":266053929},"title":"DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models"},{"paperId":"99b30a6f2694f363fdbc37243adb3c02d7e0b187","externalIds":{"ArXiv":"2312.02521","CorpusId":265659246},"title":"RetriBooru: Leakage-Free Retrieval of Conditions from Reference Images for Subject-Driven Generation"},{"paperId":"22d45b7b4cd23162dd38c9b577749d86db34075b","externalIds":{"ArXiv":"2312.02109","DBLP":"conf/cvpr/ChenTH24","DOI":"10.1109/CVPR52733.2024.00823","CorpusId":265609037},"title":"ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation"},{"paperId":"c320eb9255940cec341af7aef9c8c38a7a167d6a","externalIds":{"DBLP":"conf/cvpr/HertzVFC24","ArXiv":"2312.02133","DOI":"10.1109/CVPR52733.2024.00457","CorpusId":265608730},"title":"Style Aligned Image Generation via Shared Attention"},{"paperId":"c76e04e49dcb38b026e092a2ac44dff4259589b9","externalIds":{"DBLP":"conf/cpal/YangZWWL25","ArXiv":"2312.01255","DOI":"10.48550/arXiv.2312.01255","CorpusId":265608919},"title":"Meta ControlNet: Enhancing Task Adaptation via Meta Learning"},{"paperId":"f7f56ac5efc5ef59c49c5e938c424f74057cc1b5","externalIds":{"DBLP":"journals/corr/abs-2312-01408","ArXiv":"2312.01408","DOI":"10.48550/arXiv.2312.01408","CorpusId":265608911},"title":"Improving In-Context Learning in Diffusion Models with Visual Context-Modulated Prompts"},{"paperId":"d759a0543d3112361ff9f124ae40142aa4949093","externalIds":{"ArXiv":"2312.00777","DBLP":"journals/corr/abs-2312-00777","DOI":"10.1109/CVPR52733.2024.00639","CorpusId":265551454},"title":"VideoBooth: Diffusion-based Video Generation with Image Prompts"},{"paperId":"3aa03f01891f0fa5d749e1edce20438d7bd69ea6","externalIds":{"ArXiv":"2311.18763","DBLP":"conf/cvpr/SmithHKSJ22","DOI":"10.1109/CVPRW63382.2024.00181","CorpusId":265506232},"title":"Continual Diffusion with STAMINA: STack-And-Mask INcremental Adapters"},{"paperId":"e1e136cfed136d8b8047a83fa351650c23cb2e4d","externalIds":{"DBLP":"conf/eccv/QiHLY24","ArXiv":"2311.18435","DOI":"10.1007/978-3-031-72848-8_25","CorpusId":265506422},"title":"Layered Rendering Diffusion Model for Controllable Zero-Shot Image Synthesis"},{"paperId":"fe799bcd7f4190714ca1cd6d90617e3468aa1bee","externalIds":{"DBLP":"journals/corr/abs-2312-00079","ArXiv":"2312.00079","DOI":"10.48550/arXiv.2312.00079","CorpusId":265551490},"title":"HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models"},{"paperId":"f373c5569b45bf580b7502729a83761a791ee209","externalIds":{"DBLP":"conf/eccv/ZhaoLGZZWXJ24","ArXiv":"2311.17338","DOI":"10.1007/978-3-031-72649-1_12","CorpusId":265498336},"title":"MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing"},{"paperId":"aa709983e1d74a3bc7737b9d8735c0c35bea0175","externalIds":{"DBLP":"conf/cvpr/0002HL24","ArXiv":"2311.17461","DOI":"10.1109/CVPR52733.2024.00213","CorpusId":265498650},"title":"When StyleGAN Meets Stable Diffusion: a $\\mathcal{W}_{+}$ Adapter for Personalized Image Generation"},{"paperId":"2b922a7e8652b446afef86dd58ed3ab08942befa","externalIds":{"ArXiv":"2311.17609","DBLP":"conf/eccv/VoynovHAFC24","DOI":"10.1007/978-3-031-72980-5_9","CorpusId":265498574},"title":"Curved Diffusion: A Generative Model with Optical Geometry Control"},{"paperId":"c3bb138fc5187a0949f5cee93f5004e1a78b5796","externalIds":{"ArXiv":"2311.17086","DBLP":"journals/corr/abs-2311-17086","DOI":"10.48550/arXiv.2311.17086","CorpusId":265498854},"title":"PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation"},{"paperId":"1c6e2a4da1ead685a95c079751bf4d7a727d8180","externalIds":{"ArXiv":"2311.16465","DBLP":"journals/corr/abs-2311-16465","DOI":"10.48550/arXiv.2311.16465","CorpusId":265466475},"title":"TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering"},{"paperId":"a19c3032a08513f37f3346beed4b18d42dfd8c9d","externalIds":{"DBLP":"journals/corr/abs-2311-17083","ArXiv":"2311.17083","DOI":"10.1109/CVPR52733.2024.00661","CorpusId":265498829},"title":"CLiC: Concept Learning in Context"},{"paperId":"e73ab2183c7567d45c0a322c015de6b255f9cf13","externalIds":{"ArXiv":"2311.15841","DBLP":"conf/cvpr/HuangGFCFLW24","DOI":"10.1109/CVPR52733.2024.00745","CorpusId":265455924},"title":"Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation"},{"paperId":"ed4f1b0f6c09f59d07e817e532a25f4d25e94dbc","externalIds":{"DBLP":"journals/corr/abs-2311-14284","ArXiv":"2311.14284","DOI":"10.1007/s11263-025-02435-1","CorpusId":265444940},"title":"Paragraph-to-Image Generation with Information-Enriched Diffusion Model"},{"paperId":"86c4e5d4efba68bc22d889c38f395bec7bb040ad","externalIds":{"DBLP":"journals/corr/abs-2311-14631","ArXiv":"2311.14631","DOI":"10.1109/TCSVT.2025.3531917","CorpusId":265445503},"title":"CatVersion: Concatenating Embeddings for Diffusion-Based Text-to-Image Personalization"},{"paperId":"2fd71c10e3c8a56b516d7e470cab4d54afe28eb8","externalIds":{"ArXiv":"2311.13833","DBLP":"conf/eccv/MotamedPG24","DOI":"10.1007/978-3-031-72633-0_7","CorpusId":265444948},"title":"Lego: Learning to Disentangle and Invert Personalized Concepts Beyond Object Appearance in Text-to-Image Diffusion Models"},{"paperId":"185e88645dfc07d6ca81a55dfc66bd3452400276","externalIds":{"ArXiv":"2311.13600","DBLP":"journals/corr/abs-2311-13600","DOI":"10.48550/arXiv.2311.13600","CorpusId":265351656},"title":"ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs"},{"paperId":"183ff1cfd2370cbcf4088780bf1f6e8d6db8be0f","externalIds":{"DBLP":"journals/corr/abs-2311-12342","ArXiv":"2311.12342","DOI":"10.48550/arXiv.2311.12342","CorpusId":265308886},"title":"LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis"},{"paperId":"2f8d1cd054b11e31525ab04030e14eb78e73dba1","externalIds":{"ArXiv":"2311.11919","DBLP":"conf/wacv/AgarwalKSS25","DOI":"10.1109/WACV61041.2025.00590","CorpusId":265295502},"title":"An Image is Worth Multiple Words: Multi-Attribute Inversion for Constrained Text-To-Image Synthesis"},{"paperId":"67ca824a96088a67d80ec3c7c2910f9923c78f42","externalIds":{"DBLP":"journals/corr/abs-2311-10522","ArXiv":"2311.10522","DOI":"10.1109/ICME59968.2025.11209238","CorpusId":265281572},"title":"Enhancing Object Coherence in Layout-to-Image Synthesis"},{"paperId":"7cb051e287f5e8a71baf6d5bc527e451f17d9425","externalIds":{"DBLP":"conf/cvpr/WangZ0J24","ArXiv":"2311.10329","DOI":"10.1109/CVPR52733.2024.00733","CorpusId":265281113},"title":"High-fidelity Person-centric Subject-to-Image Synthesis"},{"paperId":"0df9eefd8739769b292140a89b30dda5eaccda61","externalIds":{"ArXiv":"2311.09753","DBLP":"journals/corr/abs-2311-09753","DOI":"10.48550/arXiv.2311.09753","CorpusId":265220699},"title":"DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics"},{"paperId":"bc6d13540a567629767c5b2102df4f789752c9e2","externalIds":{"ArXiv":"2311.04315","DBLP":"conf/wacv/HeCKYWRK25","DOI":"10.1109/WACV61041.2025.00372","CorpusId":265050824},"title":"A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization"},{"paperId":"b7b7a549629bbe7fa325572339938980e77d155c","externalIds":{"DBLP":"journals/corr/abs-2311-03054","ArXiv":"2311.03054","DOI":"10.48550/arXiv.2311.03054","CorpusId":265034144},"title":"AnyText: Multilingual Visual Text Generation And Editing"},{"paperId":"966a20904440297aec0a501118c5b677f15c90fd","externalIds":{"DBLP":"journals/corr/abs-2311-05461","ArXiv":"2311.05461","DOI":"10.1145/3581783.3612489","CorpusId":264492267},"title":"Control3D: Towards Controllable Text-to-3D Generation"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","externalIds":{"ArXiv":"2310.10647","DBLP":"journals/corr/abs-2310-10647","DOI":"10.1145/3696415","CorpusId":264172934},"title":"A Survey on Video Diffusion Models"},{"paperId":"c7590e69de511e476073ebb27958dcc9327c37e7","externalIds":{"ArXiv":"2310.10769","DBLP":"journals/corr/abs-2310-10769","DOI":"10.48550/arXiv.2310.10769","CorpusId":264172280},"title":"LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation"},{"paperId":"54b49617a447dd38b079725b74e3f0753f2b782f","externalIds":{"DBLP":"conf/iclr/XiaoL0WH24","ArXiv":"2310.08872","DOI":"10.48550/arXiv.2310.08872","CorpusId":264128211},"title":"R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation"},{"paperId":"1333fc49d5c5233dbb1673bb42c4b1c25ea589a1","externalIds":{"DBLP":"conf/cvpr/ChenZW0L24","ArXiv":"2310.08129","DOI":"10.1109/CVPR52733.2024.00738","CorpusId":263909073},"title":"Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting"},{"paperId":"6541f0f74cf6f6d0d8b4a8d9efb64d5e0729bc13","externalIds":{"DBLP":"conf/eccv/ZhaoGWZLWKS24","ArXiv":"2310.08465","DOI":"10.48550/arXiv.2310.08465","CorpusId":263909602},"title":"MotionDirector: Motion Customization of Text-to-Video Diffusion Models"},{"paperId":"61b160196ccf662f3da7a5ff6e6c8b5d3c35af1a","externalIds":{"ArXiv":"2310.08579","DBLP":"journals/corr/abs-2310-08579","DOI":"10.48550/arXiv.2310.08579","CorpusId":263909070},"title":"HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion"},{"paperId":"a63d56908f7d960343821f4acbd6efb47a4e6296","externalIds":{"ArXiv":"2310.07222","DBLP":"conf/mm/YangC023","DOI":"10.1145/3581783.3612200","CorpusId":263835196},"title":"Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model"},{"paperId":"7c483edfc0d98c044ab03a707bc16cffcfff13a4","externalIds":{"DBLP":"journals/corr/abs-2310-06347","ArXiv":"2310.06347","DOI":"10.48550/arXiv.2310.06347","CorpusId":263829757},"title":"JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling"},{"paperId":"409961b88a3c7495afd21a9a4183c0bfaa858da0","externalIds":{"ArXiv":"2310.02992","DBLP":"conf/iclr/Pan0HPCW24","DOI":"10.48550/arXiv.2310.02992","CorpusId":263620748},"title":"Kosmos-G: Generating Images in Context with Multimodal Large Language Models"},{"paperId":"4584dee8505ce8cdaa09d7c3f4b4ab6568b3e766","externalIds":{"DBLP":"journals/corr/abs-2309-16668","ArXiv":"2309.16668","DOI":"10.1145/3658237","CorpusId":263135265},"title":"RealFill: Reference-Driven Generation for Authentic Image Completion"},{"paperId":"8270a20dbc44706aaf150bbbcfa55b017fa399b9","externalIds":{"DBLP":"journals/corr/abs-2309-15508","ArXiv":"2309.15508","DOI":"10.48550/arXiv.2309.15508","CorpusId":263152042},"title":"DreamCom: Finetuning Text-guided Inpainting Model for Image Composition"},{"paperId":"b901042d6bd0ddfe8f86f488f5b17c4b53d74e40","externalIds":{"ArXiv":"2309.14859","DBLP":"journals/corr/abs-2309-14859","DOI":"10.48550/arXiv.2309.14859","CorpusId":262825238},"title":"Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation"},{"paperId":"f0410acf0c7c1e391023db24b3162ba11fc38d58","externalIds":{"DBLP":"conf/aaai/HyungSC24","ArXiv":"2309.06895","DOI":"10.48550/arXiv.2309.06895","CorpusId":261705666},"title":"MagiCapture: High-Resolution Multi-Concept Portrait Customization"},{"paperId":"3903937e84ba85b1c4d1386f5a8bddd695287b72","externalIds":{"ArXiv":"2309.05793","DBLP":"journals/corr/abs-2309-05793","DOI":"10.48550/arXiv.2309.05793","CorpusId":261696798},"title":"PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models"},{"paperId":"38d44a4269646fa69a3542aaef961efba1ea9f2a","externalIds":{"DBLP":"journals/pami/SunLDLDC24","ArXiv":"2309.04430","DOI":"10.1109/TPAMI.2024.3382753","CorpusId":261660311},"title":"Create Your World: Lifelong Text-to-Image Diffusion"},{"paperId":"c15803110bfacac95f7f0283b0b34492e20a1a3b","externalIds":{"ArXiv":"2308.12964","DBLP":"conf/iccv/KimLKHZ23","DOI":"10.1109/ICCV51070.2023.00708","CorpusId":261101003},"title":"Dense Text-to-Image Generation with Attention Modulation"},{"paperId":"f1442ebd0c1619d190f893d79c9d064149c0c06a","externalIds":{"DBLP":"journals/corr/abs-2308-10156","ArXiv":"2308.10156","DOI":"10.48550/arXiv.2308.10156","CorpusId":261049490},"title":"SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form Layout-to-Image Generation"},{"paperId":"733f42314555def0d4983e31c5598162657839e8","externalIds":{"DBLP":"journals/corr/abs-2308-10040","ArXiv":"2308.10040","DOI":"10.48550/arXiv.2308.10040","CorpusId":261049352},"title":"ControlCom: Controllable Image Composition using Diffusion Model"},{"paperId":"2854e5bab8e6f36e54c64456628a9559bf67019e","externalIds":{"ArXiv":"2308.06721","DBLP":"journals/corr/abs-2308-06721","DOI":"10.48550/arXiv.2308.06721","CorpusId":260886966},"title":"IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models"},{"paperId":"96a69d1026503262d8150bd9d958b5930fa5cdbb","externalIds":{"DBLP":"conf/mm/YuZLZWW23","ArXiv":"2307.13908","DOI":"10.1145/3581783.3612232","CorpusId":260164763},"title":"Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation"},{"paperId":"a756c588cb1de7a67a693450ef21271451294871","externalIds":{"ArXiv":"2307.11410","DBLP":"journals/corr/abs-2307-11410","DOI":"10.1145/3641519.3657469","CorpusId":260091569},"title":"Subject-Diffusion: Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"276f6117b8b8549a47461653b95e657278260ee3","externalIds":{"DBLP":"conf/cvpr/RuizLJWHPWRA24","ArXiv":"2307.06949","DOI":"10.1109/CVPR52733.2024.00624","CorpusId":259847576},"title":"HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models"},{"paperId":"bd5c066a63ae6a5fcf99e53551161446baf54a27","externalIds":{"DBLP":"conf/siggrapha/ArarGACCSB23","ArXiv":"2307.06925","DOI":"10.1145/3610548.3618173","CorpusId":259847716},"title":"Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models"},{"paperId":"c1caa303549764d220ff17dc1785985dd1ba6047","externalIds":{"DBLP":"journals/corr/abs-2307-04725","ArXiv":"2307.04725","DOI":"10.48550/arXiv.2307.04725","CorpusId":259501509},"title":"AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning"},{"paperId":"d7890d1906d95c4ae4c430b350455156d6d8aed9","externalIds":{"DBLP":"conf/iclr/PodellELBDMPR24","ArXiv":"2307.01952","CorpusId":259341735},"title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"},{"paperId":"e72b2acf078fcc7d7c91851ad2bece194ef710b5","externalIds":{"ArXiv":"2307.00300","DBLP":"journals/corr/abs-2307-00300","DOI":"10.48550/arXiv.2307.00300","CorpusId":259316083},"title":"DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation"},{"paperId":"c2bd88a919994d7c6ac53d9a286104f02a396662","externalIds":{"DBLP":"journals/corr/abs-2306-16934","ArXiv":"2306.16934","DOI":"10.48550/arXiv.2306.16934","CorpusId":259286834},"title":"DreamDiffusion: Generating High-Quality Images from Brain EEG Signals"},{"paperId":"030faace2b76aad71f926db811a02aa38b5101a7","externalIds":{"ArXiv":"2306.17154","DBLP":"journals/corr/abs-2306-17154","DOI":"10.48550/arXiv.2306.17154","CorpusId":259287552},"title":"Generate Anything Anywhere in Any Scene"},{"paperId":"313f5d7b29876dfd8411ab9f423961ff49a869bd","externalIds":{"ArXiv":"2306.14408","DBLP":"conf/eccv/WangSGCLC24","DOI":"10.1007/978-3-031-72970-6_2","CorpusId":259251938},"title":"Text-Anchored Score Composition: Tackling Condition Misalignment in Text-to-Image Diffusion Models"},{"paperId":"ed13957337aefd8c323abd3312e7a1b010a674f7","externalIds":{"ArXiv":"2306.14636","DBLP":"journals/corr/abs-2306-14636","DOI":"10.48550/arXiv.2306.14636","CorpusId":259252486},"title":"Localized Text-to-Image Generation for Free via Cross Attention Control"},{"paperId":"d115238c6ee8fcdd635247f871d25732b457d1d3","externalIds":{"DBLP":"journals/corr/abs-2306-13754","ArXiv":"2306.13754","DOI":"10.1109/ICCV51070.2023.00207","CorpusId":259252153},"title":"Zero-shot spatial layout conditioning for text-to-image diffusion models"},{"paperId":"0b3adcdac24f69642f37e149b3c1b8c07358a274","externalIds":{"DBLP":"journals/cgf/ZhangHL23","ArXiv":"2306.13078","DOI":"10.1111/cgf.14966","CorpusId":259224969},"title":"Continuous Layout Editing of Single Images with Diffusion Models"},{"paperId":"47e3280bf3178262ca7cb2b4c0300681b98467b0","externalIds":{"ArXiv":"2306.11504","DBLP":"journals/corr/abs-2306-11504","DOI":"10.48550/arXiv.2306.11504","CorpusId":259203503},"title":"Align, Adapt and Inject: Sound-guided Unified Image Generation"},{"paperId":"9e8648550fbec45dc712eb084251807b6c44e1e4","externalIds":{"DBLP":"journals/corr/abs-2306-08877","ArXiv":"2306.08877","DOI":"10.48550/arXiv.2306.08877","CorpusId":259164750},"title":"Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment"},{"paperId":"16b42fc85f4c073aa00c410cbdce965d7c6f8d4d","externalIds":{"ArXiv":"2306.07967","DBLP":"journals/corr/abs-2306-07967","DOI":"10.48550/arXiv.2306.07967","CorpusId":259144860},"title":"One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning"},{"paperId":"669a95efbb9a1388ef465c7d6eb891705842fa1c","externalIds":{"DBLP":"journals/corr/abs-2306-07280","ArXiv":"2306.07280","DOI":"10.48550/arXiv.2306.07280","CorpusId":259138650},"title":"Controlling Text-to-Image Diffusion by Orthogonal Finetuning"},{"paperId":"8d6040c2d966939c8c18a4406873a5b154635629","externalIds":{"ArXiv":"2306.06638","DBLP":"conf/siggrapha/ValevskiLML23","DOI":"10.1145/3610548.3618249","CorpusId":259138505},"title":"Face0: Instantaneously Conditioning a Text-to-Image Model on a Face"},{"paperId":"a903e1e0ffd04dd666f3537f6570d742d7be3486","externalIds":{"ArXiv":"2306.05427","DBLP":"conf/cvpr/PhungGH24","DOI":"10.1109/CVPR52733.2024.00758","CorpusId":259108247},"title":"Grounded Text-to-Image Synthesis with Attention Refocusing"},{"paperId":"dfd7b99448921fa8d74b510591b19473d3d3ab5d","externalIds":{"DBLP":"journals/corr/abs-2306-00914","ArXiv":"2306.00914","DOI":"10.48550/arXiv.2306.00914","CorpusId":258999477},"title":"Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation"},{"paperId":"95d7c841d9db6e02a237a6382decaa62bcc5825f","externalIds":{"ArXiv":"2306.00964","DBLP":"journals/corr/abs-2306-00964","DOI":"10.48550/arXiv.2306.00964","CorpusId":258999236},"title":"Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation"},{"paperId":"861370f7c2d18bed09905fde334a19cc96e83e14","externalIds":{"ArXiv":"2306.00983","DBLP":"journals/corr/abs-2306-00983","DOI":"10.48550/arXiv.2306.00983","CorpusId":258999204},"title":"StyleDrop: Text-to-Image Generation in Any Style"},{"paperId":"ea720398e02286b1ae55b2a21b52f45a00dca1ba","externalIds":{"DBLP":"conf/cvpr/SongZLCPZKA23","DOI":"10.1109/CVPR52729.2023.01756","CorpusId":260005035},"title":"ObjectStitch: Object Compositing with Diffusion Model"},{"paperId":"f7a401d767f6b67a5e8528fae656df2f79c60239","externalIds":{"DBLP":"conf/nips/LiuZSZZFLZZC23","ArXiv":"2305.19327","DOI":"10.48550/arXiv.2305.19327","CorpusId":258987908},"title":"Cones 2: Customizable Image Synthesis with Multiple Subjects"},{"paperId":"5728ecb3a11c1586c4ae53e11ab395a0263eb5f4","externalIds":{"ArXiv":"2305.18292","DBLP":"conf/nips/GuWWSCFXZCWGSS23","DOI":"10.48550/arXiv.2305.18292","CorpusId":258960192},"title":"Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models"},{"paperId":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","externalIds":{"DBLP":"conf/nips/YangGYLDH023","ArXiv":"2305.18259","DOI":"10.48550/arXiv.2305.18259","CorpusId":258960586},"title":"GlyphControl: Glyph Conditional Control for Visual Text Generation"},{"paperId":"113793840434696080208b9dd6034944fd6efd61","externalIds":{"DBLP":"journals/corr/abs-2305-17235","ArXiv":"2305.17235","DOI":"10.48550/arXiv.2305.17235","CorpusId":258959444},"title":"COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models"},{"paperId":"7ee8487178a0bb8927e97aff94d338d3b31097fe","externalIds":{"DBLP":"conf/cvpr/XuGWHES24","ArXiv":"2305.16223","DOI":"10.1109/CVPR52733.2024.00829","CorpusId":258887939},"title":"Prompt-Free Diffusion: Taking “Text” Out of Text-to-Image Diffusion Models"},{"paperId":"7657e124622858a970815a96991727884088d648","externalIds":{"ArXiv":"2305.15779","DBLP":"journals/corr/abs-2305-15779","DOI":"10.48550/arXiv.2305.15779","CorpusId":258888143},"title":"Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models"},{"paperId":"ea218a59a36ba38fedad604add9533ab0f8544c6","externalIds":{"ArXiv":"2305.16225","DBLP":"journals/tog/ZhangDTHHMLDX23","DOI":"10.1145/3618342","CorpusId":258887639},"title":"ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models"},{"paperId":"ab4036bf29853d4b6e454184730eb6d26cbc4bc7","externalIds":{"DBLP":"journals/corr/abs-2305-16311","ArXiv":"2305.16311","DOI":"10.1145/3610548.3618154","CorpusId":258888228},"title":"Break-A-Scene: Extracting Multiple Concepts from a Single Image"},{"paperId":"a483ff9557f29d21fe780b3dd969a037a3ffc3ed","externalIds":{"DBLP":"journals/corr/abs-2305-16322","ArXiv":"2305.16322","DOI":"10.48550/arXiv.2305.16322","CorpusId":258888112},"title":"Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models"},{"paperId":"dc0c132b273456b288a785414db2fa72edf87b1a","externalIds":{"DBLP":"journals/corr/abs-2305-14720","ArXiv":"2305.14720","DOI":"10.48550/arXiv.2305.14720","CorpusId":258865473},"title":"BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing"},{"paperId":"d69e2a14768cca5dd3f367e9adbb74b33cb8b609","externalIds":{"ArXiv":"2305.11520","DOI":"10.1109/TIP.2026.3654412","CorpusId":258823368,"PubMed":"41576113"},"title":"LaCon: Late-Constraint Controllable Visual Generation"},{"paperId":"000ddab70b4ff08cafc7977f1816f5325a122b4e","externalIds":{"DBLP":"conf/icdip/NiZ23","DOI":"10.1145/3604078.3604160","CorpusId":264492567},"title":"Natural Image Reconstruction from fMRI Based on Self-supervised Representation Learning and Latent Diffusion Model"},{"paperId":"e779781f1bea273573fc9d3f1a5e874bcff2cd2b","externalIds":{"DBLP":"conf/nips/ChenHL0CW23","ArXiv":"2305.10855","DOI":"10.48550/arXiv.2305.10855","CorpusId":258762187},"title":"TextDiffuser: Diffusion Models as Text Painters"},{"paperId":"f6fdac9b5e771394d22bfd5fbaf8147a52b6e792","externalIds":{"ArXiv":"2305.11147","DBLP":"conf/nips/QinZYFYZWNXSE0X23","DOI":"10.48550/arXiv.2305.11147","CorpusId":258762776},"title":"UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild"},{"paperId":"1a3d6119d9513ad27fa4fc3262e517ec6a6d2261","externalIds":{"DBLP":"journals/ijcv/XiaoYFDH25","ArXiv":"2305.10431","DOI":"10.1007/s11263-024-02227-z","CorpusId":258740710},"title":"FastComposer: Tuning-Free Multi-subject Image Generation with Localized Attention"},{"paperId":"e01ed6611f9c998c237cda814ff8366a5acb6c3d","externalIds":{"ArXiv":"2305.06131","DBLP":"journals/corr/abs-2305-06131","DOI":"10.48550/arXiv.2305.06131","CorpusId":258588157},"title":"Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era"},{"paperId":"314047a5aad780af9efef7ebd4a41e6995666543","externalIds":{"DBLP":"conf/siggraph/TewelGCA23","ArXiv":"2305.01644","DOI":"10.1145/3588432.3591506","CorpusId":258436985},"title":"Key-Locked Rank One Editing for Text-to-Image Personalization"},{"paperId":"57be0448d168e8d6d0b6e0d1a4405fb5fbaa1b56","externalIds":{"DBLP":"journals/corr/abs-2305-01115","ArXiv":"2305.01115","DOI":"10.48550/arXiv.2305.01115","CorpusId":258437037},"title":"In-Context Learning Unlocked for Diffusion Models"},{"paperId":"940da2b97449e3aaa138570cebb7064a53210b11","externalIds":{"ArXiv":"2304.06720","DBLP":"journals/corr/abs-2304-06720","DOI":"10.1109/ICCV51070.2023.00694","CorpusId":258108187},"title":"Expressive Text-to-Image Generation with Rich Text"},{"paperId":"d0590894ef9edb5fce917cb8221e5ab0226522a9","externalIds":{"DBLP":"journals/tmlr/SmithHZHKSJ24","ArXiv":"2304.06027","DOI":"10.48550/arXiv.2304.06027","CorpusId":258078844},"title":"Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA"},{"paperId":"0f2d05007f866f716acdb3136135c39904c6ab19","externalIds":{"DBLP":"journals/corr/abs-2304-05818","ArXiv":"2304.05818","DOI":"10.1145/3581783.3612599","CorpusId":258078921},"title":"Gradient-Free Textual Inversion"},{"paperId":"1ea2140567bbed461c19ff02d0dd193c6709f4da","externalIds":{"DBLP":"conf/iccv/JuZZWZX23","ArXiv":"2304.04269","DOI":"10.1109/ICCV51070.2023.01465","CorpusId":258048904},"title":"HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation"},{"paperId":"9733025aea2ba71792be10c18d635e8fc1455e31","externalIds":{"DBLP":"journals/corr/abs-2304-03411","ArXiv":"2304.03411","DOI":"10.1109/CVPR52733.2024.00816","CorpusId":258041269},"title":"InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning"},{"paperId":"90e4ebaa16df6ead9c34e31d0621aae4e5739ce2","externalIds":{"DBLP":"journals/corr/abs-2304-02642","ArXiv":"2304.02642","DOI":"10.48550/arXiv.2304.02642","CorpusId":257952647},"title":"Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"83b8e18488d8f31dd017ec0b26531cef4b635b36","externalIds":{"DBLP":"conf/nips/ChenHLRJCC23","ArXiv":"2304.00186","DOI":"10.48550/arXiv.2304.00186","CorpusId":257913352},"title":"Subject-driven Text-to-Image Generation via Apprenticeship Learning"},{"paperId":"242580cb5dec8a55207b91035bae70e06a6d289e","externalIds":{"DBLP":"journals/corr/abs-2303-18181","ArXiv":"2303.18181","DOI":"10.48550/arXiv.2303.18181","CorpusId":257901164},"title":"A Closer Look at Parameter-Efficient Tuning in Diffusion Models"},{"paperId":"1fe4628388d9e3bc8d4ab0d4cefc6316739af202","externalIds":{"DBLP":"conf/cvpr/ZhengZLQSL23","ArXiv":"2303.17189","DOI":"10.1109/CVPR52729.2023.02154","CorpusId":257834036},"title":"LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation"},{"paperId":"e6ea175f8ee51d89d41f9f23b0760a6c3e510f85","externalIds":{"DBLP":"journals/corr/abs-2303-14412","ArXiv":"2303.14412","DOI":"10.1109/CVPR52729.2023.01370","CorpusId":257766327},"title":"Freestyle Layout-to-Image Synthesis"},{"paperId":"f81bda850d829460c33844dd01e775c23abdf9f3","externalIds":{"DBLP":"journals/corr/abs-2308-04249","ArXiv":"2308.04249","DOI":"10.1145/3581783.3613832","CorpusId":257757357},"title":"MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion"},{"paperId":"f677fc0f6a7a4f7b324b8f14700e14715cb06bca","externalIds":{"DBLP":"conf/siggrapha/Huang00C024","ArXiv":"2303.13495","DOI":"10.1145/3680528.3687658","CorpusId":257687552},"title":"ReVersion: Diffusion-Based Relation Inversion from Images"},{"paperId":"c2917da5abbf0f0cfb4ce366b180359ff8a9deae","externalIds":{"DBLP":"journals/corr/abs-2303-13508","ArXiv":"2303.13508","DOI":"10.1109/ICCV51070.2023.00223","CorpusId":257687714},"title":"DreamBooth3D: Subject-Driven Text-to-3D Generation"},{"paperId":"6ae34677bc41e1818a899583b25e379dadc42a85","externalIds":{"ArXiv":"2303.11305","DBLP":"conf/iccv/HanLZMMY23","DOI":"10.1109/ICCV51070.2023.00673","CorpusId":257631648},"title":"SVDiff: Compact Parameter Space for Diffusion Fine-Tuning"},{"paperId":"2c70684973bc4d7b6f8404a647b8031c4d3c8383","externalIds":{"ArXiv":"2303.11328","DBLP":"journals/corr/abs-2303-11328","DOI":"10.1109/ICCV51070.2023.00853","CorpusId":257631738},"title":"Zero-1-to-3: Zero-shot One Image to 3D Object"},{"paperId":"a88e005e4a0e268d68cfd0bffc9975687b8ff918","externalIds":{"ArXiv":"2303.10056","DBLP":"journals/corr/abs-2303-10056","DOI":"10.1109/ICCV51070.2023.02110","CorpusId":257623076},"title":"GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation"},{"paperId":"ce06533ecc98ba221a4db427738884c6a6af6eee","externalIds":{"DBLP":"journals/corr/abs-2303-09833","ArXiv":"2303.09833","DOI":"10.1109/ICCV51070.2023.02118","CorpusId":257622962},"title":"FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model"},{"paperId":"d4b0fa75565e873eea165e7059435bb55ddeb9ef","externalIds":{"ArXiv":"2303.09319","DBLP":"journals/corr/abs-2303-09319","DOI":"10.48550/arXiv.2303.09319","CorpusId":257557302},"title":"Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation"},{"paperId":"1d70fb149cdee56c50843891dd17f9f4d5a72ff4","externalIds":{"DBLP":"journals/corr/abs-2303-09522","ArXiv":"2303.09522","DOI":"10.48550/arXiv.2303.09522","CorpusId":257557484},"title":"P+: Extended Textual Conditioning in Text-to-Image Generation"},{"paperId":"35ccd924de9e8483bdcf144cbf2edf09be157b7e","externalIds":{"ArXiv":"2303.07909","DBLP":"journals/corr/abs-2303-07909","DOI":"10.48550/arXiv.2303.07909","CorpusId":257505012},"title":"Text-to-image Diffusion Models in Generative AI: A Survey"},{"paperId":"2824f18b3aeaae51b3fbe0d629c9b8a728da86d7","externalIds":{"DBLP":"conf/cvpr/TakagiN23","DOI":"10.1109/CVPR52729.2023.01389","CorpusId":253762952},"title":"High-resolution image reconstruction with latent diffusion models from human brain activity"},{"paperId":"ba84fa079c2e2e16a255f65367b75ecba2e806b7","externalIds":{"DBLP":"journals/corr/abs-2303-05125","ArXiv":"2303.05125","DOI":"10.48550/arXiv.2303.05125","CorpusId":257427549},"title":"Cones: Concept Neurons in Diffusion Models for Customized Generation"},{"paperId":"1a450fcd40aeea64544fad08844de1119c33f03f","externalIds":{"PubMedCentral":"10511448","ArXiv":"2303.05334","DOI":"10.1038/s41598-023-42891-8","CorpusId":260439960,"PubMed":"37731047"},"title":"Natural scene reconstruction from fMRI signals using generative latent diffusion"},{"paperId":"e15900cf7c93d4b6e45a12fe3534840c910467e1","externalIds":{"DBLP":"journals/corr/abs-2302-13848","ArXiv":"2302.13848","DOI":"10.1109/ICCV51070.2023.01461","CorpusId":257219968},"title":"ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation"},{"paperId":"2f9f467128ffd754482b342c7196e69b9e52eac9","externalIds":{"ArXiv":"2302.12764","DBLP":"conf/siggraph/HamHLSZH23","DOI":"10.1145/3588432.3591549","CorpusId":257205737},"title":"Modulating Pretrained Diffusion Models for Multimodal Image Synthesis"},{"paperId":"26e5b933b8f60bd749d428b5ff813b2abcd765d8","externalIds":{"ArXiv":"2302.09778","DBLP":"conf/icml/HuangC0SZZ23","DOI":"10.48550/arXiv.2302.09778","CorpusId":257038979},"title":"Composer: Creative and Controllable Image Synthesis with Composable Conditions"},{"paperId":"58842cdca3ea68f7b9e638b288fc247a6f26dafc","externalIds":{"DBLP":"conf/aaai/MouWXW0QS24","ArXiv":"2302.08453","DOI":"10.48550/arXiv.2302.08453","CorpusId":256900833},"title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models"},{"paperId":"00b167a24f57f96294a4581bb185a57b0a4af365","externalIds":{"DBLP":"journals/corr/abs-2302-08908","ArXiv":"2302.08908","DOI":"10.48550/arXiv.2302.08908","CorpusId":257019804},"title":"LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation"},{"paperId":"d24b4f34197df0257390b57f02537e6ce3284f2e","externalIds":{"ArXiv":"2302.07121","DBLP":"conf/iclr/BansalCSSGGG24","DOI":"10.1109/CVPRW59228.2023.00091","CorpusId":256846836},"title":"Universal Guidance for Diffusion Models"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"3821c2d78758084cfbdd5071e7d6d31a151c10e8","externalIds":{"ArXiv":"2302.04841","DBLP":"conf/nips/VoronovKBR23","CorpusId":259262648},"title":"Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics"},{"paperId":"c3c7464acb90049c5f520b0732dc7435ba3690bd","externalIds":{"DBLP":"journals/tog/CheferAVWC23","ArXiv":"2301.13826","DOI":"10.1145/3592116","CorpusId":256416326},"title":"Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"259cf9356e90d536aba42c026b1192586a6a6cab","externalIds":{"DBLP":"conf/wacv/CaoYLYH024","ArXiv":"2301.12141","DOI":"10.1109/WACV57701.2024.00419","CorpusId":256390482},"title":"What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion"},{"paperId":"994a1ce6677b496bd3c0c63aceafc6556005e994","externalIds":{"DBLP":"conf/cvpr/LiLWMYGLL23","ArXiv":"2301.07093","DOI":"10.1109/CVPR52729.2023.02156","CorpusId":255942528},"title":"GLIGEN: Open-Set Grounded Text-to-Image Generation"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","externalIds":{"DBLP":"conf/acl/LiuGSCRNBM0C23","ACL":"2023.acl-long.900","ArXiv":"2212.10562","DOI":"10.48550/arXiv.2212.10562","CorpusId":254877579},"title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"25de00096c45121a06668bc501f91adec5d0aff9","externalIds":{"ArXiv":"2212.05032","DBLP":"conf/iclr/FengHFJANBWW23","DOI":"10.48550/arXiv.2212.05032","CorpusId":254535649},"title":"Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis"},{"paperId":"144eca44e250cc462f6fc3a172abb865978f66f5","externalIds":{"DBLP":"conf/cvpr/KumariZ0SZ23","ArXiv":"2212.04488","DOI":"10.1109/CVPR52729.2023.00192","CorpusId":254408780},"title":"Multi-Concept Customization of Text-to-Image Diffusion"},{"paperId":"a6ad30123bef4b19ee40c3d63cfabf00d211f0ef","externalIds":{"DBLP":"conf/cvpr/ZhangHGMR23","ArXiv":"2212.04489","DOI":"10.1109/CVPR52729.2023.00584","CorpusId":254408758},"title":"SINE: SINgle Image Editing with Text-to-Image Diffusion Models"},{"paperId":"55036dea7f6068d6b5de6ffe178bb324d01918a0","externalIds":{"DBLP":"conf/siggraph/VoynovAC23","ArXiv":"2211.13752","DOI":"10.1145/3588432.3591560","CorpusId":254018130},"title":"Sketch-Guided Text-to-Image Diffusion Models"},{"paperId":"97029b53d0252ea68472423dea33e5aa2316926d","externalIds":{"DBLP":"conf/iccv/XuWZWS23","ArXiv":"2211.08332","DOI":"10.1109/ICCV51070.2023.00713","CorpusId":253523371},"title":"Versatile Diffusion: Text, Images and Variations All in One Diffusion Model"},{"paperId":"c668b80885784a2c02b7837978ee95fefd108f1d","externalIds":{"DBLP":"journals/corr/abs-2211-06956","ArXiv":"2211.06956","DOI":"10.1109/CVPR52729.2023.02175","CorpusId":253510456},"title":"Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding"},{"paperId":"e24f4b28167b05fbf7d29000490fc0a4e4c109c7","externalIds":{"ArXiv":"2211.01324","DBLP":"journals/corr/abs-2211-01324","DOI":"10.48550/arXiv.2211.01324","CorpusId":253254800},"title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"85e959eef45114974c8f8643e88af23936fff3d1","externalIds":{"DBLP":"journals/corr/abs-2210-07558","ACL":"2023.eacl-main.239","ArXiv":"2210.07558","DOI":"10.48550/arXiv.2210.07558","CorpusId":252907428},"title":"DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation"},{"paperId":"f170754f8ab3187514292c12b1cbb431c0a8a634","externalIds":{"ArXiv":"2210.09292","DBLP":"journals/corr/abs-2210-09292","DOI":"10.48550/arXiv.2210.09292","CorpusId":252918532},"title":"Efficient Diffusion Models for Vision: A Survey"},{"paperId":"9a610e3110b82e12b9c1794a4437988c98a99769","externalIds":{"DBLP":"journals/corr/abs-2210-00990","ArXiv":"2210.00990","DOI":"10.1109/CVPR52729.2023.01900","CorpusId":252683603},"title":"Visual Prompt Tuning for Generative Transfer Learning"},{"paperId":"7e2ca1df1e92e91c113daf056c0e00d1b22c5f25","externalIds":{"DBLP":"conf/nips/LinSS22","ArXiv":"2210.01769","DOI":"10.48550/arXiv.2210.01769","CorpusId":252693323},"title":"Mind Reader: Reconstructing complex images from brain activities"},{"paperId":"4c94d04afa4309ec2f06bdd0fe3781f91461b362","externalIds":{"DBLP":"conf/iclr/PooleJBM23","ArXiv":"2209.14988","DOI":"10.48550/arXiv.2209.14988","CorpusId":252596091},"title":"DreamFusion: Text-to-3D using 2D Diffusion"},{"paperId":"ec1ac8df419a241c3cc6bfd209a38b494af792ee","externalIds":{"DBLP":"journals/corr/abs-2209-14491","ArXiv":"2209.14491","DOI":"10.48550/arXiv.2209.14491","CorpusId":252596087},"title":"Re-Imagen: Retrieval-Augmented Text-to-Image Generator"},{"paperId":"878af9a76c7251bab973f8885dd106d6ec795f45","externalIds":{"ArXiv":"2209.12746","DBLP":"journals/corr/abs-2209-12746","DOI":"10.48550/arXiv.2209.12746","CorpusId":252531859},"title":"LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space"},{"paperId":"efa1647594b236361610a20d507127f0586a379b","externalIds":{"DBLP":"journals/corr/abs-2209-04747","ArXiv":"2209.04747","DOI":"10.1109/TPAMI.2023.3261988","CorpusId":252199918,"PubMed":"37030794"},"title":"Diffusion Models in Vision: A Survey"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","externalIds":{"DBLP":"journals/corr/abs-2209-00796","ArXiv":"2209.00796","DOI":"10.1145/3626235","CorpusId":252070859},"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744","externalIds":{"ArXiv":"2208.12242","DBLP":"conf/cvpr/RuizLJPRA23","DOI":"10.1109/CVPR52729.2023.02155","CorpusId":251800180},"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"paperId":"17d068e78e6f25e65cb08319b19b58279bb8b214","externalIds":{"DBLP":"journals/corr/abs-2208-11970","ArXiv":"2208.11970","DOI":"10.48550/arXiv.2208.11970","CorpusId":251799923},"title":"Understanding Diffusion Models: A Unified Perspective"},{"paperId":"5406129d9d7d00dc310671c43597101b0ee93629","externalIds":{"ArXiv":"2208.01618","DBLP":"journals/corr/abs-2208-01618","DOI":"10.48550/arXiv.2208.01618","CorpusId":251253049},"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"},{"paperId":"af9f365ed86614c800f082bd8eb14be76072ad16","externalIds":{"DBLP":"journals/corr/abs-2207-12598","ArXiv":"2207.12598","DOI":"10.48550/arXiv.2207.12598","CorpusId":249145348},"title":"Classifier-Free Diffusion Guidance"},{"paperId":"c3978479da5bf611619f6a33feb1611256104ab1","externalIds":{"DBLP":"journals/ieeejas/YangLZS22","DOI":"10.1109/jas.2022.105647","CorpusId":249266996},"title":"Part Decomposition and Refinement Network for Human Parsing"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"7002ae048e4b8c9133a55428441e8066070995cb","externalIds":{"ArXiv":"2112.10741","DBLP":"journals/corr/abs-2112-10741","CorpusId":245335086},"title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"},{"paperId":"da4261a957eaa96bf626e9641ef68ebed1d5333f","externalIds":{"DBLP":"journals/corr/abs-2111-11431","ArXiv":"2111.11431","CorpusId":237262876},"title":"RedCaps: web-curated image-text data created by the people, for the people"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"c1ff08b59f00c44f34dfdde55cd53370733a2c19","externalIds":{"MAG":"3174807077","DBLP":"conf/nips/KarrasALHHLA21","ArXiv":"2106.12423","CorpusId":235606261},"title":"Alias-Free Generative Adversarial Networks"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"d36b3883847fa56acaa2304d0a1ab1f344abd205","externalIds":{"DBLP":"journals/corr/abs-2106-07368","ArXiv":"2106.07368","CorpusId":235422629},"title":"Quality-Aware Network for Face Parsing"},{"paperId":"64ea8f180d0682e6c18d1eb688afdb2027c02794","externalIds":{"ArXiv":"2105.05233","DBLP":"journals/corr/abs-2105-05233","CorpusId":234357997},"title":"Diffusion Models Beat GANs on Image Synthesis"},{"paperId":"2b0c09dd6542438b0ffae62118e1445ee5b074d9","externalIds":{"DBLP":"journals/corr/abs-2103-05997","ArXiv":"2103.05997","DOI":"10.1109/TMM.2022.3217413","CorpusId":232170171},"title":"Quality-Aware Network for Human Parsing"},{"paperId":"98e565fa06f6c7bf7c46833b5106b26dc45130c4","externalIds":{"ArXiv":"2103.01913","DBLP":"conf/sigir/Srinivasan0CBN21","DOI":"10.1145/3404835.3463257","CorpusId":232092726},"title":"WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"e90b5085cef4a61bb56ef035693098e14edcc524","externalIds":{"DOI":"10.1109/ICRCICN50933.2020.9296192","CorpusId":229373783},"title":"Imagenation - A DCGAN based method for Image Reconstruction from fMRI"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"014576b866078524286802b1d0e18628520aa886","externalIds":{"ArXiv":"2010.02502","DBLP":"journals/corr/abs-2010-02502","MAG":"3092442149","CorpusId":222140788},"title":"Denoising Diffusion Implicit Models"},{"paperId":"12d93427182c88e04e64fe074960cf7ce67f9d89","externalIds":{"MAG":"3096538303","ArXiv":"2009.09447","DBLP":"conf/eccv/YangSWHLXJX20","DOI":"10.1007/978-3-030-58610-2_25","CorpusId":221819460},"title":"Renovating Parsing R-CNN for Accurate Multiple Human Parsing"},{"paperId":"14fdc18d9c164e5b0d6d946b3238c04e81921358","externalIds":{"MAG":"3035574324","DBLP":"conf/cvpr/KarrasLAHLA20","ArXiv":"1912.04958","DOI":"10.1109/cvpr42600.2020.00813","CorpusId":209202273},"title":"Analyzing and Improving the Image Quality of StyleGAN"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","externalIds":{"MAG":"2983040767","ArXiv":"1911.02116","ACL":"2020.acl-main.747","DBLP":"conf/acl/ConneauKGCWGGOZ20","DOI":"10.18653/v1/2020.acl-main.747","CorpusId":207880568},"title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"5e19eba1e6644f7c83f607383d256deea71f87ae","externalIds":{"DBLP":"conf/iccv/HowardPALSCWCTC19","ArXiv":"1905.02244","MAG":"2944779197","DOI":"10.1109/ICCV.2019.00140","CorpusId":146808333},"title":"Searching for MobileNetV3"},{"paperId":"e2751a898867ce6687e08a5cc7bdb562e999b841","externalIds":{"ArXiv":"1904.01355","DBLP":"journals/corr/abs-1904-01355","MAG":"2925359305","DOI":"10.1109/ICCV.2019.00972","CorpusId":91184137},"title":"FCOS: Fully Convolutional One-Stage Object Detection"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"9abc8fa5ea961dd2e9951f298e026de9e6453df1","externalIds":{"DBLP":"journals/corr/abs-1811-12596","ArXiv":"1811.12596","MAG":"2902046184","DOI":"10.1109/CVPR.2019.00045","CorpusId":54434679},"title":"Parsing R-CNN for Instance-Level Human Analysis"},{"paperId":"4f17b0a6794c5cb241545ad316ad4d00a2d6c2b6","externalIds":{"MAG":"2897839226","DBLP":"conf/mm/TirupatturRSS18","DOI":"10.1145/3240508.3240641","CorpusId":52083141},"title":"ThoughtViz: Visualizing Human Thoughts Using Generative Adversarial Network"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"744fe47157477235032f7bb3777800f9f2f45e52","externalIds":{"MAG":"2766527293","DBLP":"conf/iclr/KarrasALL18","ArXiv":"1710.10196","CorpusId":3568073},"title":"Progressive Growing of GANs for Improved Quality, Stability, and Variation"},{"paperId":"70c59dc3470ae867016f6ab0e008ac8ba03774a1","externalIds":{"DBLP":"conf/fgr/CaoSXPZ18","MAG":"2766818358","ArXiv":"1710.08092","DOI":"10.1109/FG.2018.00020","CorpusId":216009},"title":"VGGFace2: A Dataset for Recognising Faces across Pose and Age"},{"paperId":"5e7136b89a3598d16e4d1b396f646abd4f6c50c2","externalIds":{"MAG":"2766524960","DBLP":"conf/mm/KavasidisPSGS17","DOI":"10.1145/3123266.3127907","CorpusId":7501660},"title":"Brain2Image: Converting Brain Signals into Images"},{"paperId":"2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d","externalIds":{"MAG":"2737258237","DBLP":"conf/cvpr/ZhouZPFB017","DOI":"10.1109/CVPR.2017.544","CorpusId":5636055},"title":"Scene Parsing through ADE20K Dataset"},{"paperId":"acd87843a451d18b4dc6474ddce1ae946429eaf1","externalIds":{"MAG":"2739748921","DBLP":"conf/icml/ArjovskyCB17","CorpusId":2057420},"title":"Wasserstein Generative Adversarial Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"be0ef77fb0345c5851bb5d297f3ed84ae3c581ee","externalIds":{"MAG":"2940630528","ArXiv":"1703.06868","DBLP":"conf/iccv/HuangB17","DOI":"10.1109/ICCV.2017.167","CorpusId":6576859},"title":"Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization"},{"paperId":"9e60942aa15670ed9ee03af3c0ae011fa4966b7c","externalIds":{"DBLP":"journals/spl/ZhangZLQ16","MAG":"2341528187","ArXiv":"1604.02878","DOI":"10.1109/LSP.2016.2603342","CorpusId":10585115},"title":"Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks"},{"paperId":"b5c26ab8767d046cb6e32d959fdf726aee89bb62","externalIds":{"DBLP":"conf/aaai/SzegedyIVA17","MAG":"2952505871","ArXiv":"1602.07261","DOI":"10.1609/aaai.v31i1.11231","CorpusId":1023605},"title":"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","externalIds":{"MAG":"2953106684","ArXiv":"1506.01497","DBLP":"journals/pami/RenHG017","DOI":"10.1109/TPAMI.2016.2577031","CorpusId":10328909,"PubMed":"27295650"},"title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"5aa26299435bdf7db874ef1640a6c3b5a4a2c394","externalIds":{"DBLP":"journals/corr/SchroffKP15","MAG":"2096733369","ArXiv":"1503.03832","DOI":"10.1109/CVPR.2015.7298682","CorpusId":206592766},"title":"FaceNet: A unified embedding for face recognition and clustering"},{"paperId":"da8d53f9a85b40a695585aa461286e373c6b74d4","externalIds":{"DBLP":"conf/cvpr/AndrilukaPGS14","MAG":"2080873731","DOI":"10.1109/CVPR.2014.471","CorpusId":206592419},"title":"2D Human Pose Estimation: New Benchmark and State of the Art Analysis"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"2a002ce457f7ab3088fbd2691734f1ce79f750c4","externalIds":{"MAG":"2113325037","DBLP":"conf/cvpr/ToshevS14","ArXiv":"1312.4659","DOI":"10.1109/CVPR.2014.214","CorpusId":206592152},"title":"DeepPose: Human Pose Estimation via Deep Neural Networks"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"d21d2a615c7b92f1b1796e0cb357a4529b9209c4","externalIds":{"MAG":"2170565777","DOI":"10.1111/J.2007.0906-7590.05171.X","CorpusId":86160700},"title":"Methods to account for spatial autocorrelation in the analysis of species distributional data : a review"},{"paperId":"79ad463104c7b7afeab11c2046fe7c18d5108ac6","externalIds":{"DOI":"10.1080/10131750485310161","CorpusId":218497666},"title":"Pattern"},{"paperId":"f65020fc3b1692d7989e099d6b6e698be5a50a93","externalIds":{"MAG":"1999874108","DBLP":"conf/icml/PieterN04","DOI":"10.1145/1015330.1015430","CorpusId":207155342},"title":"Apprenticeship learning via inverse reinforcement learning"},{"paperId":"549e553dd7e3284059a8fd6adf971911626e16aa","externalIds":{"DBLP":"journals/corr/abs-2305-15194","DOI":"10.48550/arXiv.2305.15194","CorpusId":258865186},"title":"DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models"},{"paperId":"9da554d6f56938ff1f50491713f2919a534b9f64","externalIds":{"CorpusId":265038204},"title":"DisenBooth: Disentangled Parameter-Efﬁcient Tuning for Subject-Driven Text-to-Image Generation"},{"paperId":"1c7471996a4f2e08a5ef592a6ffcde65a034a1e4","externalIds":{"DBLP":"journals/corr/abs-2303-17870","DOI":"10.48550/arXiv.2303.17870","CorpusId":257900697},"title":"GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently"},{"paperId":"c278f87987d812274b78498d0aeead836a9abf27","externalIds":{"DBLP":"journals/corr/abs-2312-06573","DOI":"10.48550/arXiv.2312.06573","CorpusId":266162697},"title":"ControlNet-XS: Designing an Efficient and Effective Architecture for Controlling Text-to-Image Diffusion Models"},{"paperId":"b013c9eb1284554ae696fba02bd4d7fc599890b6","externalIds":{"DBLP":"journals/corr/abs-2312-00330","DOI":"10.48550/arXiv.2312.00330","CorpusId":265551401},"title":"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter"},{"paperId":"76deae79c597c9f32a4c4a292b3c5de4f9bad2ea","externalIds":{"DBLP":"journals/corr/abs-2302-12228","DOI":"10.48550/arXiv.2302.12228","CorpusId":257102411},"title":"Designing an Encoder for Fast Personalization of Text-to-Image Models"},{"paperId":"0d648623e1e7180edfb2671201ffb4b1db9abb9e","externalIds":{"DBLP":"journals/corr/abs-2310-12274","DOI":"10.48550/arXiv.2310.12274","CorpusId":264305977},"title":"An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning"},{"paperId":"82c09a2f3cc6b9d112f4c2d03ad0cf1f3cf63e04","externalIds":{"DBLP":"journals/corr/abs-2306-04607","DOI":"10.48550/arXiv.2306.04607","CorpusId":271746201},"title":"Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt"},{"paperId":"97665d64e4b92acec6f1740b215481993cb8f9fa","externalIds":{"DBLP":"journals/corr/abs-2211-11337","DOI":"10.48550/arXiv.2211.11337","CorpusId":265094956},"title":"DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"14021336683ff95e02a28662e0d6c19cc1c24325","externalIds":{"CorpusId":236241840},"title":"UvA-DARE (Digital Academic Repository) Deep 3D human pose estimation: A review"},{"paperId":"5410086e3c7cb4cc48f73f75e843b4109befe092","externalIds":{"CorpusId":282134003},"title":"Representing Scenes as Neural Radiance Fields for View Synthesis"}]}