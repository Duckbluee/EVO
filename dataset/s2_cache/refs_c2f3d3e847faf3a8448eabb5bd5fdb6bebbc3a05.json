{"references":[{"paperId":"48eb81867bee27cefb4ed03e6c1dbbf97df8b481","externalIds":{"ArXiv":"2503.08216","DBLP":"journals/corr/abs-2503-08216","DOI":"10.48550/arXiv.2503.08216","CorpusId":276928067},"title":"Attention Hijackers: Detect and Disentangle Attention Hijacking in LVLMs for Hallucination Mitigation"},{"paperId":"2e96ddea02dbc3db2f7484b1501cf63ad1dd6368","externalIds":{"ArXiv":"2503.07772","CorpusId":276929180},"title":"Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs"},{"paperId":"55308dc3466ddb4ff3b12412290b1239a919c344","externalIds":{"DBLP":"journals/corr/abs-2503-06486","ArXiv":"2503.06486","DOI":"10.48550/arXiv.2503.06486","CorpusId":276903670},"title":"PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training"},{"paperId":"cd245068a71294b7ff6feadb27257e59532eff0d","externalIds":{"DBLP":"conf/cvpr/SuoZSWWZ25","ArXiv":"2503.00361","DOI":"10.1109/CVPR52734.2025.02783","CorpusId":276742294},"title":"Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding"},{"paperId":"33b1ab00c015b8791b454d5e3f36afa8c5ee518e","externalIds":{"DBLP":"journals/corr/abs-2502-18536","ArXiv":"2502.18536","DOI":"10.48550/arXiv.2502.18536","CorpusId":276617515},"title":"FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA"},{"paperId":"b2c032f6122321392303dd6b68784be86b5308c6","externalIds":{"DBLP":"conf/iclr/0009WKMSRSMS025","ArXiv":"2502.06130","DOI":"10.48550/arXiv.2502.06130","CorpusId":276250073},"title":"Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models"},{"paperId":"502bea683e65203d966482d268a181c1bff25bcc","externalIds":{"ArXiv":"2502.03628","DBLP":"journals/corr/abs-2502-03628","DOI":"10.48550/arXiv.2502.03628","CorpusId":276161662},"title":"The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering"},{"paperId":"d5f5ceb864b5612c9983e8327328ace4fd175065","externalIds":{"DBLP":"journals/corr/abs-2501-19164","ArXiv":"2501.19164","DOI":"10.48550/arXiv.2501.19164","CorpusId":276079767},"title":"Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs"},{"paperId":"3ef9dbd95a174d4c175fcd603f62b8cc197e8d7e","externalIds":{"DBLP":"journals/corr/abs-2501-17811","ArXiv":"2501.17811","DOI":"10.48550/arXiv.2501.17811","CorpusId":275954151},"title":"Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling"},{"paperId":"dbaf5a90873dc3a2270d276d61bd8d16350da383","externalIds":{"DBLP":"journals/corr/abs-2501-09695","ArXiv":"2501.09695","DOI":"10.1109/CVPR52734.2025.00992","CorpusId":275570496},"title":"Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key"},{"paperId":"8006bc9cc0f00a26a60ea1096ce07845585af1aa","externalIds":{"DBLP":"journals/corr/abs-2501-02699","ArXiv":"2501.02699","DOI":"10.48550/arXiv.2501.02699","CorpusId":275336849},"title":"EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models"},{"paperId":"0b2fb66b02a0922b3d577f0d1487d6adf78adb19","externalIds":{"DBLP":"journals/corr/abs-2501-01926","ArXiv":"2501.01926","DOI":"10.48550/arXiv.2501.01926","CorpusId":275324196},"title":"Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding"},{"paperId":"5fa9bdc8709835d1c5fd1158f5ed72b6a8b1b226","externalIds":{"DBLP":"conf/cvpr/0007ZC0L025","ArXiv":"2412.13817","DOI":"10.1109/CVPR52734.2025.01364","CorpusId":274822473},"title":"Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection"},{"paperId":"41d8d18f8ca9942ab62fab8092cd13757640d720","externalIds":{"ArXiv":"2412.13949","DBLP":"conf/acl/HeZGFHJ0CW25","DOI":"10.48550/arXiv.2412.13949","CorpusId":274822932},"title":"Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence"},{"paperId":"e7eedf906a4e712bc5680c2c0da3a796f361c7a6","externalIds":{"DBLP":"conf/aaai/WuFPWYC25","ArXiv":"2412.11124","DOI":"10.48550/arXiv.2412.11124","CorpusId":274776977},"title":"Combating Multimodal LLM Hallucination via Bottom-Up Holistic Reasoning"},{"paperId":"f19db28a8fdcdc2eaa44c777c40714e2622bf0ff","externalIds":{"ArXiv":"2412.10302","DBLP":"journals/corr/abs-2412-10302","DOI":"10.48550/arXiv.2412.10302","CorpusId":274762981},"title":"DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding"},{"paperId":"2feb4d83da1892db3934fcf406c8beb6cd10ded1","externalIds":{"DBLP":"journals/corr/abs-2411-15839","ArXiv":"2411.15839","DOI":"10.48550/arXiv.2411.15839","CorpusId":274235063},"title":"VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding"},{"paperId":"dd6b124606e3696dcddc93c889a824feaa322117","externalIds":{"DBLP":"conf/acl/FuXSK025","ArXiv":"2411.10436","DOI":"10.48550/arXiv.2411.10436","CorpusId":274116880},"title":"Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization"},{"paperId":"36bfaa765bed53b57f5c7d331e14a65d217de402","externalIds":{"ArXiv":"2411.09968","DBLP":"journals/corr/abs-2411-09968","DOI":"10.48550/arXiv.2411.09968","CorpusId":274116957},"title":"Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs"},{"paperId":"8e3d6656c3f5a1b4c23181240c0c5f9b71890a1f","externalIds":{"DBLP":"journals/corr/abs-2411-12591","ArXiv":"2411.12591","DOI":"10.48550/arXiv.2411.12591","CorpusId":274141418},"title":"Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination"},{"paperId":"73020a07af4cfc42286e299097a0e35d2fe71a6c","externalIds":{"DBLP":"conf/emnlp/XieLXK24","ArXiv":"2411.02712","DOI":"10.18653/v1/2024.findings-emnlp.775","CorpusId":273821696},"title":"V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization"},{"paperId":"48b23225d036b8aa306dfcfa9c35cfc4195e4bd1","externalIds":{"ArXiv":"2410.15926","DBLP":"conf/nips/XingLLL24","DOI":"10.48550/arXiv.2410.15926","CorpusId":273502310},"title":"Mitigating Object Hallucination via Concentric Causal Attention"},{"paperId":"0f974ca56416ddd2ecac470742698707507ae07b","externalIds":{"DBLP":"journals/corr/abs-2410-13321","ArXiv":"2410.13321","DOI":"10.48550/arXiv.2410.13321","CorpusId":273403606},"title":"Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding"},{"paperId":"d2626d990c9b2a2ffc4de76b266b4653e36fffc4","externalIds":{"ArXiv":"2410.11779","DBLP":"journals/corr/abs-2410-11779","DOI":"10.48550/arXiv.2410.11779","CorpusId":273351334},"title":"MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation"},{"paperId":"a354c2b5f567470aea44828a98c0cb6b32fe1a9e","externalIds":{"DBLP":"conf/emnlp/GongMWW24","ACL":"2024.emnlp-main.439","ArXiv":"2410.04514","DOI":"10.48550/arXiv.2410.04514","CorpusId":273186947},"title":"DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination"},{"paperId":"b1879fcb6e881fc6e5a766d6cde00150f36ad854","externalIds":{"DBLP":"conf/iclr/JiangKPG25","ArXiv":"2410.02762","DOI":"10.48550/arXiv.2410.02762","CorpusId":273098177},"title":"Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations"},{"paperId":"61bc39b1da0363759edf82445d5ebe4e3e9f92ba","externalIds":{"DBLP":"conf/emnlp/YuanQXL24","ArXiv":"2409.20429","ACL":"2024.emnlp-main.105","DOI":"10.48550/arXiv.2409.20429","CorpusId":272986953},"title":"HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding"},{"paperId":"4f4f4014979fe35d28444509ad96e8af1effa85d","externalIds":{"DBLP":"conf/nips/Bai0MWGClZS24","ArXiv":"2409.19603","DOI":"10.48550/arXiv.2409.19603","CorpusId":272986987},"title":"One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"},{"paperId":"a22b23b65cdbc0335b872ed7c7a7206c710f8997","externalIds":{"ArXiv":"2409.12191","DBLP":"journals/corr/abs-2409-12191","DOI":"10.48550/arXiv.2409.12191","CorpusId":272704132},"title":"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"},{"paperId":"e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0","externalIds":{"DBLP":"conf/coling/QuS0LDC25","ArXiv":"2408.17150","DOI":"10.48550/arXiv.2408.17150","CorpusId":272310343},"title":"Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning"},{"paperId":"5b3991fe7d8f6fc0a7fbd42938e2988ea37efe14","externalIds":{"DBLP":"journals/corr/abs-2408-11039","ArXiv":"2408.11039","DOI":"10.48550/arXiv.2408.11039","CorpusId":271909855},"title":"Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model"},{"paperId":"325d77cc8f6fe77045c0d6d9eb32fc7019f64e63","externalIds":{"DBLP":"journals/corr/abs-2408-10433","ArXiv":"2408.10433","DOI":"10.48550/arXiv.2408.10433","CorpusId":271909100},"title":"CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs"},{"paperId":"50e9d2ea8536e21381849c740fc31534f67a3b9f","externalIds":{"DBLP":"conf/iclr/BaiX0W0BS25","ArXiv":"2408.07249","CorpusId":271865382},"title":"Bridging Information Asymmetry in Text-video Retrieval: A Data-centric Approach"},{"paperId":"1a71f7b216b710b936da666027014adb83af8e7a","externalIds":{"DBLP":"journals/corr/abs-2408-03326","ArXiv":"2408.03326","DOI":"10.48550/arXiv.2408.03326","CorpusId":271719914},"title":"LLaVA-OneVision: Easy Visual Task Transfer"},{"paperId":"171807aeeb88f0c7983bc6cc960b5605441d7121","externalIds":{"DBLP":"journals/corr/abs-2408-00555","ArXiv":"2408.00555","DOI":"10.1145/3742434","CorpusId":271600627},"title":"Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation"},{"paperId":"882588a4fe6134bb5d474f7e1ca788f3eb7db2e2","externalIds":{"DBLP":"conf/eccv/LiuZC24","ArXiv":"2407.21771","DOI":"10.48550/arXiv.2407.21771","CorpusId":271570970},"title":"Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs"},{"paperId":"4a390773b52b2b64a6e5bf5fd184fa05ba84dbaa","externalIds":{"DBLP":"journals/corr/abs-2407-20505","ArXiv":"2407.20505","DOI":"10.48550/arXiv.2407.20505","CorpusId":271544372},"title":"Interpreting and Mitigating Hallucination in MLLMs through Multi-agent Debate"},{"paperId":"968bd4cf71c66bb153527778836e54c85ee6162c","externalIds":{"DBLP":"conf/acl/ZhongFZL0GMX024","ArXiv":"2407.00569","DOI":"10.48550/arXiv.2407.00569","CorpusId":270870516},"title":"Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models"},{"paperId":"896d85d6310ffc6e4e3d328b54f127e898841bd4","externalIds":{"DBLP":"conf/cvpr/AnTL0LW0ZL25","ArXiv":"2406.12718","DOI":"10.1109/CVPR52734.2025.02784","CorpusId":270562057},"title":"Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention"},{"paperId":"e7e0c4c0bb6847428f8c77ef3cb12129986328ea","externalIds":{"DBLP":"conf/nips/KimKKR24","ArXiv":"2406.01920","DOI":"10.48550/arXiv.2406.01920","CorpusId":270226898},"title":"CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models"},{"paperId":"62ab99c2f4853f3e7e7345fbb700a33d0b373afb","externalIds":{"ArXiv":"2405.18654","CorpusId":270095096},"title":"Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment"},{"paperId":"db646f0eb37bb97fda3a89f94c81e507f9421ba9","externalIds":{"ArXiv":"2405.15356","DBLP":"conf/nips/LyuCGSS24","DOI":"10.48550/arXiv.2405.15356","CorpusId":270045668},"title":"Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization"},{"paperId":"c3e374afdfc6aaf7c2d9ec0c8ead80bd40f4387e","externalIds":{"ArXiv":"2405.14974","DBLP":"journals/corr/abs-2405-14974","DOI":"10.48550/arXiv.2405.14974","CorpusId":270045796},"title":"LOVA3: Learning to Visual Question Answering, Asking and Assessment"},{"paperId":"1a4e522e302e6b35e259ffaaff7e10bd1095398b","externalIds":{"ArXiv":"2404.19752","DBLP":"journals/corr/abs-2404-19752","DOI":"10.1109/CVPR52733.2024.01331","CorpusId":269457182},"title":"Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation"},{"paperId":"7d0d587ca37ee09247886684a220bb1625e77910","externalIds":{"ArXiv":"2404.16375","DBLP":"journals/corr/abs-2404-16375","DOI":"10.48550/arXiv.2404.16375","CorpusId":269362176},"title":"List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs"},{"paperId":"27d55a944b5c02b8c10eb250773d8eb082e06476","externalIds":{"DBLP":"journals/corr/abs-2404-14233","ArXiv":"2404.14233","DOI":"10.48550/arXiv.2404.14233","CorpusId":269293208},"title":"Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback"},{"paperId":"fa84ef486184eb1c3d63949b700342bbcaf7b0c7","externalIds":{"DBLP":"journals/tmlr/JingD25","ArXiv":"2404.05046","DOI":"10.48550/arXiv.2404.05046","CorpusId":269005571},"title":"FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback"},{"paperId":"982bc3b4025e971f95d8ba071f7bb667cc71e7a4","externalIds":{"DBLP":"conf/acl/WangPDB24","ArXiv":"2403.18715","DOI":"10.48550/arXiv.2403.18715","CorpusId":268724017},"title":"Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding"},{"paperId":"222ed42c00e0b6134e5ebcb6dc068a811ef81cb4","externalIds":{"DBLP":"journals/corr/abs-2403-14003","ArXiv":"2403.14003","DOI":"10.1109/CVPR52733.2024.01356","CorpusId":268553740},"title":"Multi-Modal Hallucination Control by Visual Information Grounding"},{"paperId":"7751f6cdec0f4473c1733eec91699744a7d5176f","externalIds":{"DBLP":"journals/corr/abs-2403-00425","ArXiv":"2403.00425","DOI":"10.48550/arXiv.2403.00425","CorpusId":268201673},"title":"HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding"},{"paperId":"8887d4d41320e446ba60aecd4c5e8e8b466288a0","externalIds":{"DBLP":"conf/cvpr/ZhuJCXY025","ArXiv":"2402.18476","DOI":"10.1109/CVPRW67362.2025.00150","CorpusId":268041475},"title":"IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding"},{"paperId":"f8a642fbb51e0b0ae4774781309545d15d6d9b11","externalIds":{"ArXiv":"2402.11411","DBLP":"journals/corr/abs-2402-11411","DOI":"10.48550/arXiv.2402.11411","CorpusId":267750239},"title":"Aligning Modalities in Vision Large Language Models via Preference Fine-tuning"},{"paperId":"bf54792cf01761a2c51ac3410287797fff665cd4","externalIds":{"ArXiv":"2402.09801","DBLP":"conf/emnlp/XingZWACLZD24","ACL":"2024.emnlp-main.67","DOI":"10.48550/arXiv.2402.09801","CorpusId":267681756},"title":"EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models"},{"paperId":"1cfb7fba7194860e8b8818eb5e87e0a8e14e518a","externalIds":{"DBLP":"conf/eccv/YanBCZHL24","ArXiv":"2402.06118","DOI":"10.48550/arXiv.2402.06118","CorpusId":267616939},"title":"ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling"},{"paperId":"ca00f4056f9039d3c1a4c3a113f5ee0527149b66","externalIds":{"ArXiv":"2401.06209","DBLP":"journals/corr/abs-2401-06209","DOI":"10.1109/CVPR52733.2024.00914","CorpusId":266976992},"title":"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","externalIds":{"ArXiv":"2312.14238","DBLP":"journals/corr/abs-2312-14238","DOI":"10.1109/CVPR52733.2024.02283","CorpusId":266521410},"title":"Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"1c9bab7ab072c619133c936b5b85160e5373e638","externalIds":{"ArXiv":"2312.14233","DBLP":"conf/cvpr/JainYS24","DOI":"10.1109/CVPR52733.2024.02644","CorpusId":266521081},"title":"VCoder: Versatile Vision Encoders for Multimodal Large Language Models"},{"paperId":"2b14d9e190022e388476ebb24eb1a84349ca0de4","externalIds":{"DBLP":"journals/corr/abs-2312-10665","ArXiv":"2312.10665","DOI":"10.48550/arXiv.2312.10665","CorpusId":266348439},"title":"Silkie: Preference Distillation for Large Visual Language Models"},{"paperId":"06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f","externalIds":{"DBLP":"conf/mmm/WangHLLL24","ArXiv":"2312.01701","DOI":"10.48550/arXiv.2312.01701","CorpusId":265609837},"title":"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites"},{"paperId":"62aa2c64f3f4a1d357f1f1e6f5ce8cb0bbe9bb6d","externalIds":{"DBLP":"conf/cvpr/VillaASG25","ArXiv":"2312.02219","DOI":"10.1109/CVPRW67362.2025.00054","CorpusId":265659263},"title":"Behind the Magic, MERLIM: Multi-Modal Evaluation Benchmark for Large Image-Language Models"},{"paperId":"0f9a3c5c6a54fca6be2afa0fd5fd34eed96a31e8","externalIds":{"DBLP":"conf/cvpr/YuYZHHCHL0024","ArXiv":"2312.00849","DOI":"10.1109/CVPR52733.2024.01310","CorpusId":265608723},"title":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-Grained Correctional Human Feedback"},{"paperId":"49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c","externalIds":{"DBLP":"journals/corr/abs-2311-17911","ArXiv":"2311.17911","DOI":"10.1109/CVPR52733.2024.01274","CorpusId":265498818},"title":"OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation"},{"paperId":"328eb183007bf4aefbf42437b42a15db375803e3","externalIds":{"DBLP":"conf/cvpr/LengZCLLMB24","ArXiv":"2311.16922","DOI":"10.1109/CVPR52733.2024.01316","CorpusId":265466833},"title":"Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding"},{"paperId":"2baf63dede1a96cae314c4be99bd3cf9f49b148e","externalIds":{"DBLP":"journals/corr/abs-2311-16839","ArXiv":"2311.16839","DOI":"10.48550/arXiv.2311.16839","CorpusId":265466428},"title":"Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization"},{"paperId":"7c261866e9d8ddc42f3c1f0b1c2c882182d47fc9","externalIds":{"DBLP":"journals/corr/abs-2311-16479","ArXiv":"2311.16479","DOI":"10.48550/arXiv.2311.16479","CorpusId":265466729},"title":"Mitigating Hallucination in Visual Language Models with Visual Supervision"},{"paperId":"cf193b5b34178a444cb9bd9f51beb4124b753935","externalIds":{"ArXiv":"2311.13614","DBLP":"journals/corr/abs-2311-13614","DOI":"10.1109/CVPR52733.2024.01230","CorpusId":265445443},"title":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","externalIds":{"DBLP":"journals/corr/abs-2311-13165","ArXiv":"2311.13165","DOI":"10.1109/BigData59044.2023.10386743","CorpusId":265351653},"title":"Multimodal Large Language Models: A Survey"},{"paperId":"18940a4ccd955c72930ee0f8771ff710a9afeef3","externalIds":{"ArXiv":"2311.07397","DBLP":"journals/corr/abs-2311-07397","DOI":"10.48550/arXiv.2311.07397","CorpusId":265149533},"title":"An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation"},{"paperId":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","externalIds":{"ArXiv":"2311.07575","DBLP":"journals/corr/abs-2311-07575","DOI":"10.48550/arXiv.2311.07575","CorpusId":265150267},"title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"},{"paperId":"e066347a8896058f50d0259b91b6ca3c40f52c2d","externalIds":{"ArXiv":"2311.06791","DBLP":"journals/corr/abs-2311-06791","DOI":"10.48550/arXiv.2311.06791","CorpusId":265149700},"title":"InfMLLM: A Unified Framework for Visual-Language Tasks"},{"paperId":"ad13b213681b6f634bc83a264df246e83dd9a9d9","externalIds":{"DBLP":"conf/cvpr/YeXYYHL0Z024","ArXiv":"2311.04257","DOI":"10.1109/CVPR52733.2024.01239","CorpusId":265050943},"title":"mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"},{"paperId":"18a3ec6c7aca5fc6e21455db46b6aaff22bc1a35","externalIds":{"DBLP":"journals/corr/abs-2311-03287","ArXiv":"2311.03287","DOI":"10.48550/arXiv.2311.03287","CorpusId":265033982},"title":"Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"93c525267e93c78309a5b28a3eb0780704125744","externalIds":{"ArXiv":"2310.00754","DBLP":"conf/iclr/ZhouCYZDFBY24","DOI":"10.48550/arXiv.2310.00754","CorpusId":263334335},"title":"Analyzing and Mitigating Object Hallucination in Large Vision-Language Models"},{"paperId":"07d639b011f48615c1154cb6cdbc067bfe331348","externalIds":{"DBLP":"journals/corr/abs-2310-00653","ArXiv":"2310.00653","DOI":"10.48550/arXiv.2310.00653","CorpusId":263333963},"title":"Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"21091f8133ab034baacb92fdb958e14989eb427f","externalIds":{"DBLP":"conf/iclr/DeletangRDCGMGW24","ArXiv":"2309.10668","DOI":"10.48550/arXiv.2309.10668","CorpusId":262054258},"title":"Language Modeling Is Compression"},{"paperId":"a95a90c07b94a126533309f01995af194dc3781f","externalIds":{"DBLP":"conf/iccv/FanBXZHZSSLSBZF23","ArXiv":"2309.09858","DOI":"10.1109/ICCV51070.2023.01264","CorpusId":262045019},"title":"Unsupervised Open-Vocabulary Object Localization in Videos"},{"paperId":"3803d1f291e162bdaa4678a2c5a2bbcf63c050f4","externalIds":{"ArXiv":"2309.07915","DBLP":"conf/iclr/ZhaoCSMA0LWHC24","DOI":"10.48550/arXiv.2309.07915","CorpusId":261823391},"title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning"},{"paperId":"ed5020eeda1fbe8c29b1282d654b34abee22d90f","externalIds":{"DBLP":"conf/iclr/ChuangXLKGH24","ArXiv":"2309.03883","DOI":"10.48550/arXiv.2309.03883","CorpusId":261582463},"title":"DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models"},{"paperId":"54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f","externalIds":{"DBLP":"journals/corr/abs-2309-03905","ArXiv":"2309.03905","DOI":"10.48550/arXiv.2309.03905","CorpusId":261582620},"title":"ImageBind-LLM: Multi-modality Instruction Tuning"},{"paperId":"73814a52609a9ee4c8f1b115e376b6a300ab6a57","externalIds":{"ArXiv":"2309.02301","DBLP":"journals/corr/abs-2309-02301","DOI":"10.48550/arXiv.2309.02301","CorpusId":261557047},"title":"CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning"},{"paperId":"bb1083425517bdac8d9a6438fcf5032543acb20e","externalIds":{"DBLP":"journals/corr/abs-2308-15126","ArXiv":"2308.15126","DOI":"10.48550/arXiv.2308.15126","CorpusId":261276646},"title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models"},{"paperId":"e47d276bad18f441950c8136672ae6864e95323f","externalIds":{"ArXiv":"2308.12714","DBLP":"conf/aaai/WangWHPZZDLLWH24","DOI":"10.48550/arXiv.2308.12714","CorpusId":261100735},"title":"VIGC: Visual Instruction Generation and Correction"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","externalIds":{"ArXiv":"2308.10792","DBLP":"journals/corr/abs-2308-10792","DOI":"10.1145/3777411","CorpusId":261049152},"title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"30cc95639cffca4ffa8c0eafbc502636c0c88fa5","externalIds":{"DBLP":"journals/corr/abs-2308-09936","ArXiv":"2308.09936","DOI":"10.48550/arXiv.2308.09936","CorpusId":261049015},"title":"BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"},{"paperId":"cb712ab2b3bbaef6bff02efa7295ea420b58f654","externalIds":{"DBLP":"conf/iclr/0006PGG0ZCTZZ24","ArXiv":"2308.04152","CorpusId":260704723},"title":"Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"b37b1dc72b1882858f5120f2cd6883134089a6ed","externalIds":{"ArXiv":"2307.06281","DBLP":"journals/corr/abs-2307-06281","DOI":"10.48550/arXiv.2307.06281","CorpusId":259837088},"title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","externalIds":{"ArXiv":"2307.03601","DBLP":"conf/eccv/ZhangSCXSZLCL24","DOI":"10.48550/arXiv.2307.03601","CorpusId":259375716},"title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","externalIds":{"ArXiv":"2307.02469","ACL":"2024.naacl-long.440","DBLP":"conf/naacl/ZengZZXWWZKS24","DOI":"10.48550/arXiv.2307.02469","CorpusId":259342291},"title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"e2a58fd18961c3941102989e3a3d0d27c615e015","externalIds":{"ArXiv":"2306.15195","DBLP":"journals/corr/abs-2306-15195","DOI":"10.48550/arXiv.2306.15195","CorpusId":259262082},"title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","externalIds":{"ArXiv":"2306.13394","DBLP":"journals/corr/abs-2306-13394","DOI":"10.48550/arXiv.2306.13394","CorpusId":259243928},"title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"7a1e71cb1310c4a873e7a4e54d1a6dab0553adce","externalIds":{"ArXiv":"2306.01116","DBLP":"journals/corr/abs-2306-01116","DOI":"10.48550/arXiv.2306.01116","CorpusId":259063761},"title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","externalIds":{"DBLP":"journals/corr/abs-2305-16355","ArXiv":"2305.16355","ACL":"2023.tllm-1.2","DOI":"10.48550/arXiv.2305.16355","CorpusId":258947721},"title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"236c7dafea3df7ecffb5f18ec780d12f2f27d4b0","externalIds":{"DBLP":"conf/nips/HuangBZZZSLLZLF23","ArXiv":"2305.08322","DOI":"10.48550/arXiv.2305.08322","CorpusId":258685666},"title":"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"81e7e82245c2f230eeb8aaaa1a2b2604c143754a","externalIds":{"ArXiv":"2305.04790","DBLP":"journals/corr/abs-2305-04790","DOI":"10.48550/arXiv.2305.04790","CorpusId":258557672},"title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","externalIds":{"DBLP":"journals/pami/LiZCWPCYLL25","ArXiv":"2305.03726","DOI":"10.1109/TPAMI.2025.3571946","CorpusId":258547300,"PubMed":"40392642"},"title":"Otter: A Multi-Modal Model With In-Context Instruction Tuning"},{"paperId":"0046306876ff2d5600699327e52bc29fa5e9ec91","externalIds":{"ArXiv":"2305.01278","DBLP":"conf/nips/Zhang0Y00LC23","DOI":"10.48550/arXiv.2305.01278","CorpusId":258436945},"title":"Transfer Visual Prompt Generator across LLMs"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","externalIds":{"ArXiv":"2304.15010","DBLP":"journals/corr/abs-2304-15010","DOI":"10.48550/arXiv.2304.15010","CorpusId":258418343},"title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","externalIds":{"DBLP":"journals/corr/abs-2304-14178","ArXiv":"2304.14178","DOI":"10.48550/arXiv.2304.14178","CorpusId":258352455},"title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"9e8cb8c91a0acb6e661b58ad724aa758490f2bea","externalIds":{"ArXiv":"2304.03277","DBLP":"journals/corr/abs-2304-03277","CorpusId":257985497},"title":"Instruction Tuning with GPT-4"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"a4a41319d5805a29316f24ed9519f09db77d4c29","externalIds":{"ArXiv":"2301.13848","DBLP":"journals/tacl/ZhangLDLMH24a","ACL":"2024.tacl-1.3","DOI":"10.1162/tacl_a_00632","CorpusId":256416014},"title":"Benchmarking Large Language Models for News Summarization"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","externalIds":{"DBLP":"journals/corr/abs-2212-09597","ArXiv":"2212.09597","ACL":"2023.acl-long.294","DOI":"10.48550/arXiv.2212.09597","CorpusId":254854219},"title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"26590b0c0e22b8c06c31ad51eda4fbab00a85e80","externalIds":{"DBLP":"conf/cvpr/MaHGGGK23","ArXiv":"2212.07796","DOI":"10.1109/CVPR52729.2023.01050","CorpusId":254685851},"title":"@ CREPE: Can Vision-Language Foundation Models Reason Compositionally?"},{"paperId":"88b62496cbc52072bfa8f4b29d172b0477b701bc","externalIds":{"DBLP":"conf/acl/LiHFLEHZL23","ArXiv":"2210.15097","ACL":"2023.acl-long.687","DOI":"10.48550/arXiv.2210.15097","CorpusId":253157949},"title":"Contrastive Decoding: Open-ended Text Generation as Optimization"},{"paperId":"91fb2254c5942048425e642c8a6c8d400006150e","externalIds":{"DBLP":"journals/corr/abs-2210-01504","ACL":"2023.acl-long.805","ArXiv":"2210.01504","DOI":"10.48550/arXiv.2210.01504","CorpusId":252693065},"title":"Knowledge Unlearning for Mitigating Privacy Risks in Language Models"},{"paperId":"10667c1ae4b49808772b5a377c5b52196701267f","externalIds":{"DBLP":"conf/iclr/Yuksekgonul0KJ023","ArXiv":"2210.01936","DOI":"10.48550/arXiv.2210.01936","CorpusId":252734947},"title":"When and why vision-language models behave like bags-of-words, and what to do about it?"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","externalIds":{"DBLP":"journals/corr/abs-2205-11916","ArXiv":"2205.11916","CorpusId":249017743},"title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","externalIds":{"DBLP":"journals/corr/abs-2205-01917","ArXiv":"2205.01917","DOI":"10.48550/arXiv.2205.01917","CorpusId":248512473},"title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"50796b0f3edf9cb5ff1e447c298b33755378aa4f","externalIds":{"DBLP":"conf/acl/DuQLDQY022","ACL":"2022.acl-long.26","ArXiv":"2103.10360","DOI":"10.18653/v1/2022.acl-long.26","CorpusId":247519241},"title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","externalIds":{"ArXiv":"2102.05918","DBLP":"conf/icml/JiaYXCPPLSLD21","CorpusId":231879586},"title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","externalIds":{"MAG":"3082115681","DBLP":"journals/corr/abs-2009-01325","ArXiv":"2009.01325","CorpusId":221665105},"title":"Learning to summarize from human feedback"},{"paperId":"659bf9ce7175e1ec266ff54359e2bd76e0b7ff31","externalIds":{"DBLP":"conf/nips/LewisPPPKGKLYR020","MAG":"3027879771","ArXiv":"2005.11401","CorpusId":218869575},"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"85079d64d6fdd0ba5318fda119d152f2d2946391","externalIds":{"DBLP":"journals/corr/abs-2001-05876","MAG":"2998988444","ArXiv":"2001.05876","DOI":"10.1609/AAAI.V34I07.6898","CorpusId":210702035},"title":"Show, Recall, and Tell: Image Captioning with Recall Mechanism"},{"paperId":"c5ff974a69fd0c760b4855b819e61e89f31cfffe","externalIds":{"MAG":"2983943451","DBLP":"conf/iccv/0005LZPYZLS19","DOI":"10.1109/ICCV.2019.00852","CorpusId":207967883},"title":"Objects365: A Large-Scale, High-Quality Dataset for Object Detection"},{"paperId":"1097cf8cf5961589ff693b069002e7181e24e631","externalIds":{"DBLP":"conf/icdar/0001SSC19","MAG":"3004268082","DOI":"10.1109/ICDAR.2019.00156","CorpusId":209413409},"title":"OCR-VQA: Visual Question Answering by Reading Text in Images"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"4921243268c81d0d6db99053a9d004852225a622","externalIds":{"MAG":"2962735233","DBLP":"conf/emnlp/RohrbachHBDS18","ArXiv":"1809.02156","ACL":"D18-1437","DOI":"10.18653/v1/D18-1437","CorpusId":52176506},"title":"Object Hallucination in Image Captioning"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"915b5b12f9bdebc321e970ecd713458c3479d70e","externalIds":{"MAG":"2952211726","DBLP":"journals/corr/KafleK17","ArXiv":"1703.09684","DOI":"10.1109/ICCV.2017.217","CorpusId":18050299},"title":"An Analysis of Visual Question Answering Algorithms"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"88512be44744615f4baa8e14f600f036db4c2433","externalIds":{"MAG":"2507296351","DBLP":"journals/corr/ZhouZPFBT16","ArXiv":"1608.05442","DOI":"10.1007/s11263-018-1140-0","CorpusId":11371972},"title":"Semantic Understanding of Scenes Through the ADE20K Dataset"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2f2ade8c4944a96a44e6f70ef403b80b058d1725","externalIds":{"DBLP":"conf/sp/CaoY15","MAG":"1488996941","DOI":"10.1109/SP.2015.35","CorpusId":5945696},"title":"Towards Making Systems Forget with Machine Unlearning"},{"paperId":"7ffdbc358b63378f07311e883dddacc9faeeaf4b","externalIds":{"ArXiv":"1504.08083","CorpusId":206770307},"title":"Fast R-CNN"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"843959ffdccf31c6694d135fad07425924f785b1","externalIds":{"MAG":"2025768430","DBLP":"conf/icml/VincentLBM08","DOI":"10.1145/1390156.1390294","CorpusId":207168299},"title":"Extracting and composing robust features with denoising autoencoders"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"7d47ee5f84103529f84297c98c21dadb4742e3ff","externalIds":{"MAG":"2013784666","DOI":"10.1093/BIOMET/39.3-4.324","CorpusId":121987403},"title":"RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"},{"paperId":"049294f95695a6255b7c1c1b33cd01a75f4f5fdf","externalIds":{"DBLP":"journals/corr/abs-2501-06553","DOI":"10.48550/arXiv.2501.06553","CorpusId":277244598},"title":"VASparse: Towards Efficient Visual Hallucination Mitigation for Large Vision-Language Model via Visual-Aware Sparsification"},{"paperId":"a92de3c944ced3b43c7c19b586e294bd8649a77f","externalIds":{"DBLP":"conf/iclr/LiuYZ25","CorpusId":278499463},"title":"Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering"},{"paperId":"a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824","externalIds":{"DBLP":"journals/corr/abs-2402-08680","DOI":"10.48550/arXiv.2402.08680","CorpusId":267636873},"title":"Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance"},{"paperId":"0370f0e8459bc687c6adaaff2e34de35bb480d81","externalIds":{"DBLP":"journals/corr/abs-2402-03757","DOI":"10.48550/arXiv.2402.03757","CorpusId":267500239},"title":"The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs"},{"paperId":"8c8527f7615d53cbc21b9c3536486540f1c75000","externalIds":{"DBLP":"journals/corr/abs-2401-17981","DOI":"10.48550/arXiv.2401.17981","CorpusId":275689146},"title":"Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study"},{"paperId":"6c1fa4ad807820ad0361a6db424ff757e3b79a6d","externalIds":{"DBLP":"journals/corr/abs-2405-17821","DOI":"10.48550/arXiv.2405.17821","CorpusId":270068142},"title":"RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","externalIds":{"DBLP":"journals/corr/abs-2308-12966","DOI":"10.48550/arXiv.2308.12966","CorpusId":263875678},"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"65d5728ea17f016382870aa27aac1e78d590b50c","externalIds":{"DBLP":"journals/corr/abs-2310-14566","DOI":"10.48550/arXiv.2310.14566","CorpusId":264426657},"title":"HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models"},{"paperId":"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","externalIds":{"CorpusId":264172201},"title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"},{"paperId":"b7e207b4f8e7285d60558ce0eeeb072c3fed2034","externalIds":{"DBLP":"journals/corr/abs-2305-12798","DOI":"10.48550/arXiv.2305.12798","CorpusId":278899824},"title":"LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space"},{"paperId":"6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce","externalIds":{"MAG":"2525491769","DOI":"10.1201/b18055-8","CorpusId":208945385},"title":"google,我,萨娜"},{"paperId":"cfee1826dd4743eab44c6e27a0cc5970effa4d80","externalIds":{"CorpusId":264403242},"title":"Improving Image Generation with Better Captions"}]}