{"references":[{"paperId":"0c4f46e4dcae5527018e6432fb60cfe8c3354e97","externalIds":{"DBLP":"journals/corr/abs-2312-14125","ArXiv":"2312.14125","DOI":"10.48550/arXiv.2312.14125","CorpusId":266435847},"title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation"},{"paperId":"a135798ae5435537062674ba4af96190d216d815","externalIds":{"DBLP":"journals/corr/abs-2311-08245","ArXiv":"2311.08245","DOI":"10.48550/arXiv.2311.08245","CorpusId":265158131},"title":"TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity Recognition"},{"paperId":"45a476cb04cccee74b9ddabce4d58d928be99f7d","externalIds":{"ArXiv":"2310.19736","DBLP":"journals/corr/abs-2310-19736","DOI":"10.48550/arXiv.2310.19736","CorpusId":264825354},"title":"Evaluating Large Language Models: A Comprehensive Survey"},{"paperId":"985f0c89c5a607742ec43c1fdc2cbfe54541cbad","externalIds":{"ArXiv":"2310.05737","CorpusId":263830733},"title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation"},{"paperId":"5548bc789717316089a9b75dfd3a4069ffb7fa11","externalIds":{"DBLP":"conf/iccvw/MaZFFBWHLS23","DOI":"10.1109/ICCVW60793.2023.00297","CorpusId":266462144},"title":"LLaViLo: Boosting Video Moment Retrieval via Adapter-Based Multimodal Modeling"},{"paperId":"f4c2b23fc463867f6c54e82fac0ccbb6617c5324","externalIds":{"DOI":"10.1016/j.birob.2023.100131","CorpusId":264564300},"title":"Large language models for human-robot interaction: A review"},{"paperId":"70192d260e19943e04ec7b17d49f803954fd4363","externalIds":{"ArXiv":"2309.15785","DBLP":"conf/cvpr/LiuLGLS024","DOI":"10.1109/CVPR52733.2024.01296","CorpusId":265871674},"title":"BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"10603f24428220cb3b78b0c23fb7e24cbee71f95","externalIds":{"ArXiv":"2308.07749","DBLP":"journals/corr/abs-2308-07749","DOI":"10.48550/arXiv.2308.07749","CorpusId":260900206},"title":"Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","externalIds":{"DBLP":"journals/corr/abs-2308-00692","ArXiv":"2308.00692","DOI":"10.1109/CVPR52733.2024.00915","CorpusId":260351258},"title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"38939304bb760473141c2aca0305e44fbe04e6e8","externalIds":{"ArXiv":"2307.15818","DBLP":"conf/corl/ZitkovichYXXXXW23","DOI":"10.48550/arXiv.2307.15818","CorpusId":260293142},"title":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"},{"paperId":"71f619edf263408ea2a339189858a590b8a4efb1","externalIds":{"DBLP":"journals/corr/abs-2307-12574","ArXiv":"2307.12574","DOI":"10.1109/ICCV51070.2023.01076","CorpusId":260125438},"title":"A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation"},{"paperId":"888728745dbb769e29ed475d4f7661eebe1a71cf","externalIds":{"DBLP":"journals/tist/ChangWWWYZCYWWYZCYYX24","ArXiv":"2307.03109","DOI":"10.1145/3641289","CorpusId":259360395},"title":"A Survey on Evaluation of Large Language Models"},{"paperId":"d612bdfefe3340bfa13fd814d51a6b8992df5b1d","externalIds":{"PubMedCentral":"10386633","DBLP":"journals/sensors/SurekSSMC23","DOI":"10.3390/s23146384","CorpusId":259909409,"PubMed":"37514677"},"title":"Video-Based Human Activity Recognition Using Deep Learning Approaches"},{"paperId":"1a61b1f46ef6afb58ee2ecb689c757f8ad2f84d0","externalIds":{"DBLP":"conf/cvpr/WangLL0LYZ0W24","ArXiv":"2307.00040","DOI":"10.1109/CVPR52733.2024.00891","CorpusId":259316865},"title":"Disco: Disentangled Control for Realistic Human Dance Generation"},{"paperId":"4b7abf858d96f2bba86acdac8b2b024e8ce0f7a4","externalIds":{"DBLP":"journals/corr/abs-2306-16017","ArXiv":"2306.16017","DOI":"10.1145/3594739.3610741","CorpusId":259274931},"title":"Toward Pioneering Sensors and Features Using Large Language Models in Human Activity Recognition"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","externalIds":{"DBLP":"journals/corr/abs-2306-07207","ArXiv":"2306.07207","DOI":"10.1145/3796716","CorpusId":259138706},"title":"Valley: Video Assistant with Large Language Model Enhanced Ability"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","externalIds":{"ArXiv":"2306.05424","DBLP":"journals/corr/abs-2306-05424","DOI":"10.48550/arXiv.2306.05424","CorpusId":259108333},"title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"f9bfc6d9ba1665b73af3323d46c7642b852759ef","externalIds":{"DBLP":"journals/corr/abs-2305-13292","ArXiv":"2305.13292","DOI":"10.48550/arXiv.2305.13292","CorpusId":258832930},"title":"VideoLLM: Modeling Video Sequence with Large Language Models"},{"paperId":"385d3b5ba6faa8ad360df0c79da04985c7968e00","externalIds":{"ArXiv":"2305.11033","DBLP":"journals/corr/abs-2305-11033","DOI":"10.48550/arXiv.2305.11033","CorpusId":258762759},"title":"Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","externalIds":{"DBLP":"journals/corr/abs-2305-06355","ArXiv":"2305.06355","DOI":"10.1007/s11432-024-4321-9","CorpusId":258588306},"title":"VideoChat: chat-centric video understanding"},{"paperId":"373e83711020a7de53acc13c04f2a073063fe173","externalIds":{"ArXiv":"2311.06329","DBLP":"journals/corr/abs-2311-06329","DOI":"10.1109/AIRC57904.2023.10303174","CorpusId":264977095},"title":"A Survey of AI Text-to-Image and AI Text-to-Video Generators"},{"paperId":"1f8de70416ba92a7438c137fd09c387e3d424847","externalIds":{"DBLP":"journals/corr/abs-2306-03597","ArXiv":"2306.03597","DOI":"10.1016/j.cviu.2023.103741","CorpusId":258996461},"title":"Human-Object Interaction Prediction in Videos through Gaze Following"},{"paperId":"f5a0c57f90c6abe31482e9f320ccac5ee789b135","externalIds":{"ArXiv":"2304.08818","DBLP":"journals/corr/abs-2304-08818","DOI":"10.1109/CVPR52729.2023.02161","CorpusId":258187553},"title":"Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"},{"paperId":"3d4004812be9ca50bbb072097136edc43bf59f23","externalIds":{"DBLP":"journals/air/RafiqRC23","DOI":"10.1007/s10462-023-10414-6","CorpusId":258085720},"title":"Video description: A comprehensive survey of deep learning approaches"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"0810afe92ec3b0b31183648aae2b4dc77dea734f","externalIds":{"DBLP":"conf/iccv/Li00WHWQ23","ArXiv":"2303.16058","DOI":"10.1109/ICCV51070.2023.01826","CorpusId":257771777},"title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models"},{"paperId":"b0a6e21f64dcf46683f34c6ab1251298556692ae","externalIds":{"DOI":"10.1109/ICSCDS56580.2023.10104870","CorpusId":258336155},"title":"Visual Questions Answering Developments, Applications, Datasets and Opportunities: A State-of-the-Art Survey"},{"paperId":"923a03032014a12c4e8b26511c0394e1b915fe74","externalIds":{"DBLP":"conf/iccv/KhachatryanMTHW23","ArXiv":"2303.13439","DOI":"10.1109/ICCV51070.2023.01462","CorpusId":257687280},"title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"},{"paperId":"32a3c2fbd3e733bd0eea938517fec2ff8dc7c701","externalIds":{"DBLP":"conf/iccv/CeylanHM23","ArXiv":"2303.12688","DOI":"10.1109/ICCV51070.2023.02121","CorpusId":257663916},"title":"Pix2Video: Video Editing using Image Diffusion"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"0938d0ccc1c633fa0f8c067d914358b1ef53a44b","externalIds":{"ArXiv":"2302.14115","DBLP":"conf/cvpr/YangNSMPLSS23","DOI":"10.1109/CVPR52729.2023.01032","CorpusId":257232853},"title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning"},{"paperId":"0bbc24c1afde36b0ca508ef097688f176c9c4fbb","externalIds":{"DBLP":"journals/sensors/MorshedSAL23","PubMedCentral":"9963970","DOI":"10.3390/s23042182","CorpusId":256936214,"PubMed":"36850778"},"title":"Human Action Recognition: A Taxonomy-Based Survey, Updates, and Opportunities"},{"paperId":"bb876cb814fe0e14ead87ca0cd651f3c7c1153b1","externalIds":{"ArXiv":"2301.00182","DBLP":"journals/corr/abs-2301-00182","DOI":"10.1109/CVPR52729.2023.00640","CorpusId":255372986},"title":"Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models"},{"paperId":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","externalIds":{"DBLP":"conf/iccv/WuGWLGSHSQS23","ArXiv":"2212.11565","DOI":"10.1109/ICCV51070.2023.00701","CorpusId":254974187},"title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"},{"paperId":"70feb009bc1e8b1cb8dff64bf9fd67789636438b","externalIds":{"DBLP":"conf/cvpr/Guo0LT0TH23","ArXiv":"2212.10846","DOI":"10.1109/CVPR52729.2023.01046","CorpusId":261081574},"title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","externalIds":{"ArXiv":"2212.10403","DBLP":"conf/acl/0009C23","DOI":"10.48550/arXiv.2212.10403","CorpusId":254877753},"title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"af727a67edbec474f55d298331a913ac2805419c","externalIds":{"ArXiv":"2212.05221","DBLP":"journals/corr/abs-2212-05221","DOI":"10.1109/CVPR52729.2023.02238","CorpusId":254564204},"title":"Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory"},{"paperId":"933b37b21e9d61139660088adb032ff3fdf56d86","externalIds":{"DBLP":"conf/cvpr/0006MKG23","ArXiv":"2212.04501","DOI":"10.1109/CVPR52729.2023.00637","CorpusId":254408789},"title":"Learning Video Representations from Large Language Models"},{"paperId":"8240048df28571e36e12aaab9f0ce249d4e7cd37","externalIds":{"ArXiv":"2212.02802","DBLP":"conf/cvpr/KimSKCKY23","DOI":"10.1109/CVPR52729.2023.00590","CorpusId":254275011},"title":"Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding"},{"paperId":"a7019f9a2ecb9983b66456ae32978e2574625980","externalIds":{"DBLP":"journals/corr/abs-2211-12194","ArXiv":"2211.12194","DOI":"10.1109/CVPR52729.2023.00836","CorpusId":253761522},"title":"SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation"},{"paperId":"1dc15f09fd07dd6a752027c664c4d8c9a668bd1b","externalIds":{"DBLP":"conf/icip/VasluianuT22","DOI":"10.1109/ICIP46576.2022.9897883","CorpusId":253335027},"title":"Efficient Video Enhancement Transformer"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"5845da273338376979b39904bc4f38deaa28ed48","externalIds":{"DBLP":"conf/nof/AzminAS22","DOI":"10.1109/NoF55974.2022.9942521","CorpusId":253114911},"title":"Bandwidth Prediction in 5G Mobile Networks Using Informer"},{"paperId":"10667c1ae4b49808772b5a377c5b52196701267f","externalIds":{"DBLP":"conf/iclr/Yuksekgonul0KJ023","ArXiv":"2210.01936","DOI":"10.48550/arXiv.2210.01936","CorpusId":252734947},"title":"When and why vision-language models behave like bags-of-words, and what to do about it?"},{"paperId":"1e33716e8820b867d5a8aaebab44c2d3135ea4ac","externalIds":{"DBLP":"conf/iclr/SingerPH00ZHYAG23","ArXiv":"2209.14792","CorpusId":252595919},"title":"Make-A-Video: Text-to-Video Generation without Text-Video Data"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","externalIds":{"DBLP":"journals/corr/abs-2209-11302","ArXiv":"2209.11302","DOI":"10.1109/ICRA48891.2023.10161317","CorpusId":252519594},"title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"761a953493e7fb9dd3a6176f3365612fbc26b531","externalIds":{"ArXiv":"2209.03355","DBLP":"journals/eaai/AngaranoSMC23","DOI":"10.48550/arXiv.2209.03355","CorpusId":252118828},"title":"Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation"},{"paperId":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744","externalIds":{"ArXiv":"2208.12242","DBLP":"conf/cvpr/RuizLJPRA23","DOI":"10.1109/CVPR52729.2023.02155","CorpusId":251800180},"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"paperId":"a930064dbb82dcf5bd91171c1fd93fc22c339727","externalIds":{"DBLP":"journals/corr/abs-2207-09814","ArXiv":"2207.09814","DOI":"10.48550/arXiv.2207.09814","CorpusId":250698723},"title":"NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","externalIds":{"ArXiv":"2207.04429","DBLP":"conf/corl/ShahOIL22","DOI":"10.48550/arXiv.2207.04429","CorpusId":250426345},"title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"2b02227aa9c1edcf903cd5c897fd539f82406d62","externalIds":{"ArXiv":"2206.07307","DBLP":"journals/corr/abs-2206-07307","DOI":"10.48550/arXiv.2206.07307","CorpusId":249674654},"title":"VCT: A Video Compression Transformer"},{"paperId":"4175003efc9cbf2781c0fd8ef8e6bcb756316296","externalIds":{"ArXiv":"2206.07696","DBLP":"journals/corr/abs-2206-07696","DOI":"10.48550/arXiv.2206.07696","CorpusId":249674747},"title":"Diffusion Models for Video Prediction and Infilling"},{"paperId":"b3d0e66c56fd748d8dbd03366a44c0fda473667e","externalIds":{"DBLP":"journals/corr/abs-2206-01038","ArXiv":"2206.01038","DOI":"10.1109/TMM.2022.3232034","CorpusId":249282213},"title":"A Survey on Video Action Recognition in Sports: Datasets, Methods and Applications"},{"paperId":"707bd332d2c21dc5eb1f02a52d4a0506199aae76","externalIds":{"ArXiv":"2205.15868","DBLP":"journals/corr/abs-2205-15868","DOI":"10.48550/arXiv.2205.15868","CorpusId":249209614},"title":"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers"},{"paperId":"805748eec6be59ae0cd92a48400a902b3b7ed8e6","externalIds":{"DBLP":"conf/nips/HarveyNMWW22","ArXiv":"2205.11495","DOI":"10.48550/arXiv.2205.11495","CorpusId":248986725},"title":"Flexible Diffusion Modeling of Long Videos"},{"paperId":"518b4636fc7edf0aeee1131b4a9054cfb51593b5","externalIds":{"DBLP":"journals/corr/abs-2205-10468","ArXiv":"2205.10468","DOI":"10.48550/arXiv.2205.10468","CorpusId":248986400},"title":"Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"},{"paperId":"3890d82362d07064687a4b5e9024fc4c92921998","externalIds":{"DBLP":"journals/corr/abs-2205-09853","ArXiv":"2205.09853","DOI":"10.48550/arXiv.2205.09853","CorpusId":248965384},"title":"MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation"},{"paperId":"1929bf84fb9896a4cb41d4f6b0114d422f4b92c9","externalIds":{"DBLP":"conf/cvpr/Liu0FQ22","ArXiv":"2204.04216","DOI":"10.1109/CVPR52688.2022.00560","CorpusId":248085533},"title":"Learning Trajectory-Aware Transformer for Video Super-Resolution"},{"paperId":"12e16ffe014d4b31b4a8e5b161ac004bcf74da0d","externalIds":{"DBLP":"journals/air/GaoZYSDH23","DOI":"10.1007/s10462-022-10176-7","CorpusId":248062644},"title":"Deep learning for video object segmentation: a review"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","externalIds":{"DBLP":"conf/nips/HoSGC0F22","ArXiv":"2204.03458","DOI":"10.48550/arXiv.2204.03458","CorpusId":248006185},"title":"Video Diffusion Models"},{"paperId":"c0e8812789e96f5a7aa3ad940dba1c237aec822d","externalIds":{"ArXiv":"2204.02491","DBLP":"journals/corr/abs-2204-02491","DOI":"10.1007/978-3-031-19784-0_41","CorpusId":247996703},"title":"Text2LIVE: Text-Driven Layered Image and Video Editing"},{"paperId":"859893aadb0d30d38b6f856392056188c18d0c78","externalIds":{"ArXiv":"2203.14186","DBLP":"conf/cvpr/GengLDZ22","DOI":"10.1109/CVPR52688.2022.01692","CorpusId":247762014},"title":"RSTT: Real-time Spatial Temporal Transformer for Space-Time Video Super-Resolution"},{"paperId":"fe726a04e2d6e400972671bcd84e5143cd4720e8","externalIds":{"DBLP":"journals/tomccap/LiHZLL23","DOI":"10.1145/3511603","CorpusId":247386119},"title":"Spherical Convolution Empowered Viewport Prediction in 360 Video Multicast with Limited FoV Feedback"},{"paperId":"5231ad92b7dcbb41919bed94e82bb090d5fa51cc","externalIds":{"DBLP":"journals/ijon/HuDLPLD22","DOI":"10.1016/j.neucom.2022.03.069","CorpusId":247734648},"title":"Online human action detection and anticipation in videos: A survey"},{"paperId":"ef4741b12cb8f01bcc68708c8cffcdc4237383f7","externalIds":{"ArXiv":"2202.10571","DBLP":"conf/iclr/YuTMKK0S22","CorpusId":247025714},"title":"Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks"},{"paperId":"cd8691beda82e06083fecae1e78009bd40a5a80e","externalIds":{"DBLP":"journals/tomccap/DammeVT22","DOI":"10.1145/3474833","CorpusId":246928839},"title":"Machine Learning Based Content-Agnostic Viewport Prediction for 360-Degree Video"},{"paperId":"d97e0adbade91d76b10e8790205a71877a9be42b","externalIds":{"DBLP":"journals/corr/abs-2112-14683","ArXiv":"2112.14683","DOI":"10.1109/CVPR52688.2022.00361","CorpusId":245537141},"title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2"},{"paperId":"50fc0b16e47d356017c3d67c77d1ddb1002bfc91","externalIds":{"DBLP":"journals/corr/abs-2112-09401","ArXiv":"2112.09401","DOI":"10.1145/3588764","CorpusId":245329411},"title":"AI-Empowered Persuasive Video Generation: A Survey"},{"paperId":"cbdd3e2fcfd85d546f3ab18f644434097baa7590","externalIds":{"ArXiv":"2111.13196","DBLP":"conf/cvpr/LinLL0G0LW22","DOI":"10.1109/CVPR52688.2022.01742","CorpusId":244709307},"title":"SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning"},{"paperId":"0ffb3f53a4988a67070a8123249718f9fe31ade6","externalIds":{"DBLP":"conf/mswim/GuoZ21","DOI":"10.1145/3479239.3485701","CorpusId":244392466},"title":"A Video-Quality Driven Strategy in Short Video Streaming"},{"paperId":"acb0aab03b3be24d21c636be1f9f9444b0879da5","externalIds":{"DBLP":"conf/mmsp/ChaoOS21","DOI":"10.1109/MMSP53017.2021.9733647","CorpusId":237301232},"title":"Transformer-based Long-Term Viewport Prediction in 360° Video: Scanpath is All You Need"},{"paperId":"7806ad7885d732040cb1fbf23857bba5b6779edd","externalIds":{"DBLP":"journals/tog/KastenOWD21","ArXiv":"2109.11418","DOI":"10.1145/3478513.3480546","CorpusId":237605410},"title":"Layered neural atlases for consistent video editing"},{"paperId":"4e0709e83f62f7011ec98dd9182aa3cde81aa66d","externalIds":{"DBLP":"journals/corr/abs-2109-08029","ArXiv":"2109.08029","DOI":"10.1016/j.eswa.2022.118669","CorpusId":237532531},"title":"Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering"},{"paperId":"90d865e8e2f32c7f1d557fbf8e77ac30cc93132f","externalIds":{"DBLP":"journals/network/LiuLCWILJ21","MAG":"3096564590","DOI":"10.36227/techrxiv.13138940","CorpusId":238833490},"title":"Point Cloud Video Streaming: Challenges and Solutions"},{"paperId":"ddb76d742375ef5187f42bfc0f7f4b237e35e879","externalIds":{"DBLP":"conf/wowmom/KattadigeMCJWJT21","DOI":"10.1109/WoWMoM51794.2021.00034","CorpusId":235780573},"title":"VideoTrain: A Generative Adversarial Framework for Synthetic Video Traffic Generation"},{"paperId":"9c7aad6a82d2aac1395164ed3147fa4407a85acd","externalIds":{"ArXiv":"2105.14424","DBLP":"journals/corr/abs-2105-14424","DOI":"10.1109/ICPR56361.2022.9956687","CorpusId":235254799},"title":"Gaze Estimation using Transformer"},{"paperId":"1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa","externalIds":{"DBLP":"conf/nips/DingYHZZYLZSYT21","ArXiv":"2105.13290","CorpusId":235212350},"title":"CogView: Mastering Text-to-Image Generation via Transformers"},{"paperId":"3618e503068e5f0e4f17ad1557a9bd6692daea79","externalIds":{"MAG":"3158432584","DBLP":"conf/iclr/TianRCO0MT21","ArXiv":"2104.15069","CorpusId":232275342},"title":"A Good Image Generator Is What You Need for High-Resolution Video Synthesis"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"47c1c6f46db3d77d4941f270646059d2aaae73f8","externalIds":{"DOI":"10.1109/PICC51425.2020.9362485","CorpusId":232153509},"title":"A Review of Video Generation Approaches"},{"paperId":"ee12f02830a5e1f7cadd3623dfed495cf099bc82","externalIds":{"DBLP":"journals/corr/abs-2012-06567","ArXiv":"2012.06567","MAG":"3113370935","CorpusId":228375268},"title":"A Comprehensive Study of Deep Video Action Recognition"},{"paperId":"baf5478fbf0a2f0ca2af287a35f3f5469afcd936","externalIds":{"MAG":"3109697227","ArXiv":"2011.14752","DBLP":"journals/corr/abs-2011-14752","CorpusId":227227989},"title":"A Comprehensive Review on Recent Methods and Challenges of Video Description"},{"paperId":"5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b","externalIds":{"MAG":"3095698190","ArXiv":"2011.02250","DBLP":"journals/corr/abs-2011-02250","DOI":"10.1145/3487891","CorpusId":226246346},"title":"Video Generative Adversarial Networks: A Review"},{"paperId":"83e9e62ae6b81d2adb1f4660aac0e9c37e4ba03c","externalIds":{"MAG":"3096788322","DBLP":"journals/cm/HooftVWTBTS20","DOI":"10.1109/MCOM.001.2000242","CorpusId":226268295},"title":"From Capturing to Rendering: Volumetric Media Delivery with Six Degrees of Freedom"},{"paperId":"0277d8a16307084f1840a22ad60477b7eb5e9246","externalIds":{"DBLP":"journals/corr/abs-2009-07833","ArXiv":"2009.07833","MAG":"3108702002","DOI":"10.1145/3414685.3417760","CorpusId":221737172},"title":"Layered neural rendering for retiming people in video"},{"paperId":"db3010eedc3a19ed1b4ae06f9006179db05f34bc","externalIds":{"ArXiv":"2004.05937","MAG":"3015735225","DBLP":"journals/pami/WangY22","DOI":"10.1109/TPAMI.2021.3055564","CorpusId":215745611,"PubMed":"33513099"},"title":"Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks"},{"paperId":"08a80e33fc646482b9fedc9f153238d960d670e5","externalIds":{"DBLP":"journals/corr/abs-1904-12165","MAG":"2991019415","ArXiv":"1904.12165","DOI":"10.1109/ICCV.2019.00770","CorpusId":139103210},"title":"Improved Conditional VRNNs for Video Prediction"},{"paperId":"62f9f17e82c9c898e720a36e6096d91d8bc7a2b5","externalIds":{"DBLP":"journals/corr/abs-1901-09953","ArXiv":"1901.09953","MAG":"2911736397","CorpusId":59336241},"title":"TGAN: Deep Tensor Generative Adversarial Nets for Large Image Generation"},{"paperId":"0c5f6d07b2a355312ba50132bab30832d1a4d883","externalIds":{"MAG":"3031246127","ArXiv":"1811.09245","DBLP":"journals/ijcv/SaitoSKK20","DOI":"10.1007/s11263-020-01333-y","CorpusId":218978582},"title":"Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN"},{"paperId":"e3a10973ee4e3be5fc53bee96e4d8e56469e432a","externalIds":{"DBLP":"conf/iccv/ChanGZE19","MAG":"2984529706","ArXiv":"1808.07371","DOI":"10.1109/ICCV.2019.00603","CorpusId":52070144},"title":"Everybody Dance Now"},{"paperId":"de3b9eb697feed3d097e3f671afe395f48c1ab76","externalIds":{"MAG":"2952558689","ArXiv":"1802.07687","DBLP":"journals/corr/abs-1802-07687","CorpusId":3663219},"title":"Stochastic Video Generation with a Learned Prior"},{"paperId":"e0244344a0106fb1cc502b093bd46e0e91418d3b","externalIds":{"MAG":"2773608377","DBLP":"conf/eccv/OliuSE18","ArXiv":"1712.00311","DOI":"10.1007/978-3-030-01264-9_44","CorpusId":3957727},"title":"Folded Recurrent Neural Networks for Future Video Prediction"},{"paperId":"b8375ff50b8a6f1a10dd809129a18df96888ac8b","externalIds":{"ArXiv":"1706.08033","DBLP":"conf/iclr/VillegasYHLL17","MAG":"2615413256","CorpusId":24069181},"title":"Decomposing Motion and Content for Natural Video Sequence Prediction"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6","externalIds":{"DBLP":"journals/corr/KayCSZHVVGBNSZ17","ArXiv":"1705.06950","MAG":"2619947201","CorpusId":27300853},"title":"The Kinetics Human Action Video Dataset"},{"paperId":"b01871c114b122340209562972ff515b86b16ccf","externalIds":{"MAG":"2963629403","DBLP":"journals/corr/KalchbrennerOSD16","ArXiv":"1610.00527","CorpusId":14278057},"title":"Video Pixel Networks"},{"paperId":"ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1","externalIds":{"MAG":"2520707650","ArXiv":"1609.02612","DBLP":"conf/nips/VondrickPT16","DOI":"10.13016/M26GIH-TNYZ","CorpusId":9933254},"title":"Generating Videos with Scene Dynamics"},{"paperId":"549169531738e93c43e20fc80bafe76ba0544860","externalIds":{"DBLP":"journals/icl/HooftPWHRBT16","MAG":"2507174453","DOI":"10.1109/LCOMM.2016.2601087","CorpusId":1585695},"title":"HTTP/2-Based Adaptive Streaming of HEVC Video Over 4G/LTE Networks"},{"paperId":"0936352b78a52bc5d2b5e3f04233efc56664af51","externalIds":{"MAG":"2423557781","ArXiv":"1606.05328","DBLP":"journals/corr/OordKVEGK16","CorpusId":14989939},"title":"Conditional Image Generation with PixelCNN Decoders"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"d94750bc374747d3e90c99e25ab2e238882f46ae","externalIds":{"DBLP":"journals/tcsv/SullivanOHW12","MAG":"2146395539","DOI":"10.1109/TCSVT.2012.2221191","CorpusId":64404},"title":"Overview of the High Efficiency Video Coding (HEVC) Standard"},{"paperId":"99c01639bf9ee1098bdf676a5f5e34177be4a69d","externalIds":{"DBLP":"journals/ieeemm/Sodagar11","MAG":"2127034683","DOI":"10.1109/MMUL.2011.71","CorpusId":14431014},"title":"The MPEG-DASH Standard for Multimedia Streaming Over the Internet"},{"paperId":"aa8183ac0429f9b0240c0e21eb534b21ace08bb7","externalIds":{"MAG":"2140199336","DBLP":"journals/tcsv/WiegandSBL03","DOI":"10.1109/TCSVT.2003.815165","CorpusId":3540699},"title":"Overview of the H.264/AVC video coding standard"},{"paperId":"ccfaf773a4c4d6ac3e3da2c573846488cabfe449","externalIds":{"DBLP":"journals/corr/abs-2312-09767","DOI":"10.48550/arXiv.2312.09767","CorpusId":266335297},"title":"DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models"},{"paperId":"1054668810e6925ed0530aa0978430f73dde937b","externalIds":{"DBLP":"conf/iclr/XiangTZ23","CorpusId":259298839},"title":"MIMT: Masked Image Modeling Transformer for Video Compression"},{"paperId":"428a6fc9bce37cdc27feb82dcb3241b120ff2176","externalIds":{"DBLP":"conf/exag/RanellaE23","CorpusId":264559591},"title":"Towards Automated Video Game Commentary Using Generative AI"},{"paperId":"82af929bd8cd8c41dcc423e5c2921eb6290bdcda","externalIds":{"DOI":"10.1007/978-981-16-6723-7_32","CorpusId":246055699},"title":"Hierarchical Language Modeling for Dense Video Captioning"},{"paperId":"5880829f5ac7d16659cc1495a239149254c2223f","externalIds":{"DBLP":"journals/corr/abs-2211-09310","DOI":"10.48550/arXiv.2211.09310","CorpusId":268241174},"title":"Problem Behaviors Recognition in Videos using Language-Assisted Deep Learning Model for Children with Autism"}]}