{"references":[{"paperId":"59756f4e786d3212f793b8b709a3d37ebbc16d94","externalIds":{"DBLP":"journals/corr/abs-2404-02657","ArXiv":"2404.02657","DOI":"10.48550/arXiv.2404.02657","CorpusId":268876464},"title":"Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models"},{"paperId":"7351898febca53d01453283c9b1a541b662e1ed3","externalIds":{"DBLP":"journals/corr/abs-2403-00818","ArXiv":"2403.00818","DOI":"10.48550/arXiv.2403.00818","CorpusId":268230534},"title":"DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models"},{"paperId":"49b7baceecd32f81a08aa8e84e2fe71c2b879ee6","externalIds":{"ArXiv":"2402.03898","DBLP":"conf/icml/KoKCY24","DOI":"10.48550/arXiv.2402.03898","CorpusId":267499832},"title":"DistiLLM: Towards Streamlined Distillation for Large Language Models"},{"paperId":"b24e899ec0f77eef2fc87a9b8e50516367aa1f97","externalIds":{"DBLP":"conf/nips/LiuTZYX0YJ024","ArXiv":"2401.10166","DOI":"10.48550/arXiv.2401.10166","CorpusId":267035250},"title":"VMamba: Visual State Space Model"},{"paperId":"38c48a1cd296d16dc9c56717495d6e44cc354444","externalIds":{"DBLP":"conf/icml/ZhuL0W0W24","ArXiv":"2401.09417","DOI":"10.48550/arXiv.2401.09417","CorpusId":267028142},"title":"Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"},{"paperId":"745594bd0dc3e9dc86f74e100cd2c98ed36256c0","externalIds":{"DBLP":"journals/corr/abs-2401-04081","ArXiv":"2401.04081","DOI":"10.48550/arXiv.2401.04081","CorpusId":266844147},"title":"MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts"},{"paperId":"d2b0cec5c7e222a8f2cafa48257982c3cabf41e2","externalIds":{"DBLP":"journals/corr/abs-2312-07950","ArXiv":"2312.07950","DOI":"10.48550/arXiv.2312.07950","CorpusId":266191842},"title":"CBQ: Cross-Block Quantization for Large Language Models"},{"paperId":"689c358c5f9b5b1693a8bcc7e6e0460012f5cf9e","externalIds":{"ArXiv":"2312.00785","DBLP":"journals/corr/abs-2312-00785","DOI":"10.1109/CVPR52733.2024.02157","CorpusId":265552038},"title":"Sequential Modeling Enables Scalable Learning for Large Vision Models"},{"paperId":"7bbc7595196a0606a07506c4fb1473e5e87f6082","externalIds":{"ArXiv":"2312.00752","DBLP":"journals/corr/abs-2312-00752","CorpusId":265551773},"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"},{"paperId":"90a9f862a478476881538c6a75b7b55d8aebe3cd","externalIds":{"DBLP":"journals/corr/abs-2311-01689","ArXiv":"2311.01689","DOI":"10.48550/arXiv.2311.01689","CorpusId":265019207},"title":"Data-Free Distillation of Language Model by Text-to-Text Transfer"},{"paperId":"ffdc017b1d2b493feaac9efa854882fe23d50dcf","externalIds":{"ArXiv":"2310.08041","DBLP":"journals/corr/abs-2310-08041","DOI":"10.48550/arXiv.2310.08041","CorpusId":263908852},"title":"QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models"},{"paperId":"564855d475ed9197dd7516594557ff886ff623e5","externalIds":{"ArXiv":"2310.05424","DBLP":"journals/corr/abs-2310-05424","DOI":"10.48550/arXiv.2310.05424","CorpusId":263830054},"title":"Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"},{"paperId":"fdc53c2c10742464087c0525f77e32604827a21d","externalIds":{"DBLP":"conf/iclr/XiaoTCHL24","ArXiv":"2309.17453","DOI":"10.48550/arXiv.2309.17453","CorpusId":263310483},"title":"Efficient Streaming Language Models with Attention Sinks"},{"paperId":"8ec117feff6ee10e3b20a19ac101fee5c99e14d7","externalIds":{"DBLP":"journals/corr/abs-2309-14021","ArXiv":"2309.14021","DOI":"10.48550/arXiv.2309.14021","CorpusId":262460763},"title":"LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression"},{"paperId":"633e3fe49fe9c314f7245f77401c2e4a95e925a9","externalIds":{"DBLP":"journals/corr/abs-2309-05516","ArXiv":"2309.05516","DOI":"10.48550/arXiv.2309.05516","CorpusId":261697282},"title":"Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs"},{"paperId":"00e889fcfaf4396a20f37f681cf8b14f3e878879","externalIds":{"ArXiv":"2309.04255","DBLP":"journals/corr/abs-2309-04255","DOI":"10.48550/arXiv.2309.04255","CorpusId":261660737},"title":"LLMCad: Fast and Scalable On-device Large Language Model Inference"},{"paperId":"dcb74fb63acd87d3db0a77de89720300fb28b50a","externalIds":{"DBLP":"journals/corr/abs-2309-02031","ArXiv":"2309.02031","DOI":"10.1109/TPAMI.2024.3392941","CorpusId":261531260,"PubMed":"38656856"},"title":"A Survey on Efficient Vision Transformers: Algorithms, Techniques, and Performance Benchmarking"},{"paperId":"eb2c2330177f765038a2b17e2ee3498965865797","externalIds":{"DBLP":"journals/corr/abs-2308-13137","ArXiv":"2308.13137","DOI":"10.48550/arXiv.2308.13137","CorpusId":261214575},"title":"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"},{"paperId":"3ed75cde4b35ba0b867d6162b8d23b3fe5198d10","externalIds":{"DBLP":"conf/iccv/FrumkinGM23","ArXiv":"2308.10814","DOI":"10.1109/ICCV51070.2023.01557","CorpusId":261049574},"title":"Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers"},{"paperId":"9cd53423a22dd5e83d8d5669600437c59b570cbf","externalIds":{"ArXiv":"2308.07641","DBLP":"journals/corr/abs-2308-07641","DOI":"10.48550/arXiv.2308.07641","CorpusId":260899992},"title":"Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping"},{"paperId":"67ee27880d8c9b1c220792513e5d33b40434b07c","externalIds":{"DBLP":"journals/pami/ChengZS24a","ArXiv":"2308.06767","DOI":"10.1109/TPAMI.2024.3447085","CorpusId":260887757,"PubMed":"39167504"},"title":"A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations"},{"paperId":"131ba9932572c92155874db93626cf299659254e","externalIds":{"ArXiv":"2308.00442","DBLP":"journals/corr/abs-2308-00442","DOI":"10.1109/ICCV51070.2023.00548","CorpusId":260351423},"title":"FLatten Transformer: Vision Transformer using Focused Linear Attention"},{"paperId":"0b777965bfb066dcef9a86510a5d7f305b71db94","externalIds":{"ArXiv":"2307.12612","DBLP":"journals/corr/abs-2307-12612","DOI":"10.48550/arXiv.2307.12612","CorpusId":260125992},"title":"Less is More: Focus Attention for Efficient DETR"},{"paperId":"240103933ffe3dac2179cc160a2bd91299357a53","externalIds":{"DBLP":"journals/corr/abs-2307-08621","ArXiv":"2307.08621","CorpusId":259937453},"title":"Retentive Network: A Successor to Transformer for Large Language Models"},{"paperId":"53e393c419506f2dfa3ab40a9ecafa385aea67e5","externalIds":{"ArXiv":"2307.08500","DBLP":"conf/iccv/ZhaoSL23","DOI":"10.1109/ICCV51070.2023.00565","CorpusId":259937633},"title":"Cumulative Spatial Knowledge Distillation for Vision Transformers"},{"paperId":"ce9435c82dc9b576f2037aa2f4357a520be9b2aa","externalIds":{"DBLP":"journals/corr/abs-2307-02628","ArXiv":"2307.02628","DOI":"10.48550/arXiv.2307.02628","CorpusId":259360560},"title":"SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference"},{"paperId":"4bf8c6ccd054a1144e19eafff431841a3b9a3e50","externalIds":{"ArXiv":"2307.00498","DBLP":"journals/pr/ChenBHWTL23","DOI":"10.48550/arXiv.2307.00498","CorpusId":259324270},"title":"Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning"},{"paperId":"87127eb8c9374a038493ddafbd43dffcf6b0a652","externalIds":{"ArXiv":"2307.00331","DBLP":"journals/tmlr/HuangSDC24","CorpusId":259316570},"title":"Quantization Variation: A New Perspective on Training Transformers with Low-Bit Precision"},{"paperId":"7d22ad3573101337bca2091fb0114b377c4f3db6","externalIds":{"DBLP":"journals/corr/abs-2306-11695","ArXiv":"2306.11695","DOI":"10.48550/arXiv.2306.11695","CorpusId":259203115},"title":"A Simple and Effective Pruning Approach for Large Language Models"},{"paperId":"275d8bfe7d9c671d1cb4f525434d10a7dbd8778a","externalIds":{"ArXiv":"2306.08543","CorpusId":259164722},"title":"MiniLLM: On-Policy Distillation of Large Language Models"},{"paperId":"3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc","externalIds":{"DBLP":"journals/corr/abs-2306-07629","ArXiv":"2306.07629","DOI":"10.48550/arXiv.2306.07629","CorpusId":259144954},"title":"SqueezeLLM: Dense-and-Sparse Quantization"},{"paperId":"b94e95d76001a1273303f11c6cd429d17f626b9b","externalIds":{"DBLP":"conf/iclr/HatamizadehHYT024","ArXiv":"2306.06189","DOI":"10.48550/arXiv.2306.06189","CorpusId":259137503},"title":"FasterViT: Fast Vision Transformers with Hierarchical Attention"},{"paperId":"db9507cdd3e2d7d9c90ed185bd831e55c62dcec9","externalIds":{"DBLP":"journals/sigmobile/LinTTYXH24","ArXiv":"2306.00978","DOI":"10.1145/3714983.3714987","CorpusId":258999941},"title":"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"},{"paperId":"7a1e71cb1310c4a873e7a4e54d1a6dab0553adce","externalIds":{"ArXiv":"2306.01116","DBLP":"journals/corr/abs-2306-01116","DOI":"10.48550/arXiv.2306.01116","CorpusId":259063761},"title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"},{"paperId":"6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2","externalIds":{"DBLP":"conf/acl/LiuO0CSMSKC24","ArXiv":"2305.17888","DOI":"10.48550/arXiv.2305.17888","CorpusId":258959117},"title":"LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"},{"paperId":"da1946bb4220e743e8f46946397a9b31e609df74","externalIds":{"DBLP":"journals/corr/abs-2305-15781","ArXiv":"2305.15781","DOI":"10.48550/arXiv.2305.15781","CorpusId":258887757},"title":"VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale"},{"paperId":"c193eb176985a81ae64f63c5e50b2f11cfb7c4e6","externalIds":{"DBLP":"journals/corr/abs-2305-15805","ArXiv":"2305.15805","DOI":"10.48550/arXiv.2305.15805","CorpusId":258888224},"title":"Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"50bf60b1439368caa941b386d1ed0c364dd7fe38","externalIds":{"DBLP":"conf/emnlp/LiuDLSL23","ArXiv":"2305.13999","DOI":"10.48550/arXiv.2305.13999","CorpusId":258841879},"title":"Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model"},{"paperId":"a10843d1349fff8d2a7d9722f800802187fef67f","externalIds":{"DBLP":"conf/nips/KimLKPYKL23","ArXiv":"2305.14152","DOI":"10.48550/arXiv.2305.14152","CorpusId":258841104},"title":"Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"},{"paperId":"aca65ea2730e3f49c0ff6fb7761e66756dc82255","externalIds":{"DBLP":"journals/corr/abs-2305-12972","ArXiv":"2305.12972","DOI":"10.48550/arXiv.2305.12972","CorpusId":258832388},"title":"VanillaNet: the Power of Minimalism in Deep Learning"},{"paperId":"026b3396a63ed5772329708b7580d633bb86bec9","externalIds":{"DBLP":"conf/emnlp/PengAAAABCCCDDG23","ArXiv":"2305.13048","DOI":"10.18653/v1/2023.findings-emnlp.936","CorpusId":258832459},"title":"RWKV: Reinventing RNNs for the Transformer Era"},{"paperId":"5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200","externalIds":{"DBLP":"journals/corr/abs-2305-13245","ArXiv":"2305.13245","DOI":"10.48550/arXiv.2305.13245","CorpusId":258833177},"title":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"},{"paperId":"c23249337a53a1ebb24af46091d43737e5c8bf3c","externalIds":{"ArXiv":"2305.09098","DBLP":"conf/naacl/WuHLLWZY24","DOI":"10.48550/arXiv.2305.09098","CorpusId":258714653},"title":"Weight-Inherited Distillation for Task-Agnostic BERT Compression"},{"paperId":"b95c0abfd9b820fe73fcb12182e2f05c23635740","externalIds":{"DBLP":"conf/ijcnn/XiaoLYG23","ArXiv":"2305.06559","DOI":"10.1109/IJCNN54540.2023.10191205","CorpusId":258615522},"title":"Patch-wise Mixed-Precision Quantization of Vision Transformer"},{"paperId":"9a83aeadc8db65fb6da39ec977360541cddaff5c","externalIds":{"DBLP":"journals/corr/abs-2305-07027","ArXiv":"2305.07027","DOI":"10.1109/CVPR52729.2023.01386","CorpusId":258615318},"title":"EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention"},{"paperId":"3e4085e5869f1b7959707a1e1d7d273b6057eb4e","externalIds":{"DBLP":"journals/tmlr/LiAZMKMMALCLZZW23","ArXiv":"2305.06161","CorpusId":258588247},"title":"StarCoder: may the source be with you!"},{"paperId":"aad167be3c902388ea625da4117fcae4325b8b7d","externalIds":{"ArXiv":"2305.02301","DBLP":"journals/corr/abs-2305-02301","DOI":"10.48550/arXiv.2305.02301","CorpusId":258461606},"title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"},{"paperId":"56fa65d8dc41708082f9b2ef7752c49cee9ebe01","externalIds":{"DBLP":"conf/acl/WangWLGYR23","ACL":"2023.acl-long.304","ArXiv":"2305.01879","DOI":"10.48550/arXiv.2305.01879","CorpusId":258461058},"title":"SCOTT: Self-Consistent Chain-of-Thought Distillation"},{"paperId":"389ec3e8902a5dcfcde1adec735854e93f845937","externalIds":{"DBLP":"journals/corr/abs-2304-14402","ACL":"2024.eacl-long.57","ArXiv":"2304.14402","DOI":"10.48550/arXiv.2304.14402","CorpusId":258352678},"title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"61a4023aad982d435dcde9f40bb9c2a735e88a9c","externalIds":{"DBLP":"conf/cvpr/WangZLWYLCLL23","ArXiv":"2304.05659","DOI":"10.1109/CVPR52729.2023.01388","CorpusId":258079340},"title":"RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer"},{"paperId":"eb3415db1b322b1f7bf95f8697aed701b0d40f88","externalIds":{"DBLP":"journals/corr/abs-2304-04487","ArXiv":"2304.04487","DOI":"10.48550/arXiv.2304.04487","CorpusId":258048436},"title":"Inference with Reference: Lossless Acceleration of Large Language Models"},{"paperId":"2a44c6b7f291f625314a82ba3131e605009fd533","externalIds":{"ArXiv":"2304.01089","DBLP":"journals/corr/abs-2304-01089","DOI":"10.48550/arXiv.2304.01089","CorpusId":257913374},"title":"RPTQ: Reorder-based Post-training Quantization for Large Language Models"},{"paperId":"7c4ebbc31bdac379bac9c44fa0792233a5b67f7b","externalIds":{"ArXiv":"2304.00253","DBLP":"conf/cvpr/XuLLGGL023","DOI":"10.1109/CVPR52729.2023.00374","CorpusId":257912858},"title":"Q-DETR: An Efficient Low-Bit Quantized Detection Transformer"},{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","externalIds":{"ArXiv":"2303.10845","DBLP":"journals/corr/abs-2303-10845","CorpusId":257666647},"title":"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"e60b6836b45ad0ae02a5fa663c8c31119f0c0a94","externalIds":{"DBLP":"journals/corr/abs-2303-04935","ArXiv":"2303.04935","DOI":"10.1109/CVPR52729.2023.02333","CorpusId":257427497},"title":"X-Pruner: eXplainable Pruning for Vision Transformers"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"998ac3e945857cf2676ee7efdbaf443a0c6f820a","externalIds":{"DBLP":"journals/corr/abs-2302-10866","ArXiv":"2302.10866","DOI":"10.48550/arXiv.2302.10866","CorpusId":257050308},"title":"Hyena Hierarchy: Towards Larger Convolutional Language Models"},{"paperId":"ae3ac5509c445327a23431409624a1333aa825b0","externalIds":{"DBLP":"journals/corr/abs-2302-03773","ArXiv":"2302.03773","DOI":"10.48550/arXiv.2302.03773","CorpusId":256662734},"title":"What Matters In The Structured Pruning of Generative Language Models?"},{"paperId":"42d3b9e9111efb70a11167096738d7dd344f7e5a","externalIds":{"DBLP":"conf/icml/LiuLC23","ArXiv":"2302.02210","DOI":"10.48550/arXiv.2302.02210","CorpusId":256615191},"title":"Oscillation-free Quantization for Low-bit Vision Transformers"},{"paperId":"a1f8082505c7e90b0a033e1b9da0a97d67aad66c","externalIds":{"DBLP":"journals/corr/abs-2302-01318","ArXiv":"2302.01318","DOI":"10.48550/arXiv.2302.01318","CorpusId":256503945},"title":"Accelerating Large Language Model Decoding with Speculative Sampling"},{"paperId":"fbd49b25bdab98c171af49962a41139c73dacbde","externalIds":{"DBLP":"conf/icml/FuPOSK23","ArXiv":"2301.12726","DOI":"10.48550/arXiv.2301.12726","CorpusId":256390607},"title":"Specializing Smaller Language Models towards Multi-Step Reasoning"},{"paperId":"909ad57ce8caa6b390a65ae09db352d27d8f3996","externalIds":{"DBLP":"journals/corr/abs-2301-00774","ArXiv":"2301.00774","DOI":"10.48550/arXiv.2301.00774","CorpusId":255372747},"title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"},{"paperId":"5a77b508302771fc083bf24e0bcda8553c9b5421","externalIds":{"DBLP":"journals/corr/abs-2212-14052","ArXiv":"2212.14052","DOI":"10.48550/arXiv.2212.14052","CorpusId":255340454},"title":"Hungry Hungry Hippos: Towards Language Modeling with State Space Models"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","externalIds":{"ACL":"2023.acl-long.830","DBLP":"conf/acl/HoSY23","ArXiv":"2212.10071","DOI":"10.48550/arXiv.2212.10071","CorpusId":254877399},"title":"Large Language Models Are Reasoning Teachers"},{"paperId":"f9ad1fffa1cc76fd5db3ff758c0839492c5147c4","externalIds":{"DBLP":"journals/corr/abs-2212-10670","ArXiv":"2212.10670","DOI":"10.48550/arXiv.2212.10670","CorpusId":254926556},"title":"In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models"},{"paperId":"a05ebb7be3de58b28af75f9c4d340fa03eeb7fb1","externalIds":{"DBLP":"journals/pami/WangWXZL23","DOI":"10.1109/TPAMI.2022.3229313","CorpusId":254909458,"PubMed":"37015428"},"title":"Quantformer: Learning Extremely Low-Precision Vision Transformers"},{"paperId":"126a4776ff8315fd506766cb8f3c722cf746ad9e","externalIds":{"DBLP":"journals/corr/abs-2212-08410","ACL":"2023.acl-short.151","ArXiv":"2212.08410","DOI":"10.48550/arXiv.2212.08410","CorpusId":254823156},"title":"Teaching Small Language Models to Reason"},{"paperId":"529ddd65f6c252aaea91d6a8e5b55b7bc3951841","externalIds":{"DBLP":"conf/iccv/LiXYG23","ArXiv":"2212.08254","DOI":"10.1109/ICCV51070.2023.01580","CorpusId":254823125},"title":"RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers"},{"paperId":"8fd462f6248d5e3f1b6602697c09489086b5655f","externalIds":{"DBLP":"conf/acl/ShridharSS23","ArXiv":"2212.00193","DOI":"10.18653/v1/2023.findings-acl.441","CorpusId":258762841},"title":"Distilling Reasoning Capabilities into Smaller Language Models"},{"paperId":"d8e9f8c8a37cb4cd26b92ad0d942d641cd512644","externalIds":{"DBLP":"journals/corr/abs-2211-17192","ArXiv":"2211.17192","DOI":"10.48550/arXiv.2211.17192","CorpusId":254096365},"title":"Fast Inference from Transformers via Speculative Decoding"},{"paperId":"8b87d39baf53d982bad7df8ab6c5c8e67c124c67","externalIds":{"ArXiv":"2211.16056","DBLP":"journals/corr/abs-2211-16056","DOI":"10.1109/CVPR52729.2023.01946","CorpusId":254069623},"title":"NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers"},{"paperId":"2c994fadbb84fb960d8306ee138dbeef41a5b323","externalIds":{"ArXiv":"2211.10438","DBLP":"conf/icml/XiaoLSWDH23","DOI":"10.48550/arXiv.2211.10438","CorpusId":253708271},"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"},{"paperId":"977351c92f156db27592e88b14dee2c22d4b312a","externalIds":{"DBLP":"conf/cvpr/YouXDWZ0VL23","ArXiv":"2211.10526","DOI":"10.1109/CVPR52729.2023.01387","CorpusId":253734615},"title":"Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6","externalIds":{"DBLP":"journals/corr/abs-2210-17323","ArXiv":"2210.17323","CorpusId":253237200},"title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"},{"paperId":"7d29a84a589aa5655e5d3fed8d725ea472816599","externalIds":{"ArXiv":"2210.06726","DBLP":"journals/corr/abs-2210-06726","DOI":"10.48550/arXiv.2210.06726","CorpusId":252873123},"title":"Explanations from Large Language Models Make Small Reasoners Better"},{"paperId":"dfdb2894d50e095ce97f994ed6cee38554c4c84f","externalIds":{"DBLP":"conf/nips/LiX000G22","ArXiv":"2210.06707","DOI":"10.48550/arXiv.2210.06707","CorpusId":252873138},"title":"Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer"},{"paperId":"b5e71069f091d52f474a2928ed07b6546157af82","externalIds":{"DBLP":"conf/mm/DingQYCLWL22","ArXiv":"2303.14341","DOI":"10.1145/3503161.3547826","CorpusId":252783098},"title":"Towards Accurate Post-Training Quantization for Vision Transformer"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"3f6243097a58e386aea1215fed4f372dee07a100","externalIds":{"ArXiv":"2209.13325","DBLP":"conf/nips/WeiZZGZZYL22","DOI":"10.48550/arXiv.2209.13325","CorpusId":252545187},"title":"Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"},{"paperId":"70e91e16eb321067d9402710e14a40cf28311f73","externalIds":{"DBLP":"conf/iclr/MaZKHGNMZ23","ArXiv":"2209.10655","DOI":"10.48550/arXiv.2209.10655","CorpusId":252439127},"title":"Mega: Moving Average Equipped Gated Attention"},{"paperId":"ac0730c8cf3a148b56f01def5ff0b809460481f8","externalIds":{"DBLP":"journals/tnn/LiCXG24","ArXiv":"2209.05687","DOI":"10.1109/TNNLS.2023.3301007","CorpusId":252211787,"PubMed":"37578910"},"title":"PSAQ-ViT V2: Toward Accurate and General Data-Free Quantization for Vision Transformers"},{"paperId":"d40c667826c0274350104b199143adc502a41a38","externalIds":{"DBLP":"journals/corr/abs-2209-02432","ArXiv":"2209.02432","DOI":"10.48550/arXiv.2209.02432","CorpusId":252090218},"title":"ViTKD: Practical Guidelines for ViT feature knowledge distillation"},{"paperId":"30a7390ec0103684eba9fb6bde1983d706fb57b3","externalIds":{"DBLP":"journals/corr/abs-2208-11580","ArXiv":"2208.11580","DOI":"10.48550/arXiv.2208.11580","CorpusId":251765570},"title":"Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"},{"paperId":"f27c847e2909f30745f4a3528b574f5acfd76ea7","externalIds":{"DBLP":"conf/fpl/LiSLMYX0LLWLF22","ArXiv":"2208.05163","DOI":"10.1109/FPL57034.2022.00027","CorpusId":251467937},"title":"Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization"},{"paperId":"2fe71acc2c3f1e75b6149dea72838f0b594ad013","externalIds":{"DBLP":"conf/eccv/WuZPLXFY22","ArXiv":"2207.10666","DOI":"10.48550/arXiv.2207.10666","CorpusId":250920355},"title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","externalIds":{"DBLP":"conf/nips/SchusterFG0B0TM22","ArXiv":"2207.07061","DOI":"10.48550/arXiv.2207.07061","CorpusId":250526382},"title":"Confident Adaptive Language Modeling"},{"paperId":"9fb327c55a30b9771a364f45f33f77778756a164","externalIds":{"DBLP":"conf/iccv/LiG23","ArXiv":"2207.01405","DOI":"10.1109/ICCV51070.2023.01565","CorpusId":250264564},"title":"I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference"},{"paperId":"d451901a6a12c61179289cac7a4588a86c234112","externalIds":{"DBLP":"conf/aaai/0004HWCCC22","DOI":"10.1609/aaai.v36i3.20222","CorpusId":250294994},"title":"Width & Depth Pruning for Vision Transformers"},{"paperId":"eaef083b9d661f42cc0d89d9d8156218f33a91d9","externalIds":{"DBLP":"journals/corr/abs-2206-13947","ArXiv":"2206.13947","DOI":"10.48550/arXiv.2206.13947","CorpusId":250089125},"title":"Long Range Language Modeling via Gated State Spaces"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"e03609f2587f690867e7ea0bedaf0db25282c548","externalIds":{"DBLP":"conf/nips/YaoAZWLH22","ArXiv":"2206.01861","DOI":"10.48550/arXiv.2206.01861","CorpusId":249395624},"title":"ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"bf6ce546c589fa8054b3972b266532664914bd21","externalIds":{"DBLP":"journals/corr/abs-2205-13213","ArXiv":"2205.13213","DOI":"10.48550/arXiv.2205.13213","CorpusId":249097732},"title":"Fast Vision Transformers with HiLo Attention"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"3a1dbfb6875bfac8251627d60db313623fbb8b04","externalIds":{"DBLP":"journals/corr/abs-2204-12997","ArXiv":"2204.12997","DOI":"10.1109/CVPR52688.2022.01174","CorpusId":248406101},"title":"DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers"},{"paperId":"c26bb68806a992bf4fc85b5639e1657a445c4781","externalIds":{"DBLP":"journals/corr/abs-2204-09179","ArXiv":"2204.09179","DOI":"10.48550/arXiv.2204.09179","CorpusId":248266346},"title":"On the Representation Collapse of Sparse Mixture of Experts"},{"paperId":"9e82736043eebe3f71eb86cbef6e2ac45306ece5","externalIds":{"ACL":"2022.acl-long.107","DBLP":"conf/acl/XiaZC22","ArXiv":"2204.00408","DOI":"10.48550/arXiv.2204.00408","CorpusId":247922354},"title":"Structured Pruning Learns Compact and Accurate Models"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","externalIds":{"DBLP":"journals/corr/abs-2203-15556","ArXiv":"2203.15556","CorpusId":247778764},"title":"Training Compute-Optimal Large Language Models"},{"paperId":"71e15a9a52dcafca57bff5f310b95e2c7d0cfc87","externalIds":{"DBLP":"conf/nips/0001GB22","ArXiv":"2203.14343","CorpusId":247762199},"title":"Diagonal State Spaces are as Effective as Structured State Spaces"},{"paperId":"fcd40a09376e2897b05352ca010786eabf9c9117","externalIds":{"DBLP":"journals/corr/abs-2203-13483","ArXiv":"2203.13483","DOI":"10.48550/arXiv.2203.13483","CorpusId":247748812},"title":"MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"},{"paperId":"4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a","externalIds":{"DBLP":"journals/corr/abs-2203-08243","ArXiv":"2203.08243","DOI":"10.48550/arXiv.2203.08243","CorpusId":247475977},"title":"Unified Visual Transformer Compression"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"fb5324a22f3e3208a07634433d8e3e403876582f","externalIds":{"DBLP":"journals/corr/abs-2203-02250","ArXiv":"2203.02250","DOI":"10.1007/978-3-031-20083-0_10","CorpusId":247244905},"title":"Patch Similarity Aware Data-Free Quantization for Vision Transformers"},{"paperId":"625270a54be430ad6262f848babb0d106af5b183","externalIds":{"DBLP":"journals/corr/abs-2202-07800","ArXiv":"2202.07800","CorpusId":246867285},"title":"Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"13f7a106bb3814ad1fab25fd1356e99e91f402d3","externalIds":{"ArXiv":"2201.07703","DBLP":"journals/corr/abs-2201-07703","CorpusId":246035548},"title":"Q-ViT: Fully Differentiable Quantization for Vision Transformer"},{"paperId":"0d9b8ccb1135b8e380dd8015b080158c6aae3ae5","externalIds":{"DBLP":"journals/corr/abs-2201-02767","ArXiv":"2201.02767","CorpusId":245837195},"title":"QuadTree Attention for Vision Transformers"},{"paperId":"5ab70d95ca49702a3dd49b39d9396d8136b52311","externalIds":{"DBLP":"conf/cvpr/ChavanS0LCX22","ArXiv":"2201.00814","DOI":"10.1109/CVPR52688.2022.00488","CorpusId":245650313},"title":"Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"68f141724814839d556a989646194be88641b143","externalIds":{"ArXiv":"2112.11446","DBLP":"journals/corr/abs-2112-11446","CorpusId":245353475},"title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"e0d272e01929024f28f0f7cacf26177cd60b3ee7","externalIds":{"DBLP":"journals/corr/abs-2111-14330","ArXiv":"2111.14330","CorpusId":244714783},"title":"Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity"},{"paperId":"1ee05cd919590eaba129caa0fda5e850c87b75a5","externalIds":{"DBLP":"conf/ijcai/LinZSLZ22","ArXiv":"2111.13824","DOI":"10.24963/ijcai.2022/164","CorpusId":248300269},"title":"FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer"},{"paperId":"b476c932e959cfe645911786f1a070c70b5375c6","externalIds":{"ArXiv":"2111.12294","DBLP":"journals/corr/abs-2111-12294","DOI":"10.1109/CVPR52688.2022.01066","CorpusId":244527347},"title":"An Image Patch is a Wave: Phase-Aware Vision MLP"},{"paperId":"39a620939887c9fc1f9bdd7ecfabde985a4aad3a","externalIds":{"ArXiv":"2111.12293","DBLP":"conf/eccv/YuanXCWS22","DOI":"10.1007/978-3-031-19775-8_12","CorpusId":244527659},"title":"PTQ4ViT: Post-training Quantization for Vision Transformers with Twin Uniform Quantization"},{"paperId":"57150ca7d793d6f784cf82da1c349edf7beb6bc2","externalIds":{"DBLP":"conf/cvpr/YuLZSZWFY22","ArXiv":"2111.11418","DOI":"10.1109/CVPR52688.2022.01055","CorpusId":244478080},"title":"MetaFormer is Actually What You Need for Vision"},{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","externalIds":{"DBLP":"conf/iclr/GuGR22","ArXiv":"2111.00396","CorpusId":240354066},"title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"ee8984a6712791d4e0f2c776dad8119a3b893dd9","externalIds":{"ArXiv":"2110.14883","DBLP":"conf/icpp/LiLBFHLW023","DOI":"10.1145/3605573.3605613","CorpusId":240070340},"title":"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"},{"paperId":"574072ae4556649de1c59d8f284955730f5a71a0","externalIds":{"DBLP":"journals/corr/abs-2110-08460","ArXiv":"2110.08460","CorpusId":239016567},"title":"A Short Study on Compressing Decoder-Based Language Models"},{"paperId":"b6f616e9305e59c9dc7ccf33c311ede47584caf6","externalIds":{"ArXiv":"2110.08152","ACL":"2022.acl-short.24","DBLP":"journals/corr/abs-2110-08152","DOI":"10.18653/v1/2022.acl-short.24","CorpusId":239009526},"title":"Kronecker Decomposition for GPT Compression"},{"paperId":"521ccc898395a2818fced22b4cf371b0e5121f94","externalIds":{"ACL":"2022.naacl-main.341","DBLP":"journals/corr/abs-2110-07178","ArXiv":"2110.07178","DOI":"10.18653/v1/2022.naacl-main.341","CorpusId":238857304},"title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"},{"paperId":"c051ee2ad7ac203a26fa8f50eb6312424c729b27","externalIds":{"DBLP":"conf/cvpr/YangYSMLK23","ArXiv":"2110.04869","DOI":"10.1109/CVPR52729.2023.01779","CorpusId":257833491},"title":"Global Vision Transformer Pruning with Hessian-Aware Saliency"},{"paperId":"da74a10824193be9d3889ce0d6ed4c6f8ee48b9e","externalIds":{"DBLP":"conf/iclr/MehtaR22","ArXiv":"2110.02178","CorpusId":238354201},"title":"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"},{"paperId":"e9a09f8e474b4c74c700ebbe84d5b0696395a521","externalIds":{"DBLP":"conf/nips/BaiHS0KL22","ArXiv":"2109.15082","CorpusId":238227163},"title":"Towards Efficient Post-training Quantization of Pre-trained Language Models"},{"paperId":"73bcf4577284fa116ee73487b7cbb85c8266eaa0","externalIds":{"ACL":"2021.emnlp-main.627","DBLP":"conf/emnlp/BondarenkoNB21","ArXiv":"2109.12948","DOI":"10.18653/v1/2021.emnlp-main.627","CorpusId":237940329},"title":"Understanding and Overcoming the Challenges of Efficient Transformer Quantization"},{"paperId":"01b1293ddea9bcd6df1185b0b934503de01d6561","externalIds":{"ArXiv":"2109.04838","DBLP":"journals/corr/abs-2109-04838","ACL":"2021.emnlp-main.829","DOI":"10.18653/v1/2021.emnlp-main.829","CorpusId":237485472},"title":"Block Pruning For Faster Transformers"},{"paperId":"58970a426b687bb080b7fed3b4b78ab1ebaa56f4","externalIds":{"DBLP":"conf/cvpr/GuoT00WXX022","ArXiv":"2108.13341","DOI":"10.1109/CVPR52688.2022.00090","CorpusId":237353156},"title":"Hire-MLP: Vision MLP via Hierarchical Rearrangement"},{"paperId":"b4ac117b20e06d3e98d9eb3b28ca47e1a1e5dd5d","externalIds":{"DBLP":"journals/corr/abs-2112-14938","ArXiv":"2112.14938","DOI":"10.24963/ijcai.2021/472","CorpusId":237100483},"title":"Automatic Mixed-Precision Quantization Search of BERT"},{"paperId":"f75cddf2d42ed01b34686704eb3504becef67442","externalIds":{"ArXiv":"2107.10224","DBLP":"conf/iclr/ChenXGCLL22","CorpusId":236154781},"title":"CycleMLP: A MLP-like Architecture for Dense Prediction"},{"paperId":"71363797140647ebb3f540584de0a8758d2f7aa2","externalIds":{"DBLP":"conf/iclr/LianYSG22","ArXiv":"2107.08391","CorpusId":236087421},"title":"AS-MLP: An Axial Shifted MLP Architecture for Vision"},{"paperId":"761240b06248b9836ee564bdab61559c84b681ed","externalIds":{"ArXiv":"2107.06263","DBLP":"conf/cvpr/Guo0WT00X22","DOI":"10.1109/CVPR52688.2022.01186","CorpusId":235829175},"title":"CMT: Convolutional Neural Networks Meet Vision Transformers"},{"paperId":"66775d9f16b3f4ca43dba2b31c7c42ca6dcba72b","externalIds":{"DBLP":"journals/corr/abs-2107-02960","ArXiv":"2107.02960","MAG":"3179517581","DOI":"10.1109/ICCV48922.2021.00008","CorpusId":235755073},"title":"GLiT: Neural Architecture Search for Global and Local Image Transformer"},{"paperId":"c723187a2230749b1e706df2217e928c8271a660","externalIds":{"ArXiv":"2107.01378","DBLP":"conf/nips/HaoGJ0T00022","CorpusId":235731728},"title":"Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation"},{"paperId":"c156b1b30e3dd9284615e5304f2fb2826c09d0ff","externalIds":{"ArXiv":"2107.00910","DBLP":"conf/kdd/KimSTGKHK22","DOI":"10.1145/3534678.3539260","CorpusId":235727659},"title":"Learned Token Pruning for Transformers"},{"paperId":"800cfb3d23115cdcd4d114234b65bbdf2080f798","externalIds":{"DBLP":"conf/cvpr/DongBCZYYCG22","ArXiv":"2107.00652","DOI":"10.1109/CVPR52688.2022.01181","CorpusId":235694312},"title":"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"},{"paperId":"9b6af0e358e76d22f209c75b1702c3e6ea7815b1","externalIds":{"DBLP":"conf/nips/RaoZZLZ21","ArXiv":"2107.00645","CorpusId":235694359},"title":"Global Filter Networks for Image Classification"},{"paperId":"48418b285a92376a38daafa664a2dd07d42e3fe3","externalIds":{"DBLP":"journals/corr/abs-2107-00641","ArXiv":"2107.00641","CorpusId":235694438},"title":"Focal Self-attention for Local-Global Interactions in Vision Transformers"},{"paperId":"d645bd08fc19d52164695f9cd5ae863345459a06","externalIds":{"ArXiv":"2107.00651","DBLP":"conf/iccv/ChenPFL21","DOI":"10.1109/ICCV48922.2021.01205","CorpusId":235694428},"title":"AutoFormer: Searching Transformers for Visual Recognition"},{"paperId":"c295391129426d89ec58cebb049d1cd2e976deec","externalIds":{"ArXiv":"2106.14156","DBLP":"conf/nips/LiuWHZMG21","CorpusId":235658553},"title":"Post-Training Quantization for Vision Transformer"},{"paperId":"e43eaeca5077d01061a38aebd24f8e3fa5948ad9","externalIds":{"DBLP":"conf/cvpr/RenGHXTHZ22","ArXiv":"2106.12378","DOI":"10.1109/CVPR52688.2022.01627","CorpusId":235606268},"title":"Co-advise: Cross Inductive Bias Distillation"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","externalIds":{"ArXiv":"2106.04560","DBLP":"journals/corr/abs-2106-04560","DOI":"10.1109/CVPR52688.2022.01179","CorpusId":235367962},"title":"Scaling Vision Transformers"},{"paperId":"0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe","externalIds":{"DBLP":"conf/nips/RollerSSW21","ArXiv":"2106.04426","CorpusId":235367626},"title":"Hash Layers For Large Sparse Models"},{"paperId":"efbe9f591090018f78b42c84613c8afda9292fdb","externalIds":{"DBLP":"conf/nips/ChenCGYZW21","ArXiv":"2106.04533","CorpusId":235367934},"title":"Chasing Sparsity in Vision Transformers: An End-to-End Exploration"},{"paperId":"f43b98fcc2d56c60fc71bce96374c1e6b8e12c66","externalIds":{"DBLP":"journals/corr/abs-2106-03650","ArXiv":"2106.03650","CorpusId":235358157},"title":"Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer"},{"paperId":"33fd56e5067a1e8a9713378af3e1c1c08d5ce93b","externalIds":{"ArXiv":"2106.02852","DBLP":"journals/corr/abs-2106-02852","DOI":"10.1109/CVPR52688.2022.01185","CorpusId":235358476},"title":"Patch Slimming for Efficient Vision Transformers"},{"paperId":"2e8149dafb864ec3675087c99bf5572fcf4eb170","externalIds":{"ArXiv":"2106.02689","DBLP":"journals/corr/abs-2106-02689","CorpusId":235359074},"title":"RegionViT: Regional-to-Local Attention for Vision Transformers"},{"paperId":"dbdcabd0444ad50b68ee09e30f39b66e9068f5d2","externalIds":{"DBLP":"conf/nips/RaoZLLZH21","ArXiv":"2106.02034","CorpusId":235313562},"title":"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"},{"paperId":"07e987364bf0be1949e379f976f8dea675977337","externalIds":{"DBLP":"conf/cvpr/FangXW00022","ArXiv":"2105.15168","DOI":"10.1109/CVPR52688.2022.01175","CorpusId":235254672},"title":"MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens"},{"paperId":"14b97585f136671742f6ce4151081e487b1fc1fe","externalIds":{"ArXiv":"2105.15075","DBLP":"conf/nips/WangHSHH21","CorpusId":239885997},"title":"Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition"},{"paperId":"fb987ebe5ff5276fbbe6a5c5b16b6bfd759afa37","externalIds":{"MAG":"3169938586","DBLP":"journals/corr/abs-2106-00515","ArXiv":"2106.00515","DOI":"10.1007/978-3-031-20053-3_17","CorpusId":235266033},"title":"KVT: k-NN Attention for Boosting Vision Transformers"},{"paperId":"d5e999aae76d5270ef272076979c809817458212","externalIds":{"DBLP":"journals/corr/abs-2105-14103","ArXiv":"2105.14103","CorpusId":235254329},"title":"An Attention Free Transformer"},{"paperId":"48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5","externalIds":{"DBLP":"journals/corr/abs-2105-03404","ArXiv":"2105.03404","DOI":"10.1109/TPAMI.2022.3206148","CorpusId":234094263,"PubMed":"36094972"},"title":"ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"},{"paperId":"fc92009ab34045f9e6d490684c7761f768e88c54","externalIds":{"DBLP":"journals/pami/GuoLMH23","ArXiv":"2105.02358","DOI":"10.1109/TPAMI.2022.3211006","CorpusId":233864910,"PubMed":"36197869"},"title":"Beyond Self-Attention: External Attention Using Two Linear Layers for Visual Tasks"},{"paperId":"67571d29190faea9fbd104acd16274f8c4edf254","externalIds":{"ArXiv":"2105.01601","DBLP":"conf/nips/TolstikhinHKBZU21","CorpusId":233714958},"title":"MLP-Mixer: An all-MLP Architecture for Vision"},{"paperId":"6709d5583f658f589ae6a2184805933aceb18849","externalIds":{"DBLP":"conf/nips/ChuTWZRWXS21","ArXiv":"2104.13840","CorpusId":234364557},"title":"Twins: Revisiting the Design of Spatial Attention in Vision Transformers"},{"paperId":"78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","externalIds":{"DBLP":"journals/corr/abs-2104-12369","MAG":"3158631574","ArXiv":"2104.12369","CorpusId":233394012},"title":"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"},{"paperId":"c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d","externalIds":{"ACL":"2021.emnlp-main.406","DBLP":"journals/corr/abs-2104-08803","ArXiv":"2104.08803","DOI":"10.18653/v1/2021.emnlp-main.406","CorpusId":233296246},"title":"Consistent Accelerated Inference via Confident Adaptive Transformers"},{"paperId":"93efaf8c27940aaef145d8bcbca957be634d26e5","externalIds":{"ArXiv":"2104.08500","CorpusId":233296620},"title":"Vision Transformer Pruning"},{"paperId":"5b68522f58b61e7235b852677337ef3725075fd9","externalIds":{"ArXiv":"2104.06399","DBLP":"conf/iccv/XuXCT21","DOI":"10.1109/ICCV48922.2021.00983","CorpusId":233219797},"title":"Co-Scale Conv-Attentional Image Transformers"},{"paperId":"003326a15fc4a8833785a47a741d7712474fa256","externalIds":{"ArXiv":"2104.01136","DBLP":"conf/iccv/GrahamETSJJD21","DOI":"10.1109/ICCV48922.2021.01204","CorpusId":233004577},"title":"LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference"},{"paperId":"b15ea460c77a4ee8aa159a30ab0331deedfcf392","externalIds":{"DBLP":"conf/icml/LewisBDGZ21","ArXiv":"2103.16716","CorpusId":232428341},"title":"BASE Layers: Simplifying Training of Large, Sparse Models"},{"paperId":"40f4d7fe800810288a80f84cdb357a8f4c28e880","externalIds":{"DBLP":"conf/iccv/HeoYHCCO21","ArXiv":"2103.16302","DOI":"10.1109/ICCV48922.2021.01172","CorpusId":232417451},"title":"Rethinking Spatial Dimensions of Vision Transformers"},{"paperId":"0eff37167876356da2163b2e396df2719adf7de9","externalIds":{"DBLP":"journals/corr/abs-2103-14899","ArXiv":"2103.14899","DOI":"10.1109/ICCV48922.2021.00041","CorpusId":232404237},"title":"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"},{"paperId":"ac591dbf261777e05d89c27f9a7bcb06f88aab5a","externalIds":{"DBLP":"conf/iccv/PanZLH021","ArXiv":"2103.10619","DOI":"10.1109/ICCV48922.2021.00043","CorpusId":232290833},"title":"Scalable Vision Transformers with Hierarchical Pooling"},{"paperId":"9ed25f101f19ea735ca300848948ed64064b97ca","externalIds":{"ArXiv":"2103.02143","DBLP":"journals/corr/abs-2103-02143","CorpusId":232105052},"title":"Random Feature Attention"},{"paperId":"0ae67202f0584afccefa770865d14a46655d2975","externalIds":{"DBLP":"conf/nips/HanXWGXW21","ArXiv":"2103.00112","CorpusId":232076027},"title":"Transformer in Transformer"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"8fb1c04dab87ca6c116495e4d03c46c9547e4ec3","externalIds":{"DBLP":"journals/corr/abs-2102-12122","ArXiv":"2102.12122","DOI":"10.1109/ICCV48922.2021.00061","CorpusId":232035922},"title":"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"},{"paperId":"b4d207a2096aee4a3764933373eef6edb574c952","externalIds":{"DBLP":"journals/corr/abs-2102-08124","ArXiv":"2102.08124","CorpusId":231934142},"title":"Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks"},{"paperId":"5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f","externalIds":{"MAG":"3127067080","DBLP":"journals/corr/abs-2102-04010","ArXiv":"2102.04010","CorpusId":231847094},"title":"Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch"},{"paperId":"6fa1cfc4f97f03a8485692418c7aa1a06c574a85","externalIds":{"DBLP":"journals/corr/abs-2102-03902","ArXiv":"2102.03902","DOI":"10.1609/aaai.v35i16.17664","CorpusId":231847231,"PubMed":"34745767"},"title":"Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention"},{"paperId":"1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a","externalIds":{"DBLP":"journals/corr/abs-2102-02611","ArXiv":"2102.02611","CorpusId":231802365},"title":"CKConv: Continuous Kernel Convolution For Sequential Data"},{"paperId":"dbe077f8521ecbe0a1477d6148c726d4f053d9c9","externalIds":{"DBLP":"journals/corr/abs-2101-11986","ArXiv":"2101.11986","DOI":"10.1109/ICCV48922.2021.00060","CorpusId":231719476},"title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"d06bda5e759c8e8569754d8f7235cf1a9a1e9985","externalIds":{"DBLP":"journals/corr/abs-2101-01321","ArXiv":"2101.01321","MAG":"3170233084","CorpusId":230523998},"title":"I-BERT: Integer-only BERT Quantization"},{"paperId":"c375e121926db9551f224ff235018ea38bb159b7","externalIds":{"DBLP":"conf/acl/BaiZHSJJLLK20","ACL":"2021.acl-long.334","ArXiv":"2012.15701","DOI":"10.18653/v1/2021.acl-long.334","CorpusId":229923538},"title":"BinaryBERT: Pushing the Limit of BERT Quantization"},{"paperId":"ad7ddcc14984caae308c397f1a589aae75d4ab71","externalIds":{"ArXiv":"2012.12877","DBLP":"journals/corr/abs-2012-12877","CorpusId":229363322},"title":"Training data-efficient image transformers & distillation through attention"},{"paperId":"6f6f73e69ee0d9d5d7d088bb882db1851d98175a","externalIds":{"DBLP":"conf/cvpr/Chen000DLMX0021","ArXiv":"2012.00364","MAG":"3109319753","DOI":"10.1109/CVPR46437.2021.01212","CorpusId":227239228},"title":"Pre-Trained Image Processing Transformer"},{"paperId":"e8f6eb89897c5880a99748f23c3d3763346eea45","externalIds":{"DBLP":"journals/corr/abs-2011-00593","MAG":"3095273266","ArXiv":"2011.00593","CorpusId":226226888},"title":"MixKD: Towards Efficient Distillation of Large-scale Language Models"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"b285b90ab4d6d1d726efc726b92f4075363004e6","externalIds":{"ArXiv":"2010.10732","MAG":"3094522247","DBLP":"journals/corr/abs-2010-10732","CorpusId":224814108},"title":"SCOP: Scientific Control for Reliable Neural Network Pruning"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","externalIds":{"DBLP":"conf/iclr/ChoromanskiLDSG21","ArXiv":"2009.14794","MAG":"3091156754","CorpusId":222067132},"title":"Rethinking Attention with Performers"},{"paperId":"097210dc65924f8ce59523faf444e635523dc714","externalIds":{"MAG":"3098576111","DBLP":"journals/corr/abs-2009-12812","ArXiv":"2009.12812","ACL":"2020.emnlp-main.37","DOI":"10.18653/v1/2020.emnlp-main.37","CorpusId":221970445},"title":"TernaryBERT: Distillation-aware Ultra-low Bit BERT"},{"paperId":"0964490205fdc38c2f0980c9d778069089ca92e3","externalIds":{"ArXiv":"2008.07669","MAG":"3099512283","DBLP":"conf/nips/GuDERR20","CorpusId":221150566},"title":"HiPPO: Recurrent Memory with Optimal Polynomial Projections"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","externalIds":{"ArXiv":"2007.14062","DBLP":"journals/corr/abs-2007-14062","MAG":"3045733172","CorpusId":220831004},"title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","externalIds":{"MAG":"3081168214","DBLP":"conf/kdd/RasleyRRH20","DOI":"10.1145/3394486.3406703","CorpusId":221191193},"title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","externalIds":{"MAG":"3040573126","DBLP":"conf/iclr/LepikhinLXCFHKS21","ArXiv":"2006.16668","CorpusId":220265858},"title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"6f68e1bb253925d8431588555d3010419f322e04","externalIds":{"DBLP":"conf/icml/KatharopoulosV020","MAG":"3037798801","ArXiv":"2006.16236","CorpusId":220250819},"title":"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"},{"paperId":"9bb5665fe48e7122beda73a53316de9f7f243b19","externalIds":{"MAG":"3035332806","DBLP":"conf/cvpr/WangWCLL0LH20","ArXiv":"2006.08509","DOI":"10.1109/cvpr42600.2020.00215","CorpusId":219687796},"title":"APQ: Joint Search for Network Architecture, Pruning and Quantization Policy"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","externalIds":{"MAG":"3096609285","DBLP":"conf/eccv/CarionMSUKZ20","ArXiv":"2005.12872","DOI":"10.1007/978-3-030-58452-8_13","CorpusId":218889832},"title":"End-to-End Object Detection with Transformers"},{"paperId":"8747f028acccde9ee7c35c858da8091613d3e574","externalIds":{"DBLP":"conf/aaai/LiuMZCX21","MAG":"3112673818","DOI":"10.1609/aaai.v35i15.17584","CorpusId":229283620},"title":"Faster Depth-Adaptive Transformers"},{"paperId":"c5cc2340766d68ece08bb1520d357bcf8c03ad48","externalIds":{"DBLP":"conf/acl/SchwartzSSDS20","MAG":"3016791490","ArXiv":"2004.07453","ACL":"2020.acl-main.593","DOI":"10.18653/v1/2020.acl-main.593","CorpusId":215785895},"title":"The Right Tool for the Job: Matching Model and Instance Complexities"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","externalIds":{"DBLP":"journals/corr/abs-2004-05150","MAG":"3015468748","ArXiv":"2004.05150","CorpusId":215737171},"title":"Longformer: The Long-Document Transformer"},{"paperId":"2573af4e13d9a5dddb257d22cd38a600528d9a8b","externalIds":{"DBLP":"journals/corr/abs-2004-02984","ArXiv":"2004.02984","ACL":"2020.acl-main.195","MAG":"3034457371","DOI":"10.18653/v1/2020.acl-main.195","CorpusId":215238853},"title":"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"},{"paperId":"5d34881ff68bd203ff790187e7e5c9e034389cfa","externalIds":{"MAG":"3014568172","DBLP":"journals/corr/abs-2004-02178","ArXiv":"2004.02178","ACL":"2020.acl-main.537","DOI":"10.18653/v1/2020.acl-main.537","CorpusId":214802887},"title":"FastBERT: a Self-distilling BERT with Adaptive Inference Time"},{"paperId":"9c5a239b75bade55c830b164e2fadc424e879137","externalIds":{"ACL":"2020.acl-main.202","MAG":"3022128899","ArXiv":"2004.05686","DBLP":"conf/acl/MukherjeeA20","DOI":"10.18653/v1/2020.acl-main.202","CorpusId":218502458},"title":"XtremeDistil: Multi-stage Distillation for Massive Multilingual Models"},{"paperId":"1c332cfa211400fc6f56983fb01a6692046116dd","externalIds":{"DBLP":"conf/nips/HouHSJCL20","MAG":"3101731278","ArXiv":"2004.04037","CorpusId":215415863},"title":"DynaBERT: Dynamic BERT with Adaptive Width and Depth"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","externalIds":{"DBLP":"journals/corr/abs-2003-08271","MAG":"3088409176","ArXiv":"2003.08271","DOI":"10.1007/s11431-020-1647-3","CorpusId":212747830},"title":"Pre-trained models for natural language processing: A survey"},{"paperId":"850464c9006261bd632c4203f3e630db09a32faf","externalIds":{"MAG":"3010035731","DBLP":"journals/corr/abs-2003-02389","ArXiv":"2003.02389","CorpusId":212415013},"title":"Comparing Rewinding and Fine-tuning in Neural Network Pruning"},{"paperId":"c6c734e16f66fbfcefac7625cc64599e83292c1e","externalIds":{"MAG":"3101045333","DBLP":"conf/nips/WangW0B0020","ArXiv":"2002.10957","CorpusId":211296536},"title":"MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","externalIds":{"MAG":"3033156098","CorpusId":226096901},"title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","externalIds":{"MAG":"3006439205","DBLP":"journals/corr/abs-2002-05202","ArXiv":"2002.05202","CorpusId":211096588},"title":"GLU Variants Improve Transformer"},{"paperId":"2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7","externalIds":{"DBLP":"conf/emnlp/XuZGWZ20","MAG":"3101248447","ArXiv":"2002.02925","ACL":"2020.emnlp-main.633","DOI":"10.18653/v1/2020.emnlp-main.633","CorpusId":211066200},"title":"BERT-of-Theseus: Compressing BERT by Progressive Module Replacing"},{"paperId":"94f94e8892261d0377159379ca5a166ceae19a14","externalIds":{"DBLP":"conf/icml/GoyalCRCSV20","MAG":"3034742519","CorpusId":219792793},"title":"PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"},{"paperId":"765866ecb5fe6a225d4e791498caf6a8351c16c7","externalIds":{"DBLP":"journals/corr/abs-2001-04589","ArXiv":"2001.04589","MAG":"2999008758","CorpusId":210472463},"title":"Faster Transformer Decoding: N-gram Masked Self-Attention"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","externalIds":{"DBLP":"journals/corr/abs-2001-04451","MAG":"2994673210","ArXiv":"2001.04451","CorpusId":209315300},"title":"Reformer: The Efficient Transformer"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","externalIds":{"MAG":"2970971581","DBLP":"journals/corr/abs-1912-01703","ArXiv":"1912.01703","CorpusId":202786778},"title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"f51497f463566581874c941353dd9d80069c5b77","externalIds":{"DBLP":"conf/iclr/RaePJHL20","MAG":"2995575179","ArXiv":"1911.05507","CorpusId":207930593},"title":"Compressive Transformers for Long-Range Sequence Modelling"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"7be8c119dbe065c52125ee7716601751f3116844","externalIds":{"MAG":"2988841832","ArXiv":"1911.00172","DBLP":"journals/corr/abs-1911-00172","CorpusId":207870430},"title":"Generalization through Memorization: Nearest Neighbor Language Models"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"4585611042d2be0d997ee135e3fe219d668db9ec","externalIds":{"MAG":"2981757109","DBLP":"conf/iclr/ElbayadGGA20","ArXiv":"1910.10073","CorpusId":204824061},"title":"Depth-Adaptive Transformer"},{"paperId":"ce106590145e89ea4b621c99665862967ccf5dac","externalIds":{"DBLP":"journals/corr/abs-1910-06188","ArXiv":"1910.06188","MAG":"2979314664","DOI":"10.1109/EMC2-NIPS53020.2019.00016","CorpusId":204509218},"title":"Q8BERT: Quantized 8Bit BERT"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","externalIds":{"DBLP":"journals/corr/abs-1910-01108","ArXiv":"1910.01108","MAG":"2978017171","CorpusId":203626972},"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1","externalIds":{"MAG":"2996159613","DBLP":"conf/iclr/FanGJ20","ArXiv":"1909.11556","CorpusId":202750230},"title":"Reducing Transformer Depth on Demand with Structured Dropout"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","externalIds":{"MAG":"2973727699","ArXiv":"1909.08053","DBLP":"journals/corr/abs-1909-08053","CorpusId":202660670},"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"5b446648504afeecf7c73028aa02c2da16db6224","externalIds":{"DBLP":"conf/emnlp/LiLHTQWL19","MAG":"2970531613","ACL":"D19-1573","ArXiv":"1909.06708","DOI":"10.18653/v1/D19-1573","CorpusId":202577368},"title":"Hint-Based Training for Non-Autoregressive Machine Translation"},{"paperId":"745e4b36a1759177871288cae51fbae0b873b5e5","externalIds":{"MAG":"2973061659","DBLP":"conf/aaai/ShenDYMYGMK20","ArXiv":"1909.05840","DOI":"10.1609/AAAI.V34I05.6409","CorpusId":202565587},"title":"Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"},{"paperId":"bb3f24186972fbc6d8dcd3327dabe7da1e0e4ce8","externalIds":{"MAG":"2974893078","DBLP":"journals/corr/abs-1909-08174","ArXiv":"1909.08174","CorpusId":202660914},"title":"Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks"},{"paperId":"2f9d4887d0022400fc40c774c4c78350c3bc5390","externalIds":{"MAG":"2971702703","ACL":"D19-1374","DBLP":"journals/corr/abs-1909-00100","ArXiv":"1909.00100","DOI":"10.18653/v1/D19-1374","CorpusId":202122780},"title":"Small and Practical BERT Models for Sequence Labeling"},{"paperId":"80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","externalIds":{"DBLP":"conf/emnlp/SunCGL19","MAG":"2969515962","ACL":"D19-1441","ArXiv":"1908.09355","DOI":"10.18653/v1/D19-1441","CorpusId":201670719},"title":"Patient Knowledge Distillation for BERT Model Compression"},{"paperId":"93ad19fbc85360043988fa9ea7932b7fdf1fa948","externalIds":{"DBLP":"journals/corr/abs-1908-08962","MAG":"2969601108","ArXiv":"1908.08962","CorpusId":201666324},"title":"Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"},{"paperId":"07a64686ce8e43ac475a8d820a8a9f1d87989583","externalIds":{"MAG":"2951528897","DBLP":"journals/corr/abs-1905-09418","ACL":"P19-1580","ArXiv":"1905.09418","DOI":"10.18653/v1/P19-1580","CorpusId":162183964},"title":"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"},{"paperId":"b03c7ff961822183bab66b2e594415e585d3fd09","externalIds":{"ArXiv":"1905.10650","MAG":"2945767825","DBLP":"conf/nips/MichelLN19","CorpusId":166227946},"title":"Are Sixteen Heads Really Better than One?"},{"paperId":"1a858b96d2fdfeadf8c0f7126cbd55825223fb9d","externalIds":{"MAG":"2944508492","ArXiv":"1905.03696","DBLP":"conf/iccv/DongYGMK19","DOI":"10.1109/ICCV.2019.00038","CorpusId":148571720},"title":"HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","externalIds":{"ArXiv":"1904.10509","DBLP":"journals/corr/abs-1904-10509","MAG":"2940744433","CorpusId":129945531},"title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"947928b4e7ff7ebb4a65d88d9c553a1fe5da7070","externalIds":{"MAG":"2927216998","DBLP":"conf/iccv/ChenW0YLSXX019","ArXiv":"1904.01186","DOI":"10.1109/ICCV.2019.00361","CorpusId":91183944},"title":"Data-Free Learning of Student Networks"},{"paperId":"a08293b2c9c5bcddb023cc7eb3354d4d86bfae89","externalIds":{"ArXiv":"1903.12136","DBLP":"journals/corr/abs-1903-12136","MAG":"2924902521","CorpusId":85543565},"title":"Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"},{"paperId":"2a31319e73d4486716168b65cdf7559baeda18ce","externalIds":{"ACL":"N19-1133","MAG":"2915716523","DBLP":"journals/corr/abs-1902-09113","ArXiv":"1902.09113","DOI":"10.18653/v1/N19-1133","CorpusId":209009596},"title":"Star-Transformer"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","externalIds":{"ArXiv":"1901.02860","DBLP":"conf/acl/DaiYYCLS19","MAG":"2964110616","ACL":"P19-1285","DOI":"10.18653/v1/P19-1285","CorpusId":57759363},"title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"cf440ccce4a7a8681e238b4f26d5b95109add55d","externalIds":{"MAG":"2963247446","DBLP":"conf/iclr/LeeAT19","ArXiv":"1810.02340","CorpusId":52920837},"title":"SNIP: Single-shot Network Pruning based on Connection Sensitivity"},{"paperId":"8f880afbfea328c496d1ab96a123bb57d4b506b5","externalIds":{"MAG":"2798170643","DBLP":"conf/eccv/ZhangYZTWFW18","ArXiv":"1804.03294","DOI":"10.1007/978-3-030-01237-3_12","CorpusId":4752389},"title":"A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","externalIds":{"ArXiv":"1802.05751","DBLP":"conf/icml/ParmarVUKSKT18","MAG":"2950739196","CorpusId":3353110},"title":"Image Transformer"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","externalIds":{"DBLP":"journals/corr/abs-1802-05365","MAG":"2949856395","ArXiv":"1802.05365","ACL":"N18-1202","DOI":"10.18653/v1/N18-1202","CorpusId":3626819},"title":"Deep Contextualized Word Representations"},{"paperId":"4feef0fd284feb1233399b400eb897f59ec92755","externalIds":{"MAG":"2765407302","DBLP":"journals/corr/abs-1710-09412","ArXiv":"1710.09412","CorpusId":3162051},"title":"mixup: Beyond Empirical Risk Minimization"},{"paperId":"3b4d671a8c7018c0b42673ba581e5ff3ae762d6c","externalIds":{"MAG":"2764043458","ArXiv":"1710.01878","DBLP":"conf/iclr/ZhuG18","CorpusId":27494814},"title":"To prune, or not to prune: exploring the efficacy of pruning for model compression"},{"paperId":"ee53c9480132fc0d09b1192226cb2c460462fd6d","externalIds":{"ArXiv":"1707.06168","DBLP":"journals/corr/HeZS17","MAG":"2737121650","DOI":"10.1109/ICCV.2017.155","CorpusId":20157893},"title":"Channel Pruning for Accelerating Very Deep Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"896de8418884f4aab1ae4a60027500c9e8baffc3","externalIds":{"MAG":"2610140147","DBLP":"conf/icpr/Teerapittayanon16","ArXiv":"1709.01686","DOI":"10.1109/ICPR.2016.7900006","CorpusId":2916466},"title":"BranchyNet: Fast inference via early exiting from deep neural networks"},{"paperId":"57a10537978600fd33dcdd48922c791609a4851a","externalIds":{"ACL":"D16-1139","DBLP":"conf/emnlp/KimR16","ArXiv":"1606.07947","MAG":"2463507112","DOI":"10.18653/v1/D16-1139","CorpusId":8451212},"title":"Sequence-Level Knowledge Distillation"},{"paperId":"642d0f49b7826adcf986616f4af77e736229990f","externalIds":{"MAG":"2119144962","DBLP":"journals/corr/HanMD15","ArXiv":"1510.00149","CorpusId":2134321},"title":"Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"},{"paperId":"d9e022b5b10af145e728471e13aa07783b509e0e","externalIds":{"DBLP":"journals/moc/Pan17","MAG":"599168895","ArXiv":"1506.02285","DOI":"10.1090/mcom/3204","CorpusId":2993876},"title":"Fast approximate computations with Cauchy matrices and polynomials"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"8604f376633af8b347e31d84c6150a93b11e34c2","externalIds":{"DBLP":"journals/corr/RomeroBKCGB14","MAG":"2964118293","ArXiv":"1412.6550","CorpusId":2723173},"title":"FitNets: Hints for Thin Deep Nets"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","externalIds":{"MAG":"2949429431","ArXiv":"1409.1556","DBLP":"journals/corr/SimonyanZ14a","CorpusId":14124313},"title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"021fc345d40d3e6332cd2ef276e2eaa5e71102e4","externalIds":{"MAG":"1996901117","DBLP":"journals/corr/JaderbergVZ14","ArXiv":"1405.3866","DOI":"10.5244/C.28.88","CorpusId":17864746},"title":"Speeding up Convolutional Neural Networks with Low Rank Expansions"},{"paperId":"d770060812fb646b3846a7d398a3066145b5e3c8","externalIds":{"MAG":"2134797427","ArXiv":"1312.6184","DBLP":"journals/corr/BaC13","CorpusId":11536917},"title":"Do Deep Nets Really Need to be Deep?"},{"paperId":"eff61216e0136886e1158625b1e5a88ed1a7cbce","externalIds":{"DBLP":"journals/corr/DenilSDRF13","MAG":"2161591461","ArXiv":"1306.0543","DOI":"10.14288/1.0165555","CorpusId":1639981},"title":"Predicting Parameters in Deep Learning"},{"paperId":"a9c1379fa08a07e28bb85343f7f7d4d09c302bac","externalIds":{"MAG":"2108278040","DBLP":"conf/wsdm/CambazogluZCCLZD10","DOI":"10.1145/1718487.1718538","CorpusId":2281383},"title":"Early exit optimizations for additive machine learned ranking systems"},{"paperId":"30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9","externalIds":{"MAG":"2294370754","DBLP":"conf/kdd/BucilaCN06","DOI":"10.1145/1150402.1150464","CorpusId":11253972},"title":"Model compression"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"35c0183e9940feb567b4417115be8460bc127cfa","externalIds":{"MAG":"1534096605","CorpusId":60628113},"title":"Linear System Theory and Design"},{"paperId":"052b1d8ce63b07fec3de9dbb583772d860b7c769","externalIds":{"MAG":"1498436455","DOI":"10.1038/323533a0","CorpusId":205001834},"title":"Learning representations by back-propagating errors"},{"paperId":"8b16dc5b4c0728147eef1647a6ab7f786333b76c","externalIds":{"DBLP":"conf/iclr/LeeKLH23","CorpusId":259298782},"title":"Sparse Token Transformer with Attention Back Tracking"},{"paperId":"72c03b873e8c5cd86b15bf73186df341da4731c9","externalIds":{"DBLP":"conf/iclr/SyedGS23","CorpusId":259950394},"title":"Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models"},{"paperId":"29c7f009df21d0112c48dec254ff80cc45fac3af","externalIds":{"DBLP":"conf/nips/SchaefferMK23","ArXiv":"2304.15004","DOI":"10.48550/arXiv.2304.15004","CorpusId":258418299},"title":"Are Emergent Abilities of Large Language Models a Mirage?"},{"paperId":"51cda783aa6a97e0b3b5915a2bb5a35f31f3c083","externalIds":{"DBLP":"journals/corr/abs-2306-13649","DOI":"10.48550/arXiv.2306.13649","CorpusId":259244026},"title":"GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models"},{"paperId":"0088b2b6f7983a9ac1b53e34a307d68a3383f42c","externalIds":{"DBLP":"conf/acl/TaoHBWJLLW23","DOI":"10.18653/v1/2023.findings-acl.692","CorpusId":259858812},"title":"Structured Pruning for Efficient Generative Pre-trained Language Models"},{"paperId":"e3dfeb8be76960036fbb4439e7cff4b9c7184998","externalIds":{"DBLP":"journals/corr/abs-2306-09299","DOI":"10.48550/arXiv.2306.09299","CorpusId":259165464},"title":"Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind"},{"paperId":"81051b830a4f5606106765902a51ba281c9230f9","externalIds":{"DBLP":"journals/corr/abs-2304-09145","DOI":"10.48550/arXiv.2304.09145","CorpusId":258187503},"title":"Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"},{"paperId":"af67be0fff8d087a0d8554b6e8998ab12409bbda","externalIds":{"DBLP":"journals/corr/abs-2307-00526","DOI":"10.48550/arXiv.2307.00526","CorpusId":259316802},"title":"TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"},{"paperId":"bb8f7fbec020675d269ccfa0e6e603f02b664c0d","externalIds":{"DBLP":"journals/corr/abs-2305-13888","DOI":"10.48550/arXiv.2305.13888","CorpusId":273994248},"title":"PaD: Program-aided Distillation Specializes Large Models in Reasoning"},{"paperId":"104f7a96eba307056e1038e183ee8c24d009ba13","externalIds":{"DBLP":"journals/corr/abs-2206-09557","DOI":"10.48550/arXiv.2206.09557","CorpusId":249890271},"title":"nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"},{"paperId":"76579d7425474606583aa82e2b16702980b9a03a","externalIds":{"DBLP":"conf/nips/ZhenglZYTXRP22","CorpusId":258509611},"title":"SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","externalIds":{"ArXiv":"2103.14030","DBLP":"conf/iccv/LiuL00W0LG21","DOI":"10.1109/ICCV48922.2021.00986","CorpusId":232352874},"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"}]}