{"abstract":"Multi-modality magnetic resonance imaging (MRI) data prove more effective than a single modality in diagnosis, yet data collection for paired MRI images is a challenge. It is usually time consuming and may raise privacy concern. In this study, we explore cross-modality translation between MRI T1-weighted and T2-weighted images with 3D dual conditional generative adversarial networks (CGANs), and aim to synthesize missing modality of MRI images to mitigate the issue of insufficient data. To that end, a novel 3D-method, MRI translation generative adversarial network (MRI-Trans-GAN) is established. First, different from existing 3D big-patch methods, MRI-Trans-GAN is based on modified 3D patches with larger size in sagittal and coronal (frontal) axes and smaller size in vertical axis, i.e., 128×128×3 voxels. In addition, the tensor shape in vertical axis is fixed in forward process, and as a result, our model uses less memory than 3D big-patch methods. Moreover, it considers voxels relationship among adjacent slice other 2D-methods ignore. Second, we investigate loss functions and leverage wasserstein distance with gradient penalty and spectral normalization over traditional adversarial losses for better image quality and fidelity. Third, extensive experiments demonstrate the superiority of the proposed model over the state-of-the-art. It shows that our method renders better quality in terms of peak signal to noise ratio (PSNR) and structural similarity (SSIM)."}