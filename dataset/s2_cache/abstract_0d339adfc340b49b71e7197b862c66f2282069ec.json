{"abstract":"Recent advances in protein language models (PLMs) have transformed protein engineering, yet unlike their counterparts in natural language processing (NLP), current PLMs exhibit a fundamental limitation: they excel in either protein language understanding (PLU) or protein language generation (PLG), but rarely both. This fragmentation hinders progress in protein engineering. To bridge this gap, we introduce ProLLaMA, a multitask PLM enhanced by the evolutionary protein generation framework (EPGF). We construct a comprehensive instruction dataset containing approximately 13 million samples with over 11â€‰000 superfamily annotations to facilitate better modeling of sequence-function landscapes. We leverage a two-stage training approach to develop ProLLaMA, a multitask LLM with protein domain expertise. Our EPGF addresses the mismatch between statisticsl language modeling and biological constraints through three innovations: a multidimensional interpretable scorer, hierarchical efficient decoding, and a probabilistic-biophysical joint selection mechanism. Extensive experiments demonstrate that ProLLaMA excels in both unconditional and controllable protein generation tasks, achieving superior structural quality metrics compared to existing PLMs. Additionally, ProLLaMA demonstrates strong understanding capabilities with a 67.1% exact match rate in superfamily prediction. EPGF significantly enhances the biological viability of generated sequences, as evidenced by improved biophysical scores (+4.3%) and structural metrics (+14.5%)."}