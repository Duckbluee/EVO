{"abstract":"In few-shot classification, performing well on a testing dataset is a challenging task due to the restricted amount of labelled data available and the unknown distribution. Many previously proposed techniques rely on prototypical representations of the support set in order to classify a query set. Although this approach works well with a large, in-domain support set, accuracy suffers when transitioning to an out-of-domain setting, especially when using small support sets. To address out-of-domain performance degradation with small support sets, we propose Masked Embedding Modeling for Few-Shot Learning (MEM-FS), a novel, self-supervised, generative technique that reinforces few-shot-classification accuracy for a prototypical backbone model. MEM-FS leverages the data completion capabilities of a masked autoencoder to expand a given embedded support set. To further increase out-of-domain performance, we also introduce Rapid Domain Adjustment (RDA), a novel, self-supervised process for quickly conditioning MEM-FS to a new domain. We show that masked support embeddings generated by MEM-FS+RDA can significantly improve backbone performance on both out-of-domain and in-domain datasets. Our experiments demonstrate that applying the proposed technique to an inductive classifier achieves state-of-the-art performance on mini-imagenet, the CVPR L2ID Classification Challenge, and a newly proposed dataset, IKEA-FS. We provide code for this work at https://github.com/Brikwerk/MEM-FS"}