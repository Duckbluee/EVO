{"paperId":"be445964370b5ea8c15aa1bf6cf10f951fa90b9a","externalIds":{"ArXiv":"2404.04925","DBLP":"journals/corr/abs-2404-04925","DOI":"10.48550/arXiv.2404.04925","CorpusId":269005862},"title":"Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2284759913","name":"Libo Qin"},{"authorId":"2133447633","name":"Qiguang Chen"},{"authorId":"2295681570","name":"Yuhang Zhou"},{"authorId":"2295678203","name":"Zhi Chen"},{"authorId":"2295785468","name":"Yinghui Li"},{"authorId":"2266754933","name":"Lizi Liao"},{"authorId":"2266810224","name":"Min Li"},{"authorId":"2262459819","name":"Wanxiang Che"},{"authorId":"2284755458","name":"Philip S. Yu"}],"abstract":"Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs."}