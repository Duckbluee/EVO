{"references":[{"paperId":"14969de024c6e2e396fae835d737602733d9418a","externalIds":{"ArXiv":"2412.20404","DBLP":"journals/corr/abs-2412-20404","DOI":"10.48550/arXiv.2412.20404","CorpusId":275133398},"title":"Open-Sora: Democratizing Efficient Video Production for All"},{"paperId":"bc2ddba47a9d6b378abd8a8584849f3e4c410f24","externalIds":{"DBLP":"journals/corr/abs-2410-18072","ArXiv":"2410.18072","DOI":"10.48550/arXiv.2410.18072","CorpusId":273532769},"title":"WorldSimBench: Towards Video Generation Models as World Simulators"},{"paperId":"ed92a7be6b2acc36baf0ed43454a97dbade0c893","externalIds":{"ArXiv":"2410.13720","CorpusId":273403698},"title":"Movie Gen: A Cast of Media Foundation Models"},{"paperId":"c3d746a8ae633431cdd64aa285c627f8ef847f42","externalIds":{"ArXiv":"2407.16124","DBLP":"journals/corr/abs-2407-16124","DOI":"10.48550/arXiv.2407.16124","CorpusId":271334698},"title":"Fréchet Video Motion Distance: A Metric for Evaluating Motion Consistency in Videos"},{"paperId":"a9ac6c4b165d8c4bf1a949d0931e7ca7a21fc55b","externalIds":{"DBLP":"journals/corr/abs-2407-15595","ArXiv":"2407.15595","DOI":"10.48550/arXiv.2407.15595","CorpusId":271328803},"title":"Discrete Flow Matching"},{"paperId":"d9535d396e8f019588d658b22b2f69754be87e7f","externalIds":{"DBLP":"conf/nips/GuptaYGBKLR24","ArXiv":"2405.05852","DOI":"10.48550/arXiv.2405.05852","CorpusId":269635284},"title":"Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control"},{"paperId":"f2ab985efa11956ab5e25a7f2a8dec67c63a6b3c","externalIds":{"ArXiv":"2405.00620","DOI":"10.3390/rs17162845","CorpusId":269484451},"title":"Lane Graph Extraction from Aerial Imagery via Lane Segmentation Refinement with Diffusion Models"},{"paperId":"6e0543f8371e524c23f86e4d2277f94955bba80f","externalIds":{"ArXiv":"2403.06098","DBLP":"conf/nips/WangY24","DOI":"10.48550/arXiv.2403.06098","CorpusId":269293420},"title":"VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models"},{"paperId":"41a66997ce0a366bba3becf7c3f37c9aebb13fbd","externalIds":{"DBLP":"journals/corr/abs-2403-03206","ArXiv":"2403.03206","DOI":"10.48550/arXiv.2403.03206","CorpusId":268247980},"title":"Scaling Rectified Flow Transformers for High-Resolution Image Synthesis"},{"paperId":"3e47cdf1cd18467ac7aa2b66b04ad0d1afda1ae3","externalIds":{"DBLP":"journals/corr/abs-2403-01329","ArXiv":"2403.01329","DOI":"10.48550/arXiv.2403.01329","CorpusId":268231006},"title":"Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models"},{"paperId":"c064de2c71ebc5cf05493f49dc312b033c36b3b9","externalIds":{"DBLP":"conf/icml/BruceDEPS0LMSAA24","ArXiv":"2402.15391","DOI":"10.48550/arXiv.2402.15391","CorpusId":267897982},"title":"Genie: Generative Interactive Environments"},{"paperId":"66a05b7405aa3591a8fb74e5958c8d6dc994606e","externalIds":{"DBLP":"journals/corr/abs-2402-13185","ArXiv":"2402.13185","DOI":"10.1145/3746027.3755462","CorpusId":267760278},"title":"UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing"},{"paperId":"d599dc40c9cb8d6d76554ee7d21d20c22cc7cdb5","externalIds":{"ArXiv":"2402.04324","DBLP":"journals/tmlr/RenYZWDHC24","DOI":"10.48550/arXiv.2402.04324","CorpusId":267523282},"title":"ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation"},{"paperId":"450011c7e089675b25dad11cd7611c310a7a8e9b","externalIds":{"DBLP":"journals/corr/abs-2401-15977","ArXiv":"2401.15977","DOI":"10.1145/3641519.3657497","CorpusId":267311454},"title":"Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling"},{"paperId":"94f7d8bce3bb848d127c8f113afc5bb0243579df","externalIds":{"DBLP":"conf/siggrapha/Bar-TalCTHPZEHL24","ArXiv":"2401.12945","DOI":"10.1145/3680528.3687614","CorpusId":267095113},"title":"Lumiere: A Space-Time Diffusion Model for Video Generation"},{"paperId":"43c44948874dd19dbb4df4b3127e6b248cb3c7e2","externalIds":{"ArXiv":"2401.09985","DBLP":"journals/corr/abs-2401-09985","DOI":"10.48550/arXiv.2401.09985","CorpusId":267035033},"title":"WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens"},{"paperId":"492bc8339d8aac442c4ec13f8c1d59e980a3af2f","externalIds":{"DBLP":"conf/cvpr/ChenZCXWWS24","ArXiv":"2401.09047","DOI":"10.1109/CVPR52733.2024.00698","CorpusId":267028095},"title":"VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models"},{"paperId":"e0eac8c64be3313e581c28a495bec192e7e67284","externalIds":{"ArXiv":"2401.03048","DBLP":"journals/tmlr/Ma0CJ0LC025","CorpusId":266844878},"title":"Latte: Latent Diffusion Transformer for Video Generation"},{"paperId":"9be8950710e375cf47a002e9b4094b04814bde62","externalIds":{"DBLP":"journals/corr/abs-2401-01827","ArXiv":"2401.01827","DOI":"10.48550/arXiv.2401.01827","CorpusId":266741873},"title":"Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions"},{"paperId":"74f310af61b146b4b264bca0d5db6e55a4db8f08","externalIds":{"DBLP":"journals/tmlr/MelnikSLMSGR23","ArXiv":"2312.10728","DOI":"10.48550/arXiv.2312.10728","CorpusId":266359268},"title":"Benchmarks for Physical Reasoning AI"},{"paperId":"905ba940236b00bebb2fd348d4d932e7887b0c0a","externalIds":{"DBLP":"journals/corr/abs-2312-06662","ArXiv":"2312.06662","DOI":"10.48550/arXiv.2312.06662","CorpusId":266163109},"title":"Photorealistic Video Generation with Diffusion Models"},{"paperId":"8b28792f8405b737229afb92c99c579b86d8aa98","externalIds":{"DBLP":"journals/corr/abs-2312-06674","ArXiv":"2312.06674","DOI":"10.48550/arXiv.2312.06674","CorpusId":266174345},"title":"Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations"},{"paperId":"4e9a8141da2a8c603722b07d096109207f8e0b66","externalIds":{"DBLP":"journals/corr/abs-2311-17982","ArXiv":"2311.17982","DOI":"10.1109/CVPR52733.2024.02060","CorpusId":265506207},"title":"VBench: Comprehensive Benchmark Suite for Video Generative Models"},{"paperId":"c8dc4af5c61f95cc79b7f83e8339efa62af8f811","externalIds":{"ArXiv":"2311.17117","DBLP":"conf/cvpr/Hu24","DOI":"10.1109/CVPR52733.2024.00779","CorpusId":265499043},"title":"Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"},{"paperId":"9d587b3f1d0608bf89fdb4ea6eed3312a73e938c","externalIds":{"DBLP":"conf/cvpr/XuZLYLZFS24","ArXiv":"2311.16498","DOI":"10.1109/CVPR52733.2024.00147","CorpusId":265466012},"title":"MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"8467c119251ea48204586d1c3c67aa8fb781f7c4","externalIds":{"ArXiv":"2311.10982","DBLP":"journals/corr/abs-2311-10982","DOI":"10.1109/CVPR52733.2024.00845","CorpusId":265294415},"title":"Make Pixels Dance: High-Dynamic Video Generation"},{"paperId":"85b10400864187230714506412c85610c786b5c3","externalIds":{"ArXiv":"2311.10709","DBLP":"conf/eccv/GirdharSBDARSYPM24","DOI":"10.48550/arXiv.2311.10709","CorpusId":265281059},"title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning"},{"paperId":"9b86ce1bde87b304141641b49299f4d0f1f7ba1d","externalIds":{"ArXiv":"2311.04145","DBLP":"journals/corr/abs-2311-04145","DOI":"10.48550/arXiv.2311.04145","CorpusId":265043460},"title":"I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models"},{"paperId":"b020cac5955b48c22ac59fa74bc49f6e3260a637","externalIds":{"ArXiv":"2310.20700","DBLP":"conf/iclr/Chen0ZZMY0L0024","DOI":"10.48550/arXiv.2310.20700","CorpusId":264820183},"title":"SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction"},{"paperId":"1891c3756f870d902a0b793a1dcd5cc34c778612","externalIds":{"DBLP":"journals/corr/abs-2310-19512","ArXiv":"2310.19512","DOI":"10.48550/arXiv.2310.19512","CorpusId":264803867},"title":"VideoCrafter1: Open Diffusion Models for High-Quality Video Generation"},{"paperId":"083bab4a967c2221d9f4da9110fe37d8ca679078","externalIds":{"DBLP":"conf/eccv/XingXZCYLLWSW24","ArXiv":"2310.12190","DOI":"10.48550/arXiv.2310.12190","CorpusId":264306292},"title":"DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors"},{"paperId":"c3d14e7a319ab764297a60112ce74af201762a73","externalIds":{"DBLP":"journals/corr/abs-2310-06114","ArXiv":"2310.06114","DOI":"10.48550/arXiv.2310.06114","CorpusId":263830899},"title":"Learning Interactive Real-World Simulators"},{"paperId":"8fafd95a6ffbecf9c1b5f4542ac4b78a00602551","externalIds":{"DBLP":"journals/corr/abs-2310-00426","ArXiv":"2310.00426","DOI":"10.48550/arXiv.2310.00426","CorpusId":263334265},"title":"PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis"},{"paperId":"98478ac589e5b40a20630ff54bb4eec4ab4c5f6b","externalIds":{"ArXiv":"2309.17080","DBLP":"journals/corr/abs-2309-17080","DOI":"10.48550/arXiv.2309.17080","CorpusId":263310665},"title":"GAIA-1: A Generative World Model for Autonomous Driving"},{"paperId":"a5b7fc1bff0910ff31975ec0a15ed30c41f0a968","externalIds":{"DBLP":"journals/ijcv/ZhangWLZRGGS25","ArXiv":"2309.15818","DOI":"10.1007/s11263-024-02271-9","CorpusId":263151295},"title":"Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation"},{"paperId":"ed4603ea341acc26cab24f41aa40524fb7779917","externalIds":{"DBLP":"journals/ijcv/WangCMZHWYHYYGWSJCLDLQL25","ArXiv":"2309.15103","DOI":"10.1007/s11263-024-02295-1","CorpusId":262823915},"title":"LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models"},{"paperId":"4c170259862c65d4077d63ee9784a109aa42c34c","externalIds":{"DBLP":"journals/corr/abs-2309-13274","ArXiv":"2309.13274","DOI":"10.48550/arXiv.2309.13274","CorpusId":262464470},"title":"GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER"},{"paperId":"616e2783f7e5f2b879a0d3171a73a31d99189259","externalIds":{"DBLP":"journals/corr/abs-2309-02773","ArXiv":"2309.02773","DOI":"10.1109/TIP.2025.3551648","CorpusId":261557357,"PubMed":"40126966"},"title":"Diffusion Model is Secretly a Training-Free Open Vocabulary Semantic Segmenter"},{"paperId":"82e29cc0a07e25998021c9f9af426cae11a62953","externalIds":{"DBLP":"conf/cvpr/TianACKG24","ArXiv":"2308.12469","DOI":"10.1109/CVPR52733.2024.00341","CorpusId":261101006},"title":"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion"},{"paperId":"a89b6674d38fd2cf9eddf12c20f3aae1a6637dec","externalIds":{"DBLP":"journals/corr/abs-2308-10916","ArXiv":"2308.10916","DOI":"10.1109/ICCV51070.2023.01736","CorpusId":261065075},"title":"Diffusion Model as Representation Learner"},{"paperId":"f84a328afa81ecbd651636633bea0618045d0dfc","externalIds":{"ArXiv":"2307.15055","DBLP":"journals/corr/abs-2307-15055","DOI":"10.1109/ICCV51070.2023.01818","CorpusId":260203056},"title":"PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking"},{"paperId":"c1caa303549764d220ff17dc1785985dd1ba6047","externalIds":{"DBLP":"journals/corr/abs-2307-04725","ArXiv":"2307.04725","DOI":"10.48550/arXiv.2307.04725","CorpusId":259501509},"title":"AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning"},{"paperId":"d7890d1906d95c4ae4c430b350455156d6d8aed9","externalIds":{"DBLP":"conf/iclr/PodellELBDMPR24","ArXiv":"2307.01952","CorpusId":259341735},"title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"},{"paperId":"1e09b83fe064826a9a1ac61a7bdc00f26be41aee","externalIds":{"DBLP":"journals/corr/abs-2306-07954","ArXiv":"2306.07954","DOI":"10.1145/3610548.3618160","CorpusId":259144797},"title":"Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation"},{"paperId":"05aea4a4646cb971bb253ced42e8935034a60b57","externalIds":{"ArXiv":"2306.07257","DBLP":"conf/mm/Zhu0H0TCGSF23","DOI":"10.1145/3581783.3612707","CorpusId":259138745},"title":"MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images"},{"paperId":"6e6d3daeb11675414391bd935a9e4e84dcff8d47","externalIds":{"DBLP":"conf/icassp/ZhuZLZ23","DOI":"10.1109/ICASSP49357.2023.10094937","CorpusId":258532430},"title":"Audio-Driven Talking Head Video Generation with Diffusion Model"},{"paperId":"f02ea7a18f00859d9ea1b321e3385ae7d0170639","externalIds":{"DBLP":"journals/corr/abs-2306-02018","ArXiv":"2306.02018","DOI":"10.48550/arXiv.2306.02018","CorpusId":259075720},"title":"VideoComposer: Compositional Video Synthesis with Motion Controllability"},{"paperId":"8de8177147fc2dda4b430ca6d6cb2fdc179756aa","externalIds":{"ArXiv":"2306.01732","DBLP":"journals/corr/abs-2306-01732","DOI":"10.48550/arXiv.2306.01732","CorpusId":259063794},"title":"Video Colorization with Pre-trained Text-to-Image Diffusion Models"},{"paperId":"52b10ae66d025e99fbb602935e155f97f4f0696f","externalIds":{"DBLP":"journals/corr/abs-2306-00943","ArXiv":"2306.00943","DOI":"10.1109/TVCG.2024.3365804","CorpusId":258999372,"PubMed":"38354074"},"title":"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance"},{"paperId":"481e00d33baba3a2c8023e0185630bb17240bca5","externalIds":{"ArXiv":"2305.18264","DBLP":"journals/corr/abs-2305-18264","DOI":"10.48550/arXiv.2305.18264","CorpusId":258960293},"title":"Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising"},{"paperId":"529191401a8a5f0a8bdb2a1c01301d76af585a3a","externalIds":{"DBLP":"journals/corr/abs-2305-13077","ArXiv":"2305.13077","DOI":"10.48550/arXiv.2305.13077","CorpusId":258832670},"title":"ControlVideo: Training-free Controllable Text-to-Video Generation"},{"paperId":"9f411fda2ad5b141a3115f707bcf5ee865b3fb94","externalIds":{"DBLP":"conf/nips/TangYZ0B23","ArXiv":"2305.11846","DOI":"10.48550/arXiv.2305.11846","CorpusId":258822817},"title":"Any-to-Any Generation via Composable Diffusion"},{"paperId":"02bc11de9d2f75bad48166098aa6b30fffee4d70","externalIds":{"ArXiv":"2305.10474","DBLP":"journals/corr/abs-2305-10474","DOI":"10.1109/ICCV51070.2023.02096","CorpusId":258762178},"title":"Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models"},{"paperId":"5f51eda9f7abddca027941d50fb0b6bf6f508eff","externalIds":{"ArXiv":"2305.08850","DBLP":"journals/corr/abs-2305-08850","DOI":"10.48550/arXiv.2305.08850","CorpusId":258686590},"title":"Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts"},{"paperId":"c92bb55d93da48b4aee6ebf56620f35de067833b","externalIds":{"ArXiv":"2305.08854","DBLP":"journals/corr/abs-2305-08854","DOI":"10.48550/arXiv.2305.08854","CorpusId":258686633},"title":"Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models"},{"paperId":"8482fb463dfecb4c24b1af7b7f47307e726bcc05","externalIds":{"DBLP":"journals/corr/abs-2305-05464","ArXiv":"2305.05464","DOI":"10.1109/LSP.2024.3398538","CorpusId":258564566},"title":"Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer"},{"paperId":"15efd2755d422c2bd801fdd2bfdc6dca9adf337c","externalIds":{"DBLP":"journals/corr/abs-2305-04001","ArXiv":"2305.04001","DOI":"10.48550/arXiv.2305.04001","CorpusId":258557566},"title":"AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion"},{"paperId":"efcbc21d945651ed5248b6c51e1c065283d68c47","externalIds":{"DBLP":"conf/icml/PooladianBDALC23","ArXiv":"2304.14772","DOI":"10.48550/arXiv.2304.14772","CorpusId":258418096},"title":"Multisample Flow Matching: Straightening Flows with Minibatch Couplings"},{"paperId":"d64755f140ad742495518714ebd457b4d95ce341","externalIds":{"ArXiv":"2304.14404","DBLP":"journals/corr/abs-2304-14404","DOI":"10.48550/arXiv.2304.14404","CorpusId":258352582},"title":"Motion-Conditioned Diffusion Model for Controllable Video Synthesis"},{"paperId":"f5a0c57f90c6abe31482e9f320ccac5ee789b135","externalIds":{"ArXiv":"2304.08818","DBLP":"journals/corr/abs-2304-08818","DOI":"10.1109/CVPR52729.2023.02161","CorpusId":258187553},"title":"Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"},{"paperId":"8ad199f11f386319ebd2706c372562677c98fae3","externalIds":{"DBLP":"journals/corr/abs-2304-08477","ArXiv":"2304.08477","DOI":"10.48550/arXiv.2304.08477","CorpusId":258180320},"title":"Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation"},{"paperId":"31a1b96392c65a70ab944e276170c81fdffcc73b","externalIds":{"DBLP":"journals/corr/abs-2304-08551","ArXiv":"2304.08551","DOI":"10.48550/arXiv.2304.08551","CorpusId":258187320},"title":"Generative Disco: Text-to-Video Generation for Music Visualization"},{"paperId":"c3939dcd0d61af653fbc2bdbc5c7ad0d8adda30d","externalIds":{"ArXiv":"2304.06818","DBLP":"journals/corr/abs-2304-06818","DOI":"10.48550/arXiv.2304.06818","CorpusId":258170508},"title":"Soundini: Sound-Guided Diffusion for Natural Video Editing"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"ee73edebd42626d9c2d91e35fd2ed3cdb0fb26d0","externalIds":{"DBLP":"conf/aaai/MaHCWC0C24","ArXiv":"2304.01186","DOI":"10.48550/arXiv.2304.01186","CorpusId":257912672},"title":"Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos"},{"paperId":"782838a8699e10b80a0a359f2f7c448aef2ee429","externalIds":{"ArXiv":"2303.17599","DBLP":"journals/corr/abs-2303-17599","DOI":"10.48550/arXiv.2303.17599","CorpusId":257833877},"title":"Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models"},{"paperId":"4702d5a163477c734a54f3ed2d171dca1504eaae","externalIds":{"DBLP":"conf/iccv/LiPDBP23","ArXiv":"2303.16203","DOI":"10.1109/ICCV51070.2023.00210","CorpusId":257771787},"title":"Your Diffusion Model is Secretly a Zero-Shot Classifier"},{"paperId":"923a03032014a12c4e8b26511c0394e1b915fe74","externalIds":{"DBLP":"conf/iccv/KhachatryanMTHW23","ArXiv":"2303.13439","DOI":"10.1109/ICCV51070.2023.01462","CorpusId":257687280},"title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"},{"paperId":"32a3c2fbd3e733bd0eea938517fec2ff8dc7c701","externalIds":{"DBLP":"conf/iccv/CeylanHM23","ArXiv":"2303.12688","DOI":"10.1109/ICCV51070.2023.02121","CorpusId":257663916},"title":"Pix2Video: Video Editing using Image Diffusion"},{"paperId":"0c7a877e7952785216767ad9fa3b850fe77cce52","externalIds":{"ACL":"2023.acl-long.73","DBLP":"conf/acl/YinWYWWNYLL0FGW23","ArXiv":"2303.12346","DOI":"10.48550/arXiv.2303.12346","CorpusId":257663639},"title":"NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation"},{"paperId":"14ccb8bcceb6de10eda6ad08bec242a4f2946497","externalIds":{"DBLP":"conf/iccv/QiCZLWSC23","ArXiv":"2303.09535","DOI":"10.1109/ICCV51070.2023.01460","CorpusId":257557738},"title":"FateZero: Fusing Attentions for Zero-shot Text-based Video Editing"},{"paperId":"26c6090b7e7ba4513f82aa28d41360c60770c618","externalIds":{"ArXiv":"2303.08320","DBLP":"journals/corr/abs-2303-08320","DOI":"10.1109/CVPR52729.2023.00984","CorpusId":257532642},"title":"VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation"},{"paperId":"cd7427ba511c126ed0cf32ba9404b6aa53da0153","externalIds":{"DBLP":"journals/corr/abs-2303-08797","ArXiv":"2303.08797","DOI":"10.48550/arXiv.2303.08797","CorpusId":257532329},"title":"Stochastic Interpolants: A Unifying Framework for Flows and Diffusions"},{"paperId":"90889ff0ce930528e65b1b761bf9bcf9195e5051","externalIds":{"ArXiv":"2303.06614","DBLP":"conf/nips/LuBTP23","DOI":"10.48550/arXiv.2303.06614","CorpusId":257495808},"title":"Synthetic Experience Replay"},{"paperId":"c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0","externalIds":{"DBLP":"conf/eccv/LiuZRLZYJLYSZZ24","ArXiv":"2303.05499","DOI":"10.48550/arXiv.2303.05499","CorpusId":257427307},"title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"},{"paperId":"6283502d6900a0b403e2454b1cb1cf16ddefd5a7","externalIds":{"DBLP":"conf/cvpr/LiuZ00J24","ArXiv":"2303.04761","DOI":"10.1109/CVPR52733.2024.00821","CorpusId":257405406},"title":"Video-P2P: Video Editing with Cross-Attention Control"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"9a01fc428d195a9c5ea2005dc2943a650d59aa76","externalIds":{"DBLP":"journals/corr/abs-2302-06671","ArXiv":"2302.06671","DOI":"10.48550/arXiv.2302.06671","CorpusId":256846801},"title":"GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"07be0ec1f45e21a1032616535d0290ee6bfe0f6b","externalIds":{"DBLP":"conf/iccv/EsserCAGG23","ArXiv":"2302.03011","DOI":"10.1109/ICCV51070.2023.00675","CorpusId":256615582},"title":"Structure and Content-Guided Video Synthesis with Diffusion Models"},{"paperId":"9758ddd6ffbaac75aa0447a9664e6989811a05e2","externalIds":{"DBLP":"journals/corr/abs-2302-01329","ArXiv":"2302.01329","DOI":"10.48550/arXiv.2302.01329","CorpusId":256503757},"title":"Dreamix: Video Diffusion Models are General Video Editors"},{"paperId":"5396c55bee2a2abf2207e1cc5e5ae72c9edef9fa","externalIds":{"ArXiv":"2302.00482","DBLP":"journals/tmlr/0001FMHZRWB24","CorpusId":259847293},"title":"Improving and generalizing flow-based generative models with minibatch optimal transport"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"a677b65bf1e1f9881bccc90fe08261e11f79fab3","externalIds":{"ArXiv":"2301.03396","DBLP":"conf/wacv/StypulkowskiV0Z24","DOI":"10.1109/WACV57701.2024.00502","CorpusId":255546475},"title":"Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation"},{"paperId":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","externalIds":{"DBLP":"conf/iccv/WuGWLGSHSQS23","ArXiv":"2212.11565","DOI":"10.1109/ICCV51070.2023.00701","CorpusId":254974187},"title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"},{"paperId":"736973165f98105fec3729b7db414ae4d80fcbeb","externalIds":{"DBLP":"journals/corr/abs-2212-09748","ArXiv":"2212.09748","DOI":"10.1109/ICCV51070.2023.00387","CorpusId":254854389},"title":"Scalable Diffusion Models with Transformers"},{"paperId":"c5e4571f52b6e36e41574b2035d3bbf31b6a0ad8","externalIds":{"DBLP":"journals/pami/MelnikMMPAHRRR24","ArXiv":"2212.09102","DOI":"10.1109/TPAMI.2024.3350004","CorpusId":254853703,"PubMed":"38224501"},"title":"Face Generation and Editing With StyleGAN: A Survey"},{"paperId":"466aed7471eefcb54bfc762ad009e7cd5e81a2b2","externalIds":{"ArXiv":"2212.08420","DBLP":"conf/cvpr/SariyildizALK23","DOI":"10.1109/CVPR52729.2023.00774","CorpusId":257772061},"title":"Fake it Till You Make it: Learning Transferable Representations from Synthetic ImageNet Clones"},{"paperId":"f19dfc360088922cf1d423c538662aae8d542c28","externalIds":{"DBLP":"journals/corr/abs-2211-15657","ArXiv":"2211.15657","DOI":"10.48550/arXiv.2211.15657","CorpusId":254044710},"title":"Is Conditional Generative Modeling all you need for Decision-Making?"},{"paperId":"b000d6865db824af1563708fb7a545ddd65c6b3a","externalIds":{"DBLP":"conf/cvpr/TumanyanGBD23","ArXiv":"2211.12572","DOI":"10.1109/CVPR52729.2023.00191","CorpusId":253801961},"title":"Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation"},{"paperId":"94b690162ead76af6a487d6e10998ea585c035d1","externalIds":{"DBLP":"journals/corr/abs-2211-11018","ArXiv":"2211.11018","DOI":"10.48550/arXiv.2211.11018","CorpusId":253735209},"title":"MagicVideo: Efficient Video Generation With Latent Diffusion Models"},{"paperId":"a2d2bbe4c542173662a444b33b76c66992697830","externalIds":{"DBLP":"conf/cvpr/BrooksHE23","ArXiv":"2211.09800","DOI":"10.1109/CVPR52729.2023.01764","CorpusId":253581213},"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions"},{"paperId":"e935731a9e821ee2b90d8f8a07d84d99fc0e0f1f","externalIds":{"DOI":"10.1109/ICCKE57176.2022.9960039","CorpusId":254101013},"title":"A Survey on Semi-Automated and Automated Approaches for Video Annotation"},{"paperId":"8fe6a16df99a87f79fc0cb0fbd1af44f79f7885f","externalIds":{"DBLP":"conf/acl/WangMMYHC23","ArXiv":"2210.14896","ACL":"2023.acl-long.51","DOI":"10.48550/arXiv.2210.14896","CorpusId":253116574},"title":"DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"af68f10ab5078bfc519caae377c90ee6d9c504e9","externalIds":{"ArXiv":"2210.02747","DBLP":"journals/corr/abs-2210-02747","CorpusId":252734897},"title":"Flow Matching for Generative Modeling"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"1e33716e8820b867d5a8aaebab44c2d3135ea4ac","externalIds":{"DBLP":"conf/iclr/SingerPH00ZHYAG23","ArXiv":"2209.14792","CorpusId":252595919},"title":"Make-A-Video: Text-to-Video Generation without Text-Video Data"},{"paperId":"244054a4254a2147e43a3dad9c124b9b7eb4a04a","externalIds":{"DBLP":"journals/corr/abs-2209-03003","ArXiv":"2209.03003","DOI":"10.48550/arXiv.2209.03003","CorpusId":252111177},"title":"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow"},{"paperId":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744","externalIds":{"ArXiv":"2208.12242","DBLP":"conf/cvpr/RuizLJPRA23","DOI":"10.1109/CVPR52729.2023.02155","CorpusId":251800180},"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"paperId":"5406129d9d7d00dc310671c43597101b0ee93629","externalIds":{"ArXiv":"2208.01618","DBLP":"journals/corr/abs-2208-01618","DOI":"10.48550/arXiv.2208.01618","CorpusId":251253049},"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"},{"paperId":"04e541391e8dce14d099d00fb2c21dbbd8afe87f","externalIds":{"DBLP":"journals/corr/abs-2208-01626","ArXiv":"2208.01626","DOI":"10.48550/arXiv.2208.01626","CorpusId":251252882},"title":"Prompt-to-Prompt Image Editing with Cross Attention Control"},{"paperId":"af9f365ed86614c800f082bd8eb14be76072ad16","externalIds":{"DBLP":"journals/corr/abs-2207-12598","ArXiv":"2207.12598","DOI":"10.48550/arXiv.2207.12598","CorpusId":249145348},"title":"Classifier-Free Diffusion Guidance"},{"paperId":"01724c36660359545e1368fc80c99f4bde44a190","externalIds":{"DBLP":"conf/eccv/ChengS22","ArXiv":"2207.07115","DOI":"10.48550/arXiv.2207.07115","CorpusId":250526250},"title":"XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model"},{"paperId":"2f4c451922e227cbbd4f090b74298445bbd900d0","externalIds":{"DBLP":"journals/corr/abs-2206-00364","ArXiv":"2206.00364","DOI":"10.48550/arXiv.2206.00364","CorpusId":249240415},"title":"Elucidating the Design Space of Diffusion-Based Generative Models"},{"paperId":"707bd332d2c21dc5eb1f02a52d4a0506199aae76","externalIds":{"ArXiv":"2205.15868","DBLP":"journals/corr/abs-2205-15868","DOI":"10.48550/arXiv.2205.15868","CorpusId":249209614},"title":"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers"},{"paperId":"805748eec6be59ae0cd92a48400a902b3b7ed8e6","externalIds":{"DBLP":"conf/nips/HarveyNMWW22","ArXiv":"2205.11495","DOI":"10.48550/arXiv.2205.11495","CorpusId":248986725},"title":"Flexible Diffusion Modeling of Long Videos"},{"paperId":"3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e","externalIds":{"ArXiv":"2205.09991","DBLP":"journals/corr/abs-2205-09991","DOI":"10.48550/arXiv.2205.09991","CorpusId":248965046},"title":"Planning with Diffusion for Flexible Behavior Synthesis"},{"paperId":"75bb9eda70751c63fc54dbe63377c673b7dbdb15","externalIds":{"DBLP":"journals/corr/abs-2204-14217","ArXiv":"2204.14217","DOI":"10.48550/arXiv.2204.14217","CorpusId":248476190},"title":"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","externalIds":{"DBLP":"conf/nips/HoSGC0F22","ArXiv":"2204.03458","DOI":"10.48550/arXiv.2204.03458","CorpusId":248006185},"title":"Video Diffusion Models"},{"paperId":"21fd946d6841a20feb9f19c8188fcdad6e91f84d","externalIds":{"ArXiv":"2204.03638","DBLP":"journals/corr/abs-2204-03638","DOI":"10.48550/arXiv.2204.03638","CorpusId":248006334},"title":"Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer"},{"paperId":"ef4741b12cb8f01bcc68708c8cffcdc4237383f7","externalIds":{"ArXiv":"2202.10571","DBLP":"conf/iclr/YuTMKK0S22","CorpusId":247025714},"title":"Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"97bee918b08c244eb2e54d41e8ea6da00a3e5dbf","externalIds":{"DBLP":"conf/eccv/WuLJYFJD22","ArXiv":"2111.12417","DOI":"10.1007/978-3-031-19787-1_41","CorpusId":244527261},"title":"NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion"},{"paperId":"e1a3e6856b6ac6af3600b5954392e5368603fd1b","externalIds":{"ArXiv":"2111.10337","DBLP":"conf/cvpr/XueHZS00FG22","DOI":"10.1109/CVPR52688.2022.00498","CorpusId":244462849},"title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions"},{"paperId":"e3d06054af531ee2f42270d43100b309c28546ef","externalIds":{"ArXiv":"2108.05997","DBLP":"conf/iccv/KeWWMY21","DOI":"10.1109/ICCV48922.2021.00510","CorpusId":237048383},"title":"MUSIQ: Multi-scale Image Quality Transformer"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4","externalIds":{"DBLP":"journals/jmlr/HoSCFNS22","ArXiv":"2106.15282","CorpusId":235619773},"title":"Cascaded Diffusion Models for High Fidelity Image Generation"},{"paperId":"3618e503068e5f0e4f17ad1557a9bd6692daea79","externalIds":{"MAG":"3158432584","DBLP":"conf/iclr/TianRCO0MT21","ArXiv":"2104.15069","CorpusId":232275342},"title":"A Good Image Generator Is What You Need for High-Resolution Video Synthesis"},{"paperId":"2d9ae4c167510ed78803735fc57ea67c3cc55a35","externalIds":{"DBLP":"journals/corr/abs-2104-10157","ArXiv":"2104.10157","CorpusId":233307257},"title":"VideoGPT: Video Generation using VQ-VAE and Transformers"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"cc0736fed8d18736a920417b9d3f2c1c0fd4bbf0","externalIds":{"ArXiv":"2103.03319","DBLP":"journals/pami/JafarianP23","DOI":"10.1109/TPAMI.2022.3231558","CorpusId":240354231,"PubMed":"37015532"},"title":"Self-Supervised 3D Representation Learning of Dressed Humans From Social Media Videos"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"014576b866078524286802b1d0e18628520aa886","externalIds":{"ArXiv":"2010.02502","DBLP":"journals/corr/abs-2010-02502","MAG":"3092442149","CorpusId":222140788},"title":"Denoising Diffusion Implicit Models"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"1156e277fa7ec195b043161d3c5c97715da17658","externalIds":{"DBLP":"conf/nips/0011E20","ArXiv":"2006.09011","MAG":"3035384201","CorpusId":219708245},"title":"Improved Techniques for Training Score-Based Generative Models"},{"paperId":"3230e2d6b4671cc03974af2219c6d3270e6fac70","externalIds":{"MAG":"3013965544","DBLP":"conf/ijcai/Teed021","ArXiv":"2003.12039","DOI":"10.1007/978-3-030-58536-5_24","CorpusId":214667893},"title":"RAFT: Recurrent All-Pairs Field Transforms for Optical Flow"},{"paperId":"797389ca052efd160ed759d7ef7adf9c30a917d6","externalIds":{"DBLP":"journals/corr/abs-2003-00196","ArXiv":"2003.00196","MAG":"2970315999","CorpusId":202767986},"title":"First Order Motion Model for Image Animation"},{"paperId":"7af72a461ed7cda180e7eab878efd5f35d79bbf4","externalIds":{"DBLP":"conf/icml/ChenK0H20","MAG":"3034978746","ArXiv":"2002.05709","CorpusId":211096730},"title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"b5b01749791a5cc753b5f260963c5f79e1d32609","externalIds":{"MAG":"3087916743","DBLP":"journals/corr/abs-1912-08860","ArXiv":"1912.08860","DOI":"10.1016/J.NEUNET.2020.09.016","CorpusId":209414948,"PubMed":"33039788"},"title":"Lower Dimensional Kernels for Video Discriminators"},{"paperId":"12c37cb419121cdb43f2c6620303932f43e2e1b7","externalIds":{"MAG":"2976617189","CorpusId":204087649},"title":"Adversarial Video Generation on Complex Datasets"},{"paperId":"965359b3008ab50dd04e171551220ec0e7f83aba","externalIds":{"MAG":"2971034910","ArXiv":"1907.05600","DBLP":"conf/nips/SongE19","CorpusId":196470871},"title":"Generative Modeling by Estimating Gradients of the Data Distribution"},{"paperId":"b59233aab8364186603967bc12d88af48cc0992d","externalIds":{"DBLP":"journals/corr/abs-1812-01717","ArXiv":"1812.01717","MAG":"2902437806","CorpusId":54458806},"title":"Towards Accurate Generative Models of Video: A New Metric & Challenges"},{"paperId":"0c5f6d07b2a355312ba50132bab30832d1a4d883","externalIds":{"MAG":"3031246127","ArXiv":"1811.09245","DBLP":"journals/ijcv/SaitoSKK20","DOI":"10.1007/s11263-020-01333-y","CorpusId":218978582},"title":"Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN"},{"paperId":"62dccab9ab715f33761a5315746ed02e48eed2a0","externalIds":{"DBLP":"journals/corr/abs-1808-01340","MAG":"2887051120","ArXiv":"1808.01340","CorpusId":51927456},"title":"A Short Note about Kinetics-600"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"da2a88ecf2e1d17f2b2c33061127a74bdfc0a78a","externalIds":{"MAG":"2787289806","DBLP":"journals/cviu/Borji19","ArXiv":"1802.03446","DOI":"10.1016/J.CVIU.2018.10.009","CorpusId":3627712},"title":"Pros and Cons of GAN Evaluation Measures"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"87a818723a2ada66a1193baf17b0383d9766781b","externalIds":{"MAG":"2950466956","ArXiv":"1709.07592","DBLP":"journals/corr/abs-1709-07592","DOI":"10.1109/CVPR.2018.00251","CorpusId":1504491},"title":"Learning to Generate Time-Lapse Videos Using Multi-stage Dynamic Generative Adversarial Networks"},{"paperId":"e76edb86f270c3a77ed9f5a1e1b305461f36f96f","externalIds":{"MAG":"2951910147","DBLP":"conf/cvpr/Tulyakov0YK18","ArXiv":"1707.04993","DOI":"10.1109/CVPR.2018.00165","CorpusId":4475365},"title":"MoCoGAN: Decomposing Motion and Content for Video Generation"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","externalIds":{"MAG":"2963981733","DBLP":"conf/nips/HeuselRUNH17","CorpusId":326772},"title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"b61a3f8b80bbd44f24544dc915f52fd30bbdf485","externalIds":{"ArXiv":"1705.07750","MAG":"2619082050","DBLP":"conf/cvpr/CarreiraZ17","DOI":"10.1109/CVPR.2017.502","CorpusId":206596127},"title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"},{"paperId":"86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6","externalIds":{"DBLP":"journals/corr/KayCSZHVVGBNSZ17","ArXiv":"1705.06950","MAG":"2619947201","CorpusId":27300853},"title":"The Kinetics Human Action Video Dataset"},{"paperId":"062c41dad67bb68fefd9ff0c5c4d296e796004dc","externalIds":{"MAG":"2952493843","DBLP":"conf/iccv/SaitoMS17","ArXiv":"1611.06624","DOI":"10.1109/ICCV.2017.308","CorpusId":6945308},"title":"Temporal Generative Adversarial Nets with Singular Value Clipping"},{"paperId":"7fc464470b441c691d10e7331b14a525bc79b8bb","externalIds":{"DBLP":"conf/miccai/CicekALBR16","MAG":"2464708700","ArXiv":"1606.06650","DOI":"10.1007/978-3-319-46723-8_49","CorpusId":2164893},"title":"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation"},{"paperId":"571b0750085ae3d939525e62af510ee2cee9d5ea","externalIds":{"DBLP":"conf/nips/SalimansGZCRCC16","ArXiv":"1606.03498","MAG":"2949938177","CorpusId":1687220},"title":"Improved Techniques for Training GANs"},{"paperId":"b8e2e9f3ba008e28257195ec69a00e07f260131d","externalIds":{"DBLP":"conf/cvpr/XuMYR16","MAG":"2425121537","DOI":"10.1109/CVPR.2016.571","CorpusId":206594535},"title":"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","externalIds":{"MAG":"2183341477","DBLP":"conf/cvpr/SzegedyVISW16","ArXiv":"1512.00567","DOI":"10.1109/CVPR.2016.308","CorpusId":206593880},"title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"2dcef55a07f8607a819c21fe84131ea269cc2e3c","externalIds":{"MAG":"2129069237","DBLP":"journals/corr/Sohl-DicksteinW15","ArXiv":"1503.03585","CorpusId":14888175},"title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"},{"paperId":"d25c65d261ea0e6a458be4c50c40ffe5bc508f77","externalIds":{"MAG":"1522734439","DBLP":"conf/iccv/TranBFTP15","DOI":"10.1109/ICCV.2015.510","CorpusId":1122604},"title":"Learning Spatiotemporal Features with 3D Convolutional Networks"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","externalIds":{"ArXiv":"1409.0575","DBLP":"journals/corr/RussakovskyDSKSMHKKBBF14","MAG":"2546241758","DOI":"10.1007/s11263-015-0816-y","CorpusId":2930547},"title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"18fb0138e5e8522f55dce621d44dbc5024dace0d","externalIds":{"MAG":"1981781955","DBLP":"journals/mva/ReddyS13","DOI":"10.1007/s00138-012-0450-4","CorpusId":10875786},"title":"Recognizing 50 human action categories of web videos"},{"paperId":"da9e411fcf740569b6b356f330a1d0fc077c8d7c","externalIds":{"MAG":"24089286","ArXiv":"1212.0402","DBLP":"journals/corr/abs-1212-0402","CorpusId":7197134},"title":"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"},{"paperId":"184ac0766262312ba76bbdece4e7ffad0aa8180b","externalIds":{"DBLP":"journals/pami/BengioCV13","MAG":"2952111767","ArXiv":"1206.5538","DOI":"10.1109/TPAMI.2013.50","CorpusId":393948,"PubMed":"23787338"},"title":"Representation Learning: A Review and New Perspectives"},{"paperId":"872bae24c109f7c30e052ac218b17a8b028d08a0","externalIds":{"MAG":"2013035813","DBLP":"journals/neco/Vincent11","DOI":"10.1162/NECO_a_00142","CorpusId":5560643,"PubMed":"21492012"},"title":"A Connection Between Score Matching and Denoising Autoencoders"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"9966e890f2eedb4577e11b9d5a66380a4d9341fe","externalIds":{"DBLP":"journals/jmlr/Hyvarinen05","MAG":"1505878979","CorpusId":1152227},"title":"Estimation of Non-Normalized Statistical Models by Score Matching"},{"paperId":"eae2e0fa72e898c289365c0af16daf57a7a6cf40","externalIds":{"MAG":"2133665775","DBLP":"journals/tip/WangBSS04","DOI":"10.1109/TIP.2003.819861","CorpusId":207761262,"PubMed":"15376593"},"title":"Image quality assessment: from error visibility to structural similarity"},{"paperId":"7af366c2d5b1a46a872adda781235845bd671f03","externalIds":{"DBLP":"journals/corr/abs-2403-14468","DOI":"10.48550/arXiv.2403.14468","CorpusId":279586284},"title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks"},{"paperId":"90428f3a8caa5082f825ebf3138514ddf273dae3","externalIds":{"CorpusId":253581838},"title":"Supplementary Materials for: NULL-text Inversion for Editing Real Images using Guided Diffusion Models"},{"paperId":"2d66cfa143ec6a600020762ec0c22dcc5f5a4be2","externalIds":{"DBLP":"journals/corr/abs-2312-04557","DOI":"10.48550/arXiv.2312.04557","CorpusId":271403848},"title":"GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation"},{"paperId":"735fcf085059f419112b76f7217e7f1407efcbb0","externalIds":{"DBLP":"journals/corr/abs-2211-13221","DOI":"10.48550/arXiv.2211.13221","CorpusId":253802030},"title":"Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths"},{"paperId":"775f42ed458b8c5b0f2094ea4ff5b64c557b1a34","externalIds":{"CorpusId":251881108},"title":"A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"7449f8982f9de36faf9d34bca680444ab827657c","externalIds":{"CorpusId":122512502},"title":"The Frkhet Distance between Multivariate Normal Distributions"},{"paperId":"d53bcbac7ea19173e95d3bd855b998fab765737d","externalIds":{"MAG":"1509982784","CorpusId":57814228},"title":"WordNet: An Electronic Lexical Database"}]}