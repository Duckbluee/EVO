{"references":[{"paperId":"3637afb75e5b83803aff08a3ffdd35b02fa93775","externalIds":{"ArXiv":"2406.01288","DBLP":"journals/corr/abs-2406-01288","DOI":"10.48550/arXiv.2406.01288","CorpusId":270213981},"title":"Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses"},{"paperId":"bcb694c1fbf55ce884cbe89ff7c489f933586ef7","externalIds":{"DBLP":"journals/corr/abs-2404-16369","ArXiv":"2404.16369","DOI":"10.48550/arXiv.2404.16369","CorpusId":269362721},"title":"Don't Say No: Jailbreaking LLM by Suppressing Refusal"},{"paperId":"cf56a7b28fb27279b1c94fb920b5722cf50c8852","externalIds":{"DBLP":"journals/corr/abs-2404-16873","ArXiv":"2404.16873","DOI":"10.48550/arXiv.2404.16873","CorpusId":269430799},"title":"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs"},{"paperId":"88d5634a52645f6b05a03536be1f26a2b9bba232","externalIds":{"DBLP":"conf/iclr/AndriushchenkoC25","ArXiv":"2404.02151","DOI":"10.48550/arXiv.2404.02151","CorpusId":268857047},"title":"Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks"},{"paperId":"3af84f2fa373896f06886f3807526c4a70eea5ca","externalIds":{"ArXiv":"2404.00495","DBLP":"journals/corr/abs-2404-00495","DOI":"10.48550/arXiv.2404.00495","CorpusId":268819146},"title":"Configurable Safety Tuning of Language Models with Synthetic Preference Data"},{"paperId":"55eed6f9ede6c4187d849224d61e60fda73a54df","externalIds":{"ArXiv":"2403.12171","DBLP":"journals/corr/abs-2403-12171","DOI":"10.48550/arXiv.2403.12171","CorpusId":268531982},"title":"EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models"},{"paperId":"220291492a71f3a93889f2d759f33dbe1e7f7238","externalIds":{"DBLP":"journals/corr/abs-2403-02475","ArXiv":"2403.02475","DOI":"10.48550/arXiv.2403.02475","CorpusId":268247648},"title":"Enhancing LLM Safety via Constrained Direct Preference Optimization"},{"paperId":"8ba57771dd6345821a0cbe83c4c7eb50f66b7b65","externalIds":{"ArXiv":"2403.04783","DBLP":"journals/corr/abs-2403-04783","DOI":"10.48550/arXiv.2403.04783","CorpusId":268297202},"title":"AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks"},{"paperId":"60f653b14da8cc6866e5062a9700bb5f85a387ca","externalIds":{"ArXiv":"2403.00867","DBLP":"conf/nips/HuCH24","DOI":"10.48550/arXiv.2403.00867","CorpusId":268230819},"title":"Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes"},{"paperId":"f6fa682b62c7981402336ca57da1196ccbf3fc54","externalIds":{"DBLP":"conf/uss/LiuZZDM024","ArXiv":"2402.18104","DOI":"10.48550/arXiv.2402.18104","CorpusId":268041845},"title":"Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction"},{"paperId":"72f51c3ef967f7905e3194296cf6fd8337b1a437","externalIds":{"DBLP":"journals/corr/abs-2402-16717","ArXiv":"2402.16717","DOI":"10.48550/arXiv.2402.16717","CorpusId":268032340},"title":"CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"},{"paperId":"09e2391ab4c266f96c71cc379c1a7c7ddd40ee33","externalIds":{"DBLP":"conf/emnlp/LiWCZH24","ArXiv":"2402.16914","DOI":"10.48550/arXiv.2402.16914","CorpusId":268031868},"title":"DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers"},{"paperId":"1f9f25aad947030fe3206114fa2ac75e8b590515","externalIds":{"ArXiv":"2402.16192","DBLP":"journals/corr/abs-2402-16192","DOI":"10.48550/arXiv.2402.16192","CorpusId":267938320},"title":"Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing"},{"paperId":"b23cf88fd5a99ae68618a3b64d3cd3b3c0c6a65f","externalIds":{"DBLP":"journals/corr/abs-2402-15911","ArXiv":"2402.15911","DOI":"10.48550/arXiv.2402.15911","CorpusId":267938152},"title":"PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails"},{"paperId":"d0746668f85fcdbe00306bd7486a24f8750207d5","externalIds":{"DBLP":"journals/corr/abs-2402-15302","ArXiv":"2402.15302","DOI":"10.48550/arXiv.2402.15302","CorpusId":267897715},"title":"How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries"},{"paperId":"d82a827fa21cce0b601ad21b6db606b8413ed547","externalIds":{"DBLP":"journals/corr/abs-2402-15180","ArXiv":"2402.15180","DOI":"10.48550/arXiv.2402.15180","CorpusId":267897711},"title":"Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement"},{"paperId":"4ebfb0589ba587d6912661c1fe1082db705476af","externalIds":{"DBLP":"journals/corr/abs-2402-14872","ArXiv":"2402.14872","DOI":"10.48550/arXiv.2402.14872","CorpusId":267897371},"title":"Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs"},{"paperId":"4699f20628ebb318005b3cce9005c78847ca6daf","externalIds":{"ArXiv":"2402.14020","DBLP":"journals/corr/abs-2402-14020","DOI":"10.48550/arXiv.2402.14020","CorpusId":267770475},"title":"Coercing LLMs to do and reveal (almost) anything"},{"paperId":"0a691e58a36cdcdaaf72294e88420f79e61e85c7","externalIds":{"DBLP":"journals/corr/abs-2402-11753","ArXiv":"2402.11753","DOI":"10.48550/arXiv.2402.11753","CorpusId":267750708},"title":"ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"},{"paperId":"09f78ccd20f865c288b421376baddb0096c651be","externalIds":{"ArXiv":"2402.11755","DBLP":"journals/corr/abs-2402-11755","DOI":"10.48550/arXiv.2402.11755","CorpusId":267750421},"title":"SPML: A DSL for Defending Language Models Against Prompt Attacks"},{"paperId":"17c64281e2b9947c7f362b213162c5f1331d0002","externalIds":{"ArXiv":"2402.10601","CorpusId":267740378},"title":"When\"Competency\"in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers"},{"paperId":"ef5da08aad746173b7ddf589068c6abf00205fea","externalIds":{"ArXiv":"2402.09674","DBLP":"journals/corr/abs-2402-09674","DOI":"10.48550/arXiv.2402.09674","CorpusId":267682038},"title":"PAL: Proxy-Guided Black-Box Attack on Large Language Models"},{"paperId":"f2e88c26bc1ebdd4adc5f83ab56cb4276120745d","externalIds":{"ArXiv":"2402.10260","DBLP":"conf/nips/SoulyLBTHPASEWT24","DOI":"10.48550/arXiv.2402.10260","CorpusId":267740669},"title":"A StrongREJECT for Empty Jailbreaks"},{"paperId":"37560b8b09f7c859ba45cbadaf712f2c38727dbe","externalIds":{"ArXiv":"2402.09132","DBLP":"journals/corr/abs-2402-09132","DOI":"10.48550/arXiv.2402.09132","CorpusId":267658087},"title":"Exploring the Adversarial Capabilities of Large Language Models"},{"paperId":"6a2e7da48e3588feee6e5dceceae3cd87bd02ae9","externalIds":{"ArXiv":"2402.09154","DBLP":"journals/corr/abs-2402-09154","DOI":"10.48550/arXiv.2402.09154","CorpusId":267657696},"title":"Attacking Large Language Models with Projected Gradient Descent"},{"paperId":"2139e414bdf6a5ea7ec4052d3f65a8d49991494b","externalIds":{"DBLP":"journals/corr/abs-2402-08983","ArXiv":"2402.08983","DOI":"10.48550/arXiv.2402.08983","CorpusId":267658033},"title":"SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"},{"paperId":"490e815b3be11ba97631783d9ae946b8f8517fd6","externalIds":{"DBLP":"conf/acl/ChangLLWWL24","ArXiv":"2402.09091","DOI":"10.48550/arXiv.2402.09091","CorpusId":267657689},"title":"Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues"},{"paperId":"a2a4ddbed34916cfa345e957cf060da99685e37b","externalIds":{"DBLP":"journals/corr/abs-2402-08416","ArXiv":"2402.08416","DOI":"10.48550/arXiv.2402.08416","CorpusId":267636904},"title":"Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning"},{"paperId":"b7ef6182f617ef3e7cc9682f562f794115a4c62c","externalIds":{"DBLP":"conf/icml/GuoYZQ024","ArXiv":"2402.08679","DOI":"10.48550/arXiv.2402.08679","CorpusId":267637251},"title":"COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability"},{"paperId":"3ff40bca6276bfe646b684f3dddf9dbab1610424","externalIds":{"DBLP":"conf/acl/ChuL000Z25","ArXiv":"2402.05668","DOI":"10.18653/v1/2025.acl-long.1045","CorpusId":267547966},"title":"JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs"},{"paperId":"b82ccc66c14f531a444c74d2a9a9d86a86a8be99","externalIds":{"ArXiv":"2402.04249","DBLP":"conf/icml/MazeikaPYZ0MSLB24","DOI":"10.48550/arXiv.2402.04249","CorpusId":267499790},"title":"HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal"},{"paperId":"c7830808c6342d673ebc318a179195018d3b5d11","externalIds":{"ArXiv":"2402.03299","DBLP":"journals/corr/abs-2402-03299","DOI":"10.48550/arXiv.2402.03299","CorpusId":267411743},"title":"GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models"},{"paperId":"b1f243b586e87fe12ff8fe1a501f11ea5fc5ad44","externalIds":{"ArXiv":"2401.18018","DBLP":"conf/icml/ZhengY0M0CHP24","CorpusId":267334949},"title":"On Prompt-Driven Safeguarding for Large Language Models"},{"paperId":"88d59e31575f5b3dd88a2c2033b55f628c2adbc9","externalIds":{"DBLP":"journals/corr/abs-2401-17256","ArXiv":"2401.17256","DOI":"10.48550/arXiv.2401.17256","CorpusId":267320277},"title":"Weak-to-Strong Jailbreaking on Large Language Models"},{"paperId":"dd35405d8e562fa1df4338839878e9c94817cfdd","externalIds":{"DBLP":"journals/corr/abs-2401-17263","ArXiv":"2401.17263","DOI":"10.48550/arXiv.2401.17263","CorpusId":267320750},"title":"Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks"},{"paperId":"c44622a46fac5fcd6296358d8c0e06efc93a7767","externalIds":{"DBLP":"journals/corr/abs-2401-16765","ArXiv":"2401.16765","DOI":"10.48550/arXiv.2401.16765","CorpusId":267320768},"title":"A Cross-Language Investigation into Jailbreak Attacks in Large Language Models"},{"paperId":"684ede7013b3d844dae13090350a5c27b196727f","externalIds":{"DBLP":"conf/acl/ZhangZLSG0WLZ24","ArXiv":"2401.11880","DOI":"10.48550/arXiv.2401.11880","CorpusId":267069372},"title":"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety"},{"paperId":"f3ba2736c795a7f01765360989848c6144add1c7","externalIds":{"ArXiv":"2401.09798","DBLP":"journals/corr/abs-2401-09798","DOI":"10.3390/app14093558","CorpusId":267034876},"title":"All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks"},{"paperId":"8fd29e810540c40846cddce3cbdf5060cd59fb57","externalIds":{"DBLP":"conf/coling/Zhang0ZT25","ArXiv":"2401.06561","CorpusId":266977251},"title":"Intention Analysis Makes LLMs A Good Jailbreak Defender"},{"paperId":"732ce53c573475f2691a7cfc716cf4f568d17360","externalIds":{"DBLP":"conf/acl/0005LZY0S24","ArXiv":"2401.06373","DOI":"10.48550/arXiv.2401.06373","CorpusId":266977395},"title":"How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs"},{"paperId":"952b2e2aadeec56ee4cffbe23f9f70bcfcb14b53","externalIds":{"DBLP":"journals/corr/abs-2312-10982","ArXiv":"2312.10982","DOI":"10.48550/arXiv.2312.10982","CorpusId":266359311},"title":"A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models"},{"paperId":"ac3ae10014337879bb566dee8b6ae840e3897384","externalIds":{"DBLP":"conf/iclr/SiththaranjanLH24","ArXiv":"2312.08358","DOI":"10.48550/arXiv.2312.08358","CorpusId":266191810},"title":"Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF"},{"paperId":"89641466373aa9ce2976e3f384b0791a7bd0931c","externalIds":{"ArXiv":"2312.04782","DBLP":"journals/corr/abs-2312-04782","DOI":"10.48550/arXiv.2312.04782","CorpusId":266149700},"title":"Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs"},{"paperId":"d88ed3e41cc248229b505a0c37f4fd7d39238ba4","externalIds":{"ArXiv":"2312.04127","DBLP":"journals/corr/abs-2312-04127","DOI":"10.48550/arXiv.2312.04127","CorpusId":266045606},"title":"Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak"},{"paperId":"14e8cf5a5e6a7b35e618b08f5cf06f572b3a54e0","externalIds":{"ArXiv":"2312.02119","DBLP":"conf/nips/MehrotraZKNASK24","DOI":"10.48550/arXiv.2312.02119","CorpusId":265609901},"title":"Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"},{"paperId":"383c598625110e0a4c60da4db10a838ef822fbcf","externalIds":{"ArXiv":"2312.02003","DBLP":"journals/corr/abs-2312-02003","DOI":"10.1016/j.hcc.2024.100211","CorpusId":265609409},"title":"A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"},{"paperId":"9045649163477319dafba4403dc915c3388dceda","externalIds":{"ArXiv":"2311.14876","DBLP":"conf/bigdataconf/SinghAN23","DOI":"10.1109/BigData59044.2023.10386814","CorpusId":265457196},"title":"Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles"},{"paperId":"263a58f4fd32caca1dad2351af4d711aec451fe6","externalIds":{"ArXiv":"2311.11855","DBLP":"journals/corr/abs-2311-11855","DOI":"10.48550/arXiv.2311.11855","CorpusId":265294905},"title":"Evil Geniuses: Delving into the Safety of LLM-based Agents"},{"paperId":"c4ff1be5c254b60b96b7455eefcc4ec9583f82ed","externalIds":{"ACL":"2024.naacl-long.118","ArXiv":"2311.08268","DBLP":"conf/naacl/DingKMCXCH24","DOI":"10.48550/arXiv.2311.08268","CorpusId":265664913},"title":"A Wolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"},{"paperId":"709af143f78bc62413c50ea1a7ee75b0702c4f59","externalIds":{"ACL":"2024.naacl-long.107","ArXiv":"2311.07689","DBLP":"journals/corr/abs-2311-07689","DOI":"10.48550/arXiv.2311.07689","CorpusId":265157927},"title":"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming"},{"paperId":"b78b5ce5f21f46d8149824463f8eebd6103d49aa","externalIds":{"ArXiv":"2311.05608","DBLP":"journals/corr/abs-2311-05608","DOI":"10.48550/arXiv.2311.05608","CorpusId":265067328},"title":"FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"},{"paperId":"ccae9fcb1f344e56a3f7cb05a4b49a6e658f9dd2","externalIds":{"DBLP":"journals/corr/abs-2311-05553","ArXiv":"2311.05553","ACL":"2024.naacl-short.59","DOI":"10.48550/arXiv.2311.05553","CorpusId":265067269},"title":"Removing RLHF Protections in GPT-4 via Fine-Tuning"},{"paperId":"99eee47dad469790042b973bc7cb40cb63a660b1","externalIds":{"DBLP":"journals/corr/abs-2311-03191","ArXiv":"2311.03191","DOI":"10.48550/arXiv.2311.03191","CorpusId":265033222},"title":"DeepInception: Hypnotize Large Language Model to Be Jailbreaker"},{"paperId":"bfc0e3e651cd4b715272fe68add8a180a112293c","externalIds":{"ArXiv":"2311.03348","DBLP":"journals/corr/abs-2311-03348","DOI":"10.48550/arXiv.2311.03348","CorpusId":265043220},"title":"Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation"},{"paperId":"d1b5151231a790c7a60f620e21860593dae9a1c5","externalIds":{"DBLP":"journals/corr/abs-2310-20624","ArXiv":"2310.20624","DOI":"10.48550/arXiv.2310.20624","CorpusId":264808400},"title":"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"},{"paperId":"1227c2fcb8437441b7d72a29a4bc9eef1f5275d2","externalIds":{"ArXiv":"2310.15140","CorpusId":264451545},"title":"AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models"},{"paperId":"e314d182fd9d35a05870b38a56ee38eb3149b47d","externalIds":{"ArXiv":"2310.12505","DBLP":"conf/emnlp/DengWFDW023","DOI":"10.48550/arXiv.2310.12505","CorpusId":264306378},"title":"Attack Prompt Generation for Red Teaming and Defending Large Language Models"},{"paperId":"4f63c5a89c7299a864c6c48aa1844fb0fe8c9437","externalIds":{"ArXiv":"2310.10844","DBLP":"journals/corr/abs-2310-10844","DOI":"10.48550/arXiv.2310.10844","CorpusId":264172191},"title":"Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"},{"paperId":"4637f79ddfaf923ce569996ffa5b6cda1996faa1","externalIds":{"DBLP":"conf/satml/ChaoRDHP025","ArXiv":"2310.08419","DOI":"10.1109/SaTML64287.2025.00010","CorpusId":263908890},"title":"Jailbreaking Black Box Large Language Models in Twenty Queries"},{"paperId":"ac27dd71af3ee93e1129482ceececbae7dd0d0e8","externalIds":{"DBLP":"journals/corr/abs-2310-06987","ArXiv":"2310.06987","DOI":"10.48550/arXiv.2310.06987","CorpusId":263835408},"title":"Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"},{"paperId":"6b135e922a0c673aeb0b05c5aeecdb6c794791c6","externalIds":{"DBLP":"journals/corr/abs-2310-06387","ArXiv":"2310.06387","DOI":"10.48550/arXiv.2310.06387","CorpusId":263830179,"PubMed":"41628045"},"title":"Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations"},{"paperId":"1a9f394b5b7f5bcdecee487174a3f4fc65d30e33","externalIds":{"ArXiv":"2310.06474","DBLP":"conf/iclr/0010ZPB24","DOI":"10.48550/arXiv.2310.06474","CorpusId":263831094},"title":"Multilingual Jailbreak Challenges in Large Language Models"},{"paperId":"8cf9b49698fdb1b754df2556576412a7b44929f6","externalIds":{"DBLP":"journals/tmlr/Robey0HP25","ArXiv":"2310.03684","DOI":"10.48550/arXiv.2310.03684","CorpusId":263671542},"title":"SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"},{"paperId":"0e0e706e13f160e74cac9556f28ab9a358c148d2","externalIds":{"DBLP":"journals/corr/abs-2310-03693","ArXiv":"2310.03693","DOI":"10.48550/arXiv.2310.03693","CorpusId":263671523},"title":"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"},{"paperId":"84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed","externalIds":{"ArXiv":"2310.02949","DBLP":"journals/corr/abs-2310-02949","DOI":"10.48550/arXiv.2310.02949","CorpusId":263620436},"title":"Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models"},{"paperId":"f3f23f7f9f5369aade19f20bc5d028cce7b9c9aa","externalIds":{"ArXiv":"2310.04451","DBLP":"journals/corr/abs-2310-04451","DOI":"10.48550/arXiv.2310.04451","CorpusId":263831566},"title":"AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"},{"paperId":"764fc56883bf83392cac99a7b5a264ac9fe2cdc5","externalIds":{"ArXiv":"2310.02446","DBLP":"journals/corr/abs-2310-02446","DOI":"10.48550/arXiv.2310.02446","CorpusId":263620377},"title":"Low-Resource Languages Jailbreak GPT-4"},{"paperId":"66bc83b4b321cac7084cc51b3c6178d9de54e9c1","externalIds":{"ArXiv":"2309.11830","CorpusId":262084103},"title":"Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"},{"paperId":"d4177489596748e43aa571f59556097f2cc4c8be","externalIds":{"ArXiv":"2309.10253","DBLP":"journals/corr/abs-2309-10253","DOI":"10.48550/arXiv.2309.10253","CorpusId":262055242},"title":"GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts"},{"paperId":"cd29c25c489562b409a60f83365f93f33ee1a0a1","externalIds":{"DBLP":"journals/corr/abs-2309-14348","ArXiv":"2309.14348","DOI":"10.48550/arXiv.2309.14348","CorpusId":262827619},"title":"Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"},{"paperId":"a4c921bdef167ae54cc3a40643e6e3ed13d49a61","externalIds":{"DBLP":"conf/iclr/0001SARJH024","ArXiv":"2309.07875","DOI":"10.48550/arXiv.2309.07875","CorpusId":261823321},"title":"Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions"},{"paperId":"9b9a4fa3ed510fc6eb1bf831979235f3d9f8b556","externalIds":{"ArXiv":"2309.07045","DBLP":"conf/acl/ZhangLWSHL0L0H24","DOI":"10.48550/arXiv.2309.07045","CorpusId":261706197},"title":"SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions"},{"paperId":"b574245f3db22b5eb7fe64bd8b0a147dab467b60","externalIds":{"DBLP":"conf/iclr/LiWZ0024","ArXiv":"2309.07124","DOI":"10.48550/arXiv.2309.07124","CorpusId":261705563},"title":"RAIN: Your Language Models Can Align Themselves without Finetuning"},{"paperId":"3c784cd3150a359e269c70cfbadd18774d66055d","externalIds":{"DBLP":"journals/corr/abs-2309-05274","ArXiv":"2309.05274","DOI":"10.1109/ICASSP48485.2024.10448041","CorpusId":261681918},"title":"FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models"},{"paperId":"1ab91d6ac7afc1a0121487a9089fa70edc1634d4","externalIds":{"DBLP":"journals/corr/abs-2309-02705","ArXiv":"2309.02705","DOI":"10.48550/arXiv.2309.02705","CorpusId":261557007},"title":"Certifying LLM Safety against Adversarial Prompting"},{"paperId":"f846c0c59608f0a8ff18f4c52adba87bf49dc229","externalIds":{"ArXiv":"2309.01446","DBLP":"journals/corr/abs-2309-01446","DOI":"10.48550/arXiv.2309.01446","CorpusId":261530019},"title":"Open Sesame! Universal Black Box Jailbreaking of Large Language Models"},{"paperId":"3e30a7ac4886b28eb50151f58e14a1d698cccd0e","externalIds":{"ArXiv":"2309.00614","DBLP":"journals/corr/abs-2309-00614","DOI":"10.48550/arXiv.2309.00614","CorpusId":261494182},"title":"Baseline Defenses for Adversarial Attacks Against Aligned Language Models"},{"paperId":"b8b8d5655df1c6a71bbb713387863e34cc055332","externalIds":{"DBLP":"journals/corr/abs-2308-14132","ArXiv":"2308.14132","DOI":"10.48550/arXiv.2308.14132","CorpusId":261245172},"title":"Detecting Language Model Attacks with Perplexity"},{"paperId":"288063323dddad9bea7eb1230a2048546435687e","externalIds":{"ArXiv":"2308.13387","DBLP":"journals/corr/abs-2308-13387","DOI":"10.48550/arXiv.2308.13387","CorpusId":261214837},"title":"Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs"},{"paperId":"9f859726b3d8dffd96a1f55de4122617751cc1b4","externalIds":{"ArXiv":"2308.09662","DBLP":"journals/corr/abs-2308-09662","DOI":"10.48550/arXiv.2308.09662","CorpusId":261030829},"title":"Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment"},{"paperId":"838cd69a0b6c9c244a6eebb0f4742c0625132de6","externalIds":{"DBLP":"journals/corr/abs-2308-08747","ArXiv":"2308.08747","DOI":"10.1109/TASLPRO.2025.3606231","CorpusId":261031244},"title":"An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-Tuning"},{"paperId":"897940fb5dd4d739b88c4659c4565d05f48d06b8","externalIds":{"DBLP":"conf/iclr/YuanJW0H0T24","ArXiv":"2308.06463","DOI":"10.48550/arXiv.2308.06463","CorpusId":260887189},"title":"GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"},{"paperId":"1104d766527dead44a40532e8a89444d9cef5c65","externalIds":{"DBLP":"journals/corr/abs-2308-03825","ArXiv":"2308.03825","DOI":"10.1145/3658644.3670388","CorpusId":260704242},"title":"\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"},{"paperId":"b67eb8213a63be8a4b0274728ffdc50bfa109e10","externalIds":{"ArXiv":"2308.01263","ACL":"2024.naacl-long.301","DBLP":"conf/naacl/RottgerKVA0H24","DOI":"10.48550/arXiv.2308.01263","CorpusId":260378842},"title":"XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models"},{"paperId":"47030369e97cc44d4b2e3cf1be85da0fd134904a","externalIds":{"DBLP":"journals/corr/abs-2307-15043","ArXiv":"2307.15043","CorpusId":260202961},"title":"Universal and Transferable Adversarial Attacks on Aligned Language Models"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"ace98e1e58bcc364afbb2feff6d136232f5f47da","externalIds":{"ArXiv":"2307.08487","DBLP":"journals/corr/abs-2307-08487","DOI":"10.48550/arXiv.2307.08487","CorpusId":259937347},"title":"Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models"},{"paperId":"d62c4d00b277e948956b6610ce2644e88fe1577b","externalIds":{"DBLP":"journals/cacm/Cerf23c","ArXiv":"2307.05782","DOI":"10.1007/978-981-96-6259-3","CorpusId":259837466,"PubMed":"38320147"},"title":"Large Language Models"},{"paperId":"92930ed3560ea6c86d53cf52158bc793b089054d","externalIds":{"ArXiv":"2307.04657","DBLP":"conf/nips/JiLDPZB0SW023","DOI":"10.48550/arXiv.2307.04657","CorpusId":259501579},"title":"BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"},{"paperId":"929305892d4ddae575a0fc23227a8139f7681632","externalIds":{"DBLP":"journals/corr/abs-2307-02483","ArXiv":"2307.02483","DOI":"10.48550/arXiv.2307.02483","CorpusId":259342528},"title":"Jailbroken: How Does LLM Safety Training Fail?"},{"paperId":"317ad53bea6fb603c20f692bb2f1a01e2dc86161","externalIds":{"ArXiv":"2307.00691","DBLP":"journals/access/GuptaAAPP23","DOI":"10.1109/ACCESS.2023.3300381","CorpusId":259316122},"title":"From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy"},{"paperId":"1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642","externalIds":{"ArXiv":"2306.09442","DBLP":"journals/corr/abs-2306-09442","DOI":"10.48550/arXiv.2306.09442","CorpusId":259187620},"title":"Explore, Establish, Exploit: Red Teaming Language Models from Scratch"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"1abfc211793c683972ded8d3268475e3ee7a88b0","externalIds":{"DBLP":"journals/corr/abs-2305-14950","ArXiv":"2305.14950","DOI":"10.48550/arXiv.2305.14950","CorpusId":258865399},"title":"Adversarial Demonstration Attacks on Large Language Models"},{"paperId":"fc50a6202e2f675604543c1ae4ef22ec74f61ad5","externalIds":{"ArXiv":"2305.13860","DBLP":"journals/corr/abs-2305-13860","DOI":"10.48550/arXiv.2305.13860","CorpusId":258841501},"title":"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study"},{"paperId":"e01515c6138bc525f7aec30fc85f2adf028d4156","externalIds":{"DBLP":"journals/corr/abs-2305-03047","ArXiv":"2305.03047","DOI":"10.48550/arXiv.2305.03047","CorpusId":258479665},"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"},{"paperId":"59fc49dfd81b92661437eaf7e339c0792ccd8755","externalIds":{"ArXiv":"2304.10436","DBLP":"journals/corr/abs-2304-10436","DOI":"10.48550/arXiv.2304.10436","CorpusId":258236069},"title":"Safety Assessment of Chinese Large Language Models"},{"paperId":"025ca4c125d6ecabc816a56f160e5c992abc76d9","externalIds":{"DBLP":"journals/corr/abs-2304-05197","ArXiv":"2304.05197","DOI":"10.48550/arXiv.2304.05197","CorpusId":258060250},"title":"Multi-step Jailbreaking Privacy Attacks on ChatGPT"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"2f94f03fdac62d05f0f416b7b3855d1f597afee9","externalIds":{"DBLP":"journals/corr/abs-2303-04381","ArXiv":"2303.04381","DOI":"10.48550/arXiv.2303.04381","CorpusId":257405439},"title":"Automatically Auditing Large Language Models via Discrete Optimization"},{"paperId":"0cf694b8f85ab2e11d45595de211a15cfbadcd22","externalIds":{"DBLP":"journals/corr/abs-2302-05733","ArXiv":"2302.05733","DOI":"10.1109/SPW63631.2024.00018","CorpusId":256827239},"title":"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"},{"paperId":"17bcb1edbe068e8fe6a97da552c70a77a15bbce7","externalIds":{"DBLP":"journals/corr/abs-2209-07858","ArXiv":"2209.07858","DOI":"10.48550/arXiv.2209.07858","CorpusId":252355458},"title":"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"4a425e98a656412e21c67f0d9bcd956730fba340","externalIds":{"MAG":"628267102","DOI":"10.1163/9789004660373","CorpusId":190932133},"title":"The Bad and the Ugly"},{"paperId":"c2cfa4d147d3f1e1fcf9584a7d4947321b4160c6","externalIds":{"DBLP":"journals/corr/abs-2402-14968","DOI":"10.48550/arXiv.2402.14968","CorpusId":271908478},"title":"Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment"},{"paperId":"0015e5dd084475781302040504585a229df7a5db","externalIds":{"DBLP":"journals/corr/abs-2402-16006","DOI":"10.48550/arXiv.2402.16006","CorpusId":276435916},"title":"From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings"},{"paperId":"6c6154e73f2a5d7aeb09ba7b9fa8742ab64d1e1b","externalIds":{"DBLP":"journals/corr/abs-2305-14965","DOI":"10.48550/arXiv.2305.14965","CorpusId":258865314},"title":"Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks"},{"paperId":"6b17231df7cc4c30fa5e1ea7eb9df0d4875caeac","externalIds":{"DBLP":"journals/corr/abs-2312-10766","DOI":"10.48550/arXiv.2312.10766","CorpusId":271325522},"title":"A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection"},{"paperId":"d347fb65bd375771b013c517c861db926322e410","externalIds":{"DBLP":"conf/emnlp/SchulhoffPKBSAT23","DOI":"10.18653/v1/2023.emnlp-main.302","CorpusId":266163876},"title":"Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition"}]}