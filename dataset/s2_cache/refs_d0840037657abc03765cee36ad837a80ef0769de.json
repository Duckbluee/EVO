{"references":[{"paperId":"12c47f0ca75b1b410c6c5ebeb6cdae187497f452","externalIds":{"ArXiv":"2407.06027","DBLP":"journals/corr/abs-2407-06027","DOI":"10.48550/arXiv.2407.06027","CorpusId":271050270},"title":"PAS: Data-Efficient Plug-and-Play Prompt Augmentation System"},{"paperId":"90d464a20585cbf1c3e1a8a4e6c9276c3eca6bfd","externalIds":{"ArXiv":"2407.03104","DBLP":"journals/corr/abs-2407-03104","DOI":"10.48550/arXiv.2407.03104","CorpusId":270924158},"title":"KeyVideoLLM: Towards Large-scale Video Keyframe Selection"},{"paperId":"84f355b1172c7b0d6ceceaf47faa281ed1c4f166","externalIds":{"ArXiv":"2407.01937","DBLP":"journals/corr/abs-2407-01937","DOI":"10.48550/arXiv.2407.01937","CorpusId":270878751},"title":"Efficient-Empathy: Towards Efficient and Effective Selection of Empathy Data"},{"paperId":"e8053a7b234d2936cbc993a4f2b13ec63eb6ec90","externalIds":{"DBLP":"conf/iclr/LiuZMZDPJL25","ArXiv":"2407.01492","DOI":"10.48550/arXiv.2407.01492","CorpusId":270870508},"title":"RegMix: Data Mixture as Regression for Language Model Pre-training"},{"paperId":"94773f22b5befd0e167a7de525d29bec2b09937a","externalIds":{"ArXiv":"2406.16860","DBLP":"journals/corr/abs-2406-16860","DOI":"10.48550/arXiv.2406.16860","CorpusId":270703300},"title":"Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs"},{"paperId":"5d3eacfd4e44dd5fa88026edc5848f8bd5efea0e","externalIds":{"ArXiv":"2406.08418","DBLP":"journals/corr/abs-2406-08418","DOI":"10.48550/arXiv.2406.08418","CorpusId":270391186},"title":"OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text"},{"paperId":"e9fadb414d5aa10eea5f3e877356a9b4f64d6e7a","externalIds":{"ArXiv":"2405.14908","CorpusId":270045202},"title":"BiMix: A Bivariate Data Mixing Law for Language Model Pretraining"},{"paperId":"1c326499778585bf7e5629a130a830eed4f1d729","externalIds":{"DBLP":"conf/cvpr/GoyalMLRK24","ArXiv":"2404.07177","DOI":"10.1109/CVPR52733.2024.02142","CorpusId":269033049},"title":"Scaling Laws for Data Filtering—Data Curation Cannot be Compute Agnostic"},{"paperId":"c231dcc7f85e52c2a0e225022a4b755d472b65bb","externalIds":{"DBLP":"journals/corr/abs-2403-16952","ArXiv":"2403.16952","DOI":"10.48550/arXiv.2403.16952","CorpusId":268681464},"title":"Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance"},{"paperId":"6675bcf6dc97c87da7afda223938ec7e51ecc3b2","externalIds":{"ArXiv":"2403.09611","DBLP":"journals/corr/abs-2403-09611","CorpusId":268384865},"title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"},{"paperId":"1e0049ecb981fa356f984641585b21fd19e4e254","externalIds":{"DBLP":"journals/corr/abs-2403-04473","ArXiv":"2403.04473","DOI":"10.48550/arXiv.2403.04473","CorpusId":268264810},"title":"TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document"},{"paperId":"952a2ecdc3e8938e5bac718206f6355fda029a79","externalIds":{"ArXiv":"2403.02677","DBLP":"journals/corr/abs-2403-02677","DOI":"10.48550/arXiv.2403.02677","CorpusId":268249158},"title":"Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters"},{"paperId":"8eae862d9669e7001eeee17b49fba793df9672c4","externalIds":{"ArXiv":"2402.19479","DBLP":"conf/cvpr/ChenSMDCJF0RYT24","DOI":"10.1109/CVPR52733.2024.01265","CorpusId":268091168},"title":"Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers"},{"paperId":"5aa4e4d90cac81f8ec7001ae25356d75f02efbb1","externalIds":{"ArXiv":"2402.18041","DBLP":"journals/corr/abs-2402-18041","DOI":"10.48550/arXiv.2402.18041","CorpusId":268041439},"title":"Datasets for Large Language Models: A Comprehensive Survey"},{"paperId":"5e71d0e85f65a1c0fb2af7bff281209122c58932","externalIds":{"ArXiv":"2402.17193","DBLP":"conf/iclr/0006LCF24","DOI":"10.48550/arXiv.2402.17193","CorpusId":268032247},"title":"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method"},{"paperId":"e95bb39748a497fbeed1b221fb3d1296c2b1eec2","externalIds":{"ArXiv":"2402.16827","DBLP":"journals/tmlr/AlbalakEXLL0MHP24","DOI":"10.48550/arXiv.2402.16827","CorpusId":268032975},"title":"A Survey on Data Selection for Language Models"},{"paperId":"1ff5422e92e9691b67440842d4d601e388e99f9b","externalIds":{"ArXiv":"2402.16705","DBLP":"conf/nips/Liu0W0WH024","DOI":"10.52202/079017-3102","CorpusId":268032188},"title":"SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection"},{"paperId":"a1714677252a39d1835824efb185beb0113ca189","externalIds":{"ArXiv":"2402.11530","DBLP":"journals/corr/abs-2402-11530","DOI":"10.48550/arXiv.2402.11530","CorpusId":267751050},"title":"Efficient Multimodal Learning from Data-centric Perspective"},{"paperId":"1036f6dfe75af06fbcdb3447dbe9be8613bf857c","externalIds":{"ArXiv":"2402.04788","DBLP":"conf/icml/ChenCZWLZZ00024","CorpusId":267523079},"title":"MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark"},{"paperId":"95898b1f82cf7ad7d96fcc85b4def7f086325af5","externalIds":{"ArXiv":"2402.04333","DBLP":"journals/corr/abs-2402-04333","DOI":"10.48550/arXiv.2402.04333","CorpusId":267522839},"title":"LESS: Selecting Influential Data for Targeted Instruction Tuning"},{"paperId":"c5ddd74a27263866e9fd1b395752a4d65794ecb1","externalIds":{"ArXiv":"2402.02055","DBLP":"journals/corr/abs-2402-02055","DOI":"10.48550/arXiv.2402.02055","CorpusId":267412854},"title":"Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning"},{"paperId":"11dfbd2f49624b73d0c417dbabbd0278ae25da44","externalIds":{"DBLP":"journals/corr/abs-2401-16553","ArXiv":"2401.16553","DOI":"10.48550/arXiv.2401.16553","CorpusId":267320816},"title":"SelectLLM: Can LLMs Select Important Instructions to Annotate?"},{"paperId":"5f58863dd6474d6f127be995b5871e7c60f2792f","externalIds":{"DBLP":"journals/corr/abs-2312-17432","ArXiv":"2312.17432","DOI":"10.1109/TCSVT.2025.3566695","CorpusId":266690572},"title":"Video Understanding With Large Language Models: A Survey"},{"paperId":"41113411e1748a34bb80f12c761b7af1ed6dbb90","externalIds":{"ArXiv":"2312.15685","DBLP":"conf/iclr/0131Z00H24","DOI":"10.48550/arXiv.2312.15685","CorpusId":266551413},"title":"What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning"},{"paperId":"83d12fa58743015f8abe4097fb58088f2d13c7f0","externalIds":{"DBLP":"journals/corr/abs-2312-11508","ArXiv":"2312.11508","DOI":"10.48550/arXiv.2312.11508","CorpusId":266362177},"title":"Rethinking the Instruction Quality: LIFT is What You Need"},{"paperId":"1ae9afce62c60fd0bfc9f5b57f8d8a1bbc3641eb","externalIds":{"ArXiv":"2312.06720","DBLP":"journals/corr/abs-2312-06720","DOI":"10.48550/arXiv.2312.06720","CorpusId":266174198},"title":"Audio-Visual LLM for Video Understanding"},{"paperId":"2f79b5eebe8f04566938d4d1eafbc885346f4f80","externalIds":{"ArXiv":"2312.01700","CorpusId":265609639},"title":"Data Management For Training Large Language Models: A Survey"},{"paperId":"e263e08a20080a2543d0ca29d3d63c4717a8beb6","externalIds":{"DBLP":"journals/corr/abs-2311-18799","ArXiv":"2311.18799","DOI":"10.48550/arXiv.2311.18799","CorpusId":265506093},"title":"X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning"},{"paperId":"ea3448eb86a233189631d914721e587d45931b64","externalIds":{"DBLP":"conf/cvpr/0002WH00LWX0L0024","ArXiv":"2311.17005","DOI":"10.1109/CVPR52733.2024.02095","CorpusId":265466214},"title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"},{"paperId":"4223ff010b2a91a62a131f12d14e7a8eb684e71d","externalIds":{"DBLP":"conf/eccv/ChenLCLYG24","ArXiv":"2311.15562","DOI":"10.48550/arXiv.2311.15562","CorpusId":265456354},"title":"Fully Authentic Visual Question Answering Dataset from Online Communities"},{"paperId":"e3f7ad05b1652c6ada78cffbe405bceb723bc70c","externalIds":{"ArXiv":"2311.15653","DBLP":"journals/corr/abs-2311-15653","DOI":"10.48550/arXiv.2311.15653","CorpusId":265457248},"title":"MoDS: Model-oriented Data Selection for Instruction Tuning"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","externalIds":{"DBLP":"journals/corr/abs-2311-13165","ArXiv":"2311.13165","DOI":"10.1109/BigData59044.2023.10386743","CorpusId":265351653},"title":"Multimodal Large Language Models: A Survey"},{"paperId":"f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560","externalIds":{"DBLP":"conf/eccv/ChenLDZHWZL24","ArXiv":"2311.12793","DOI":"10.48550/arXiv.2311.12793","CorpusId":265308687},"title":"ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"},{"paperId":"391eaeb1092c2b145ff0e5a2fa61637a42921fce","externalIds":{"DBLP":"conf/cvpr/ChenSCJD24","ArXiv":"2311.10081","DOI":"10.1109/CVPR52733.2024.01350","CorpusId":265221232},"title":"DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","externalIds":{"ArXiv":"2311.10122","ACL":"2024.emnlp-main.342","DBLP":"journals/corr/abs-2311-10122","DOI":"10.48550/arXiv.2311.10122","CorpusId":265281544},"title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"0f993809c1fe00403ecea66d8f572832f075cfe4","externalIds":{"ArXiv":"2311.10774","ACL":"2024.naacl-long.70","DBLP":"journals/corr/abs-2311-10774","DOI":"10.48550/arXiv.2311.10774","CorpusId":265294419},"title":"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","externalIds":{"DBLP":"conf/cvpr/0001TZC024","ArXiv":"2311.08046","DOI":"10.1109/CVPR52733.2024.01300","CorpusId":265157455},"title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"d3367dc9a7a1d7ae70a06eadc02b2430f4529f7c","externalIds":{"DBLP":"journals/corr/abs-2311-07226","ArXiv":"2311.07226","DOI":"10.48550/arXiv.2311.07226","CorpusId":265149884},"title":"Large Language Models for Robotics: A Survey"},{"paperId":"bf14244669d5505f63343d4365d99d24aa6c5e82","externalIds":{"ArXiv":"2311.06607","DBLP":"conf/cvpr/LiYLMZYSLB24","DOI":"10.1109/CVPR52733.2024.02527","CorpusId":265150038},"title":"Monkey: Image Resolution and Text Label are Important Things for Large Multi-Modal Models"},{"paperId":"2f566575a246752d59438e2bde22f88680927af9","externalIds":{"ArXiv":"2311.04219","DBLP":"journals/corr/abs-2311-04219","DOI":"10.48550/arXiv.2311.04219","CorpusId":265043616},"title":"OtterHD: A High-Resolution Multi-modality Model"},{"paperId":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","externalIds":{"DBLP":"journals/corr/abs-2311-03356","ArXiv":"2311.03356","DOI":"10.1109/CVPR52733.2024.01236","CorpusId":265043538},"title":"GLaMM: Pixel Grounding Large Multimodal Model"},{"paperId":"590954e15e247cc343710ee97e396ad99f52970f","externalIds":{"ArXiv":"2311.00288","DBLP":"conf/emnlp/KungY0CP23","DOI":"10.48550/arXiv.2311.00288","CorpusId":264832712},"title":"Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks"},{"paperId":"53f087d7f5ac5a24910c0fa0162079a2c35d8f64","externalIds":{"DBLP":"conf/icml/FanPJ24","ArXiv":"2310.15393","DOI":"10.48550/arXiv.2310.15393","CorpusId":264439382},"title":"DoGE: Domain Reweighting with Generalization Estimation"},{"paperId":"4a8f27d1bdcd9932b06be69231ce039579be7988","externalIds":{"ArXiv":"2310.07699","DBLP":"conf/eccv/LaiZZWBTDGSCYC24","DOI":"10.1007/978-3-031-72946-1_7","CorpusId":263835242},"title":"VeCLIP: Improving CLIP Training via Visual-Enriched Captions"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"d081501a74ef2934a2c30755b17fb5c339399b88","externalIds":{"ArXiv":"2310.02110","DBLP":"conf/cvpr/0002EAYALM24","DOI":"10.1109/CVPR52733.2024.02116","CorpusId":263608782},"title":"Sieve: Multimodal Dataset Pruning Using Image Captioning Models"},{"paperId":"8946891e94831adc8cddb0d32311cce2445c96d2","externalIds":{"DBLP":"conf/iclr/LuBX0LH0CG024","ArXiv":"2310.02255","CorpusId":264491155},"title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","externalIds":{"DBLP":"journals/corr/abs-2310-02239","ArXiv":"2310.02239","DOI":"10.48550/arXiv.2310.02239","CorpusId":263608981},"title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"b4d65b8647be58a751d3708001b87d9cf435e5bb","externalIds":{"ArXiv":"2310.01936","DBLP":"journals/corr/abs-2310-01936","DOI":"10.48550/arXiv.2310.01936","CorpusId":263609126},"title":"Constructing Image-Text Pair Dataset from Books"},{"paperId":"b901042d6bd0ddfe8f86f488f5b17c4b53d74e40","externalIds":{"ArXiv":"2309.14859","DBLP":"journals/corr/abs-2309-14859","DOI":"10.48550/arXiv.2309.14859","CorpusId":262825238},"title":"Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"22ebfc211d184ed615729378a43fde175bf14478","externalIds":{"ArXiv":"2309.00615","DBLP":"journals/corr/abs-2309-00615","DOI":"10.48550/arXiv.2309.00615","CorpusId":261493787},"title":"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following"},{"paperId":"2928e5a5ee488104c5d7b636f577baef2d470310","externalIds":{"DBLP":"journals/corr/abs-2308-16890","ArXiv":"2308.16890","DOI":"10.48550/arXiv.2308.16890","CorpusId":261397179},"title":"TouchStone: Evaluating Vision-Language Models by Language Models"},{"paperId":"e3052ebca5eeae6a8a73e44517903d39746f5f3a","externalIds":{"ArXiv":"2308.12032","ACL":"2024.naacl-long.421","DBLP":"journals/corr/abs-2308-12032","DOI":"10.18653/v1/2024.naacl-long.421","CorpusId":261076515},"title":"From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"},{"paperId":"11cf88dce827bd67cbfa60400306318022e736d5","externalIds":{"DBLP":"journals/corr/abs-2308-12284","ArXiv":"2308.12284","DOI":"10.48550/arXiv.2308.12284","CorpusId":261076313},"title":"D4: Improving LLM Pretraining via Document De-Duplication and Diversification"},{"paperId":"30cc95639cffca4ffa8c0eafbc502636c0c88fa5","externalIds":{"DBLP":"journals/corr/abs-2308-09936","ArXiv":"2308.09936","DOI":"10.48550/arXiv.2308.09936","CorpusId":261049015},"title":"BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"},{"paperId":"656a6b3c0348d69cf9f98f95cbf68046941a4f29","externalIds":{"DBLP":"journals/corr/abs-2308-09126","ArXiv":"2308.09126","DOI":"10.48550/arXiv.2308.09126","CorpusId":261031047},"title":"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","externalIds":{"ArXiv":"2308.02490","DBLP":"journals/corr/abs-2308-02490","DOI":"10.48550/arXiv.2308.02490","CorpusId":260611572},"title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d","externalIds":{"ArXiv":"2307.16449","DBLP":"conf/cvpr/SongCWZZWCG0ZLH24","DOI":"10.1109/CVPR52733.2024.01725","CorpusId":260333927},"title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding"},{"paperId":"4309d572a37d655779f9dce6a2c98c66334132de","externalIds":{"DBLP":"journals/corr/abs-2307-16125","ArXiv":"2307.16125","DOI":"10.48550/arXiv.2307.16125","CorpusId":260334888},"title":"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"},{"paperId":"37568c230d6397aac6960720a105e741efc87b82","externalIds":{"ArXiv":"2307.13900","DBLP":"journals/corr/abs-2307-13900","DOI":"10.48550/arXiv.2307.13900","CorpusId":260164994},"title":"FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction"},{"paperId":"8b1ac02ae662c06de72daeac171a0a4461f1e692","externalIds":{"DBLP":"conf/nips/NguyenGIOS23","ArXiv":"2307.10350","DOI":"10.48550/arXiv.2307.10350","CorpusId":259991316},"title":"Improving Multimodal Datasets with Image Captioning"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","externalIds":{"DBLP":"conf/ijcai/ZhaoYGYWZSPDH024","ArXiv":"2307.09474","DOI":"10.48550/arXiv.2307.09474","CorpusId":259951197},"title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","externalIds":{"DBLP":"journals/corr/abs-2307-08581","ArXiv":"2307.08581","DOI":"10.48550/arXiv.2307.08581","CorpusId":259937702},"title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"369b449415d50387fba048bbd4d26ee890df84b5","externalIds":{"ArXiv":"2307.06942","DBLP":"conf/iclr/WangH00YML0C00024","DOI":"10.48550/arXiv.2307.06942","CorpusId":259847783},"title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"},{"paperId":"98f8793a18eaced0ce93f5202065496cc5a84943","externalIds":{"ArXiv":"2307.07063","DBLP":"journals/corr/abs-2307-07063","DOI":"10.48550/arXiv.2307.07063","CorpusId":259924601},"title":"Bootstrapping Vision-Language Learning with Decoupled Language Pre-training"},{"paperId":"b37b1dc72b1882858f5120f2cd6883134089a6ed","externalIds":{"ArXiv":"2307.06281","DBLP":"journals/corr/abs-2307-06281","DOI":"10.48550/arXiv.2307.06281","CorpusId":259837088},"title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"e2a58fd18961c3941102989e3a3d0d27c615e015","externalIds":{"ArXiv":"2306.15195","DBLP":"journals/corr/abs-2306-15195","DOI":"10.48550/arXiv.2306.15195","CorpusId":259262082},"title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"2922768fd451ecdb45f48c1a83eb57f54a91221b","externalIds":{"DBLP":"journals/corr/abs-2306-11644","ArXiv":"2306.11644","CorpusId":259203998},"title":"Textbooks Are All You Need"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","externalIds":{"ArXiv":"2306.09093","DBLP":"journals/corr/abs-2306-09093","DOI":"10.48550/arXiv.2306.09093","CorpusId":259165461},"title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","externalIds":{"DBLP":"journals/corr/abs-2306-07207","ArXiv":"2306.07207","DOI":"10.1145/3796716","CorpusId":259138706},"title":"Valley: Video Assistant with Large Language Model Enhanced Ability"},{"paperId":"d7a4b09a0e2c2d7b118144cf09895c640896da7b","externalIds":{"DBLP":"journals/corr/abs-2306-04362","ArXiv":"2306.04362","DOI":"10.48550/arXiv.2306.04362","CorpusId":259095579},"title":"Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks"},{"paperId":"b26b07597645522782e919c32f3f84c54a4b7cbf","externalIds":{"ArXiv":"2306.03208","DBLP":"conf/sustainlp/AttenduC23","ACL":"2023.sustainlp-1.9","DOI":"10.48550/arXiv.2306.03208","CorpusId":259088773},"title":"NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"7a1e71cb1310c4a873e7a4e54d1a6dab0553adce","externalIds":{"ArXiv":"2306.01116","DBLP":"journals/corr/abs-2306-01116","DOI":"10.48550/arXiv.2306.01116","CorpusId":259063761},"title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"},{"paperId":"d4af12327385260116dfd68ed1ec6d0602d26d1f","externalIds":{"ArXiv":"2305.20088","DBLP":"journals/corr/abs-2305-20088","DOI":"10.48550/arXiv.2305.20088","CorpusId":258987272},"title":"Improving CLIP Training with Language Rewrites"},{"paperId":"4e33c5756aa18d248cf50fef9382acda1e0f65da","externalIds":{"DBLP":"journals/corr/abs-2305-18500","ArXiv":"2305.18500","DOI":"10.48550/arXiv.2305.18500","CorpusId":258967371},"title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset"},{"paperId":"3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a","externalIds":{"ArXiv":"2305.18565","DBLP":"journals/corr/abs-2305-18565","DOI":"10.48550/arXiv.2305.18565","CorpusId":258967670},"title":"PaLI-X: On Scaling up a Multilingual Vision and Language Model"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"9b4f7c97c0b83a80c32bc0b93595cbcfb4ecb16d","externalIds":{"DBLP":"journals/corr/abs-2305-10429","ArXiv":"2305.10429","DOI":"10.48550/arXiv.2305.10429","CorpusId":258741043},"title":"DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"},{"paperId":"5c7aaee5651221893ea0e67c363cab4c4be53b83","externalIds":{"ArXiv":"2305.09246","DBLP":"journals/corr/abs-2305-09246","DOI":"10.48550/arXiv.2305.09246","CorpusId":258715090},"title":"Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning"},{"paperId":"4b203ee52e27cbf27d210dd671951150729a8259","externalIds":{"DBLP":"conf/cvpr/XueY0P0M0XXNS24","ArXiv":"2305.08275","DOI":"10.1109/CVPR52733.2024.02558","CorpusId":258686565},"title":"ULIP-2: Towards Scalable Multimodal Pre-Training for 3D Understanding"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","externalIds":{"ArXiv":"2305.04160","DBLP":"journals/corr/abs-2305-04160","DOI":"10.48550/arXiv.2305.04160","CorpusId":258558106},"title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"},{"paperId":"f9570989919338079088270a9cf1a7afc8db8093","externalIds":{"DBLP":"journals/corr/abs-2304-14108","ArXiv":"2304.14108","DOI":"10.48550/arXiv.2304.14108","CorpusId":258352812},"title":"DataComp: In search of the next generation of multimodal datasets"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","externalIds":{"DBLP":"journals/corr/abs-2304-14178","ArXiv":"2304.14178","DOI":"10.48550/arXiv.2304.14178","CorpusId":258352455},"title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"03755613d50e1958a97bfaad2efb976f786fbb70","externalIds":{"DBLP":"journals/pami/LiuCHGZWT25","ArXiv":"2304.08345","DOI":"10.1109/TPAMI.2024.3479776","CorpusId":258179576,"PubMed":"39418158"},"title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset"},{"paperId":"78f599fbd62dcc4a8dbab9d2f6056815dfc5b84c","externalIds":{"DBLP":"journals/corr/abs-2304-08442","ArXiv":"2304.08442","DOI":"10.48550/arXiv.2304.08442","CorpusId":258180536},"title":"The MiniPile Challenge for Data-Efficient Language Models"},{"paperId":"df958800014d310b6df34ad83d771314d68fbb2d","externalIds":{"DBLP":"conf/nips/ZhuHAGDFYSW023","ArXiv":"2304.06939","DOI":"10.48550/arXiv.2304.06939","CorpusId":258170467},"title":"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text"},{"paperId":"690df0820f35a47e1ce44f90e6ddb4132aa09267","externalIds":{"DBLP":"journals/pami/ZhangHJL24","ArXiv":"2304.00685","DOI":"10.1109/TPAMI.2024.3369699","CorpusId":257913547,"PubMed":"38408000"},"title":"Vision-Language Models for Vision Tasks: A Survey"},{"paperId":"3dfed62c61f650eb114f0f0aa26b4e7d37b963a6","externalIds":{"DBLP":"journals/corr/abs-2303-17395","ArXiv":"2303.17395","DOI":"10.1109/TASLP.2024.3419446","CorpusId":257834090},"title":"WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research"},{"paperId":"4f2ae5fa2dc74af9c36ee57b359a4b3241006a92","externalIds":{"DBLP":"conf/icml/ParkGILM23","ArXiv":"2303.14186","DOI":"10.48550/arXiv.2303.14186","CorpusId":257757261},"title":"TRAK: Attributing Model Behavior at Scale"},{"paperId":"12c6be503e4e5b7c9cb1810152d4364f26628a8d","externalIds":{"ArXiv":"2303.10158","DBLP":"journals/corr/abs-2303-10158","DOI":"10.1145/3711118","CorpusId":257622614},"title":"Data-centric Artificial Intelligence: A Survey"},{"paperId":"638b08154fbb71fd34db2aae6cb40045577fe0de","externalIds":{"DBLP":"journals/corr/abs-2303-09540","ArXiv":"2303.09540","DOI":"10.48550/arXiv.2303.09540","CorpusId":257557221},"title":"SemDeDup: Data-efficient learning at web-scale through semantic deduplication"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"aafae4730b1add0b3e243e011db9ac87428f83cd","externalIds":{"ArXiv":"2302.09432","DBLP":"journals/corr/abs-2302-09432","DOI":"10.48550/arXiv.2302.09432","CorpusId":257038067},"title":"BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark"},{"paperId":"74013b7cfa0fc524803350fca51341004565eb22","externalIds":{"DBLP":"conf/nips/XieS0L23","ArXiv":"2302.03169","DOI":"10.48550/arXiv.2302.03169","CorpusId":256627727},"title":"Data Selection for Language Models via Importance Resampling"},{"paperId":"b62b2b24378165df6a3dc904dde2b600745310f2","externalIds":{"DBLP":"journals/corr/abs-2301-02241","ArXiv":"2301.02241","DOI":"10.48550/arXiv.2301.02241","CorpusId":255440514},"title":"CiT: Curation in Training for Effective Vision-Language Data"},{"paperId":"5425de16356015c2f26d2a50684c6c46d6998f51","externalIds":{"PubMedCentral":"9810617","DOI":"10.1038/s41597-022-01899-x","CorpusId":255439889,"PubMed":"36596836"},"title":"MIMIC-IV, a freely accessible electronic health record dataset"},{"paperId":"6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","externalIds":{"ACL":"2023.acl-long.806","ArXiv":"2212.09689","DBLP":"conf/acl/HonovichSLS23","DOI":"10.48550/arXiv.2212.09689","CorpusId":254853659},"title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"},{"paperId":"5a3c1afe73d8bcc8288d17cb17be2baec8a98464","externalIds":{"ArXiv":"2212.03533","DBLP":"journals/corr/abs-2212-03533","CorpusId":254366618},"title":"Text Embeddings by Weakly-Supervised Contrastive Pre-training"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","externalIds":{"DBLP":"journals/corr/abs-2211-15533","ArXiv":"2211.15533","DOI":"10.48550/arXiv.2211.15533","CorpusId":254044610},"title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"7ca41cc5fc364b713aba5b573ae4ada801fd788a","externalIds":{"DBLP":"conf/iclr/SilcockDYD23","ArXiv":"2210.04261","DOI":"10.48550/arXiv.2210.04261","CorpusId":252780502},"title":"Noise-Robust De-Duplication at Scale"},{"paperId":"b03c078303326ff022f525fccdf028b73ccb1cb4","externalIds":{"DBLP":"journals/corr/abs-2210-02410","ArXiv":"2210.02410","DOI":"10.48550/arXiv.2210.02410","CorpusId":252715476},"title":"The Vendi Score: A Diversity Evaluation Metric for Machine Learning"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","externalIds":{"ArXiv":"2209.06794","DBLP":"journals/corr/abs-2209-06794","DOI":"10.48550/arXiv.2209.06794","CorpusId":252222320},"title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"41644f156a381d7baba2651fe092bd1d506c65d4","externalIds":{"DBLP":"journals/corr/abs-2208-01545","ArXiv":"2208.01545","DOI":"10.48550/arXiv.2208.01545","CorpusId":245502009},"title":"The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence"},{"paperId":"45122c8f76a4e2fd0163d1f0522db37e97ea4721","externalIds":{"DBLP":"conf/nips/SorscherGSGM22","ArXiv":"2206.14486","DOI":"10.48550/arXiv.2206.14486","CorpusId":250113273},"title":"Beyond neural scaling laws: beating power law scaling via data pruning"},{"paperId":"4da2b6b1677c0178416f3a613fcafdedf9ac25c4","externalIds":{"DBLP":"conf/sigir/YaoS0CH23","ArXiv":"2205.12487","DOI":"10.1145/3539618.3591879","CorpusId":249062580},"title":"End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models"},{"paperId":"aa4d9972af3264d032dbee58501ed4ac49477103","externalIds":{"ArXiv":"2205.10487","DBLP":"journals/corr/abs-2205-10487","DOI":"10.48550/arXiv.2205.10487","CorpusId":248986979},"title":"Scaling Laws and Interpretability of Learning from Repeated Data"},{"paperId":"11cfe791cde06121ce53c84fc09e7939722f8464","externalIds":{"DBLP":"journals/corr/abs-2205-01491","ArXiv":"2205.01491","DOI":"10.1016/j.patcog.2023.109347","CorpusId":248505691},"title":"A Comprehensive Survey of Image Augmentation Techniques for Deep Learning"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"78ad010fe89b5e54befdced487c3848242afb981","externalIds":{"DBLP":"journals/corr/abs-2204-13653","ArXiv":"2204.13653","DOI":"10.48550/arXiv.2204.13653","CorpusId":248427081},"title":"GRIT: General Robust Image Task Benchmark"},{"paperId":"c5d8c27bf231d6289e2e1acbcecd8ff5162cba53","externalIds":{"DBLP":"journals/corr/abs-2204-09634","ArXiv":"2204.09634","DOI":"10.48550/arXiv.2204.09634","CorpusId":248266787},"title":"Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"12124de2038fda868fcb93c3da1996dd157e0390","externalIds":{"DBLP":"conf/cvpr/LiWTXW022","ArXiv":"2203.14072","DOI":"10.1109/CVPR52688.2022.01852","CorpusId":247763132},"title":"Learning to Answer Questions in Dynamic Audio-Visual Scenarios"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"b611c501269224702d1a9942c8600a31ec66ab28","externalIds":{"ArXiv":"2203.10244","DBLP":"conf/acl/MasryLTJH22","ACL":"2022.findings-acl.177","DOI":"10.48550/arXiv.2203.10244","CorpusId":247593713},"title":"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"55c36748f2a7c060c3313349c730b053ed03fbf7","externalIds":{"DBLP":"journals/corr/abs-2202-06539","ArXiv":"2202.06539","CorpusId":246823128},"title":"Deduplicating Training Data Mitigates Privacy Risks in Language Models"},{"paperId":"fe0e647ac5bbe9b127caddfcb52b9f723d6f158c","externalIds":{"DBLP":"conf/nips/GuMLHMLYHZJXX22","ArXiv":"2202.06767","CorpusId":249847758},"title":"Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"9a1f352ef21044700c180882038c28c3b2361914","externalIds":{"DBLP":"journals/corr/abs-2112-06409","ArXiv":"2112.06409","DOI":"10.1007/s00778-022-00775-9","CorpusId":245123976},"title":"Data collection and quality challenges in deep learning: a data-centric AI perspective"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"f3ebc4834fe9cb40d5c00a7afd81c3c3a079489b","externalIds":{"DBLP":"journals/js/ZhaoGLL21","DOI":"10.1155/2021/7325600","CorpusId":245055266},"title":"Optimization Algorithm for Point Cloud Quality Enhancement Based on Statistical Filtering"},{"paperId":"da4261a957eaa96bf626e9641ef68ebed1d5333f","externalIds":{"DBLP":"journals/corr/abs-2111-11431","ArXiv":"2111.11431","CorpusId":237262876},"title":"RedCaps: web-curated image-text data created by the people, for the people"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"a6e25ca9ee9d3e45c6d1957c0dc3324a9816c34e","externalIds":{"ArXiv":"2107.07075","DBLP":"conf/nips/PaulGD21","CorpusId":235898952},"title":"Deep Learning on a Data Diet: Finding Important Examples Early in Training"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","externalIds":{"ACL":"2022.acl-long.577","DBLP":"journals/corr/abs-2107-06499","ArXiv":"2107.06499","DOI":"10.18653/v1/2022.acl-long.577","CorpusId":235829052},"title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"f6a3d7edbb791994daa5dcb9747ce9dfc4b3f99e","externalIds":{"DBLP":"conf/nips/KothawadeBKI21","ArXiv":"2107.00717","CorpusId":235727554},"title":"SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios"},{"paperId":"4fffa5245d3972077c83614c2a08a47cb578631e","externalIds":{"ArXiv":"2106.07447","DBLP":"journals/corr/abs-2106-07447","DOI":"10.1109/taslp.2021.3122291","CorpusId":235421619},"title":"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"},{"paperId":"18f37f62d2bf3c2e34e2bde78545b47e92d7b72d","externalIds":{"DBLP":"journals/corr/abs-2105-09996","ArXiv":"2105.09996","ACL":"2021.findings-acl.370","DOI":"10.18653/v1/2021.findings-acl.370","CorpusId":235125628},"title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding"},{"paperId":"99d171aa12175c4d5d97d91c4032757719e22ae0","externalIds":{"DBLP":"conf/iclr/GuoDLZ22","ArXiv":"2105.04714","CorpusId":234357892},"title":"Sample and Computation Redistribution for Efficient Face Detection"},{"paperId":"1c30efa04394f3e75d25ea1332a96cd354189dca","externalIds":{"DBLP":"conf/iccv/KayserCSEDAL21","ArXiv":"2105.03761","DOI":"10.1109/ICCV48922.2021.00128","CorpusId":234338081},"title":"e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks"},{"paperId":"63d8426ba1f51a8525dd19fd8ec92934ec71aea5","externalIds":{"ACL":"2021.findings-acl.84","ArXiv":"2105.03075","DBLP":"journals/corr/abs-2105-03075","DOI":"10.18653/v1/2021.findings-acl.84","CorpusId":234093015},"title":"A Survey of Data Augmentation Approaches for NLP"},{"paperId":"5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec","externalIds":{"DBLP":"conf/acl/LuccioniV20","ArXiv":"2105.02732","ACL":"2021.acl-short.24","DOI":"10.18653/v1/2021.acl-short.24","CorpusId":233864521},"title":"What’s in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"67c4528de75ae743d23a4eea5c03316a7a8cf9d1","externalIds":{"DBLP":"conf/icml/YangY00R22","ArXiv":"2103.06191","CorpusId":232170418},"title":"A Study of Face Obfuscation in ImageNet"},{"paperId":"98e565fa06f6c7bf7c46833b5106b26dc45130c4","externalIds":{"ArXiv":"2103.01913","DBLP":"conf/sigir/Srinivasan0CBN21","DOI":"10.1145/3404835.3463257","CorpusId":232092726},"title":"WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"7395a49128c7bea9bf16044a99b50091c7603265","externalIds":{"ArXiv":"2102.00195","DBLP":"journals/corr/abs-2102-00195","DOI":"10.1146/annurev-vision-100419-120301","CorpusId":231740979,"PubMed":"34348034"},"title":"Quantifying Visual Image Quality: A Bayesian View"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","externalIds":{"DBLP":"journals/corr/abs-2101-00027","ArXiv":"2101.00027","CorpusId":230435736},"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"b3383f4ec36e024f7dda3f1c708f076b8465e307","externalIds":{"MAG":"3107583115","DBLP":"conf/iccv/YangMSLS21","ArXiv":"2012.00451","DOI":"10.1109/iccv48922.2021.00171","CorpusId":227238996},"title":"Just Ask: Learning to Answer Questions from Millions of Narrated Videos"},{"paperId":"9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2","externalIds":{"DBLP":"journals/corr/abs-2010-12421","ArXiv":"2010.12421","ACL":"2020.findings-emnlp.148","MAG":"3099215402","DOI":"10.18653/v1/2020.findings-emnlp.148","CorpusId":225062026},"title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"eedd6bb9cc3c0ea77acc61cc5cb945ad46d0c167","externalIds":{"MAG":"3104057403","DBLP":"conf/nips/GiunchigliaL20","ArXiv":"2010.10151","CorpusId":224803614},"title":"Coherent Hierarchical Multi-Label Classification Networks"},{"paperId":"5260c6b7b402ffae46bc53b337c71e7b80239432","externalIds":{"MAG":"3090388844","DBLP":"journals/corr/abs-2010-00475","ArXiv":"2010.00475","DOI":"10.1109/taslp.2021.3133208","CorpusId":222090007},"title":"FSD50K: An Open Dataset of Human-Labeled Sound Events"},{"paperId":"b40bfcf339de3f0dba08fabb2b58b9368ff4c51a","externalIds":{"DBLP":"conf/wacv/MathewKJ21","ArXiv":"2007.00398","MAG":"3040138106","DOI":"10.1109/WACV48630.2021.00225","CorpusId":220280200},"title":"DocVQA: A Dataset for VQA on Document Images"},{"paperId":"14b65a86c82e38fce0eb3506e0d4084ad5cdb583","externalIds":{"MAG":"3033187248","DBLP":"conf/iclr/HeLGC21","ArXiv":"2006.03654","CorpusId":219531210},"title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"8ae3421420ec57d551ef2f524b48dcc78f9337fc","externalIds":{"MAG":"3084095723","DBLP":"journals/tacl/FomichevaSYBGFA20","ArXiv":"2005.10608","DOI":"10.1162/tacl_a_00330","CorpusId":218763134},"title":"Unsupervised Quality Estimation for Neural Machine Translation"},{"paperId":"3f6570fd55dc5855f93a56150e6d99c7944a1c1e","externalIds":{"DBLP":"conf/nips/KielaFMGSRT20","ArXiv":"2005.04790","MAG":"3023989664","CorpusId":218581273},"title":"The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes"},{"paperId":"270f3bea8ca801870a6cc56b4d36f7f2019c9ed0","externalIds":{"MAG":"3105163367","DBLP":"journals/corr/abs-2004-09297","ArXiv":"2004.09297","CorpusId":215827489},"title":"MPNet: Masked and Permuted Pre-training for Language Understanding"},{"paperId":"0dde065405210ebc399c58ab6b7e843a18caad51","externalIds":{"ArXiv":"2003.01355","MAG":"3010108619","DBLP":"journals/corr/abs-2003-01355","CorpusId":211818153},"title":"CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6","externalIds":{"MAG":"3002330681","DBLP":"journals/corr/abs-2001-08435","ArXiv":"2001.08435","DOI":"10.5281/ZENODO.3608135","CorpusId":210868223},"title":"The Pushshift Reddit Dataset"},{"paperId":"5475307932954ae1636735cda083d236d45f429f","externalIds":{"MAG":"3010360545","DOI":"10.1088/1742-6596/1453/1/012085","CorpusId":215994154},"title":"A Comparison on Data Augmentation Methods Based on Deep Learning for Audio Classification"},{"paperId":"f51497f463566581874c941353dd9d80069c5b77","externalIds":{"DBLP":"conf/iclr/RaePJHL20","MAG":"2995575179","ArXiv":"1911.05507","CorpusId":207930593},"title":"Compressive Transformers for Long-Range Sequence Modelling"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","externalIds":{"DBLP":"journals/corr/abs-1911-00359","MAG":"2989539713","ACL":"2020.lrec-1.494","ArXiv":"1911.00359","CorpusId":207870323},"title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2b37cc68c9819cf0f980676935007d4135d8ac8c","externalIds":{"ArXiv":"1910.09387","DBLP":"journals/corr/abs-1910-09387","MAG":"2981182029","DOI":"10.1109/ICASSP40776.2020.9052990","CorpusId":204800739},"title":"Clotho: an Audio Captioning Dataset"},{"paperId":"1097cf8cf5961589ff693b069002e7181e24e631","externalIds":{"DBLP":"conf/icdar/0001SSC19","MAG":"3004268082","DOI":"10.1109/ICDAR.2019.00156","CorpusId":209413409},"title":"OCR-VQA: Visual Question Answering by Reading Text in Images"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","externalIds":{"DBLP":"journals/corr/abs-1908-10084","MAG":"2970641574","ArXiv":"1908.10084","ACL":"D19-1410","DOI":"10.18653/v1/D19-1410","CorpusId":201646309},"title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"e5eba5d952c8ce7124e090b4adbb08700ebd6411","externalIds":{"DBLP":"journals/corr/abs-1908-00222","MAG":"2965084509","ArXiv":"1908.00222","DOI":"10.1007/978-3-030-58545-7_30","CorpusId":199064623},"title":"Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling"},{"paperId":"92343cecdc990380de362b969eec60081959f507","externalIds":{"MAG":"2948902769","CorpusId":195505104},"title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"9311779489e597315488749ee6c386bfa3f3512e","externalIds":{"DBLP":"journals/corr/abs-1906-03327","ArXiv":"1906.03327","MAG":"2948859046","DOI":"10.1109/ICCV.2019.00272","CorpusId":182952863},"title":"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"},{"paperId":"4f2c1af57c056102806a184517313804f66e7447","externalIds":{"ArXiv":"1906.02467","DBLP":"conf/aaai/YuXYYZZT19","MAG":"2964220823","DOI":"10.1609/aaai.v33i01.33019127","CorpusId":69645185},"title":"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"},{"paperId":"c4798919e74411d87f7745840e45b8bcf61128ff","externalIds":{"MAG":"2945761034","DBLP":"conf/naacl/KimKLK19","ACL":"N19-1011","DOI":"10.18653/v1/N19-1011","CorpusId":174799768},"title":"AudioCaps: Generating Captions for Audios in The Wild"},{"paperId":"0033346700dc450ac22c9b704eab0e906d868662","externalIds":{"ArXiv":"1905.13648","DBLP":"conf/iccv/BitenTMBRJVK19","MAG":"2947555604","DOI":"10.1109/ICCV.2019.00439","CorpusId":173188651},"title":"Scene Text Visual Question Answering"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","externalIds":{"DBLP":"conf/cvpr/MarinoRFM19","ArXiv":"1906.00067","MAG":"2947312908","DOI":"10.1109/CVPR.2019.00331","CorpusId":173991173},"title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"af1f7739283bdbd2b7a94903041f6d6afd991907","externalIds":{"MAG":"2936135081","DBLP":"journals/corr/abs-1904-08920","ArXiv":"1904.08920","DOI":"10.1109/CVPR.2019.00851","CorpusId":85553602},"title":"Towards VQA Models That Can Read"},{"paperId":"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796","externalIds":{"ArXiv":"1904.03493","DBLP":"conf/iccv/WangWCLWW19","MAG":"2925419377","DOI":"10.1109/ICCV.2019.00468","CorpusId":102352148},"title":"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"},{"paperId":"206b2aeb81b29463968b8deb1efce51941f18208","externalIds":{"DBLP":"conf/cvpr/BaekLHYL19","MAG":"2967615747","ArXiv":"1904.01941","DOI":"10.1109/CVPR.2019.00959","CorpusId":102480461},"title":"Character Region Awareness for Text Detection"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","externalIds":{"MAG":"2963005248","DBLP":"conf/iclr/SaxtonGHK19","ArXiv":"1904.01557","CorpusId":85504763},"title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"3360768fcb265a8b7c1ee5ba7cfe03de0e2fad62","externalIds":{"MAG":"2950513705","ArXiv":"1902.07816","DBLP":"journals/corr/abs-1902-07816","CorpusId":67787922},"title":"Mixture Models for Diverse Machine Translation: Tricks of the Trade"},{"paperId":"86c8e5e2979377f87c7fdb2108497d074943d462","externalIds":{"MAG":"2913352150","ArXiv":"1901.08079","DBLP":"journals/corr/abs-1901-08079","PubMedCentral":"6805558","DOI":"10.1186/s12859-019-3119-4","CorpusId":59222825,"PubMed":"31640539"},"title":"A question-entailment approach to question answering"},{"paperId":"f1a17b7c4cae4513731f6d81b433e338cf4114eb","externalIds":{"DBLP":"journals/mia/BustosPSI20","ArXiv":"1901.07441","MAG":"2912664121","DOI":"10.1016/J.MEDIA.2020.101797","CorpusId":58981612,"PubMed":"32877839"},"title":"PadChest: A large chest x-ray image dataset with multi-label annotated reports"},{"paperId":"3d29ce781f297dc543e44dfb39990baff3a3acca","externalIds":{"ArXiv":"1901.07042","CorpusId":58981909},"title":"MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs"},{"paperId":"4921243268c81d0d6db99053a9d004852225a622","externalIds":{"MAG":"2962735233","DBLP":"conf/emnlp/RohrbachHBDS18","ArXiv":"1809.02156","ACL":"D18-1437","DOI":"10.18653/v1/D18-1437","CorpusId":52176506},"title":"Object Hallucination in Image Captioning"},{"paperId":"1553084dcbf2235428e7dbf57b57e567c5ea4d1f","externalIds":{"MAG":"2889048668","ArXiv":"1808.10583","DBLP":"journals/corr/abs-1808-10583","CorpusId":52145151},"title":"AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"1ea9f643171115e4a89e77c9a770c593f0794712","externalIds":{"MAG":"2951984970","DBLP":"journals/corr/abs-1806-00035","ArXiv":"1806.00035","CorpusId":44104089},"title":"Assessing Generative Models via Precision and Recall"},{"paperId":"b259467149d5e5b8fd012c006fdbaa273e5f7e24","externalIds":{"DBLP":"conf/amta/SchamoniHR18","MAG":"2886194718","ACL":"W18-1814","CorpusId":3941259},"title":"A Dataset and Reranking Method for Multimodal MT of User-Generated Image Captions"},{"paperId":"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c","externalIds":{"DBLP":"conf/cvpr/Gurari0SGLGLB18","MAG":"2788643321","ArXiv":"1802.08218","DOI":"10.1109/CVPR.2018.00380","CorpusId":3831582},"title":"VizWiz Grand Challenge: Answering Visual Questions from Blind People"},{"paperId":"0fe73c19513dfd17372d8ef58da0d0149725832c","externalIds":{"DBLP":"journals/corr/abs-1802-06893","ArXiv":"1802.06893","MAG":"2950062634","ACL":"L18-1550","CorpusId":3411445},"title":"Learning Word Vectors for 157 Languages"},{"paperId":"3ff40f0760bd8d3c46d72147b0f5b0d4aee2a24f","externalIds":{"MAG":"2768477045","DBLP":"journals/corr/abs-1711-06475","ArXiv":"1711.06475","CorpusId":34240712},"title":"AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding"},{"paperId":"057b80e235b10799d03876ad25465208a4c64caf","externalIds":{"DBLP":"conf/mm/XuZX0Z0Z17","MAG":"2765716052","DOI":"10.1145/3123266.3123427","CorpusId":3864050},"title":"Video Question Answering via Gradually Refined Attention over Appearance and Motion"},{"paperId":"7d5cf22c70484fe217936c66741fb73b2a278bde","externalIds":{"MAG":"2765383698","ACL":"Q18-1021","DBLP":"journals/corr/abs-1710-06481","ArXiv":"1710.06481","DOI":"10.1162/tacl_a_00021","CorpusId":9192723},"title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents"},{"paperId":"ee909ad489244016cf301bb7d7d8eeea423dbf35","externalIds":{"MAG":"2963017553","DBLP":"conf/iccv/HendricksWSSDR17","ArXiv":"1708.01641","DOI":"10.1109/ICCV.2017.618","CorpusId":1061352},"title":"Localizing Moments in Video with Natural Language"},{"paperId":"c342c71cb23199f112d0bc644fcce56a7306bf94","externalIds":{"MAG":"2774918944","DBLP":"conf/iclr/SenerS18","CorpusId":3383786},"title":"Active Learning for Convolutional Neural Networks: A Core-Set Approach"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","externalIds":{"MAG":"2963981733","DBLP":"conf/nips/HeuselRUNH17","CorpusId":326772},"title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"b68811a9b5cafe4795a11c1048541750068b7ad0","externalIds":{"MAG":"2949901290","ArXiv":"1706.04261","DBLP":"conf/iccv/GoyalKMMWKHFYMH17","DOI":"10.1109/ICCV.2017.622","CorpusId":834612},"title":"The “Something Something” Video Database for Learning and Evaluating Visual Common Sense"},{"paperId":"e9bd6f0b04a0ddf9fcdf3a5fd1cfe87f8ae9cfff","externalIds":{"DBLP":"conf/iccv/GaoSYN17","MAG":"2964089981","ArXiv":"1705.02101","DOI":"10.1109/ICCV.2017.563","CorpusId":31663499},"title":"TALL: Temporal Activity Localization via Language Query"},{"paperId":"b2f521c02c6ed3080c5fe123e938cdf4555e6fd2","externalIds":{"DBLP":"conf/cvpr/JangSYKK17","ArXiv":"1704.04497","MAG":"2606982687","DOI":"10.1109/CVPR.2017.149","CorpusId":3030826},"title":"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"},{"paperId":"5faadd640c959efc2790ba1f5d0a0d64a9e14a73","externalIds":{"MAG":"2612467436","ArXiv":"1703.06476","CorpusId":88517375},"title":"Practical Coreset Constructions for Machine Learning"},{"paperId":"5ba2218b708ca64ab556e39d5997202e012717d5","externalIds":{"MAG":"2593116425","DBLP":"conf/icassp/GemmekeEFJLMPR17","DOI":"10.1109/ICASSP.2017.7952261","CorpusId":21519176},"title":"Audio Set: An ontology and human-labeled dataset for audio events"},{"paperId":"e10a5e0baf2aa87d804795af071808a9377cc80a","externalIds":{"MAG":"2784025607","DBLP":"conf/aaai/ZhouXC18","ArXiv":"1703.09788","DOI":"10.1609/aaai.v32i1.12342","CorpusId":19713015},"title":"Towards Automatic Learning of Procedures From Web Instructional Videos"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"42b1cb0030e174ba4395d987df77cfa6d112d221","externalIds":{"DBLP":"journals/corr/ArmeniSZS17","ArXiv":"1702.01105","MAG":"2586114507","CorpusId":2730848},"title":"Joint 2D-3D-Semantic Data for Indoor Scene Understanding"},{"paperId":"5feb32a73dd1bd9e13f84a7b3344497a5545106b","externalIds":{"DBLP":"journals/corr/JoulinGBDJM16","MAG":"2563351168","ArXiv":"1612.03651","CorpusId":16196524},"title":"FastText.zip: Compressing text classification models"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"12d0cf8ae5ffe1b89345e1dcead22be592d844b2","externalIds":{"MAG":"2964288524","DBLP":"conf/eccv/SunS16","ArXiv":"1607.01719","DOI":"10.1007/978-3-319-49409-8_35","CorpusId":12453047},"title":"Deep CORAL: Correlation Alignment for Deep Domain Adaptation"},{"paperId":"b8e2e9f3ba008e28257195ec69a00e07f260131d","externalIds":{"DBLP":"conf/cvpr/XuMYR16","MAG":"2425121537","DOI":"10.1109/CVPR.2016.571","CorpusId":206594535},"title":"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"},{"paperId":"154c22ca5eef149aedc8a986fa684ca1fd14e7dc","externalIds":{"MAG":"3037858973","DBLP":"journals/ijcv/RohrbachTRTPLCS17","ArXiv":"1605.03705","DOI":"10.1007/s11263-016-0987-1","CorpusId":18217052},"title":"Movie Description"},{"paperId":"927987a48c2a519bbc097d8b6c925b64a85b7d8e","externalIds":{"ACL":"N16-1147","MAG":"3037785019","ArXiv":"1604.03968","DBLP":"journals/corr/HuangFMMADGHKBZ16","DOI":"10.18653/v1/N16-1147","CorpusId":2574224},"title":"Visual Storytelling"},{"paperId":"e18ec2c9f0b4a817b8cf0435822bbc879d7db698","externalIds":{"MAG":"2307512708","DBLP":"journals/corr/KembhaviSKSHF16","ArXiv":"1603.07396","DOI":"10.1007/978-3-319-46493-0_15","CorpusId":2682274},"title":"A Diagram is Worth a Dozen Images"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"b7325b788320f96f7b152768226f16e390ab6475","externalIds":{"MAG":"2253806798","ArXiv":"1601.07140","DBLP":"journals/corr/VeitMNMB16","CorpusId":2838551},"title":"COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images"},{"paperId":"def584565d05d6a8ba94de6621adab9e301d375d","externalIds":{"MAG":"2962749469","DBLP":"journals/corr/ZhuGBF15","ArXiv":"1511.03416","DOI":"10.1109/CVPR.2016.540","CorpusId":5714907},"title":"Visual7W: Grounded Question Answering in Images"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","externalIds":{"MAG":"2416885651","DBLP":"conf/cvpr/AndreasRDK16","ArXiv":"1511.02799","DOI":"10.1109/CVPR.2016.12","CorpusId":5276660},"title":"Neural Module Networks"},{"paperId":"e65142010431ffc089b272a1174214e00693e503","externalIds":{"MAG":"2963109634","ArXiv":"1511.02283","DBLP":"journals/corr/MaoHTCYM15","DOI":"10.1109/CVPR.2016.9","CorpusId":8745888},"title":"Generation and Comprehension of Unambiguous Object Descriptions"},{"paperId":"99e6f700d374e34c8376f1f43af994b278924f28","externalIds":{"MAG":"2052666245","DBLP":"conf/mm/Piczak15","DOI":"10.1145/2733373.2806390","CorpusId":17567398},"title":"ESC: Dataset for Environmental Sound Classification"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"43795b7bac3d921c4e579964b54187bdbf6c6330","externalIds":{"DBLP":"journals/corr/VenugopalanXDRMS14","ACL":"N15-1173","ArXiv":"1412.4729","MAG":"2964241990","DOI":"10.3115/v1/N15-1173","CorpusId":52316421},"title":"Translating Videos to Natural Language Using Deep Recurrent Neural Networks"},{"paperId":"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","externalIds":{"ArXiv":"1412.2306","DBLP":"journals/corr/KarpathyF14","MAG":"1905882502","DOI":"10.1109/CVPR.2015.7298932","CorpusId":8517067},"title":"Deep visual-semantic alignments for generating image descriptions"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","externalIds":{"ACL":"D14-1086","DBLP":"conf/emnlp/KazemzadehOMB14","MAG":"2251512949","DOI":"10.3115/v1/D14-1086","CorpusId":6308361},"title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","externalIds":{"ArXiv":"1409.0575","DBLP":"journals/corr/RussakovskyDSKSMHKKBBF14","MAG":"2546241758","DOI":"10.1007/s11263-015-0816-y","CorpusId":2930547},"title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"1d71896eebf16e0113a65c07a16479a03361ca60","externalIds":{"DBLP":"conf/ijcnn/WangS14","MAG":"2011674654","DOI":"10.1109/IJCNN.2014.6889457","CorpusId":16736675},"title":"A new active labeling method for deep learning"},{"paperId":"7c8a51d04522496c43db68f2582efd45eaf59fea","externalIds":{"MAG":"1920022804","DBLP":"conf/cvpr/WuSKYZTX15","DOI":"10.1109/CVPR.2015.7298801","CorpusId":206592833},"title":"3D ShapeNets: A deep representation for volumetric shapes"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"44040913380206991b1991daf1192942e038fe31","externalIds":{"ACL":"Q14-1006","DBLP":"journals/tacl/YoungLHH14","MAG":"2185175083","DOI":"10.1162/tacl_a_00166","CorpusId":3104920},"title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"72729882f8fa3d9084eaece513f6bf9630be5901","externalIds":{"DBLP":"conf/acl/ChenD11","MAG":"2164290393","ACL":"P11-1020","CorpusId":215717103},"title":"Collecting Highly Parallel Data for Paraphrase Evaluation"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"9dc1748099dd4321d42fb84bc7ee1f71e7814459","externalIds":{"DBLP":"journals/talip/GaoL04","MAG":"2073147434","DOI":"10.1145/1034780.1034781","CorpusId":322920},"title":"Introduction to the special issue on statistical language modeling"},{"paperId":"534805683c27accb27d66d9425f759b798df380a","externalIds":{"MAG":"1755205674","DBLP":"conf/scia/Farneback03","DOI":"10.1007/3-540-45103-X_50","CorpusId":15601477},"title":"Two-Frame Motion Estimation Based on Polynomial Expansion"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","externalIds":{"MAG":"2140679639","DBLP":"conf/nips/BengioDV00","CorpusId":221275765},"title":"A Neural Probabilistic Language Model"},{"paperId":"42bb0ac384fb87933be67f63b98d90a45d2fe6e9","externalIds":{"MAG":"2012833704","DBLP":"conf/stoc/Charikar02","DOI":"10.1145/509907.509965","CorpusId":4229473},"title":"Similarity estimation techniques from rounding algorithms"},{"paperId":"c6586e7c73cc1c9e9a251947425c54c5051be626","externalIds":{"DBLP":"journals/pieee/Rosenfeld00","MAG":"2100506586","DOI":"10.1109/5.880083","CorpusId":10959945},"title":"Two decades of statistical language modeling: where do we go from here?"},{"paperId":"8addb1718c2bc6bbb0d82cd1a57b41198bf65965","externalIds":{"MAG":"2968856527","DBLP":"conf/sequences/Broder97","DOI":"10.1109/SEQUEN.1997.666900","CorpusId":11748509},"title":"On the resemblance and containment of documents"},{"paperId":"1337a72073e2c80da6e53b776ccb469d5abb0579","externalIds":{"MAG":"1969173824","DBLP":"conf/soda/ManberM90","DOI":"10.1137/0222058","CorpusId":5074629},"title":"Suffix arrays: a new method for on-line string searches"},{"paperId":"7e7d7799ffb0ad44169dbeab2326a150830db1c3","externalIds":{"DBLP":"journals/corr/abs-2306-13840","DOI":"10.48550/arXiv.2306.13840","CorpusId":259252412},"title":"Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data"},{"paperId":"8f683dbe9ac52a4faef2464b99eabbbba1ab211d","externalIds":{"DBLP":"conf/iclr/XiaL0S0L23","CorpusId":259298636},"title":"Moderate Coreset: A Universal Method of Data Selection for Real-world Data-efficient Deep Learning"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","externalIds":{"DBLP":"journals/corr/abs-2308-12966","DOI":"10.48550/arXiv.2308.12966","CorpusId":263875678},"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"29c7f009df21d0112c48dec254ff80cc45fac3af","externalIds":{"DBLP":"conf/nips/SchaefferMK23","ArXiv":"2304.15004","DOI":"10.48550/arXiv.2304.15004","CorpusId":258418299},"title":"Are Emergent Abilities of Large Language Models a Mirage?"},{"paperId":"e730164e17975547564a1eaa70cea5884b16c89d","externalIds":{"CorpusId":259937133},"title":"Instruction : Translate the phrase ” Bonne chance ” into English Response : Good Luck"},{"paperId":"9e52e82773f2f5be5c7239068bf4f6029d71f844","externalIds":{"DBLP":"conf/emnlp/OverbayAzPK23","DOI":"10.18653/v1/2023.emnlp-main.251","CorpusId":266163887},"title":"mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images"},{"paperId":"06a551db36cac42ac9743299ae9d9a48ef560218","externalIds":{"DBLP":"journals/corr/abs-2310-07699","DOI":"10.48550/arXiv.2310.07699","CorpusId":280676269},"title":"From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions"},{"paperId":"5755f5ed765dd6295f6780dc5b2e3a1d32a1aa1c","externalIds":{"DBLP":"journals/corr/abs-2212-05171","DOI":"10.48550/arXiv.2212.05171","CorpusId":263881739},"title":"ULIP: Learning Unified Representation of Language, Image and Point Cloud for 3D Understanding"},{"paperId":"c69f9a5185b4c29525bedb2dcc79d20b42c14cc6","externalIds":{"DBLP":"conf/acl-dialdoc/HonovichAHTKCSS22","ACL":"2022.naacl-main.287","DOI":"10.48550/arXiv.2204.04991","CorpusId":247694170},"title":"TRUE: Re-evaluating Factual Consistency Evaluation"},{"paperId":"b9478e237b58160c65acd2c41894493d27e2c277","externalIds":{"MAG":"3169113923","DBLP":"journals/aiopen/YuanZDDLCZYT21","DOI":"10.1016/J.AIOPEN.2021.06.001","CorpusId":236712622},"title":"WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"},{"paperId":"177d986fb4944d4b681a988c290f25785cf7a86d","externalIds":{"DBLP":"conf/nips/BandyV21","CorpusId":237263180},"title":"Addressing \"Documentation Debt\" in Machine Learning: A Retrospective Datasheet for BookCorpus"},{"paperId":"18316673291b537b74eee1b369009379688da73a","externalIds":{"DBLP":"conf/nips/LeiBB21","CorpusId":263868665},"title":"Detecting Moments and Highlights in Videos via Natural Language Queries"},{"paperId":"5c5751d45e298cea054f32b392c12c61027d2fe7","externalIds":{"MAG":"3015453090","DBLP":"conf/acl/LoWNKW20","ACL":"2020.acl-main.447","DOI":"10.18653/V1/2020.ACL-MAIN.447","CorpusId":215416146},"title":"S2ORC: The Semantic Scholar Open Research Corpus"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8b55402ffee2734bfc7d5d7595500916e1ef04e8","externalIds":{"MAG":"2904565150","DBLP":"conf/iccv/AgrawalAD0CJ0BP19","ArXiv":"1812.08658","DOI":"10.1109/ICCV.2019.00904","CorpusId":56517630},"title":"nocaps: novel object captioning at scale"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"66661a68dbf1d98d794fd025113b103683510303","externalIds":{"DBLP":"conf/interspeech/KoPPK15","MAG":"2407080277","DOI":"10.21437/Interspeech.2015-711","CorpusId":7360763},"title":"Audio augmentation for speech recognition"},{"paperId":"b4fc91e543ec868658cde6170f1e59c33292e595","externalIds":{"DBLP":"conf/interspeech/KombrinkMKB11","MAG":"2293185259","DOI":"10.21437/Interspeech.2011-720","CorpusId":39718},"title":"Recurrent Neural Network Based Language Modeling in Meeting Recognition"},{"paperId":"9819b600a828a57e1cde047bbe710d3446b30da5","externalIds":{"MAG":"179875071","DBLP":"conf/interspeech/MikolovKBCK10","DOI":"10.21437/Interspeech.2010-343","CorpusId":17048224},"title":"Recurrent neural network based language model"},{"paperId":"635e1b5261ad1545aab7acde48efa267ae428fc3","externalIds":{"MAG":"2185917628","CorpusId":17075066},"title":"Implementation and Benchmarking of Perceptual Image Hash Functions"},{"paperId":"24de1048791bac4972ecc16d1c3c1de23691407d","externalIds":{"CorpusId":266378240},"title":"Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"}]}