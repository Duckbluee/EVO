{"paperId":"c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05","externalIds":{"ArXiv":"2404.18930","DBLP":"journals/corr/abs-2404-18930","DOI":"10.48550/arXiv.2404.18930","CorpusId":269449935},"title":"Hallucination of Multimodal Large Language Models: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2404.18930, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2237427303","name":"Zechen Bai"},{"authorId":"2299164401","name":"Pichao Wang"},{"authorId":"39102205","name":"Tianjun Xiao"},{"authorId":"2264160722","name":"Tong He"},{"authorId":"2008848359","name":"Zongbo Han"},{"authorId":"2298907819","name":"Zheng Zhang"},{"authorId":"2269732179","name":"Mike Zheng Shou"}],"abstract":"This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination."}