{"references":[{"paperId":"2c710db17e7c651b4653d9872668cd6ff7baf69f","externalIds":{"DBLP":"journals/corr/abs-2406-02511","ArXiv":"2406.02511","DOI":"10.48550/arXiv.2406.02511","CorpusId":270226636},"title":"V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation"},{"paperId":"f77f35f07a02c15ed6bcad3bff379b8ebc4820d3","externalIds":{"ArXiv":"2404.10394","DBLP":"journals/corr/abs-2404-10394","DOI":"10.1145/3658162","CorpusId":269157648},"title":"Portrait3D: Text-Guided High-Quality 3D Portrait Generation Using Pyramid Representation and GANs Prior"},{"paperId":"f9511d7409f72ecaa2634af02f4be3a8643c4037","externalIds":{"ArXiv":"2404.01647","DBLP":"journals/corr/abs-2404-01647","DOI":"10.48550/arXiv.2404.01647","CorpusId":268856608},"title":"EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis"},{"paperId":"78396d47cff7a5d2e1330d9f75a2619bb7e1b713","externalIds":{"DBLP":"conf/aaai/SunXLX24","DOI":"10.1609/aaai.v38i5.28309","CorpusId":268692581},"title":"FG-EmoTalk: Talking Head Video Generation with Fine-Grained Controllable Facial Expressions"},{"paperId":"c1bd6c981c946b1954bfe75b38397eaa2cc61c7f","externalIds":{"DBLP":"conf/eccv/ZhouMFYY24","ArXiv":"2402.06149","DOI":"10.48550/arXiv.2402.06149","CorpusId":267616686},"title":"HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting"},{"paperId":"ee7f93a00402316c9c185b08b2f9904f203f2f74","externalIds":{"ArXiv":"2401.12900","CorpusId":267094789},"title":"PSAvatar: A Point-based Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting"},{"paperId":"e4679a63505651f3a7dfddd596b747af77f52efb","externalIds":{"ArXiv":"2312.03029","DOI":"10.1109/TPAMI.2025.3597940","CorpusId":265693957,"PubMed":"40802635"},"title":"HHAvatar: Gaussian Head Avatar with Dynamic Hairs."},{"paperId":"90bc388771469da88997b197bd010a5bba8ca960","externalIds":{"DBLP":"conf/cvpr/PengHS0ZZ00F24","ArXiv":"2311.17590","DOI":"10.1109/CVPR52733.2024.00070","CorpusId":265498445},"title":"SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis"},{"paperId":"aa8f9d28c5069759de43a5b75e846224b59e60d8","externalIds":{"ArXiv":"2311.02549","DBLP":"conf/wacv/NiL0H24","DOI":"10.1109/WACV57701.2024.00488","CorpusId":265033572},"title":"3D-Aware Talking-Head Video Motion Transfer"},{"paperId":"468afb2e981a55f6ab22412193f711a11b98a63c","externalIds":{"DBLP":"journals/corr/abs-2311-00994","ArXiv":"2311.00994","DOI":"10.1109/WACV57701.2024.00628","CorpusId":264935446},"title":"LaughTalk: Expressive 3D Talking Head Generation with Laughter"},{"paperId":"13e8310d5c14ebdf48da64e181987db7f494d5dc","externalIds":{"DBLP":"conf/iccv/TanJP23","DOI":"10.1109/ICCV51070.2023.02024","CorpusId":267013410},"title":"EMMN: Emotional Motion Memory Network for Audio-driven Emotional Talking Face Generation"},{"paperId":"8414b2eacff7ba614be73b2436a4c705a6f2426d","externalIds":{"ArXiv":"2308.16909","DBLP":"journals/corr/abs-2308-16909","DOI":"10.1109/ICCV51070.2023.02089","CorpusId":261395372},"title":"StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation"},{"paperId":"5b2488d6bf80247697648415506cac15e8c3eeed","externalIds":{"ArXiv":"2308.16041","DBLP":"journals/corr/abs-2308-16041","DOI":"10.48550/arXiv.2308.16041","CorpusId":261339019},"title":"From Pixels to Portraits: A Comprehensive Survey of Talking Head Generation Techniques and Applications"},{"paperId":"4fa85f06ad62139aa10b16112ddffae257ae4ce0","externalIds":{"DBLP":"conf/iccv/BounareliTAPT23","ArXiv":"2307.10797","DOI":"10.1109/ICCV51070.2023.00657","CorpusId":259991520},"title":"HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces"},{"paperId":"27c667bf7bc83fcb07bd539e7a29eabdfb539b2c","externalIds":{"DBLP":"journals/corr/abs-2307-09906","ArXiv":"2307.09906","DOI":"10.1109/ICCV51070.2023.02108","CorpusId":259982826},"title":"Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head Video Generation"},{"paperId":"39c25a2b5c54bbf5cd7e0882437fa35cb81a3b29","externalIds":{"DBLP":"conf/iccv/LiZ00023","ArXiv":"2307.09323","DOI":"10.1109/ICCV51070.2023.00696","CorpusId":259950721},"title":"Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis"},{"paperId":"3788221c44c65e864420ae240aee8d9647ea7982","externalIds":{"DOI":"10.17743/jaes.2022.0081","CorpusId":259880125},"title":"Audio-Driven Talking Face Generation: A Review"},{"paperId":"b59ae4597fc05521d238cc2628dbfcf8b0019f68","externalIds":{"DBLP":"conf/cvpr/WangZZZSZZ23","DOI":"10.1109/CVPR52729.2023.01330","CorpusId":261004421},"title":"LipFormer: High-fidelity and Generalizable Talking Face Generation with A Pre-learned Facial Codebook"},{"paperId":"a03efb04dfb2490f90c1a953c49a577594852320","externalIds":{"DBLP":"journals/corr/abs-2305-11718","ArXiv":"2305.11718","DOI":"10.1109/CVPR52729.2023.02164","CorpusId":258823089},"title":"Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization"},{"paperId":"930db876df86e24b413d8dda0311bc4040f9bf98","externalIds":{"ArXiv":"2305.10456","DBLP":"journals/corr/abs-2305-10456","DOI":"10.48550/arXiv.2305.10456","CorpusId":258762854},"title":"LPMM: Intuitive Pose Control for Neural Talking-Head Model via Landmark-Parameter Morphable Model"},{"paperId":"43f8ab6000fc9e94b60e5fa23fa9a8ec9000b4c6","externalIds":{"DBLP":"journals/corr/abs-2305-02594","ArXiv":"2305.02594","DOI":"10.48550/arXiv.2305.02594","CorpusId":258479785},"title":"Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator"},{"paperId":"660793e2e799e0e3e2b4990667cf5d6587c5746e","externalIds":{"DBLP":"conf/cvpr/HuangC0023","ArXiv":"2304.10530","DOI":"10.1109/CVPR52729.2023.00589","CorpusId":258236244},"title":"Collaborative Diffusion for Multi-Modal Face Generation and Editing"},{"paperId":"e1dbcfb8bb047e47b6e2719e42298759715a726f","externalIds":{"ArXiv":"2304.10168","DBLP":"conf/cvpr/GaoZWLML23","DOI":"10.1109/CVPR52729.2023.00543","CorpusId":258236338},"title":"High-Fidelity and Freely Controllable Talking Head Video Generation"},{"paperId":"011a186a04143d30c94c5101431417c114b4d95c","externalIds":{"DBLP":"journals/corr/abs-2304-05097","ArXiv":"2304.05097","DOI":"10.1109/CVPR52729.2023.01723","CorpusId":258059713},"title":"One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field"},{"paperId":"b7b38a47017f46c0e0184e2f2c5800abdb060cff","externalIds":{"DBLP":"journals/corr/abs-2304-00334","ArXiv":"2304.00334","DOI":"10.1109/TMM.2025.3581808","CorpusId":257913419},"title":"TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles"},{"paperId":"0d5ba1dedd9b69bc7fd4db8b5a44e87e3b78b305","externalIds":{"DBLP":"journals/corr/abs-2303-17789","ArXiv":"2303.17789","DOI":"10.1109/ICME55011.2023.00359","CorpusId":257900774},"title":"FONT: Flow-guided One-shot Talking Head Generation with Natural Head Motions"},{"paperId":"525e04db32139395dcf7125cf330dae35af84f35","externalIds":{"DBLP":"journals/tomccap/ZouFYVS25","ArXiv":"2303.16611","DOI":"10.1145/3653455","CorpusId":257805165},"title":"4D Facial Expression Diffusion Model"},{"paperId":"3a663f0c1128820f9f17a93efe38d75a8b3f98a8","externalIds":{"DBLP":"conf/cvpr/MaZQLZ23","ArXiv":"2303.14662","DOI":"10.1109/CVPR52729.2023.01621","CorpusId":257766513},"title":"OTAvatar: One-Shot Talking Face Avatar with Controllable Tri-Plane Rendering"},{"paperId":"35ccd924de9e8483bdcf144cbf2edf09be157b7e","externalIds":{"ArXiv":"2303.07909","DBLP":"journals/corr/abs-2303-07909","DOI":"10.48550/arXiv.2303.07909","CorpusId":257505012},"title":"Text-to-image Diffusion Models in Generative AI: A Survey"},{"paperId":"f13d4d12d924727114182da54980a04be051fc87","externalIds":{"ArXiv":"2303.03988","DBLP":"journals/corr/abs-2303-03988","DOI":"10.48550/arXiv.2303.03988","CorpusId":257378103},"title":"DINet: Deformation Inpainting Network for Realistic Face Visually Dubbing on High Resolution Video"},{"paperId":"26e5b933b8f60bd749d428b5ff813b2abcd765d8","externalIds":{"ArXiv":"2302.09778","DBLP":"conf/icml/HuangC0SZZ23","DOI":"10.48550/arXiv.2302.09778","CorpusId":257038979},"title":"Composer: Creative and Controllable Image Synthesis with Composable Conditions"},{"paperId":"b8c1bdac473d0cfbb20a69f7f91ec28ecef6ab15","externalIds":{"DBLP":"conf/icassp/LiuWFCYDH23","ArXiv":"2302.08197","DOI":"10.1109/ICASSP49357.2023.10094598","CorpusId":256900670},"title":"OPT: One-shot Pose-Controllable Talking Head Generation"},{"paperId":"112aa142185ba80f2f0c95fd09c57f010bc3e340","externalIds":{"ArXiv":"2303.14044","DBLP":"journals/corr/abs-2303-14044","DOI":"10.1007/s41095-023-0343-7","CorpusId":257757315},"title":"MusicFace: Music-driven expressive singing face synthesis"},{"paperId":"f7865a3f3c5795829e49d7285e1696a18d5f08c8","externalIds":{"DBLP":"journals/corr/abs-2301-13430","ArXiv":"2301.13430","DOI":"10.48550/arXiv.2301.13430","CorpusId":256416230},"title":"GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis"},{"paperId":"a572366d7372db2451d2dc5595dfb7fdb9d7cf08","externalIds":{"DBLP":"conf/cvpr/PangZQFCS023","ArXiv":"2301.06281","DOI":"10.1109/CVPR52729.2023.00049","CorpusId":255942001},"title":"DPE: Disentanglement of Pose and Expression for General Video Portrait Editing"},{"paperId":"4febb8643b05c698a94dc147f92be8899222419e","externalIds":{"DBLP":"journals/ivc/BigioiBSZJMC24","ArXiv":"2301.04474","DOI":"10.48550/arXiv.2301.04474","CorpusId":255595497},"title":"Speech Driven Video Editing via an Audio-Conditioned Diffusion Model"},{"paperId":"285b2ee4d02aa57409a5c430e2a81a44b41a6c4c","externalIds":{"ArXiv":"2301.03786","DBLP":"conf/cvpr/ShenZM0Z0L23","DOI":"10.1109/CVPR52729.2023.00197","CorpusId":258236723},"title":"DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation"},{"paperId":"a677b65bf1e1f9881bccc90fe08261e11f79fab3","externalIds":{"ArXiv":"2301.03396","DBLP":"conf/wacv/StypulkowskiV0Z24","DOI":"10.1109/WACV57701.2024.00502","CorpusId":255546475},"title":"Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation"},{"paperId":"9ef7df2ace0723583ba22bf165188cd7d2044e93","externalIds":{"DBLP":"journals/corr/abs-2301-01081","ArXiv":"2301.01081","DOI":"10.48550/arXiv.2301.01081","CorpusId":255393710},"title":"StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles"},{"paperId":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","externalIds":{"DBLP":"conf/iccv/WuGWLGSHSQS23","ArXiv":"2212.11565","DOI":"10.1109/ICCV51070.2023.00701","CorpusId":254974187},"title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"},{"paperId":"a1fba5f07a7fb3abcf71231fff59f1a0bf44f491","externalIds":{"ArXiv":"2212.08062","DBLP":"conf/cvpr/ZhangQZZW0CW023","DOI":"10.1109/CVPR52729.2023.02116","CorpusId":254685502},"title":"MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation"},{"paperId":"458e3d2be80c401fb47e562d9d57012bd63da1c3","externalIds":{"ArXiv":"2212.04248","DBLP":"conf/iccv/YuYZWWW23","DOI":"10.1109/ICCV51070.2023.00703","CorpusId":254409018},"title":"Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors"},{"paperId":"d2edfedc42d598dbc4095b7584d42a9ac7e80c82","externalIds":{"DBLP":"journals/csur/KammounSTOA23","DOI":"10.1145/3527850","CorpusId":247795062},"title":"Generative Adversarial Networks for Face Generation: A Survey"},{"paperId":"09d1f64e9e25bf8e5e5af4011462a47e814a3070","externalIds":{"ArXiv":"2211.14758","DBLP":"journals/corr/abs-2211-14758","DOI":"10.1145/3550469.3555399","CorpusId":254044546},"title":"VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild"},{"paperId":"0420946337719d1231310147e44b6864185576e8","externalIds":{"ArXiv":"2211.14506","DBLP":"journals/corr/abs-2211-14506","DOI":"10.1109/CVPR52729.2023.01724","CorpusId":254044314},"title":"Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis"},{"paperId":"a7019f9a2ecb9983b66456ae32978e2574625980","externalIds":{"DBLP":"journals/corr/abs-2211-12194","ArXiv":"2211.12194","DOI":"10.1109/CVPR52729.2023.00836","CorpusId":253761522},"title":"SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation"},{"paperId":"9016c0b90dc62bb07d5b9481be48acf59bf665eb","externalIds":{"DBLP":"conf/mm/HuangZL22","DOI":"10.1145/3503161.3551574","CorpusId":252782448},"title":"Audio-driven Talking Head Generation with Transformer and 3D Morphable Model"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"9ee4d77687cd9156765ef26a8a921faa7c1fc320","externalIds":{"DBLP":"journals/corr/abs-2210-01794","ArXiv":"2210.01794","DOI":"10.48550/arXiv.2210.01794","CorpusId":252693221},"title":"Implicit Warping for Animation with Image Sets"},{"paperId":"eb1fe3c181d6c9fca547ae991c676f5f51a1abc9","externalIds":{"DBLP":"conf/wacv/NiLHX23","ArXiv":"2210.01559","DOI":"10.1109/WACV56688.2023.00049","CorpusId":252693450},"title":"Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis"},{"paperId":"4d0b528bc9f7e7e373dbba62bea8d9df43cffcca","externalIds":{"DBLP":"journals/tvcg/TangZYZCMW24","ArXiv":"2209.05434","DOI":"10.1109/TVCG.2023.3323578","CorpusId":252917593,"PubMed":"37847635"},"title":"3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation"},{"paperId":"1c25f538d3fae572cc4ad8dfa4d4ffb173506572","externalIds":{"DBLP":"journals/corr/abs-2208-02210","ArXiv":"2208.02210","DOI":"10.1109/TPAMI.2023.3253243","CorpusId":251280330,"PubMed":"37028333"},"title":"Free-HeadGAN: Neural Talking Head Synthesis With Explicit Gaze Control"},{"paperId":"6c6c996ba34fc02d00bc945b8d5c49807a3a552c","externalIds":{"DBLP":"conf/eccv/ZhuWZJTZLL22","ArXiv":"2207.12393","DOI":"10.48550/arXiv.2207.12393","CorpusId":251040093},"title":"CelebV-HQ: A Large-Scale Video Facial Attributes Dataset"},{"paperId":"ee6ca9ee3f097e2fe9b065a13599921ebc5e0148","externalIds":{"DBLP":"conf/eccv/ShenLZDZL22","ArXiv":"2207.11770","DOI":"10.48550/arXiv.2207.11770","CorpusId":251040886},"title":"Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis"},{"paperId":"5303af7a8d00883fa09552c4df672970bc1ec366","externalIds":{"DBLP":"journals/corr/abs-2207-11243","ArXiv":"2207.11243","DOI":"10.48550/arXiv.2207.11243","CorpusId":251018645},"title":"Multiface: A Dataset for Neural Face Rendering"},{"paperId":"c91eec8943a376bfb15db40c9cc5679ed8c801d5","externalIds":{"DBLP":"conf/nips/BergmanKWCLW22","ArXiv":"2206.14314","DOI":"10.48550/arXiv.2206.14314","CorpusId":250113850},"title":"Generative Neural Articulated Radiance Fields"},{"paperId":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","externalIds":{"DBLP":"journals/tmlr/YuXKLBWVKYAHHPLZBW22","ArXiv":"2206.10789","DOI":"10.48550/arXiv.2206.10789","CorpusId":249926846},"title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"},{"paperId":"62338f4697f2cdc35d7b19fe4893256dc61c8cd8","externalIds":{"DBLP":"journals/corr/abs-2206-08343","ArXiv":"2206.08343","DOI":"10.48550/arXiv.2206.08343","CorpusId":249712520},"title":"Realistic One-shot Mesh-based Head Avatars"},{"paperId":"0f2bfc65729d9ee6e446a7ba282eb7b874a251dd","externalIds":{"DBLP":"conf/cvpr/GuptaKD22","DOI":"10.1109/CVPRW56347.2022.00220","CorpusId":251110033},"title":"RV-GAN: Recurrent GAN for Unconditional Video Generation"},{"paperId":"c013a7eecbba0dd3f0ca7cbdf9e242eb7860590f","externalIds":{"DBLP":"conf/cvpr/LiangPGZHHHLD022","DOI":"10.1109/CVPR52688.2022.00338","CorpusId":250520154},"title":"Expressive Talking Head Generation with Granular Audio-Visual Control"},{"paperId":"b4b96250fc9ed7b377a57ec571613190cade9671","externalIds":{"DBLP":"conf/cvpr/HsuTW22","DOI":"10.1109/CVPR52688.2022.00072","CorpusId":250626723},"title":"Dual-Generator Face Reenactment"},{"paperId":"0b70d51c42f95ea2354c108d974c0d35574f4270","externalIds":{"ArXiv":"2205.15278","DBLP":"journals/corr/abs-2205-15278","DOI":"10.1145/3528233.3530745","CorpusId":249192023},"title":"EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model"},{"paperId":"e95e23102c6e2de2c501c453d8d580c9f705acb1","externalIds":{"DBLP":"journals/corr/abs-2204-11312","ArXiv":"2204.11312","DOI":"10.1109/CVPR52688.2022.01967","CorpusId":248021955},"title":"EMOCA: Emotion Driven Monocular Face Capture and Animation"},{"paperId":"7aeb8b24b2aa2565830470b0cfe57bfd2c1bb440","externalIds":{"DBLP":"conf/iccvw/OorloffY23","ArXiv":"2203.14512","DOI":"10.1109/ICCVW60793.2023.00322","CorpusId":256846532},"title":"Expressive Talking Head Video Encoding in StyleGAN2 Latent Space"},{"paperId":"bc6bfac73406c4ad4d263a1e7c1a9923696abc3c","externalIds":{"DBLP":"journals/corr/abs-2203-14367","ArXiv":"2203.14367","DOI":"10.1109/CVPR52688.2022.00364","CorpusId":247763055},"title":"Thin-Plate Spline Motion Model for Image Animation"},{"paperId":"a3672263121f8e6409d3afbcc367001997f1312c","externalIds":{"DBLP":"conf/iclr/WangYBD22","ArXiv":"2203.09043","DOI":"10.48550/arXiv.2203.09043","CorpusId":247518586},"title":"Latent Image Animator: Learning to Animate Images via Latent Space Navigation"},{"paperId":"fff60ef90f7ebfcc48999da55866c79b9ee68549","externalIds":{"DBLP":"journals/corr/abs-2203-06605","ArXiv":"2203.06605","DOI":"10.1109/CVPR52688.2022.00339","CorpusId":247446630},"title":"Depth-Aware Generative Adversarial Network for Talking Head Video Generation"},{"paperId":"e0b7934f2114942535a1a3578138ea48c65ca304","externalIds":{"DBLP":"conf/eccv/YinZCCFWBWWY22","ArXiv":"2203.04036","DOI":"10.48550/arXiv.2203.04036","CorpusId":247315448},"title":"StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN"},{"paperId":"8fbc2d349d3d0945efa5e92fd3713734ce63d19e","externalIds":{"DBLP":"conf/cvpr/LeeKKCH22","ArXiv":"2203.01941","DOI":"10.1109/CVPR52688.2022.01123","CorpusId":247244535},"title":"Autoregressive Image Generation using Residual Quantization"},{"paperId":"4f0c2ec8bcd224a741890109f46b501e00aea35b","externalIds":{"DBLP":"conf/cvpr/CaiODG22","ArXiv":"2202.13162","DOI":"10.1109/CVPR52688.2022.00395","CorpusId":247158374},"title":"Pix2NeRF: Unsupervised Conditional $\\pi$-GAN for Single Image to Neural Radiance Fields Translation"},{"paperId":"5aeade1f8bd33c6b7c93c034648297704450553a","externalIds":{"DBLP":"journals/corr/abs-2202-06079","ArXiv":"2202.06079","DOI":"10.1109/WACV56688.2023.00440","CorpusId":246823836},"title":"Text and Image Guided 3D Avatar Generation and Manipulation"},{"paperId":"6661752197f093895709e5778d502160571888f4","externalIds":{"ArXiv":"2202.00046","DBLP":"journals/corr/abs-2202-00046","CorpusId":246442040},"title":"Finding Directions in GAN's Latent Space for Neural Face Reenactment"},{"paperId":"664d2fcb440338a313bcf089e804aef64d94c2eb","externalIds":{"ArXiv":"2201.00791","DBLP":"journals/corr/abs-2201-00791","CorpusId":245650782},"title":"DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering"},{"paperId":"6296aa7cab06eaf058f7291040b320b5a83c0091","externalIds":{"DBLP":"journals/corr/abs-2203-00667","ArXiv":"2203.00667","DOI":"10.1109/ICCCNT56998.2023.10306417","CorpusId":1033682},"title":"Generative Adversarial Networks"},{"paperId":"7c0a7419114db2209c2f386bc1537e90417cf9d4","externalIds":{"DBLP":"conf/cvpr/ChanLCNPMGGTKKW22","ArXiv":"2112.07945","DOI":"10.1109/CVPR52688.2022.01565","CorpusId":245144673},"title":"Efficient Geometry-aware 3D Generative Adversarial Networks"},{"paperId":"dd23991294bf53c6301ab79fa49752e6458d2eab","externalIds":{"DBLP":"conf/cvpr/FanLSWK22","ArXiv":"2112.05329","DOI":"10.1109/CVPR52688.2022.01821","CorpusId":245117892},"title":"FaceFormer: Speech-Driven 3D Facial Animation with Transformers"},{"paperId":"c63c28feb43fdfd938e17e707bba06823fb9b7ed","externalIds":{"ArXiv":"2112.05637","DBLP":"journals/corr/abs-2112-05637","DOI":"10.1109/CVPR52688.2022.01973","CorpusId":245117349},"title":"HeadNeRF: A Realtime NeRF-based Parametric Head Model"},{"paperId":"a5ec595ac6a70a50d9cb29702728ed14454d0515","externalIds":{"ArXiv":"2112.02749","DBLP":"journals/corr/abs-2112-02749","DOI":"10.1609/aaai.v36i3.20154","CorpusId":244909125},"title":"One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning"},{"paperId":"b86fa3ef0e30a91fb423c548b44011d9ea80d297","externalIds":{"ArXiv":"2112.00585","DBLP":"journals/corr/abs-2112-00585","DOI":"10.1109/CVPR52688.2022.01822","CorpusId":244773392},"title":"Neural Emotion Director: Speech-preserving semantic control of facial expressions in “in-the-wild” videos"},{"paperId":"414e554d281d529401c873cb9c97186365ec5dd8","externalIds":{"ArXiv":"2111.14822","DBLP":"journals/corr/abs-2111-14822","DOI":"10.1109/CVPR52688.2022.01043","CorpusId":244714856},"title":"Vector Quantized Diffusion Model for Text-to-Image Synthesis"},{"paperId":"ec90ffa017a2cc6a51342509ce42b81b478aefb3","externalIds":{"ArXiv":"2111.12077","DBLP":"conf/cvpr/BarronMVSH22","DOI":"10.1109/CVPR52688.2022.00539","CorpusId":244488448},"title":"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields"},{"paperId":"ba0eb489230fd50cb2848732e27424b77124ddcf","externalIds":{"ArXiv":"2111.06849","DBLP":"conf/nips/JiangDWL21","CorpusId":244102982},"title":"Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data"},{"paperId":"37c9c4e7648f639c0b36f150fc6c6c90b3682f4a","externalIds":{"DBLP":"journals/corr/abs-2111-05826","ArXiv":"2111.05826","DOI":"10.1145/3528233.3530757","CorpusId":243938678},"title":"Palette: Image-to-Image Diffusion Models"},{"paperId":"69ebaf53591c047afa8fed58da88249088956ad6","externalIds":{"DBLP":"journals/corr/abs-2111-04928","ArXiv":"2111.04928","DOI":"10.1109/3DV53792.2021.00077","CorpusId":243860760},"title":"SAFA: Structure Aware Face Animation"},{"paperId":"032f8887fbaa622870353669f50734ec593ea1a7","externalIds":{"DBLP":"journals/tvcg/ZhangNFLZBG23","DOI":"10.1109/TVCG.2021.3117484","CorpusId":238357375,"PubMed":"34606458"},"title":"3D Talking Face with Personalized Pose Dynamics"},{"paperId":"d2bd71ad1aa29ca573dacc3e3fba8820c843d73d","externalIds":{"DBLP":"conf/iccv/Ren0CL021","ArXiv":"2109.08379","DOI":"10.1109/ICCV48922.2021.01350","CorpusId":237562793},"title":"PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering"},{"paperId":"2d75460e2d3209efdea049726b87df9efd4c6bb6","externalIds":{"ArXiv":"2109.02081","DBLP":"journals/csur/ShaZSLM23","DOI":"10.1145/3575656","CorpusId":237420545},"title":"Deep Person Generation: A Survey from the Perspective of Face, Pose, and Cloth Synthesis"},{"paperId":"426c9d639c11514df7ba6e96d2343695561ba0eb","externalIds":{"DBLP":"journals/corr/abs-2108-08827","ArXiv":"2108.08827","CorpusId":237213665},"title":"ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis"},{"paperId":"7d6fedd4df465f822925f0d6f20c6221bbb10810","externalIds":{"DBLP":"conf/iccv/ZhangZHZNBG21","ArXiv":"2108.07938","DOI":"10.1109/ICCV48922.2021.00384","CorpusId":237194731},"title":"FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning"},{"paperId":"9318d7a44a4098222ccd573ad14e968ba09fbdd4","externalIds":{"DBLP":"conf/ijcai/WangLDFY21","ArXiv":"2107.09293","DOI":"10.24963/ijcai.2021/152","CorpusId":236134151},"title":"Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion"},{"paperId":"c9e8cb4bb2c8e5d6dd4e631fc347f7f1415f2150","externalIds":{"DBLP":"conf/cvpr/ZhangL0F21","DOI":"10.1109/CVPR46437.2021.00366","CorpusId":235657610},"title":"Flow-guided One-shot Talking Face Generation with a High-resolution Audio-visual Dataset"},{"paperId":"863c8a696bddadbf1dcdc027a29f0cb129482b2d","externalIds":{"DBLP":"journals/corr/abs-2104-14557","ArXiv":"2104.14557","DOI":"10.1109/ICCV48922.2021.01357","CorpusId":233444209},"title":"Learned Spatial Representations for Few-shot Talking-Head Synthesis"},{"paperId":"122d8f3209c8c0e2516181efda7fa6c911ebd70b","externalIds":{"DBLP":"journals/corr/abs-2104-11116","ArXiv":"2104.11116","DOI":"10.1109/CVPR46437.2021.00416","CorpusId":233346930},"title":"Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation"},{"paperId":"1f122f73182db2f71fac27e014a23ae9b268138d","externalIds":{"DBLP":"journals/corr/abs-2104-11280","ArXiv":"2104.11280","DOI":"10.1109/CVPR46437.2021.01344","CorpusId":233388032},"title":"Motion Representations for Articulated Animation"},{"paperId":"ad5c0cbc88e2ab0599f3f5a47eee27d8163de7d1","externalIds":{"DBLP":"conf/aaai/LiWZDZYF21","ArXiv":"2104.07995","DOI":"10.1609/aaai.v35i3.16286","CorpusId":233289555},"title":"Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation"},{"paperId":"a81ef3138365f762334736769174738413cccdd9","externalIds":{"DBLP":"conf/iccv/GuoCLLBZ21","ArXiv":"2103.11078","DOI":"10.1109/ICCV48922.2021.00573","CorpusId":232307613},"title":"AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis"},{"paperId":"0ae67202f0584afccefa770865d14a46655d2975","externalIds":{"DBLP":"conf/nips/HanXWGXW21","ArXiv":"2103.00112","CorpusId":232076027},"title":"Transformer in Transformer"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"bd66f8a4e883ddf8a337dabdad88bc12b72d7c0e","externalIds":{"MAG":"3128513378","DBLP":"journals/air/AcheampongNC21","DOI":"10.1007/s10462-021-09958-2","CorpusId":233940858},"title":"Transformer models for text-based emotion detection: a review of BERT-based approaches"},{"paperId":"1f1965d16dcba30673a70a70076ee15baeacd0b5","externalIds":{"ArXiv":"2012.08261","DBLP":"conf/iccv/DoukasZS21","DOI":"10.1109/iccv48922.2021.01413","CorpusId":237266979},"title":"HeadGAN: One-shot Neural Head Synthesis and Editing"},{"paperId":"3b9abbc46f85edd74e0d25811f016b1ba5da33d7","externalIds":{"MAG":"3112871073","ArXiv":"2012.04012","DBLP":"journals/corr/abs-2012-04012","DOI":"10.1145/3476576.3476646","CorpusId":236094976},"title":"Learning an animatable detailed 3D face model from in-the-wild images"},{"paperId":"890398bb6364141f8b4a798fbc1c1605a871bd1d","externalIds":{"DBLP":"journals/corr/abs-2012-03065","MAG":"3111632845","ArXiv":"2012.03065","DOI":"10.1109/CVPR46437.2021.00854","CorpusId":227342468},"title":"Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction"},{"paperId":"85089ce8830e9fcd24ee6b157c22bfa34d7d828a","externalIds":{"ArXiv":"2011.15126","DBLP":"conf/cvpr/WangM021","MAG":"3110383967","DOI":"10.1109/CVPR46437.2021.00991","CorpusId":227238673},"title":"One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing"},{"paperId":"bb8656979f38d95062ba55640c1be65535f57c6a","externalIds":{"MAG":"3109420014","ArXiv":"2011.12100","DBLP":"conf/cvpr/Niemeyer021","DOI":"10.1109/CVPR46437.2021.01129","CorpusId":227151657},"title":"GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields"},{"paperId":"0e5d8b7873a2b1c28d1df1d040f67cd9a2687b53","externalIds":{"MAG":"3102692962","ArXiv":"2011.04439","DBLP":"journals/corr/abs-2011-04439","DOI":"10.1109/WACV48630.2021.00137","CorpusId":226281395},"title":"FACEGAN: Facial Attribute Controllable rEenactment GAN"},{"paperId":"9c160a71d3265eedaf7645c39be073c966f10433","externalIds":{"MAG":"3101631197","DBLP":"conf/mm/PrajwalMNJ20","ArXiv":"2008.10010","DOI":"10.1145/3394171.3413532","CorpusId":221266065},"title":"A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild"},{"paperId":"d235afbd0c3f1515a1ac0f7dbb119d57addf81ce","externalIds":{"DBLP":"journals/corr/abs-2008-10174","MAG":"3095664976","ArXiv":"2008.10174","DOI":"10.1007/978-3-030-58610-2_31","CorpusId":221266339},"title":"Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars"},{"paperId":"faf041ad5c239e3cd0079373330282188d13e0ad","externalIds":{"MAG":"3067169454","ArXiv":"2008.07783","DBLP":"conf/mm/YaoYSZ20","DOI":"10.1145/3394171.3413865","CorpusId":221150518},"title":"Mesh Guided One-shot Face Reenactment Using Graph Convolutional Networks"},{"paperId":"66d8edc6dc19550eff7a0ea930cdd431d6ed1c6a","externalIds":{"ArXiv":"2008.05023","MAG":"3048601078","DBLP":"conf/wacv/RichardLMGTS21","DOI":"10.1109/WACV48630.2021.00009","CorpusId":221103580},"title":"Audio- and Gaze-driven Facial Animation of Codec Avatars"},{"paperId":"dee8650c0a65588a09eb86751c600fb67a030bbc","externalIds":{"MAG":"3047985687","ArXiv":"2008.03592","DBLP":"journals/corr/abs-2008-03592","DOI":"10.1109/tmm.2021.3099900","CorpusId":221141930},"title":"Speech Driven Talking Face Generation From a Single Image and an Emotion Condition"},{"paperId":"f7e40f631825034e474be5640e6ac9ed7a6917c7","externalIds":{"ArXiv":"2007.08547","MAG":"3107666850","DBLP":"conf/eccv/ChenCLLKXX20","DOI":"10.1007/978-3-030-58545-7_3","CorpusId":220633152},"title":"Talking-head Generation with Rhythmic Head Motion"},{"paperId":"62d337dbaead376ca042f23d62c0d4b65ec98546","externalIds":{"DBLP":"journals/corr/abs-2007-02442","ArXiv":"2007.02442","MAG":"3101267796","CorpusId":220364071},"title":"GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"3ac827181a5d1d4e27eb662c2547fd2d6eb87c3b","externalIds":{"MAG":"3034257218","DBLP":"conf/cvpr/Zhang0ZBT20","DOI":"10.1109/CVPR42600.2020.01235","CorpusId":219633603},"title":"DAVD-Net: Deep Audio-Aided Video Decompression of Talking Heads"},{"paperId":"b63a9dc18baf645b4766b2b2ec8461c2c843275a","externalIds":{"MAG":"3023706973","ArXiv":"2005.03201","DBLP":"journals/corr/abs-2005-03201","CorpusId":218537849},"title":"What comprises a good talking-head video generation?: A Survey and Benchmark"},{"paperId":"7a7c4e666e5dd8a489770643b72fd99cf708f7c1","externalIds":{"ArXiv":"2004.12992","DBLP":"journals/corr/abs-2004-12992","MAG":"3019952993","CorpusId":216553251},"title":"MakeItTalk: Speaker-Aware Talking Head Animation"},{"paperId":"b75c05214cd21ed1e6a75e6aea9552e1b31b3974","externalIds":{"MAG":"3035515747","DBLP":"conf/cvpr/MenMJML20","ArXiv":"2003.12267","DOI":"10.1109/cvpr42600.2020.00513","CorpusId":214693045},"title":"Controllable Person Image Synthesis With Attribute-Decomposed GAN"},{"paperId":"797389ca052efd160ed759d7ef7adf9c30a917d6","externalIds":{"DBLP":"journals/corr/abs-2003-00196","ArXiv":"2003.00196","MAG":"2970315999","CorpusId":202767986},"title":"First Order Motion Model for Image Animation"},{"paperId":"bec253e82076dc363b8fd72d5c8fadf8f5b7e475","externalIds":{"ArXiv":"2002.10137","MAG":"3010434693","CorpusId":212414741},"title":"Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","externalIds":{"MAG":"3088592174","ACL":"2020.osact-1.2","ArXiv":"2003.00104","DBLP":"journals/corr/abs-2003-00104","CorpusId":211678011},"title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"369e9e0bb58bb4602a205a085bd683978a6d1f77","externalIds":{"DBLP":"journals/corr/abs-2001-05201","ArXiv":"2001.05201","MAG":"2999966482","DOI":"10.1109/tifs.2022.3146783","CorpusId":210701290},"title":"Everybody’s Talkin’: Let Me Talk as You Want"},{"paperId":"b7de44a2edb7570fddbdcb2ee69df9f17adc3396","externalIds":{"DBLP":"journals/corr/abs-1912-05566","ArXiv":"1912.05566","MAG":"2995238198","DOI":"10.1007/978-3-030-58517-4_42","CorpusId":209323961},"title":"Neural Voice Puppetry: Audio-driven Facial Reenactment"},{"paperId":"14fdc18d9c164e5b0d6d946b3238c04e81921358","externalIds":{"MAG":"3035574324","DBLP":"conf/cvpr/KarrasLAHLA20","ArXiv":"1912.04958","DOI":"10.1109/cvpr42600.2020.00813","CorpusId":209202273},"title":"Analyzing and Improving the Image Quality of StyleGAN"},{"paperId":"eeb47fa4adb457b47f90c018d8048b74ca963cb2","externalIds":{"DBLP":"conf/wacv/MittalW20","MAG":"3025521275","ArXiv":"1910.00726","DOI":"10.1109/WACV45572.2020.9093527","CorpusId":203626802},"title":"Animating Face using Disentangled Audio Representations"},{"paperId":"7cc106cc903926a975653349c9b2bfb60d475890","externalIds":{"DBLP":"conf/interspeech/DahmaniCGO19","MAG":"2953719307","DOI":"10.21437/INTERSPEECH.2019-2848","CorpusId":198316560},"title":"Conditional Variational Auto-Encoder for Text-Driven Expressive AudioVisual Speech Synthesis"},{"paperId":"f06ab43069480c09d7170075a6ea74669a71f139","externalIds":{"MAG":"2950448185","ArXiv":"1906.06337","DBLP":"journals/corr/abs-1906-06337","DOI":"10.1007/s11263-019-01251-8","CorpusId":189928265},"title":"Realistic Speech-Driven Facial Animation with GANs"},{"paperId":"5542100d280eb670a72b3431fe5155256bafb96e","externalIds":{"MAG":"2960274051","DBLP":"journals/tog/FriedTZFSGGJTA19","ArXiv":"1906.01524","DOI":"10.1145/3306346.3323028","CorpusId":174798209},"title":"Text-based editing of talking-head video"},{"paperId":"ed45bc75ca3406866e1bb9e95ad251536e9b985c","externalIds":{"MAG":"2990452356","ArXiv":"1905.08233","DBLP":"conf/iccv/ZakharovSBL19","DOI":"10.1109/ICCV.2019.00955","CorpusId":159040543},"title":"Few-Shot Adversarial Learning of Realistic Neural Talking Head Models"},{"paperId":"a0852cd9a026bc90168fa85fa422cb0e48f98394","externalIds":{"DBLP":"journals/corr/abs-1905-03820","MAG":"3046779505","ArXiv":"1905.03820","DOI":"10.1109/CVPR.2019.00802","CorpusId":109936942},"title":"Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss"},{"paperId":"91a1370d26ee903296bb4990a84c23f2fa8e8c83","externalIds":{"DBLP":"conf/cvpr/CudeiroBLRB19","MAG":"2981263323","ArXiv":"1905.03079","DOI":"10.1109/CVPR.2019.01034","CorpusId":122437581},"title":"Capture, Learning, and Synthesis of 3D Speaking Styles"},{"paperId":"14927abbd16abbf1c1dbed1a2f70dd0a9bea8120","externalIds":{"DBLP":"conf/wacv/TripathyKR20","MAG":"3024079478","ArXiv":"1904.01909","DOI":"10.1109/WACV45572.2020.9093474","CorpusId":102486843},"title":"ICface: Interpretable and Controllable Face Reenactment Using GANs"},{"paperId":"b4f8c1353aa2d88cacfaef1b3afba74dbf427d89","externalIds":{"DBLP":"journals/corr/abs-1901-08971","MAG":"2912336782","ArXiv":"1901.08971","DOI":"10.1109/ICCV.2019.00009","CorpusId":59292011},"title":"FaceForensics++: Learning to Detect Manipulated Facial Images"},{"paperId":"4b473ad2bf0348c8a912c3af206fbf275f286320","externalIds":{"DBLP":"conf/cvpr/SiarohinLT0S19","ArXiv":"1812.08861","MAG":"2960788800","DOI":"10.1109/CVPR.2019.00248","CorpusId":56657859},"title":"Animating Arbitrary Objects via Deep Motion Transfer"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"60fa866c24c78b3f022651370cc194f3865560d0","externalIds":{"MAG":"2897492880","ArXiv":"1810.06990","DBLP":"conf/fgr/YangZFYWXLSC19","DOI":"10.1109/FG.2019.8756582","CorpusId":53094248},"title":"LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild"},{"paperId":"f2d257625e8029f6f4998deb6279f97e07e2893c","externalIds":{"DBLP":"journals/corr/abs-1810-02508","MAG":"2950366894","ACL":"P19-1050","ArXiv":"1810.02508","DOI":"10.18653/v1/P19-1050","CorpusId":52932143},"title":"MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations"},{"paperId":"f9edf84ca07ba4b17f309b16f685af2819ead564","externalIds":{"MAG":"2951868055","DBLP":"conf/cvpr/LiuLS19","ArXiv":"1809.06647","DOI":"10.1109/CVPR.2019.01215","CorpusId":56895281},"title":"Attribute-Aware Face Aging With Wavelet-Based Generative Adversarial Networks"},{"paperId":"fcecc4ef2c32dbedda61648febb39a0f905c367e","externalIds":{"MAG":"2963469293","DBLP":"journals/corr/abs-1809-02108","ArXiv":"1809.02108","DOI":"10.1109/TPAMI.2018.2889052","CorpusId":52168058,"PubMed":"30582526"},"title":"Deep Audio-Visual Speech Recognition"},{"paperId":"a46174aa635759070984ed7062c9402695bce830","externalIds":{"ArXiv":"1809.00496","MAG":"2891205112","DBLP":"journals/corr/abs-1809-00496","CorpusId":52155419},"title":"LRS3-TED: a large-scale dataset for visual speech recognition"},{"paperId":"c5b55f410365bb889c25a9f0354f2b86ec61c4f0","externalIds":{"MAG":"2963841322","ArXiv":"1808.06601","DBLP":"conf/nips/Wang0ZYTKC18","CorpusId":52049245},"title":"Video-to-Video Synthesis"},{"paperId":"2bfccbf6f4e88a92a7b1f2b5c588b68c5fa45a92","externalIds":{"ArXiv":"1807.11079","DBLP":"conf/eccv/WuZLQL18","MAG":"2883183894","DOI":"10.1007/978-3-030-01246-5_37","CorpusId":51869042},"title":"ReenactGAN: Learning to Reenact Faces via Boundary Transfer"},{"paperId":"9ea992f009492888c482d5f4006281eaa8b758e7","externalIds":{"DBLP":"conf/eccv/WilesKZ18","MAG":"2884460600","ArXiv":"1807.10550","DOI":"10.1007/978-3-030-01261-8_41","CorpusId":51866642},"title":"X2Face: A network for controlling face generation by using images, audio, and pose codes"},{"paperId":"1816f98e2a4dd54690c2689cf529699d8843e847","externalIds":{"MAG":"2883082281","DBLP":"conf/aaai/Zhou000W19","ArXiv":"1807.07860","DOI":"10.1609/aaai.v33i01.33019299","CorpusId":49905113},"title":"Talking Face Generation by Adversarially Disentangled Audio-Visual Representation"},{"paperId":"ce36b3448cfaf7b5e771fce8aa84475293c51f51","externalIds":{"ArXiv":"1807.06358","DBLP":"conf/nips/HuangLHST18","MAG":"2950378732","CorpusId":49865829},"title":"IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis"},{"paperId":"8875ae233bc074f5cd6c4ebba447b536a7e847a5","externalIds":{"MAG":"2950872062","DBLP":"journals/corr/abs-1806-05622","ArXiv":"1806.05622","DOI":"10.21437/Interspeech.2018-1929","CorpusId":49211906},"title":"VoxCeleb2: Deep Speaker Recognition"},{"paperId":"30ba5f8839604e571765c110641940df9b533df2","externalIds":{"MAG":"2810311710","DOI":"10.1121/1.5042758","CorpusId":206480019,"PubMed":"29960497"},"title":"A corpus of audio-visual Lombard speech with frontal and profile views."},{"paperId":"892c911ca68f5b4bad59cde7eeb6c738ec6c4586","externalIds":{"PubMedCentral":"5955500","MAG":"2803193013","DOI":"10.1371/journal.pone.0196391","CorpusId":21704094,"PubMed":"29768426"},"title":"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English"},{"paperId":"d7e12f93339dc9a97cc325a4a3e9a13bdffb4988","externalIds":{"MAG":"2952005872","DBLP":"conf/eccv/ChenLMDX18","ArXiv":"1803.10404","DOI":"10.1007/978-3-030-01234-2_32","CorpusId":4435268},"title":"Lip Movements Generation at a Glance"},{"paperId":"a5abc6ea4083ff16e8d493828a2477cd4b9cdf1a","externalIds":{"ArXiv":"1803.09803","MAG":"2950226348","DBLP":"journals/corr/abs-1803-09803","DOI":"10.1007/978-3-319-93764-9_35","CorpusId":4795115},"title":"Generating Talking Face Landmarks from Speech"},{"paperId":"c468bbde6a22d961829e1970e6ad5795e05418d1","externalIds":{"ArXiv":"1801.03924","MAG":"2783879794","DBLP":"journals/corr/abs-1801-03924","DOI":"10.1109/CVPR.2018.00068","CorpusId":4766599},"title":"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"},{"paperId":"8b35c00edfa4edfd7a99d816e671023d2c000d55","externalIds":{"MAG":"2771088323","ArXiv":"1711.10485","DBLP":"conf/cvpr/XuZHZGH018","DOI":"10.1109/CVPR.2018.00143","CorpusId":8858625},"title":"AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks"},{"paperId":"744fe47157477235032f7bb3777800f9f2f45e52","externalIds":{"MAG":"2766527293","DBLP":"conf/iclr/KarrasALL18","ArXiv":"1710.10196","CorpusId":3568073},"title":"Progressive Growing of GANs for Improved Quality, Stability, and Variation"},{"paperId":"95b803d07c37e8349bd7b1318367d8237c76cbc0","externalIds":{"MAG":"2739192055","DBLP":"journals/tog/KarrasALHL17","DOI":"10.1145/3072959.3073658","CorpusId":13515193},"title":"Audio-driven facial animation by joint end-to-end learning of pose and emotion"},{"paperId":"e76edb86f270c3a77ed9f5a1e1b305461f36f96f","externalIds":{"MAG":"2951910147","DBLP":"conf/cvpr/Tulyakov0YK18","ArXiv":"1707.04993","DOI":"10.1109/CVPR.2018.00165","CorpusId":4475365},"title":"MoCoGAN: Decomposing Motion and Content for Video Generation"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","externalIds":{"MAG":"2963981733","DBLP":"conf/nips/HeuselRUNH17","CorpusId":326772},"title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"13b44d1040bf8fc1edb9de23f50af1f324e63697","externalIds":{"MAG":"2949785681","DBLP":"conf/eccv/LuTT18","DOI":"10.1007/978-3-030-01258-8_18","CorpusId":52957983},"title":"Attribute-Guided Face Generation Using Conditional CycleGAN"},{"paperId":"a8632cf6c1ef4319966564328d187876d3bef363","externalIds":{"MAG":"2613087992","ArXiv":"1705.02966","DBLP":"conf/bmvc/ChungJZ17","DOI":"10.5244/C.31.109","CorpusId":11058994},"title":"You said that?"},{"paperId":"cd54941cc46656005d31b1b24e3a002a7acd5b3f","externalIds":{"MAG":"2570575067","DBLP":"journals/jiis/CzyzewskiKBKS17","DOI":"10.1007/s10844-016-0438-z","CorpusId":21036636},"title":"An audio-visual corpus for multimodal automatic speech recognition"},{"paperId":"74f1c93dd3a8c3f9fa59fadef9a744234b2977eb","externalIds":{"DBLP":"conf/accv/ChungZ16","MAG":"2594690981","DOI":"10.1007/978-3-319-54184-6_6","CorpusId":19806033},"title":"Lip Reading in the Wild"},{"paperId":"ba11b4feb04a472cb5e5962697ed6faa653dc647","externalIds":{"MAG":"2906447902","DBLP":"journals/cacm/ThiesZSTN19","ArXiv":"2007.14808","DOI":"10.1145/3292039","CorpusId":52858569},"title":"Face2Face: Real-Time Face Capture and Reenactment of RGB Videos"},{"paperId":"571b0750085ae3d939525e62af510ee2cee9d5ea","externalIds":{"DBLP":"conf/nips/SalimansGZCRCC16","ArXiv":"1606.03498","MAG":"2949938177","CorpusId":1687220},"title":"Improved Techniques for Training GANs"},{"paperId":"6c7f040a150abf21dbcefe1f22e0f98fa184f41a","externalIds":{"DBLP":"journals/corr/ReedAYLSL16","ArXiv":"1605.05396","MAG":"2949999304","CorpusId":1563370},"title":"Generative Adversarial Text to Image Synthesis"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","externalIds":{"MAG":"2183341477","DBLP":"conf/cvpr/SzegedyVISW16","ArXiv":"1512.00567","DOI":"10.1109/CVPR.2016.308","CorpusId":206593880},"title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"0875fc92cce33df5cf7df169590dbf0ca00d2652","externalIds":{"DBLP":"journals/corr/MansimovPBS15","MAG":"2952033389","ArXiv":"1511.02793","CorpusId":9996719},"title":"Generating Images from Captions with Attention"},{"paperId":"ced771ffdb701592f49aee69aa61c45b790c9889","externalIds":{"MAG":"1779294284","DBLP":"journals/mta/FanXYWS16","DOI":"10.1007/s11042-015-2944-3","CorpusId":11527350},"title":"A deep bidirectional LSTM approach for video-realistic talking head"},{"paperId":"8c028b86f5551757becd4c4304bddebb49e880b3","externalIds":{"MAG":"1569907127","DBLP":"conf/icassp/FanWSX15","DOI":"10.1109/ICASSP.2015.7178899","CorpusId":15362638},"title":"Photo-real talking head with deep bidirectional LSTM"},{"paperId":"d6829332d0596659272451920d9ff778b0b400af","externalIds":{"MAG":"2029199293","DBLP":"journals/tmm/HarteG15","DOI":"10.1109/TMM.2015.2407694","CorpusId":9479308},"title":"TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech"},{"paperId":"334da45ee9a6325cd8ca0142586b820f88de6a77","externalIds":{"MAG":"2030931454","DBLP":"journals/taffco/CaoCKGNV14","DOI":"10.1109/TAFFC.2014.2336244","CorpusId":14369452,"PubMed":"25653738"},"title":"CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset"},{"paperId":"5f5dc5b9a2ba710937e2c413b37b053cd673df02","externalIds":{"DBLP":"journals/corr/KingmaW13","MAG":"2951004968","ArXiv":"1312.6114","CorpusId":216078090},"title":"Auto-Encoding Variational Bayes"},{"paperId":"ec3dcb42ef35cb552bc6bfe2d83ca854c0ed14a7","externalIds":{"MAG":"2296650210","DBLP":"conf/interspeech/ZhangWLSS13","DOI":"10.21437/Interspeech.2013-629","CorpusId":2190089},"title":"A new language independent, photo-realistic talking head driven by voice only"},{"paperId":"944a54f54b156ffced95c0ea0bb47b35ee1a62d1","externalIds":{"DBLP":"journals/tog/BouazizWP13","MAG":"1973757125","DOI":"10.1145/2461912.2461976","CorpusId":5602384},"title":"Online modeling for realtime facial animation"},{"paperId":"9813167ef062a991749cc5a9a1ef85a6a03881e5","externalIds":{"MAG":"2004789217","DBLP":"journals/pr/XieL07","DOI":"10.1016/j.patcog.2006.12.001","CorpusId":1674370},"title":"A coupled HMM approach to video-realistic speech animation"},{"paperId":"5129350ec0bd8f1fe78a9b864865709f8d8de058","externalIds":{"MAG":"2015143272","DOI":"10.1121/1.2229005","CorpusId":15852230,"PubMed":"17139705"},"title":"An audio-visual corpus for speech perception and automatic speech recognition."},{"paperId":"eae2e0fa72e898c289365c0af16daf57a7a6cf40","externalIds":{"MAG":"2133665775","DBLP":"journals/tip/WangBSS04","DOI":"10.1109/TIP.2003.819861","CorpusId":207761262,"PubMed":"15376593"},"title":"Image quality assessment: from error visibility to structural similarity"},{"paperId":"0ebe9caeecda4a6426128266aa33211c74a5e423","externalIds":{"MAG":"2123778317","DBLP":"journals/tcsv/AleksicK04","DOI":"10.1109/TCSVT.2004.826760","CorpusId":6198929},"title":"Speech-to-video synthesis using MPEG-4 compliant visual features"},{"paperId":"cd05baa8d33fd27e547858e467fc5cad174b0e50","externalIds":{"MAG":"2566228680","DBLP":"conf/pricai/LeeY02","DOI":"10.1007/3-540-45683-X_60","CorpusId":32064363},"title":"Audio-to-Visual Conversion Using Hidden Markov Models"},{"paperId":"ae14e64d6d491ccf792012f0fdfad161a44f7f26","externalIds":{"DBLP":"journals/vlsisp/ChoiLH01","MAG":"2128173845","DOI":"10.1023/A:1011171430700","CorpusId":29365760},"title":"Hidden Markov Model Inversion for Audio-to-Visual Conversion in an MPEG-4 Facial Animation System"},{"paperId":"71d67283157475c4e6460c52408c00e9f6b8d2fe","externalIds":{"MAG":"2237250383","DBLP":"conf/siggraph/BlanzV99","DOI":"10.1145/3596711.3596730","CorpusId":203705211},"title":"A Morphable Model For The Synthesis Of 3D Faces"},{"paperId":"060d064400246f35a0e71494b8bd24413683f067","externalIds":{"DBLP":"conf/siggraph/PighinHLSS98","DOI":"10.1145/280814.280825","CorpusId":74926},"title":"Synthesizing realistic facial expressions from photographs"},{"paperId":"76b532e2cb573fdf29f3ae68dc1372f3319c93c2","externalIds":{"MAG":"2152826865","DBLP":"journals/pami/CootesET01","DOI":"10.1007/BFb0054760","CorpusId":2230657},"title":"Active Appearance Models"},{"paperId":"9295d40353cbe4c56bbcf813abefb028b4a5ed65","externalIds":{"MAG":"2047618352","DBLP":"journals/speech/YamamotoNS98","DOI":"10.1109/AFGR.1998.670941","CorpusId":5979035},"title":"Lip movement synthesis from speech based on hidden Markov models"},{"paperId":"dd555c5f25ee3587d6d7719f968f9ece71ccad35","externalIds":{"DBLP":"conf/siggraph/LeeTW95","MAG":"2236592196","DOI":"10.1145/218380.218407","CorpusId":890033},"title":"Realistic modeling for facial animation"},{"paperId":"a788399fa74f48f788404b89d3c14e5c59c46dd8","externalIds":{"DBLP":"conf/propor/BernardoC24","ACL":"2024.propor-1.64","CorpusId":268240785},"title":"A Speech-Driven Talking Head based on a Two-Stage Generative Framework"},{"paperId":"ccfaf773a4c4d6ac3e3da2c573846488cabfe449","externalIds":{"DBLP":"journals/corr/abs-2312-09767","DOI":"10.48550/arXiv.2312.09767","CorpusId":266335297},"title":"DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models"},{"paperId":"335c6a82dadce4b860f4d77241ff48c4809a6cef","externalIds":{"DBLP":"journals/iajit/BadrH22","DOI":"10.34028/iajit/19/6/2","CorpusId":252727182},"title":"VoxCeleb1: speaker age-group classification using probabilistic neural network"},{"paperId":"6caf3307096a15832ace34a0d54cd28413503f8b","externalIds":{"DBLP":"journals/corr/abs-2102-07064","CorpusId":231924858},"title":"NeRF-: Neural Radiance Fields Without Known Camera Parameters"},{"paperId":"ce752f69e167db27d4c186f5dfcd064e9b6e7f3b","externalIds":{"MAG":"3099284785","DBLP":"conf/eccv/WangWSYWQHQL20","DOI":"10.1007/978-3-030-58589-1_42","CorpusId":221727985},"title":"MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation"},{"paperId":"8e431ca2fdb96320e8ea10cf7cfa0730fcae9ceb","externalIds":{"DBLP":"journals/taffco/BussoPBASP17","MAG":"2342475039","DOI":"10.1109/TAFFC.2016.2515617","CorpusId":15011148},"title":"MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception"},{"paperId":"429d73f95aac6d26c24f7eef1754bfee0051e489","externalIds":{"MAG":"46876218","CorpusId":13925044},"title":"Performance Driven Facial Animation using Blendshape Interpolation"},{"paperId":"e082dbb95210bb84693042b198d31087db286443","externalIds":{"MAG":"1992227954","DBLP":"conf/mmsp/ChoiH99","DOI":"10.1109/MMSP.1999.793816","CorpusId":42719854},"title":"Baum-Welch hidden Markov model inversion for reliable audio-to-visual conversion"},{"paperId":"ae607fb2e07311c9929c1ae51d35e9e7ea67fdf7","externalIds":{"CorpusId":266378508},"title":"of Generalizable One-shot 3D Neural Head Avatar"},{"paperId":"5410086e3c7cb4cc48f73f75e843b4109befe092","externalIds":{"CorpusId":282134003},"title":"Representing Scenes as Neural Radiance Fields for View Synthesis"}]}