{"abstract":"Autoencoders (AE) have been used successfully as unsupervised learners for inferring latent information, learning hidden features and reducing the dimensionality of the data. In this paper, we propose a new AE architecture: Gate-Layer AE (GLAE). The novelty of GLAE lies in its ability to encourage learning of the relationships among different input variables, which affords it with an inherent ability to recover missing variables from the available ones and to act as a concurrent multi-function approximator.GLAE uses a network architecture that associates each input with a binary gate acting as a switch that turns on or off the flow to each input unit, while synchronising its action with data flow to the network. We test GLAE with different coding sizes and compare its performance against the Classic AE, Denoising AE and Variational AE. The evaluation uses Electroencephalograph (EEG) data with an aim to reconstruct the EEG signal when some data are missing. The results demonstrate GLAEâ€™s superior performance in reconstructing EEG signals with up to 25% missing data in an input stream."}