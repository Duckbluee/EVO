{"abstract":"Representation learning on text-attributed graphs (TAGs), where nodes are associated with textual descriptions, is crucial for textual and relational knowledge systems, such as social media and recommendation scenarios. However, state-of-the-art embedding methods for TAGs primarily focus on fine-tuning pre-trained language models (PLMs) using structure-aware training objectives. While effective, these methods are tailored for individual TAG and cannot generalize across various graph scenarios. Given the shared textual space, leveraging multiple TAGs for joint fine-tuning, aligning text and graph structure from different aspects, would be more beneficial. Therefore, we propose the Unified Graph Language Model (UniGLM), a novel foundation model pretrained over multiple TAGs from a variety of domains, which can generalize well to both in-domain and cross-domain graph scenarios. Specifically, UniGLM fine-tunes well-established PLMs (e.g., Sentence-BERT) using a domain-aware contrastive learning objective that unifies structure heterogeneity and node statistics across various domains with an adaptive and learnable positive sample selection scheme. Additionally, a lazy updating module is introduced to speed up training by reducing repetitive encoding of positive samples. Extensive datasets across multiple domains, downstream tasks (node classification and link prediction), and a spectrum of graph backbones (supervised and self-supervised graph models) are conducted to compare UniGLM with state-of-the-art baselines. Our empirical observations suggest that UniGLM can generate informative representations for cross-domain graphs observed in the training. More importantly, UniGLM also exhibits competitive transfer ability in encoding unseen TAGs that are not used for training. This study provides deep insights into how to adapt PLMs to graph data and demonstrates the potential of building foundation model for graph representation learning."}