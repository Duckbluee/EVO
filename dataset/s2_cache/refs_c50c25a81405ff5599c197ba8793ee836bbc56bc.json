{"references":[{"paperId":"5475d2d982984dd5e1eaef384685254a1afaaed6","externalIds":{"DBLP":"conf/nips/GuptaSN23","ArXiv":"2311.15303","DOI":"10.48550/arXiv.2311.15303","CorpusId":265456268},"title":"Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement"},{"paperId":"bbab59db3533398fa9924481bd87fb2d323ba778","externalIds":{"DBLP":"journals/corr/abs-2310-16410","ArXiv":"2310.16410","DOI":"10.48550/arXiv.2310.16410","CorpusId":264451628},"title":"Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero"},{"paperId":"965c4b4142dd6d6c98d278e3dcf9f3c3ec1b5ed3","externalIds":{"ArXiv":"2309.16928","DBLP":"conf/nips/ZarlengaCDWSJ23","DOI":"10.48550/arXiv.2309.16928","CorpusId":263310415},"title":"Learning to Receive Help: Intervention-Aware Concept Embedding Models"},{"paperId":"024087f125814fa70c6411fba9a4caff75878983","externalIds":{"DBLP":"journals/corr/abs-2309-12482","ArXiv":"2309.12482","DOI":"10.48550/arXiv.2309.12482","CorpusId":262217015},"title":"State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding"},{"paperId":"6812ee04ee2383bc40e683d252fd2e56cdbeb80e","externalIds":{"ArXiv":"2303.12872","DBLP":"journals/corr/abs-2303-12872","DOI":"10.1145/3600211.3604692","CorpusId":257687699},"title":"Human Uncertainty in Concept-Based AI Systems"},{"paperId":"4d42039c4eaabd2e7aed40ff7aec16b8c27b791b","externalIds":{"DBLP":"conf/icml/MarconatoBFCPT23","ArXiv":"2302.01242","DOI":"10.48550/arXiv.2302.01242","CorpusId":256503535},"title":"Neuro Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept Rehearsal"},{"paperId":"8b9b9db68ed03d1cff19acdb98420d5ece1d30e1","externalIds":{"ArXiv":"2301.13445","DBLP":"journals/corr/abs-2301-13445","DOI":"10.48550/arXiv.2301.13445","CorpusId":256416377},"title":"A Survey of Explainable AI in Deep Visual Modeling: Methods and Metrics"},{"paperId":"e695a6a6d8a677f528add0118effc7736da35709","externalIds":{"ArXiv":"2301.11911","DBLP":"journals/corr/abs-2301-11911","DOI":"10.48550/arXiv.2301.11911","CorpusId":256358599},"title":"Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees"},{"paperId":"b6ea8f01aafcaf689f190c5193880d1cba0461ef","externalIds":{"DBLP":"journals/corr/abs-2301-10367","ArXiv":"2301.10367","DOI":"10.1609/aaai.v37i10.26392","CorpusId":256231214},"title":"Towards Robust Metrics for Concept Representation Evaluation"},{"paperId":"a9cd05d3421801ba94f76fafc74573cbf81d9a50","externalIds":{"DBLP":"journals/corr/abs-2301-06324","ArXiv":"2301.06324","DOI":"10.1007/s11263-025-02474-8","CorpusId":255942098},"title":"Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification"},{"paperId":"f8b78b5145e908c0be60840c03461fcd8da19e75","externalIds":{"DBLP":"conf/iccv/WangLCL0MF023","ArXiv":"2301.04011","DOI":"10.1109/ICCV51070.2023.00197","CorpusId":255569763},"title":"Learning Support and Trivial Prototypes for Interpretable Image Classification"},{"paperId":"7ec03dcac7fdf9e6b2032d2185bcd64a894a4833","externalIds":{"DBLP":"conf/wacv/SachaRSTZ23","ArXiv":"2301.12276","DOI":"10.1109/WACV56688.2023.00153","CorpusId":256390547},"title":"ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts"},{"paperId":"3537c817e011e25df9de88fc13ffba35ca9c68b3","externalIds":{"DBLP":"conf/wacv/KhormujiO23","DOI":"10.1109/WACV56688.2023.00147","CorpusId":256648506},"title":"A Protocol for Evaluating Model Interpretation Methods from Visual Explanations"},{"paperId":"791dd1dce875d0b85b156d3d6684ffaf2fbbc3df","externalIds":{"DBLP":"conf/aistats/AlabdulmohsinCD23","ArXiv":"2212.11254","DOI":"10.48550/arXiv.2212.11254","CorpusId":254926474},"title":"Adapting to Latent Subgroup Shifts via Concepts and Proxies"},{"paperId":"d3fb854e4e97cab40d1c076cd6e88439a0227249","externalIds":{"DBLP":"conf/iclr/AdebayoMAK22","ArXiv":"2212.04629","DOI":"10.48550/arXiv.2212.04629","CorpusId":251648657},"title":"Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation"},{"paperId":"025d4b73c1d0f267ca39fa9649413d88ed3cc5bc","externalIds":{"DBLP":"journals/csur/GaoGJHYZ24","ArXiv":"2212.03954","DOI":"10.1145/3644073","CorpusId":254408768},"title":"Going Beyond XAI: A Systematic Survey for Explanation-Guided Learning"},{"paperId":"e63b3f2ac5eee86c10e2265b483957dc27d970ee","externalIds":{"DBLP":"journals/corr/abs-2211-16080","ArXiv":"2211.16080","DOI":"10.48550/arXiv.2211.16080","CorpusId":254069697},"title":"Understanding and Enhancing Robustness of Concept-based Models"},{"paperId":"8b1f1a20e28ceb35eebecf72c76b729eb16d9147","externalIds":{"DBLP":"journals/corr/abs-2211-10807","ArXiv":"2211.10807","DOI":"10.48550/arXiv.2211.10807","CorpusId":253735151},"title":"Concept-based Explanations using Non-negative Concept Activation Vectors and Decision Tree for CNN Models"},{"paperId":"39be6e8eaf20d17e42ba5ad3bc04127704aad6b3","externalIds":{"DBLP":"journals/corr/abs-2210-11841","ArXiv":"2210.11841","DOI":"10.48550/arXiv.2210.11841","CorpusId":253080402},"title":"Diffusion Visual Counterfactual Explanations"},{"paperId":"ab5e6f8e01ce666fef96fd790fdb30e4e94d18ad","externalIds":{"DBLP":"journals/corr/abs-2208-14966","ArXiv":"2208.14966","DOI":"10.48550/arXiv.2208.14966","CorpusId":251953466},"title":"Concept Gradient: Concept-based Interpretation Without Linear Assumption"},{"paperId":"e983cdf156d932819a032837a9e11c8ee38566d6","externalIds":{"DBLP":"journals/corr/abs-2208-10431","ArXiv":"2208.10431","DOI":"10.24963/ijcai.2024/168","CorpusId":251718906},"title":"ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition"},{"paperId":"5767b9a0d1b6ff9aa890dd7eaf4ff3ec3f93b11e","externalIds":{"PubMedCentral":"9995896","ArXiv":"2207.14526","DBLP":"journals/frai/TesoASD23","DOI":"10.3389/frai.2023.1066049","CorpusId":251196687,"PubMed":"36909207"},"title":"Leveraging explanations in interactive machine learning: An overview"},{"paperId":"514eec7a2c3c9445b4a8a8408c47594709cf9be1","externalIds":{"ArXiv":"2207.01916","DBLP":"journals/corr/abs-2207-01916","DOI":"10.48550/arXiv.2207.01916","CorpusId":250279756},"title":"Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep Discriminative Models"},{"paperId":"582830d60a92e64df40d198702d9b6378aa589dd","externalIds":{"DBLP":"journals/corr/abs-2206-13413","ArXiv":"2206.13413","DOI":"10.1145/3534678.3539419","CorpusId":250073027},"title":"RES: A Robust Framework for Guiding Visual Explanation"},{"paperId":"aaff53391f99912bc7f730a7fbbc860e8dba7177","externalIds":{"ArXiv":"2206.09575","DBLP":"journals/corr/abs-2206-09575","DOI":"10.48550/arXiv.2206.09575","CorpusId":249889250},"title":"C-SENN: Contrastive Self-Explaining Neural Network"},{"paperId":"da64dd0e109323ccb94349acfba6b36fb668a7ef","externalIds":{"ArXiv":"2206.05981","DBLP":"journals/corr/abs-2206-05981","DOI":"10.1145/3581641.3584074","CorpusId":249626123},"title":"Efficient Human-in-the-loop System for Guiding DNNs Attention"},{"paperId":"da135c5294aca324ce7ff3e0d30d7a1e1ed74f83","externalIds":{"ArXiv":"2206.05257","DBLP":"journals/corr/abs-2206-05257","DOI":"10.48550/arXiv.2206.05257","CorpusId":249605335},"title":"Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces"},{"paperId":"1557669e5b43c7dbffe7a9f23b80264098cdc7ea","externalIds":{"DBLP":"conf/iclr/BontempelliTTGP23","ArXiv":"2205.15769","DOI":"10.48550/arXiv.2205.15769","CorpusId":249209899},"title":"Concept-level Debugging of Part-Prototype Networks"},{"paperId":"c0c98210454aba32628e3dbca031bde364914725","externalIds":{"ArXiv":"2205.15612","DBLP":"conf/nips/MarconatoPT22","DOI":"10.48550/arXiv.2205.15612","CorpusId":249209924},"title":"GlanceNets: Interpretabile, Leak-proof Concept-based Models"},{"paperId":"144e7e0e94e5a75f2dc5991b5276d17d643e74f4","externalIds":{"DBLP":"conf/cvpr/KeswaniRRB22","ArXiv":"2204.11830","DOI":"10.1109/CVPR52688.2022.00999","CorpusId":248377257},"title":"Proto2Proto: Can you recognize the car, the way I do?"},{"paperId":"97d6725cd526507cd965d578f6c8a6938151ba86","externalIds":{"DBLP":"journals/inffus/WeberLBS23","ArXiv":"2203.08008","DOI":"10.48550/arXiv.2203.08008","CorpusId":247450840},"title":"Beyond Explaining: Opportunities and Challenges of XAI-Based Model Improvement"},{"paperId":"ebab9b8337daa82787ce7c7e19a2013f813c4d47","externalIds":{"DBLP":"journals/corr/abs-2203-10690","ArXiv":"2203.10690","DOI":"10.1109/JBHI.2022.3152267","CorpusId":247056898,"PubMed":"35192467"},"title":"CNN Attention Guidance for Improved Orthopedics Radiographic Fracture Classification"},{"paperId":"192a2fcafab8dd0197902729f41303f991f85893","externalIds":{"DBLP":"journals/access/SawadaN22","ArXiv":"2202.01459","DOI":"10.1109/ACCESS.2022.3167702","CorpusId":246485723},"title":"Concept Bottleneck Model With Additional Unsupervised Concepts"},{"paperId":"9d465145b4f65d1aa5f8bd774c77c002b71c9bfb","externalIds":{"DBLP":"journals/corr/abs-2202-00391","ArXiv":"2202.00391","CorpusId":246442316},"title":"Right for the Right Latent Factors: Debiasing Generative Models via Disentanglement"},{"paperId":"76699167c1ec70899fcbd5635e1ede1d172141a7","externalIds":{"DBLP":"journals/corr/abs-2202-12451","ArXiv":"2202.12451","DOI":"10.3233/faia210362","CorpusId":245692349},"title":"Human-Centered Concept Explanations for Neural Networks"},{"paperId":"592fb9a455fc8c603869eae3818bfa7221909db7","externalIds":{"DBLP":"conf/cvpr/StammerMSK22","ArXiv":"2112.02290","DOI":"10.1109/CVPR52688.2022.01007","CorpusId":244909074},"title":"Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations"},{"paperId":"4497f092aa44d4b47659226fad348a1834efd416","externalIds":{"PubMedCentral":"9704706","DBLP":"journals/corr/abs-2111-09259","ArXiv":"2111.09259","DOI":"10.1073/pnas.2206625119","CorpusId":244270420,"PubMed":"36375061"},"title":"Acquisition of chess knowledge in AlphaZero"},{"paperId":"7a446c41025d65f39ddee6263700d34c16469c96","externalIds":{"ArXiv":"2109.11160","CorpusId":246904438},"title":"Toward a Unified Framework for Debugging Concept-based Models"},{"paperId":"5697da09cdfd3ce7768b126ced6b5a0b82e4e884","externalIds":{"ArXiv":"2109.04518","DBLP":"conf/aaai/TranFAS22","DOI":"10.1609/aaai.v36i9.21195","CorpusId":237485333},"title":"Unsupervised Causal Binary Concepts Discovery with VAE for Black-box Model Explanation"},{"paperId":"73fce3aa3611186c3fcd3e7f8da62c1eb3dcf0db","externalIds":{"ArXiv":"2108.11761","DBLP":"conf/cvpr/0001VSB22","DOI":"10.1109/CVPR52688.2022.01004","CorpusId":244772958},"title":"A Framework for Learning Ante-hoc Explainable Models via Concepts"},{"paperId":"ee215c04d90e2c1427713f903f850e44e344788d","externalIds":{"ArXiv":"2108.10612","DBLP":"conf/pkdd/RymarczykPKKSZ22","DOI":"10.1007/978-3-031-26387-3_26","CorpusId":252090436},"title":"ProtoMIL: Multiple Instance Learning with Prototypical Parts for Whole-Slide Image Classification"},{"paperId":"6c1727808feed06c13b1c586ce19af6825f2a4c6","externalIds":{"DBLP":"journals/corr/abs-2108-13828","ArXiv":"2108.13828","DOI":"10.1109/IJCNN52387.2021.9534369","CorpusId":237363956},"title":"PACE: Posthoc Architecture-Agnostic Concept Extractor for Explaining CNNs"},{"paperId":"57db5ffda93877ae809c875637ff4f7043249e3d","externalIds":{"ArXiv":"2106.13314","DBLP":"journals/corr/abs-2106-13314","CorpusId":235652059},"title":"Promises and Pitfalls of Black-Box Concept Learning Models"},{"paperId":"17893c069a05b9a3b98a363f115fff1dbd74cd22","externalIds":{"DBLP":"journals/corr/abs-2106-10947","ArXiv":"2106.10947","DOI":"10.1016/j.future.2022.02.020","CorpusId":235489851},"title":"Leveraging Conditional Generative Models in a General Explanation Framework of Classifier Decisions"},{"paperId":"31fd462355a72e03bc375cd6b372f35a560dd36e","externalIds":{"ArXiv":"2106.08641","DBLP":"journals/corr/abs-2106-08641","CorpusId":235446973},"title":"Best of both worlds: local and global explanations with human-understandable concepts"},{"paperId":"28fc865105bf91c70d13e7e19effc53da9d247b1","externalIds":{"DBLP":"conf/iclr/GhandehariounKL22","ArXiv":"2105.15164","CorpusId":235254386},"title":"DISSECT: Disentangled Simultaneous Explanations via Concept Traversals"},{"paperId":"70f7968a97a4e4b99217cc40c4131be45f4702e0","externalIds":{"ArXiv":"2105.10172","DBLP":"journals/corr/abs-2105-10172","CorpusId":235125526},"title":"Explainable Machine Learning with Prior Knowledge: An Overview"},{"paperId":"de072b055ad741729e712ef94de4688997ddc578","externalIds":{"DBLP":"journals/corr/abs-2105-04289","ArXiv":"2105.04289","CorpusId":234339919},"title":"Do Concept Bottleneck Models Learn as Intended?"},{"paperId":"289c5a0fd64fc6586073bdeb06b905c12e9fa096","externalIds":{"DBLP":"conf/chi/ShenLLDQHV21","DOI":"10.1145/3411763.3451798","CorpusId":233987814},"title":"Human-AI Interactive and Continuous Sensemaking: A Case Study of Image Classification using Scribble Attention Maps"},{"paperId":"0981cbb415a602b599a3549282429ca3acf35a85","externalIds":{"ArXiv":"2104.13369","DBLP":"conf/iccv/LangGYWEHFIGIM21","MAG":"3159635074","DOI":"10.1109/ICCV48922.2021.00073","CorpusId":233407984},"title":"Explaining in Style: Training a GAN to explain a classifier in StyleSpace"},{"paperId":"fb4014b0d201fc880962f44d3bb076a308ef7cc8","externalIds":{"DBLP":"journals/corr/abs-2104-06917","ArXiv":"2104.06917","MAG":"3156314563","CorpusId":233231517},"title":"Is Disentanglement all you need? Comparing Concept-based & Disentanglement Approaches"},{"paperId":"0ef56a25b685f929e6151f705bd3f65c7df5c889","externalIds":{"ArXiv":"2103.10663","DBLP":"conf/cvpr/KimKSY21","DOI":"10.1109/CVPR46437.2021.01546","CorpusId":232290837},"title":"XProtoNet: Diagnosis in Chest Radiography with Global and Local Explanations"},{"paperId":"5b6c582d51266be9aa7e32bfdc20891e5231eca4","externalIds":{"DBLP":"journals/corr/abs-2102-02201","ArXiv":"2102.02201","ACL":"2022.lnls-1.4","DOI":"10.18653/v1/2022.lnls-1.4","CorpusId":231786356},"title":"When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data"},{"paperId":"03db82055bf039503c8111e997757f122aa5438d","externalIds":{"ArXiv":"2012.14261","DBLP":"journals/tetci/ZhangTLT21","DOI":"10.1109/TETCI.2021.3100641","CorpusId":229678413},"title":"A Survey on Neural Network Interpretability"},{"paperId":"f156ecbbb9243522275490d698c6825f4d2e01af","externalIds":{"DBLP":"journals/entropy/LinardatosPK21","PubMedCentral":"7824368","DOI":"10.3390/e23010018","CorpusId":229722844,"PubMed":"33375658"},"title":"Explainable AI: A Review of Machine Learning Interpretability Methods"},{"paperId":"1887e290e2d2154bfc4939a7a826df2e5f91f4bc","externalIds":{"MAG":"3113020496","DBLP":"journals/corr/abs-2012-06523","ArXiv":"2012.06523","CorpusId":228375006},"title":"Dependency Decomposition and a Reject Option for Explainable Models"},{"paperId":"423a793ff1c19047c47b4666b7ee9bb5cf539f45","externalIds":{"MAG":"3111792455","DBLP":"journals/corr/abs-2012-02898","ArXiv":"2012.02898","CorpusId":227334816},"title":"Learning Interpretable Concept-Based Models with Human Feedback"},{"paperId":"329f9972e42ad78854282d50161c8f208c8e4983","externalIds":{"DBLP":"journals/corr/abs-2011-12854","MAG":"3108007727","ArXiv":"2011.12854","DOI":"10.1109/CVPR46437.2021.00362","CorpusId":227162700},"title":"Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations"},{"paperId":"ab9f91fe93b59bf44811e6fc1fcdf1b61cf9a5c9","externalIds":{"MAG":"3093551005","ArXiv":"2010.13233","DBLP":"journals/corr/abs-2010-13233","CorpusId":224817690},"title":"Now You See Me (CME): Concept-based Model Extraction"},{"paperId":"3710d3301574bd2d58bb848d5adad0339d5ad0b6","externalIds":{"DBLP":"journals/adac/VojirK20","MAG":"3086877397","DOI":"10.1007/s11634-020-00419-2","CorpusId":225234878},"title":"Editable machine learning models? A rule-based framework for user studies of explainability"},{"paperId":"42c061c1e329b157e6dc4f27f40bed534bddc871","externalIds":{"ArXiv":"2007.11500","DBLP":"conf/iclr/BahadoriH21","CorpusId":233254411},"title":"Debiasing Concept-based Explanations with Causal Analysis"},{"paperId":"3a24bfb77ed271fef948058e414850f89b0955a7","externalIds":{"DBLP":"conf/icml/KohNTMPKL20","ArXiv":"2007.04612","MAG":"3041871339","CorpusId":220424448},"title":"Concept Bottleneck Models"},{"paperId":"92902e212cd1408f2bfbefbbb0157abe1b05a18e","externalIds":{"DBLP":"conf/aaai/ZhangM0ER21","ArXiv":"2006.15417","DOI":"10.1609/aaai.v35i13.17389","CorpusId":235349029},"title":"Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors"},{"paperId":"5156381d63bb3e873533b08f203cb56c2d79b6c9","externalIds":{"MAG":"3102696055","ArXiv":"2006.15055","DBLP":"journals/corr/abs-2006-15055","CorpusId":220127924},"title":"Object-Centric Learning with Slot Attention"},{"paperId":"785964444715fca13abca787cc98a4f058d02a86","externalIds":{"DBLP":"conf/nips/OShaughnessyCCR20","MAG":"3037185591","ArXiv":"2006.13913","CorpusId":220041787},"title":"Generative causal explanations of black-box classifiers"},{"paperId":"dc0668754e95da573cd64dbe5e2fed07ac9ddf97","externalIds":{"MAG":"3026885507","ArXiv":"2005.09635","DBLP":"journals/corr/abs-2005-09635","DOI":"10.1109/tpami.2020.3034267","CorpusId":218719151,"PubMed":"33108282"},"title":"InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs"},{"paperId":"57c585300cb00fd85cfc06465d1887b36f70bef0","externalIds":{"DBLP":"journals/corr/abs-2004-03623","MAG":"3034794187","ArXiv":"2004.03623","DOI":"10.1109/cvpr42600.2020.00480","CorpusId":215415828},"title":"PatchVAE: Learning Local Latent Codes for Recognition"},{"paperId":"d03e66a84b92f520235079083d3c0947b2c910e0","externalIds":{"MAG":"2998512575","DBLP":"conf/aaai/AkulaWZ20","DOI":"10.1609/AAAI.V34I03.5643","CorpusId":208278278},"title":"CoCoX: Generating Conceptual and Counterfactual Explanations via Fault-Lines"},{"paperId":"d64c3993d1c797b23a8c0a54e1829d453a1c241d","externalIds":{"MAG":"3004430916","DBLP":"journals/corr/abs-2002-03549","ArXiv":"2002.03549","CorpusId":211069150},"title":"Adversarial TCAV - Robust and Effective Interpretation of Intermediate Layers in Neural Networks"},{"paperId":"8ff64db54f9ae64d132720fa9b4b2b5ff1226810","externalIds":{"ArXiv":"2002.01660","DBLP":"journals/corr/abs-2002-01660","MAG":"3004896569","CorpusId":211032173},"title":"CHAIN: Concept-harmonized Hierarchical Inference Interpretation of Deep Convolutional Neural Networks"},{"paperId":"18c6547b1af98bec9ce8df3f80cea8e63a57f47d","externalIds":{"DBLP":"journals/corr/abs-2002-01650","ArXiv":"2002.01650","MAG":"3004725381","DOI":"10.1038/s42256-020-00265-z","CorpusId":211031886},"title":"Concept whitening for interpretable image recognition"},{"paperId":"907f087e34fbc16067a2e47bed8c55c4c26946f3","externalIds":{"DBLP":"journals/natmi/SchramowskiSTBH20","MAG":"3035989815","ArXiv":"2001.05371","DOI":"10.1038/s42256-020-0212-3","CorpusId":219955775},"title":"Making deep neural networks right for the right scientific reasons by interacting with their explanations"},{"paperId":"e720172b00149854d6f6adfbfc54c31049cefc1c","externalIds":{"MAG":"3191346550","DBLP":"journals/inffus/AndersWNSML22","DOI":"10.1016/j.inffus.2021.07.015","CorpusId":237172173},"title":"Finding and removing Clever Hans: Using explanation methods to debug and improve deep models"},{"paperId":"0f9c4a03a903abe206f8389a5f17d85c9e8999ce","externalIds":{"DBLP":"journals/jvis/JiaLLZL20","MAG":"2988437828","DOI":"10.1007/s12650-019-00607-z","CorpusId":209923308},"title":"Visualizing surrogate decision trees of convolutional neural networks"},{"paperId":"3185fd6ca0664a06ad0db58ea92142f0e97cb89e","externalIds":{"DBLP":"journals/corr/abs-1910-09772","MAG":"2981628132","ArXiv":"1910.09772","CorpusId":204824219},"title":"Weakly Supervised Disentanglement with Guarantees"},{"paperId":"c12dda55bdff2f4cebf0a274331de8d117c2b7aa","externalIds":{"MAG":"3007960301","DBLP":"conf/nips/YehKALPR20","ArXiv":"1910.07969","CorpusId":213097139},"title":"On Completeness-aware Concept-Based Explanations in Deep Neural Networks"},{"paperId":"9e1d61022113f439c33ca0a1d56ed863ce42fa4a","externalIds":{"MAG":"3003707492","DBLP":"conf/icdm/Gyawali0KGHSW19","ArXiv":"1909.01839","DOI":"10.1109/ICDM.2019.00127","CorpusId":202539801},"title":"Improving Disentangled Representation Learning with the Beta Bernoulli Process"},{"paperId":"60a74f2c77a66e2cfe4b02291f18a4edc077d2a9","externalIds":{"MAG":"2909699200","DBLP":"conf/uai/TonoliniJM19","CorpusId":68152956},"title":"Variational Sparse Coding"},{"paperId":"5809135c6ad6d0392be20a3143f2f74ab6f64ef4","externalIds":{"ArXiv":"1907.07165","DBLP":"journals/corr/abs-1907-07165","MAG":"2959325723","CorpusId":196831528},"title":"Explaining Classifiers with Causal Concept Effect (CaCE)"},{"paperId":"6811983cf2e3cd5975a997ad6339bd1ef3150e30","externalIds":{"MAG":"2945764361","DBLP":"journals/dint/JiWSZWY19","DOI":"10.1162/dint_a_00013","CorpusId":162183409},"title":"Microsoft Concept Graph: Mining Semantic Concepts for Short Text Understanding"},{"paperId":"50f76736c3090c6effac25400e5e40cc0b7b5ad9","externalIds":{"ArXiv":"1904.12584","MAG":"2953388933","DBLP":"journals/corr/abs-1904-12584","CorpusId":108296442},"title":"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision"},{"paperId":"4ecfa481430b56bc3fcd735114e1954c4b46cff5","externalIds":{"MAG":"2970030610","DBLP":"conf/nips/GhorbaniWZK19","CorpusId":184487319},"title":"Towards Automatic Concept-based Explanations"},{"paperId":"f57254ac87c6a46c0cb0e582b173fe183b84a389","externalIds":{"MAG":"2958514452","DBLP":"conf/aies/TesoK19","DOI":"10.1145/3306618.3314293","CorpusId":173984765},"title":"Explanatory Interactive Machine Learning"},{"paperId":"9d15ebe3f5aaf32a9f835f88703241461324c35b","externalIds":{"MAG":"2891021031","DBLP":"journals/corr/abs-1810-02338","ArXiv":"1810.02338","CorpusId":52919654},"title":"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"},{"paperId":"aa2ddae22760249729ac2c2c4e24c8b665bcd40e","externalIds":{"MAG":"2895739182","DBLP":"conf/eccv/ZhouSBT18","DOI":"10.1007/978-3-030-01237-3_8","CorpusId":51903952},"title":"Interpretable Basis Decomposition for Visual Explanation"},{"paperId":"3df952d4a724655f7520ff95d4b2cef90fff0cae","externalIds":{"MAG":"2886794383","DBLP":"journals/cacm/DuLH20","ArXiv":"1808.00033","DOI":"10.1145/3359786","CorpusId":51893222},"title":"Techniques for interpretable machine learning"},{"paperId":"aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1","externalIds":{"ArXiv":"1807.10221","MAG":"2953120679","DBLP":"conf/eccv/XiaoLZJS18","DOI":"10.1007/978-3-030-01228-1_26","CorpusId":50781105},"title":"Unified Perceptual Parsing for Scene Understanding"},{"paperId":"83040001210751239553269727b9ea53e152af71","externalIds":{"MAG":"2963305465","DBLP":"conf/atal/Tenenbaum18","DOI":"10.65109/ahmk4753","CorpusId":260496023},"title":"Building Machines that Learn and Think Like People"},{"paperId":"cc145f046788029322835979a14459652da7247e","externalIds":{"DBLP":"journals/corr/abs-1806-10574","MAG":"2971048680","ArXiv":"1806.10574","CorpusId":49482223},"title":"This looks like that: deep learning for interpretable image recognition"},{"paperId":"0cf102da6dd4276115c63cbb6797f24ed450fea1","externalIds":{"MAG":"2809671526","DBLP":"journals/corr/abs-1806-07538","ArXiv":"1806.07538","CorpusId":49324194},"title":"Towards Robust Interpretability with Self-Explaining Neural Networks"},{"paperId":"b7587cf14a0cb5e3cc27fec1304cb492be1d9748","externalIds":{"ArXiv":"1806.01756","MAG":"2901620665","DBLP":"journals/corr/abs-1806-01756","CorpusId":46939926},"title":"Concept-Oriented Deep Learning"},{"paperId":"de99fbe728dfd3d337ab13ae27512ab028444c6a","externalIds":{"MAG":"2952038846","DBLP":"conf/cvpr/FongV18","ArXiv":"1801.03454","DOI":"10.1109/CVPR.2018.00910","CorpusId":2738204},"title":"Net2Vec: Quantifying and Explaining How Concepts are Encoded by Filters in Deep Neural Networks"},{"paperId":"38fb1902c6a2ab4f767d4532b28a92473ea737aa","externalIds":{"DBLP":"journals/corr/abs-1712-01815","MAG":"2772709170","ArXiv":"1712.01815","CorpusId":33081038},"title":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"},{"paperId":"682b9d2212258fd5edbfca589c86390c31a956b0","externalIds":{"ArXiv":"1711.11279","MAG":"2796885425","DBLP":"conf/icml/KimWGCWVS18","CorpusId":51737170},"title":"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"},{"paperId":"bf9af274a17eea6ca3721acffab14e5c8e9f097a","externalIds":{"MAG":"2964180856","ArXiv":"1710.04806","DBLP":"journals/corr/abs-1710-04806","DOI":"10.1609/aaai.v32i1.11771","CorpusId":19106556},"title":"Deep Learning for Case-based Reasoning through Prototypes: A Neural Network that Explains its Predictions"},{"paperId":"744464cd6fa8341633cd3b5d378faab18a3b543a","externalIds":{"MAG":"2952857044","ArXiv":"1704.05796","DBLP":"journals/corr/BauZKOT17","DOI":"10.1109/CVPR.2017.354","CorpusId":378410},"title":"Network Dissection: Quantifying Interpretability of Deep Visual Representations"},{"paperId":"7db2afdc5eb5db46cc64185d0a51ed079b0976e8","externalIds":{"MAG":"2952901124","DBLP":"conf/ijcai/RossHD17","ArXiv":"1703.03717","DOI":"10.24963/ijcai.2017/371","CorpusId":7053611},"title":"Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations"},{"paperId":"f302e136c41db5de1d624412f68c9174cf7ae8be","externalIds":{"MAG":"2949197630","DBLP":"conf/icml/SundararajanTY17","ArXiv":"1703.01365","CorpusId":16747630},"title":"Axiomatic Attribution for Deep Networks"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","externalIds":{"MAG":"2962858109","DBLP":"conf/iccv/SelvarajuCDVPB17","ArXiv":"1610.02391","DOI":"10.1007/s11263-019-01228-7","CorpusId":15019293},"title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","externalIds":{"DBLP":"conf/emnlp/PenningtonSM14","ACL":"D14-1162","MAG":"2250539671","DOI":"10.3115/v1/D14-1162","CorpusId":1957433},"title":"GloVe: Global Vectors for Word Representation"},{"paperId":"b56b1e0acd3301c925bb2b074fe3fb8e0dbf5379","externalIds":{"MAG":"1994606570","DBLP":"conf/vl/KuleszaSBYKW13","DOI":"10.1109/VLHCC.2013.6645235","CorpusId":6960803},"title":"Too much, too little, or just right? Ways explanations impact end users' mental models"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","externalIds":{"MAG":"2950577311","DBLP":"journals/corr/abs-1301-3781","ArXiv":"1301.3781","CorpusId":5959482},"title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"c56e758ba18066a8cdc333f15dfdb7ea6af4d283","externalIds":{"MAG":"2067713319","DBLP":"journals/nn/StallkampSSI12","DOI":"10.1016/j.neunet.2012.02.016","CorpusId":14580063,"PubMed":"22394690"},"title":"Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition"},{"paperId":"760103b363b1557372e048c4c31b5f01162bfcfa","externalIds":{"DBLP":"conf/sigmod/WuLWZ12","MAG":"2138605095","DOI":"10.1145/2213836.2213891","CorpusId":14775471},"title":"Probase: a probabilistic taxonomy for text understanding"},{"paperId":"7b548e851653ac7ef0697a31778656db6be7d928","externalIds":{"DBLP":"conf/nips/LiuLC07","MAG":"2148252020","CorpusId":10782621},"title":"Semi-Supervised Multitask Learning"},{"paperId":"43a267ed5f865bf00b6c741eaf69307e6fc08b7a","externalIds":{"MAG":"2152444902","DOI":"10.7551/mitpress/1602.001.0001","CorpusId":261782746},"title":"The Big Book of Concepts"},{"paperId":"19088a582f2eb657ac1803f1ea1b79058d5c3dc7","externalIds":{"MAG":"1562353621","DOI":"10.1017/CBO9780511528446.003","CorpusId":153629957},"title":"A Value for n-person Games"},{"paperId":"ed820008a5e3694147c155359387ab0120732acc","externalIds":{"DBLP":"journals/tmlr/ZarlengaSNKJ23","CorpusId":260942393},"title":"TabCBM: Concept-based Interpretable Neural Networks for Tabular Data"},{"paperId":"45edaa7e3ff7c46d5a5f0cfcbad2532426c5a8b3","externalIds":{"CorpusId":253244644},"title":"A Typology to Explore the Mitigation of Shortcut Behavior"},{"paperId":"64696edae8dea73d698011d09bed9465b0bfbdd5","externalIds":{"DBLP":"journals/pvldb/DadvarGS22","CorpusId":252000643},"title":"POEM: Pattern-Oriented Explanations of CNN Models"},{"paperId":"90e271126c7200d5ed76fc86a66a6a33a859c508","externalIds":{"DBLP":"conf/edbt/ShawiSS21","DOI":"10.5441/002/edbt.2021.38","CorpusId":232283709},"title":"Towards Automated Concept-based Decision TreeExplanations for CNNs"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"a0564ba7ef9f37c286c55f0703fdbe1cc1937a90","externalIds":{"CorpusId":203694137},"title":"Toward Faithful Explanatory Active Learning with Self-explainable Neural Nets ?"},{"paperId":"3f154331ad64b7ad487855241ab494647976aac4","externalIds":{"CorpusId":29803098},"title":"Knowledge Via An Explanatory Graph"},{"paperId":"eb98100daafa74da8af6570c7689fbdef89bc8c0","externalIds":{"MAG":"238210499","CorpusId":60393389},"title":"Deep Learning through Concept-Based Inquiry."},{"paperId":"c2ec86f01d7c54e9a90688e98c98a819ec781679","externalIds":{"CorpusId":8455756},"title":"Numerics of Gram-Schmidt orthogonalization"}]}