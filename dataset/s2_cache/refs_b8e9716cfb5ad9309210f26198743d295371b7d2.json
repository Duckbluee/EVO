{"references":[{"paperId":"ab8e6df5001dbb9b48445220099425aff536b3e8","externalIds":{"DBLP":"journals/corr/abs-2403-08319","ArXiv":"2403.08319","ACL":"2024.emnlp-main.486","DOI":"10.48550/arXiv.2403.08319","CorpusId":268379757},"title":"Knowledge Conflicts for LLMs: A Survey"},{"paperId":"16d6e1ed1cf72212f6154644f3aa59d18bc95fda","externalIds":{"ArXiv":"2401.06066","DBLP":"conf/acl/DaiDZXGCLZYWXLH24","DOI":"10.48550/arXiv.2401.06066","CorpusId":266933338},"title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"},{"paperId":"411114f989a3d1083d90afd265103132fee94ebe","externalIds":{"DBLP":"journals/corr/abs-2401-04088","ArXiv":"2401.04088","DOI":"10.48550/arXiv.2401.04088","CorpusId":266844877},"title":"Mixtral of Experts"},{"paperId":"9542cc130dfb70c1614af0d130977b440f26d206","externalIds":{"ArXiv":"2312.08636","DBLP":"conf/aaai/XinDWYD24","DOI":"10.48550/arXiv.2312.08636","CorpusId":266210307},"title":"MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning"},{"paperId":"b355b6a89ff412d61cabc83dd30fd42c16ed50e9","externalIds":{"DBLP":"journals/corr/abs-2311-04661","ArXiv":"2311.04661","DOI":"10.48550/arXiv.2311.04661","CorpusId":265050790},"title":"Massive Editing for Large Language Models via Meta Learning"},{"paperId":"a2f44031dfa55f6da176955bcc37aac726de3a00","externalIds":{"DBLP":"journals/corr/abs-2310-20587","ArXiv":"2310.20587","DOI":"10.48550/arXiv.2310.20587","CorpusId":264802494},"title":"Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning"},{"paperId":"42016f91e5b1da63174d45acb96bc89b64aa124d","externalIds":{"DBLP":"journals/csur/WangZLZCL25","ArXiv":"2310.16218","DOI":"10.1145/3698590","CorpusId":264487359},"title":"Knowledge Editing for Large Language Models: A Survey"},{"paperId":"2ccb452691a5d3e3b600caaec119df9ff44688bd","externalIds":{"DBLP":"journals/corr/abs-2310-02575","ArXiv":"2310.02575","DOI":"10.48550/arXiv.2310.02575","CorpusId":263620126},"title":"AdaMerging: Adaptive Model Merging for Multi-Task Learning"},{"paperId":"749d59f887c8ac83fd4f5178465e8b03e463358c","externalIds":{"DBLP":"journals/corr/abs-2309-15025","ArXiv":"2309.15025","DOI":"10.48550/arXiv.2309.15025","CorpusId":262824801},"title":"Large Language Model Alignment: A Survey"},{"paperId":"4dd5752eff94246005db1028f8281c2f17545f44","externalIds":{"DBLP":"journals/corr/abs-2308-10438","ArXiv":"2308.10438","DOI":"10.1109/ICCV51070.2023.01600","CorpusId":261049780},"title":"Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks"},{"paperId":"b11096a2237ce57a90c18220c6fa1498b98c17f1","externalIds":{"DBLP":"journals/kbs/HanLLP23","DOI":"10.1016/j.knosys.2023.110826","CorpusId":260848746},"title":"A divide and conquer framework for Knowledge Editing"},{"paperId":"f999f0396a674cf5c1a195fb99386e2b44fedb92","externalIds":{"DBLP":"conf/iccv/LiuS23","ArXiv":"2307.08114","DOI":"10.1109/ICCV51070.2023.01712","CorpusId":259937354},"title":"Tangent Model Composition for Ensembling and Continual Fine-tuning"},{"paperId":"19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e","externalIds":{"DBLP":"conf/aaai/00010LYHLW24","ArXiv":"2306.17492","DOI":"10.48550/arXiv.2306.17492","CorpusId":259308873},"title":"Preference Ranking Optimization for Human Alignment"},{"paperId":"01400b34d1ff8c176d46c139118a91760e96b889","externalIds":{"ArXiv":"2306.06955","DBLP":"journals/corr/abs-2306-06955","DOI":"10.1007/s10462-024-10862-8","CorpusId":259138728},"title":"A brief review of hypernetworks in deep learning"},{"paperId":"fa168fe11fa77c9728412e49596f8a808b67a4d8","externalIds":{"ArXiv":"2306.03241","CorpusId":259089314},"title":"Early Weight Averaging meets High Learning Rates for LLM Pre-training"},{"paperId":"a81f15cac6934323b70ea129fff90724f0d97525","externalIds":{"DBLP":"journals/corr/abs-2305-18413","ArXiv":"2305.18413","DOI":"10.48550/arXiv.2305.18413","CorpusId":258967892},"title":"Learning to Learn from APIs: Black-Box Data-Free Meta-Learning"},{"paperId":"ad4b365630f1c13d74d78f0f5d8cee87ef356d41","externalIds":{"DBLP":"conf/nips/MalladiGNDL0A23","ArXiv":"2305.17333","DOI":"10.48550/arXiv.2305.17333","CorpusId":258959274},"title":"Fine-Tuning Language Models with Just Forward Passes"},{"paperId":"5b8f0460d408a8688d9ee0cba127c779d3291d99","externalIds":{"ArXiv":"2305.13735","DBLP":"journals/corr/abs-2305-13735","DOI":"10.48550/arXiv.2305.13735","CorpusId":258841835},"title":"Aligning Large Language Models through Synthetic Feedback"},{"paperId":"f5c73d9e6641b018b633690102121f5605d34fb0","externalIds":{"DBLP":"conf/emnlp/YaoWT0LDC023","ArXiv":"2305.13172","DOI":"10.48550/arXiv.2305.13172","CorpusId":258833129},"title":"Editing Large Language Models: Problems, Methods, and Opportunities"},{"paperId":"c3eee48481b3b8f4be18026e389fadf9a53ad192","externalIds":{"DBLP":"journals/corr/abs-2305-10665","ArXiv":"2305.10665","DOI":"10.48550/arXiv.2305.10665","CorpusId":258762314},"title":"Content-based Unrestricted Adversarial Attack"},{"paperId":"ebf35cef5c249d90b40043fffa41f8802c27f132","externalIds":{"DBLP":"conf/acl/AkyurekAKCWT23","ArXiv":"2305.08844","ACL":"2023.acl-long.427","DOI":"10.48550/arXiv.2305.08844","CorpusId":258685337},"title":"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs"},{"paperId":"f7f1b23797c4886fcf7be184201d0a7d47baa40c","externalIds":{"DBLP":"journals/corr/abs-2305-03053","ArXiv":"2305.03053","DOI":"10.48550/arXiv.2305.03053","CorpusId":258480011},"title":"ZipIt! Merging Models from Different Tasks without Training"},{"paperId":"9e8cb8c91a0acb6e661b58ad724aa758490f2bea","externalIds":{"ArXiv":"2304.03277","DBLP":"journals/corr/abs-2304-03277","CorpusId":257985497},"title":"Instruction Tuning with GPT-4"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"bdd12d8112c7a6c8ffc15a7bde6395714b0a4b7d","externalIds":{"ArXiv":"2303.11183","DBLP":"journals/corr/abs-2303-11183","DOI":"10.1109/CVPR52729.2023.00747","CorpusId":257631876},"title":"Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning"},{"paperId":"8b32aa33601514976d88fabcb060a5cd38d34006","externalIds":{"ArXiv":"2303.02861","DBLP":"journals/corr/abs-2303-02861","DOI":"10.48550/arXiv.2303.02861","CorpusId":257365136},"title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning"},{"paperId":"76b19363b10d7ea783e4a6494eae40d73c8e9628","externalIds":{"DBLP":"journals/natmi/DingQYWYSHCCCYZWLZCLTLS23","DOI":"10.1038/s42256-023-00626-4","CorpusId":257316425},"title":"Parameter-efficient fine-tuning of large-scale pre-trained language models"},{"paperId":"b0435af3063195e8ae880489e64ccde64e6d7563","externalIds":{"DBLP":"conf/nips/LiPHGGY23","ArXiv":"2302.11520","DOI":"10.48550/arXiv.2302.11520","CorpusId":257079124},"title":"Guiding Large Language Models via Directional Stimulus Prompting"},{"paperId":"e8e035f9768a4d4e7fe9a2e167cd93d170407b1b","externalIds":{"DBLP":"journals/corr/abs-2302-08215","ArXiv":"2302.08215","DOI":"10.48550/arXiv.2302.08215","CorpusId":256900692},"title":"Aligning Language Models with Preferences through f-divergence Minimization"},{"paperId":"0e3d1457a66e442fae46c8f96886dc76aef3b085","externalIds":{"DBLP":"journals/corr/abs-2302-04870","ArXiv":"2302.04870","DOI":"10.48550/arXiv.2302.04870","CorpusId":256697131},"title":"Offsite-Tuning: Transfer Learning without Full Model"},{"paperId":"f2b0017ddd77fa38760a18145e63553105a1a236","externalIds":{"DBLP":"journals/corr/abs-2301-13688","ArXiv":"2301.13688","DOI":"10.48550/arXiv.2301.13688","CorpusId":256415991},"title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"},{"paperId":"47a541269d4ef70f37f0d3a57483312c4c6c2ad5","externalIds":{"DBLP":"conf/eacl/CohenGBG23","ACL":"2023.findings-eacl.139","ArXiv":"2301.12810","DOI":"10.48550/arXiv.2301.12810","CorpusId":256389395},"title":"Crawling The Internal Knowledge-Base of Language Models"},{"paperId":"2cd72e71299c5d62d5cdb1164df5236172d418c4","externalIds":{"DBLP":"journals/corr/abs-2301-00355","ArXiv":"2301.00355","DOI":"10.48550/arXiv.2301.00355","CorpusId":253306024},"title":"Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits"},{"paperId":"4467927fa4ebddc91b4107b8976b094e1c499d8b","externalIds":{"DBLP":"journals/corr/abs-2301-00265","ArXiv":"2301.00265","DOI":"10.48550/arXiv.2301.00265","CorpusId":255372930,"PubMed":"38490115"},"title":"Source-Free Unsupervised Domain Adaptation: A Survey"},{"paperId":"3ff08b5ca57e786d8af7b204ef94c9972bd9a61e","externalIds":{"ArXiv":"2212.09849","DBLP":"conf/iclr/Jin0P023","DOI":"10.48550/arXiv.2212.09849","CorpusId":254877510},"title":"Dataless Knowledge Fusion by Merging Weights of Language Models"},{"paperId":"8255b71fae79c92d1b3d72caa1e563c80dc36a0b","externalIds":{"DBLP":"journals/corr/abs-2212-05956","ArXiv":"2212.05956","DOI":"10.48550/arXiv.2212.05956","CorpusId":254564561},"title":"Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging"},{"paperId":"71ba5f845bd22d42003675b7cea970ca9e590bcc","externalIds":{"ArXiv":"2212.04089","DBLP":"conf/iclr/IlharcoRWSHF23","DOI":"10.48550/arXiv.2212.04089","CorpusId":254408495},"title":"Editing Models with Task Arithmetic"},{"paperId":"efb6899d2eeb27fbd099220146f511aeb78acfa9","externalIds":{"DBLP":"journals/corr/abs-2211-16327","ArXiv":"2211.16327","DOI":"10.48550/arXiv.2211.16327","CorpusId":254069358},"title":"On the power of foundation models"},{"paperId":"2f325e874aa12ebbf8f5028076e7c4c75831ce54","externalIds":{"ArXiv":"2211.13009","DBLP":"journals/corr/abs-2211-13009","DOI":"10.48550/arXiv.2211.13009","CorpusId":253801626},"title":"Federated Learning on Non-IID Graphs via Structural Knowledge Sharing"},{"paperId":"a5f4018833c6327b6dcb1f4001fcbe2e76743f3b","externalIds":{"ArXiv":"2211.02077","DBLP":"conf/nips/WuLHAW022","DOI":"10.48550/arXiv.2211.02077","CorpusId":253370634},"title":"Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization"},{"paperId":"9a1d94a930168918a1a1e1939b089d16d58d7865","externalIds":{"DBLP":"journals/corr/abs-2210-05643","ArXiv":"2210.05643","DOI":"10.48550/arXiv.2210.05643","CorpusId":252815771},"title":"A Kernel-Based View of Language Model Fine-Tuning"},{"paperId":"f4032272986e2a686019ef468bcba2fc33d3aba8","externalIds":{"DBLP":"journals/corr/abs-2210-06175","ArXiv":"2210.06175","DOI":"10.1109/SLT54892.2023.10023274","CorpusId":252846702},"title":"Exploring Efficient-Tuning Methods in Self-Supervised Speech Models"},{"paperId":"7471cb40a33e9d971a922b5dff5ca9b4a73ca609","externalIds":{"DBLP":"journals/corr/abs-2210-03329","ArXiv":"2210.03329","DOI":"10.48550/arXiv.2210.03329","CorpusId":252762125},"title":"Calibrating Factual Knowledge in Pretrained Language Models"},{"paperId":"891edceb78a274b0c2494d8176bc4d6f6e3f9cbc","externalIds":{"DBLP":"journals/corr/abs-2210-00045","ArXiv":"2210.00045","DOI":"10.48550/arXiv.2210.00045","CorpusId":252683988},"title":"Calibrating Sequence likelihood Improves Conditional Language Generation"},{"paperId":"34e1c62586f0c86af60a6ff2c3e1121c1ebd779a","externalIds":{"DBLP":"journals/corr/abs-2209-14981","ArXiv":"2209.14981","DOI":"10.48550/arXiv.2209.14981","CorpusId":252595635},"title":"Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","externalIds":{"DBLP":"journals/corr/abs-2209-14375","ArXiv":"2209.14375","DOI":"10.48550/arXiv.2209.14375","CorpusId":252596089},"title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"a9e20180153f6c139a4b6f2791b535fa6ffc3959","externalIds":{"ArXiv":"2209.04836","DBLP":"journals/corr/abs-2209-04836","DOI":"10.48550/arXiv.2209.04836","CorpusId":252199400},"title":"Git Re-Basin: Merging Models modulo Permutation Symmetries"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"1d650f1afd45c59ff907396fe8b678595dcb85ea","externalIds":{"DBLP":"conf/icml/MitchellLBMF22","ArXiv":"2206.06520","CorpusId":249642147},"title":"Memory-Based Model Editing at Scale"},{"paperId":"0f30612423381eb5d271c4ca4f4254149b0d22fa","externalIds":{"ACL":"2022.emnlp-main.259","ArXiv":"2205.11200","DBLP":"conf/emnlp/SunHQZHQ22","DOI":"10.18653/v1/2022.emnlp-main.259","CorpusId":252907221},"title":"BBTv2: Towards a Gradient-Free Future with Large Language Models"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"134e5566cc2d542039414c0de88e5e2f10dc8704","externalIds":{"ArXiv":"2204.03809","DBLP":"conf/icml/PillutlaMMRS022","DOI":"10.48550/arXiv.2204.03809","CorpusId":248069201},"title":"Federated Learning with Partial Model Personalization"},{"paperId":"52241619d157dd925160d35b75b6cb917fcffc27","externalIds":{"DBLP":"conf/icassp/Eeckth23a","ArXiv":"2203.16082","DOI":"10.1109/ICASSP49357.2023.10095837","CorpusId":247792861},"title":"Using Adapters to Overcome Catastrophic Forgetting in End-to-End Automatic Speech Recognition"},{"paperId":"54020e5fe48ebb250f27d744e20a63cac2988a84","externalIds":{"DBLP":"conf/icml/WortsmanIGRLMNF22","ArXiv":"2203.05482","DOI":"10.48550/arXiv.2203.05482","CorpusId":247362886},"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","externalIds":{"DBLP":"conf/nips/MengBAB22","ArXiv":"2202.05262","CorpusId":255825985},"title":"Locating and Editing Factual Associations in GPT"},{"paperId":"5faa744dcc28cbbdd9bd67eb703320c6e2d85e52","externalIds":{"DBLP":"journals/corr/abs-2201-08531","ArXiv":"2201.08531","CorpusId":246210164},"title":"Black-box Prompt Learning for Pre-trained Language Models"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","externalIds":{"DBLP":"journals/corr/abs-2201-08239","ArXiv":"2201.08239","CorpusId":246063428},"title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"098de88c847887fc0536561fe5f8a193156b67cf","externalIds":{"DBLP":"conf/aspdac/DupuisNOB22","DOI":"10.1109/ASP-DAC52403.2022.9712487","CorpusId":247037631},"title":"A Heuristic Exploration of Retraining-free Weight-Sharing for CNN Compression"},{"paperId":"002c58077a1f1b296468b117230a1199e91f35c2","externalIds":{"ArXiv":"2201.03514","DBLP":"conf/icml/SunSQHQ22","CorpusId":245836882},"title":"Black-Box Tuning for Language-Model-as-a-Service"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"06b20a1c6883464fcb2855adc146874fe7937c41","externalIds":{"ArXiv":"2111.09832","DBLP":"journals/corr/abs-2111-09832","CorpusId":244345933},"title":"Merging Models with Fisher-Weighted Averaging"},{"paperId":"9286ac6e9b1aacd7d93496eb4615ae7678876d2a","externalIds":{"DBLP":"journals/corr/abs-2110-11309","ArXiv":"2110.11309","CorpusId":239050360},"title":"Fast Model Editing at Scale"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","externalIds":{"ArXiv":"2110.08207","DBLP":"journals/corr/abs-2110-08207","CorpusId":239009562},"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"e553407be283d018e275f472d4d2fd709a6c9248","externalIds":{"DBLP":"conf/acl/GuHLH22","ArXiv":"2109.04332","ACL":"2022.acl-long.576","DOI":"10.18653/v1/2022.acl-long.576","CorpusId":237452236},"title":"PPT: Pre-trained Prompt Tuning for Few-shot Learning"},{"paperId":"9289826beb6206eeaf500105f7329d6d5a495d8a","externalIds":{"DBLP":"journals/corr/abs-2109-01903","ArXiv":"2109.01903","DOI":"10.1109/CVPR52688.2022.00780","CorpusId":237420687},"title":"Robust fine-tuning of zero-shot models"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"656ed155c2d345c19d9bff4b50f2ae00db8407cc","externalIds":{"ArXiv":"2106.04647","DBLP":"conf/nips/MahabadiHR21","CorpusId":235356070},"title":"Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"},{"paperId":"da454295392cf4caaa39cc465734237ffe55392f","externalIds":{"DBLP":"journals/corr/abs-2105-11259","ArXiv":"2105.11259","DOI":"10.1016/j.aiopen.2022.11.003","CorpusId":235166723},"title":"PTR: Prompt Tuning with Rules for Text Classification"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"5270b626feb66c8c363e93ba6608daae93c5003b","externalIds":{"DBLP":"journals/corr/abs-2012-00363","ArXiv":"2012.00363","MAG":"3107969673","CorpusId":227238659},"title":"Modifying Memories in Transformer Models"},{"paperId":"9cd5cadbc59806f0102dadd091ba49fb3b642bc3","externalIds":{"MAG":"3109949687","ArXiv":"2011.13464","DBLP":"journals/corr/abs-2011-13464","DOI":"10.1016/j.cobeha.2021.01.002","CorpusId":227209573},"title":"Meta-learning in natural and artificial intelligence"},{"paperId":"f8492a321d66c381637b693a24af994af41b3cdf","externalIds":{"DBLP":"journals/corr/abs-2009-07888","MAG":"3085267010","ArXiv":"2009.07888","DOI":"10.1109/TPAMI.2023.3292075","CorpusId":221761694,"PubMed":"37402188"},"title":"Transfer Learning in Deep Reinforcement Learning: A Survey"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","externalIds":{"MAG":"3138154797","ArXiv":"2006.05525","DBLP":"journals/ijcv/GouYMT21","DOI":"10.1007/s11263-021-01453-z","CorpusId":219559263},"title":"Knowledge Distillation: A Survey"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"98ef0db84e62aef969629264c9de1f4d0013f3b9","externalIds":{"ArXiv":"2005.00247","ACL":"2021.eacl-main.39","DBLP":"journals/corr/abs-2005-00247","MAG":"3023528699","DOI":"10.18653/v1/2021.eacl-main.39","CorpusId":218470208},"title":"AdapterFusion: Non-Destructive Task Composition for Transfer Learning"},{"paperId":"60b8ad6177230ad5402af409a6edb5af441baeb4","externalIds":{"MAG":"3021397474","DBLP":"conf/sigir/KhattabZ20","ArXiv":"2004.12832","DOI":"10.1145/3397271.3401075","CorpusId":216553223},"title":"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT"},{"paperId":"b145ea2049648535d6081407ebd315b072248183","externalIds":{"DBLP":"journals/corr/abs-2002-06715","MAG":"2994797252","ArXiv":"2002.06715","CorpusId":211132990},"title":"BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning"},{"paperId":"a7fb5ca30d5633ee42e8e3a149e1efc85d5336e6","externalIds":{"DBLP":"journals/tmm/LouDLCLWG20","MAG":"3000419664","DOI":"10.1109/TMM.2020.2966885","CorpusId":214298790},"title":"Towards Efficient Front-End Visual Sensing for Digital Retina: A Model-Centric Paradigm"},{"paperId":"a75649771901a4881b44c0ceafa469fcc6e6f968","externalIds":{"MAG":"3044438666","ArXiv":"1911.12543","DBLP":"journals/tacl/JiangXAN20","DOI":"10.1162/tacl_a_00324","CorpusId":208513249},"title":"How Can We Know What Language Models Know?"},{"paperId":"4a4646a5ce6b57e369403e4efea1a2e4559fe9f1","externalIds":{"DBLP":"journals/corr/abs-1911-03090","MAG":"2989195139","ArXiv":"1911.03090","CorpusId":207847573},"title":"What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning"},{"paperId":"261004890f902acd810ac4e9b1025ca5981ceedf","externalIds":{"MAG":"3098105997","ArXiv":"1910.05653","DBLP":"conf/nips/SinghJ20","CorpusId":204512191},"title":"Model Fusion via Optimal Transport"},{"paperId":"d8d680aea59295c020b9d53d78dd8d954a876845","externalIds":{"DBLP":"conf/cvpr/SunLCS19","MAG":"2952940302","ArXiv":"1812.02391","DOI":"10.1109/CVPR.2019.00049","CorpusId":54448258},"title":"Meta-Transfer Learning for Few-Shot Learning"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","externalIds":{"MAG":"2963323070","ACL":"P18-2124","ArXiv":"1806.03822","DBLP":"journals/corr/abs-1806-03822","DOI":"10.18653/v1/P18-2124","CorpusId":47018994},"title":"Know What You Donâ€™t Know: Unanswerable Questions for SQuAD"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"90dc22818bd2d97d8deaff168b0137b75a962767","externalIds":{"MAG":"2795900505","ArXiv":"1803.02999","DBLP":"journals/corr/abs-1803-02999","CorpusId":4587331},"title":"On First-Order Meta-Learning Algorithms"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","externalIds":{"DBLP":"journals/corr/WilliamsNB17","MAG":"2963846996","ArXiv":"1704.05426","ACL":"N18-1101","DOI":"10.18653/v1/N18-1101","CorpusId":3432876},"title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","externalIds":{"MAG":"2604763608","DBLP":"journals/corr/FinnAL17","ArXiv":"1703.03400","CorpusId":6719686},"title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"510e26733aaff585d65701b9f1be7ca9d5afc586","externalIds":{"DBLP":"journals/corr/ShazeerMMDLHD17","MAG":"2952339051","ArXiv":"1701.06538","CorpusId":12462234},"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"},{"paperId":"ccdc50de7a0602d3df5ed9bd9782565bbff2a8eb","externalIds":{"DBLP":"journals/tmi/TajbakhshSGHKGL16","ArXiv":"1706.00712","MAG":"2346062110","DOI":"10.1109/TMI.2016.2535302","CorpusId":32710,"PubMed":"26978662"},"title":"Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"5cf0d213f3253cd46673d955209f8463db73cc51","externalIds":{"DBLP":"journals/lre/BussoBLKMKCLN08","MAG":"2146334809","DOI":"10.1007/S10579-008-9076-6","CorpusId":11820063},"title":"IEMOCAP: interactive emotional dyadic motion capture database"},{"paperId":"3a74772a6011675ce2bdc87100fffcf4d18f5907","externalIds":{"DBLP":"journals/corr/abs-2403-09606","DOI":"10.48550/arXiv.2403.09606","CorpusId":277245089},"title":"Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey"},{"paperId":"dfcbc59870f77d2b3fff328c1ab09a655d714a01","externalIds":{"DBLP":"conf/uai/WangWSSS0SG22","CorpusId":252898974},"title":"Meta-learning without data via Wasserstein distributionally-robust model fusion"},{"paperId":"c8e842c976dfc1f4b243ffe691508f1767a2d068","externalIds":{"DBLP":"conf/nips/SongZCY022","CorpusId":258509493},"title":"Efficient and Effective Multi-task Grouping via Meta Learning on Task Combinations"},{"paperId":"fab77ef95280bd1085e15c313393da39f7cb925a","externalIds":{"CorpusId":254222797},"title":"On Convexity and Linear Mode Connectivity in Neural Networks"},{"paperId":"f507218ddc13e03d549c50bd50c026066505f80f","externalIds":{"DBLP":"journals/tsp/ZhangHHH22","DOI":"10.1109/TSP.2022.3222734","CorpusId":253621838},"title":"Distributed Reptile Algorithm for Meta-Learning Over Multi-Agent Systems"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"}]}