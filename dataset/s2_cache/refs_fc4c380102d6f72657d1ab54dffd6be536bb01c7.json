{"references":[{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","externalIds":{"ArXiv":"2312.14238","DBLP":"journals/corr/abs-2312-14238","DOI":"10.1109/CVPR52733.2024.02283","CorpusId":266521410},"title":"Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"1c9bab7ab072c619133c936b5b85160e5373e638","externalIds":{"ArXiv":"2312.14233","DBLP":"conf/cvpr/JainYS24","DOI":"10.1109/CVPR52733.2024.02644","CorpusId":266521081},"title":"VCoder: Versatile Vision Encoders for Multimodal Large Language Models"},{"paperId":"d1f925c65d56ff4de5d317a54d47d6df34b17d4e","externalIds":{"DBLP":"conf/cvpr/JiangXDCYYYZHZ24","ArXiv":"2312.06968","DOI":"10.1109/CVPR52733.2024.02553","CorpusId":266174200},"title":"Hallucination Augmented Contrastive Learning for Multimodal Large Language Model"},{"paperId":"0f9a3c5c6a54fca6be2afa0fd5fd34eed96a31e8","externalIds":{"DBLP":"conf/cvpr/YuYZHHCHL0024","ArXiv":"2312.00849","DOI":"10.1109/CVPR52733.2024.01310","CorpusId":265608723},"title":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-Grained Correctional Human Feedback"},{"paperId":"49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c","externalIds":{"DBLP":"journals/corr/abs-2311-17911","ArXiv":"2311.17911","DOI":"10.1109/CVPR52733.2024.01274","CorpusId":265498818},"title":"OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation"},{"paperId":"328eb183007bf4aefbf42437b42a15db375803e3","externalIds":{"DBLP":"conf/cvpr/LengZCLLMB24","ArXiv":"2311.16922","DOI":"10.1109/CVPR52733.2024.01316","CorpusId":265466833},"title":"Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding"},{"paperId":"2baf63dede1a96cae314c4be99bd3cf9f49b148e","externalIds":{"DBLP":"journals/corr/abs-2311-16839","ArXiv":"2311.16839","DOI":"10.48550/arXiv.2311.16839","CorpusId":265466428},"title":"Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","externalIds":{"ArXiv":"2311.10122","ACL":"2024.emnlp-main.342","DBLP":"journals/corr/abs-2311-10122","DOI":"10.48550/arXiv.2311.10122","CorpusId":265281544},"title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"de1894742b7f2e4fe02d9ff94761d6178e0a5d3c","externalIds":{"DBLP":"conf/naacl/LeePJS24","ACL":"2024.naacl-long.23","ArXiv":"2311.07362","DOI":"10.48550/arXiv.2311.07362","CorpusId":265150082},"title":"Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision"},{"paperId":"18940a4ccd955c72930ee0f8771ff710a9afeef3","externalIds":{"ArXiv":"2311.07397","DBLP":"journals/corr/abs-2311-07397","DOI":"10.48550/arXiv.2311.07397","CorpusId":265149533},"title":"An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation"},{"paperId":"bf14244669d5505f63343d4365d99d24aa6c5e82","externalIds":{"ArXiv":"2311.06607","DBLP":"conf/cvpr/LiYLMZYSLB24","DOI":"10.1109/CVPR52733.2024.02527","CorpusId":265150038},"title":"Monkey: Image Resolution and Text Label are Important Things for Large Multi-Modal Models"},{"paperId":"1e909e2a8cdacdcdff125ebcc566f37cb869a1c8","externalIds":{"ArXiv":"2311.05232","DBLP":"journals/tois/HuangYMZFWCPFQL25","DOI":"10.1145/3703155","CorpusId":265067168},"title":"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","externalIds":{"DBLP":"conf/eccv/LiuCLZLRZYSZZGL24","ArXiv":"2311.05437","DOI":"10.48550/arXiv.2311.05437","CorpusId":265067489},"title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"16e5a2d85ee023544e8f2f1921e6b6176ad7c2d0","externalIds":{"DBLP":"journals/corr/abs-2310-20357","ArXiv":"2310.20357","DOI":"10.48550/arXiv.2310.20357","CorpusId":264770799},"title":"Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model"},{"paperId":"1ddbd08ad8cf22a5c66c4242194c4286328533bf","externalIds":{"ArXiv":"2310.09478","DBLP":"journals/corr/abs-2310-09478","DOI":"10.48550/arXiv.2310.09478","CorpusId":264146906},"title":"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"},{"paperId":"458111ac5a0f73bb35a2acf55298268be25ccfa2","externalIds":{"ArXiv":"2310.07704","DBLP":"journals/corr/abs-2310-07704","DOI":"10.48550/arXiv.2310.07704","CorpusId":263834718},"title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity"},{"paperId":"e8d513bc7554a83161f2fb26c8299b471581cdb6","externalIds":{"DBLP":"conf/emnlp/0008TL0WC023","ArXiv":"2310.08475","DOI":"10.48550/arXiv.2310.08475","CorpusId":263908997},"title":"Can We Edit Multimodal Large Language Models?"},{"paperId":"c36d7bc6adeee7d16df1194c44cb66c48c9ae681","externalIds":{"ArXiv":"2310.05338","DBLP":"journals/corr/abs-2310-05338","ACL":"2024.alvr-1.4","DOI":"10.48550/arXiv.2310.05338","CorpusId":263829510},"title":"Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"93c525267e93c78309a5b28a3eb0780704125744","externalIds":{"ArXiv":"2310.00754","DBLP":"conf/iclr/ZhouCYZDFBY24","DOI":"10.48550/arXiv.2310.00754","CorpusId":263334335},"title":"Analyzing and Mitigating Object Hallucination in Large Vision-Language Models"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f","externalIds":{"DBLP":"journals/corr/abs-2309-03905","ArXiv":"2309.03905","DOI":"10.48550/arXiv.2309.03905","CorpusId":261582620},"title":"ImageBind-LLM: Multi-modality Instruction Tuning"},{"paperId":"73814a52609a9ee4c8f1b115e376b6a300ab6a57","externalIds":{"ArXiv":"2309.02301","DBLP":"journals/corr/abs-2309-02301","DOI":"10.48550/arXiv.2309.02301","CorpusId":261557047},"title":"CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning"},{"paperId":"d00735241af700d21762d2f3ca00d920241a15a4","externalIds":{"DBLP":"journals/corr/abs-2309-01219","ArXiv":"2309.01219","DOI":"10.1162/coli.a.16","CorpusId":261530162},"title":"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"},{"paperId":"bb1083425517bdac8d9a6438fcf5032543acb20e","externalIds":{"DBLP":"journals/corr/abs-2308-15126","ArXiv":"2308.15126","DOI":"10.48550/arXiv.2308.15126","CorpusId":261276646},"title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models"},{"paperId":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","externalIds":{"DBLP":"journals/corr/abs-2308-13437","ArXiv":"2308.13437","DOI":"10.48550/arXiv.2308.13437","CorpusId":261214794},"title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models"},{"paperId":"e47d276bad18f441950c8136672ae6864e95323f","externalIds":{"ArXiv":"2308.12714","DBLP":"conf/aaai/WangWHPZZDLLWH24","DOI":"10.48550/arXiv.2308.12714","CorpusId":261100735},"title":"VIGC: Visual Instruction Generation and Correction"},{"paperId":"658cd67a91da86cf451e6f1b015f762b56015172","externalIds":{"DBLP":"conf/aaai/GunjalYB24","ArXiv":"2308.06394","DOI":"10.48550/arXiv.2308.06394","CorpusId":260887222},"title":"Detecting and Preventing Hallucinations in Large Vision Language Models"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"9541cf136f442e992f10021c53081f33c73a2ed0","externalIds":{"ArXiv":"2306.03514","DBLP":"conf/cvpr/ZhangHMLLXQLLLG22","DOI":"10.1109/CVPRW63382.2024.00179","CorpusId":259089333},"title":"Recognize Anything: A Strong Image Tagging Model"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"bd5deadc58ee45b5e004378ba1d54a96bc947b4a","externalIds":{"ArXiv":"2305.14251","DBLP":"conf/emnlp/MinKLLYKIZH23","DOI":"10.48550/arXiv.2305.14251","CorpusId":258841470},"title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"},{"paperId":"206400aba5f12f734cdd2e4ab48ef6014ea60773","externalIds":{"DBLP":"journals/corr/abs-2305-10355","ArXiv":"2305.10355","DOI":"10.48550/arXiv.2305.10355","CorpusId":258740697},"title":"Evaluating Object Hallucination in Large Vision-Language Models"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","externalIds":{"DBLP":"journals/corr/abs-2305-05665","ArXiv":"2305.05665","DOI":"10.1109/CVPR52729.2023.01457","CorpusId":258564264},"title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","externalIds":{"ArXiv":"2304.15010","DBLP":"journals/corr/abs-2304-15010","DOI":"10.48550/arXiv.2304.15010","CorpusId":258418343},"title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"7cfbd36c0043098589cbaf18dca2b41d8dc24abe","externalIds":{"DBLP":"conf/eacl/DaiLJSF23","ACL":"2023.eacl-main.156","ArXiv":"2210.07688","DOI":"10.48550/arXiv.2210.07688","CorpusId":252907639},"title":"Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training"},{"paperId":"6f69730e5ebed45d59d092ddd44d1da24047d00c","externalIds":{"ArXiv":"2205.13115","DBLP":"conf/naacl/00010KDBB22","DOI":"10.48550/arXiv.2205.13115","CorpusId":249097750},"title":"Fine-grained Image Captioning with CLIP Reward"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"66ee488cf3dad5bb83804124367460edddd3c271","externalIds":{"DBLP":"conf/ijcai/LongCHY22","ArXiv":"2204.07356","DOI":"10.48550/arXiv.2204.07356","CorpusId":248218612},"title":"Vision-and-Language Pretrained Models: A Survey"},{"paperId":"3def68bd0f856886d34272840a7f81588f2bc082","externalIds":{"DBLP":"journals/corr/abs-2202-03629","ArXiv":"2202.03629","DOI":"10.1145/3571730","CorpusId":246652372},"title":"Survey of Hallucination in Natural Language Generation"},{"paperId":"889feabe31ba0d24c093ac94d54a06eecb87e3f4","externalIds":{"DBLP":"conf/emnlp/DziriMZB21","ACL":"2021.emnlp-main.168","ArXiv":"2104.08455","DOI":"10.18653/v1/2021.emnlp-main.168","CorpusId":233296059},"title":"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","externalIds":{"DBLP":"journals/corr/abs-1904-09751","MAG":"2938704169","ArXiv":"1904.09751","CorpusId":127986954},"title":"The Curious Case of Neural Text Degeneration"},{"paperId":"4921243268c81d0d6db99053a9d004852225a622","externalIds":{"MAG":"2962735233","DBLP":"conf/emnlp/RohrbachHBDS18","ArXiv":"1809.02156","ACL":"D18-1437","DOI":"10.18653/v1/D18-1437","CorpusId":52176506},"title":"Object Hallucination in Image Captioning"},{"paperId":"3bf09b2e2639add154a9fe6ff98cc373d3e90e4e","externalIds":{"ArXiv":"1803.09845","DBLP":"conf/cvpr/LuYBP18","MAG":"2795151422","DOI":"10.1109/CVPR.2018.00754","CorpusId":4406645},"title":"Neural Baby Talk"},{"paperId":"698d83e2ba10d94c2a0723e907eb297ff4a6249d","externalIds":{"DBLP":"journals/corr/abs-2310-01779","DOI":"10.48550/arXiv.2310.01779","CorpusId":263608690},"title":"HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","externalIds":{"DBLP":"journals/corr/abs-2308-12966","DOI":"10.48550/arXiv.2308.12966","CorpusId":263875678},"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"fbae34c21a6a0cbf3f9e2710b7fce0e011aec72c","externalIds":{"DBLP":"journals/corr/abs-2311-01477","DOI":"10.48550/arXiv.2311.01477","CorpusId":265019245},"title":"FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models"},{"paperId":"a6a2df6b37121a673a44ad3b06a55002d5acf192","externalIds":{"DBLP":"journals/corr/abs-2309-04041","DOI":"10.48550/arXiv.2309.04041","CorpusId":267682638},"title":"Evaluation and Mitigation of Agnosia in Multimodal Large Language Models"},{"paperId":"356770d13ad2e7b60961e5bc7368ffd3b9a2bcd9","externalIds":{"DBLP":"conf/nips/StiennonO0ZLVRA20","CorpusId":263874153},"title":"Learning to summarize with human feedback"}]}