{"references":[{"paperId":"ed4e20bcd73b1138d3bb2ed4dbbf5e8b224ef5c7","externalIds":{"ArXiv":"2311.18248","DBLP":"journals/corr/abs-2311-18248","DOI":"10.1145/3664647.3681294","CorpusId":265506728},"title":"mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model"},{"paperId":"ad13b213681b6f634bc83a264df246e83dd9a9d9","externalIds":{"DBLP":"conf/cvpr/YeXYYHL0Z024","ArXiv":"2311.04257","DOI":"10.1109/CVPR52733.2024.01239","CorpusId":265050943},"title":"mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"},{"paperId":"2f566575a246752d59438e2bde22f88680927af9","externalIds":{"ArXiv":"2311.04219","DBLP":"journals/corr/abs-2311-04219","DOI":"10.48550/arXiv.2311.04219","CorpusId":265043616},"title":"OtterHD: A High-Resolution Multi-modality Model"},{"paperId":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","externalIds":{"DBLP":"journals/corr/abs-2311-03356","ArXiv":"2311.03356","DOI":"10.1109/CVPR52733.2024.01236","CorpusId":265043538},"title":"GLaMM: Pixel Grounding Large Multimodal Model"},{"paperId":"2313afae52d98e569da2dedbf14daf9efc74e7cf","externalIds":{"DBLP":"journals/corr/abs-2311-03079","ArXiv":"2311.03079","DOI":"10.48550/arXiv.2311.03079","CorpusId":265034288},"title":"CogVLM: Visual Expert for Pretrained Language Models"},{"paperId":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","externalIds":{"DBLP":"journals/corr/abs-2311-00571","ArXiv":"2311.00571","DOI":"10.48550/arXiv.2311.00571","CorpusId":264833352},"title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing"},{"paperId":"c67a58bb5eb9cb6557a6032bb058a5cab978907f","externalIds":{"DBLP":"journals/corr/abs-2310-17956","ArXiv":"2310.17956","DOI":"10.48550/arXiv.2310.17956","CorpusId":264555208},"title":"Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare"},{"paperId":"1ddbd08ad8cf22a5c66c4242194c4286328533bf","externalIds":{"ArXiv":"2310.09478","DBLP":"journals/corr/abs-2310-09478","DOI":"10.48550/arXiv.2310.09478","CorpusId":264146906},"title":"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"},{"paperId":"458111ac5a0f73bb35a2acf55298268be25ccfa2","externalIds":{"ArXiv":"2310.07704","DBLP":"journals/corr/abs-2310-07704","DOI":"10.48550/arXiv.2310.07704","CorpusId":263834718},"title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity"},{"paperId":"d0e1a78bd77c9b6179bc773f0b7bada8df3ce39f","externalIds":{"DBLP":"conf/iui/0002SS025","ArXiv":"2310.04869","DOI":"10.1145/3708359.3712129","CorpusId":263830178},"title":"ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"5ba1525dc6d382ee0a4a1ca3c64fc5907ca64c67","externalIds":{"DBLP":"conf/iclr/GeZZGLWS24","ArXiv":"2310.01218","DOI":"10.48550/arXiv.2310.01218","CorpusId":263605855},"title":"Making LLaMA SEE and Draw with SEED Tokenizer"},{"paperId":"f2f9c02a7eb484dd7b7ac46892856e3f278eed77","externalIds":{"DBLP":"journals/corr/abs-2309-16058","ArXiv":"2309.16058","ACL":"2024.emnlp-industry.98","DOI":"10.18653/v1/2024.emnlp-industry.98","CorpusId":263137930},"title":"AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"},{"paperId":"c1e450284e7d6cac1855330a1197df8537df653f","externalIds":{"DBLP":"journals/corr/abs-2309-15112","ArXiv":"2309.15112","DOI":"10.48550/arXiv.2309.15112","CorpusId":262824937},"title":"InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","externalIds":{"DBLP":"journals/corr/abs-2309-11499","ArXiv":"2309.11499","DOI":"10.48550/arXiv.2309.11499","CorpusId":261975252},"title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","externalIds":{"DBLP":"journals/corr/abs-2309-08637","ArXiv":"2309.08637","DOI":"10.48550/arXiv.2309.08637","CorpusId":261898073},"title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f","externalIds":{"DBLP":"journals/corr/abs-2309-03905","ArXiv":"2309.03905","DOI":"10.48550/arXiv.2309.03905","CorpusId":261582620},"title":"ImageBind-LLM: Multi-modality Instruction Tuning"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","externalIds":{"DBLP":"conf/eccv/XuWWCPL24","ArXiv":"2308.16911","DOI":"10.48550/arXiv.2308.16911","CorpusId":261397321},"title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"ee0c9e8a935e047f2c030ecbfad93c2d80d642d7","externalIds":{"ArXiv":"2308.16463","DBLP":"journals/corr/abs-2308-16463","DOI":"10.48550/arXiv.2308.16463","CorpusId":261395170},"title":"Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models"},{"paperId":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","externalIds":{"DBLP":"journals/corr/abs-2308-13437","ArXiv":"2308.13437","DOI":"10.48550/arXiv.2308.13437","CorpusId":261214794},"title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models"},{"paperId":"e47d276bad18f441950c8136672ae6864e95323f","externalIds":{"ArXiv":"2308.12714","DBLP":"conf/aaai/WangWHPZZDLLWH24","DOI":"10.48550/arXiv.2308.12714","CorpusId":261100735},"title":"VIGC: Visual Instruction Generation and Correction"},{"paperId":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","externalIds":{"ArXiv":"2308.12966","CorpusId":261101015},"title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"},{"paperId":"da96ec9c32d63292e506ba8f8ea8e838df998c02","externalIds":{"DBLP":"journals/corr/abs-2308-10253","ArXiv":"2308.10253","DOI":"10.48550/arXiv.2308.10253","CorpusId":261049617},"title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"},{"paperId":"30cc95639cffca4ffa8c0eafbc502636c0c88fa5","externalIds":{"DBLP":"journals/corr/abs-2308-09936","ArXiv":"2308.09936","DOI":"10.48550/arXiv.2308.09936","CorpusId":261049015},"title":"BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","externalIds":{"DBLP":"journals/corr/abs-2308-06595","ArXiv":"2308.06595","DOI":"10.48550/arXiv.2308.06595","CorpusId":260887670},"title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"cb712ab2b3bbaef6bff02efa7295ea420b58f654","externalIds":{"DBLP":"conf/iclr/0006PGG0ZCTZZ24","ArXiv":"2308.04152","CorpusId":260704723},"title":"Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","externalIds":{"ArXiv":"2308.02490","DBLP":"journals/corr/abs-2308-02490","DOI":"10.48550/arXiv.2308.02490","CorpusId":260611572},"title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"659a12d71d8709c132ccd9ccd235f0024cae0239","externalIds":{"DBLP":"conf/iclr/Wang0LWHXCLZ0CL24","ArXiv":"2308.01907","DOI":"10.48550/arXiv.2308.01907","CorpusId":260438589},"title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","externalIds":{"DBLP":"journals/corr/abs-2308-00692","ArXiv":"2308.00692","DOI":"10.1109/CVPR52733.2024.00915","CorpusId":260351258},"title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"4309d572a37d655779f9dce6a2c98c66334132de","externalIds":{"DBLP":"journals/corr/abs-2307-16125","ArXiv":"2307.16125","DOI":"10.48550/arXiv.2307.16125","CorpusId":260334888},"title":"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","externalIds":{"DBLP":"conf/ijcai/ZhaoYGYWZSPDH024","ArXiv":"2307.09474","DOI":"10.48550/arXiv.2307.09474","CorpusId":259951197},"title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","externalIds":{"DBLP":"journals/corr/abs-2307-08581","ArXiv":"2307.08581","DOI":"10.48550/arXiv.2307.08581","CorpusId":259937702},"title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"b37b1dc72b1882858f5120f2cd6883134089a6ed","externalIds":{"ArXiv":"2307.06281","DBLP":"journals/corr/abs-2307-06281","DOI":"10.48550/arXiv.2307.06281","CorpusId":259837088},"title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"451a3f03aca4aa87b93981364842137417549e58","externalIds":{"DBLP":"journals/corr/abs-2307-04087","ArXiv":"2307.04087","DOI":"10.48550/arXiv.2307.04087","CorpusId":259501644},"title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","externalIds":{"ArXiv":"2307.03601","DBLP":"conf/eccv/ZhangSCXSZLCL24","DOI":"10.48550/arXiv.2307.03601","CorpusId":259375716},"title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","externalIds":{"ArXiv":"2307.02499","DBLP":"journals/corr/abs-2307-02499","DOI":"10.48550/arXiv.2307.02499","CorpusId":259360848},"title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"82a9b8984e26fdf234431459bdb445fbcfc3cb76","externalIds":{"ArXiv":"2307.01003","DBLP":"conf/aaai/ChenLDW24","DOI":"10.48550/arXiv.2307.01003","CorpusId":259316495},"title":"Visual Instruction Tuning with Polite Flamingo"},{"paperId":"da08e9f21ef361e0e1242f8849a18a4ea1a3d27e","externalIds":{"DBLP":"journals/corr/abs-2307-01139","ArXiv":"2307.01139","DOI":"10.48550/arXiv.2307.01139","CorpusId":259316643},"title":"SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions"},{"paperId":"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","externalIds":{"DBLP":"journals/corr/abs-2306-17107","ArXiv":"2306.17107","DOI":"10.48550/arXiv.2306.17107","CorpusId":259287523},"title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"},{"paperId":"e2a58fd18961c3941102989e3a3d0d27c615e015","externalIds":{"ArXiv":"2306.15195","DBLP":"journals/corr/abs-2306-15195","DOI":"10.48550/arXiv.2306.15195","CorpusId":259262082},"title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","externalIds":{"ArXiv":"2306.13394","DBLP":"journals/corr/abs-2306-13394","DOI":"10.48550/arXiv.2306.13394","CorpusId":259243928},"title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"0f8d12775a4685575f1489796b5dee9e11fbdfb5","externalIds":{"DBLP":"journals/corr/abs-2306-12174","ArXiv":"2306.12174","DOI":"10.48550/arXiv.2306.12174","CorpusId":259212438},"title":"OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","externalIds":{"ArXiv":"2306.09093","DBLP":"journals/corr/abs-2306-09093","DOI":"10.48550/arXiv.2306.09093","CorpusId":259165461},"title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","externalIds":{"ArXiv":"2306.08640","DBLP":"journals/corr/abs-2306-08640","DOI":"10.48550/arXiv.2306.08640","CorpusId":259164559},"title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","externalIds":{"DBLP":"journals/corr/abs-2306-07207","ArXiv":"2306.07207","DOI":"10.1145/3796716","CorpusId":259138706},"title":"Valley: Video Assistant with Large Language Model Enhanced Ability"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","externalIds":{"ArXiv":"2306.06687","DBLP":"conf/nips/YinWCSLLH0S0SO23","DOI":"10.48550/arXiv.2306.06687","CorpusId":259138958},"title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","externalIds":{"ArXiv":"2306.05424","DBLP":"journals/corr/abs-2306-05424","DOI":"10.48550/arXiv.2306.05424","CorpusId":259108333},"title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","externalIds":{"ArXiv":"2306.05425","DBLP":"journals/corr/abs-2306-05425","DOI":"10.48550/arXiv.2306.05425","CorpusId":259108295},"title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8","externalIds":{"DBLP":"journals/corr/abs-2306-04387","ArXiv":"2306.04387","DOI":"10.48550/arXiv.2306.04387","CorpusId":259095896},"title":"M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","externalIds":{"DBLP":"journals/corr/abs-2306-00890","ArXiv":"2306.00890","DOI":"10.48550/arXiv.2306.00890","CorpusId":258999820},"title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","externalIds":{"ArXiv":"2305.18752","DBLP":"conf/nips/YangSLZGLS23","DOI":"10.48550/arXiv.2305.18752","CorpusId":258967184},"title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","externalIds":{"ArXiv":"2305.16103","DBLP":"journals/corr/abs-2305-16103","DOI":"10.48550/arXiv.2305.16103","CorpusId":258887944},"title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","externalIds":{"DBLP":"journals/corr/abs-2305-16355","ArXiv":"2305.16355","ACL":"2023.tllm-1.2","DOI":"10.48550/arXiv.2305.16355","CorpusId":258947721},"title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","externalIds":{"ArXiv":"2305.15021","DBLP":"journals/corr/abs-2305-15021","DOI":"10.48550/arXiv.2305.15021","CorpusId":258865718},"title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","externalIds":{"DBLP":"journals/corr/abs-2305-15023","ArXiv":"2305.15023","DOI":"10.48550/arXiv.2305.15023","CorpusId":258865326},"title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"2ad8183c72a90511383a32ccaeea313eb85f4085","externalIds":{"DBLP":"journals/corr/abs-2305-14167","ArXiv":"2305.14167","DOI":"10.48550/arXiv.2305.14167","CorpusId":258841764},"title":"DetGPT: Detect What You Need via Reasoning"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","externalIds":{"DBLP":"conf/nips/WangCCWZZLLZQD23","ArXiv":"2305.11175","DOI":"10.48550/arXiv.2305.11175","CorpusId":258762579},"title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","externalIds":{"ArXiv":"2305.10415","DBLP":"journals/corr/abs-2305-10415","DOI":"10.48550/arXiv.2305.10415","CorpusId":258741360},"title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","externalIds":{"DBLP":"journals/corr/abs-2305-06355","ArXiv":"2305.06355","DOI":"10.1007/s11432-024-4321-9","CorpusId":258588306},"title":"VideoChat: chat-centric video understanding"},{"paperId":"81e7e82245c2f230eeb8aaaa1a2b2604c143754a","externalIds":{"ArXiv":"2305.04790","DBLP":"journals/corr/abs-2305-04790","DOI":"10.48550/arXiv.2305.04790","CorpusId":258557672},"title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","externalIds":{"DBLP":"journals/pami/LiZCWPCYLL25","ArXiv":"2305.03726","DOI":"10.1109/TPAMI.2025.3571946","CorpusId":258547300,"PubMed":"40392642"},"title":"Otter: A Multi-Modal Model With In-Context Instruction Tuning"},{"paperId":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","externalIds":{"ArXiv":"2305.03701","DBLP":"journals/corr/abs-2305-03701","DOI":"10.1109/TMM.2024.3428317","CorpusId":258546868},"title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"a677938545f63ad44c87d09f85dd231980a8476f","externalIds":{"DBLP":"journals/corr/abs-2305-00201","ArXiv":"2305.00201","DOI":"10.48550/arXiv.2305.00201","CorpusId":258426716},"title":"Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","externalIds":{"ArXiv":"2304.15010","DBLP":"journals/corr/abs-2304-15010","DOI":"10.48550/arXiv.2304.15010","CorpusId":258418343},"title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"9e8cb8c91a0acb6e661b58ad724aa758490f2bea","externalIds":{"ArXiv":"2304.03277","DBLP":"journals/corr/abs-2304-03277","CorpusId":257985497},"title":"Instruction Tuning with GPT-4"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"ecd0b23e4828fca585a05eff56563852d35858d9","externalIds":{"DOI":"10.1007/s00113-023-01296-y","CorpusId":256787765,"PubMed":"36763148"},"title":"ChatGPT"},{"paperId":"0c0300f53c01ae609c97395c98de4c9d85d92876","externalIds":{"DBLP":"conf/acl/XuSH23","ACL":"2023.acl-long.641","ArXiv":"2212.10773","DOI":"10.48550/arXiv.2212.10773","CorpusId":254926784},"title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"},{"paperId":"1b31dbf44e68b698120552366df03e6e35a1e428","externalIds":{"ArXiv":"2212.08051","DBLP":"conf/cvpr/DeitkeSSWMVSEKF23","DOI":"10.1109/CVPR52729.2023.01263","CorpusId":254685588},"title":"Objaverse: A Universe of Annotated 3D Objects"},{"paperId":"a02fbaf22237a1aedacb1320b6007cd70c1fe6ec","externalIds":{"DBLP":"journals/corr/abs-2212-04356","ArXiv":"2212.04356","CorpusId":252923993},"title":"Robust Speech Recognition via Large-Scale Weak Supervision"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"b611c501269224702d1a9942c8600a31ec66ab28","externalIds":{"ArXiv":"2203.10244","DBLP":"conf/acl/MasryLTJH22","ACL":"2022.findings-acl.177","DOI":"10.48550/arXiv.2203.10244","CorpusId":247593713},"title":"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"},{"paperId":"8f6c652a392995bd047a2f7b94474ab1e6e23ff0","externalIds":{"DBLP":"conf/cvpr/AzumaMKK22","ArXiv":"2112.10482","DOI":"10.1109/CVPR52688.2022.01854","CorpusId":245334889},"title":"ScanQA: 3D Question Answering for Spatial Scene Understanding"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"dd2819016c6bf244c39b3e6707b60389bbdbcd21","externalIds":{"DBLP":"conf/cvpr/YuTR00L22","ArXiv":"2111.14819","DOI":"10.1109/CVPR52688.2022.01871","CorpusId":244714512},"title":"Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"147756438919051f46b39de1957cd90db3096b88","externalIds":{"DBLP":"journals/corr/abs-2104-02691","ArXiv":"2104.02691","DOI":"10.1109/CVPR46437.2021.01659","CorpusId":233033796},"title":"Localizing Visual Sounds the Hard Way"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","externalIds":{"DBLP":"journals/corr/abs-2102-09542","ArXiv":"2102.09542","DOI":"10.1109/ISBI48211.2021.9434010","CorpusId":231951663},"title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"f05126c1a792ea64a7af0c8c68b03bcddec5b297","externalIds":{"DBLP":"journals/corr/abs-2101-11272","ArXiv":"2101.11272","DOI":"10.1609/aaai.v35i15.17635","CorpusId":231719058},"title":"VisualMRC: Machine Reading Comprehension on Document Images"},{"paperId":"2b64e5b2ca9cae71207fe7887766e4b7050de307","externalIds":{"MAG":"3109959312","ArXiv":"2011.13681","DBLP":"journals/corr/abs-2011-13681","CorpusId":227210234},"title":"Point and Ask: Incorporating Pointing into Visual Question Answering"},{"paperId":"3e3df220673388402b6b114eab68a9c5396210b1","externalIds":{"MAG":"3109340983","DBLP":"journals/iotj/ZhangT21","ArXiv":"2011.08612","DOI":"10.1109/JIOT.2020.3039359","CorpusId":226975900},"title":"Empowering Things With Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things"},{"paperId":"72e0dccf59f126a64f970fe9f4712b3221a3be8c","externalIds":{"DBLP":"journals/corr/abs-2010-12435","MAG":"3093937711","ArXiv":"2010.12435","DOI":"10.36227/techrxiv.13127537.v1","CorpusId":225062587},"title":"Pathological Visual Question Answering"},{"paperId":"b40bfcf339de3f0dba08fabb2b58b9368ff4c51a","externalIds":{"DBLP":"conf/wacv/MathewKJ21","ArXiv":"2007.00398","MAG":"3040138106","DOI":"10.1109/WACV48630.2021.00225","CorpusId":220280200},"title":"DocVQA: A Dataset for VQA on Document Images"},{"paperId":"2b37cc68c9819cf0f980676935007d4135d8ac8c","externalIds":{"ArXiv":"1910.09387","DBLP":"journals/corr/abs-1910-09387","MAG":"2981182029","DOI":"10.1109/ICASSP40776.2020.9052990","CorpusId":204800739},"title":"Clotho: an Audio Captioning Dataset"},{"paperId":"52e47212cf498953d428225386a0c1a695ae5c4a","externalIds":{"MAG":"2948685905","DBLP":"journals/isci/LiGWGLK19","DOI":"10.1016/J.INS.2019.06.011","CorpusId":195580308},"title":"Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening"},{"paperId":"c5ff974a69fd0c760b4855b819e61e89f31cfffe","externalIds":{"MAG":"2983943451","DBLP":"conf/iccv/0005LZPYZLS19","DOI":"10.1109/ICCV.2019.00852","CorpusId":207967883},"title":"Objects365: A Large-Scale, High-Quality Dataset for Object Detection"},{"paperId":"ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96","externalIds":{"DBLP":"journals/corr/abs-1909-02164","MAG":"2996657743","ArXiv":"1909.02164","CorpusId":198917339},"title":"TabFact: A Large-scale Dataset for Table-based Fact Verification"},{"paperId":"1097cf8cf5961589ff693b069002e7181e24e631","externalIds":{"DBLP":"conf/icdar/0001SSC19","MAG":"3004268082","DOI":"10.1109/ICDAR.2019.00156","CorpusId":209413409},"title":"OCR-VQA: Visual Question Answering by Reading Text in Images"},{"paperId":"4f2c1af57c056102806a184517313804f66e7447","externalIds":{"ArXiv":"1906.02467","DBLP":"conf/aaai/YuXYYZZT19","MAG":"2964220823","DOI":"10.1609/aaai.v33i01.33019127","CorpusId":69645185},"title":"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"},{"paperId":"f902a64f7d08aaa6bfca7463e8729952ddc6134e","externalIds":{"DBLP":"conf/cvpr/GuptaDG19","MAG":"2965565989","ArXiv":"1908.03195","DOI":"10.1109/CVPR.2019.00550","CorpusId":195441339},"title":"LVIS: A Dataset for Large Vocabulary Instance Segmentation"},{"paperId":"c4798919e74411d87f7745840e45b8bcf61128ff","externalIds":{"MAG":"2945761034","DBLP":"conf/naacl/KimKLK19","ACL":"N19-1011","DOI":"10.18653/v1/N19-1011","CorpusId":174799768},"title":"AudioCaps: Generating Captions for Audios in The Wild"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","externalIds":{"DBLP":"conf/cvpr/MarinoRFM19","ArXiv":"1906.00067","MAG":"2947312908","DOI":"10.1109/CVPR.2019.00331","CorpusId":173991173},"title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"af1f7739283bdbd2b7a94903041f6d6afd991907","externalIds":{"MAG":"2936135081","DBLP":"journals/corr/abs-1904-08920","ArXiv":"1904.08920","DOI":"10.1109/CVPR.2019.00851","CorpusId":85553602},"title":"Towards VQA Models That Can Read"},{"paperId":"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796","externalIds":{"ArXiv":"1904.03493","DBLP":"conf/iccv/WangWCLWW19","MAG":"2925419377","DOI":"10.1109/ICCV.2019.00468","CorpusId":102352148},"title":"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"044c56af7005c2013ce24c7199af716319378d7f","externalIds":{"MAG":"2964213933","ArXiv":"1901.09107","DBLP":"conf/cvpr/AlAmriCD0CEBMHA19","DOI":"10.1109/CVPR.2019.00774","CorpusId":59316836},"title":"Audio Visual Scene-Aware Dialog"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","externalIds":{"MAG":"2958882215","ArXiv":"1811.10830","DBLP":"conf/cvpr/ZellersBFC19","DOI":"10.1109/CVPR.2019.00688","CorpusId":53734356},"title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"1553084dcbf2235428e7dbf57b57e567c5ea4d1f","externalIds":{"MAG":"2889048668","ArXiv":"1808.10583","DBLP":"journals/corr/abs-1808-10583","CorpusId":52145151},"title":"AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c","externalIds":{"DBLP":"conf/cvpr/Gurari0SGLGLB18","MAG":"2788643321","ArXiv":"1802.08218","DOI":"10.1109/CVPR.2018.00380","CorpusId":3831582},"title":"VizWiz Grand Challenge: Answering Visual Questions from Blind People"},{"paperId":"ca011427853d34ce4ec9ccafde8a70c9eacc3e21","externalIds":{"PubMedCentral":"5816885","MAG":"2794284562","DBLP":"journals/cin/VoulodimosDDP18","DOI":"10.1155/2018/7068349","CorpusId":3557281,"PubMed":"29487619"},"title":"Deep Learning for Computer Vision: A Brief Review"},{"paperId":"057b80e235b10799d03876ad25465208a4c64caf","externalIds":{"DBLP":"conf/mm/XuZX0Z0Z17","MAG":"2765716052","DOI":"10.1145/3123266.3123427","CorpusId":3864050},"title":"Video Question Answering via Gradually Refined Attention over Appearance and Motion"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"96dd1fc39a368d23291816d57763bc6eb4f7b8d6","externalIds":{"MAG":"2963916161","DBLP":"journals/corr/KrishnaHRLN17","ArXiv":"1705.00754","DOI":"10.1109/ICCV.2017.83","CorpusId":1026139},"title":"Dense-Captioning Events in Videos"},{"paperId":"b2f521c02c6ed3080c5fe123e938cdf4555e6fd2","externalIds":{"DBLP":"conf/cvpr/JangSYKK17","ArXiv":"1704.04497","MAG":"2606982687","DOI":"10.1109/CVPR.2017.149","CorpusId":3030826},"title":"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"d997beefc0922d97202789d2ac307c55c2c52fba","externalIds":{"MAG":"2950642167","DBLP":"conf/cvpr/QiSMG17","ArXiv":"1612.00593","DOI":"10.1109/CVPR.2017.16","CorpusId":5115938},"title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"},{"paperId":"d0d5aa7f797113c825053f4c4fd3772dc3601139","externalIds":{"MAG":"2949643062","DBLP":"journals/corr/SigurdssonRFLG16","ArXiv":"1607.07429","DOI":"10.1609/hcomp.v4i1.13290","CorpusId":3614922},"title":"Much Ado About Time: Exhaustive Annotation of Temporal Data"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"e65142010431ffc089b272a1174214e00693e503","externalIds":{"MAG":"2963109634","ArXiv":"1511.02283","DBLP":"journals/corr/MaoHTCYM15","DOI":"10.1109/CVPR.2016.9","CorpusId":8745888},"title":"Generation and Comprehension of Unambiguous Object Descriptions"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","externalIds":{"ArXiv":"1505.04870","DBLP":"conf/iccv/PlummerWCCHL15","MAG":"2568262903","DOI":"10.1007/s11263-016-0965-7","CorpusId":6941275},"title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","externalIds":{"ACL":"D14-1086","DBLP":"conf/emnlp/KazemzadehOMB14","MAG":"2251512949","DOI":"10.3115/v1/D14-1086","CorpusId":6308361},"title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","externalIds":{"MAG":"2949429431","ArXiv":"1409.1556","DBLP":"journals/corr/SimonyanZ14a","CorpusId":14124313},"title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"7c8a51d04522496c43db68f2582efd45eaf59fea","externalIds":{"MAG":"1920022804","DBLP":"conf/cvpr/WuSKYZTX15","DOI":"10.1109/CVPR.2015.7298801","CorpusId":206592833},"title":"3D ShapeNets: A deep representation for volumetric shapes"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"44040913380206991b1991daf1192942e038fe31","externalIds":{"ACL":"Q14-1006","DBLP":"journals/tacl/YoungLHH14","MAG":"2185175083","DOI":"10.1162/tacl_a_00166","CorpusId":3104920},"title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"02b28f3b71138a06e40dbd614abf8568420ae183","externalIds":{"DBLP":"conf/icvgip/NilsbackZ08","MAG":"2533598788","DOI":"10.1109/ICVGIP.2008.47","CorpusId":15193013},"title":"Automated Flower Classification over a Large Number of Classes"},{"paperId":"ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2","externalIds":{"MAG":"2155904486","DBLP":"conf/cvpr/LiFP04","DOI":"10.1016/j.cviu.2005.09.012","CorpusId":2156851},"title":"Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"},{"paperId":"db95b0785118db34b553e6b5f137ea2bee5a0639","externalIds":{"MAG":"2079148163","DOI":"10.1136/BMJ.4.5674.53-E","CorpusId":383200},"title":"Cats and dogs"},{"paperId":"8b55402ffee2734bfc7d5d7595500916e1ef04e8","externalIds":{"MAG":"2904565150","DBLP":"conf/iccv/AgrawalAD0CJ0BP19","ArXiv":"1812.08658","DOI":"10.1109/ICCV.2019.00904","CorpusId":56517630},"title":"nocaps: novel object captioning at scale"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","externalIds":{"CorpusId":53712941},"title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"eae500ce89f7cc5cd48a58c4ba7edb2f02826b85","externalIds":{"MAG":"780950768","CorpusId":16632981},"title":"Collecting a Large-scale Dataset of Fine-grained Cars"}]}