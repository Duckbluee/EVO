{"references":[{"paperId":"36f25cf98cec366674cbd0911ce72ae377ce087b","externalIds":{"ArXiv":"2602.00557","CorpusId":285271018},"title":"ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation"},{"paperId":"92a09cdfc19f3f582d89c28c1b4f386299cc69e1","externalIds":{"DBLP":"conf/iclr/RaviGHHR0KRRGMP25","ArXiv":"2408.00714","DOI":"10.48550/arXiv.2408.00714","CorpusId":271601113},"title":"SAM 2: Segment Anything in Images and Videos"},{"paperId":"3dd7e010f0bf64db7332c757020c4ef26c52f358","externalIds":{"DBLP":"journals/tcsv/ZhengYWZHHL24","DOI":"10.1109/TCSVT.2023.3287201","CorpusId":259412445},"title":"Dynamic Spatial Focus for Efficient Compressed Video Action Recognition"},{"paperId":"eb119d5987506828aba1df3656886c25ef0cde56","externalIds":{"DBLP":"journals/corr/abs-2311-04414","ArXiv":"2311.04414","DOI":"10.1109/WACV57701.2024.00680","CorpusId":265050493},"title":"Learning the What and How of Annotation in Video Object Segmentation"},{"paperId":"f5f2c05d9e4bc98a345e3433c887f2dd7cef482e","externalIds":{"ArXiv":"2311.01989","DBLP":"journals/corr/abs-2311-01989","DOI":"10.48550/arXiv.2311.01989","CorpusId":265019004},"title":"Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation"},{"paperId":"88b0416955d0ff5bb06366f42577d1de0f84995a","externalIds":{"DBLP":"journals/corr/abs-2310-18709","ArXiv":"2310.18709","DOI":"10.1109/CVPR52734.2025.01265","CorpusId":264590605},"title":"Audio-Visual Instance Segmentation"},{"paperId":"f8e63df903d95f08839016db3d59d07af7f1275f","externalIds":{"DBLP":"journals/corr/abs-2310-16003","ArXiv":"2310.16003","DOI":"10.48550/arXiv.2310.16003","CorpusId":264439289},"title":"CVPR 2023 Text Guided Video Editing Competition"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","externalIds":{"ArXiv":"2310.10647","DBLP":"journals/corr/abs-2310-10647","DOI":"10.1145/3696415","CorpusId":264172934},"title":"A Survey on Video Diffusion Models"},{"paperId":"e36bf81abd714922655f0ecacb07c80601e056ef","externalIds":{"ArXiv":"2310.09021","DBLP":"journals/corr/abs-2310-09021","DOI":"10.48550/arXiv.2310.09021","CorpusId":264128101},"title":"Generative AI-driven Semantic Communication Framework for NextG Wireless Network"},{"paperId":"581b81302725e16ea3c8d5c93f1c5dacd91aa3bc","externalIds":{"DBLP":"journals/corr/abs-2310-08683","ArXiv":"2310.08683","DOI":"10.48550/arXiv.2310.08683","CorpusId":264127970},"title":"Virtual Augmented Reality for Atari Reinforcement Learning"},{"paperId":"30d5bce1792a58a49ab4e7d91d8ae1375ce938d2","externalIds":{"DBLP":"conf/icra/ChuHTDGF24","ArXiv":"2310.06992","DOI":"10.1109/ICRA57147.2024.10611726","CorpusId":263835071},"title":"Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models"},{"paperId":"05df852d87566d335bbbf1864d191910205f482b","externalIds":{"DBLP":"conf/iccvw/KristanMDFCZLDZ23","DOI":"10.1109/ICCVW60793.2023.00195","CorpusId":266187044},"title":"The First Visual Object Tracking Segmentation VOTS2023 Challenge Results"},{"paperId":"e009e3015fce1bd0cca7f44393b7ffae138a5458","externalIds":{"ArXiv":"2310.00783","DBLP":"journals/corr/abs-2310-00783","DOI":"10.48550/arXiv.2310.00783","CorpusId":263605393},"title":"Propagating Semantic Labels in Video Data"},{"paperId":"5dad17c35786c83fab905664612d67f5801afdee","externalIds":{"ArXiv":"2309.13863","DBLP":"conf/iros/LinMA0WL0Y24","DOI":"10.1109/IROS58592.2024.10802079","CorpusId":262460735},"title":"SuPerPM: A Surgical Perception Framework Based on Deep Point Matching Learned from Physical Constrained Simulation Data"},{"paperId":"f10ec6b6e0deb4fc0f9aec807de0b4964927ad17","externalIds":{"DBLP":"journals/titb/KimJCKLRKLZLLL25","ArXiv":"2309.13539","DOI":"10.1109/JBHI.2025.3540306","CorpusId":262466332,"PubMed":"40031601"},"title":"MediViSTA: Medical Video Segmentation Via Temporal Fusion SAM Adaptation for Echocardiography"},{"paperId":"b6f2bf38db13fe0d9a9f6387bd4b8dfab7e5d040","externalIds":{"DBLP":"conf/aaai/WangLLD0L24","ArXiv":"2309.07929","DOI":"10.48550/arXiv.2309.07929","CorpusId":261881829},"title":"Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer"},{"paperId":"f97bef41ca29cb05fc1ffcde18a6db1886b4d5da","externalIds":{"ArXiv":"2309.06728","DBLP":"journals/corr/abs-2309-06728","DOI":"10.48550/arXiv.2309.06728","CorpusId":261705977},"title":"Leveraging Foundation models for Unsupervised Audio-Visual Segmentation"},{"paperId":"ac7357374fcbfd48caa02777ddcc30bd19295328","externalIds":{"DBLP":"journals/corr/abs-2309-03903","ArXiv":"2309.03903","DOI":"10.1109/ICCV51070.2023.00127","CorpusId":261582334},"title":"Tracking Anything with Decoupled Video Segmentation"},{"paperId":"ee6dc7fa5a426dd579ff595c0bfcdda528397ae9","externalIds":{"ArXiv":"2308.14461","DBLP":"conf/iccvw/FilliouxGCMBBCJ23","DOI":"10.1109/ICCVW60793.2023.00425","CorpusId":261244212},"title":"Spatio-Temporal Analysis of Patient-Derived Organoid Videos Using Deep Learning for the Prediction of Drug Efficacy"},{"paperId":"53e8d327e7ceda6f4efd321752da57edbaee6257","externalIds":{"DBLP":"journals/fgcs/BenjdiraKA25","ArXiv":"2308.11236","DOI":"10.1016/j.future.2025.107723","CorpusId":261065191},"title":"ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts"},{"paperId":"6eaf524a64de7c079eaa7a10621b0182523ec5e4","externalIds":{"ArXiv":"2308.11774","DBLP":"journals/corr/abs-2308-11774","DOI":"10.1117/12.3008392","CorpusId":261076398},"title":"SAMSNeRF: segment anything model (SAM) guides dynamic surgical scene reconstruction by neural radiance field (NeRF)"},{"paperId":"2cfbeb0c2a0c268773e3b952f022aada9cfd0538","externalIds":{"DBLP":"journals/corr/abs-2308-08746","ArXiv":"2308.08746","DOI":"10.48550/arXiv.2308.08746","CorpusId":261031148},"title":"SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation"},{"paperId":"10603f24428220cb3b78b0c23fb7e24cbee71f95","externalIds":{"ArXiv":"2308.07749","DBLP":"journals/corr/abs-2308-07749","DOI":"10.48550/arXiv.2308.07749","CorpusId":260900206},"title":"Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model"},{"paperId":"6d1eb9d2dcc89b4019e22cc89b2b50584f33bb60","externalIds":{"ArXiv":"2308.06974","DBLP":"journals/corr/abs-2308-06974","DOI":"10.48550/arXiv.2308.06974","CorpusId":260887496},"title":"A One Stop 3D Target Reconstruction and multilevel Segmentation Method"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","externalIds":{"ArXiv":"2308.00675","DBLP":"journals/corr/abs-2308-00675","DOI":"10.48550/arXiv.2308.00675","CorpusId":260351459},"title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"fe37211c32ee4ae6a8fb47ac81e8c24147b9ec99","externalIds":{"DBLP":"conf/aaai/ZhouHTY24","ArXiv":"2307.16586","DOI":"10.48550/arXiv.2307.16586","CorpusId":260334156},"title":"SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model"},{"paperId":"730a68f75b5c647d49fd8b17135d478e32d912bc","externalIds":{"ArXiv":"2307.13974","DBLP":"journals/corr/abs-2307-13974","DOI":"10.48550/arXiv.2307.13974","CorpusId":260164798},"title":"Tracking Anything in High Quality"},{"paperId":"83c48aa341850af478247e3b34ba1ee1db9f1236","externalIds":{"ArXiv":"2307.10802","DBLP":"journals/corr/abs-2307-10802","DOI":"10.48550/arXiv.2307.10802","CorpusId":259991096},"title":"Meta-Transformer: A Unified Framework for Multimodal Learning"},{"paperId":"6f18d59ad9a4e72fd9a8fdb19e05e251e740452d","externalIds":{"ArXiv":"2307.02508","DBLP":"journals/corr/abs-2307-02508","DOI":"10.48550/arXiv.2307.02508","CorpusId":259360624},"title":"ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking"},{"paperId":"d8156b4043c2eeba96d52fd31d215c607c860fb1","externalIds":{"DBLP":"journals/nn/LiZTZLL26","ArXiv":"2307.00997","DOI":"10.48550/arXiv.2307.00997","CorpusId":259316417,"PubMed":"40854288"},"title":"RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation"},{"paperId":"725b762ba2c18d2cbd621b942ba544d82faa1375","externalIds":{"DBLP":"conf/wacv/RajicKTTD025","ArXiv":"2307.01197","DOI":"10.1109/WACV61041.2025.00901","CorpusId":259316593},"title":"Segment Anything Meets Point Tracking"},{"paperId":"ebdb59f41d6cf5146fcc6bbf7784e0595338ddc7","externalIds":{"DBLP":"journals/corr/abs-2307-01024","ArXiv":"2307.01024","DOI":"10.1109/ICARM62033.2024.10715901","CorpusId":259316137},"title":"SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation"},{"paperId":"1a61b1f46ef6afb58ee2ecb689c757f8ad2f84d0","externalIds":{"DBLP":"conf/cvpr/WangLL0LYZ0W24","ArXiv":"2307.00040","DOI":"10.1109/CVPR52733.2024.00891","CorpusId":259316865},"title":"Disco: Disentangled Control for Realistic Human Dance Generation"},{"paperId":"cc005225b7ed8b68b70b20a0c9a9f21b6c2708c3","externalIds":{"DBLP":"journals/corr/abs-2306-17075","ArXiv":"2306.17075","DOI":"10.48550/arXiv.2306.17075","CorpusId":259287484},"title":"Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization"},{"paperId":"806409319ef6fab1f7c95962c985866f762277d6","externalIds":{"DBLP":"journals/corr/abs-2306-14056","ArXiv":"2306.14056","DOI":"10.1109/ECMR59166.2023.10256271","CorpusId":259251746},"title":"Stable Yaw Estimation of Boats from the Viewpoint of UAVs and USVs"},{"paperId":"c01e94a36be5b3578fedb17205205b330290a778","externalIds":{"DBLP":"journals/corr/abs-2306-12156","ArXiv":"2306.12156","DOI":"10.48550/arXiv.2306.12156","CorpusId":259212104},"title":"Fast Segment Anything"},{"paperId":"ea56a40bbad010f7fb854ff66a1638fa927e4041","externalIds":{"DBLP":"journals/corr/abs-2306-03908","ArXiv":"2306.03908","DOI":"10.48550/arXiv.2306.03908","CorpusId":259088699},"title":"SAM3D: Segment Anything in 3D Scenes"},{"paperId":"51258f439f11eb7abb55eacc60a42a5e804a8083","externalIds":{"ArXiv":"2306.02291","DBLP":"journals/corr/abs-2306-02291","DOI":"10.48550/arXiv.2306.02291","CorpusId":259075828},"title":"3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW"},{"paperId":"36fff28902cfb6c99c7c98f284639ad8e0133c44","externalIds":{"DBLP":"journals/corr/abs-2306-01567","ArXiv":"2306.01567","DOI":"10.48550/arXiv.2306.01567","CorpusId":259063834},"title":"Segment Anything in High Quality"},{"paperId":"7073e3bc142b14579a999bb45a4b10d94394be25","externalIds":{"DBLP":"journals/corr/abs-2305-16698","ArXiv":"2305.16698","DOI":"10.1109/TCSVT.2023.3320688","CorpusId":258947383},"title":"Detect Any Shadow: Segment Anything for Video Shadow Detection"},{"paperId":"13a5140fc0b269c408ecfc666cb297410bc753c5","externalIds":{"DBLP":"conf/iclr/LiuZL0WS24","ArXiv":"2305.13310","DOI":"10.48550/arXiv.2305.13310","CorpusId":258832414},"title":"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching"},{"paperId":"529191401a8a5f0a8bdb2a1c01301d76af585a3a","externalIds":{"DBLP":"journals/corr/abs-2305-13077","ArXiv":"2305.13077","DOI":"10.48550/arXiv.2305.13077","CorpusId":258832670},"title":"ControlVideo: Training-free Controllable Text-to-Video Generation"},{"paperId":"251445d8b22b1c25ccad96f284c085dca49b57f3","externalIds":{"DBLP":"journals/pr/ZhangZDDZ25","ArXiv":"2305.12659","DOI":"10.1016/j.patcog.2024.111100","CorpusId":258833355},"title":"UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model"},{"paperId":"04c36e581a4114096c07598eec310028206c4cc9","externalIds":{"DBLP":"journals/corr/abs-2305-10503","ArXiv":"2305.10503","DOI":"10.48550/arXiv.2305.10503","CorpusId":258762719},"title":"OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields"},{"paperId":"5f51eda9f7abddca027941d50fb0b6bf6f508eff","externalIds":{"ArXiv":"2305.08850","DBLP":"journals/corr/abs-2305-08850","DOI":"10.48550/arXiv.2305.08850","CorpusId":258686590},"title":"Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts"},{"paperId":"1856bebc4cb35e68368d9c83bd2ac2d26cd4bcfa","externalIds":{"DBLP":"journals/corr/abs-2305-08196","ArXiv":"2305.08196","DOI":"10.48550/arXiv.2305.08196","CorpusId":258686670},"title":"A Comprehensive Survey on Segment Anything Model for Vision and Beyond"},{"paperId":"998d03198aa924791dea4ba2b8f44b0c9beb7b7a","externalIds":{"DBLP":"journals/corr/abs-2306-06211","ArXiv":"2306.06211","DOI":"10.48550/arXiv.2306.06211","CorpusId":259138732},"title":"A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering"},{"paperId":"5cff0de864f4aa860b329f94924be43129b8dbce","externalIds":{"DBLP":"journals/corr/abs-2305-06524","ArXiv":"2305.06524","DOI":"10.48550/arXiv.2305.06524","CorpusId":258615151},"title":"Can SAM Boost Video Super-Resolution?"},{"paperId":"bbdc4118df106d4ba7af9d7d94d7f0a1144c11e2","externalIds":{"ArXiv":"2305.06558","DBLP":"journals/corr/abs-2305-06558","DOI":"10.48550/arXiv.2305.06558","CorpusId":258615204},"title":"Segment and Track Anything"},{"paperId":"757ed5bd81d6ddf10d3a119d208ba2a9a279c051","externalIds":{"ArXiv":"2305.03678","CorpusId":258547223},"title":"Towards Segment Anything Model (SAM) for Medical Image Segmentation: A Survey"},{"paperId":"90720183d174744680fa7a3028996b3399c64fa2","externalIds":{"DBLP":"conf/iclr/Zhang0GYP000024","ArXiv":"2305.03048","DOI":"10.48550/arXiv.2305.03048","CorpusId":258480276},"title":"Personalize Segment Anything Model with One Shot"},{"paperId":"f00d63cdf9bbd0649a549c7ed3d3012238f38dc8","externalIds":{"ArXiv":"2305.01836","DBLP":"journals/corr/abs-2305-01836","DOI":"10.48550/arXiv.2305.01836","CorpusId":258461460},"title":"AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation"},{"paperId":"c5848778879345fb053d6b7a4d04b15158a5ecd9","externalIds":{"DBLP":"journals/corr/abs-2305-01443","ArXiv":"2305.01443","DOI":"10.48550/arXiv.2305.01443","CorpusId":258436950},"title":"Scalable Mask Annotation for Video Text Spotting"},{"paperId":"3baff4c1bf394fb4a7c686ea9b95101e24f7f3f9","externalIds":{"DBLP":"journals/corr/abs-2304-14674","ArXiv":"2304.14674","DOI":"10.48550/arXiv.2304.14674","CorpusId":258418238},"title":"SAM Meets Robotic Surgery: An Empirical Study in Robustness Perspective"},{"paperId":"564dbc73a0099f2aac8f9edcda4f98aaa2cddb14","externalIds":{"DBLP":"journals/corr/abs-2304-13425","ArXiv":"2304.13425","DOI":"10.48550/arXiv.2304.13425","CorpusId":258332112},"title":"Learnable Ophthalmology SAM"},{"paperId":"c0cf0971c153a84bbf0729d289b3b960969bb5dd","externalIds":{"ArXiv":"2304.11968","DBLP":"journals/corr/abs-2304-11968","DOI":"10.48550/arXiv.2304.11968","CorpusId":258298171},"title":"Track Anything: Segment Anything Meets Videos"},{"paperId":"bb2ce7b7b2bf14859124304a5cf00849d65b3b9b","externalIds":{"ArXiv":"2304.11595","DBLP":"journals/corr/abs-2304-11595","DOI":"10.48550/arXiv.2304.11595","CorpusId":258298820},"title":"Segment Anything in Non-Euclidean Domains: Challenges and Opportunities"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"34e95464be6cc3041041f145758493401b8a75e8","externalIds":{"DBLP":"journals/corr/abs-2304-06025","ArXiv":"2304.06025","DOI":"10.1109/ICCV51070.2023.02073","CorpusId":258078892},"title":"DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"07f07d4d59fdbc3596284f51057cb006779d42c1","externalIds":{"DBLP":"journals/corr/abs-2304-02020","ArXiv":"2304.02020","DOI":"10.1145/3664930","CorpusId":257952516},"title":"A Bibliometric Review of Large Language Models Research from 2017 to 2023"},{"paperId":"923a03032014a12c4e8b26511c0394e1b915fe74","externalIds":{"DBLP":"conf/iccv/KhachatryanMTHW23","ArXiv":"2303.13439","DOI":"10.1109/ICCV51070.2023.01462","CorpusId":257687280},"title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"},{"paperId":"3faaca5bfafe11f41617bb28199528772f0e254f","externalIds":{"ArXiv":"2303.09514","DBLP":"journals/corr/abs-2303-09514","DOI":"10.1109/ISBI53787.2023.10230819","CorpusId":257557473},"title":"MATIS: Masked-Attention Transformers for Surgical Instrument Segmentation"},{"paperId":"c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0","externalIds":{"DBLP":"conf/eccv/LiuZRLZYJLYSZZ24","ArXiv":"2303.05499","DOI":"10.48550/arXiv.2303.05499","CorpusId":257427307},"title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"},{"paperId":"6e49b9891675b66cff80373777031638aff21ed6","externalIds":{"DBLP":"conf/cvpr/ShiHLZCSQDL23","ArXiv":"2303.01237","DOI":"10.1109/CVPR52729.2023.00160","CorpusId":257280104},"title":"FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation"},{"paperId":"746bb45433f6b24d3ae64d6cd51c4e9d00a0ffa7","externalIds":{"DBLP":"journals/corr/abs-2302-10035","ArXiv":"2302.10035","DOI":"10.1007/s11633-022-1410-8","CorpusId":257038341},"title":"Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"},{"paperId":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","externalIds":{"ArXiv":"2302.07842","DBLP":"journals/tmlr/MialonDLNPRRSDC23","CorpusId":256868474},"title":"Augmented Language Models: a Survey"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"4f93c440d73bab46f7b2d884af2b2c2e1f5a4c06","externalIds":{"DBLP":"journals/tcsv/ZhaoWYLG23","DOI":"10.1109/TCSVT.2022.3202531","CorpusId":251940498},"title":"Gait-Assisted Video Person Retrieval"},{"paperId":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","externalIds":{"DBLP":"conf/iccv/WuGWLGSHSQS23","ArXiv":"2212.11565","DOI":"10.1109/ICCV51070.2023.00701","CorpusId":254974187},"title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"},{"paperId":"c1ddae8106deaeb15120a6a0f04111a63f4e2a7c","externalIds":{"ArXiv":"2211.11743","DBLP":"journals/corr/abs-2211-11743","DOI":"10.48550/arXiv.2211.11743","CorpusId":253734983},"title":"SinFusion: Training Diffusion Models on a Single Image or Video"},{"paperId":"94b690162ead76af6a487d6e10998ea585c035d1","externalIds":{"DBLP":"journals/corr/abs-2211-11018","ArXiv":"2211.11018","DOI":"10.48550/arXiv.2211.11018","CorpusId":253735209},"title":"MagicVideo: Efficient Video Generation With Latent Diffusion Models"},{"paperId":"0b8d03a6d201b8995a90558710e074a2b8e76f7c","externalIds":{"ArXiv":"2211.06885","DBLP":"conf/cvpr/LiuPZPLSA23","DOI":"10.1109/CVPR52729.2023.01007","CorpusId":253510477},"title":"SCOTCH and SODA: A Transformer Video Shadow Detection Framework"},{"paperId":"9094fe5b2a5592886c2380156356ccb522fb7f91","externalIds":{"DBLP":"conf/iccv/QiKSG0GJL023","ArXiv":"2211.05776","DOI":"10.1109/ICCV51070.2023.00374","CorpusId":257913050},"title":"High Quality Entity Segmentation"},{"paperId":"67299867b0c0e14bc947fc7f10cbfb6e339118a1","externalIds":{"DBLP":"journals/corr/abs-2210-09782","ArXiv":"2210.09782","DOI":"10.48550/arXiv.2210.09782","CorpusId":252967711},"title":"Decoupling Features in Hierarchical Propagation for Video Object Segmentation"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"1e33716e8820b867d5a8aaebab44c2d3135ea4ac","externalIds":{"DBLP":"conf/iclr/SingerPH00ZHYAG23","ArXiv":"2209.14792","CorpusId":252595919},"title":"Make-A-Video: Text-to-Video Generation without Text-Video Data"},{"paperId":"c89da5aa9697ab9d5366353ec29b3e9c1b610469","externalIds":{"DBLP":"journals/ijcv/DunnhoferFFM23","PubMedCentral":"9816211","ArXiv":"2209.13502","DOI":"10.1007/s11263-022-01694-6","CorpusId":252544980,"PubMed":"36624862"},"title":"Visual Object Tracking in First Person Vision"},{"paperId":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744","externalIds":{"ArXiv":"2208.12242","DBLP":"conf/cvpr/RuizLJPRA23","DOI":"10.1109/CVPR52729.2023.02155","CorpusId":251800180},"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"paperId":"7d2d7c52311968855130c123acf2fbf70f948129","externalIds":{"DBLP":"journals/corr/abs-2207-14012","ArXiv":"2207.14012","DOI":"10.48550/arXiv.2207.14012","CorpusId":251135214},"title":"Video Mask Transfiner for High-Quality Video Instance Segmentation"},{"paperId":"65edfa85e5e665d51540a2c7ae1bcb6381793f68","externalIds":{"DBLP":"conf/eccv/WuLJBYB22","ArXiv":"2207.10661","DOI":"10.48550/arXiv.2207.10661","CorpusId":250918749},"title":"In Defense of Online Models for Video Instance Segmentation"},{"paperId":"01724c36660359545e1368fc80c99f4bde44a190","externalIds":{"DBLP":"conf/eccv/ChengS22","ArXiv":"2207.07115","DOI":"10.48550/arXiv.2207.07115","CorpusId":250526250},"title":"XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model"},{"paperId":"46e2af2ce21fb2c3385ade876cf931a3727b5da8","externalIds":{"ArXiv":"2207.05042","DBLP":"journals/corr/abs-2207-05042","DOI":"10.48550/arXiv.2207.05042","CorpusId":250426446},"title":"Audio-Visual Segmentation"},{"paperId":"c170ab309fabcdb0d5258f509ae6d0f12f8d3482","externalIds":{"ArXiv":"2206.15255","DBLP":"conf/miccai/WangLFD22","DOI":"10.48550/arXiv.2206.15255","CorpusId":250144276},"title":"Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery"},{"paperId":"53c20ea6fcab5537f84ac54f700286a409893cb2","externalIds":{"DBLP":"conf/cvpr/MiaoWWLZWY22","DOI":"10.1109/CVPR52688.2022.02036","CorpusId":247936219},"title":"Large-scale Video Panoptic Segmentation in the Wild: A Benchmark"},{"paperId":"66ee488cf3dad5bb83804124367460edddd3c271","externalIds":{"DBLP":"conf/ijcai/LongCHY22","ArXiv":"2204.07356","DOI":"10.48550/arXiv.2204.07356","CorpusId":248218612},"title":"Vision-and-Language Pretrained Models: A Survey"},{"paperId":"fcf3918398e3023419679eb9933946ae69fb3035","externalIds":{"ArXiv":"2204.04656","DBLP":"journals/corr/abs-2204-04656","DOI":"10.1109/CVPR52688.2022.01828","CorpusId":248085089},"title":"Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation"},{"paperId":"32db2d409384575aeae453acc45220b51fe96301","externalIds":{"DBLP":"journals/corr/abs-2203-10541","ArXiv":"2203.10541","DOI":"10.1109/CVPR52688.2022.00869","CorpusId":247594984},"title":"Unsupervised Domain Adaptation for Nighttime Aerial Tracking"},{"paperId":"58a3fedc03ab9f5908c077c115eff4c8d2d87660","externalIds":{"DBLP":"conf/eccv/ZhangWL22","ArXiv":"2202.07925","DOI":"10.1007/978-3-031-19772-7_29","CorpusId":246867281},"title":"ActionFormer: Localizing Moments of Actions with Transformers"},{"paperId":"cc9826c222ac1e81b4b374dd9e0df130f298b1e8","externalIds":{"ArXiv":"2201.03546","DBLP":"journals/corr/abs-2201-03546","CorpusId":245836975},"title":"Language-driven Semantic Segmentation"},{"paperId":"f7410f535bda5b5a9888512fb954193245c1d0b2","externalIds":{"DBLP":"journals/corr/abs-2201-01266","ArXiv":"2201.01266","DOI":"10.1007/978-3-031-08999-2_22","CorpusId":245668780},"title":"Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images"},{"paperId":"2397eb2febb96e5a620147eff64e6875c4b63959","externalIds":{"DBLP":"conf/cvpr/WuJSYL22","ArXiv":"2201.00487","DOI":"10.1109/CVPR52688.2022.00492","CorpusId":245650885},"title":"Language as Queries for Referring Video Object Segmentation"},{"paperId":"a6c29950e1f164a6fcd222e0938d465e2f67c2ba","externalIds":{"DBLP":"conf/aaai/XuWLL22","ArXiv":"2112.02853","DOI":"10.1609/aaai.v36i3.20200","CorpusId":244909160},"title":"Reliable Propagation-Correction Modulation for Video Object Segmentation"},{"paperId":"97bee918b08c244eb2e54d41e8ea6da00a3e5dbf","externalIds":{"DBLP":"conf/eccv/WuLJYFJD22","ArXiv":"2111.12417","DOI":"10.1007/978-3-031-19787-1_41","CorpusId":244527261},"title":"NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","externalIds":{"DBLP":"conf/cvpr/HeCXLDG22","ArXiv":"2111.06377","DOI":"10.1109/CVPR52688.2022.01553","CorpusId":243985980},"title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"407b494da190391ba46153318df68cc3384aab1e","externalIds":{"ArXiv":"2110.11661","CorpusId":240419592},"title":"UVO Challenge on Video-based Open-World Segmentation 2021: 1st Place Solution"},{"paperId":"7806ad7885d732040cb1fbf23857bba5b6779edd","externalIds":{"DBLP":"journals/tog/KastenOWD21","ArXiv":"2109.11418","DOI":"10.1145/3478513.3480546","CorpusId":237605410},"title":"Layered neural atlases for consistent video editing"},{"paperId":"9b1546bc2ecbc2a699d83f113c549806212a7398","externalIds":{"ArXiv":"2109.08591","DBLP":"conf/eccv/HaimFGSBDI22","DOI":"10.1007/978-3-031-19790-1_30","CorpusId":237563160},"title":"Diverse Generation from a Single Video Made Possible"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"513a2fec28bcd619994aa848d2786c4d70cf1442","externalIds":{"DBLP":"journals/tifs/KongCLWRK22","ArXiv":"2107.05821","DOI":"10.1109/TIFS.2022.3169921","CorpusId":247997002},"title":"Detect and Locate: Exposing Face Manipulation by Semantic- and Noise-Level Telltales"},{"paperId":"f3536a13c4bc97de0f703afbafb7318dce56e946","externalIds":{"DBLP":"journals/corr/abs-2106-05210","ArXiv":"2106.05210","CorpusId":235376958},"title":"Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation"},{"paperId":"cf1f0d76d9d4065bdf3ce6000e5ae0ecbedfe8e8","externalIds":{"MAG":"3165405144","DBLP":"journals/corr/abs-2105-14678","ArXiv":"2105.14678","DOI":"10.1109/TCSVT.2021.3083257","CorpusId":235254327},"title":"Image-to-Video Generation via 3D Facial Dynamics"},{"paperId":"156ddb6feea4339d4258049aa840efbbd92ad705","externalIds":{"DBLP":"journals/corr/abs-2105-06993","ArXiv":"2105.06993","DOI":"10.1109/CVPR46437.2021.00448","CorpusId":234680114},"title":"Omnimatte: Associating Objects and Their Effects in Video"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"57076b4306819203521d14dfad08693c2f5452f7","externalIds":{"DBLP":"journals/corr/abs-2104-04691","ArXiv":"2104.04691","DOI":"10.1109/ICCV48922.2021.01060","CorpusId":233209798},"title":"Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation"},{"paperId":"ea28802c3d531ea1d585de232375267384319633","externalIds":{"ArXiv":"2103.06533","DBLP":"conf/cvpr/ChenW0SFL021","MAG":"3135333065","DOI":"10.1109/CVPR46437.2021.00274","CorpusId":232185332},"title":"Triple-cooperative Video Shadow Detection"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"bfb1961040e04f0afe7a06a14f73dda28ba75f71","externalIds":{"ArXiv":"2102.01894","DBLP":"journals/corr/abs-2102-01894","DOI":"10.1109/ICCV48922.2021.01327","CorpusId":231786590},"title":"Relaxed Transformer Decoders for Direct Action Proposal Generation"},{"paperId":"a4e45054b6680d8f8584e389180e0c402a679843","externalIds":{"ArXiv":"2101.08540","DBLP":"journals/corr/abs-2101-08540","CorpusId":231662416},"title":"Activity Graph Transformer for Temporal Action Localization"},{"paperId":"7223fb56eeece5806f8d25718bbc78386e17f19f","externalIds":{"DBLP":"conf/wacv/AroraL21","DOI":"10.1109/WACV48630.2021.00135","CorpusId":230095451},"title":"SinGAN-GIF: Learning a Generative Video Model from a Single GIF"},{"paperId":"084576d2b607dfe3796b2fd5db2de9eb541e9aa9","externalIds":{"DBLP":"conf/cvpr/ChanWYDL21","MAG":"3107955907","ArXiv":"2012.02181","DOI":"10.1109/CVPR46437.2021.00491","CorpusId":227254634},"title":"BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"1a3284e3c7bc58a5f453e6573d9107bfb3686b9e","externalIds":{"MAG":"3101642700","DBLP":"conf/nips/GurBW20","ArXiv":"2006.12226","CorpusId":219966229},"title":"Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single Sample"},{"paperId":"7f327e86fdd1e5aafe362ad8053bf94c56cab245","externalIds":{"MAG":"3035244041","DBLP":"conf/cvpr/LiuLZYDX20","DOI":"10.1109/cvpr42600.2020.00816","CorpusId":211544924},"title":"ARShadowGAN: Shadow Generative Adversarial Network for Augmented Reality in Single Light Scenes"},{"paperId":"d47a682723f710395454687319bb55635e653105","externalIds":{"DBLP":"journals/corr/abs-2005-14050","MAG":"3032388710","ArXiv":"2005.14050","ACL":"2020.acl-main.485","DOI":"10.18653/v1/2020.acl-main.485","CorpusId":218971825},"title":"Language (Technology) is Power: A Critical Survey of “Bias” in NLP"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"c69220ee364b93a0f574df2ee678ae7fd1621e70","externalIds":{"ArXiv":"2004.14489","MAG":"3048537404","DBLP":"journals/corr/abs-2004-14489","DOI":"10.1145/3386569.3392453","CorpusId":216868355},"title":"Interactive video stylization using few-shot patch-based training"},{"paperId":"adacccd99a42c3145ec6392a1a6b08878376e38b","externalIds":{"MAG":"3015067232","DBLP":"journals/corr/abs-2004-00305","ArXiv":"2004.00305","DOI":"10.1109/cvpr42600.2020.00633","CorpusId":214743010},"title":"High-Performance Long-Term Tracking With Meta-Updater"},{"paperId":"bd6d0224e94a19caa1669f2bd7b97709090b01bf","externalIds":{"DBLP":"journals/corr/abs-2001-11190","MAG":"3003209274","ArXiv":"2001.11190","CorpusId":210965971},"title":"2018 Robotic Scene Segmentation Challenge"},{"paperId":"e7d5ecbc2397f9b29dfaea921fb6d493f4c3703a","externalIds":{"ArXiv":"1908.08918","DBLP":"journals/corr/abs-1908-08918","MAG":"3088230178","DOI":"10.1109/TRO.2020.3020739","CorpusId":201645354},"title":"DefSLAM: Tracking and Mapping of Deforming Scenes From Monocular Sequences"},{"paperId":"b810da03b420d3105b35e166e99f24d9379d8aa6","externalIds":{"MAG":"2956900569","DBLP":"journals/tog/JamriskaSTLFLSS19","DOI":"10.1145/3306346.3323006","CorpusId":196834800},"title":"Stylizing video by example"},{"paperId":"82b46dd7c67b17dfc34b10b4e7db6f8391cd4886","externalIds":{"MAG":"2915232829","DBLP":"journals/tmi/LeclercSPOCEEBJ19","ArXiv":"1908.06948","DOI":"10.1109/TMI.2019.2900516","CorpusId":73510235,"PubMed":"30802851"},"title":"Deep Learning for Segmentation Using an Open Large-Scale Dataset in 2D Echocardiography"},{"paperId":"e705aeaa9440744c8341b14c7404f6a91e694571","externalIds":{"ArXiv":"1902.06426","DBLP":"journals/corr/abs-1902-06426","MAG":"2913444875","CorpusId":67787647},"title":"2017 Robotic Instrument Segmentation Challenge"},{"paperId":"8b47b9c3c35b2b2a78bff7822605b3040f87d699","externalIds":{"MAG":"2990503944","DBLP":"journals/corr/abs-1812-03982","ArXiv":"1812.03982","DOI":"10.1109/ICCV.2019.00630","CorpusId":54463801},"title":"SlowFast Networks for Video Recognition"},{"paperId":"f054fdd7a36b569eae7627cf12a4d81322dea022","externalIds":{"DBLP":"conf/eccv/XuYFYYLPCH18","MAG":"2889986507","ArXiv":"1809.00461","DOI":"10.1007/978-3-030-01228-1_36","CorpusId":52154988},"title":"YouTube-VOS: Sequence-to-Sequence Video Object Segmentation"},{"paperId":"8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b","externalIds":{"DBLP":"conf/eccv/MullerBGAG18","ArXiv":"1803.10794","MAG":"2950006892","DOI":"10.1007/978-3-030-01246-5_19","CorpusId":4455970},"title":"TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild"},{"paperId":"b82058b4bf630d33e129ab097b8cacf6cc3d4556","externalIds":{"MAG":"2794857359","DBLP":"journals/corr/abs-1803-09179","ArXiv":"1803.09179","CorpusId":4395181},"title":"FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces"},{"paperId":"da30d00c9490768e7726725482e3ecbd102f18cd","externalIds":{"MAG":"2951088888","DBLP":"conf/accv/KhorevaRS18","ArXiv":"1803.08006","DOI":"10.1007/978-3-030-20870-7_8","CorpusId":3992628},"title":"Video Object Segmentation with Language Referring Expressions"},{"paperId":"1d6a5d0299ed8458191e4e0407d4d513e6a7dd7e","externalIds":{"MAG":"2769297824","DBLP":"journals/corr/abs-1711-10275","ArXiv":"1711.10275","DOI":"10.1109/CVPR.2018.00961","CorpusId":10154243},"title":"3D Semantic Segmentation with Submanifold Sparse Convolutional Networks"},{"paperId":"c1045435c208a20f65b79baaa2d79783c2409c09","externalIds":{"MAG":"2962789486","DBLP":"journals/corr/abs-1711-09078","ArXiv":"1711.09078","DOI":"10.1007/s11263-018-01144-2","CorpusId":40412298},"title":"Video Enhancement with Task-Oriented Flow"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"79cfb51a51fc093f66aac8e858afe2e14d4a1f20","externalIds":{"MAG":"2950100464","DBLP":"journals/corr/abs-1708-02002","DOI":"10.1109/ICCV.2017.324","CorpusId":47252984},"title":"Focal Loss for Dense Object Detection"},{"paperId":"b61a3f8b80bbd44f24544dc915f52fd30bbdf485","externalIds":{"ArXiv":"1705.07750","MAG":"2619082050","DBLP":"conf/cvpr/CarreiraZ17","DOI":"10.1109/CVPR.2017.502","CorpusId":206596127},"title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"},{"paperId":"49e8fec24cce8b73706bc5fcd2c3f681addb9982","externalIds":{"ArXiv":"1704.00675","MAG":"2916743882","DBLP":"journals/corr/Pont-TusetPCASG17","CorpusId":3619941},"title":"The 2017 DAVIS Challenge on Video Object Segmentation"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"f49d7207b9981a79c0e262bf6a521dc2ca618b7c","externalIds":{"DBLP":"journals/tcsv/StoianFBC16","MAG":"2523803202","DOI":"10.1109/TCSVT.2015.2475835","CorpusId":31537462},"title":"Fast Action Localization in Large-Scale Video Archives"},{"paperId":"50004c086ffd6a201a4b782281aaa930fbfe6ecf","externalIds":{"MAG":"2432481613","DBLP":"journals/corr/MilletariNA16","ArXiv":"1606.04797","DOI":"10.1109/3DV.2016.79","CorpusId":206429151},"title":"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"},{"paperId":"05e9e85b5137016c93d042170e82f77bb551a108","externalIds":{"DBLP":"conf/cvpr/PerazziPMGGS16","MAG":"2470139095","DOI":"10.1109/CVPR.2016.85","CorpusId":1949934},"title":"A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation"},{"paperId":"67dccc9a856b60bdc4d058d83657a089b8ad4486","externalIds":{"MAG":"2952186347","DBLP":"conf/nips/SimonyanZ14","ArXiv":"1406.2199","CorpusId":11797475},"title":"Two-Stream Convolutional Networks for Action Recognition in Videos"},{"paperId":"1c67510d42446045ac4584b637693d68dce1b3f4","externalIds":{"DBLP":"conf/iccp/EcinsFA14","MAG":"2085910047","DOI":"10.1109/ICCPHOT.2014.6831803","CorpusId":11643136},"title":"Shadow free segmentation in still images using local density measure"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"930157a6eccebf84589de752963ae8682f3182f0","externalIds":{"MAG":"2949979297","DBLP":"journals/csur/ZhouYZY14","ArXiv":"1401.3409","DOI":"10.1145/2674559","CorpusId":6620224},"title":"Low-Rank Modeling and Its Applications in Image Analysis"},{"paperId":"79b949d9b35c3f51dd20fb5c746cc81fc87147eb","externalIds":{"MAG":"2115579991","DBLP":"journals/ijrr/GeigerLSU13","DOI":"10.1177/0278364913491297","CorpusId":9455111},"title":"Vision meets robotics: The KITTI dataset"},{"paperId":"7d53f0c87c8ab0de6f3e74515e3ffaf3fab40c62","externalIds":{"MAG":"1513100184","DBLP":"conf/eccv/ButlerWSB12","DOI":"10.1007/978-3-642-33783-3_44","CorpusId":4637111},"title":"A Naturalistic Open Source Movie for Optical Flow Evaluation"},{"paperId":"72729882f8fa3d9084eaece513f6bf9630be5901","externalIds":{"DBLP":"conf/acl/ChenD11","MAG":"2164290393","ACL":"P11-1020","CorpusId":215717103},"title":"Collecting Highly Parallel Data for Paraphrase Evaluation"},{"paperId":"d31e29891afbbe185cf6333c944f587112d5494d","externalIds":{"MAG":"2143373691","DBLP":"journals/pami/CucchiaraGPP03","DOI":"10.1109/TPAMI.2003.1233909","CorpusId":2259236},"title":"Detecting Moving Objects, Ghosts, and Shadows in Video Streams"},{"paperId":"bb4d57b6e1350978178470c4f8777504285b05d1","externalIds":{"MAG":"2106093534","DBLP":"journals/tcsv/SiuCS01","DOI":"10.1109/76.964787","CorpusId":14754173},"title":"A robust model generation technique for model-based video coding"},{"paperId":"85d254414570340d49d9541056e8833b1822f7a0","externalIds":{"DBLP":"journals/corr/abs-2304-09148","DOI":"10.48550/arXiv.2304.09148","CorpusId":258187610},"title":"SAM Fails to Segment Anything? - SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, and More"},{"paperId":"e6b79c12032884be401da08177a7c33ca02a6985","externalIds":{"DBLP":"journals/corr/abs-2306-05716","DOI":"10.48550/arXiv.2306.05716","CorpusId":268890276},"title":"Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots"},{"paperId":"fa5a983e334a92a6f4499daa941fd1e28d85d035","externalIds":{"DBLP":"journals/corr/abs-2309-15562","DOI":"10.48550/arXiv.2309.15562","CorpusId":262942630},"title":"Learning from SAM: Harnessing a Segmentation Foundation Model for Sim2Real Domain Adaptation through Regularization"},{"paperId":"79834a48204247f4116538d32a768404eb9a7850","externalIds":{"DBLP":"journals/corr/abs-2305-07223","DOI":"10.48550/arXiv.2305.07223","CorpusId":281333417},"title":"Hear to Segment: Unmixing the Audio to Guide the Semantic Segmentation"},{"paperId":"cd0255491af35c5bc5f7cfa473718c19f67fcac3","externalIds":{"DBLP":"conf/eccv/SeoLH20","MAG":"3104844437","DOI":"10.1007/978-3-030-58555-6_13","CorpusId":226220136},"title":"URVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark"},{"paperId":"16942e3ed6ca4dda763618f3d0e62027926d8085","externalIds":{"CorpusId":18316484},"title":"contributed equally to this work."},{"paperId":"3a9b175324ba11bc0e16c0633912d897b2fac4e2","externalIds":{"CorpusId":4246903},"title":"International Journal of Computer Vision manuscript No. (will be inserted by the editor) The PASCAL Visual Object Classes (VOC) Challenge"}]}