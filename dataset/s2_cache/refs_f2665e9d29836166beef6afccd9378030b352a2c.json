{"references":[{"paperId":"4956e8f57227ec8f642dc54e2c0c3742ebb388e7","externalIds":{"ArXiv":"2403.15378","DBLP":"conf/eccv/ZhangZDZW24","DOI":"10.48550/arXiv.2403.15378","CorpusId":268667201},"title":"Long-CLIP: Unlocking the Long-Text Capability of CLIP"},{"paperId":"78b45ddc4287a5cdc5fc50e68c1f5d405fea21be","externalIds":{"DBLP":"conf/cvpr/ZimmerWSZ0K24","ArXiv":"2403.01316","DOI":"10.1109/CVPR52733.2024.02139","CorpusId":268230584},"title":"TUMTraf V2X Cooperative Perception Dataset"},{"paperId":"dcaae45fc6c923615f9e91f93ac7e3b4acc8819d","externalIds":{"DBLP":"journals/inffus/JuZQYYXLYZ24","ArXiv":"2403.01091","DOI":"10.48550/arXiv.2403.01091","CorpusId":268230422},"title":"COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting"},{"paperId":"758c2dc290c037a6f211ec503beee70abe2d1197","externalIds":{"DBLP":"conf/corl/TianGLLWZZJLZ24","ArXiv":"2402.12289","DOI":"10.48550/arXiv.2402.12289","CorpusId":267750682},"title":"DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models"},{"paperId":"4a88eb58cb3d1f72da7ac501a28f2e9d831336c4","externalIds":{"ArXiv":"2401.05577","DBLP":"conf/cvpr/PanYNMAVR24","DOI":"10.1109/CVPR52733.2024.01398","CorpusId":266933301},"title":"VLP: Vision Language Planning for Autonomous Driving"},{"paperId":"9743f44ed6ca96aeb89358bcbb99aebe65113262","externalIds":{"ArXiv":"2401.00713","CorpusId":266693787},"title":"Graph Neural Networks in Intelligent Transportation Systems: Advances, Applications and Trends"},{"paperId":"5edf706467dc76cd09319592d18db0ad4e1fb64d","externalIds":{"DBLP":"journals/corr/abs-2312-14074","ArXiv":"2312.14074","DOI":"10.48550/arXiv.2312.14074","CorpusId":266435483},"title":"LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","externalIds":{"ArXiv":"2312.09245","DBLP":"journals/corr/abs-2312-09245","DOI":"10.1007/s44267-025-00095-w","CorpusId":266210476},"title":"DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"},{"paperId":"257eaf21e6e6926e9693e04e946ec2199b3afee1","externalIds":{"DBLP":"journals/corr/abs-2312-06352","ArXiv":"2312.06352","DOI":"10.1109/WACVW60836.2024.00104","CorpusId":266162771},"title":"NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations"},{"paperId":"98de0a73fc32e04b58d76579aef964cf686b25da","externalIds":{"ArXiv":"2312.03661","DBLP":"conf/eccv/NiePWCHXZ24","DOI":"10.48550/arXiv.2312.03661","CorpusId":265688025},"title":"Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving"},{"paperId":"3039e5c8bd6147b6ee08f0f50d52047cc3be2372","externalIds":{"ArXiv":"2311.13549","DBLP":"journals/corr/abs-2311-13549","DOI":"10.48550/arXiv.2311.13549","CorpusId":265351980},"title":"ADriver-I: A General World Model for Autonomous Driving"},{"paperId":"1e909e2a8cdacdcdff125ebcc566f37cb869a1c8","externalIds":{"ArXiv":"2311.05232","DBLP":"journals/tois/HuangYMZFWCPFQL25","DOI":"10.1145/3703155","CorpusId":265067168},"title":"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"},{"paperId":"c96e5681566e34dfbd2a5e379f075d5d3a631230","externalIds":{"ArXiv":"2310.10357","DBLP":"journals/corr/abs-2310-10357","DOI":"10.48550/arXiv.2310.10357","CorpusId":264146375},"title":"BEVGPT: Generative Pre-trained Large Model for Autonomous Driving Prediction, Decision-Making, and Planning"},{"paperId":"c465e2c612c9bb15f3efcb554e35052b6374d012","externalIds":{"DBLP":"journals/corr/abs-2310-07771","ArXiv":"2310.07771","DOI":"10.48550/arXiv.2310.07771","CorpusId":263909293},"title":"DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model"},{"paperId":"408772a7e41d7ac4cd60d861bcf351c328794250","externalIds":{"DBLP":"journals/cviu/ZanellaLMPWR24","ArXiv":"2310.02835","DOI":"10.48550/arXiv.2310.02835","CorpusId":263611937},"title":"Delving into CLIP latent space for Video Anomaly Recognition"},{"paperId":"19933dd9e03058e686ef412262eef7696cce3e8f","externalIds":{"ArXiv":"2310.03026","DBLP":"journals/corr/abs-2310-03026","DOI":"10.48550/arXiv.2310.03026","CorpusId":263620279},"title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving"},{"paperId":"77693ca00a8ef775af96b5c450aa0afdb0e10a51","externalIds":{"DBLP":"journals/corr/abs-2310-02251","ArXiv":"2310.02251","DOI":"10.1109/ICRA57147.2024.10611485","CorpusId":263608641},"title":"Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving"},{"paperId":"f01ff5acf9e086030c01beda6f433f99013ebbd4","externalIds":{"DBLP":"conf/icra/ChenSHKWBMS24","ArXiv":"2310.01957","DOI":"10.1109/ICRA57147.2024.10611018","CorpusId":263608168},"title":"Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"},{"paperId":"555dd10c4dc8570d4069389a4da785a8ff73063d","externalIds":{"ArXiv":"2310.02324","DBLP":"journals/corr/abs-2310-02324","DOI":"10.48550/arXiv.2310.02324","CorpusId":263620315},"title":"ALT-Pilot: Autonomous navigation with Language augmented Topometric maps"},{"paperId":"ccd6f8b6544f112de632e49bfbe592a0a654537d","externalIds":{"ArXiv":"2310.01412","DBLP":"journals/ral/XuZXZGWLZ24","DOI":"10.1109/LRA.2024.3440097","CorpusId":263605524},"title":"DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model"},{"paperId":"958ed4830ae80a189ecb9b93ab75a6ce2e3926fc","externalIds":{"DBLP":"journals/corr/abs-2310-01415","ArXiv":"2310.01415","DOI":"10.48550/arXiv.2310.01415","CorpusId":263605637},"title":"GPT-Driver: Learning to Drive with GPT"},{"paperId":"98478ac589e5b40a20630ff54bb4eec4ab4c5f6b","externalIds":{"ArXiv":"2309.17080","DBLP":"journals/corr/abs-2309-17080","DOI":"10.48550/arXiv.2309.17080","CorpusId":263310665},"title":"GAIA-1: A Generative World Model for Autonomous Driving"},{"paperId":"3cbfe152220de84ecf8059fa50c47587a3134c86","externalIds":{"DBLP":"conf/iclr/WenF0C0CDS0024","ArXiv":"2309.16292","DOI":"10.48550/arXiv.2309.16292","CorpusId":263136146},"title":"DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models"},{"paperId":"afdf04289e384e9750cc9a3dc901c5606738804f","externalIds":{"DBLP":"journals/corr/abs-2309-14491","ArXiv":"2309.14491","DOI":"10.1109/ICCV51070.2023.00790","CorpusId":262824886},"title":"Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving"},{"paperId":"482665786ce1956fb9ea4b694d2d8e8cf92276fa","externalIds":{"ArXiv":"2309.10228","DBLP":"conf/wacv/CuiMCYW24","DOI":"10.1109/WACVW60836.2024.00101","CorpusId":262054629},"title":"Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles"},{"paperId":"0ef8bbccbfab7e6d9c8bd09bcadfe9c5bbbff512","externalIds":{"ArXiv":"2309.09777","DBLP":"journals/corr/abs-2309-09777","DOI":"10.48550/arXiv.2309.09777","CorpusId":262044078},"title":"DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving"},{"paperId":"e14b245c2ed6d46bf2f8661fcd61b74f34d60ad7","externalIds":{"DBLP":"conf/wacv/SachdevaACR0KCD24","ArXiv":"2309.06597","DOI":"10.1109/WACV57701.2024.00734","CorpusId":261706176},"title":"Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning"},{"paperId":"e26888285436bc7998e5c95102a9beb60144be5e","externalIds":{"DBLP":"journals/corr/abs-2309-05463","ArXiv":"2309.05463","DOI":"10.48550/arXiv.2309.05463","CorpusId":261696657},"title":"Textbooks Are All You Need II: phi-1.5 technical report"},{"paperId":"7679dc8534cb1dd65c63c50b38f56386228d32d1","externalIds":{"ArXiv":"2309.05282","DBLP":"journals/corr/abs-2309-05282","DOI":"10.48550/arXiv.2309.05282","CorpusId":261681760},"title":"Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving"},{"paperId":"29d262a66c57a9f819fe0325f184b91d48a4c2a4","externalIds":{"ArXiv":"2309.04379","DBLP":"journals/corr/abs-2309-04379","DOI":"10.48550/arXiv.2309.04379","CorpusId":261660217},"title":"Language Prompt for Autonomous Driving"},{"paperId":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","externalIds":{"ArXiv":"2308.12966","CorpusId":261101015},"title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"},{"paperId":"9f0630ff9d256ab89248f87cf2bdb7cee5740d4c","externalIds":{"ArXiv":"2308.08414","DBLP":"journals/corr/abs-2308-08414","DOI":"10.1109/ICCV51070.2023.01282","CorpusId":260925582},"title":"Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer"},{"paperId":"bd8a7cc93e1515b2b644edc8f17baac7dd4df34f","externalIds":{"DBLP":"journals/corr/abs-2308-01661","ArXiv":"2308.01661","DOI":"10.48550/arXiv.2308.01661","CorpusId":260438574},"title":"BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"96e1d11e2fb6e6cf26769d7f194a695a271cde3a","externalIds":{"DBLP":"journals/corr/abs-2307-09636","ArXiv":"2307.09636","DOI":"10.48550/arXiv.2307.09636","CorpusId":259983003},"title":"Traffic-Domain Video Question Answering with Automatic Captioning"},{"paperId":"37d4f61d5e775bfec9aba436eb5b379d4c0ed5bc","externalIds":{"DOI":"10.1109/JSEN.2023.3260193","CorpusId":257788057},"title":"Configuration and Design Schemes of Environmental Sensing and Vehicle Computing Systems for Automated Driving: A Review"},{"paperId":"11bca2cafe89e14dc733504f97e2489de697ceab","externalIds":{"ArXiv":"2307.07162","DBLP":"conf/wacv/FuLWDCSQ24","DOI":"10.1109/WACVW60836.2024.00102","CorpusId":259924488},"title":"Drive Like a Human: Rethinking Autonomous Driving with Large Language Models"},{"paperId":"318128fa82a15888a5db28341c5c23d1147271f3","externalIds":{"DBLP":"journals/pami/ChenWCJGL24","ArXiv":"2306.16927","DOI":"10.1109/TPAMI.2024.3435937","CorpusId":259287283,"PubMed":"39078757"},"title":"End-to-End Autonomous Driving: Challenges and Frontiers"},{"paperId":"3b6179c293df29e31d31cea46476f104ab6950f2","externalIds":{"DBLP":"conf/iclr/Peng00HHMYW24","ArXiv":"2306.14824","DOI":"10.48550/arXiv.2306.14824","CorpusId":259262263},"title":"Kosmos-2: Grounding Multimodal Large Language Models to the World"},{"paperId":"f15534b6f95a4def989b9abbae7715ba1f28574a","externalIds":{"DBLP":"conf/ijcnn/WantiezQMS23","DOI":"10.1109/IJCNN54540.2023.10191714","CorpusId":260386412},"title":"Scene Understanding for Autonomous Driving Using Visual Question Answering"},{"paperId":"275d8bfe7d9c671d1cb4f525434d10a7dbd8778a","externalIds":{"ArXiv":"2306.08543","CorpusId":259164722},"title":"MiniLLM: On-Policy Distillation of Large Language Models"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","externalIds":{"ArXiv":"2306.05424","DBLP":"journals/corr/abs-2306-05424","DOI":"10.48550/arXiv.2306.05424","CorpusId":259108333},"title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"f02ea7a18f00859d9ea1b321e3385ae7d0170639","externalIds":{"DBLP":"journals/corr/abs-2306-02018","ArXiv":"2306.02018","DOI":"10.48550/arXiv.2306.02018","CorpusId":259075720},"title":"VideoComposer: Compositional Video Synthesis with Motion Controllability"},{"paperId":"a2f6a812924a1112c434510975eddaeb2b22523a","externalIds":{"DBLP":"conf/cvpr/LeNLCCH23","DOI":"10.1109/CVPRW59228.2023.00583","CorpusId":260917802},"title":"Tracked-Vehicle Retrieval by Natural Language Descriptions with Multi-Contextual Adaptive Knowledge"},{"paperId":"8b304a31de4b511b888f9a70f143d546abc63091","externalIds":{"DBLP":"conf/cvpr/XieLZT23","DOI":"10.1109/CVPRW59228.2023.00572","CorpusId":260919195},"title":"A Unified Multi-modal Structure for Retrieving Tracked Vehicles through Natural Language Descriptions"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"2a9f1e923ccd7aa8f9411471275bef251a1466dd","externalIds":{"ArXiv":"2305.15765","DBLP":"journals/corr/abs-2305-15765","DOI":"10.48550/arXiv.2305.15765","CorpusId":258888024},"title":"Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving"},{"paperId":"ee156428803c5bd6e7372f6b27d74bcf88390db3","externalIds":{"ArXiv":"2305.14836","DBLP":"conf/aaai/QianCZJJ24","DOI":"10.48550/arXiv.2305.14836","CorpusId":258866014},"title":"NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario"},{"paperId":"d4903c821716d57245216fa397556f58fb0d59b3","externalIds":{"DBLP":"journals/corr/abs-2305-12457","ArXiv":"2305.12457","DOI":"10.1145/3664647.3681560","CorpusId":258833171},"title":"Unsupervised Multi-view Pedestrian Detection"},{"paperId":"735ea38114bd2406a8bbc7f060cba1fc7a254d89","externalIds":{"DBLP":"journals/arobots/ElhafsiSASNP23","ArXiv":"2305.11307","DOI":"10.1007/s10514-023-10132-6","CorpusId":258823112},"title":"Semantic anomaly detection with large language models"},{"paperId":"81e7e82245c2f230eeb8aaaa1a2b2604c143754a","externalIds":{"ArXiv":"2305.04790","DBLP":"journals/corr/abs-2305-04790","DOI":"10.48550/arXiv.2305.04790","CorpusId":258557672},"title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"},{"paperId":"eeab466d688b94133950788eaf8d1bf33bf037cb","externalIds":{"ArXiv":"2305.03785","DBLP":"journals/corr/abs-2305-03785","DOI":"10.48550/arXiv.2305.03785","CorpusId":258557909},"title":"Zelda: Video Analytics using Vision-Language Models"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","externalIds":{"ArXiv":"2304.15010","DBLP":"journals/corr/abs-2304-15010","DOI":"10.48550/arXiv.2304.15010","CorpusId":258418343},"title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"9874d5ebe1c7e3bd4254a57be56c83a07a514c5c","externalIds":{"DBLP":"journals/corr/abs-2304-03135","ArXiv":"2304.03135","DOI":"10.1109/CVPR52729.2023.00644","CorpusId":257985052},"title":"VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision"},{"paperId":"90af7c6cdf4b3359f6d275afb436f54f60082364","externalIds":{"ArXiv":"2304.03284","DBLP":"journals/corr/abs-2304-03284","DOI":"10.48550/arXiv.2304.03284","CorpusId":257985532},"title":"SegGPT: Segmenting Everything In Context"},{"paperId":"690df0820f35a47e1ce44f90e6ddb4132aa09267","externalIds":{"DBLP":"journals/pami/ZhangHJL24","ArXiv":"2304.00685","DOI":"10.1109/TPAMI.2024.3369699","CorpusId":257913547,"PubMed":"38408000"},"title":"Vision-Language Models for Vision Tasks: A Survey"},{"paperId":"77c3bb63f0c6954a344465d6eccf244a96400732","externalIds":{"DBLP":"conf/icde/ZhaoLJC0023","ArXiv":"2309.12028","DOI":"10.1109/ICDE55515.2023.00178","CorpusId":260172139},"title":"Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"923a03032014a12c4e8b26511c0394e1b915fe74","externalIds":{"DBLP":"conf/iccv/KhachatryanMTHW23","ArXiv":"2303.13439","DOI":"10.1109/ICCV51070.2023.01462","CorpusId":257687280},"title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"},{"paperId":"aae2e7454945b904ca1d438d2ae4d4941ba11409","externalIds":{"DBLP":"journals/corr/abs-2303-12077","ArXiv":"2303.12077","DOI":"10.1109/ICCV51070.2023.00766","CorpusId":257636676},"title":"VAD: Vectorized Scene Representation for Efficient Autonomous Driving"},{"paperId":"1c6fbf5c76aee77b539dc3f50991d7ac6c8356e8","externalIds":{"DBLP":"journals/corr/abs-2303-09824","ArXiv":"2303.09824","DOI":"10.1109/TIV.2023.3274536","CorpusId":257622551},"title":"Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"07db8d33845ca3034f623a5bddf4c4816056d3ba","externalIds":{"DBLP":"conf/cvpr/Xu0LLZTMXDSYZM23","ArXiv":"2303.07601","DOI":"10.1109/CVPR52729.2023.01318","CorpusId":257505115},"title":"V2V4Real: A Real-World Large-Scale Dataset for Vehicle-to-Vehicle Cooperative Perception"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","externalIds":{"DBLP":"journals/corr/abs-2303-04671","ArXiv":"2303.04671","DOI":"10.48550/arXiv.2303.04671","CorpusId":257404891},"title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"cbad2da88a77bffaa237496c15c85518575bfb59","externalIds":{"DBLP":"journals/corr/abs-2303-03366","ArXiv":"2303.03366","DOI":"10.1109/CVPR52729.2023.01406","CorpusId":257365320},"title":"Referring Multi-Object Tracking"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"6d7534a41fc933f4f6a99e039f585dc57a370a29","externalIds":{"ArXiv":"2302.00673","DBLP":"conf/icra/JinLZLZZZZL23","DOI":"10.1109/ICRA48891.2023.10160326","CorpusId":256459842},"title":"ADAPT: Action-aware Driving Caption Transformer"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"f0fa292a267dc7e80c7a9d624ef1c8345f59c255","externalIds":{"ArXiv":"2301.07325","DBLP":"journals/corr/abs-2301-07325","DOI":"10.1109/TIV.2023.3244948","CorpusId":255998398},"title":"The OpenCDA Open-Source Ecosystem for Cooperative Driving Automation Research"},{"paperId":"0c17326565266c40a02b230fac3b405a4d3220b9","externalIds":{"DBLP":"journals/corr/abs-2301-04926","ArXiv":"2301.04926","DOI":"10.1109/CVPR52729.2023.00678","CorpusId":255749086},"title":"CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP"},{"paperId":"46f4ed274096a8fa561a768ac9b19f192dc4602a","externalIds":{"DBLP":"journals/corr/abs-2301-04634","ArXiv":"2301.04634","DOI":"10.1109/LRA.2024.3368234","CorpusId":255595883},"title":"Street-View Image Generation From a Bird's-Eye View Layout"},{"paperId":"fdd7d5b0f6b8641c356e170fd264cd11f70ba657","externalIds":{"DBLP":"conf/cvpr/HuYCLSZCDLWLJLD23","ArXiv":"2212.10156","DOI":"10.1109/CVPR52729.2023.01712","CorpusId":257687420},"title":"Planning-oriented Autonomous Driving"},{"paperId":"2d67f217c91d353fd664bd866c5c7ead60520874","externalIds":{"DBLP":"journals/corr/abs-2212-08273","ArXiv":"2212.08273","DOI":"10.1109/TIV.2023.3260040","CorpusId":254823222},"title":"Learning for Vehicle-to-Vehicle Cooperative Perception Under Lossy Communication"},{"paperId":"16de2006e2960ba410772c6b6d460b83c0a5cc4b","externalIds":{"ArXiv":"2212.07143","DBLP":"journals/corr/abs-2212-07143","DOI":"10.1109/CVPR52729.2023.00276","CorpusId":254636568},"title":"Reproducible Scaling Laws for Contrastive Language-Image Learning"},{"paperId":"9ceaeff7117965832f4c05fd6355d021862d0a82","externalIds":{"DBLP":"conf/cvpr/WangWCS023","ArXiv":"2212.02499","DOI":"10.1109/CVPR52729.2023.00660","CorpusId":254246343},"title":"Images Speak in Images: A Generalist Painter for In-Context Visual Learning"},{"paperId":"774408d8848b129d93fb67548ec6571d99b31a2d","externalIds":{"DBLP":"conf/cvpr/PengGJTPF23","ArXiv":"2211.15654","DOI":"10.1109/CVPR52729.2023.00085","CorpusId":254044069},"title":"OpenScene: 3D Scene Understanding with Open Vocabularies"},{"paperId":"2c994fadbb84fb960d8306ee138dbeef41a5b323","externalIds":{"ArXiv":"2211.10438","DBLP":"conf/icml/XiaoLSWDH23","DOI":"10.48550/arXiv.2211.10438","CorpusId":253708271},"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"5b44aca33c27001589b107ac7fa11c0298fefafd","externalIds":{"DBLP":"conf/iros/GaoGQLLZJC22","DOI":"10.1109/IROS47612.2022.9982041","CorpusId":255175795},"title":"Cola-HRL: Continuous-Lattice Hierarchical Reinforcement Learning for Autonomous Driving"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"1e834bcea53759ae9a36422af322b439281ffabe","externalIds":{"DBLP":"conf/icra/JainCTKG23","ArXiv":"2209.11972","DOI":"10.1109/ICRA48891.2023.10160614","CorpusId":252531195},"title":"Ground then Navigate: Language-guided Navigation in Dynamic Scenes"},{"paperId":"eb85c73e8bcfb66bcd5dde49e43c179e2c561ffe","externalIds":{"ArXiv":"2209.10767","DBLP":"journals/corr/abs-2209-10767","DOI":"10.1109/WACV56688.2023.00110","CorpusId":252439240},"title":"DRAMA: Joint Risk Localization and Captioning in Driving"},{"paperId":"eda317e8cd18904f4b140710351359f4d35b29db","externalIds":{"DBLP":"journals/corr/abs-2207-05200","ArXiv":"2207.05200","DOI":"10.48550/arXiv.2207.05200","CorpusId":250451187},"title":"Real-Time And Robust 3D Object Detection with Roadside LiDARs"},{"paperId":"d1c2302cebc720936c81ebf31124879ddf9f3eb4","externalIds":{"DBLP":"conf/cvpr/ZhangLJYGZ0LDL22","DOI":"10.1109/CVPRW56347.2022.00363","CorpusId":251019735},"title":"A Multi-granularity Retrieval System for Natural Language-based Vehicle Retrieval"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"d8463eb424fdeb698aeb8e414e0fc06ad6fcd96b","externalIds":{"DBLP":"journals/corr/abs-2205-00705","ArXiv":"2205.00705","DOI":"10.48550/arXiv.2205.00705","CorpusId":248495889},"title":"3D Object Detection with a Self-supervised Lidar Scene Flow Backbone"},{"paperId":"0dcd5e0c87835ad05d241935ff5a5c05fcbc5dc2","externalIds":{"DBLP":"conf/ivs/CressZSFDLK22","ArXiv":"2204.06527","DOI":"10.48550/arXiv.2204.06527","CorpusId":248157466},"title":"A9-Dataset: Multi-Sensor Infrastructure-Based Dataset for Mobility Research"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"a824c6e214dd0118f70af8bb05d67d94a858d076","externalIds":{"DBLP":"conf/eccv/LiWLXSLQD22","ArXiv":"2203.17270","DOI":"10.48550/arXiv.2203.17270","CorpusId":247839336},"title":"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers"},{"paperId":"a0a9def3e117b8a38f0d5ae49f58f006941355d0","externalIds":{"DBLP":"journals/tist/JiangZG22","DOI":"10.1145/3517820","CorpusId":247599673},"title":"SignDS-FL: Local Differentially Private Federated Learning with Sign-based Dimension Selection"},{"paperId":"7476e1c4fd6fa8e6d23bd7e1b45f9e799d04b284","externalIds":{"ArXiv":"2203.10638","DBLP":"journals/corr/abs-2203-10638","DOI":"10.48550/arXiv.2203.10638","CorpusId":247593762},"title":"V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer"},{"paperId":"7c5078bf51271338fbfc370cb51ae2ef50188ab6","externalIds":{"DBLP":"journals/corr/abs-2202-10124","ArXiv":"2202.10124","CorpusId":247011277},"title":"Multi-Task Conditional Imitation Learning for Autonomous Navigation at Crowded Intersections"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"7002ae048e4b8c9133a55428441e8066070995cb","externalIds":{"ArXiv":"2112.10741","DBLP":"journals/corr/abs-2112-10741","CorpusId":245335086},"title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"},{"paperId":"e9a09f8e474b4c74c700ebbe84d5b0696395a521","externalIds":{"DBLP":"conf/nips/BaiHS0KL22","ArXiv":"2109.15082","CorpusId":238227163},"title":"Towards Efficient Post-training Quantization of Pre-trained Language Models"},{"paperId":"821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0","externalIds":{"ArXiv":"2109.14084","DBLP":"journals/corr/abs-2109-14084","ACL":"2021.emnlp-main.544","DOI":"10.18653/v1/2021.emnlp-main.544","CorpusId":238215257},"title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding"},{"paperId":"96bf7e5f028f46391f8ba5cac34bdadf5b6fd163","externalIds":{"DBLP":"conf/icra/XuXXHLM22","ArXiv":"2109.07644","DOI":"10.1109/icra46639.2022.9812038","CorpusId":237532732},"title":"OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication"},{"paperId":"dbe41b7715078917c2f65dfda316c34d47afe697","externalIds":{"DBLP":"journals/corr/abs-2109-01827","ArXiv":"2109.01827","DOI":"10.1109/icra46639.2022.9812253","CorpusId":237263205},"title":"GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation"},{"paperId":"96ea07447d2f9adefe03852a878517a2a6d45b96","externalIds":{"ArXiv":"2109.01134","DBLP":"journals/ijcv/ZhouYLL22","DOI":"10.1007/s11263-022-01653-1","CorpusId":237386023},"title":"Learning to Prompt for Vision-Language Models"},{"paperId":"ddbd9baf007d86f6174b2d41c36fda1c5ec64b58","externalIds":{"ArXiv":"2106.11037","DBLP":"journals/corr/abs-2106-11037","CorpusId":235490450},"title":"One Million Scenes for Autonomous Driving: ONCE Dataset"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"6562e258e43a0fbc3622dc6353afd7c3251977c8","externalIds":{"DBLP":"journals/corr/abs-2105-10968","ArXiv":"2105.10968","DOI":"10.1109/itsc48978.2021.9564944","CorpusId":235166650},"title":"HOME: Heatmap Output for future Motion Estimation"},{"paperId":"c0559fc7e7d6ea0f783ba791ddd5deaa74cf58a9","externalIds":{"ArXiv":"2104.09224","DBLP":"journals/corr/abs-2104-09224","DOI":"10.1109/CVPR46437.2021.00700","CorpusId":233148602},"title":"Multi-Modal Fusion Transformer for End-to-End Autonomous Driving"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"4bbf33f2f3e15fbbaaa32948cbd0989741acda97","externalIds":{"ArXiv":"2103.09151","DBLP":"conf/ivs/WuYRRW23","DOI":"10.1109/IV55152.2023.10186386","CorpusId":232240647},"title":"Adversarial Driving: Attacking End-to-End Autonomous Driving"},{"paperId":"36830ba27a4b58bd50d25cf0a5214d8e88e95196","externalIds":{"DBLP":"journals/corr/abs-2103-05423","ArXiv":"2103.05423","CorpusId":232168831},"title":"Deep Learning based 3D Segmentation: A Survey"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"bad3534cc797606d1fe3cb09713407783e77cac4","externalIds":{"ArXiv":"2101.11174","DBLP":"journals/eswa/JiangL22","DOI":"10.1016/j.eswa.2022.117921","CorpusId":231718752},"title":"Graph Neural Network for Traffic Forecasting: A Survey"},{"paperId":"1358757b7d2e095e4606f59da548d580a2b59677","externalIds":{"DBLP":"journals/corr/abs-2101-04741","ArXiv":"2101.04741","CorpusId":231592665},"title":"CityFlow-NL: Tracking and Retrieval of Vehicles at City Scale by Natural Language Descriptions"},{"paperId":"78060fa493f597378a286d74c9979deb2775ff25","externalIds":{"MAG":"3108490973","DBLP":"journals/corr/abs-2012-01526","ArXiv":"2012.01526","DOI":"10.1109/ICCV48922.2021.01495","CorpusId":227254503},"title":"From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting"},{"paperId":"d4de794b700a91150d9fd8228b48f49ba54b3b30","externalIds":{"DBLP":"journals/ett/YuanNROBT22","MAG":"2972381239","DOI":"10.1002/ett.4427","CorpusId":203216722},"title":"Machine learning for next‐generation intelligent transportation systems: A survey"},{"paperId":"7dc04d1f17f1a71d684c01693d0de66fd6517f46","externalIds":{"DBLP":"conf/icca/LiFL20","MAG":"3106673115","DOI":"10.1109/ICCA51439.2020.9264412","CorpusId":227278354},"title":"A Survey on federated learning*"},{"paperId":"70bc865301a87cba2ddd892f6a5ee88368c1b271","externalIds":{"MAG":"3109791956","ArXiv":"2008.05930","DBLP":"journals/corr/abs-2008-05930","DOI":"10.1007/978-3-030-58592-1_25","CorpusId":221112947},"title":"Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable Semantic Representations"},{"paperId":"5ff9429efe7b6c4e7c039064920600e2b7664828","externalIds":{"DBLP":"journals/corr/abs-2006-14480","MAG":"3037058446","ArXiv":"2006.14480","CorpusId":220056216},"title":"One Thousand and One Hours: Self-driving Motion Prediction Dataset"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","externalIds":{"MAG":"3096609285","DBLP":"conf/eccv/CarionMSUKZ20","ArXiv":"2005.12872","DOI":"10.1007/978-3-030-58452-8_13","CorpusId":218889832},"title":"End-to-End Object Detection with Transformers"},{"paperId":"a46c2964c584889297c731407ad9e15909e0d687","externalIds":{"MAG":"3035292170","ArXiv":"2003.09405","DBLP":"journals/corr/abs-2003-09405","DOI":"10.1109/CVPR42600.2020.00954","CorpusId":214605757},"title":"Explainable Object-Induced Action Decision for Autonomous Vehicles"},{"paperId":"2dbc968d0e2fe7592473f2c4857c5e8b8b6c58c7","externalIds":{"DBLP":"journals/tnn/TampuuMSFN22","ArXiv":"2003.06404","MAG":"3011462663","DOI":"10.1109/TNNLS.2020.3043505","CorpusId":212717861,"PubMed":"33373304"},"title":"A Survey of End-to-End Driving: Architectures and Training Methods"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"8406903fd2f0eb25349bf071ccfaae3947e2a9cd","externalIds":{"MAG":"3035172746","DBLP":"conf/cvpr/SunKDCPTGZCCVHN20","ArXiv":"1912.04838","DOI":"10.1109/CVPR42600.2020.00252","CorpusId":209140225},"title":"Scalability in Perception for Autonomous Driving: Waymo Open Dataset"},{"paperId":"c8b041226ef3231d87297ac546bd1c2fd82b35ca","externalIds":{"MAG":"3003954087","DBLP":"conf/iros/NMKGBK19","DOI":"10.1109/IROS40897.2019.8967929","CorpusId":210971098},"title":"Talk to the Vehicle: Language Conditioned Autonomous Navigation of Self Driving Cars"},{"paperId":"ecf2a5496c765c8a0133c45952e82e3756961a11","externalIds":{"DBLP":"journals/corr/abs-1909-10838","MAG":"2970603850","ArXiv":"1909.10838","ACL":"D19-1215","DOI":"10.18653/v1/D19-1215","CorpusId":202734592},"title":"Talk2Car: Taking Control of Your Self-Driving Car"},{"paperId":"93d6752f11d5db3687cc9f895f219b1bed7e1023","externalIds":{"MAG":"2992272656","ArXiv":"1907.09693","DBLP":"journals/tkde/LiWWHWLLH23","DOI":"10.1109/TKDE.2021.3124599","CorpusId":198179889},"title":"A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection"},{"paperId":"d9f1f536c67e650992ec4afd66166740f860c99b","externalIds":{"MAG":"2953303875","DBLP":"journals/access/YurtseverLCT20","ArXiv":"1906.05113","DOI":"10.1109/ACCESS.2020.2983149","CorpusId":186206717},"title":"A Survey of Autonomous Driving: Common Practices and Emerging Technologies"},{"paperId":"dee86cf4ddab965169297960b55a4a2c0aee58d5","externalIds":{"MAG":"2986684110","DBLP":"conf/cvpr/KimMCTC19","ArXiv":"1911.06978","DOI":"10.1109/CVPR.2019.01084","CorpusId":204241631},"title":"Grounding Human-To-Vehicle Advice for Self-Driving Vehicles"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"4511f4100decc138031f93212cfd921bf42f72e2","externalIds":{"DBLP":"conf/iccv/BehleyGMQBSG19","MAG":"2965803762","DOI":"10.1109/ICCV.2019.00939","CorpusId":199441943},"title":"SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"9be285ae8994b68868b54c16ab355e93adce41ad","externalIds":{"DBLP":"journals/corr/abs-1903-09254","MAG":"2965843888","ArXiv":"1903.09254","DOI":"10.1109/CVPR.2019.00900","CorpusId":85459559},"title":"CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification"},{"paperId":"3bb322718d64a34b91b29c8230c5978de5d7fb7a","externalIds":{"MAG":"2905076052","ArXiv":"1812.05784","DBLP":"conf/cvpr/LangVCZYB19","DOI":"10.1109/CVPR.2019.01298","CorpusId":55701967},"title":"PointPillars: Fast Encoders for Object Detection From Point Clouds"},{"paperId":"b59233aab8364186603967bc12d88af48cc0992d","externalIds":{"DBLP":"journals/corr/abs-1812-01717","ArXiv":"1812.01717","MAG":"2902437806","CorpusId":54458806},"title":"Towards Accurate Generative Models of Video: A New Metric & Challenges"},{"paperId":"f75f0750a00f6f85107985c70eca9c275b5e0962","externalIds":{"MAG":"3042113477","ArXiv":"1811.12354","DBLP":"conf/cvpr/ChenSMSA19","DOI":"10.1109/CVPR.2019.01282","CorpusId":54078068},"title":"TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments"},{"paperId":"9d0907770cd4619aa6a36139a859e8f09bc9f0ef","externalIds":{"MAG":"2885138528","ArXiv":"1807.11546","DBLP":"conf/eccv/KimRDCA18","DOI":"10.1007/978-3-030-01216-8_35","CorpusId":51887402},"title":"Textual Explanations for Self-Driving Vehicles"},{"paperId":"e65c2b0feddfe4c89e9955ca9b5ece6ef416628f","externalIds":{"DBLP":"conf/cvpr/YuCWXCLMD20","ArXiv":"1805.04687","MAG":"3016101116","DOI":"10.1109/cvpr42600.2020.00271","CorpusId":215415900},"title":"BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning"},{"paperId":"7a82d83f818cdc4ac714e468446bc2499ff9caa7","externalIds":{"MAG":"2952467804","DBLP":"journals/corr/abs-1801-01582","ArXiv":"1801.01582","DOI":"10.1109/CVPR.2018.00434","CorpusId":4576781},"title":"Object Referring in Videos with Language and Human Gaze"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","externalIds":{"MAG":"2963981733","DBLP":"conf/nips/HeuselRUNH17","CorpusId":326772},"title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"fc755b712c3a649de4cc635a5671cc89344772ad","externalIds":{"MAG":"2735338899","DOI":"10.1109/CCDC.2017.7979175","CorpusId":42598149},"title":"Sensor fusion: A review of methods and applications"},{"paperId":"03ba6a574991d904a5d17f9c57f9ae26378baae8","externalIds":{"MAG":"2594507094","ArXiv":"1702.05693","DBLP":"conf/cvpr/ZhangBS17","DOI":"10.1109/CVPR.2017.474","CorpusId":14020880},"title":"CityPersons: A Diverse Dataset for Pedestrian Detection"},{"paperId":"0e3cc46583217ec81e87045a4f9ae3478a008227","externalIds":{"DBLP":"journals/corr/BojarskiTDFFGJM16","ArXiv":"1604.07316","MAG":"2342840547","CorpusId":15780954},"title":"End to End Learning for Self-Driving Cars"},{"paperId":"c8c494ee5488fe20e0aa01bddf3fc4632086d654","externalIds":{"DBLP":"journals/corr/CordtsORREBFRS16","MAG":"2953139137","ArXiv":"1604.01685","DOI":"10.1109/CVPR.2016.350","CorpusId":502946},"title":"The Cityscapes Dataset for Semantic Urban Scene Understanding"},{"paperId":"30feade758d15844a5746fd0de7983d5a0e4af02","externalIds":{"MAG":"2793429163","DBLP":"journals/corr/abs-1803-01937","ArXiv":"1803.01937","CorpusId":3714849},"title":"ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"3dffacda086689c1bcb01a8dad4557a4e92b8205","externalIds":{"DBLP":"journals/ai/LuoXMZLK21","ArXiv":"1409.7618","MAG":"2295615964","DOI":"10.1016/j.artint.2020.103448","CorpusId":17036197},"title":"Multiple object tracking: A literature review"},{"paperId":"79b949d9b35c3f51dd20fb5c746cc81fc87147eb","externalIds":{"MAG":"2115579991","DBLP":"journals/ijrr/GeigerLSU13","DOI":"10.1177/0278364913491297","CorpusId":9455111},"title":"Vision meets robotics: The KITTI dataset"},{"paperId":"4326d7e9933c77ff9dc53056c62ef6712d90c633","externalIds":{"DBLP":"journals/ijrr/KaramanF11","MAG":"2952108591","ArXiv":"1105.1186","DOI":"10.1177/0278364911406761","CorpusId":14876957},"title":"Sampling-based algorithms for optimal motion planning"},{"paperId":"3e083dc8aeb7983a5cdff146985363d38caf0886","externalIds":{"MAG":"2107775979","DBLP":"conf/cvpr/DollarWSP09","DOI":"10.1109/CVPR.2009.5206631","CorpusId":2451341},"title":"Pedestrian detection: A benchmark"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"afcd60559f9bc54bc2e0fd063fc0eb2bef87482e","externalIds":{"MAG":"2000359213","DBLP":"conf/icra/LaValleK99","DOI":"10.1109/ROBOT.1999.770022","CorpusId":18187370},"title":"Randomized Kinodynamic Planning"},{"paperId":"8b250ccabe8abc285481f15e6dd5e6f38a067055","externalIds":{"MAG":"1978301848","DBLP":"journals/ijrr/BarraquandL91","DOI":"10.1177/027836499101000604","CorpusId":34869088},"title":"Robot Motion Planning: A Distributed Representation Approach"},{"paperId":"221aa3be55a4ead8fc2aa83b12aac370bfba72f5","externalIds":{"MAG":"2183506185","DBLP":"journals/tssc/HartNR68","DOI":"10.1109/TSSC.1968.300136","CorpusId":206799161},"title":"A Formal Basis for the Heuristic Determination of Minimum Cost Paths"},{"paperId":"825298ea1933f01f1cfbff0c5e9bca4c60fdb5cd","externalIds":{"DBLP":"journals/corr/abs-2309-13193","DOI":"10.48550/arXiv.2309.13193","CorpusId":262460657},"title":"SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model"},{"paperId":"5ef82a8c8aa50f99285f2143b57ca4e82da1af80","externalIds":{"DBLP":"journals/corr/abs-2303-10512","DOI":"10.48550/arXiv.2303.10512","CorpusId":257631760},"title":"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"},{"paperId":"7a29f47f6509011fe5b19462abf6607867b68373","externalIds":{"CorpusId":263218031},"title":"GPT-4V(ision) System Card"},{"paperId":"326f0f326787d7c3542df6da3ba111fd6c7e53b6","externalIds":{"DBLP":"journals/corr/abs-2304-08083","DOI":"10.48550/arXiv.2304.08083","CorpusId":267365640},"title":"Causality-aware Visual Scene Discovery for Cross-Modal Question Reasoning"},{"paperId":"06d8562831c32844285a691c5250d04726df3c61","externalIds":{"DBLP":"journals/corr/abs-2307-12980","DOI":"10.48550/arXiv.2307.12980","CorpusId":260357841},"title":"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models"},{"paperId":"b9f5df8abd5c2d73c962c7cb9eb3916f5acd557a","externalIds":{"ACL":"2022.acl-long.71","DBLP":"conf/acl/LiuTF022","DOI":"10.18653/v1/2022.acl-long.71","CorpusId":248780060},"title":"Multi-Granularity Structural Knowledge Distillation for Language Model Compression"},{"paperId":"775f42ed458b8c5b0f2094ea4ff5b64c557b1a34","externalIds":{"CorpusId":251881108},"title":"A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27"},{"paperId":"1fda440226e81df93610f6d77dc51375afd5a2dc","externalIds":{"DBLP":"journals/access/PodschwadtTHRC22","DOI":"10.1109/ACCESS.2022.3219049","CorpusId":253353302},"title":"A Survey of Deep Learning Architectures for Privacy-Preserving Machine Learning With Fully Homomorphic Encryption"},{"paperId":"65d544fc3fe404ada6945df77ed58fef0618d819","externalIds":{"DBLP":"journals/access/LiQSWX21","DOI":"10.1109/ACCESS.2020.3047091","CorpusId":230994316},"title":"The Traffic Scene Understanding and Prediction Based on Image Captioning"},{"paperId":"b7e34296ea9a74193b906b37c5bb6d939ba68017","externalIds":{"CorpusId":219632972},"title":"Supplemental Material to: Advisable Learning for Self-driving Vehicles by Internalizing Observation-to-Action Rules"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"d7e18e8e88cb9447b276a5b74d4790de8a8b2185","externalIds":{"MAG":"2905316674","DBLP":"journals/itsm/ChenHTC19","DOI":"10.1109/MITS.2018.2884515","CorpusId":58674085},"title":"Parallel Motion Planning: Learning a Deep Planning Model against Emergencies"},{"paperId":"875b416e3c0420a46f47920d5ae3a504974174fa","externalIds":{"DOI":"10.4271/j3016_201609","CorpusId":53007988},"title":"Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles"}]}