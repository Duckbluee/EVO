{"abstract":"With the continuous improvements of deepfake methods, forgery messages have transitioned from single-modality to multi-modal fusion, posing new challenges for existing forgery detection algorithms. In this letter, we propose <bold>AVT<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math></inline-formula>-DWF</bold>, the <bold>A</bold>udio-<bold>V</bold>isual dual <bold>T</bold>ransformers grounded in <bold>D</bold>ynamic <bold>W</bold>eight <bold>F</bold>usion, which aims to amplify both intra- and cross-modal forgery cues, thereby enhancing detection capabilities. AVT<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math></inline-formula>-DWF adopts a dual-stage approach to capture both spatial characteristics and temporal dynamics of facial expressions. This is achieved through a face transformer with an <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math></inline-formula>-frame-wise tokenization strategy encoder and an audio transformer encoder. Subsequently, it uses multi-modal conversion with dynamic weight fusion to address the challenge of heterogeneous information fusion between audio and visual modalities. Experiments on DeepfakeTIMIT, FakeAVCeleb, and DFDC datasets indicate that AVT<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math></inline-formula>-DWF achieves state-of-the-art performance intra- and cross-dataset Deepfake detection."}