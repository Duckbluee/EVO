{"abstract":"Foundation models have exhibited unprecedented capabilities across various domains and tasks. Models like CLIP bridge cross-modal representations, while text-to-image diffusion models excel in realistic image generation. While the complexity of these models makes retraining infeasible, their superior performance has driven research to explore how to efficiently use them for downstream tasks. Our work explores how to leverage these models for dense visual prediction tasks, specifically image segmentation. To avoid the annotation cost or training large diffusion models, we constrain our method to be zero-shot and training-free. Our pipeline, dubbed FreeSeg-Diff, uses open-source foundation models to perform open-vocabulary segmentation as follows: (a) retrieving image caption (via BLIP-2) and visual features (via Stable Diffusion), (b) clustering and binarizing features to form class-agnostic object masks, (c) mapping these masks to textual classes using CLIP with open vocabulary support, and (d) refining coarse masks. FreeSeg-Diff surpasses many training-based methods on Pascal VOC and COCO datasets and delivers competitive results against recent weakly-supervised segmentation approaches. We provide experiments demonstrating the superiority of diffusion model features over other pre-trained models. Project page: https://bcorrad.github.io/freesegdiff/."}