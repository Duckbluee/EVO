{"abstract":"Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theoretically prove that the suboptimality of the estimated value in our proposed method scales with $\\tilde{\\mathcal O}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} + \\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations, $|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM, and $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the errors incurred by adopting LLMs as action space pruner and value function proxy, respectively. Our experiments in chess and go demonstrate the capability of our method to address challenges beyond the scope of MCTS and improve the performance of the directly application of LLMs."}