{"references":[{"paperId":"0fad9dd4f0ea41732594f90209907bfad1ba506e","externalIds":{"DBLP":"journals/corr/abs-2406-04244","ArXiv":"2406.04244","DOI":"10.48550/arXiv.2406.04244","CorpusId":270285708},"title":"Benchmark Data Contamination of Large Language Models: A Survey"},{"paperId":"b4a803ee85bf10f7b458324fb0998e77bdfdfb3c","externalIds":{"DBLP":"journals/corr/abs-2405-11930","ArXiv":"2405.11930","DOI":"10.48550/arXiv.2405.11930","CorpusId":269921283},"title":"Data Contamination Calibration for Black-box LLMs"},{"paperId":"af43203952a41fc52be2eb571e2f62ecf0007646","externalIds":{"DBLP":"journals/corr/abs-2403-06644","ArXiv":"2403.06644","DOI":"10.48550/arXiv.2403.06644","CorpusId":268353040},"title":"Elephants Never Forget: Testing Language Models for Memorization of Tabular Data"},{"paperId":"6bf28ebbb8df92582bc53a7fe49016e0caa4c074","externalIds":{"DBLP":"journals/corr/abs-2403-04811","ArXiv":"2403.04811","DOI":"10.48550/arXiv.2403.04811","CorpusId":268297237},"title":"Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models"},{"paperId":"a932b662645ab4a348c44c73bb81876cb415ae95","externalIds":{"ArXiv":"2402.09910","DBLP":"conf/icml/DuarteZO024","DOI":"10.48550/arXiv.2402.09910","CorpusId":267681760},"title":"DE-COP: Detecting Copyrighted Content in Language Models Training Data"},{"paperId":"2837a485b5895efea2eb63a707db2196be1d4a2f","externalIds":{"DBLP":"conf/acl/RanaldiRORGFRZ24","ArXiv":"2402.08100","DOI":"10.18653/v1/2024.findings-acl.827","CorpusId":267636801},"title":"Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation"},{"paperId":"798feda076ad710df65d509a7884bd15937c8056","externalIds":{"ACL":"2024.eacl-long.5","DBLP":"journals/corr/abs-2402-03927","ArXiv":"2402.03927","DOI":"10.48550/arXiv.2402.03927","CorpusId":267499939},"title":"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs"},{"paperId":"4d249bbfc172d5d4360244447f9e2245e318803d","externalIds":{"DBLP":"journals/corr/abs-2402-02823","ArXiv":"2402.02823","DOI":"10.48550/arXiv.2402.02823","CorpusId":267412617},"title":"Evading Data Contamination Detection for Language Models is (too) Easy"},{"paperId":"ac45bbf9940512d9d686cf8cd3a95969bc313570","externalIds":{"DBLP":"journals/corr/abs-2402-00838","ArXiv":"2402.00838","DOI":"10.48550/arXiv.2402.00838","CorpusId":267365485},"title":"OLMo: Accelerating the Science of Language Models"},{"paperId":"88aeeaec712a38d741c59b8e6faab2709e678759","externalIds":{"ArXiv":"2401.10463","DBLP":"journals/corr/abs-2401-10463","DOI":"10.48550/arXiv.2401.10463","CorpusId":267061159},"title":"Critical Data Size of Language Models from a Grokking Perspective"},{"paperId":"b2fda33b7c122c044a7faa185d250d59ce9e4453","externalIds":{"DBLP":"journals/corr/abs-2401-06059","ArXiv":"2401.06059","DOI":"10.48550/arXiv.2401.06059","CorpusId":266933004},"title":"Investigating Data Contamination for Pre-training Language Models"},{"paperId":"a7b20c1bba14d4cd2b317138496d35d47142d30f","externalIds":{"ArXiv":"2312.16337","DBLP":"conf/aaai/LiF24","DOI":"10.48550/arXiv.2312.16337","CorpusId":266573740},"title":"Task Contamination: Language Models May Not Be Few-Shot Anymore"},{"paperId":"8106d03fb984afd8c3d066cd4f993eb2616a0da5","externalIds":{"DBLP":"journals/corr/abs-2312-12343","ArXiv":"2312.12343","DOI":"10.48550/arXiv.2312.12343","CorpusId":266362809},"title":"LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction"},{"paperId":"227b5f8206b64858edeef6723b96af14133077e3","externalIds":{"DBLP":"journals/corr/abs-2311-04850","ArXiv":"2311.04850","DOI":"10.48550/arXiv.2311.04850","CorpusId":265050721},"title":"Rethinking Benchmark and Contamination for Language Models with Rephrased Samples"},{"paperId":"84725855d10b531eb8cbe54935dda0440c2fc750","externalIds":{"ArXiv":"2311.01964","DBLP":"journals/corr/abs-2311-01964","DOI":"10.48550/arXiv.2311.01964","CorpusId":265019021},"title":"Don't Make Your LLM an Evaluation Benchmark Cheater"},{"paperId":"71c9d2b995c19d43c519eb5ca9504ac790490398","externalIds":{"DBLP":"journals/corr/abs-2310-19341","ArXiv":"2310.19341","DOI":"10.48550/arXiv.2310.19341","CorpusId":264802115},"title":"Skywork: A More Open Bilingual Foundation Model"},{"paperId":"cd2f4aaf98bb1e020cff310000c8049d3460c54e","externalIds":{"DBLP":"journals/corr/abs-2310-18018","ArXiv":"2310.18018","DOI":"10.48550/arXiv.2310.18018","CorpusId":264555419},"title":"NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark"},{"paperId":"c871377b208814713c18e25633866323a2982136","externalIds":{"ArXiv":"2310.17623","DBLP":"journals/corr/abs-2310-17623","DOI":"10.48550/arXiv.2310.17623","CorpusId":264490730},"title":"Proving Test Set Contamination in Black Box Language Models"},{"paperId":"2abc0cc453862e282a8014b9c5772b59da098674","externalIds":{"ArXiv":"2310.10628","DBLP":"journals/corr/abs-2310-10628","DOI":"10.48550/arXiv.2310.10628","CorpusId":264172693},"title":"Data Contamination Through the Lens of Time"},{"paperId":"e800ff2229ef60b74663d8fe4e330243729b046c","externalIds":{"DBLP":"journals/corr/abs-2309-10677","ArXiv":"2309.10677","DOI":"10.48550/arXiv.2309.10677","CorpusId":262055119},"title":"Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation"},{"paperId":"f9ab990ca3c0715e31854ec1087af572af8de8a6","externalIds":{"DBLP":"journals/corr/abs-2309-08632","ArXiv":"2309.08632","DOI":"10.48550/arXiv.2309.08632","CorpusId":262045629},"title":"Pretraining on the Test Set Is All You Need"},{"paperId":"751563cf0c32fe4dfa43d3416c916f8eb053e5f3","externalIds":{"DBLP":"conf/acl/PiktusOAOZSBPL23","ACL":"2023.acl-demo.57","ArXiv":"2306.01481","DOI":"10.48550/arXiv.2306.01481","CorpusId":259064270},"title":"GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration"},{"paperId":"cb754310302086dfbbcd098263200e2a03f65874","externalIds":{"DBLP":"conf/acl/MatternMJSSB23","ArXiv":"2305.18462","DOI":"10.48550/arXiv.2305.18462","CorpusId":258967264},"title":"Membership Inference Attacks against Language Models via Neighbourhood Comparison"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"fc30093e9f55ae1c0a1d2c4c4e5341998adede66","externalIds":{"DBLP":"conf/emnlp/JacoviCGG23","ArXiv":"2305.10160","DOI":"10.48550/arXiv.2305.10160","CorpusId":258741333},"title":"Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks"},{"paperId":"133b97e40017a9bbbadd10bcd7f13088a97ca3cc","externalIds":{"DBLP":"conf/emnlp/GevaBFG23","ArXiv":"2304.14767","DOI":"10.48550/arXiv.2304.14767","CorpusId":258417932},"title":"Dissecting Recall of Factual Associations in Auto-Regressive Language Models"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","externalIds":{"DBLP":"conf/icml/BidermanSABOHKP23","ArXiv":"2304.01373","DOI":"10.48550/arXiv.2304.01373","CorpusId":257921893},"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"7d40189f3fa56728b8d210628e98fc204961778f","externalIds":{"ArXiv":"2303.12767","DBLP":"journals/corr/abs-2303-12767","ACL":"2023.trustnlp-1.5","DOI":"10.18653/v1/2023.trustnlp-1.5","CorpusId":257663514},"title":"Can we trust the evaluation on ChatGPT?"},{"paperId":"2ed0030d06ac2cc739c8460c102ae4713d10d1e1","externalIds":{"DBLP":"conf/acl/PiktusAVLDLJR23","ArXiv":"2302.14035","ACL":"2023.acl-demo.29","DOI":"10.48550/arXiv.2302.14035","CorpusId":257219882},"title":"The ROOTS Search Tool: Data Transparency for LLMs"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"870693e9de7711d46ec621eab77b5b282f2caf47","externalIds":{"DBLP":"journals/corr/abs-2210-03588","ACL":"2023.eacl-main.19","ArXiv":"2210.03588","DOI":"10.48550/arXiv.2210.03588","CorpusId":252762655},"title":"Understanding Transformer Memorization Recall Through Idioms"},{"paperId":"48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775","externalIds":{"ACL":"2022.emnlp-main.233","DBLP":"conf/emnlp/BlevinsZ22","ArXiv":"2204.08110","DOI":"10.18653/v1/2022.emnlp-main.233","CorpusId":252780005},"title":"Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"ae9f2d6a29daec0eaf5700d327df0f4f30558b74","externalIds":{"DBLP":"journals/corr/abs-2203-16639","ArXiv":"2203.16639","DOI":"10.48550/arXiv.2203.16639","CorpusId":247839322},"title":"FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations"},{"paperId":"a1d1983a7b19845141e6505bd32dc395e5a136ba","externalIds":{"ArXiv":"2201.02177","DBLP":"journals/corr/abs-2201-02177","CorpusId":245769834},"title":"Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"},{"paperId":"54d7ae7cfee56b0e19fd42c45d365f760a41794d","externalIds":{"ArXiv":"2112.03570","DBLP":"conf/sp/CarliniCN0TT22","DOI":"10.1109/sp46214.2022.9833649","CorpusId":244920593},"title":"Membership Inference Attacks From First Principles"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1adadbfa95e43a70fcd17e6ce947a0652b86bfc3","externalIds":{"ArXiv":"2104.08758","DBLP":"conf/emnlp/DodgeSMAIGM021","ACL":"2021.emnlp-main.98","DOI":"10.18653/v1/2021.emnlp-main.98","CorpusId":237568724},"title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus"},{"paperId":"5ac627f229fa8d54f5ad43f7f99e9b29d93ada29","externalIds":{"DBLP":"journals/corr/abs-2102-10073","ArXiv":"2102.10073","CorpusId":231979055},"title":"Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","externalIds":{"DBLP":"journals/corr/abs-2101-00027","ArXiv":"2101.00027","CorpusId":230435736},"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"4a54d58a4b20e4f3af25cea3c188a12082a95e02","externalIds":{"DBLP":"conf/emnlp/GevaSBL21","ACL":"2021.emnlp-main.446","ArXiv":"2012.14913","DOI":"10.18653/v1/2021.emnlp-main.446","CorpusId":229923720},"title":"Transformer Feed-Forward Layers Are Key-Value Memories"},{"paperId":"df7d26339adf4eb0c07160947b9d2973c24911ba","externalIds":{"DBLP":"journals/corr/abs-2012-07805","MAG":"3112689365","ArXiv":"2012.07805","CorpusId":229156229},"title":"Extracting Training Data from Large Language Models"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"7c41e58832f3af5fd9e09674924d6b5f822e8eac","externalIds":{"ACL":"2020.acl-main.742","MAG":"3035172316","DBLP":"conf/acl/SuhrCSL20","DOI":"10.18653/v1/2020.acl-main.742","CorpusId":220047209},"title":"Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"b5ef0f91663f0cbd6910dec9a890c138f7ec10e0","externalIds":{"DBLP":"journals/corr/abs-2004-06165","MAG":"3091588028","ArXiv":"2004.06165","DOI":"10.1007/978-3-030-58577-8_8","CorpusId":215754208},"title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"4ae52766028e69186052ea8f33a137fbbbdb986a","externalIds":{"MAG":"3035252911","ArXiv":"2004.04696","DBLP":"conf/acl/SellamDP20","ACL":"2020.acl-main.704","DOI":"10.18653/v1/2020.acl-main.704","CorpusId":215548699},"title":"BLEURT: Learning Robust Metrics for Text Generation"},{"paperId":"347e837b1aa03c9d17c69a522929000f0a0f0a51","externalIds":{"MAG":"2990957769","DBLP":"journals/corr/abs-1911-11763","ArXiv":"1911.11763","DOI":"10.1109/cvpr42600.2020.00499","CorpusId":208291327},"title":"SuperGlue: Learning Feature Matching With Graph Neural Networks"},{"paperId":"636904d91d9dd1a641a595d9578ba7640f35aa74","externalIds":{"DBLP":"conf/acl/TalmorB19","MAG":"2947567669","ACL":"P19-1485","ArXiv":"1905.13453","DOI":"10.18653/v1/P19-1485","CorpusId":173188058},"title":"MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","externalIds":{"MAG":"2789352267","DBLP":"conf/icml/LakeB18","CorpusId":46761158},"title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7","externalIds":{"MAG":"2795435272","DBLP":"conf/csfw/YeomGFJ18","DOI":"10.1109/CSF.2018.00027","CorpusId":2656445},"title":"Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting"},{"paperId":"f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d","externalIds":{"ArXiv":"1610.05820","DBLP":"journals/corr/ShokriSS16","MAG":"2535690855","DOI":"10.1109/SP.2017.41","CorpusId":10488675},"title":"Membership Inference Attacks Against Machine Learning Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"}]}