{"abstract":"Federated Learning (FL) is a distributed learning method, where participants learn a model collaboratively by exchanging information without sharing their ‘raw’ databases. This is of great importance in areas where databases hold private information of individuals, for e.g., healthcare information. In this paper we address two major challenges in FL for Linear Regression models when used on private databases in a Mobile Edge Computing (MEC) environment: 1) Providing provable privacy guarantees and 2) Reduction in communication and computational overheads at the client. We propose our scheme Differentially Private-Coded Federated Linear Regression (DP-CFL) in which each client generates “coded” data by perturbed random linear combinations of its data and shares them with a MEC Server. The MEC Server combines coded data from multiple clients and performs gradient descent in the coded domain without any further communication from clients. We analytically prove (∊, δ)-Differential Privacy guarantee for our scheme and use a combination of analysis and simulations to show the computation and communication savings of our approach with respect to a Federated Differentially Private-Stochastic Gradient Descent (FDP-SGD)."}