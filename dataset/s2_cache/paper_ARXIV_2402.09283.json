{"paperId":"e79671a83e25288fedd897e1c9e6152f70f7f52e","externalIds":{"DBLP":"journals/corr/abs-2402-09283","ArXiv":"2402.09283","ACL":"2024.naacl-long.375","DOI":"10.48550/arXiv.2402.09283","CorpusId":267658120},"title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2402.09283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2284178258","name":"Zhichen Dong"},{"authorId":"2254279326","name":"Zhanhui Zhou"},{"authorId":"2268678836","name":"Chao Yang"},{"authorId":"2254280929","name":"Jing Shao"},{"authorId":"2268675804","name":"Yu Qiao"}],"abstract":"Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety."}