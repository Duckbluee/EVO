{"references":[{"paperId":"7d0d587ca37ee09247886684a220bb1625e77910","externalIds":{"ArXiv":"2404.16375","DBLP":"journals/corr/abs-2404-16375","DOI":"10.48550/arXiv.2404.16375","CorpusId":269362176},"title":"List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs"},{"paperId":"bdb103a4bfafda077b36aa592536425c2695eef3","externalIds":{"DBLP":"journals/corr/abs-2404-14688","ArXiv":"2404.14688","DOI":"10.1002/adts.202500062","CorpusId":269302905},"title":"FMint: Bridging Human Designed and Data Pretrained Models for Differential EquationÂ Foundation Model for Dynamical Simulation"},{"paperId":"60f2cb7bea3bf55eebddaea37669418c0c39aad6","externalIds":{"DBLP":"journals/corr/abs-2403-19949","ArXiv":"2403.19949","DOI":"10.1109/CVPR52733.2024.01168","CorpusId":268793893},"title":"FairCLIP: Harnessing Fairness in Vision-Language Learning"},{"paperId":"1dad0ffa0a63506c41cd3a6b83e187360b3189c5","externalIds":{"ArXiv":"2402.10104","DBLP":"conf/acl/ZhangLZYLM24","DOI":"10.48550/arXiv.2402.10104","CorpusId":267682193},"title":"GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving"},{"paperId":"9cf6e252b8a5566910e6fceda7d6a25c5b54be33","externalIds":{"ACL":"2024.naacl-long.117","ArXiv":"2402.07398","DBLP":"journals/corr/abs-2402-07398","DOI":"10.48550/arXiv.2402.07398","CorpusId":267627576},"title":"VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization"},{"paperId":"1cfb7fba7194860e8b8818eb5e87e0a8e14e518a","externalIds":{"DBLP":"conf/eccv/YanBCZHL24","ArXiv":"2402.06118","DOI":"10.48550/arXiv.2402.06118","CorpusId":267616939},"title":"ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling"},{"paperId":"ec8e2b45c4601730015608a58e33409224a81228","externalIds":{"ArXiv":"2402.05935","DBLP":"conf/icml/LiuZQHLZGLJZSXH24","DOI":"10.48550/arXiv.2402.05935","CorpusId":267547619},"title":"SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models"},{"paperId":"c719c57788016b61172773ff951560ee5280e9c7","externalIds":{"ArXiv":"2402.04236","DBLP":"conf/iclr/QiDWBLHX0LDT25","CorpusId":267500084},"title":"CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning"},{"paperId":"a091bf215c716a146140f81c751712db628c8e20","externalIds":{"ArXiv":"2402.03766","DBLP":"journals/corr/abs-2402-03766","DOI":"10.48550/arXiv.2402.03766","CorpusId":267500104},"title":"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model"},{"paperId":"32c260ddd7e2d0b05c58f2e67d244b8036699db4","externalIds":{"DBLP":"conf/icml/KangGYS024","ArXiv":"2402.03181","DOI":"10.48550/arXiv.2402.03181","CorpusId":267412330},"title":"C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models"},{"paperId":"c1b5195bc09a2232ec2b69e5a2a6bd39b3162c62","externalIds":{"DBLP":"conf/icml/Jin00XCJHSLZ0GM24","ArXiv":"2402.03161","DOI":"10.48550/arXiv.2402.03161","CorpusId":267412251},"title":"Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization"},{"paperId":"bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e","externalIds":{"ArXiv":"2402.01364","DBLP":"journals/corr/abs-2402-01364","DOI":"10.48550/arXiv.2402.01364","CorpusId":267406164},"title":"Continual Learning for Large Language Models: A Survey"},{"paperId":"bce43cb9af37a0c8d90f8cadaebd6bb002685edd","externalIds":{"ArXiv":"2401.16420","DBLP":"journals/corr/abs-2401-16420","DOI":"10.48550/arXiv.2401.16420","CorpusId":267311889},"title":"InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model"},{"paperId":"af9676f9beaeca214bfbf7f2897828820d48abdc","externalIds":{"DBLP":"journals/corr/abs-2401-16160","ArXiv":"2401.16160","DOI":"10.48550/arXiv.2401.16160","CorpusId":267312176},"title":"LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs"},{"paperId":"c5db6c2726911b72d534f97bd4d1ed63f6431340","externalIds":{"ArXiv":"2401.16158","DBLP":"journals/corr/abs-2401-16158","DOI":"10.48550/arXiv.2401.16158","CorpusId":267311877},"title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception"},{"paperId":"19261c6ad20c6c1e5585a8afcb88196173cbc8a6","externalIds":{"DBLP":"conf/acl/HeYM0D0L024","ArXiv":"2401.13919","DOI":"10.48550/arXiv.2401.13919","CorpusId":267211622},"title":"WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models"},{"paperId":"41f12456780aecd204a210ce04b1a92d022b8c4c","externalIds":{"ArXiv":"2401.12503","DBLP":"journals/corr/abs-2401-12503","DOI":"10.48550/arXiv.2401.12503","CorpusId":267095012},"title":"Small Language Model Meets with Reinforced Vision Vocabulary"},{"paperId":"a3ae8705daa21f4be6970d8c7685112e1e95b843","externalIds":{"ArXiv":"2401.12863","DBLP":"conf/aaai/MondalMPSR24","DOI":"10.48550/arXiv.2401.12863","CorpusId":267095090},"title":"KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning"},{"paperId":"140cfda71bfff852c3e205b7ad61854b78c76982","externalIds":{"DBLP":"journals/corr/abs-2401-11708","ArXiv":"2401.11708","DOI":"10.48550/arXiv.2401.11708","CorpusId":267068823},"title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"},{"paperId":"4bfb79a29006559b49781a99e6f9c124a8907138","externalIds":{"ArXiv":"2401.10061","CorpusId":267035099},"title":"DiffusionAgent: Navigating Expert Models for Agentic Image Generation"},{"paperId":"616e98ba9e60f36c6ee226cc66c787610f0bbb62","externalIds":{"ArXiv":"2401.10208","DBLP":"journals/corr/abs-2401-10208","DOI":"10.48550/arXiv.2401.10208","CorpusId":267034565},"title":"MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer"},{"paperId":"bd7271fd7f595f66c47eaee28d5f556731882a18","externalIds":{"ArXiv":"2401.09181","DBLP":"journals/corr/abs-2401-09181","DOI":"10.48550/arXiv.2401.09181","CorpusId":267028471},"title":"Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer"},{"paperId":"ff61aef2fef3a235bfaa123158a990c4f5f27d1a","externalIds":{"ACL":"2024.emnlp-main.929","DBLP":"journals/corr/abs-2401-07324","ArXiv":"2401.07324","DOI":"10.48550/arXiv.2401.07324","CorpusId":266999372},"title":"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent"},{"paperId":"d9263ba30a16067c44884ad559802505acf2f82d","externalIds":{"DBLP":"journals/corr/abs-2401-03201","ArXiv":"2401.03201","DOI":"10.1109/ICMEW63481.2024.10645462","CorpusId":266844828},"title":"3DMIT: 3D Multi-Modal Instruction Tuning for Scene Understanding"},{"paperId":"ece33ee67d74c29cd2a83c505e5bf0b818f9c2a1","externalIds":{"DBLP":"journals/corr/abs-2401-02330","ArXiv":"2401.02330","DOI":"10.1145/3688863.3689575","CorpusId":266755915},"title":"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model"},{"paperId":"d98aa44f79fe798ad5ff0cac6e7bf32ee30bd156","externalIds":{"DBLP":"journals/corr/abs-2401-01523","ArXiv":"2401.01523","DOI":"10.1145/3729239","CorpusId":266741673},"title":"GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse"},{"paperId":"575f403261d5f99526f0b4dfc8644352d6c4467a","externalIds":{"DBLP":"conf/acl/0005RSMBKPNL24","ArXiv":"2401.00908","DOI":"10.48550/arXiv.2401.00908","CorpusId":266725585},"title":"DocLLM: A layout-aware generative language model for multimodal document understanding"},{"paperId":"98ab627dd147db88b5e5cfa9a74f1bd8da110021","externalIds":{"DBLP":"journals/corr/abs-2312-16862","ArXiv":"2312.16862","DOI":"10.48550/arXiv.2312.16862","CorpusId":266572996},"title":"TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones"},{"paperId":"1e1230ef1de1ba9c4f6cb4789184a295133afac0","externalIds":{"ArXiv":"2312.16602","DBLP":"journals/corr/abs-2312-16602","DOI":"10.48550/arXiv.2312.16602","CorpusId":266573642},"title":"Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","externalIds":{"ArXiv":"2312.14238","DBLP":"journals/corr/abs-2312-14238","DOI":"10.1109/CVPR52733.2024.02283","CorpusId":266521410},"title":"Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"c672ec79f55cef8f7a32cd8dddfa981b893f1567","externalIds":{"DBLP":"journals/corr/abs-2312-14135","ArXiv":"2312.14135","DOI":"10.1109/CVPR52733.2024.01243","CorpusId":266436019},"title":"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"},{"paperId":"4b1b5e219fb41a7413599c3b2ca6a7fdf045d1a5","externalIds":{"DBLP":"conf/cvpr/SunCZZYWRL0W24","ArXiv":"2312.13286","DOI":"10.1109/CVPR52733.2024.01365","CorpusId":266374640},"title":"Generative Multimodal Models are In-Context Learners"},{"paperId":"35a17f896847614a71df772bbe2b66ae231cabc7","externalIds":{"DBLP":"conf/cvpr/Gao0ZMHZL24","ArXiv":"2312.10908","DOI":"10.1109/CVPR52733.2024.01259","CorpusId":266359627},"title":"CLOVA: A Closed-LOop Visual Assistant with Tool Usage and Update"},{"paperId":"46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5","externalIds":{"ArXiv":"2312.10997","DBLP":"journals/corr/abs-2312-10997","CorpusId":266359151},"title":"Retrieval-Augmented Generation for Large Language Models: A Survey"},{"paperId":"b5503967d39557a77c70076c308183e92d6d775a","externalIds":{"DBLP":"conf/cvpr/YuanLLTLQZZ24","ArXiv":"2312.10032","DOI":"10.1109/CVPR52733.2024.02664","CorpusId":266335219},"title":"Osprey: Pixel Understanding with Visual Instruction Tuning"},{"paperId":"e50583008fe4ec049e42fdc01727ce98f6d86a35","externalIds":{"DBLP":"conf/cvpr/HongWLXYJWWD0024","ArXiv":"2312.08914","DOI":"10.1109/CVPR52733.2024.01354","CorpusId":266210390},"title":"CogAgent: A Visual Language Model for GUI Agents"},{"paperId":"ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","externalIds":{"DBLP":"journals/corr/abs-2312-09251","ArXiv":"2312.09251","DOI":"10.48550/arXiv.2312.09251","CorpusId":266210376},"title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation"},{"paperId":"d23f08611ea1e64f691ecddee1f7f48c8015eea6","externalIds":{"ArXiv":"2312.05278","CorpusId":266162357},"title":"Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects"},{"paperId":"d77bc1a237b67c57b0c1b99b4802e703747a9688","externalIds":{"DBLP":"journals/corr/abs-2312-02896","ArXiv":"2312.02896","DOI":"10.48550/arXiv.2312.02896","CorpusId":265658792},"title":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"},{"paperId":"0f9a3c5c6a54fca6be2afa0fd5fd34eed96a31e8","externalIds":{"DBLP":"conf/cvpr/YuYZHHCHL0024","ArXiv":"2312.00849","DOI":"10.1109/CVPR52733.2024.01310","CorpusId":265608723},"title":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-Grained Correctional Human Feedback"},{"paperId":"78582ad19779a69d97b797a3c6eb2397f99398b6","externalIds":{"DBLP":"conf/cvpr/TangYKLZB24","ArXiv":"2311.18775","DOI":"10.1109/CVPR52733.2024.02589","CorpusId":265506621},"title":"CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation"},{"paperId":"e263e08a20080a2543d0ca29d3d63c4717a8beb6","externalIds":{"DBLP":"journals/corr/abs-2311-18799","ArXiv":"2311.18799","DOI":"10.48550/arXiv.2311.18799","CorpusId":265506093},"title":"X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning"},{"paperId":"b50d19c5c298f6562c3b3c6c3822a351bdc89260","externalIds":{"ArXiv":"2311.16502","DBLP":"journals/corr/abs-2311-16502","DOI":"10.1109/CVPR52733.2024.00913","CorpusId":265466525},"title":"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"},{"paperId":"fe92a35c51ebba91ed99f7da0e0124434229a469","externalIds":{"DBLP":"journals/corr/abs-2311-16206","ArXiv":"2311.16206","DOI":"10.48550/arXiv.2311.16206","CorpusId":265466199},"title":"Continual Instruction Tuning for Large Multimodal Models"},{"paperId":"e99de66608a3b060d54548b9e9a7c39961872cd7","externalIds":{"DBLP":"journals/corr/abs-2311-16476","ArXiv":"2311.16476","DOI":"10.48550/arXiv.2311.16476","CorpusId":265466524},"title":"LANS: A Layout-Aware Neural Solver for Plane Geometry Problem"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","externalIds":{"DBLP":"journals/corr/abs-2311-13165","ArXiv":"2311.13165","DOI":"10.1109/BigData59044.2023.10386743","CorpusId":265351653},"title":"Multimodal Large Language Models: A Survey"},{"paperId":"f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560","externalIds":{"DBLP":"conf/eccv/ChenLDZHWZL24","ArXiv":"2311.12793","DOI":"10.48550/arXiv.2311.12793","CorpusId":265308687},"title":"ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"},{"paperId":"451539c0d0f5f5785ff58d09ca5e67a5f129f9de","externalIds":{"ArXiv":"2311.12320","DBLP":"journals/corr/abs-2311-12320","DOI":"10.1109/WACVW60836.2024.00106","CorpusId":265308931},"title":"A Survey on Multimodal Large Language Models for Autonomous Driving"},{"paperId":"dfa7120276a0a5d36c40de13278c9884305b7c7d","externalIds":{"DBLP":"journals/corr/abs-2311-11810","ArXiv":"2311.11810","DOI":"10.1007/s11432-024-4250-y","CorpusId":265295560},"title":"DocPedia: unleashing the power of large multimodal model in the frequency domain for versatile document understanding"},{"paperId":"391eaeb1092c2b145ff0e5a2fa61637a42921fce","externalIds":{"DBLP":"conf/cvpr/ChenSCJD24","ArXiv":"2311.10081","DOI":"10.1109/CVPR52733.2024.01350","CorpusId":265221232},"title":"DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"},{"paperId":"f90595f99a0c66d2bb6d0f230f17c7cd8c58f44d","externalIds":{"DBLP":"journals/corr/abs-2311-07919","ArXiv":"2311.07919","DOI":"10.48550/arXiv.2311.07919","CorpusId":265157993},"title":"Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models"},{"paperId":"de1894742b7f2e4fe02d9ff94761d6178e0a5d3c","externalIds":{"DBLP":"conf/naacl/LeePJS24","ACL":"2024.naacl-long.23","ArXiv":"2311.07362","DOI":"10.48550/arXiv.2311.07362","CorpusId":265150082},"title":"Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision"},{"paperId":"bf14244669d5505f63343d4365d99d24aa6c5e82","externalIds":{"ArXiv":"2311.06607","DBLP":"conf/cvpr/LiYLMZYSLB24","DOI":"10.1109/CVPR52733.2024.02527","CorpusId":265150038},"title":"Monkey: Image Resolution and Text Label are Important Things for Large Multi-Modal Models"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","externalIds":{"DBLP":"conf/eccv/LiuCLZLRZYSZZGL24","ArXiv":"2311.05437","DOI":"10.48550/arXiv.2311.05437","CorpusId":265067489},"title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"ad13b213681b6f634bc83a264df246e83dd9a9d9","externalIds":{"DBLP":"conf/cvpr/YeXYYHL0Z024","ArXiv":"2311.04257","DOI":"10.1109/CVPR52733.2024.01239","CorpusId":265050943},"title":"mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"},{"paperId":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","externalIds":{"DBLP":"journals/corr/abs-2311-03356","ArXiv":"2311.03356","DOI":"10.1109/CVPR52733.2024.01236","CorpusId":265043538},"title":"GLaMM: Pixel Grounding Large Multimodal Model"},{"paperId":"288e7224d53d68669eb67f2496e068dc965c639e","externalIds":{"DBLP":"conf/eccv/LiuLGCLZLCQDW24","ArXiv":"2310.17796","DOI":"10.48550/arXiv.2310.17796","CorpusId":264555643},"title":"ControlLLM: Augment Language Models with Tools by Searching on Graphs"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"8946891e94831adc8cddb0d32311cce2445c96d2","externalIds":{"DBLP":"conf/iclr/LuBX0LH0CG024","ArXiv":"2310.02255","CorpusId":264491155},"title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"},{"paperId":"8eb99f1ed884356871ddbcf1377b82359071906a","externalIds":{"DBLP":"conf/iclr/ZhuLNYCWPJZLZ0024","ArXiv":"2310.01852","DOI":"10.48550/arXiv.2310.01852","CorpusId":263608698},"title":"LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","externalIds":{"DBLP":"journals/corr/abs-2310-02239","ArXiv":"2310.02239","DOI":"10.48550/arXiv.2310.02239","CorpusId":263608981},"title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"4f5c4ae2026b2bd97e26c6969e54cc634895e477","externalIds":{"DBLP":"journals/corr/abs-2309-15564","ArXiv":"2309.15564","DOI":"10.48550/arXiv.2309.15564","CorpusId":262898422},"title":"Jointly Training Large Autoregressive Multimodal Models"},{"paperId":"f2f9c02a7eb484dd7b7ac46892856e3f278eed77","externalIds":{"DBLP":"journals/corr/abs-2309-16058","ArXiv":"2309.16058","ACL":"2024.emnlp-industry.98","DOI":"10.18653/v1/2024.emnlp-industry.98","CorpusId":263137930},"title":"AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"},{"paperId":"c1e450284e7d6cac1855330a1197df8537df653f","externalIds":{"DBLP":"journals/corr/abs-2309-15112","ArXiv":"2309.15112","DOI":"10.48550/arXiv.2309.15112","CorpusId":262824937},"title":"InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"593b42c628b49937bcd7f7c4a7d54d5f97e6b414","externalIds":{"DBLP":"journals/corr/abs-2309-14181","ArXiv":"2309.14181","DOI":"10.48550/arXiv.2309.14181","CorpusId":262824606},"title":"Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","externalIds":{"DBLP":"journals/corr/abs-2309-11499","ArXiv":"2309.11499","DOI":"10.48550/arXiv.2309.11499","CorpusId":261975252},"title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","externalIds":{"DBLP":"journals/corr/abs-2309-11419","ArXiv":"2309.11419","DOI":"10.48550/arXiv.2309.11419","CorpusId":262063906},"title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"587d0627031c165985c69036f62d5d21fc38e3f7","externalIds":{"ArXiv":"2309.02411","DBLP":"journals/corr/abs-2309-02411","DOI":"10.48550/arXiv.2309.02411","CorpusId":261556652},"title":"Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices"},{"paperId":"1a735015a1f7ef4f2ba2273ce5fcaaacfa9d1ea2","externalIds":{"ArXiv":"2309.02591","DBLP":"journals/corr/abs-2309-02591","DOI":"10.48550/arXiv.2309.02591","CorpusId":261556690},"title":"Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning"},{"paperId":"bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7","externalIds":{"DBLP":"journals/coling/GallegosRBTKDYZA24","ArXiv":"2309.00770","DOI":"10.1162/coli_a_00524","CorpusId":261530629},"title":"Bias and Fairness in Large Language Models: A Survey"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","externalIds":{"DBLP":"conf/eccv/XuWWCPL24","ArXiv":"2308.16911","DOI":"10.48550/arXiv.2308.16911","CorpusId":261397321},"title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"1245ef1926416d649b62323975c6fa22dfb885ee","externalIds":{"ArXiv":"2308.12038","DBLP":"journals/corr/abs-2308-12038","DOI":"10.48550/arXiv.2308.12038","CorpusId":261076491},"title":"Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages"},{"paperId":"da96ec9c32d63292e506ba8f8ea8e838df998c02","externalIds":{"DBLP":"journals/corr/abs-2308-10253","ArXiv":"2308.10253","DOI":"10.48550/arXiv.2308.10253","CorpusId":261049617},"title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"},{"paperId":"30cc95639cffca4ffa8c0eafbc502636c0c88fa5","externalIds":{"DBLP":"journals/corr/abs-2308-09936","ArXiv":"2308.09936","DOI":"10.48550/arXiv.2308.09936","CorpusId":261049015},"title":"BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"},{"paperId":"30cfc4e7174211aa48c965826d51db773f0d37c7","externalIds":{"DBLP":"journals/corr/abs-2308-08769","ArXiv":"2308.08769","DOI":"10.48550/arXiv.2308.08769","CorpusId":261031759},"title":"Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","externalIds":{"ArXiv":"2308.02490","DBLP":"journals/corr/abs-2308-02490","DOI":"10.48550/arXiv.2308.02490","CorpusId":260611572},"title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"659a12d71d8709c132ccd9ccd235f0024cae0239","externalIds":{"DBLP":"conf/iclr/Wang0LWHXCLZ0CL24","ArXiv":"2308.01907","DOI":"10.48550/arXiv.2308.01907","CorpusId":260438589},"title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","externalIds":{"DBLP":"conf/ijcai/ZhaoYGYWZSPDH024","ArXiv":"2307.09474","DOI":"10.48550/arXiv.2307.09474","CorpusId":259951197},"title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","externalIds":{"DBLP":"journals/corr/abs-2307-08581","ArXiv":"2307.08581","DOI":"10.48550/arXiv.2307.08581","CorpusId":259937702},"title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"98f8793a18eaced0ce93f5202065496cc5a84943","externalIds":{"ArXiv":"2307.07063","DBLP":"journals/corr/abs-2307-07063","DOI":"10.48550/arXiv.2307.07063","CorpusId":259924601},"title":"Bootstrapping Vision-Language Learning with Decoupled Language Pre-training"},{"paperId":"451a3f03aca4aa87b93981364842137417549e58","externalIds":{"DBLP":"journals/corr/abs-2307-04087","ArXiv":"2307.04087","DOI":"10.48550/arXiv.2307.04087","CorpusId":259501644},"title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","externalIds":{"ArXiv":"2307.03601","DBLP":"conf/eccv/ZhangSCXSZLCL24","DOI":"10.48550/arXiv.2307.03601","CorpusId":259375716},"title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","externalIds":{"ArXiv":"2307.02469","ACL":"2024.naacl-long.440","DBLP":"conf/naacl/ZengZZXWWZKS24","DOI":"10.48550/arXiv.2307.02469","CorpusId":259342291},"title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","externalIds":{"ArXiv":"2307.02499","DBLP":"journals/corr/abs-2307-02499","DOI":"10.48550/arXiv.2307.02499","CorpusId":259360848},"title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","externalIds":{"DBLP":"journals/corr/abs-2306-17107","ArXiv":"2306.17107","DOI":"10.48550/arXiv.2306.17107","CorpusId":259287523},"title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"},{"paperId":"3b6179c293df29e31d31cea46476f104ab6950f2","externalIds":{"DBLP":"conf/iclr/Peng00HHMYW24","ArXiv":"2306.14824","DOI":"10.48550/arXiv.2306.14824","CorpusId":259262263},"title":"Kosmos-2: Grounding Multimodal Large Language Models to the World"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","externalIds":{"ArXiv":"2306.13394","DBLP":"journals/corr/abs-2306-13394","DOI":"10.48550/arXiv.2306.13394","CorpusId":259243928},"title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"3efb81de24eb88017d6dbcf22cb4215084223fd8","externalIds":{"ArXiv":"2306.12925","DBLP":"journals/corr/abs-2306-12925","DOI":"10.48550/arXiv.2306.12925","CorpusId":259224345},"title":"AudioPaLM: A Large Language Model That Can Speak and Listen"},{"paperId":"a6d3794c23626060781da0f1ff2bcdf7457b6c43","externalIds":{"DBLP":"conf/nips/WangCPXKZXXDSTA23","ArXiv":"2306.11698","DOI":"10.48550/arXiv.2306.11698","CorpusId":259202782},"title":"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","externalIds":{"ArXiv":"2306.06687","DBLP":"conf/nips/YinWCSLLH0S0SO23","DOI":"10.48550/arXiv.2306.06687","CorpusId":259138958},"title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","externalIds":{"ArXiv":"2306.05425","DBLP":"journals/corr/abs-2306-05425","DOI":"10.48550/arXiv.2306.05425","CorpusId":259108295},"title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"9541cf136f442e992f10021c53081f33c73a2ed0","externalIds":{"ArXiv":"2306.03514","DBLP":"conf/cvpr/ZhangHMLLXQLLLG22","DOI":"10.1109/CVPRW63382.2024.00179","CorpusId":259089333},"title":"Recognize Anything: A Strong Image Tagging Model"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"ed8ac4ff13d32a291bbe74f3e5a138800bba47fd","externalIds":{"DBLP":"conf/cvpr/WangBDBPLAMSSW23","DOI":"10.1109/CVPR52729.2023.01838","CorpusId":260068316},"title":"Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","externalIds":{"DBLP":"journals/corr/abs-2306-00890","ArXiv":"2306.00890","DOI":"10.48550/arXiv.2306.00890","CorpusId":258999820},"title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"6fb5c0eff3696ef252aca9638e10176ecce7cecb","externalIds":{"ArXiv":"2305.17216","DBLP":"conf/nips/KohFS23","DOI":"10.48550/arXiv.2305.17216","CorpusId":258959284},"title":"Generating Images with Multimodal Language Models"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","externalIds":{"DBLP":"journals/corr/abs-2305-16355","ArXiv":"2305.16355","ACL":"2023.tllm-1.2","DOI":"10.48550/arXiv.2305.16355","CorpusId":258947721},"title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","externalIds":{"ArXiv":"2305.15021","DBLP":"journals/corr/abs-2305-15021","DOI":"10.48550/arXiv.2305.15021","CorpusId":258865718},"title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"2ad8183c72a90511383a32ccaeea313eb85f4085","externalIds":{"DBLP":"journals/corr/abs-2305-14167","ArXiv":"2305.14167","DOI":"10.48550/arXiv.2305.14167","CorpusId":258841764},"title":"DetGPT: Detect What You Need via Reasoning"},{"paperId":"9f411fda2ad5b141a3115f707bcf5ee865b3fb94","externalIds":{"DBLP":"conf/nips/TangYZ0B23","ArXiv":"2305.11846","DOI":"10.48550/arXiv.2305.11846","CorpusId":258822817},"title":"Any-to-Any Generation via Composable Diffusion"},{"paperId":"5cac6430bd379c9d2fe13137dfd6ae7721a2679f","externalIds":{"DBLP":"conf/emnlp/ZhangLZZWZQ23","ArXiv":"2305.11000","DOI":"10.48550/arXiv.2305.11000","CorpusId":258762683},"title":"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"},{"paperId":"ebf35cef5c249d90b40043fffa41f8802c27f132","externalIds":{"DBLP":"conf/acl/AkyurekAKCWT23","ArXiv":"2305.08844","ACL":"2023.acl-long.427","DOI":"10.48550/arXiv.2305.08844","CorpusId":258685337},"title":"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","externalIds":{"DBLP":"journals/corr/abs-2305-05665","ArXiv":"2305.05665","DOI":"10.1109/CVPR52729.2023.01457","CorpusId":258564264},"title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"81e7e82245c2f230eeb8aaaa1a2b2604c143754a","externalIds":{"ArXiv":"2305.04790","DBLP":"journals/corr/abs-2305-04790","DOI":"10.48550/arXiv.2305.04790","CorpusId":258557672},"title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"},{"paperId":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","externalIds":{"ArXiv":"2305.04160","DBLP":"journals/corr/abs-2305-04160","DOI":"10.48550/arXiv.2305.04160","CorpusId":258558106},"title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","externalIds":{"DBLP":"journals/pami/LiZCWPCYLL25","ArXiv":"2305.03726","DOI":"10.1109/TPAMI.2025.3571946","CorpusId":258547300,"PubMed":"40392642"},"title":"Otter: A Multi-Modal Model With In-Context Instruction Tuning"},{"paperId":"f9570989919338079088270a9cf1a7afc8db8093","externalIds":{"DBLP":"journals/corr/abs-2304-14108","ArXiv":"2304.14108","DOI":"10.48550/arXiv.2304.14108","CorpusId":258352812},"title":"DataComp: In search of the next generation of multimodal datasets"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","externalIds":{"DBLP":"journals/corr/abs-2304-14178","ArXiv":"2304.14178","DOI":"10.48550/arXiv.2304.14178","CorpusId":258352455},"title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"8bc617c9139648d7a92991d70c671230bac7b2e2","externalIds":{"DBLP":"conf/aaai/HuangLYSCYWHHLR24","ArXiv":"2304.12995","DOI":"10.48550/arXiv.2304.12995","CorpusId":258309430},"title":"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"df958800014d310b6df34ad83d771314d68fbb2d","externalIds":{"DBLP":"conf/nips/ZhuHAGDFYSW023","ArXiv":"2304.06939","DOI":"10.48550/arXiv.2304.06939","CorpusId":258170467},"title":"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"3dfed62c61f650eb114f0f0aa26b4e7d37b963a6","externalIds":{"DBLP":"journals/corr/abs-2303-17395","ArXiv":"2303.17395","DOI":"10.1109/TASLP.2024.3419446","CorpusId":257834090},"title":"WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","externalIds":{"ArXiv":"2303.11381","DBLP":"journals/corr/abs-2303-11381","DOI":"10.48550/arXiv.2303.11381","CorpusId":257637012},"title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","externalIds":{"DBLP":"conf/iccv/SurisMV23","ArXiv":"2303.08128","DOI":"10.1109/ICCV51070.2023.01092","CorpusId":257505358},"title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","externalIds":{"DBLP":"journals/corr/abs-2303-04671","ArXiv":"2303.04671","DOI":"10.48550/arXiv.2303.04671","CorpusId":257404891},"title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","externalIds":{"ArXiv":"2302.14045","DBLP":"conf/nips/Huang0WHSML0MPL23","DOI":"10.48550/arXiv.2302.14045","CorpusId":257219775},"title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"57eb41c7b5cbffb134cdcf67e455c9c852024cbd","externalIds":{"DBLP":"journals/corr/abs-2301-13003","ArXiv":"2301.13003","DOI":"10.48550/arXiv.2301.13003","CorpusId":256389599},"title":"Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation"},{"paperId":"fa0f3d8aa20e8987dbc7a516d5399cfa3dc97b1b","externalIds":{"ArXiv":"2301.12503","DBLP":"journals/corr/abs-2301-12503","DOI":"10.48550/arXiv.2301.12503","CorpusId":256390486},"title":"AudioLDM: Text-to-Audio Generation with Latent Diffusion Models"},{"paperId":"24bd614505be152cd41ced539c536764bb094c5c","externalIds":{"DBLP":"journals/tip/XuWZXMZ23","DOI":"10.1109/TIP.2023.3238648","CorpusId":256319085,"PubMed":"37022079"},"title":"RSSFormer: Foreground Saliency Enhancement for Remote Sensing Land-Cover Segmentation"},{"paperId":"e965e93e76a9e6c4e4863d145b5c007b540d575d","externalIds":{"ArXiv":"2212.12017","DBLP":"journals/corr/abs-2212-12017","CorpusId":255096269},"title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"},{"paperId":"6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","externalIds":{"ACL":"2023.acl-long.806","ArXiv":"2212.09689","DBLP":"conf/acl/HonovichSLS23","DOI":"10.48550/arXiv.2212.09689","CorpusId":254853659},"title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"},{"paperId":"89a1dbbfd4c96d90b769f5d3427bd970b082898e","externalIds":{"ArXiv":"2212.09058","DBLP":"conf/icml/ChenW00T0CYW23","DOI":"10.48550/arXiv.2212.09058","CorpusId":254853660},"title":"BEATs: Audio Pre-Training with Acoustic Tokenizers"},{"paperId":"16de2006e2960ba410772c6b6d460b83c0a5cc4b","externalIds":{"ArXiv":"2212.07143","DBLP":"journals/corr/abs-2212-07143","DOI":"10.1109/CVPR52729.2023.00276","CorpusId":254636568},"title":"Reproducible Scaling Laws for Contrastive Language-Image Learning"},{"paperId":"a02fbaf22237a1aedacb1320b6007cd70c1fe6ec","externalIds":{"DBLP":"journals/corr/abs-2212-04356","ArXiv":"2212.04356","CorpusId":252923993},"title":"Robust Speech Recognition via Large-Scale Weak Supervision"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","externalIds":{"DBLP":"conf/cvpr/FangWXSWW0WC23","ArXiv":"2211.07636","DOI":"10.1109/CVPR52729.2023.01855","CorpusId":253510587},"title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"e9bc29cfcfbea4d137652d10715a9c9389349a90","externalIds":{"DBLP":"journals/corr/abs-2211-06687","ArXiv":"2211.06687","DOI":"10.1109/ICASSP49357.2023.10095969","CorpusId":253510826},"title":"Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","externalIds":{"ArXiv":"2209.06794","DBLP":"journals/corr/abs-2209-06794","DOI":"10.48550/arXiv.2209.06794","CorpusId":252222320},"title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","externalIds":{"ArXiv":"2208.10442","DBLP":"journals/corr/abs-2208-10442","DOI":"10.48550/arXiv.2208.10442","CorpusId":251719655},"title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"47a67e76ed84260ff19f7a948d764005d1edf1c9","externalIds":{"DBLP":"journals/corr/abs-2206-01718","ArXiv":"2206.01718","DOI":"10.48550/arXiv.2206.01718","CorpusId":249375629},"title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"},{"paperId":"b21670e8061a06ab97e7d6052c9345a326e84ff8","externalIds":{"ArXiv":"2205.05131","DBLP":"conf/iclr/Tay00GW0CBSZZHM23","CorpusId":252780443},"title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"5ae998e96e089b958dd2a384da6f8e7a7ee66b55","externalIds":{"DBLP":"conf/naacl/FuCLL22","ArXiv":"2205.00305","DOI":"10.48550/arXiv.2205.00305","CorpusId":248496449},"title":"AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks"},{"paperId":"66ee488cf3dad5bb83804124367460edddd3c271","externalIds":{"DBLP":"conf/ijcai/LongCHY22","ArXiv":"2204.07356","DOI":"10.48550/arXiv.2204.07356","CorpusId":248218612},"title":"Vision-and-Language Pretrained Models: A Survey"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"9dc481ec44178e797466bbad968071917842156b","externalIds":{"DBLP":"journals/corr/abs-2203-03605","ArXiv":"2203.03605","DOI":"10.48550/arXiv.2203.03605","CorpusId":247292561},"title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"0ad84c4bf7499df6945fc51b24ae2ac779f218ec","externalIds":{"DBLP":"journals/corr/abs-2202-10401","ArXiv":"2202.10401","DOI":"10.1109/CVPR52688.2022.01522","CorpusId":247011309},"title":"Vision-Language Pre-Training with Triple Contrastive Learning"},{"paperId":"24ed74ed29c057cba8b52fff4edd2c0d7f408716","externalIds":{"ArXiv":"2202.09061","DBLP":"journals/ijautcomp/ChenZHCSXX23","DOI":"10.1007/s11633-022-1369-5","CorpusId":246996617},"title":"VLP: A Survey on Vision-language Pre-training"},{"paperId":"fe0e647ac5bbe9b127caddfcb52b9f723d6f158c","externalIds":{"DBLP":"conf/nips/GuMLHMLYHZJXX22","ArXiv":"2202.06767","CorpusId":249847758},"title":"Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","externalIds":{"DBLP":"conf/icml/WangYMLBLMZZY22","ArXiv":"2202.03052","CorpusId":246634906},"title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"746bfbdaa240ebb8b55024d3da7c21a9ef97ce92","externalIds":{"DBLP":"journals/corr/abs-2201-12806","ArXiv":"2201.12806","DOI":"10.1109/icassp43922.2022.9747101","CorpusId":246431261},"title":"Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"400d619cbabeb669115bb7281a889ab869829ef5","externalIds":{"DBLP":"conf/cvpr/ZellersLLYZSKHF22","ArXiv":"2201.02639","DOI":"10.1109/CVPR52688.2022.01589","CorpusId":245837609},"title":"MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound"},{"paperId":"3f43b4239c6955b4c6647c0801fbbbcdea91a320","externalIds":{"DBLP":"conf/cvpr/BitenLXAM22","ArXiv":"2112.12494","DOI":"10.1109/CVPR52688.2022.01605","CorpusId":245425054},"title":"LaTr: Layout-Aware Transformer for Scene-Text VQA"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"32308a39204d0f414a2bc63882fcdee536c7d1a3","externalIds":{"DBLP":"journals/corr/abs-2112-01194","ArXiv":"2112.01194","DOI":"10.1609/aaai.v37i3.25414","CorpusId":244798616},"title":"Video-Text Pre-training with Learned Regions"},{"paperId":"dd2819016c6bf244c39b3e6707b60389bbdbcd21","externalIds":{"DBLP":"conf/cvpr/YuTR00L22","ArXiv":"2111.14819","DOI":"10.1109/CVPR52688.2022.01871","CorpusId":244714512},"title":"Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling"},{"paperId":"ec8afc75ec219f2a5f9ed9d7c9dde0720f69b5a2","externalIds":{"DBLP":"conf/icml/ZengZL22","ArXiv":"2111.08276","CorpusId":244129883},"title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"9bcf3b43f2323a194036cc52c6878a9b1dc7e058","externalIds":{"DBLP":"journals/corr/abs-2110-13214","ArXiv":"2110.13214","CorpusId":239885946},"title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"},{"paperId":"f3a332ff1b73acda482e5d83696b2c701f487819","externalIds":{"DBLP":"journals/corr/abs-2110-07602","ArXiv":"2110.07602","CorpusId":238857040},"title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"},{"paperId":"43a87867fe6bf4eb920f97fc753be4b727308923","externalIds":{"DBLP":"journals/corr/abs-2110-04366","ArXiv":"2110.04366","CorpusId":238583580},"title":"Towards a Unified View of Parameter-Efficient Transfer Learning"},{"paperId":"dd4dd3ed1a95beb6e6712ea356a49d1ab818f616","externalIds":{"DBLP":"journals/corr/abs-2109-09920","ArXiv":"2109.09920","DOI":"10.1016/j.aiopen.2022.01.001","CorpusId":237581134},"title":"Survey: Transformer based Video-Language Pre-training"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","externalIds":{"DBLP":"journals/corr/abs-2107-07651","ArXiv":"2107.07651","CorpusId":236034189},"title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"c401e01c9ee32fab7d02670d1c754f44fc1ff99e","externalIds":{"DBLP":"journals/corr/abs-2106-11097","ArXiv":"2106.11097","CorpusId":235490558},"title":"CLIP2Video: Mastering Video-Text Retrieval via Image CLIP"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"4fffa5245d3972077c83614c2a08a47cb578631e","externalIds":{"ArXiv":"2106.07447","DBLP":"journals/corr/abs-2106-07447","DOI":"10.1109/taslp.2021.3122291","CorpusId":235421619},"title":"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"},{"paperId":"656ed155c2d345c19d9bff4b50f2ae00db8407cc","externalIds":{"ArXiv":"2106.04647","DBLP":"conf/nips/MahabadiHR21","CorpusId":235356070},"title":"Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"},{"paperId":"f0524b3005720bcff886bcb0227f7f0dd924ff07","externalIds":{"DBLP":"conf/nips/AkbariYQCCCG21","ArXiv":"2104.11178","CorpusId":233346984},"title":"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"c16835c8e535ebd9c10a550ca9455fe384a14449","externalIds":{"DBLP":"conf/icml/BrockDSS21","ArXiv":"2102.06171","CorpusId":231879922},"title":"High-Performance Large-Scale Image Recognition Without Normalization"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","externalIds":{"ArXiv":"2102.05918","DBLP":"conf/icml/JiaYXCPPLSLD21","CorpusId":231879586},"title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"b40bfcf339de3f0dba08fabb2b58b9368ff4c51a","externalIds":{"DBLP":"conf/wacv/MathewKJ21","ArXiv":"2007.00398","MAG":"3040138106","DOI":"10.1109/WACV48630.2021.00225","CorpusId":220280200},"title":"DocVQA: A Dataset for VQA on Document Images"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"3f6570fd55dc5855f93a56150e6d99c7944a1c1e","externalIds":{"DBLP":"conf/nips/KielaFMGSRT20","ArXiv":"2005.04790","MAG":"3023989664","CorpusId":218581273},"title":"The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes"},{"paperId":"b5ef0f91663f0cbd6910dec9a890c138f7ec10e0","externalIds":{"DBLP":"journals/corr/abs-2004-06165","MAG":"3091588028","ArXiv":"2004.06165","DOI":"10.1007/978-3-030-58577-8_8","CorpusId":215754208},"title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"33eadd4e666a894306a22ba0839c5e0cef77280e","externalIds":{"ArXiv":"2003.12462","MAG":"3106859150","DBLP":"journals/corr/abs-2003-12462","DOI":"10.1007/978-3-030-58536-5_44","CorpusId":214693197},"title":"TextCaps: a Dataset for Image Captioning with Reading Comprehension"},{"paperId":"4287533d12143cdbc4948b60ecece28b6c750f17","externalIds":{"MAG":"3042266831","DBLP":"conf/eccv/ZhangSZGM20","DOI":"10.1007/978-3-030-58580-8_41","CorpusId":220647041},"title":"Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1097cf8cf5961589ff693b069002e7181e24e631","externalIds":{"DBLP":"conf/icdar/0001SSC19","MAG":"3004268082","DOI":"10.1109/ICDAR.2019.00156","CorpusId":209413409},"title":"OCR-VQA: Visual Question Answering by Reading Text in Images"},{"paperId":"56e3ce0ff4cbd05e404214d19ae264fe6c457a16","externalIds":{"DBLP":"journals/corr/abs-1905-11235","ArXiv":"1905.11235","MAG":"3016167541","DOI":"10.1109/ICASSP40776.2020.9054250","CorpusId":166228043},"title":"CIF: Continuous Integrate-And-Fire for End-To-End Speech Recognition"},{"paperId":"af1f7739283bdbd2b7a94903041f6d6afd991907","externalIds":{"MAG":"2936135081","DBLP":"journals/corr/abs-1904-08920","ArXiv":"1904.08920","DOI":"10.1109/CVPR.2019.00851","CorpusId":85553602},"title":"Towards VQA Models That Can Read"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c","externalIds":{"DBLP":"conf/cvpr/Gurari0SGLGLB18","MAG":"2788643321","ArXiv":"1802.08218","DOI":"10.1109/CVPR.2018.00380","CorpusId":3831582},"title":"VizWiz Grand Challenge: Answering Visual Questions from Blind People"},{"paperId":"7289a240c9425bc7cad87b3b835e5f0cac22f488","externalIds":{"DBLP":"conf/cvpr/KaflePCK18","MAG":"2963420691","ArXiv":"1801.08163","DOI":"10.1109/CVPR.2018.00592","CorpusId":4445015},"title":"DVQA: Understanding Data Visualizations via Question Answering"},{"paperId":"3ff40f0760bd8d3c46d72147b0f5b0d4aee2a24f","externalIds":{"MAG":"2768477045","DBLP":"journals/corr/abs-1711-06475","ArXiv":"1711.06475","CorpusId":34240712},"title":"AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"d89ee98810039d2061ed42ee8026da49c503d16b","externalIds":{"ArXiv":"1705.08045","DBLP":"conf/nips/RebuffiBV17","MAG":"2616957565","CorpusId":215826266},"title":"Learning multiple visual domains with residual adapters"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"29efbe391950ae438c63d86ad5c82b2942efb0b4","externalIds":{"DBLP":"conf/eccv/YuPYBB16","MAG":"2489434015","ArXiv":"1608.00272","DOI":"10.1007/978-3-319-46475-6_5","CorpusId":1688357},"title":"Modeling Context in Referring Expressions"},{"paperId":"b8e2e9f3ba008e28257195ec69a00e07f260131d","externalIds":{"DBLP":"conf/cvpr/XuMYR16","MAG":"2425121537","DOI":"10.1109/CVPR.2016.571","CorpusId":206594535},"title":"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"def584565d05d6a8ba94de6621adab9e301d375d","externalIds":{"MAG":"2962749469","DBLP":"journals/corr/ZhuGBF15","ArXiv":"1511.03416","DOI":"10.1109/CVPR.2016.540","CorpusId":5714907},"title":"Visual7W: Grounded Question Answering in Images"},{"paperId":"0a28efacb92d16e6e0dd4d87b5aca91b28be8853","externalIds":{"DBLP":"conf/cvpr/HeilbronEGN15","MAG":"1927052826","DOI":"10.1109/CVPR.2015.7298698","CorpusId":1710722},"title":"ActivityNet: A large-scale video benchmark for human activity understanding"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","externalIds":{"ACL":"D14-1086","DBLP":"conf/emnlp/KazemzadehOMB14","MAG":"2251512949","DOI":"10.3115/v1/D14-1086","CorpusId":6308361},"title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"44040913380206991b1991daf1192942e038fe31","externalIds":{"ACL":"Q14-1006","DBLP":"journals/tacl/YoungLHH14","MAG":"2185175083","DOI":"10.1162/tacl_a_00166","CorpusId":3104920},"title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"0407b605b8f55db72e2545586bfe8e946b691b70","externalIds":{"MAG":"2113839990","DBLP":"journals/corr/GoodfellowMDCB13","ArXiv":"1312.6211","CorpusId":12730344},"title":"An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","externalIds":{"DBLP":"conf/nips/OrdonezKB11","MAG":"2109586012","CorpusId":14579301},"title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"008bbb907d134d89da41c97cfb64da3cf9a18258","externalIds":{"DOI":"10.1126/science.263.5154.1780","CorpusId":239504029,"PubMed":"17795387"},"title":"Visual recognition."},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","externalIds":{"DBLP":"journals/corr/abs-2308-12966","DOI":"10.48550/arXiv.2308.12966","CorpusId":263875678},"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"848e690a62c327e1210532d58a6b914097cac763","externalIds":{"DBLP":"journals/corr/abs-2305-07895","DOI":"10.48550/arXiv.2305.07895","CorpusId":258685422},"title":"On the Hidden Mystery of OCR in Large Multimodal Models"},{"paperId":"ff9a7018f392d9e639a5fc8f59765967f1d28d7f","externalIds":{"DBLP":"journals/corr/abs-2311-17647","DOI":"10.48550/arXiv.2311.17647","CorpusId":271924643},"title":"VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following"},{"paperId":"ec936b808e0fab9281c050ad4010cddec92c8cbe","externalIds":{"ACL":"2022.acl-short.8","DBLP":"conf/acl/LiuJFTDY022","DOI":"10.18653/v1/2022.acl-short.8","CorpusId":248780177},"title":"P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","externalIds":{"ArXiv":"2103.14030","DBLP":"conf/iccv/LiuL00W0LG21","DOI":"10.1109/ICCV48922.2021.00986","CorpusId":232352874},"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c213af6582c0d518a6e8e14217611c733eeb1ef1","externalIds":{"MAG":"140726337","DOI":"10.1016/S0079-7421(08)60536-8","CorpusId":61019113},"title":"Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"}]}