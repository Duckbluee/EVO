{"references":[{"paperId":"d8355d1bc02ca6b11125027a5d85d0b32fa30f66","externalIds":{"ArXiv":"2310.08256","DBLP":"conf/emnlp/KangC23","DOI":"10.48550/arXiv.2310.08256","CorpusId":263908902},"title":"Impact of Co-occurrence on Factual Knowledge of Large Language Models"},{"paperId":"d59523889679aee15992c4bf6e52b134186d07d3","externalIds":{"ArXiv":"2310.06824","DBLP":"journals/corr/abs-2310-06824","DOI":"10.48550/arXiv.2310.06824","CorpusId":263831277},"title":"The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets"},{"paperId":"70017c9096cca317942156e3f91f8141da42eef1","externalIds":{"ArXiv":"2310.05797","CorpusId":263829193},"title":"In-Context Explainers: Harnessing LLMs for Explaining Black Box Models"},{"paperId":"740c783ac07039cf30b6d8a8f95e775b3297c79e","externalIds":{"DBLP":"conf/iclr/GurneeT24","ArXiv":"2310.02207","DOI":"10.48550/arXiv.2310.02207","CorpusId":263608756},"title":"Language Models Represent Space and Time"},{"paperId":"58fdf550600fc3873729d466601c5d08a51ba8a0","externalIds":{"DBLP":"journals/corr/abs-2310-01405","ArXiv":"2310.01405","DOI":"10.48550/arXiv.2310.01405","CorpusId":263605618},"title":"Representation Engineering: A Top-Down Approach to AI Transparency"},{"paperId":"04b880e1e32f37b3796d41a47d013fa07095ae32","externalIds":{"DBLP":"conf/naacl/WuYCPWLY24","ArXiv":"2310.00492","ACL":"2024.naacl-long.130","DOI":"10.48550/arXiv.2310.00492","CorpusId":263334329},"title":"From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning"},{"paperId":"8eafec7014d08043517834b5a2ed26384f188873","externalIds":{"ArXiv":"2309.12288","DBLP":"journals/corr/abs-2309-12288","CorpusId":262083829},"title":"The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\""},{"paperId":"c413a339d7784574ed43debea494ef405ee09d81","externalIds":{"DBLP":"journals/corr/abs-2309-07311","ArXiv":"2309.07311","DOI":"10.48550/arXiv.2309.07311","CorpusId":261822542},"title":"Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs"},{"paperId":"6414281e2efb42601aa7bb5eac8759a774896968","externalIds":{"DBLP":"journals/corr/abs-2309-05936","ACL":"2023.acl-long.173","ArXiv":"2309.05936","DOI":"10.18653/v1/2023.acl-long.173","CorpusId":259370557},"title":"Do PLMs Know and Understand Ontological Knowledge?"},{"paperId":"d00735241af700d21762d2f3ca00d920241a15a4","externalIds":{"DBLP":"journals/corr/abs-2309-01219","ArXiv":"2309.01219","DOI":"10.1162/coli.a.16","CorpusId":261530162},"title":"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"},{"paperId":"03bf28df6e282a7e36e1686edeb9c624e6ffb13b","externalIds":{"DBLP":"journals/corr/abs-2308-10149","ArXiv":"2308.10149","DOI":"10.48550/arXiv.2308.10149","CorpusId":261049466},"title":"A Survey on Fairness in Large Language Models"},{"paperId":"04a96b66705858c988edfcb73191c1da7d54abfb","externalIds":{"ArXiv":"2308.03296","DBLP":"journals/corr/abs-2308-03296","DOI":"10.48550/arXiv.2308.03296","CorpusId":260682872},"title":"Studying Large Language Model Generalization with Influence Functions"},{"paperId":"a37d5620210276e47cf0c9dd2898c2a82c9d0422","externalIds":{"DBLP":"journals/corr/abs-2308-03958","ArXiv":"2308.03958","DOI":"10.48550/arXiv.2308.03958","CorpusId":260704246},"title":"Simple synthetic data reduces sycophancy in large language models"},{"paperId":"c8fd279848e9c50eec85b852f22f7fffb08f97fc","externalIds":{"DBLP":"journals/corr/abs-2307-15771","ArXiv":"2307.15771","DOI":"10.48550/arXiv.2307.15771","CorpusId":260334719},"title":"The Hydra Effect: Emergent Self-repair in Language Model Computations"},{"paperId":"71d68782c3da41b77866c2fd0cb65726f60b3af1","externalIds":{"DBLP":"journals/corr/abs-2307-13339","ArXiv":"2307.13339","DOI":"10.48550/arXiv.2307.13339","CorpusId":260155139},"title":"Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions"},{"paperId":"84b77180228051040286423cec82b62c323a8fda","externalIds":{"ArXiv":"2307.11019","DBLP":"journals/corr/abs-2307-11019","DOI":"10.48550/arXiv.2307.11019","CorpusId":259991467},"title":"Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"77f02ff24909896856fec410968aef7999c29440","externalIds":{"DBLP":"journals/corr/abs-2307-09458","ArXiv":"2307.09458","DOI":"10.48550/arXiv.2307.09458","CorpusId":259950939},"title":"Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla"},{"paperId":"69f2ba0f33a54e01de32c616b64e85d5d7194067","externalIds":{"DBLP":"journals/corr/abs-2307-08678","ArXiv":"2307.08678","DOI":"10.48550/arXiv.2307.08678","CorpusId":259937644},"title":"Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations"},{"paperId":"8154fb1d828cdc390dc1fa442d84034948679c47","externalIds":{"DBLP":"journals/corr/abs-2307-11768","ArXiv":"2307.11768","DOI":"10.48550/arXiv.2307.11768","CorpusId":259980634},"title":"Question Decomposition Improves the Faithfulness of Model-Generated Reasoning"},{"paperId":"827afa7dd36e4afbb1a49c735bfbb2c69749756e","externalIds":{"DBLP":"journals/corr/abs-2307-13702","ArXiv":"2307.13702","DOI":"10.48550/arXiv.2307.13702","CorpusId":259953372},"title":"Measuring Faithfulness in Chain-of-Thought Reasoning"},{"paperId":"9806cac2d36feda043dcdfe0f4de2608127da27c","externalIds":{"ArXiv":"2307.05052","DBLP":"journals/corr/abs-2307-05052","DOI":"10.48550/arXiv.2307.05052","CorpusId":259766348},"title":"Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps"},{"paperId":"1733eb7792f7a43dd21f51f4d1017a1bffd217b5","externalIds":{"DBLP":"journals/tacl/LiuLHPBPL24","ArXiv":"2307.03172","ACL":"2024.tacl-1.9","DOI":"10.1162/tacl_a_00638","CorpusId":259360665},"title":"Lost in the Middle: How Language Models Use Long Contexts"},{"paperId":"f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0","externalIds":{"DBLP":"journals/corr/abs-2307-01981","ArXiv":"2307.01981","DOI":"10.48550/arXiv.2307.01981","CorpusId":259342254},"title":"A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis"},{"paperId":"8724579d3f126e753a0451d98ff57b165f722e72","externalIds":{"ArXiv":"2306.15447","DBLP":"journals/corr/abs-2306-15447","DOI":"10.48550/arXiv.2306.15447","CorpusId":259262181},"title":"Are aligned neural networks adversarially aligned?"},{"paperId":"8f7297454d7f44365b9bcda5ebb9439a43daf5e6","externalIds":{"DBLP":"journals/corr/abs-2306-13063","ArXiv":"2306.13063","DOI":"10.48550/arXiv.2306.13063","CorpusId":259224389},"title":"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs"},{"paperId":"0244aeb7c6927e2fb0c2e668687e160a00737dbe","externalIds":{"ArXiv":"2306.02707","DBLP":"journals/corr/abs-2306-02707","DOI":"10.48550/arXiv.2306.02707","CorpusId":259075316},"title":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4"},{"paperId":"8c5a7de7452b61cb81d6f7124ad021997e0a79c1","externalIds":{"ArXiv":"2306.01150","DBLP":"journals/corr/abs-2306-01150","ACL":"2023.acl-long.172","DOI":"10.48550/arXiv.2306.01150","CorpusId":259063796},"title":"Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning"},{"paperId":"a66ade2f872e726e4ea58278058c4b6df4cbc2be","externalIds":{"DBLP":"journals/corr/abs-2305-18029","ACL":"2023.acl-short.25","ArXiv":"2305.18029","DOI":"10.48550/arXiv.2305.18029","CorpusId":258960511},"title":"Faithfulness Tests for Natural Language Explanations"},{"paperId":"301c09d74e46436bc75ea5f56d60acc549831961","externalIds":{"ACL":"2023.acl-long.842","ArXiv":"2305.17075","DBLP":"journals/corr/abs-2305-17075","DOI":"10.48550/arXiv.2305.17075","CorpusId":258947054},"title":"CREST: A Joint Framework for Rationalization and Counterfactual Text Generation"},{"paperId":"5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35","externalIds":{"ArXiv":"2305.15717","DBLP":"journals/corr/abs-2305-15717","DOI":"10.48550/arXiv.2305.15717","CorpusId":258887629},"title":"The False Promise of Imitating Proprietary LLMs"},{"paperId":"771fd139a18595e21449b981f541c93a4e833399","externalIds":{"DBLP":"journals/corr/abs-2305-15853","ArXiv":"2305.15853","DOI":"10.48550/arXiv.2305.15853","CorpusId":258888047},"title":"Sequential Integrated Gradients: a simple but effective method for explaining language models"},{"paperId":"2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4","externalIds":{"DBLP":"journals/corr/abs-2305-14552","ArXiv":"2305.14552","DOI":"10.48550/arXiv.2305.14552","CorpusId":258865517},"title":"Sources of Hallucination by Large Language Models on Inference Tasks"},{"paperId":"e5754bb65a648f319a02d47c356df0db1e936b7f","externalIds":{"DBLP":"journals/corr/abs-2305-11426","ArXiv":"2305.11426","DOI":"10.48550/arXiv.2305.11426","CorpusId":258822903},"title":"Post Hoc Explanations of Language Models Can Improve Language Models"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"7e245b8ab88171017c8bf7925afeeb560d08272e","externalIds":{"DBLP":"journals/corr/abs-2305-09863","ArXiv":"2305.09863","DOI":"10.48550/arXiv.2305.09863","CorpusId":258741006},"title":"Explaining black box text modules in natural language with language models"},{"paperId":"7dc928f41e15f65f1267bd87b0fcfcc7e715cb56","externalIds":{"DBLP":"conf/nips/TurpinMPB23","ArXiv":"2305.04388","DOI":"10.48550/arXiv.2305.04388","CorpusId":258556812},"title":"Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting"},{"paperId":"61326b7f5ff4f574ce470217b4d6502c629191dc","externalIds":{"DBLP":"journals/corr/abs-2305-03210","ArXiv":"2305.03210","DOI":"10.1109/TVCG.2023.3327163","CorpusId":258546743,"PubMed":"37883259"},"title":"AttentionViz: A Global View of Transformer Attention"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","externalIds":{"DBLP":"journals/corr/abs-2304-13712","ArXiv":"2304.13712","DOI":"10.1145/3649506","CorpusId":258331833},"title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"69bef4ab1018cc956a77c3ccdcaa57b124ab9fcc","externalIds":{"DBLP":"conf/emnlp/PrasadSZB23","ArXiv":"2304.10703","DOI":"10.48550/arXiv.2304.10703","CorpusId":258291731},"title":"ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness"},{"paperId":"8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c","externalIds":{"ArXiv":"2303.12712","DBLP":"journals/corr/abs-2303-12712","CorpusId":257663729},"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"154493f69d7db3d49da0e51df0192c6ad5f1724a","externalIds":{"DBLP":"journals/corr/abs-2303-03846","ArXiv":"2303.03846","DOI":"10.48550/arXiv.2303.03846","CorpusId":257378479},"title":"Larger language models do in-context learning differently"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"07157c3cb9010acb84237e121dacc7ecba30f04b","externalIds":{"DBLP":"conf/emnlp/YeD23","ArXiv":"2302.04813","DOI":"10.18653/v1/2023.emnlp-main.41","CorpusId":258865150},"title":"Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting"},{"paperId":"35922cd0d6b17e45320917338e9f98cb5c1a4f6f","externalIds":{"ArXiv":"2212.10001","DBLP":"conf/acl/WangM0S0Z023","ACL":"2023.acl-long.153","DOI":"10.48550/arXiv.2212.10001","CorpusId":254877569},"title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters"},{"paperId":"34bc28087e1d6f047e2736791f79d769293f447c","externalIds":{"ArXiv":"2212.09095","DBLP":"conf/acl/BansalGDBKR23","ACL":"2023.acl-long.660","DOI":"10.48550/arXiv.2212.09095","CorpusId":254853961},"title":"Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale"},{"paperId":"391246ce9c59d61c94cca3f8bef56c95542a4708","externalIds":{"ArXiv":"2212.07919","DBLP":"journals/corr/abs-2212-07919","DOI":"10.48550/arXiv.2212.07919","CorpusId":254685985},"title":"ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning"},{"paperId":"478ec7a8001d46cde90395c4a9d9ffdec59d5ce3","externalIds":{"ArXiv":"2211.07830","DBLP":"conf/acl/BlevinsGZ23","ACL":"2023.acl-long.367","DOI":"10.48550/arXiv.2211.07830","CorpusId":253523544},"title":"Prompting Language Models for Linguistic Structure"},{"paperId":"75f7e9e2b59fb640ef9d1dff94097175daf46c4d","externalIds":{"ArXiv":"2211.08411","DBLP":"journals/corr/abs-2211-08411","CorpusId":253522998},"title":"Large Language Models Struggle to Learn Long-Tail Knowledge"},{"paperId":"d10f3857f69edf74565ff786afd5d8632849666a","externalIds":{"ArXiv":"2210.16978","ACL":"2023.acl-demo.25","DBLP":"journals/corr/abs-2210-16978","DOI":"10.48550/arXiv.2210.16978","CorpusId":253237010},"title":"XMD: An End-to-End Framework for Interactive Explanation-Based Debugging of NLP Models"},{"paperId":"e1d66f654fa056660988e1f40f06e97b792b6a59","externalIds":{"ACL":"2023.acl-long.112","DBLP":"journals/corr/abs-2210-04982","ArXiv":"2210.04982","DOI":"10.48550/arXiv.2210.04982","CorpusId":252816010},"title":"REV: Information-Theoretic Evaluation of Free-Text Rationales"},{"paperId":"4988b3d378b79eb8669112620baf1ff4e3e536fd","externalIds":{"ArXiv":"2209.07686","DBLP":"journals/corr/abs-2209-07686","DOI":"10.48550/arXiv.2209.07686","CorpusId":252355328},"title":"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango"},{"paperId":"475c3014a68d545f1d2319f94fd3ab99fc3f6bec","externalIds":{"ArXiv":"2208.11857","DBLP":"journals/corr/abs-2208-11857","DOI":"10.1145/3596490","CorpusId":251800110},"title":"Shortcut Learning of Large Language Models in Natural Language Understanding"},{"paperId":"0a9df881784009cbb8efcd037d82ae222440aade","externalIds":{"ArXiv":"2207.13948","DBLP":"journals/corr/abs-2207-13948","DOI":"10.48550/arXiv.2207.13948","CorpusId":251135127},"title":"An Interpretability Evaluation Benchmark for Pre-trained Language Models"},{"paperId":"c935ad16b132f526b289e16e84d73c5821bfc100","externalIds":{"ArXiv":"2207.07605","DBLP":"journals/corr/abs-2207-07605","DOI":"10.1038/s42256-023-00657-x","CorpusId":250607514},"title":"Algorithms to estimate Shapley value feature attributions"},{"paperId":"91496a0a35d541c32324d23a06990be089f6c830","externalIds":{"ArXiv":"2207.01736","ACL":"2022.naacl-main.84","DBLP":"conf/naacl/LiCS22","DOI":"10.48550/arXiv.2207.01736","CorpusId":250279831},"title":"Probing via Prompting"},{"paperId":"8545e249ab7a49f4a5abcfade395b90ffadb687a","externalIds":{"ArXiv":"2205.15480","DBLP":"conf/iclr/YuksekgonulW023","DOI":"10.48550/arXiv.2205.15480","CorpusId":249209990},"title":"Post-hoc Concept Bottleneck Models"},{"paperId":"60c7c46a6adccc1e8965c0c8dff40d00f573ddc2","externalIds":{"DBLP":"conf/emnlp/JoshiCLNSF022","ArXiv":"2205.12542","DOI":"10.18653/v1/2022.findings-emnlp.242","CorpusId":256631078},"title":"ER-Test: Evaluating Explanation Regularization Methods for Language Models"},{"paperId":"aa4d9972af3264d032dbee58501ed4ac49477103","externalIds":{"ArXiv":"2205.10487","DBLP":"journals/corr/abs-2205-10487","DOI":"10.48550/arXiv.2205.10487","CorpusId":248986979},"title":"Scaling Laws and Interpretability of Learning from Repeated Data"},{"paperId":"9ffefdf1fcd780cb71450b0a7a29247c66aa87be","externalIds":{"DBLP":"conf/nips/YeD22","ArXiv":"2205.03401","CorpusId":252873674},"title":"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning"},{"paperId":"a9ce00dac14e47a68b4eefcf86b3ea6a64374a76","externalIds":{"ArXiv":"2205.01287","DBLP":"journals/corr/abs-2205-01287","DOI":"10.48550/arXiv.2205.01287","CorpusId":248453465},"title":"SemAttack: Natural Textual Attacks via Different Semantic Spaces"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"e9f28c98a00766e484810598886cf48b0de66cfa","externalIds":{"DBLP":"journals/corr/abs-2204-07931","ACL":"2022.naacl-main.387","ArXiv":"2204.07931","DOI":"10.48550/arXiv.2204.07931","CorpusId":248227301},"title":"On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?"},{"paperId":"1f62c1f3202bc28d8c4e38c4a2a71051e9c86364","externalIds":{"DBLP":"journals/corr/abs-2204-05514","ArXiv":"2204.05514","ACL":"2022.acl-long.345","DOI":"10.48550/arXiv.2204.05514","CorpusId":248118978},"title":"A Comparative Study of Faithfulness Metrics for Model Interpretability Methods"},{"paperId":"341bdbcfc3febef7691a97c216ad394653211095","externalIds":{"ArXiv":"2204.02329","DBLP":"conf/emnlp/LampinenDCMTCMW22","DOI":"10.48550/arXiv.2204.02329","CorpusId":247957917},"title":"Can language models learn from explanations in context?"},{"paperId":"cf36236015c9f93f15bfafbf282f69e08bdc9c16","externalIds":{"DBLP":"conf/emnlp/GevaCWG22","ArXiv":"2203.14680","ACL":"2022.emnlp-main.3","DOI":"10.48550/arXiv.2203.14680","CorpusId":247762385},"title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"},{"paperId":"1afc53f501e0801a7385084965aed84d28d35cf4","externalIds":{"DBLP":"journals/corr/abs-2203-12709","ArXiv":"2203.12709","DOI":"10.48550/arXiv.2203.12709","CorpusId":247627697},"title":"Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation"},{"paperId":"b0ceba96db8b7b71beb2f0906568ffc143fddbf9","externalIds":{"ArXiv":"2202.11912","DBLP":"journals/corr/abs-2202-11912","CorpusId":247084007},"title":"A Rigorous Study of Integrated Gradients Method and Extensions to Internal Neuron Attributions"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","externalIds":{"DBLP":"conf/nips/MengBAB22","ArXiv":"2202.05262","CorpusId":255825985},"title":"Locating and Editing Factual Associations in GPT"},{"paperId":"be5192f6ff11ee4dd0091630d20768cad99c6e79","externalIds":{"ArXiv":"2202.00734","DBLP":"conf/icml/DasguptaFM22","CorpusId":246473190},"title":"Framework for Evaluating Faithfulness of Local Explanations"},{"paperId":"5fdb05e17fd503b8dbdbadc338e0a00829929dcc","externalIds":{"DBLP":"conf/icml/LiuLGKL022","ArXiv":"2201.12114","CorpusId":246411233},"title":"Rethinking Attention-Model Explainability through Faithfulness Violation Test"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"61ae335f89b2ecc9736fba81b074660882c6347c","externalIds":{"DBLP":"journals/corr/abs-2112-08802","ArXiv":"2112.08802","ACL":"2022.bigscience-1.5","DOI":"10.18653/v1/2022.bigscience-1.5","CorpusId":245218726},"title":"UNIREX: A Unified Learning Framework for Language Model Rationale Extraction"},{"paperId":"0bc8eae192dbd489f8857c0f6c03cc491f72d396","externalIds":{"DBLP":"journals/corr/abs-2112-06204","ArXiv":"2112.06204","DOI":"10.18653/v1/2022.findings-emnlp.255","CorpusId":245123817},"title":"Few-Shot Out-of-Domain Transfer Learning of Natural Language Explanations"},{"paperId":"fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf","externalIds":{"ArXiv":"2112.04359","DBLP":"journals/corr/abs-2112-04359","CorpusId":244954639},"title":"Ethical and social risks of harm from Language Models"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","externalIds":{"DBLP":"journals/corr/abs-2112-00114","ArXiv":"2112.00114","CorpusId":244773644},"title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"1818e24137383ece9b2f379a63674b2632e16d04","externalIds":{"ArXiv":"2111.07367","DBLP":"conf/emnlp/BastingsEZSF22","ACL":"2022.emnlp-main.64","DOI":"10.18653/v1/2022.emnlp-main.64","CorpusId":244117341},"title":"“Will You Find These Shortcuts?” A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification"},{"paperId":"90f7b61762e4454b9cba3afbb10c5f07465cff85","externalIds":{"DBLP":"conf/cikm/BarkanHCKMAK21","ArXiv":"2204.11073","DOI":"10.1145/3459637.3482126","CorpusId":240230832},"title":"Grad-SAM: Explaining Transformers via Gradient Self-Attention Maps"},{"paperId":"aaf3ebaf12baeb366ce6ff32aa36d608a7eab583","externalIds":{"DBLP":"conf/iclr/AntvergB22","ArXiv":"2110.07483","CorpusId":238856948},"title":"On the Pitfalls of Analyzing Individual Neurons in Language Models"},{"paperId":"a58d790d1ee6e08786cd44e75c3544c18ede4f63","externalIds":{"DBLP":"conf/blackboxnlp/ApidianakiS21","ACL":"2021.blackboxnlp-1.7","ArXiv":"2110.06376","DOI":"10.18653/v1/2021.blackboxnlp-1.7","CorpusId":238743942},"title":"ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for Nouns’ Semantic Properties and their Prototypicality"},{"paperId":"5710567c482376c3c7c559062e884492090f7aca","externalIds":{"ArXiv":"2108.13654","ACL":"2021.emnlp-main.805","DBLP":"conf/emnlp/Sanyal021","DOI":"10.18653/v1/2021.emnlp-main.805","CorpusId":237363853},"title":"Discretized Integrated Gradients for Explaining Language Models"},{"paperId":"dfb357818d8bebc49f7da2ea7f70be5c4ce25a06","externalIds":{"DBLP":"journals/corr/abs-2107-14000","ArXiv":"2107.14000","CorpusId":236493518},"title":"Resisting Out-of-Distribution Data Problem in Perturbation of XAI"},{"paperId":"106176d796eda5efcd8a4d84ce98267350d679b9","externalIds":{"DBLP":"journals/ijon/LauriolaLA22","MAG":"3190730109","DOI":"10.1016/j.neucom.2021.05.103","CorpusId":238835461},"title":"An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools"},{"paperId":"118dea7d937c37ab7d1b3ec958b1005bf69a0a2c","externalIds":{"MAG":"3170826848","ArXiv":"2106.02559","ACL":"2021.naacl-main.11","DBLP":"journals/corr/abs-2106-02559","DOI":"10.18653/V1/2021.NAACL-MAIN.11","CorpusId":235097491},"title":"Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing"},{"paperId":"57ed901be5d1b4d853d4f8998dadc1b60e2151f9","externalIds":{"ACL":"2021.naacl-main.72","MAG":"3167354871","DBLP":"conf/naacl/BianHCYC21","DOI":"10.18653/V1/2021.NAACL-MAIN.72","CorpusId":235097467},"title":"On Attention Redundancy: A Comprehensive Study"},{"paperId":"da130d6538eeeacfb3a0da4cff106c098f74cdd4","externalIds":{"DBLP":"conf/acl/EthayarajhJ20","ArXiv":"2105.14652","ACL":"2021.acl-short.8","MAG":"3170204075","DOI":"10.18653/v1/2021.acl-short.8","CorpusId":235254444},"title":"Attention Flows are Shapley Value Explanations"},{"paperId":"0e6338c992b6b72da05cb783f4d422ebf0462451","externalIds":{"DBLP":"conf/aaai/ArousDYBCC21","DOI":"10.1609/aaai.v35i7.16734","CorpusId":231395350},"title":"MARTA: Leveraging Human Rationales for Explainable Text Classification"},{"paperId":"528cd7c546315ae57e532ff9f57a674378a5ad6f","externalIds":{"DBLP":"conf/acl/LiZT0R20","ArXiv":"2105.07452","ACL":"2021.acl-long.325","DOI":"10.18653/v1/2021.acl-long.325","CorpusId":234742522},"title":"How is BERT surprised? Layerwise detection of linguistic anomalies"},{"paperId":"884dffa4c248663a32cacc1b37cb4a5ecd4d2b65","externalIds":{"ArXiv":"2105.03287","DBLP":"journals/corr/abs-2105-03287","CorpusId":234096057},"title":"Order in the Court: Explainable AI Methods Prone to Disagreement"},{"paperId":"32a6daa76efd00f657e65842771971decf104efc","externalIds":{"DBLP":"conf/acl/ChrysostomouA20","ArXiv":"2105.02657","ACL":"2021.acl-long.40","DOI":"10.18653/v1/2021.acl-long.40","CorpusId":233864728},"title":"Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification"},{"paperId":"11d57e1af3c1f939efcee67640ccbf9814290016","externalIds":{"ArXiv":"2104.08142","DBLP":"conf/aaai/StaceyBR22","DOI":"10.1609/aaai.v36i10.21386","CorpusId":245131647},"title":"Supervising Model Attention with Human Explanations for Robust Natural Language Inference"},{"paperId":"f2885c6a25756cf81aa23b41bc62696a5be5c94d","externalIds":{"DBLP":"journals/corr/abs-2104-05240","MAG":"3166986030","ACL":"2021.naacl-main.398","ArXiv":"2104.05240","DOI":"10.18653/V1/2021.NAACL-MAIN.398","CorpusId":233210199},"title":"Factual Probing Is [MASK]: Learning vs. Learning to Recall"},{"paperId":"4072a2333682941d23755e9b7e1e3a6d899683c6","externalIds":{"ArXiv":"2104.03869","DBLP":"conf/iclr/ChenFXXTCJ21","CorpusId":233181741},"title":"Probing BERT in Hyperbolic Spaces"},{"paperId":"b265827019f420b44c79fd87be1cc6000329c762","externalIds":{"ArXiv":"2104.01477","DBLP":"conf/emnlp/MohebbiMP21","ACL":"2021.emnlp-main.61","DOI":"10.18653/v1/2021.emnlp-main.61","CorpusId":233024695},"title":"Exploring the Role of BERT Token Representations to Explain Sentence Probing Results"},{"paperId":"e1872cbe03dd45b5323f9c30facca5e3d6d95f14","externalIds":{"ArXiv":"2104.00926","DBLP":"journals/corr/abs-2104-00926","DOI":"10.1109/TVCG.2021.3114683","CorpusId":233004528,"PubMed":"34587013"},"title":"VisQA: X-raying Vision and Language Reasoning in Transformers"},{"paperId":"97fcbad1088e219621b72ef928b2e3824c46bbd7","externalIds":{"DBLP":"journals/csur/LuoIHP24","ArXiv":"2103.11072","DOI":"10.1145/3649450","CorpusId":232307813},"title":"Local Interpretations for Explainable Natural Language Processing: A Survey"},{"paperId":"a64f6ba2f83ca489e467d611fde38321b340301a","externalIds":{"DBLP":"journals/corr/abs-2103-06922","ArXiv":"2103.06922","ACL":"2021.naacl-main.71","MAG":"3170037207","DOI":"10.18653/V1/2021.NAACL-MAIN.71","CorpusId":232222189},"title":"Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"3dcfa05a1c162e6cab927c5b08d0444f7b6691f4","externalIds":{"ArXiv":"2102.12452","DBLP":"journals/coling/Belinkov22","ACL":"2022.cl-1.7","DOI":"10.1162/coli_a_00422","CorpusId":236924832},"title":"Probing Classifiers: Promises, Shortcomings, and Advances"},{"paperId":"506e400330356db96ac68685e473f3b6ace370a3","externalIds":{"ACL":"2021.acl-long.523","ArXiv":"2101.00288","DBLP":"conf/acl/WuRHW20","DOI":"10.18653/v1/2021.acl-long.523","CorpusId":235266322},"title":"Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models"},{"paperId":"a560943f23a1e00e26abbd9d84367217b235ff28","externalIds":{"DBLP":"journals/corr/abs-2101-00196","ArXiv":"2101.00196","CorpusId":230437868},"title":"On Explaining Your Explanations of BERT: An Empirical Study with Sequence Classification"},{"paperId":"18fb344c9bfd019014996e57c465aa279b7e0151","externalIds":{"DBLP":"conf/emnlp/GuoRHBX21","ACL":"2021.emnlp-main.808","ArXiv":"2012.15781","DOI":"10.18653/v1/2021.emnlp-main.808","CorpusId":229923196},"title":"FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging"},{"paperId":"4a54d58a4b20e4f3af25cea3c188a12082a95e02","externalIds":{"DBLP":"conf/emnlp/GevaSBL21","ACL":"2021.emnlp-main.446","ArXiv":"2012.14913","DOI":"10.18653/v1/2021.emnlp-main.446","CorpusId":229923720},"title":"Transformer Feed-Forward Layers Are Key-Value Memories"},{"paperId":"84f52e131e31a47bd0d9f40a2f9fbc770024b9c9","externalIds":{"DBLP":"conf/acl/RossMP21","ACL":"2021.findings-acl.336","ArXiv":"2012.13985","DOI":"10.18653/v1/2021.findings-acl.336","CorpusId":229679941},"title":"Explaining NLP Models via Minimal Contrastive Editing (MiCE)"},{"paperId":"3728ab7974f5a2f3392ea0ded15c5104400004fb","externalIds":{"DBLP":"journals/corr/abs-2012-10289","ArXiv":"2012.10289","DOI":"10.1609/aaai.v35i17.17745","CorpusId":229332119},"title":"HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection"},{"paperId":"0acd7ff5817d29839b40197f7a4b600b7fba24e4","externalIds":{"DBLP":"conf/cvpr/CheferGW21","ArXiv":"2012.09838","MAG":"3112516115","DOI":"10.1109/CVPR46437.2021.00084","CorpusId":229297908},"title":"Transformer Interpretability Beyond Attention Visualization"},{"paperId":"fd85cc9094bb93a791f9bb0c86a6b36bb38f648b","externalIds":{"MAG":"3112541430","DOI":"10.23915/DISTILL.00024.004","CorpusId":230607419},"title":"Naturally Occurring Equivariance in Neural Networks"},{"paperId":"6906c562e182ea22c086b2bb44ed9cec8602a220","externalIds":{"MAG":"3116152597","ACL":"2020.coling-main.450","DBLP":"conf/coling/KunzK20","DOI":"10.18653/V1/2020.COLING-MAIN.450","CorpusId":227231342},"title":"Classifier Probes May Just Learn from Linear Context Features"},{"paperId":"9d1f9406ed676171d9975e27606c95633ca898b1","externalIds":{"DBLP":"conf/starsem/RavichanderHSTC20","MAG":"3110879614","ACL":"2020.starsem-1.10","CorpusId":227230677},"title":"On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT"},{"paperId":"6936f7e97ed4a587fcb347d0636f1e5cbcadf227","externalIds":{"DBLP":"journals/corr/abs-2010-12697","ArXiv":"2010.12697","MAG":"3094084754","CorpusId":225068956},"title":"Investigating Saturation Effects in Integrated Gradients"},{"paperId":"d9bc8169326d3cca0860ec76247715bc7beaf9c9","externalIds":{"ACL":"2021.acl-long.91","DBLP":"journals/corr/abs-2010-10907","ArXiv":"2010.10907","MAG":"3094519420","DOI":"10.18653/v1/2021.acl-long.91","CorpusId":224818197},"title":"Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation"},{"paperId":"c1ce7375f040327ad7c51e852ae778b6cb8168c9","externalIds":{"MAG":"3092614725","DBLP":"journals/corr/abs-2010-04922","ArXiv":"2010.04922","ACL":"2020.blackboxnlp-1.24","DOI":"10.18653/v1/2020.blackboxnlp-1.24","CorpusId":222291617},"title":"Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis"},{"paperId":"3c4d3b7085ad5f02bc732b489ace590a7bbd58c9","externalIds":{"MAG":"3102812725","ArXiv":"2010.02812","DBLP":"journals/corr/abs-2010-02812","ACL":"2020.emnlp-main.15","DOI":"10.18653/v1/2020.emnlp-main.15","CorpusId":222140751},"title":"Intrinsic Probing through Dimension Selection"},{"paperId":"472cd41fa2ba2e520706f232cae12db4a7b5e60a","externalIds":{"MAG":"3169948074","ACL":"2021.naacl-main.400","DBLP":"journals/corr/abs-2009-07502","ArXiv":"2009.07502","DOI":"10.18653/V1/2021.NAACL-MAIN.400","CorpusId":221739314},"title":"Contextualized Perturbation for Textual Adversarial Attack"},{"paperId":"6e1d2178dadab5b8017d6aced59ee25f4612dc19","externalIds":{"MAG":"3087396445","DBLP":"journals/corr/abs-2009-07494","ArXiv":"2009.07494","CorpusId":221739102},"title":"Are Interpretations Fairly Evaluated? A Definition Driven Pipeline for Post-Hoc Interpretability"},{"paperId":"cb0c40ab85ddff43e41aafabc90031c4140f31b2","externalIds":{"DBLP":"journals/corr/abs-2009-07053","MAG":"3087570533","ArXiv":"2009.07053","DOI":"10.1109/TVCG.2020.3028976","CorpusId":221703136,"PubMed":"33052855"},"title":"Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models"},{"paperId":"6150a2dab1b63b246eb2cd418fcdb5a6b6b8ae62","externalIds":{"MAG":"3038035611","DBLP":"conf/acl/HooverSG20","ACL":"2020.acl-demos.22","DOI":"10.18653/v1/2020.acl-demos.22","CorpusId":260539432},"title":"exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models"},{"paperId":"bb429a17280c2df86ac34789df880a4f728009ae","externalIds":{"DBLP":"conf/acl/SorodocGB20","ACL":"2020.acl-main.384","MAG":"3034685497","DOI":"10.18653/v1/2020.acl-main.384","CorpusId":220047937},"title":"Probing for Referential Information in Language Models"},{"paperId":"56d1003fd02346e93354ab55cd204485c268512a","externalIds":{"ArXiv":"2006.14032","DBLP":"conf/nips/MuA20","MAG":"3098680936","CorpusId":220055965},"title":"Compositional Explanations of Neurons"},{"paperId":"5a5cb1d36d3ab64fc1941d05ec4387c4e62feac7","externalIds":{"ArXiv":"2006.05656","DBLP":"conf/kdd/BaiLZLBW21","DOI":"10.1145/3447548.3467307","CorpusId":235314008},"title":"Why Attentions May Not Be Interpretable?"},{"paperId":"14b65a86c82e38fce0eb3506e0d4084ad5cdb583","externalIds":{"MAG":"3033187248","DBLP":"conf/iclr/HeLGC21","ArXiv":"2006.03654","CorpusId":219531210},"title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"cdf766403e365643ac4dfdf9e10df8da1b75b63f","externalIds":{"ArXiv":"2004.14786","ACL":"2020.acl-main.383","DBLP":"journals/corr/abs-2004-14786","MAG":"3034503989","DOI":"10.18653/v1/2020.acl-main.383","CorpusId":216914626},"title":"Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT"},{"paperId":"3d61a34611c6171f203286119f76ec52f8016580","externalIds":{"MAG":"3035281110","ArXiv":"2004.14243","ACL":"2020.acl-main.387","DBLP":"journals/corr/abs-2004-14243","DOI":"10.18653/v1/2020.acl-main.387","CorpusId":216641945},"title":"Towards Transparent and Explainable Attention Models"},{"paperId":"1686203adc5f2dbc18627ce64f66d33eb81432a5","externalIds":{"ArXiv":"2004.11207","DBLP":"journals/corr/abs-2004-11207","MAG":"3020482686","DOI":"10.1609/aaai.v35i14.17533","CorpusId":264653968},"title":"Self-Attention Attribution: Interpreting Information Interactions Inside Transformer"},{"paperId":"579476d19566efc842929ea6bdd18ab760c8cfa2","externalIds":{"MAG":"3015575765","DBLP":"journals/corr/abs-2004-03685","ACL":"2020.acl-main.386","ArXiv":"2004.03685","DOI":"10.18653/v1/2020.acl-main.386","CorpusId":215416110},"title":"Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?"},{"paperId":"06a427e1688f92053a38c73cb4e0da25177c89e7","externalIds":{"MAG":"3104423855","ACL":"2020.emnlp-main.498","DBLP":"conf/emnlp/GargR20","ArXiv":"2004.01970","DOI":"10.18653/v1/2020.emnlp-main.498","CorpusId":214802269},"title":"BAE: BERT-based Adversarial Examples for Text Classification"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","externalIds":{"MAG":"3006881356","ArXiv":"2002.12327","DBLP":"journals/corr/abs-2002-12327","DOI":"10.1162/tacl_a_00349","CorpusId":211532403},"title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","externalIds":{"MAG":"3033156098","CorpusId":226096901},"title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c94e49617f569204f989643e5462691b9b3a482b","externalIds":{"MAG":"3101656801","DBLP":"journals/corr/abs-2002-08484","ArXiv":"2002.08484","CorpusId":211204970},"title":"Estimating Training Data Influence by Tracking Gradient Descent"},{"paperId":"087dd95e13efd47aef2a6582e6801b39fc0f83d8","externalIds":{"DBLP":"conf/acl/DeYoungJRLXSW20","ACL":"2020.acl-main.408","ArXiv":"1911.03429","MAG":"3035503910","DOI":"10.18653/v1/2020.acl-main.408","CorpusId":207847663},"title":"ERASER: A Benchmark to Evaluate Rationalized NLP Models"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"47f1eb0dc42189ba7cf21b76598c8217eb1b6e05","externalIds":{"DBLP":"journals/corr/abs-1909-12434","ArXiv":"1909.12434","MAG":"2977235550","CorpusId":203591519},"title":"Learning the Difference that Makes a Difference with Counterfactually-Augmented Data"},{"paperId":"d4e2534eb6473db9712420b43092f4e84d980412","externalIds":{"MAG":"2973730398","DBLP":"journals/corr/abs-1909-09595","ArXiv":"1909.09595","DOI":"10.1109/VISUAL.2019.8933677","CorpusId":202712734},"title":"SANVis: Visual Analytics for Understanding Self-Attention Networks"},{"paperId":"be283fc67ecbfe0cb1e50718181ce7b9c0d53a74","externalIds":{"MAG":"3102568136","DBLP":"journals/corr/abs-1909-04925","ArXiv":"1909.04925","DOI":"10.1145/3357384.3358028","CorpusId":202558795},"title":"How Does BERT Answer Questions?: A Layer-Wise Analysis of Transformer Representations"},{"paperId":"199ff73d2f728e997f860b62a2322823d3e3d9e8","externalIds":{"ArXiv":"1909.03368","MAG":"2972296365","ACL":"D19-1275","DBLP":"journals/corr/abs-1909-03368","DOI":"10.18653/v1/D19-1275","CorpusId":202538609},"title":"Designing and Interpreting Probes with Control Tasks"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","externalIds":{"DBLP":"journals/corr/abs-1909-01066","MAG":"2996758945","ArXiv":"1909.01066","ACL":"D19-1250","DOI":"10.18653/v1/D19-1250","CorpusId":202539551},"title":"Language Models as Knowledge Bases?"},{"paperId":"d78aed1dac6656affa4a04cbf225ced11a83d103","externalIds":{"DBLP":"conf/emnlp/KovalevaRRR19","ArXiv":"1908.08593","ACL":"D19-1445","MAG":"2970120757","DOI":"10.18653/v1/D19-1445","CorpusId":201645145},"title":"Revealing the Dark Secrets of BERT"},{"paperId":"ce177672b00ddf46e4906157a7e997ca9338b8b9","externalIds":{"MAG":"2970726176","ACL":"D19-1002","ArXiv":"1908.04626","DBLP":"journals/corr/abs-1908-04626","DOI":"10.18653/v1/D19-1002","CorpusId":199552244},"title":"Attention is not not Explanation"},{"paperId":"1fe62a928bf5cfac0f373728f3a4de3cefe0951d","externalIds":{"DBLP":"conf/iclr/BrunnerLPRCW20","ArXiv":"1908.04211","MAG":"2977944219","CorpusId":208100714},"title":"On Identifiability in Transformers"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","externalIds":{"DBLP":"conf/acl/JawaharSS19","ACL":"P19-1356","MAG":"2948947170","DOI":"10.18653/v1/P19-1356","CorpusId":195477534},"title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2","externalIds":{"ArXiv":"1907.11932","DBLP":"conf/aaai/JinJZS20","MAG":"2996851481","DOI":"10.1609/AAAI.V34I05.6311","CorpusId":202539059},"title":"Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"be2eae7f022b7be196ab816a32bc0de4a7ac0932","externalIds":{"DBLP":"conf/icmlc/0001L19","MAG":"2999884159","DOI":"10.1109/ICMLC48188.2019.8949185","CorpusId":209901353},"title":"Deep Learning in Natural Language Processing: A State-of-the-Art Survey"},{"paperId":"95a251513853c6032bdecebd4b74e15795662986","externalIds":{"ACL":"W19-4828","MAG":"2953337107","DBLP":"journals/corr/abs-1906-04341","ArXiv":"1906.04341","DOI":"10.18653/v1/W19-4828","CorpusId":184486746},"title":"What Does BERT Look at? An Analysis of BERT’s Attention"},{"paperId":"874e9318c09c711ecd48a903b3824a3a03e2cd62","externalIds":{"MAG":"2948036864","ACL":"P19-1487","ArXiv":"1906.02361","DBLP":"journals/corr/abs-1906-02361","DOI":"10.18653/v1/P19-1487","CorpusId":174803111},"title":"Explain Yourself! Leveraging Language Models for Commonsense Reasoning"},{"paperId":"165d51a547cd920e6ac55660ad5c404dcb9562ed","externalIds":{"MAG":"2948184675","DBLP":"conf/blackboxnlp/LinTF19","ACL":"W19-4825","ArXiv":"1906.01698","DOI":"10.18653/v1/w19-4825","CorpusId":174799346},"title":"Open Sesame: Getting inside BERT’s Linguistic Knowledge"},{"paperId":"135112c7ba1762d65f39b1a61777f26ae4dfd8ad","externalIds":{"MAG":"2948140294","DBLP":"conf/acl/SerranoS19","ACL":"P19-1282","ArXiv":"1906.03731","DOI":"10.18653/v1/P19-1282","CorpusId":182953113},"title":"Is Attention Interpretable?"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","externalIds":{"ACL":"N19-1419","MAG":"2946359678","DBLP":"conf/naacl/HewittM19","DOI":"10.18653/v1/N19-1419","CorpusId":106402715},"title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"177f364e0e2ae3f3ff1b3f1a133445427c0d570a","externalIds":{"MAG":"2971196291","ArXiv":"1906.08473","DBLP":"conf/nips/HaraNM19","CorpusId":195218427},"title":"Data Cleansing for Models Trained with SGD"},{"paperId":"07a64686ce8e43ac475a8d820a8a9f1d87989583","externalIds":{"MAG":"2951528897","DBLP":"journals/corr/abs-1905-09418","ACL":"P19-1580","ArXiv":"1905.09418","DOI":"10.18653/v1/P19-1580","CorpusId":162183964},"title":"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"},{"paperId":"e2587eddd57bc4ba286d91b27c185083f16f40ee","externalIds":{"DBLP":"journals/corr/abs-1905-06316","MAG":"2908854766","ArXiv":"1905.06316","CorpusId":108300988},"title":"What do you learn from context? Probing for sentence structure in contextualized word representations"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","externalIds":{"MAG":"2953307569","ArXiv":"1905.05950","DBLP":"conf/acl/TenneyDP19","ACL":"P19-1452","DOI":"10.18653/v1/P19-1452","CorpusId":155092004},"title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"b7a717233ec3ff37385ab1b06816d0ca375f5bb3","externalIds":{"MAG":"2939984132","ArXiv":"1904.02868","DBLP":"conf/icml/GhorbaniZ19","CorpusId":102350503},"title":"Data Shapley: Equitable Valuation of Data for Machine Learning"},{"paperId":"e7e1f5a713d20cdf31e732022731fdf0d8fb4fc5","externalIds":{"DBLP":"journals/corr/abs-1904-10717","MAG":"2941681871","ArXiv":"1904.10717","ACL":"N19-1101","DOI":"10.18653/v1/N19-1101","CorpusId":129945615},"title":"Generating Token-Level Explanations for Natural Language Inference"},{"paperId":"26f8d83c5dd370e5f291798e896a9b82e6cf83ce","externalIds":{"MAG":"2951939848","ArXiv":"1903.11245","DBLP":"conf/www/DuLYJH19","DOI":"10.1145/3308558.3313545","CorpusId":85528522},"title":"On Attribution of Recurrent Neural Network Predictions via Additive Decomposition"},{"paperId":"1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f","externalIds":{"MAG":"2934842096","DBLP":"journals/corr/abs-1902-10186","ArXiv":"1902.10186","ACL":"N19-1357","DOI":"10.18653/v1/N19-1357","CorpusId":67855860},"title":"Attention is not Explanation"},{"paperId":"9c2156bc35c6f8e68aa21d4b2f339134a4d28708","externalIds":{"DBLP":"journals/corr/abs-1812-09355","MAG":"2963400886","ArXiv":"1812.09355","DOI":"10.1609/aaai.v33i01.33016309","CorpusId":56895415},"title":"What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models"},{"paperId":"a34954d9e36ea6c57743f55124a6ae444b951c2c","externalIds":{"MAG":"2890457888","DBLP":"journals/corr/abs-1811-09720","ArXiv":"1811.09720","CorpusId":53741665},"title":"Representer Point Selection for Explaining Deep Neural Networks"},{"paperId":"2e46eac625e70261e43fa765c22a2828e5dd2659","externalIds":{"ArXiv":"1811.05544","MAG":"2969262604","DBLP":"conf/intellisys/Hu19","DOI":"10.1007/978-3-030-29513-4_31","CorpusId":53305352},"title":"An Introductory Survey on Attention Mechanisms in NLP Problems"},{"paperId":"c5489d244bfc1e9b0d8c94bf6dd774ee1aca2def","externalIds":{"DBLP":"conf/iclr/BauBSDDG19","MAG":"2899032424","ArXiv":"1811.01157","CorpusId":53215110},"title":"Identifying and Controlling Important Neurons in Neural Machine Translation"},{"paperId":"ac11062f1f368d97f4c826c317bf50dcc13fdb59","externalIds":{"DBLP":"conf/emnlp/PetersNZY18","ArXiv":"1808.08949","MAG":"2950405925","ACL":"D18-1179","DOI":"10.18653/v1/D18-1179","CorpusId":52098907},"title":"Dissecting Contextual Word Embeddings: Architecture and Representation"},{"paperId":"4d00097433a538002b36cfd7a621daddde3e4c0d","externalIds":{"MAG":"2888922637","DBLP":"journals/corr/abs-1808-09031","ACL":"D18-1151","ArXiv":"1808.09031","DOI":"10.18653/v1/D18-1151","CorpusId":52113185},"title":"Targeted Syntactic Evaluation of Language Models"},{"paperId":"3df952d4a724655f7520ff95d4b2cef90fff0cae","externalIds":{"MAG":"2886794383","DBLP":"journals/cacm/DuLH20","ArXiv":"1808.00033","DOI":"10.1145/3359786","CorpusId":51893222},"title":"Techniques for interpretable machine learning"},{"paperId":"efef34c1caef102ad5cc052642d75beaaf5adcaf","externalIds":{"MAG":"2963651521","ArXiv":"1805.04218","ACL":"P18-2003","DBLP":"journals/corr/abs-1805-04218","DOI":"10.18653/v1/p18-2003","CorpusId":21663989},"title":"Deep RNNs Encode Soft Hierarchical Syntax"},{"paperId":"1abec8d03b8d1435ae920edea9b437be913a30ca","externalIds":{"MAG":"2950762753","ArXiv":"1804.09299","DBLP":"journals/tvcg/StrobeltGBPPR19","DOI":"10.1109/TVCG.2018.2865044","CorpusId":13754931,"PubMed":"30334796"},"title":"Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models"},{"paperId":"74e9053d6f44f4507bd40bbea999ee65f0cbefb2","externalIds":{"ArXiv":"1804.07781","MAG":"2950893515","ACL":"D18-1407","DBLP":"conf/emnlp/FengWGIRB18","DOI":"10.18653/v1/D18-1407","CorpusId":52003282},"title":"Pathologies of Neural Models Make Interpretations Difficult"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"682b9d2212258fd5edbfca589c86390c31a956b0","externalIds":{"ArXiv":"1711.11279","MAG":"2796885425","DBLP":"conf/icml/KimWGCWVS18","CorpusId":51737170},"title":"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"},{"paperId":"62e39c6dbcead1fe4d29017914591d929aa6ac4c","externalIds":{"MAG":"2766047647","ArXiv":"1711.00867","DBLP":"series/lncs/KindermansHAASDEK19","DOI":"10.1007/978-3-030-28954-6_14","CorpusId":28562869},"title":"The (Un)reliability of saliency methods"},{"paperId":"dcb028149bb3cf934fbd2e4cbb773ffbb9b0e49d","externalIds":{"ACL":"I17-1001","ArXiv":"1801.07772","DBLP":"journals/corr/abs-1801-07772","MAG":"2962777840","CorpusId":24544277},"title":"Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks"},{"paperId":"442e10a3c6640ded9408622005e3c2a8906ce4c2","externalIds":{"MAG":"2618851150","DBLP":"journals/corr/LundbergL17","ArXiv":"1705.07874","CorpusId":21889700},"title":"A Unified Approach to Interpreting Model Predictions"},{"paperId":"08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1","externalIds":{"ArXiv":"1703.04730","DBLP":"journals/corr/KohL17","MAG":"2597603852","CorpusId":13193974},"title":"Understanding Black-box Predictions via Influence Functions"},{"paperId":"f302e136c41db5de1d624412f68c9174cf7ae8be","externalIds":{"MAG":"2949197630","DBLP":"conf/icml/SundararajanTY17","ArXiv":"1703.01365","CorpusId":16747630},"title":"Axiomatic Attribution for Deep Networks"},{"paperId":"5c39e37022661f81f79e481240ed9b175dec6513","externalIds":{"MAG":"2594475271","ArXiv":"1702.08608","CorpusId":11319376},"title":"Towards A Rigorous Science of Interpretable Machine Learning"},{"paperId":"4c41104e871bccbd56494350a71d77a7f1da5bb0","externalIds":{"ArXiv":"1612.08220","DBLP":"journals/corr/LiMJ16a","MAG":"2562979205","CorpusId":13017314},"title":"Understanding Neural Networks through Representation Erasure"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","externalIds":{"DBLP":"journals/corr/DozatM16","MAG":"2963571341","ArXiv":"1611.01734","CorpusId":7942973},"title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"2a24b68ef180c0c8742bd494a55fb6f68864efed","externalIds":{"MAG":"2963410064","DBLP":"conf/nips/VeitWB16","CorpusId":715122},"title":"Residual Networks Behave Like Ensembles of Relatively Shallow Networks"},{"paperId":"c0883f5930a232a9c1ad601c978caede29155979","externalIds":{"DBLP":"conf/naacl/Ribeiro0G16","MAG":"2516809705","ArXiv":"1602.04938","ACL":"N16-3020","DOI":"10.1145/2939672.2939778","CorpusId":13029170},"title":"“Why Should I Trust You?”: Explaining the Predictions of Any Classifier"},{"paperId":"056713e422a0753c5eb1733d73e9f8185e2015d4","externalIds":{"MAG":"2195388612","DBLP":"journals/corr/MontavonBBSM15","ArXiv":"1512.02479","DOI":"10.1016/j.patcog.2016.11.008","CorpusId":266022338},"title":"Explaining nonlinear classification decisions with deep Taylor decomposition"},{"paperId":"fafa541419b3756968fe5b3156c6f0257cb29c23","externalIds":{"DBLP":"conf/naacl/LiCHJ16","MAG":"1601924930","ACL":"N16-1082","ArXiv":"1506.01066","DOI":"10.18653/v1/N16-1082","CorpusId":14099741},"title":"Visualizing and Understanding Neural Models in NLP"},{"paperId":"19088a582f2eb657ac1803f1ea1b79058d5c3dc7","externalIds":{"MAG":"1562353621","DOI":"10.1017/CBO9780511528446.003","CorpusId":153629957},"title":"A Value for n-person Games"},{"paperId":"5424e311319c58847b4c690d5c91090e3b6a4ac3","externalIds":{"DBLP":"journals/corr/abs-2307-01379","DOI":"10.48550/arXiv.2307.01379","CorpusId":259342406},"title":"Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models"},{"paperId":"ea0d41514a41f8273f13b3b277e7fcbbc65a8549","externalIds":{"DBLP":"journals/corr/abs-2307-10236","DOI":"10.48550/arXiv.2307.10236","CorpusId":259991714},"title":"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models"},{"paperId":"f48287e9ed131ff8ffa79b66717887c5af74f203","externalIds":{"ACL":"2023.eacl-main.234","DBLP":"conf/eacl/LadhakDSZJMH23","DOI":"10.18653/v1/2023.eacl-main.234","CorpusId":258378241},"title":"When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization"},{"paperId":"cfa37731c5d269c2ea15eb56cb6c13a008404134","externalIds":{"DBLP":"conf/blackboxnlp/ZhangWCZ22","ACL":"2022.blackboxnlp-1.24","DOI":"10.18653/v1/2022.blackboxnlp-1.24","CorpusId":256461348},"title":"Probing GPT-3’s Linguistic Knowledge on Semantic Tasks"},{"paperId":"bcf35117a0cb08fba6b4d00f9aa4f499946b57d9","externalIds":{"DBLP":"conf/acl/SikdarBH20","ACL":"2021.acl-long.71","DOI":"10.18653/v1/2021.acl-long.71","CorpusId":236460258},"title":"Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models"},{"paperId":"e09f1fc72cbd53db1a06831dda5e22d996c2f3c1","externalIds":{"ACL":"2021.eacl-main.243","DBLP":"conf/eacl/MoradiKS21","DOI":"10.18653/v1/2021.eacl-main.243","CorpusId":233189641},"title":"Measuring and Improving Faithfulness of Attention in Neural Machine Translation"},{"paperId":"7823b7c83ce4b7d739fbd3436ba2c911dfec4d82","externalIds":{"DBLP":"conf/eacl/KokaljSLPR21","ACL":"2021.hackashop-1.3","CorpusId":233364997},"title":"BERT meets Shapley: Extending SHAP Explanations to Transformer-based Classifiers"},{"paperId":"f5dfed82b0c8747e41a1206f52a6d0ea3dce4a5c","externalIds":{"DOI":"10.18653/v1/2021.acl-long","CorpusId":242941521},"title":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"},{"paperId":"ffc7d23ccaba2d0bab7dd3cbb452724c96b0b974","externalIds":{"MAG":"3040357889","DOI":"10.1007/978-981-15-5573-2_1","CorpusId":226608263},"title":"Representation Learning and NLP"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"a3347bbd82938788ec085772813c095de17a0b37","externalIds":{"CorpusId":198967870},"title":"Is BERT Really Robust? Natural Language Attack on Text Classiﬁcation and Entailment"},{"paperId":"0e1f153576c7f9f2628cdd34a1067c4d26bdc096","externalIds":{"DBLP":"series/lncs/11700","MAG":"3000716014","DOI":"10.1007/978-3-030-28954-6","CorpusId":202160253},"title":"Explainable AI: Interpreting, Explaining and Visualizing Deep Learning"},{"paperId":"b8e19d9154f49e951d30d8673585aa654b559cfc","externalIds":{"DBLP":"conf/inlg/MazzeiMB19","MAG":"2995173825","ACL":"W19-8658","DOI":"10.18653/v1/W19-8658","CorpusId":208782352},"title":"Using NLG for speech synthesis of mathematical sentences"},{"paperId":"ef9bbc83dea84df3711de01de56f2b7f91bae068","externalIds":{"DBLP":"series/lncs/MontavonBLSM19","MAG":"2973136764","DOI":"10.1007/978-3-030-28954-6_10","CorpusId":202579539},"title":"Layer-Wise Relevance Propagation: An Overview"},{"paperId":"9916356e70cfdafa2569aa26bc240aeccb46865c","externalIds":{"DOI":"10.23915/distill.00024.009","CorpusId":243426562},"title":"Weight Banding"},{"paperId":"d6e4d60d32a201ef646bcc067261df68265e6821","externalIds":{"DOI":"10.23915/distill.00024.008","CorpusId":241745558},"title":"Branch Specialization"}]}