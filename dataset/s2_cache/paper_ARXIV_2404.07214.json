{"paperId":"5506409ad47e709ed1a1e4ab0319f4f931b7e379","externalIds":{"DBLP":"journals/corr/abs-2404-07214","ArXiv":"2404.07214","DOI":"10.48550/arXiv.2404.07214","CorpusId":269043091},"title":"Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2404.07214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2275156796","name":"Akash Ghosh"},{"authorId":"2273560136","name":"Arkadeep Acharya"},{"authorId":"2244669276","name":"Sriparna Saha"},{"authorId":"2212131028","name":"Vinija Jain"},{"authorId":"2275226689","name":"Aman Chadha"}],"abstract":"The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements."}