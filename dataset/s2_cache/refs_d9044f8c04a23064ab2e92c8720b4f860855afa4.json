{"references":[{"paperId":"8f3aef383265c233075d6cd2b06eaf6fc3623556","externalIds":{"DBLP":"journals/cbm/SongDLLLYLQL24","DOI":"10.1016/j.compbiomed.2024.108463","CorpusId":269053553,"PubMed":"38640634"},"title":"DesTrans: A medical image fusion method based on Transformer and improved DenseNet"},{"paperId":"de41158515fa7260a0983e787650884a98eed811","externalIds":{"ArXiv":"2402.08132","DBLP":"journals/corr/abs-2402-08132","DOI":"10.48550/arXiv.2402.08132","CorpusId":267637145},"title":"On the Resurgence of Recurrent Models for Long Sequences - Survey and Research Opportunities in the Transformer Era"},{"paperId":"a2539713a31ee08cc8116c1355da9f96896df145","externalIds":{"DBLP":"journals/corr/abs-2402-05602","ArXiv":"2402.05602","DOI":"10.48550/arXiv.2402.05602","CorpusId":267547813},"title":"AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers"},{"paperId":"a091bf215c716a146140f81c751712db628c8e20","externalIds":{"ArXiv":"2402.03766","DBLP":"journals/corr/abs-2402-03766","DOI":"10.48550/arXiv.2402.03766","CorpusId":267500104},"title":"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model"},{"paperId":"fc4c380102d6f72657d1ab54dffd6be536bb01c7","externalIds":{"ArXiv":"2402.00253","DBLP":"journals/corr/abs-2402-00253","DOI":"10.48550/arXiv.2402.00253","CorpusId":267365472},"title":"A Survey on Hallucination in Large Vision-Language Models"},{"paperId":"afdf30b1616c21adfe904ff11cb4a1ebcd2cbe03","externalIds":{"DBLP":"journals/mta/ZhangT24","DOI":"10.1007/s11042-023-17944-9","CorpusId":267100107},"title":"Survey of deep emotion recognition in dynamic data using facial, speech and textual cues"},{"paperId":"0eb0e917f7698dd90c6dbf7d03f844a8a8250022","externalIds":{"PubMedCentral":"10800328","DOI":"10.1186/s13244-023-01541-3","CorpusId":267067636,"PubMed":"38259140"},"title":"Developing, purchasing, implementing and monitoring AI tools in radiology: practical considerations. A multi-society statement from the ACR, CAR, ESR, RANZCR & RSNA"},{"paperId":"a3ca77456142b78367dd5d53138b50dfac8086ca","externalIds":{"DBLP":"conf/cvpr/0003XKISGX24","ArXiv":"2401.12168","DOI":"10.1109/CVPR52733.2024.01370","CorpusId":267069344},"title":"SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities"},{"paperId":"616e98ba9e60f36c6ee226cc66c787610f0bbb62","externalIds":{"ArXiv":"2401.10208","DBLP":"journals/corr/abs-2401-10208","DOI":"10.48550/arXiv.2401.10208","CorpusId":267034565},"title":"MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer"},{"paperId":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","externalIds":{"DBLP":"journals/corr/abs-2401-06805","ArXiv":"2401.06805","DOI":"10.48550/arXiv.2401.06805","CorpusId":266999728},"title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning"},{"paperId":"d43cd1ee8b9154db5405d6b66a2ecc5a1bbcb089","externalIds":{"DBLP":"journals/nn/NagarajanMAR24","DOI":"10.1016/j.neunet.2024.106122","CorpusId":266945120,"PubMed":"38244356"},"title":"Bayesian DivideMix++ for Enhanced Learning with Noisy Labels"},{"paperId":"8d6319d051ef720445435606e1e89c2bd084a503","externalIds":{"DBLP":"journals/cbm/PrakashVPK24","DOI":"10.1016/j.compbiomed.2024.107977","CorpusId":266911119,"PubMed":"38217974"},"title":"A novel attention-based cross-modal transfer learning framework for predicting cardiovascular disease"},{"paperId":"4805b704e1477ea8f7dba2b308e94052a24171c9","externalIds":{"ArXiv":"2312.17183","PubMedCentral":"12405521","DBLP":"journals/npjdm/ZhaoZWZZZWX25","DOI":"10.1038/s41746-025-01964-w","CorpusId":266573350,"PubMed":"40897901"},"title":"Large-vocabulary segmentation for medical images with text prompts"},{"paperId":"fc9a9ef7e3e2b3ad26f041f0bc0b756e209b5cb1","externalIds":{"DOI":"10.18280/ria.370621","CorpusId":266883667},"title":"CNN Models Using Chest X-Ray Images for COVID-19 Detection: A Survey"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","externalIds":{"ArXiv":"2312.14238","DBLP":"journals/corr/abs-2312-14238","DOI":"10.1109/CVPR52733.2024.02283","CorpusId":266521410},"title":"Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"9542cc130dfb70c1614af0d130977b440f26d206","externalIds":{"ArXiv":"2312.08636","DBLP":"conf/aaai/XinDWYD24","DOI":"10.48550/arXiv.2312.08636","CorpusId":266210307},"title":"MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning"},{"paperId":"b1721374889899950994f67029fe899de257c140","externalIds":{"ArXiv":"2312.07814","DBLP":"journals/corr/abs-2312-07814","DOI":"10.48550/arXiv.2312.07814","CorpusId":266191621},"title":"A Foundational Multimodal Vision Language AI Assistant for Human Pathology"},{"paperId":"00b89844abecc9fb9ea687bedcd42c44421dfa23","externalIds":{"ArXiv":"2312.06037","DBLP":"journals/corr/abs-2312-06037","DOI":"10.48550/arXiv.2312.06037","CorpusId":266162896},"title":"Multimodality of AI for Education: Towards Artificial General Intelligence"},{"paperId":"d23f08611ea1e64f691ecddee1f7f48c8015eea6","externalIds":{"ArXiv":"2312.05278","CorpusId":266162357},"title":"Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects"},{"paperId":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","externalIds":{"ArXiv":"2312.03052","DBLP":"conf/cvpr/HuSLVHLKF24","DOI":"10.1109/CVPR52733.2024.00916","CorpusId":265693898},"title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"},{"paperId":"553fad1f8116b2cfe2c6e47bd9b5a0ec65a9d076","externalIds":{"DBLP":"journals/cbm/LiJAWK24","DOI":"10.1016/j.compbiomed.2023.107777","CorpusId":265783305,"PubMed":"38104516"},"title":"Medical image identification methods: A review"},{"paperId":"de0a1cee2be63e4534e42ae939c4bbaeff1440a0","externalIds":{"DBLP":"journals/inffus/Bayoudh24","DOI":"10.1016/j.inffus.2023.102217","CorpusId":266699775},"title":"A survey of multimodal hybrid deep learning for computer vision: Architectures, applications, trends, and challenges"},{"paperId":"ff5f0c5b6905a8c4b361a625b450e9ab417fa854","externalIds":{"ArXiv":"2311.16079","DBLP":"journals/corr/abs-2311-16079","DOI":"10.48550/arXiv.2311.16079","CorpusId":265456229},"title":"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models"},{"paperId":"a13a38d457ee73efc0a6011370346df0bec2d1b3","externalIds":{"PubMedCentral":"11268595","DOI":"10.1371/journal.pdig.0000413","CorpusId":265348780,"PubMed":"39046989"},"title":"Evaluating and mitigating unfairness in multimodal remote mental health assessments"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","externalIds":{"DBLP":"journals/corr/abs-2311-13165","ArXiv":"2311.13165","DOI":"10.1109/BigData59044.2023.10386743","CorpusId":265351653},"title":"Multimodal Large Language Models: A Survey"},{"paperId":"451539c0d0f5f5785ff58d09ca5e67a5f129f9de","externalIds":{"ArXiv":"2311.12320","DBLP":"journals/corr/abs-2311-12320","DOI":"10.1109/WACVW60836.2024.00106","CorpusId":265308931},"title":"A Survey on Multimodal Large Language Models for Autonomous Driving"},{"paperId":"0c8a630657a2cf5dea41472a9b5e20544ce2bd56","externalIds":{"ArXiv":"2311.11608","DBLP":"journals/jamia/LuoNZWDCFHXQPLLFTLYWSL24","DOI":"10.1093/jamia/ocae037","CorpusId":265294661,"PubMed":"38422367"},"title":"Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","externalIds":{"DBLP":"conf/eccv/LiuCLZLRZYSZZGL24","ArXiv":"2311.05437","DOI":"10.48550/arXiv.2311.05437","CorpusId":265067489},"title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"405247bdc023c6ab437bbf48555f2bf034d3328b","externalIds":{"DBLP":"journals/inffus/DingWHJGLP23","DOI":"10.1016/j.inffus.2023.101880","CorpusId":259447195},"title":"FTransCNN: Fusing Transformer and a CNN based on fuzzy logic for uncertain medical image segmentation"},{"paperId":"389fb015d72bbad3dc127093faef1a1fdebfc7b1","externalIds":{"DOI":"10.1037/dev0001623","CorpusId":264143656,"PubMed":"37843515"},"title":"A unified approach to demographic data collection for research with young children across diverse cultures."},{"paperId":"2d9310132cfe9046f4b61f6e90a5ef92c6e0ba71","externalIds":{"DBLP":"journals/corr/abs-2310-10765","ArXiv":"2310.10765","DOI":"10.48550/arXiv.2310.10765","CorpusId":264172353},"title":"BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys"},{"paperId":"44167d06dec02c8724fac5c03b6abb87b6dc36fc","externalIds":{"ArXiv":"2310.09909","DBLP":"journals/corr/abs-2310-09909","DOI":"10.48550/arXiv.2310.09909","CorpusId":264146744},"title":"Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis"},{"paperId":"1ddbd08ad8cf22a5c66c4242194c4286328533bf","externalIds":{"ArXiv":"2310.09478","DBLP":"journals/corr/abs-2310-09478","DOI":"10.48550/arXiv.2310.09478","CorpusId":264146906},"title":"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"},{"paperId":"477da0209e093448a8370862540182a1a77602fe","externalIds":{"DBLP":"conf/iclr/SungYB24","ArXiv":"2310.02998","DOI":"10.48550/arXiv.2310.02998","CorpusId":263620656},"title":"ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","externalIds":{"DBLP":"journals/corr/abs-2310-02239","ArXiv":"2310.02239","DOI":"10.48550/arXiv.2310.02239","CorpusId":263608981},"title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"8d99569e6d8cb0defeffb9803bc9d3a4512791b3","externalIds":{"DBLP":"journals/cmpb/HoyosART23","DOI":"10.1016/j.cmpb.2023.107829","CorpusId":263694139,"PubMed":"37837889"},"title":"Case studies of clinical decision-making through prescriptive models based on machine learning"},{"paperId":"a281094d05e96b7cca044fdd87ff7c3c65649e20","externalIds":{"DBLP":"journals/corr/abs-2309-10313","ArXiv":"2309.10313","DOI":"10.48550/arXiv.2309.10313","CorpusId":262055661},"title":"Investigating the Catastrophic Forgetting in Multimodal Large Language Models"},{"paperId":"af3ab5da98e0807784b57e321ed887a3666a8ab6","externalIds":{"DBLP":"journals/corr/abs-2309-10020","ArXiv":"2309.10020","DOI":"10.48550/arXiv.2309.10020","CorpusId":262055614},"title":"Multimodal Foundation Models: From Specialists to General-Purpose Assistants"},{"paperId":"fdba5cc6d687df86d0bbed368b9523266b98a04b","externalIds":{"DBLP":"journals/cbm/SeoniVSBMA23","DOI":"10.1016/j.compbiomed.2023.107441","CorpusId":261483812,"PubMed":"37683529"},"title":"Application of uncertainty quantification to artificial intelligence in healthcare: A review of last decade (2013-2023)"},{"paperId":"262337c19641ba3ec30821ed628c9e4b981e4bc4","externalIds":{"DOI":"10.1177/27523810231193337","CorpusId":261762602},"title":"Unleashing the power of meta-knowledge: Towards cumulative learning in interpreter training"},{"paperId":"754a7fc5c66f60258eadd69d21871858fbb9390b","externalIds":{"DBLP":"journals/artmed/BaumgartnerABBCCDEFGGHKLMMBNNRSS23","DOI":"10.1016/j.artmed.2023.102658","CorpusId":261548147,"PubMed":"37783540"},"title":"Fair and equitable AI in biomedical research and healthcare: Social science perspectives"},{"paperId":"916c798b34d55a78c47ae44906fe018a60a30613","externalIds":{"ArXiv":"2308.12488","DBLP":"conf/coling/0010CZGC24","ACL":"2024.lrec-main.693","DOI":"10.48550/arXiv.2308.12488","CorpusId":261100760},"title":"GPTEval: A Survey on Assessments of ChatGPT and GPT-4"},{"paperId":"20c02c51420a69ae39b74d8d73148417819dacc1","externalIds":{"DBLP":"journals/cogcom/HassijaCMSGHSSMH24","DOI":"10.1007/s12559-023-10179-8","CorpusId":261154217},"title":"Interpreting Black-Box Models: A Review on Explainable Artificial Intelligence"},{"paperId":"e3fd89a7f6b28973cfc68bfc51caebd8fb93f0bc","externalIds":{"DBLP":"journals/corr/abs-2308-09442","ArXiv":"2308.09442","DOI":"10.48550/arXiv.2308.09442","CorpusId":261030404},"title":"BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine"},{"paperId":"d9474e07c263b9e3b0a17a439bac53badf2c51e7","externalIds":{"DOI":"10.1057/s41599-023-01999-y","CorpusId":260927588},"title":"Age-related bias and artificial intelligence: a scoping review"},{"paperId":"338d8f3b199abcebc85f34016b0162ab3a9d5310","externalIds":{"DBLP":"journals/corr/abs-2308-07633","ArXiv":"2308.07633","DOI":"10.1162/tacl_a_00704","CorpusId":260900101},"title":"A Survey on Model Compression for Large Language Models"},{"paperId":"df0ddb588a200d095743e9d26fc4a9318619766e","externalIds":{"ArXiv":"2308.02463","DBLP":"journals/corr/abs-2308-02463","DOI":"10.48550/arXiv.2308.02463","CorpusId":260611504},"title":"Towards Generalist Foundation Model for Radiology"},{"paperId":"644afd030021f67bb379b91b9d2b1b177f1161e3","externalIds":{"DBLP":"journals/eswa/NamJ24","DOI":"10.1016/j.eswa.2023.121168","CorpusId":260876677},"title":"A survey on multimodal bidirectional machine learning translation of image and natural language processing"},{"paperId":"e9cb83fa64c279e6823a97cc02b7e3a4896847a7","externalIds":{"DBLP":"journals/ijon/LiHQTCWSZZHH23","DOI":"10.1016/j.neucom.2023.126720","CorpusId":261137176},"title":"Artificial intelligence accelerates multi-modal biomedical process: A Survey"},{"paperId":"2dbc94f8b99a855c9b395eaa9120053cf1930a3d","externalIds":{"DBLP":"journals/cmpb/AminizadehHTDNRTAU23","DOI":"10.1016/j.cmpb.2023.107745","CorpusId":260796587,"PubMed":"37579550"},"title":"The applications of machine learning techniques in medical data processing based on distributed computing and the Internet of Things"},{"paperId":"3a88020d756662b36d5c406a0e78a5082302f4e0","externalIds":{"DBLP":"journals/mia/ChenLWBSCPD25","ArXiv":"2307.15615","DOI":"10.48550/arXiv.2307.15615","CorpusId":260316174,"PubMed":"39612808"},"title":"A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond"},{"paperId":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","externalIds":{"ArXiv":"2307.15189","DBLP":"conf/ml4h/MoorHWYDLZRR23","DOI":"10.48550/arXiv.2307.15189","CorpusId":260316059},"title":"Med-Flamingo: a Multimodal Medical Few-shot Learner"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","externalIds":{"DBLP":"journals/corr/abs-2307-07362","ArXiv":"2307.07362","DOI":"10.1016/j.jbi.2023.104482","CorpusId":259924870,"PubMed":"37652343"},"title":"A scoping review on multimodal deep learning in biomedical images and texts"},{"paperId":"795f1de72332f2ba6e1d4c6300a586f1232cbe93","externalIds":{"DBLP":"journals/titb/NaseemKDK24","DOI":"10.1109/JBHI.2023.3294249","CorpusId":259832532,"PubMed":"37432797"},"title":"K-PathVQA: Knowledge-Aware Multimodal Representation for Pathology Visual Question Answering"},{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","externalIds":{"ArXiv":"2307.05314","DBLP":"conf/miccai/LiLHZZ23","DOI":"10.48550/arXiv.2307.05314","CorpusId":259765869},"title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"888728745dbb769e29ed475d4f7661eebe1a71cf","externalIds":{"DBLP":"journals/tist/ChangWWWYZCYWWYZCYYX24","ArXiv":"2307.03109","DOI":"10.1145/3641289","CorpusId":259360395},"title":"A Survey on Evaluation of Large Language Models"},{"paperId":"49ee175d5901febdf0463829d78a810a7090ae46","externalIds":{"PubMedCentral":"10376273","DOI":"10.3390/biology12071033","CorpusId":260220339,"PubMed":"37508462"},"title":"Transformer Architecture and Attention Mechanisms in Genome Data Analysis: A Comprehensive Review"},{"paperId":"a084474eab8710655da1a9f17e5d74d24ce6fba1","externalIds":{"DOI":"10.52339/tjet.v42i2.853","CorpusId":259633074},"title":"Deep Learning Model Compression Techniques: Advances, Opportunities, and Perspective"},{"paperId":"7cb09c77aab947c376df4fbcc4f2d877960f35d6","externalIds":{"DBLP":"conf/aaai/CaoCZYLX23","DOI":"10.1609/aaai.v37i1.25100","CorpusId":259670304},"title":"MMTN: Multi-Modal Memory Transformer Network for Image-Report Consistent Medical Report Generation"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"860b223f97b4c0e6eaa42d6be1184c97055fd848","externalIds":{"DBLP":"journals/inffus/ShaikTLXV24","ArXiv":"2306.11963","DOI":"10.1016/j.inffus.2023.102040","CorpusId":259212058},"title":"A survey of multimodal information fusion for smart healthcare: Mapping the journey from data to wisdom"},{"paperId":"3846d296a938dcc0a92784da76f9ef90d9de2e29","externalIds":{"DBLP":"journals/jbi/CaiLLLNW23","DOI":"10.1016/j.jbi.2023.104418","CorpusId":259119163,"PubMed":"37290540"},"title":"Integrating domain knowledge for biomedical text analysis into deep learning: A survey"},{"paperId":"cd012a00cf0f0f07170538e3f89732f65cb8df07","externalIds":{"DBLP":"conf/icassp/ChenXSWCXZX23","DOI":"10.1109/ICASSP49357.2023.10096411","CorpusId":258534734},"title":"A Progressive Neural Network for Acoustic Echo Cancellation"},{"paperId":"d85d7a205dfdde6cc41f46141756d14348557218","externalIds":{"DOI":"10.1038/s41551-023-01056-8","CorpusId":259277694,"PubMed":"37380750"},"title":"Algorithmic fairness in artificial intelligence for medicine and healthcare"},{"paperId":"c8a145ecdf84015a8a38ef21c3baa954fa84368e","externalIds":{"PubMedCentral":"10280591","DBLP":"journals/peerj-cs/LuLYYLZ23","DOI":"10.7717/peerj-cs.1400","CorpusId":259209374,"PubMed":"37346665"},"title":"The multi-modal fusion in visual question answering: a review of attention mechanisms"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","externalIds":{"DBLP":"journals/corr/abs-2305-16355","ArXiv":"2305.16355","ACL":"2023.tllm-1.2","DOI":"10.48550/arXiv.2305.16355","CorpusId":258947721},"title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"daf34122a0c38531aeeb55069ba98e564c263d53","externalIds":{"ArXiv":"2305.10799","DBLP":"conf/accv/ChenH24","DOI":"10.48550/arXiv.2305.10799","CorpusId":258762408},"title":"MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts"},{"paperId":"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","externalIds":{"ArXiv":"2305.11172","DBLP":"journals/corr/abs-2305-11172","DOI":"10.48550/arXiv.2305.11172","CorpusId":258762390},"title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"},{"paperId":"b1ef91c5541f88297c7551e1adf15fcee7987197","externalIds":{"DBLP":"conf/nips/YaromBCAHLOS23","ArXiv":"2305.10400","DOI":"10.48550/arXiv.2305.10400","CorpusId":258740893},"title":"What You See is What You Read? Improving Text-Image Alignment Evaluation"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","externalIds":{"DOI":"10.1002/mef2.43","CorpusId":258777310},"title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","externalIds":{"ArXiv":"2305.10415","DBLP":"journals/corr/abs-2305-10415","DOI":"10.48550/arXiv.2305.10415","CorpusId":258741360},"title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"81e7e82245c2f230eeb8aaaa1a2b2604c143754a","externalIds":{"ArXiv":"2305.04790","DBLP":"journals/corr/abs-2305-04790","DOI":"10.48550/arXiv.2305.04790","CorpusId":258557672},"title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"},{"paperId":"618f9c581f6003a79eff5ca2410e3be70bf4abd1","externalIds":{"DBLP":"journals/jbi/Murali0VN23","DOI":"10.1016/j.jbi.2023.104403","CorpusId":258897309,"PubMed":"37230406"},"title":"Towards electronic health record-based medical knowledge graph construction, completion, and applications: A literature study"},{"paperId":"e432f46393b45bb146000a92d7993b6f5ebc2493","externalIds":{"DBLP":"journals/caaitrit/RenWZ23","DOI":"10.1049/cit2.12216","CorpusId":258407510},"title":"Weakly supervised machine learning"},{"paperId":"741c7e78d0bd101b1fbf39d0c47fc519941bd275","externalIds":{"PubMedCentral":"10129875","DOI":"10.1117/1.JMI.10.6.061104","CorpusId":258361453,"PubMed":"37125409"},"title":"Toward fairness in artificial intelligence for medical image analysis: identification and mitigation of potential biases in the roadmap from data collection to model deployment"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"89c3783ebc99de414b98e82ca3bc932a81418052","externalIds":{"DBLP":"journals/ijcisys/LuDLYYZ23","DOI":"10.1007/s44196-023-00233-6","CorpusId":258052148},"title":"Multiscale Feature Extraction and Fusion of Image and Text in VQA"},{"paperId":"690df0820f35a47e1ce44f90e6ddb4132aa09267","externalIds":{"DBLP":"journals/pami/ZhangHJL24","ArXiv":"2304.00685","DOI":"10.1109/TPAMI.2024.3369699","CorpusId":257913547,"PubMed":"38408000"},"title":"Vision-Language Models for Vision Tasks: A Survey"},{"paperId":"a98862ffe4c18634a67a3df8a965a35e5e0d7ec8","externalIds":{"DOI":"10.1016/j.lindif.2023.102274","CorpusId":257445349},"title":"ChatGPT for good? On opportunities and challenges of large language models for education"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"61827838932597591901992a4d3a86b71b42ac69","externalIds":{"DOI":"10.3390/su15075930","CorpusId":257861968},"title":"A Study of CNN and Transfer Learning in Medical Imaging: Advantages, Challenges, Future Scope"},{"paperId":"8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c","externalIds":{"ArXiv":"2303.12712","DBLP":"journals/corr/abs-2303-12712","CorpusId":257663729},"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"23684a07517870cffd1f97fafbaae16ba22bd2b7","externalIds":{"ArXiv":"2303.11568","DBLP":"journals/titb/QiuLSPSZDLLXYWXL23","DOI":"10.1109/JBHI.2023.3316750","CorpusId":257636930,"PubMed":"37738186"},"title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future"},{"paperId":"6c3695030eb610071a81314b58204d49d3f0edba","externalIds":{"DBLP":"journals/tcbb/WuKZCNW24","DOI":"10.1109/TCBB.2023.3258455","CorpusId":257607963,"PubMed":"37030846"},"title":"CDT-CAD: Context-Aware Deformable Transformers for End-to-End Chest Abnormality Detection on X-Ray Images"},{"paperId":"8f3138f7ee5127faab265793be8ae278bc49d9b1","externalIds":{"DBLP":"conf/miccai/LinZZWZWX23","ArXiv":"2303.07240","DOI":"10.48550/arXiv.2303.07240","CorpusId":257496659},"title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"e177f4a4b613b56564d0e9c658445c34bf39f5b6","externalIds":{"DBLP":"journals/computers/LiLM23","DOI":"10.3390/computers12030060","CorpusId":257513218},"title":"Model Compression for Deep Neural Networks: A Survey"},{"paperId":"785650a805851c7e945523e495c5a523c60f72a4","externalIds":{"DBLP":"conf/miccai/SonsbeekDNSW23","ArXiv":"2303.05977","DOI":"10.48550/arXiv.2303.05977","CorpusId":257482250},"title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models"},{"paperId":"640ef43c4aaa75516acfcef569cd9e7819cef649","externalIds":{"DBLP":"journals/vc/SeniorSYR25","ArXiv":"2303.03761","DOI":"10.1007/s00371-024-03343-0","CorpusId":257378523},"title":"Graph neural networks in vision-language image understanding: a survey"},{"paperId":"483757dff12df441c6991dd5e7408d922fe01c3d","externalIds":{"DBLP":"journals/corr/abs-2303-03369","ArXiv":"2303.03369","DOI":"10.1109/CVPR52729.2023.01435","CorpusId":257365349},"title":"Multimodal Prompting with Missing Modalities for Visual Recognition"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"08846a61b7fb330641d5cc8d75b127291de3967d","externalIds":{"PubMedCentral":"9942039","DBLP":"journals/ijautcomp/QiuLCX23","DOI":"10.1007/s11633-022-1382-8","CorpusId":257076151},"title":"Pre-training in Medical Data: A Survey"},{"paperId":"746bb45433f6b24d3ae64d6cd51c4e9d00a0ffa7","externalIds":{"DBLP":"journals/corr/abs-2302-10035","ArXiv":"2302.10035","DOI":"10.1007/s11633-022-1410-8","CorpusId":257038341},"title":"Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"},{"paperId":"c384cfa9c992875a18c22e376e5d153b32d895b5","externalIds":{"DBLP":"journals/monet/SharmaPYSGS23","DOI":"10.1007/s11036-023-02104-y","CorpusId":257046056},"title":"Advancing Security in the Industrial Internet of Things Using Deep Progressive Neural Networks"},{"paperId":"569e323c84bcc01fc6562ea2c7084c7a2a00e703","externalIds":{"ArXiv":"2302.10909","DBLP":"journals/corr/abs-2302-10909","DOI":"10.48550/arXiv.2302.10909","CorpusId":257079226},"title":"Multi-modal Machine Learning in Engineering Design: A Review and Future Directions"},{"paperId":"b400f3f61eab83a6e667ac4b7b4ce5b804a34d35","externalIds":{"DOI":"10.1152/jn.00454.2022","CorpusId":256663031,"PubMed":"36752404"},"title":"Now and Then: How Our Understanding of Memory Replay Evolves."},{"paperId":"ecf13e7e2897800e3b4b3a16806bdf0c22f6cff9","externalIds":{"DBLP":"conf/elinfocom/ZhangK23","DOI":"10.1109/ICEIC57457.2023.10049971","CorpusId":257432237},"title":"A Survey on Attention mechanism in NLP"},{"paperId":"0ca79ce151db7cbe745377696aa8789acdb1c3aa","externalIds":{"DBLP":"journals/ton/ChenXKXL23","DOI":"10.1109/TNET.2022.3187885","CorpusId":250373772},"title":"Learning From FM Communications: Toward Accurate, Efficient, All-Terrain Vehicle Localization"},{"paperId":"64caaab51d8339f1b99874d3bddb79debbe661ca","externalIds":{"DBLP":"journals/corr/abs-2302-00402","ArXiv":"2302.00402","DOI":"10.48550/arXiv.2302.00402","CorpusId":256459873},"title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"edf1252b5d34e416422dd20a5789ddb56971748d","externalIds":{"DBLP":"journals/sensors/DhiraniMCN23","PubMedCentral":"9921682","DOI":"10.3390/s23031151","CorpusId":256188169,"PubMed":"36772190"},"title":"Ethical Dilemmas and Privacy Issues in Emerging Technologies: A Review"},{"paperId":"e709c991d71dd0789778cd9da9883c2dcd61d69b","externalIds":{"DOI":"10.1080/08820538.2023.2168486","CorpusId":255973197,"PubMed":"36651834"},"title":"Bias and Non-Diversity of Big Data in Artificial Intelligence: Focus on Retinal Diseases"},{"paperId":"e3da29b2fcdf541688bb949441a5847c6c46b1bb","externalIds":{"DBLP":"journals/npjdm/JoyceKSC23","PubMedCentral":"9849399","DOI":"10.1038/s41746-023-00751-9","CorpusId":255970305,"PubMed":"36653524"},"title":"Explainable artificial intelligence for mental health through transparency and interpretability for understandability"},{"paperId":"3b6f162394ba247790649c46b8e903d35d312352","externalIds":{"DOI":"10.1002/mp.16219","CorpusId":255748276,"PubMed":"36630691"},"title":"Semi-supervised training using cooperative labeling of weakly annotated data for nodule detection in chest CT."},{"paperId":"2553bf88f7781ea246ce30c98b00ef152f0b5961","externalIds":{"ArXiv":"2301.01893","DBLP":"journals/corr/abs-2301-01893","DOI":"10.1109/CVPR52729.2023.01054","CorpusId":255440399},"title":"GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods"},{"paperId":"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","externalIds":{"DBLP":"journals/corr/abs-2301-02228","ArXiv":"2301.02228","DOI":"10.1101/2023.01.10.23284412","CorpusId":255440664},"title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis"},{"paperId":"773949770fe0ac58ee92c9de1526bc220e3e845e","externalIds":{"DBLP":"journals/kbs/ZhaoZSWZQ23","DOI":"10.1016/j.knosys.2023.110280","CorpusId":255623351},"title":"Generative label fused network for image-text matching"},{"paperId":"71ac7ff37e3af1cdc0bc5253ac408d60272fb794","externalIds":{"DOI":"10.1016/j.autcon.2022.104663","CorpusId":253544256},"title":"Automated monitoring and evaluation of highway subgrade compaction quality using artificial neural networks"},{"paperId":"8c91a98bb5b69c9a3085521b77e0bdbc53609c25","externalIds":{"DBLP":"journals/is/Taha23","DOI":"10.1016/j.is.2023.102178","CorpusId":256049881},"title":"Semi-supervised and un-supervised clustering: A review and experimental evaluation"},{"paperId":"7462b855cd1e8997814f79c9a995d396c6da2ebd","externalIds":{"DBLP":"journals/cbm/GarceaSLM23","DOI":"10.1016/j.compbiomed.2022.106391","CorpusId":254520707,"PubMed":"36549032"},"title":"Data augmentation for medical imaging: A systematic literature review"},{"paperId":"eee74a9bbfc0c0dadbd59a0b04cf349f41e5779f","externalIds":{"DBLP":"journals/inffus/MaiSZH23","DOI":"10.1016/j.inffus.2022.11.003","CorpusId":253453253},"title":"Excavating multimodal correlation for representation learning"},{"paperId":"980fcee6365b8c537d7e2d53892ff16a8093d154","externalIds":{"ArXiv":"2210.14142","DBLP":"journals/corr/abs-2210-14142","DOI":"10.48550/arXiv.2210.14142","CorpusId":253107548},"title":"From colouring-in to pointillism: revisiting semantic segmentation supervision"},{"paperId":"6efc7397735b8ca43cf37ca3e9e6b6cde3533b3f","externalIds":{"DBLP":"conf/wacv/WangLLAB23","ArXiv":"2210.13591","DOI":"10.1109/WACV56688.2023.00113","CorpusId":253107301},"title":"Learning by Hallucinating: Vision-Language Pre-training with Weak Supervision"},{"paperId":"7e3ea8af81dc33c6960260c59249371c299405b7","externalIds":{"DBLP":"journals/remotesensing/HuangHFLLWN22","DOI":"10.3390/rs14205280","CorpusId":253142704},"title":"Developing a Dual-Stream Deep-Learning Neural Network Model for Improving County-Level Winter Wheat Yield Estimates in China"},{"paperId":"cdd9c1d23f9e89d5113f3e31821bb174c6a6afed","externalIds":{"DBLP":"journals/corr/abs-2210-10163","ArXiv":"2210.10163","ACL":"2022.emnlp-main.256","DOI":"10.48550/arXiv.2210.10163","CorpusId":252992913,"PubMed":"39144675"},"title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"},{"paperId":"b287a2765e5bceb732de39dafdf70594dc9cd664","externalIds":{"DBLP":"journals/ftcgv/GanLLWLG22","ArXiv":"2210.09263","DOI":"10.48550/arXiv.2210.09263","CorpusId":252918286},"title":"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"027b2285c29142cd7d7ec0ed6014ae2024998a33","externalIds":{"ArXiv":"2210.07503","DBLP":"journals/corr/abs-2210-07503","DOI":"10.1109/WACV56688.2023.00333","CorpusId":252907699},"title":"STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition"},{"paperId":"8dfc04006665065aac91c02668b2a4e725aa174d","externalIds":{"DBLP":"conf/mm/ZhuLZWM22","DOI":"10.1145/3503161.3548058","CorpusId":252783053},"title":"Image-Text Matching with Fine-Grained Relational Dependency and Bidirectional Attention-Based Generative Networks"},{"paperId":"be83cbda0b830a9c56dc0e25e53a09fbf955fcab","externalIds":{"DBLP":"conf/smc/WeiSQSZZ22","DOI":"10.1109/SMC53654.2022.9945109","CorpusId":253629797},"title":"BSAM: Research on image-text matching method based on Bert and self-attention mechanism"},{"paperId":"f5225015bcdad0a1daf7d205c67fc297fb9ec978","externalIds":{"ArXiv":"2210.04133","DBLP":"journals/corr/abs-2210-04133","DOI":"10.48550/arXiv.2210.04133","CorpusId":252780163},"title":"Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"c0aa6607ae32b019830dd3777d3d4f850f72ccf6","externalIds":{"DOI":"10.3390/math10193619","CorpusId":252856184},"title":"A Survey on Deep Transfer Learning and Beyond"},{"paperId":"35bc08d8a912e1d3c52934e9537ea3fa34e1c390","externalIds":{"DBLP":"journals/corr/abs-2209-15270","ArXiv":"2209.15270","DOI":"10.48550/arXiv.2209.15270","CorpusId":252668375},"title":"ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training"},{"paperId":"a4406282cf3f2f5d054efb6defa3c59a270710e5","externalIds":{"DOI":"10.1109/CEI57409.2022.9950150","CorpusId":253803769},"title":"A Survey of Image Caption Tasks"},{"paperId":"61b79fa29650e3fab431c991e79d7044a6c77778","externalIds":{"DBLP":"journals/pr/WanCZYTZ23","DOI":"10.1016/j.patcog.2022.109034","CorpusId":252269752},"title":"Low-rank 2D local discriminant graph embedding for robust image feature extraction"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","externalIds":{"ArXiv":"2208.10442","DBLP":"journals/corr/abs-2208-10442","DOI":"10.48550/arXiv.2208.10442","CorpusId":251719655},"title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"b1003e3f2807191ef4b9378a27580812271782b9","externalIds":{"ArXiv":"2208.09843","DBLP":"journals/corr/abs-2208-09843","DOI":"10.48550/arXiv.2208.09843","CorpusId":251718908},"title":"CODER: Coupled Diversity-Sensitive Momentum Contrastive Learning for Image-Text Retrieval"},{"paperId":"c3ba21c44e63bc1861ea3c23dd19d0d2a2944d6b","externalIds":{"DBLP":"conf/icpr/XingLXQJL22","DOI":"10.1109/ICPR56361.2022.9956394","CorpusId":254100790},"title":"Neural Network Decision-Making Criteria Consistency Analysis via Inputs Sensitivity"},{"paperId":"e2f90cee4466b2eb96f45714226a312e95cede3d","externalIds":{"DBLP":"journals/tmm/ChengWG23","ArXiv":"2208.09596","DOI":"10.1109/TMM.2022.3217384","CorpusId":251719137},"title":"Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks"},{"paperId":"5d09a799914ea0ce0a13af2b2ed71f3dad3f182c","externalIds":{"ArXiv":"2208.06551","DBLP":"conf/bigdataconf/HuCC23","DOI":"10.1109/BigData59044.2023.10386812","CorpusId":251564151},"title":"Exploiting Multiple Sequence Lengths in Fast End to End Training for Image Captioning"},{"paperId":"9a66f6b5143d24aed9d3f4a8f13aa4e089f3a866","externalIds":{"ArXiv":"2207.07568","DBLP":"journals/corr/abs-2207-07568","DOI":"10.48550/arXiv.2207.07568","CorpusId":250607752},"title":"Reasoning about Actions over Visual and Linguistic Modalities: A Survey"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","externalIds":{"ArXiv":"2207.04429","DBLP":"conf/corl/ShahOIL22","DOI":"10.48550/arXiv.2207.04429","CorpusId":250426345},"title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"cf472abd7f6351d2223234795d5cb218380da666","externalIds":{"DOI":"10.1109/ICEMIS56295.2022.9914173","CorpusId":252900543},"title":"Image Captioning Techniques: A Review"},{"paperId":"9406a68789bb9db1fe3a19ac7ff17d903ee8a9f4","externalIds":{"DBLP":"journals/eswa/ShaoZYDW22","DOI":"10.1016/j.eswa.2022.118221","CorpusId":251165327},"title":"Tracing the evolution of AI in the past decade and forecasting the emerging trends"},{"paperId":"b1c037a32ea0cb74232c84fab435d1392ef36f88","externalIds":{"DBLP":"journals/corr/abs-2206-14718","ArXiv":"2206.14718","DOI":"10.1109/TMI.2023.3291719","CorpusId":250113683,"PubMed":"37399157"},"title":"LViT: Language Meets Vision Transformer in Medical Image Segmentation"},{"paperId":"5b430df8b79b609503ebb0cb2fea346194fbc8fa","externalIds":{"DBLP":"conf/aaai/ZhangMZ022","DOI":"10.1609/aaai.v36i3.20235","CorpusId":250295044},"title":"Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text Matching"},{"paperId":"4c3dada9bc0f95578877adae5ddadfaa25cc1f21","externalIds":{"DBLP":"journals/corr/abs-2206-11053","ArXiv":"2206.11053","DOI":"10.48550/arXiv.2206.11053","CorpusId":249926686},"title":"Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer"},{"paperId":"3b39432ff537ec24a8263c598b9d195861d4db74","externalIds":{"DBLP":"journals/cssp/SureshJS22","DOI":"10.1007/s00034-022-02050-2","CorpusId":249805674},"title":"Image Captioning Encoder–Decoder Models Using CNN-RNN Architectures: A Comparative Study"},{"paperId":"622428f5122ad12a40229e1768ecb929fd747ee7","externalIds":{"ArXiv":"2206.06488","DBLP":"journals/pami/XuZC23","DOI":"10.1109/TPAMI.2023.3275156","CorpusId":249642175,"PubMed":"37167049"},"title":"Multimodal Learning With Transformers: A Survey"},{"paperId":"4ea12bc8dab90e3b7b01b19148ff37ad325f54cd","externalIds":{"ArXiv":"2206.04670","DBLP":"conf/nips/QianLPMHEG22","DOI":"10.48550/arXiv.2206.04670","CorpusId":249538578},"title":"PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies"},{"paperId":"4145956f67bccf55587649ccf0a8debc1533e4e5","externalIds":{"PubMedCentral":"9201625","DOI":"10.3389/fnins.2022.876065","CorpusId":249243992,"PubMed":"35720715"},"title":"O-Net: A Novel Framework With Deep Fusion of CNN and Transformer for Simultaneous Segmentation and Classification"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","externalIds":{"DBLP":"journals/corr/abs-2205-14100","ArXiv":"2205.14100","DOI":"10.48550/arXiv.2205.14100","CorpusId":249152323},"title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"f5c165b6317896a65151050201c737536fa17c31","externalIds":{"ArXiv":"2205.12005","DBLP":"conf/emnlp/LiXTWYBYCXCZHHZ22","ACL":"2022.emnlp-main.488","DOI":"10.48550/arXiv.2205.12005","CorpusId":249018141},"title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections"},{"paperId":"0bda8bd11d5ec2eefaa5fb8b3f26cc656f7edef7","externalIds":{"DBLP":"journals/mta/YusufFM22","DOI":"10.1007/s11042-022-13065-x","CorpusId":248584445},"title":"Evaluation of graph convolutional networks performance for visual question answering on reasoning datasets"},{"paperId":"b60879dda0183160c9d0a611cb7e381e6942cf75","externalIds":{"DBLP":"journals/corr/abs-2205-01818","ArXiv":"2205.01818","DOI":"10.48550/arXiv.2205.01818","CorpusId":248512950},"title":"i-Code: An Integrative and Composable Multimodal Learning Framework"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"276d4af3170490641576ec80c8b735a0a135a5fa","externalIds":{"DBLP":"journals/corr/abs-2204-13707","ArXiv":"2204.13707","DOI":"10.1145/3477495.3532064","CorpusId":248476361},"title":"Tag-assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities"},{"paperId":"66ee488cf3dad5bb83804124367460edddd3c271","externalIds":{"DBLP":"conf/ijcai/LongCHY22","ArXiv":"2204.07356","DOI":"10.48550/arXiv.2204.07356","CorpusId":248218612},"title":"Vision-and-Language Pretrained Models: A Survey"},{"paperId":"834b5b5b25e99186f900a7eb1c8d641caf024fcb","externalIds":{"DBLP":"conf/cvpr/0002R0T022","ArXiv":"2204.05454","DOI":"10.1109/CVPR52688.2022.01764","CorpusId":248118952},"title":"Are Multimodal Transformers Robust to Missing Modality?"},{"paperId":"3f968b0e35efb84a2e14449e89765239a898bb66","externalIds":{"DBLP":"journals/air/YusufFM22","DOI":"10.1007/s10462-022-10151-2","CorpusId":248078126},"title":"An analysis of graph convolutional networks and recent datasets for visual question answering"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5c72636582b0a429c8440e31defda76137ef2072","externalIds":{"ArXiv":"2203.14713","DBLP":"conf/ijcai/CaoLLNZ22","DOI":"10.48550/arXiv.2203.14713","CorpusId":247763152},"title":"Image-text Retrieval: A Survey on Recent Research and Development"},{"paperId":"c10370810c8c5ccf12ae5a604b0f23601f90c4b2","externalIds":{"DBLP":"conf/eccv/0001K0SC022","ArXiv":"2203.14395","DOI":"10.48550/arXiv.2203.14395","CorpusId":247762352},"title":"Single-Stream Multi-Level Alignment for Vision-Language Pretraining"},{"paperId":"0368102a9510ab93ab500322dbc7709b0a8a6e1b","externalIds":{"DBLP":"journals/csur/ShahbaziLAJ23","ArXiv":"2203.11852","DOI":"10.1145/3588433","CorpusId":257558858},"title":"Representation Bias in Data: A Survey on Identification and Resolution Techniques"},{"paperId":"9fb3070e60e0df8db6b7b1ecdd6e379481e22898","externalIds":{"DBLP":"conf/acl/LiGNXLLWW22","ACL":"2022.findings-acl.251","ArXiv":"2203.09067","DOI":"10.48550/arXiv.2203.09067","CorpusId":247519008},"title":"UNIMO-2: End-to-End Unified Vision-Language Grounded Learning"},{"paperId":"72aae728376e81bb2c2c014bc9e028124b29157b","externalIds":{"ArXiv":"2203.05349","DBLP":"journals/corr/abs-2203-05349","DOI":"10.48550/arXiv.2203.05349","CorpusId":247362924},"title":"Two-stream Hierarchical Similarity Reasoning for Image-text Matching"},{"paperId":"c47b51f52e6a53bb1b0d15fe40bec0e8f9100761","externalIds":{"DBLP":"journals/corr/abs-2203-01445","ArXiv":"2203.01445","DOI":"10.48550/arXiv.2203.01445","CorpusId":247223085},"title":"LILE: Look In-Depth before Looking Elsewhere - A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives"},{"paperId":"835946f65d71bd1a317f284a4e365f67948fe082","externalIds":{"DBLP":"conf/iciit/TangC22","DOI":"10.1145/3524889.3524897","CorpusId":248497557},"title":"A Survey on Human Action Recognition based on Attention Mechanism"},{"paperId":"42bad1b72259aa1ff70d7ce2539220a83f1af9a4","externalIds":{"DBLP":"journals/corr/abs-2202-12165","ArXiv":"2202.12165","DOI":"10.1016/j.imed.2022.07.002","CorpusId":247084362},"title":"Transformers in Medical Image Analysis: A Review"},{"paperId":"6fca4e69d8d7fec0b1fe118e769017b3610d1bd5","externalIds":{"DBLP":"journals/corr/abs-2207-02127","ArXiv":"2207.02127","DOI":"10.1080/01691864.2022.2035253","CorpusId":247038603},"title":"A survey of multimodal deep generative models"},{"paperId":"65a01b760850d82505c2a04faf84a3e8c50398fe","externalIds":{"DBLP":"journals/tomccap/JabeenLSOLJ23","ArXiv":"2202.09195","DOI":"10.1145/3545572","CorpusId":246996526},"title":"A Review on Methods and Applications in Multimodal Deep Learning"},{"paperId":"8d46f33a5d07c205a275801889a1adabf144bd37","externalIds":{"DBLP":"journals/corr/abs-2202-09263","ArXiv":"2202.09263","DOI":"10.1109/icassp43922.2022.9746924","CorpusId":246996708},"title":"Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion Recognition?"},{"paperId":"04248a087a834af24bfe001c9fc9ea28dab63c26","externalIds":{"DBLP":"conf/ijcai/DuLLZ22","ArXiv":"2202.10936","DOI":"10.24963/ijcai.2022/762","CorpusId":247026006},"title":"A Survey of Vision-Language Pre-Trained Models"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","externalIds":{"DBLP":"conf/icml/WangYMLBLMZZY22","ArXiv":"2202.03052","CorpusId":246634906},"title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"a37bf9d65a4d1ebf95c79b3ab973bb7a8019ac3e","externalIds":{"DBLP":"journals/corr/abs-2201-12944","ArXiv":"2201.12944","DOI":"10.1145/3617592","CorpusId":246430542},"title":"Deep Learning Approaches on Image Captioning: A Review"},{"paperId":"59cb81d7763d8de23d9b4144c16287cb6808da1d","externalIds":{"ArXiv":"2201.12382","DBLP":"journals/csur/MalkinskiM25","DOI":"10.1145/3715093","CorpusId":246430277},"title":"Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"c49eab0a53473568098b12f0ffdc4469bcf19339","externalIds":{"PubMedCentral":"8921642","DBLP":"journals/bib/StahlschmidtUS22","DOI":"10.1093/bib/bbab569","CorpusId":246361760,"PubMed":"35089332"},"title":"Multimodal deep learning for biomedical data fusion: a review"},{"paperId":"cf3b44d5c0257b9b1ff9ea0c04da307cc426a811","externalIds":{"DBLP":"journals/tcyb/ChengWWHJTZ22","DOI":"10.1109/TCYB.2022.3142013","CorpusId":246286287,"PubMed":"35077387"},"title":"Visual Relationship Detection: A Survey"},{"paperId":"0460543af553ef57717d7cc0b3f5066727392c58","externalIds":{"PubMedCentral":"9463525","DOI":"10.1016/j.ebiom.2022.104250","CorpusId":246278869,"PubMed":"36084616"},"title":"Algorithmic fairness in computational medicine"},{"paperId":"24aa57dae649b6683d8f5bc8deaf2ff549cdacc4","externalIds":{"ArXiv":"2201.09873","DBLP":"journals/mia/ShamshadKZKHKF23","DOI":"10.1016/j.media.2023.102802","CorpusId":246240729,"PubMed":"37315483"},"title":"Transformers in Medical Imaging: A Survey"},{"paperId":"7caaafd5a3ee033c98e792c7ea5b699d005753d5","externalIds":{"DBLP":"journals/corr/abs-2201-08164","ArXiv":"2201.08164","DOI":"10.1145/3583558","CorpusId":246063780},"title":"From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI"},{"paperId":"3f43b4239c6955b4c6647c0801fbbbcdea91a320","externalIds":{"DBLP":"conf/cvpr/BitenLXAM22","ArXiv":"2112.12494","DOI":"10.1109/CVPR52688.2022.01605","CorpusId":245425054},"title":"LaTr: Layout-Aware Transformer for Scene-Text VQA"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","externalIds":{"DBLP":"conf/cvpr/SinghHGCGRK22","ArXiv":"2112.04482","DOI":"10.1109/CVPR52688.2022.01519","CorpusId":244954250},"title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"a2502d2cd7144c5e2bc1d0d7ec37d2c84b37d381","externalIds":{"ArXiv":"2111.14447","DBLP":"journals/corr/abs-2111-14447","DOI":"10.1109/CVPR52688.2022.01739","CorpusId":244714558},"title":"ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic"},{"paperId":"3ea60cbce6c9065661d207fccf021c5d58a83f01","externalIds":{"DBLP":"conf/cvpr/0006GWY0LW22","ArXiv":"2111.12233","DOI":"10.1109/CVPR52688.2022.01745","CorpusId":244527510},"title":"Scaling Up Vision-Language Pretraining for Image Captioning"},{"paperId":"fbb4f897dcedfd754223d9c1e83f0906d001b42d","externalIds":{"DBLP":"journals/iet-ipr/LuoCJZS22","DOI":"10.1049/ipr2.12367","CorpusId":244511278},"title":"A thorough review of models, evaluation metrics, and datasets on image captioning"},{"paperId":"da4261a957eaa96bf626e9641ef68ebed1d5333f","externalIds":{"DBLP":"journals/corr/abs-2111-11431","ArXiv":"2111.11431","CorpusId":237262876},"title":"RedCaps: web-curated image-text data created by the people, for the people"},{"paperId":"45f686be3b96302ede327645227134e1c304dbab","externalIds":{"ArXiv":"2111.07624","DBLP":"journals/cvm/GuoXLLJMZMCH22","DOI":"10.1007/s41095-022-0271-y","CorpusId":244117862},"title":"Attention mechanisms in computer vision: A survey"},{"paperId":"333212e246fb65f7c9d43862021e78f007c48449","externalIds":{"DBLP":"journals/corr/abs-2111-06091","ArXiv":"2111.06091","DOI":"10.1109/TNNLS.2022.3227717","CorpusId":243985875,"PubMed":"37015131"},"title":"A Survey of Visual Transformers"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","externalIds":{"ArXiv":"2111.07783","DBLP":"journals/corr/abs-2111-07783","CorpusId":244117525},"title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"e82dbb208305f1614555342a5dc2231822c9285f","externalIds":{"DBLP":"journals/natmi/ZhouCZLWY22","ArXiv":"2111.03452","DOI":"10.1038/s42256-021-00425-9","CorpusId":242479260},"title":"Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","externalIds":{"DBLP":"journals/corr/abs-2111-02358","ArXiv":"2111.02358","CorpusId":241035439},"title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"fab909f3f35d289f0a780bae182c92ca1fd75b32","externalIds":{"DBLP":"journals/corr/abs-2111-02172","ArXiv":"2111.02172","CorpusId":241035560},"title":"A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition"},{"paperId":"2b8c2c708500a6603f039a76a9c843f7fddf4225","externalIds":{"DBLP":"journals/tnn/ShiYL24","ArXiv":"2111.01872","DOI":"10.1109/TNNLS.2023.3263594","CorpusId":249494399,"PubMed":"37037249"},"title":"Towards Fairness-Aware Federated Learning"},{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","externalIds":{"ArXiv":"2111.01243","DBLP":"journals/corr/abs-2111-01243","DOI":"10.1145/3605943","CorpusId":240420063},"title":"Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"},{"paperId":"6815b0621436a0594219f7db23ec66c800f392a0","externalIds":{"ArXiv":"2111.02398","DBLP":"journals/corr/abs-2111-02398","DOI":"10.1016/j.compbiomed.2021.105111","CorpusId":242758341,"PubMed":"34891095"},"title":"Transparency of Deep Neural Networks for Medical Image Analysis: A Review of Interpretability Methods"},{"paperId":"09f2b1f1bd313cf9183c138fca8f17bb228b4435","externalIds":{"DBLP":"journals/corr/abs-2110-02526","ArXiv":"2110.02526","DOI":"10.1109/CVPRW56347.2022.00502","CorpusId":238407862},"title":"Coarse-to-Fine Reasoning for Visual Question Answering"},{"paperId":"335bb73b9f316b9a1617f1215f2f213bb902b85f","externalIds":{"DBLP":"conf/iccv/KantMBPA21","DOI":"10.1109/ICCV48922.2021.00163","CorpusId":240158334},"title":"Contrast and Classify: Training Robust VQA Models"},{"paperId":"0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d","externalIds":{"DBLP":"conf/iccv/HuangSLY21","DOI":"10.1109/ICCV48922.2021.00391","CorpusId":244128455},"title":"GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"},{"paperId":"dd4dd3ed1a95beb6e6712ea356a49d1ab818f616","externalIds":{"DBLP":"journals/corr/abs-2109-09920","ArXiv":"2109.09920","DOI":"10.1016/j.aiopen.2022.01.001","CorpusId":237581134},"title":"Survey: Transformer based Video-Language Pre-training"},{"paperId":"e90e338b771c9bb9884a4b8e184cad14598fe0ce","externalIds":{"PubMedCentral":"8455966","DOI":"10.1002/ccr3.4843","CorpusId":237637731,"PubMed":"34584711"},"title":"Conventional osteosarcoma of the mandible: Report of a rare case"},{"paperId":"aa340bb5d5282aab6484047fd3c0d2dee5a71dd3","externalIds":{"MAG":"3134976640","DBLP":"journals/inffus/JiangMXSG21","DOI":"10.1016/J.INFFUS.2021.02.012","CorpusId":233538611},"title":"A review of multimodal image matching: Methods and applications"},{"paperId":"eadb1e7da375939e25083ae3936c4f4ef1f2a719","externalIds":{"DBLP":"journals/csur/MadsenRC23","ArXiv":"2108.04840","DOI":"10.1145/3546577","CorpusId":236976388},"title":"Post-hoc Interpretability for Neural NLP: A Survey"},{"paperId":"4b8f5c922932377dd21d804183c2a870b6d628bc","externalIds":{"ArXiv":"2107.13782","DBLP":"journals/inffus/RahateWRK22","DOI":"10.1016/j.inffus.2021.12.003","CorpusId":236493420},"title":"Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions"},{"paperId":"3146cfab22f88ffed4befa06e8b268d60f479017","externalIds":{"DOI":"10.1016/j.psychres.2021.114135","CorpusId":236217063,"PubMed":"34343877"},"title":"Detecting formal thought disorder by deep contextualized word representations"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","externalIds":{"DBLP":"journals/corr/abs-2107-07651","ArXiv":"2107.07651","CorpusId":236034189},"title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"1c83f3f9789df43bf937ae2618721e2da83dcc06","externalIds":{"DBLP":"journals/pami/StefaniniCBCFC23","ArXiv":"2107.06912","DOI":"10.1109/TPAMI.2022.3148210","CorpusId":244772950,"PubMed":"35130142"},"title":"From Show to Tell: A Survey on Deep Learning-Based Image Captioning"},{"paperId":"dc15eec5d08901415ec1513063a2b86a435a396b","externalIds":{"MAG":"3024810126","DBLP":"journals/expert/AlademR21","DOI":"10.1109/MIS.2020.2993266","CorpusId":219438553},"title":"A Single-Stream Segmentation and Depth Prediction CNN for Autonomous Driving"},{"paperId":"b6c14f0629a59f34b14abd4c43f6b17397eff6be","externalIds":{"DBLP":"journals/corr/abs-2106-13033","ArXiv":"2106.13033","CorpusId":235624057},"title":"A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021"},{"paperId":"2a9b33f66ccc3806af58bdab2319559f4f9d2c5e","externalIds":{"PubMedCentral":"8192112","DBLP":"journals/vc/BayoudhKHM22","DOI":"10.1007/s00371-021-02166-7","CorpusId":235410640,"PubMed":"34131356"},"title":"A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets"},{"paperId":"14853384f24ae0e044a53542206cdac4735d9a01","externalIds":{"ArXiv":"2106.02400","DBLP":"journals/corr/abs-2106-02400","DOI":"10.3233/FAIA210049","CorpusId":235352551},"title":"A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval"},{"paperId":"e435d753bf39b6d6632b553282fff091d7942316","externalIds":{"DBLP":"journals/tetci/TaoC21","MAG":"2950198253","DOI":"10.1109/TETCI.2019.2917704","CorpusId":196200430},"title":"Resilient Learning of Computational Models With Noisy Labels"},{"paperId":"7c92ce94217f89894b832c95171de0e73896d4ce","externalIds":{"DBLP":"journals/eswa/SenKDD21","MAG":"3153114498","DOI":"10.1016/J.ESWA.2021.115043","CorpusId":234799636},"title":"RDFM: An alternative approach for representing, storing, and maintaining meta-knowledge in web of data"},{"paperId":"b1905ffd714d5e7e14eff99ced51214c2d22a3bd","externalIds":{"DBLP":"conf/emnlp/IkiA21","ArXiv":"2104.08066","ACL":"2021.emnlp-main.167","DOI":"10.18653/v1/2021.emnlp-main.167","CorpusId":237604959},"title":"Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models"},{"paperId":"b43b69a6d303fe3c349d5ecf5920ff1dd47ec650","externalIds":{"DBLP":"journals/pr/LiuZZLHTL21","MAG":"3152635550","DOI":"10.1016/J.PATCOG.2021.107956","CorpusId":234862036},"title":"Dual self-attention with co-attention networks for visual question answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","externalIds":{"DBLP":"journals/corr/abs-2104-01394","ArXiv":"2104.01394","DOI":"10.1109/ISBI48211.2021.9434063","CorpusId":233024902},"title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"a56bf7ee9a56d8f84079684339a953c2df9ce76b","externalIds":{"MAG":"3146366485","DBLP":"journals/ijon/NiuZY21","DOI":"10.1016/J.NEUCOM.2021.03.091","CorpusId":233562906},"title":"A review on the attention mechanism of deep learning"},{"paperId":"f7db2e2b181df60be90fd602b1aa306e4ca8b342","externalIds":{"DBLP":"journals/corr/abs-2103-06561","ArXiv":"2103.06561","CorpusId":232185258},"title":"WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training"},{"paperId":"31d20462d35059502ffeb13e1afbba11a81c7d41","externalIds":{"DBLP":"conf/aaai/MaRZTWP21","ArXiv":"2103.05677","DOI":"10.1609/aaai.v35i3.16330","CorpusId":232170317},"title":"SMIL: Multimodal Learning with Severely Missing Modality"},{"paperId":"98e565fa06f6c7bf7c46833b5106b26dc45130c4","externalIds":{"ArXiv":"2103.01913","DBLP":"conf/sigir/Srinivasan0CBN21","DOI":"10.1145/3404835.3463257","CorpusId":232092726},"title":"WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","externalIds":{"DBLP":"journals/corr/abs-2102-09542","ArXiv":"2102.09542","DOI":"10.1109/ISBI48211.2021.9434010","CorpusId":231951663},"title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","externalIds":{"ArXiv":"2102.05918","DBLP":"conf/icml/JiaYXCPPLSLD21","CorpusId":231879586},"title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","externalIds":{"ArXiv":"2102.03334","DBLP":"journals/corr/abs-2102-03334","CorpusId":231839613},"title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"150435a8291acdd3542f89951223e19898434be9","externalIds":{"PubMedCentral":"7820237","DOI":"10.1038/s41598-021-81554-4","CorpusId":231677366,"PubMed":"33479405"},"title":"A combined convolutional and recurrent neural network for enhanced glaucoma detection"},{"paperId":"6d864a6659e2c503cd6d28d05593f0603b9a48bd","externalIds":{"ArXiv":"2101.01368","DBLP":"conf/aaai/DiaoZML21","DOI":"10.1609/aaai.v35i2.16209","CorpusId":230523667},"title":"Similarity Reasoning and Filtration for Image-Text Matching"},{"paperId":"3a906b77fa218adc171fecb28bb81c24c14dcc7b","externalIds":{"DBLP":"journals/corr/abs-2101-01169","ArXiv":"2101.01169","DOI":"10.1145/3505244","CorpusId":230435805},"title":"Transformers in Vision: A Survey"},{"paperId":"0f25b3a5a8b7cd554b294732b58c81ef25bc6b71","externalIds":{"DBLP":"journals/evi/SarvamangalaK22","PubMedCentral":"7778711","DOI":"10.1007/s12065-020-00540-3","CorpusId":230285214,"PubMed":"33425040"},"title":"Convolutional neural networks in medical image understanding: a survey"},{"paperId":"5e5fbc41106db9acaaf3a365801051e477f0e984","externalIds":{"ArXiv":"2012.15409","DBLP":"journals/corr/abs-2012-15409","ACL":"2021.acl-long.202","DOI":"10.18653/v1/2021.acl-long.202","CorpusId":229924402},"title":"UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"},{"paperId":"d40c77c010c8dbef6142903a02f2a73a85012d5d","externalIds":{"ArXiv":"2012.12556","DBLP":"journals/corr/abs-2012-12556","DOI":"10.1109/TPAMI.2022.3152247","CorpusId":236986986,"PubMed":"35180075"},"title":"A Survey on Vision Transformer"},{"paperId":"bf2f223b2f4c425275f6b31f9e0111af32b41882","externalIds":{"DBLP":"journals/tip/ZhaoWL21","MAG":"3110761028","DOI":"10.1109/TIP.2020.3042086","CorpusId":228169159,"PubMed":"33306468"},"title":"Cross-Domain Image Captioning via Cross-Modal Retrieval and Model Adaptation"},{"paperId":"c09f44e0088342ec618c7a2deeab1526d73b2d6b","externalIds":{"PubMedCentral":"8600147","MAG":"3112501082","DBLP":"journals/patterns/PaulladaRBDH21","ArXiv":"2012.05345","DOI":"10.1016/j.patter.2021.100336","CorpusId":228084012,"PubMed":"34820643"},"title":"Data and its (dis)contents: A survey of dataset development and use in machine learning research"},{"paperId":"8deceb13cb3afcfbaab06a2c655f1935445635fe","externalIds":{"DBLP":"conf/cvpr/YangLW0FWZ0L21","MAG":"3112156821","ArXiv":"2012.04638","DOI":"10.1109/CVPR46437.2021.00864","CorpusId":227736593},"title":"TAP: Text-Aware Pre-training for Text-VQA and Text-Caption"},{"paperId":"dec2f6d3215de9aa2d87d358b7933fb21eeb3bc0","externalIds":{"DBLP":"journals/tacl/BugliarelloCOE21","ArXiv":"2011.15124","DOI":"10.1162/tacl_a_00408","CorpusId":227238841},"title":"Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs"},{"paperId":"635290858e4ece0ca019bb439e353e8deec18d14","externalIds":{"PubMedCentral":"8236074","MAG":"3108932871","DOI":"10.1088/1361-6560/abcd17","CorpusId":227158566,"PubMed":"33227719"},"title":"Interpretation and Visualization Techniques for Deep Learning Models in Medical Imaging"},{"paperId":"99218731d311e97da3608ad8dcd031add7826ba8","externalIds":{"DBLP":"journals/tmm/YangFW021","MAG":"3095024162","DOI":"10.1109/tmm.2020.3035277","CorpusId":229272644},"title":"Image-Text Multimodal Emotion Classification via Multi-View Attentional Network"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","externalIds":{"DBLP":"journals/corr/abs-2010-06000","ArXiv":"2010.06000","ACL":"2020.findings-emnlp.191","MAG":"3104050923","DOI":"10.18653/v1/2020.findings-emnlp.191","CorpusId":222310689},"title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"39ca8f8ff28cc640e3b41a6bd7814ab85c586504","externalIds":{"DBLP":"journals/corr/abs-2010-04159","MAG":"3092462694","ArXiv":"2010.04159","CorpusId":222208633},"title":"Deformable DETR: Deformable Transformers for End-to-End Object Detection"},{"paperId":"1b7bb161b6a06b1b297732516686aaf350cf5fb8","externalIds":{"ArXiv":"2010.03060","DBLP":"journals/titb/LiangGZXWKJ22","DOI":"10.1109/JBHI.2021.3110805","CorpusId":234358677,"PubMed":"34495856"},"title":"Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging"},{"paperId":"b99f0d2dfe0997920c6775e62f59a365a72071d1","externalIds":{"MAG":"3018530662","DOI":"10.1016/j.jclepro.2020.121847","CorpusId":219010215},"title":"Single stream or dual stream? A game-theoretic approach to the sustainable evolution in waste recycling systems"},{"paperId":"b0903f184dfccc813d8165d0f69c87f79520014e","externalIds":{"ArXiv":"2009.08395","DBLP":"journals/corr/abs-2009-08395","MAG":"3086789505","DOI":"10.1007/978-3-030-66840-2_109","CorpusId":221761623},"title":"A Multimodal Memes Classification: A Survey and Open Research Issues"},{"paperId":"b38763d5af497b239cb7a05b751b217e939f1b94","externalIds":{"DBLP":"journals/mta/YaoWWZ22","MAG":"3081442134","DOI":"10.1007/s11042-020-09634-7","CorpusId":221284238},"title":"A comprehensive survey on convolutional neural network in medical image analysis"},{"paperId":"4043785dacd1c04ed93ec1c08ecf779f4e1717fc","externalIds":{"DBLP":"journals/pieee/ZhouGDDGMPRS21","ArXiv":"2008.09104","MAG":"3074741277","DOI":"10.1109/JPROC.2021.3054390","CorpusId":221187065,"PubMed":"37786449"},"title":"A Review of Deep Learning in Medical Imaging: Imaging Traits, Technology Trends, Case Studies With Progress Highlights, and Future Promises"},{"paperId":"7e79e6c3d3bf18078d39125921f91ba8dd396eb2","externalIds":{"DBLP":"journals/pacmhci/MiceliSY20","ArXiv":"2007.14886","MAG":"3045524757","DOI":"10.1145/3415186","CorpusId":220845859},"title":"Between Subjectivity and Imposition"},{"paperId":"bd79c3b314e2c1048c8083f4cca66b9f0a7620e7","externalIds":{"DBLP":"journals/corr/abs-2007-02790","ArXiv":"2007.02790","MAG":"3038470222","DOI":"10.1007/978-3-030-59716-0_22","CorpusId":220364511,"PubMed":"33283210"},"title":"Adversarial Uni- and Multi-modal Stream Networks for Multimodal Image Registration"},{"paperId":"94511575866a1c2e643afb37758ff4384d9f4afe","externalIds":{"MAG":"3036053874","ArXiv":"2006.10763","DOI":"10.3847/1538-4357/abeb18","CorpusId":219955507},"title":"Broken into Pieces: ATLAS and Aliqa Uma as One Single Stream"},{"paperId":"ce352c6c413604d7ee539f60d38fffae52c89540","externalIds":{"DBLP":"journals/corr/abs-2006-00753","ArXiv":"2006.00753","MAG":"3029601968","DOI":"10.1109/TPAMI.2021.3132034","CorpusId":219177178,"PubMed":"34855584"},"title":"Structured Multimodal Attentions for TextVQA"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","externalIds":{"MAG":"3096609285","DBLP":"conf/eccv/CarionMSUKZ20","ArXiv":"2005.12872","DOI":"10.1007/978-3-030-58452-8_13","CorpusId":218889832},"title":"End-to-End Object Detection with Transformers"},{"paperId":"43d0b7e69405f8e451ecf3c4ed2c897c8a4dc123","externalIds":{"MAG":"3009772231","DOI":"10.1016/j.isprsjprs.2020.02.022","CorpusId":216432386},"title":"A label-noise robust active learning sample collection method for multi-temporal urban land-cover classification and change analysis"},{"paperId":"95a48aa25a6f735df13b3c63ba36ad1bb983972e","externalIds":{"DBLP":"journals/pacmhci/HongHB20","MAG":"3019489177","ArXiv":"2004.11440","DOI":"10.1145/3392878","CorpusId":216144597},"title":"Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","externalIds":{"MAG":"3014611590","DBLP":"journals/corr/abs-2004-00849","ArXiv":"2004.00849","CorpusId":214775221},"title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"f0ed174401beb546cf1034fe804b635125dfd52d","externalIds":{"DBLP":"journals/corr/abs-2004-00218","MAG":"3083622693","ArXiv":"2004.00218","PubMedCentral":"7570704","DOI":"10.3390/s20185097","CorpusId":214743032,"PubMed":"32906819"},"title":"3D Deep Learning on Medical Images: A Review"},{"paperId":"4c7c70760029e813ef76ea5a578174d2d3ec1490","externalIds":{"DBLP":"journals/neco/GaoLCZ20","MAG":"3011727199","DOI":"10.1162/neco_a_01273","CorpusId":212748233,"PubMed":"32186998"},"title":"A Survey on Deep Learning for Multimodal Data Fusion"},{"paperId":"1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0","externalIds":{"MAG":"3009445007","DBLP":"conf/nlpcc/XiaHDZJSCBZ21","ArXiv":"2003.01473","DOI":"10.1007/978-3-030-88480-2_63","CorpusId":211817758},"title":"XGPT: Cross-modal Generative Pre-Training for Image Captioning"},{"paperId":"e4ba3a265763677dfb68567aa6b62cd5fe4a633b","externalIds":{"DBLP":"journals/ipm/LiuLHGT20","MAG":"2996421194","DOI":"10.1016/j.ipm.2019.102178","CorpusId":213501138},"title":"Image caption generation with dual attention mechanism"},{"paperId":"8283d4e2508c9a250b84118bacde9d0014711796","externalIds":{"DBLP":"journals/ecoi/GarciaA20","MAG":"2996713777","DOI":"10.1016/j.ecoinf.2019.101039","CorpusId":212749368},"title":"Shapley additive explanations for NO2 forecasting"},{"paperId":"3893783a815a5d46a26f154683115d4195b1b004","externalIds":{"MAG":"3007935259","DOI":"10.1148/radiol.2020192224","CorpusId":211160137,"PubMed":"32068507"},"title":"Preparing Medical Imaging Data for Machine Learning."},{"paperId":"a9fd5511b42206a27748f373e0fdb7eb76a23055","externalIds":{"DBLP":"journals/corr/abs-2001-07966","ArXiv":"2001.07966","MAG":"3001555892","CorpusId":210859480},"title":"ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data"},{"paperId":"6d38bdd172cdbccb11745f5f031f848679117f25","externalIds":{"DBLP":"journals/mia/KavurGBACGPCEOB21","ArXiv":"2001.06535","MAG":"3002569343","DOI":"10.1016/j.media.2020.101950","CorpusId":210839730,"PubMed":"33421920"},"title":"CHAOS Challenge - Combined (CT-MR) Healthy Abdominal Organ Segmentation"},{"paperId":"7bf93d409033b67c349f840b94966a546253b85b","externalIds":{"ArXiv":"1911.11958","MAG":"2991544046","DOI":"10.1007/s11831-020-09405-5","CorpusId":208310262,"PubMed":"34093005"},"title":"Multiscale Modeling Meets Machine Learning: What Can We Learn?"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","externalIds":{"DBLP":"conf/eccv/ChenLYK0G0020","MAG":"3090449556","DOI":"10.1007/978-3-030-58577-8_7","CorpusId":216080982},"title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"6648b4db5f12c30941ea78c695e77aded19672bb","externalIds":{"MAG":"2997591391","ArXiv":"1909.11059","DBLP":"journals/corr/abs-1909-11059","DOI":"10.1609/AAAI.V34I07.7005","CorpusId":202734445},"title":"Unified Vision-Language Pre-Training for Image Captioning and VQA"},{"paperId":"374b4b7db9bc7cfff74dd37c8dadccbd67e16f87","externalIds":{"ArXiv":"1908.10454","DBLP":"journals/mia/TajbakhshJLCWD20","MAG":"3014795415","DOI":"10.1016/j.media.2020.101693","CorpusId":201660427,"PubMed":"32289663"},"title":"Embracing Imperfect Datasets: A Review of Deep Learning Solutions for Medical Image Segmentation"},{"paperId":"4aa6298b606941a282d735fa3143da293199d2ca","externalIds":{"ArXiv":"1908.08530","MAG":"2995460200","DBLP":"conf/iclr/SuZCLLWD20","CorpusId":201317624},"title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","externalIds":{"MAG":"2969862959","DBLP":"conf/emnlp/TanB19","ACL":"D19-1514","ArXiv":"1908.07490","DOI":"10.18653/v1/D19-1514","CorpusId":201103729},"title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"4c163d4942117179d3e97182e1b280027d7d60a9","externalIds":{"DBLP":"conf/iccv/HuangWCW19","MAG":"2968549119","ArXiv":"1908.06954","DOI":"10.1109/ICCV.2019.00473","CorpusId":201070367},"title":"Attention on Attention for Image Captioning"},{"paperId":"2bc1c8bd00bbf7401afcb5460277840fd8bab029","externalIds":{"ArXiv":"1908.06066","DBLP":"journals/corr/abs-1908-06066","MAG":"2998356391","DOI":"10.1609/AAAI.V34I07.6795","CorpusId":201058752},"title":"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","externalIds":{"ArXiv":"1908.03557","MAG":"2968124245","DBLP":"journals/corr/abs-1908-03557","CorpusId":199528533},"title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","externalIds":{"MAG":"2966715458","DBLP":"journals/corr/abs-1908-02265","ArXiv":"1908.02265","CorpusId":199453025},"title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"48a7873681c6aa88b9e0e22a25c2a8245eaeb45f","externalIds":{"DBLP":"conf/ijcai/WangYQMLLF19","MAG":"2965848243","ArXiv":"1907.09748","DOI":"10.24963/ijcai.2019/526","CorpusId":198179796},"title":"Position Focused Attention Network for Image-Text Matching"},{"paperId":"72be39943d06037a11c804c36fce652494f6404c","externalIds":{"MAG":"2958992432","DOI":"10.1007/978-3-030-22868-2_90","CorpusId":199011996},"title":"Explainable Artificial Intelligence Applications in NLP, Biomedical, and Malware Classification: A Literature Review"},{"paperId":"136c05cb8dd359fb8e0dc7947172a9ecb74ccbec","externalIds":{"MAG":"2958447893","ArXiv":"1907.03950","DBLP":"conf/nips/HudsonM19","CorpusId":195847902},"title":"Learning by Abstraction: The Neural State Machine"},{"paperId":"18cb67e5970c4b1f0949a6f6db8f8cbaa68b2eb1","externalIds":{"MAG":"2914163459","DBLP":"journals/pr/XiaoWDXP19","DOI":"10.1016/J.PATCOG.2019.01.028","CorpusId":126925045},"title":"Dense semantic embedding network for image captioning"},{"paperId":"c192c7d1d94e7a64de7e18e2f2fdffbf2909fcff","externalIds":{"DBLP":"journals/access/GuoWW19","MAG":"2946165673","DOI":"10.1109/ACCESS.2019.2916887","CorpusId":169032532},"title":"Deep Multimodal Representation Learning: A Survey"},{"paperId":"2dc698077cb178286c737484dcf67c5ab19314d0","externalIds":{"DBLP":"journals/corr/abs-1905-04405","MAG":"2998631105","ArXiv":"1905.04405","DOI":"10.1109/ICCV.2019.01039","CorpusId":152282673},"title":"Language-Conditioned Graph Networks for Relational Reasoning"},{"paperId":"af1f7739283bdbd2b7a94903041f6d6afd991907","externalIds":{"MAG":"2936135081","DBLP":"journals/corr/abs-1904-08920","ArXiv":"1904.08920","DOI":"10.1109/CVPR.2019.00851","CorpusId":85553602},"title":"Towards VQA Models That Can Read"},{"paperId":"a8427ce5aee6d62800c725588e89940ed4910e0d","externalIds":{"DBLP":"journals/corr/abs-1904-02874","MAG":"2941531368","ArXiv":"1904.02874","DOI":"10.1145/3465055","CorpusId":102350916},"title":"An Attentive Survey of Attention Models"},{"paperId":"165e524750b9a3e5d47934e6fc72b9537128c116","externalIds":{"DBLP":"journals/kbs/HuangZZXL19","MAG":"2910191085","DOI":"10.1016/J.KNOSYS.2019.01.019","CorpusId":67778008},"title":"Image-text sentiment analysis via deep multimodal attentive fusion"},{"paperId":"760a7ed58cf49fa5dbcfb7f06b67cea2cd35f768","externalIds":{"MAG":"2907279673","DBLP":"journals/ijon/TanC19","DOI":"10.1016/J.NEUCOM.2018.12.026","CorpusId":67789014},"title":"Phrase-based image caption generator with hierarchical LSTM network"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"4654aa505e5bcdb089d0df202cd7ceabc9d2d41f","externalIds":{"MAG":"2915126261","ArXiv":"1902.09063","DBLP":"journals/corr/abs-1902-09063","CorpusId":67855790},"title":"A large annotated medical image dataset for the development and evaluation of segmentation algorithms"},{"paperId":"4955335096c038f2cf39fdb4cc2bc79edaf363f9","externalIds":{"PubMedCentral":"6403009","MAG":"2911605224","DOI":"10.3322/caac.21552","CorpusId":73418087,"PubMed":"30720861"},"title":"Artificial intelligence in cancer imaging: Clinical challenges and applications"},{"paperId":"125b0bde4ac0b4cb9453b205bc0c5c184af3dec2","externalIds":{"MAG":"2914306086","DBLP":"journals/tetci/LiTLF19","DOI":"10.1109/TETCI.2019.2892755","CorpusId":86426900},"title":"Visual to Text: Survey of Image and Video Captioning"},{"paperId":"89a816719613e220a64ab2590c938c23bbfe187e","externalIds":{"MAG":"2951514121","DBLP":"journals/corr/abs-1901-07031","ArXiv":"1901.07031","DOI":"10.1609/aaai.v33i01.3301590","CorpusId":58981871},"title":"CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"},{"paperId":"3d29ce781f297dc543e44dfb39990baff3a3acca","externalIds":{"ArXiv":"1901.07042","CorpusId":58981909},"title":"MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs"},{"paperId":"5132500b23d2da47129b3f4f68dd30947a29e502","externalIds":{"ArXiv":"1811.11721","MAG":"2981689412","DBLP":"conf/iccv/HuangWHHW019","DOI":"10.1109/ICCV.2019.00069","CorpusId":53846561},"title":"CCNet: Criss-Cross Attention for Semantic Segmentation"},{"paperId":"5ac18d505ed6d10e8692cbb7d33f6852e6782692","externalIds":{"ArXiv":"1811.00982","MAG":"3015310352","DBLP":"journals/corr/abs-1811-00982","DOI":"10.1007/s11263-020-01316-z","CorpusId":53296866},"title":"The Open Images Dataset V4"},{"paperId":"fe66fc9e3d9a52497119bab89ac54fc8b5f8859a","externalIds":{"DBLP":"journals/ijon/BaiA18","MAG":"2803259101","DOI":"10.1016/J.NEUCOM.2018.05.080","CorpusId":51613554},"title":"A survey on automatic image caption generation"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","externalIds":{"DBLP":"conf/miccai/PelkaKRNF18","MAG":"2897980926","DOI":"10.1007/978-3-030-01364-6_20","CorpusId":53087891},"title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"f01698c61fa4786c6c514fe252d5049b01cd010d","externalIds":{"MAG":"2941328520","DOI":"10.1109/ICCUBEA.2018.8697360","CorpusId":133608323},"title":"Image Caption Generation Using Deep Learning Technique"},{"paperId":"37a052144b510b8827634c38146b190d8b2c8d0b","externalIds":{"MAG":"2884065486","DBLP":"journals/corr/abs-1807-10225","ArXiv":"1807.10225","DOI":"10.1007/978-3-030-00536-8_1","CorpusId":50786007},"title":"Medical Image Synthesis for Data Augmentation and Anonymization using Generative Adversarial Networks"},{"paperId":"cc23c580b7d8063415fb6eb512053d1079b849de","externalIds":{"MAG":"2987119394","ArXiv":"1807.07984","DBLP":"journals/tkdd/LeeRKAK19","DOI":"10.1145/3363574","CorpusId":219890523},"title":"Attention Models in Graphs"},{"paperId":"6ebc83f1c22488f17db7506ccd256c89a51ef485","externalIds":{"DBLP":"conf/ijcai/LiZLZZ18","MAG":"2808413133","DOI":"10.24963/ijcai.2018/577","CorpusId":51607177},"title":"Multi-modal Sentence Summarization with Modality Attention and Image Filtering"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","externalIds":{"MAG":"2804243936","ArXiv":"1805.07932","DBLP":"conf/nips/KimJZ18","CorpusId":29150617},"title":"Bilinear Attention Networks"},{"paperId":"45dd2a3cd7c27f2e9509b023d702408f5ac11c9d","externalIds":{"MAG":"2793424345","DBLP":"journals/corr/abs-1803-08024","ArXiv":"1803.08024","DOI":"10.1007/978-3-030-01225-0_13","CorpusId":3994012},"title":"Stacked Cross Attention for Image-Text Matching"},{"paperId":"609512f19e06bf393cb79fbf57183f75b8d889d2","externalIds":{"MAG":"2962718314","ArXiv":"1802.00927","DBLP":"conf/aaai/ZadehLMPCM18","DOI":"10.1609/aaai.v32i1.12021","CorpusId":3621881},"title":"Memory Fusion Network for Multi-view Sequential Learning"},{"paperId":"7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","externalIds":{"ArXiv":"1801.07736","DBLP":"journals/corr/abs-1801-07736","MAG":"2951433039","CorpusId":3655946},"title":"MaskGAN: Better Text Generation via Filling in the ______"},{"paperId":"6acb911d57720367d1ae7b9bce8ab9f9dcd9aadb","externalIds":{"MAG":"2735305611","DBLP":"journals/ijon/KinghornZS18","DOI":"10.1016/j.neucom.2017.07.014","CorpusId":4314719},"title":"A region-based image caption generator with refined descriptions"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","externalIds":{"DBLP":"journals/corr/abs-1708-01471","ArXiv":"1708.01471","MAG":"2963150162","DOI":"10.1109/ICCV.2017.202","CorpusId":3005731},"title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","externalIds":{"MAG":"2745461083","DBLP":"conf/cvpr/00010BT0GZ18","ArXiv":"1707.07998","DOI":"10.1109/CVPR.2018.00636","CorpusId":3753452},"title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91","externalIds":{"ArXiv":"1705.09406","DBLP":"journals/pami/BaltrusaitisAM19","MAG":"2951127645","DOI":"10.1109/TPAMI.2018.2798607","CorpusId":10137425,"PubMed":"29994351"},"title":"Multimodal Machine Learning: A Survey and Taxonomy"},{"paperId":"58b6bd06ea58c367c64286126ba14128b45041b8","externalIds":{"ArXiv":"1705.02315","MAG":"2611650229","DBLP":"conf/cvpr/WangPLLBS17","DOI":"10.1109/CVPR.2017.369","CorpusId":263796294},"title":"ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases"},{"paperId":"f338fe8ed2e8d87feea5c6538a5a05a06b3696db","externalIds":{"ArXiv":"1704.00763","MAG":"2952326480","DBLP":"conf/cvpr/ChenBFWN17","DOI":"10.1109/CVPR.2017.657","CorpusId":2398022},"title":"AMC: Attention Guided Multi-modal Correlation Learning for Image Search"},{"paperId":"7d1a7dae43b630d61d19d6cf139824380f2cf42f","externalIds":{"DBLP":"conf/aaai/LiTDZT17","MAG":"2604729005","DOI":"10.1609/aaai.v31i1.11236","CorpusId":29170844},"title":"Image Caption with Global-Local Attention"},{"paperId":"d85704f4814e9fa5ff0b68b1e5cad9e6527d0bbf","externalIds":{"MAG":"2951916837","ArXiv":"1612.07086","DBLP":"conf/iccv/GuWCC17","DOI":"10.1109/ICCV.2017.138","CorpusId":3928398},"title":"An Empirical Study of Language CNN for Image Captioning"},{"paperId":"e18ec2c9f0b4a817b8cf0435822bbc879d7db698","externalIds":{"MAG":"2307512708","DBLP":"journals/corr/KembhaviSKSHF16","ArXiv":"1603.07396","DOI":"10.1007/978-3-319-46493-0_15","CorpusId":2682274},"title":"A Diagram is Worth a Dozen Images"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"b0c065cd43aa7280e766b5dcbcc7e26abce59330","externalIds":{"MAG":"1910657905","DBLP":"journals/pami/BadrinarayananK17","ArXiv":"1511.00561","DOI":"10.1109/TPAMI.2016.2644615","CorpusId":60814714,"PubMed":"28060704"},"title":"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"},{"paperId":"9ee46e72fd00004dcf92f4fc03854ae3851a3a2c","externalIds":{"MAG":"1524992328","DOI":"10.1002/mrd.22489","CorpusId":1606670,"PubMed":"26153368"},"title":"The ImageJ ecosystem: An open platform for biomedical image analysis"},{"paperId":"a72b8bbd039989db39769da836cdb287737deb92","externalIds":{"MAG":"1895989618","DBLP":"conf/cvpr/ChenZ15","DOI":"10.1109/CVPR.2015.7298856","CorpusId":6785090},"title":"Mind's eye: A recurrent visual representation for image caption generation"},{"paperId":"c657ab339517fb8def7ce7f83bb81e746d558218","externalIds":{"MAG":"2119852447","PubMedCentral":"4521131","DOI":"10.1093/ije/dyv098","CorpusId":2768178,"PubMed":"26050254"},"title":"Data Resource Profile: Clinical Practice Research Datalink (CPRD)"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","externalIds":{"ArXiv":"1505.04870","DBLP":"conf/iccv/PlummerWCCHL15","MAG":"2568262903","DOI":"10.1007/s11263-016-0965-7","CorpusId":6941275},"title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"68478207cf3e4fc44bf1602abe82c7ac7f288872","externalIds":{"DBLP":"conf/cvpr/PanMYLR16","MAG":"1573040851","ArXiv":"1505.01861","DOI":"10.1109/CVPR.2016.497","CorpusId":14432549},"title":"Jointly Modeling Embedding and Translation to Bridge Video and Language"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","externalIds":{"DBLP":"journals/corr/AntolALMBZP15","MAG":"1933349210","ArXiv":"1505.00468","DOI":"10.1007/s11263-016-0966-6","CorpusId":3180429},"title":"VQA: Visual Question Answering"},{"paperId":"e58a110fa1e4ddf247d5c614d117d64bfbe135c4","externalIds":{"MAG":"2950019618","ArXiv":"1505.00487","DBLP":"journals/corr/VenugopalanRDMD15","DOI":"10.1109/ICCV.2015.515","CorpusId":4228546},"title":"Sequence to Sequence -- Video to Text"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","externalIds":{"MAG":"2950178297","DBLP":"conf/icml/XuBKCCSZB15","ArXiv":"1502.03044","CorpusId":1055111},"title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745","externalIds":{"ArXiv":"1412.6632","MAG":"1811254738","DBLP":"journals/corr/MaoXYWY14a","CorpusId":3509328},"title":"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)"},{"paperId":"43795b7bac3d921c4e579964b54187bdbf6c6330","externalIds":{"DBLP":"journals/corr/VenugopalanXDRMS14","ACL":"N15-1173","ArXiv":"1412.4729","MAG":"2964241990","DOI":"10.3115/v1/N15-1173","CorpusId":52316421},"title":"Translating Videos to Natural Language Using Deep Recurrent Neural Networks"},{"paperId":"ac3ee98020251797c2b401e1389461df88e52e62","externalIds":{"MAG":"1924770834","DBLP":"journals/corr/ChungGCB14","ArXiv":"1412.3555","CorpusId":5201925},"title":"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"},{"paperId":"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","externalIds":{"ArXiv":"1412.2306","DBLP":"journals/corr/KarpathyF14","MAG":"1905882502","DOI":"10.1109/CVPR.2015.7298932","CorpusId":8517067},"title":"Deep visual-semantic alignments for generating image descriptions"},{"paperId":"f01fc808592ea7c473a69a6e7484040a435f36d9","externalIds":{"DBLP":"journals/pami/DonahueHRVGSD17","MAG":"2951183276","ArXiv":"1411.4389","DOI":"10.1109/CVPR.2015.7298878","CorpusId":5736847,"PubMed":"27608449"},"title":"Long-term recurrent convolutional networks for visual recognition and description"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","externalIds":{"MAG":"1895577753","DBLP":"journals/corr/VinyalsTBE14","ArXiv":"1411.4555","DOI":"10.1109/CVPR.2015.7298935","CorpusId":1169492},"title":"Show and tell: A neural image caption generator"},{"paperId":"82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9","externalIds":{"MAG":"2159243025","DBLP":"journals/corr/MaoXYWY14","ArXiv":"1410.1090","CorpusId":3527896},"title":"Explain Images with Multimodal Recurrent Neural Networks"},{"paperId":"e15cf50aa89fee8535703b9f9512fca5bfc43327","externalIds":{"DBLP":"journals/corr/SzegedyLJSRAEVR14","MAG":"2097117768","ArXiv":"1409.4842","DOI":"10.1109/CVPR.2015.7298594","CorpusId":206592484},"title":"Going deeper with convolutions"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","externalIds":{"MAG":"2949429431","ArXiv":"1409.1556","DBLP":"journals/corr/SimonyanZ14a","CorpusId":14124313},"title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"9814df8bd00ba999c4d1e305a7e9bca579dc7c75","externalIds":{"MAG":"68733909","DBLP":"journals/jair/HodoshYH13","DOI":"10.1613/jair.3994","CorpusId":928608},"title":"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics"},{"paperId":"2c2f4644bf953f56c460e2b8597e86ae47d0af87","externalIds":{"MAG":"2145598468","DBLP":"conf/fgr/MetallinouN13","DOI":"10.1109/FG.2013.6553804","CorpusId":18014114},"title":"Annotation and processing of continuous emotional attributes: Challenges and opportunities"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","externalIds":{"DBLP":"conf/nips/OrdonezKB11","MAG":"2109586012","CorpusId":14579301},"title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"76a1dca3a9c2b0229c1b12c95752dcf40dc95a11","externalIds":{"DBLP":"conf/emnlp/YangTDA11","ACL":"D11-1041","MAG":"1858383477","CorpusId":1539668},"title":"Corpus-Guided Sentence Generation of Natural Images"},{"paperId":"fbdbe747c6aa8b35b981d21e475ff1506a1bae66","externalIds":{"ACL":"W11-0326","MAG":"1687846465","DBLP":"conf/conll/LiKBBC11","CorpusId":10702193},"title":"Composing Simple Image Descriptions using Web-scale N-grams"},{"paperId":"eaaed23a2d94feb2f1c3ff22a25777c7a78f3141","externalIds":{"DBLP":"conf/eccv/FarhadiHSYRHF10","MAG":"1897761818","DOI":"10.1007/978-3-642-15561-1_2","CorpusId":13272863},"title":"Every Picture Tells a Story: Generating Sentences from Images"},{"paperId":"8ec550ce7e4d45fee4975cb5a1330d536b21fd6e","externalIds":{"MAG":"2122328291","DOI":"10.1212/WNL.0b013e3181cb3e25","CorpusId":29495563,"PubMed":"20042704"},"title":"Alzheimer's Disease Neuroimaging Initiative (ADNI)"},{"paperId":"88659a25acb87f61fe5e3139ffe7786477955a4e","externalIds":{"MAG":"2109076134","DBLP":"conf/chi/WangPQSMS08","DOI":"10.1145/1357054.1357129","CorpusId":11629490},"title":"Aligning temporal data by sentinel events: discovering patterns in electronic health records"},{"paperId":"038b582cccb00c54589c5563d9a00ee28dad83b0","externalIds":{"DBLP":"journals/neuroimage/YushkevichPHSHG06","MAG":"2127890285","DOI":"10.1016/j.neuroimage.2006.01.015","CorpusId":1660596,"PubMed":"16545965"},"title":"User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability"},{"paperId":"6e248de974a43536cb508b8e35e16c7c9d84cbbb","externalIds":{"DBLP":"journals/access/HabibWPTRMOS24","DOI":"10.1109/ACCESS.2023.3342866","CorpusId":266298438},"title":"GACnet-Text-to-Image Synthesis With Generative Models Using Attention Mechanisms With Contrastive Learning"},{"paperId":"d9902e3a3d9fd065d6d682dff08c220d2926bcfc","externalIds":{"DBLP":"journals/access/JavedMJRS24","DOI":"10.1109/ACCESS.2023.3340884","CorpusId":266097281},"title":"Ethical Frameworks for Machine Learning in Sensitive Healthcare Applications"},{"paperId":"cd2d1a0f73ba8c40f882a386cd367899785fb877","externalIds":{"DBLP":"journals/corr/abs-2304-14454","DOI":"10.48550/arXiv.2304.14454","CorpusId":263888272},"title":"PMC-LLaMA: Further Finetuning LLaMA on Medical Papers"},{"paperId":"9159408884cbe7f5f7a79d90c9f91ba5cee0d932","externalIds":{"DBLP":"journals/access/PatilBGN23","DOI":"10.1109/ACCESS.2023.3266377","CorpusId":258088263},"title":"A Survey of Text Representation and Embedding Techniques in NLP"},{"paperId":"c3c79dacd53bc2f70f339a54a64cb3ffa44a7956","externalIds":{"DBLP":"conf/nips/LiZSR23","CorpusId":268030832},"title":"Characterizing the Impacts of Semi-supervised Learning for Weak Supervision"},{"paperId":"eda2ca7f66ec08a79b72269a30b114cc5f5cdbc5","externalIds":{"DBLP":"journals/tgrs/JiMZPL23","DOI":"10.1109/TGRS.2023.3332317","CorpusId":265191433},"title":"Knowledge-Aided Momentum Contrastive Learning for Remote-Sensing Image Text Retrieval"},{"paperId":"2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75","externalIds":{"DBLP":"journals/corr/abs-2301-13823","DOI":"10.48550/arXiv.2301.13823","CorpusId":256416164},"title":"Grounding Language Models to Images for Multimodal Generation"},{"paperId":"379b4d871cebf431208154adaaff7ea946b2bb38","externalIds":{"DBLP":"journals/corr/abs-2305-06152","DOI":"10.48550/arXiv.2305.06152","CorpusId":258587972},"title":"Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","externalIds":{"DBLP":"journals/corr/abs-2308-12966","DOI":"10.48550/arXiv.2308.12966","CorpusId":263875678},"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"84dc889beff9d51fe429cff8c92735e7410ee3c2","externalIds":{"DBLP":"journals/corr/abs-2306-14565","DOI":"10.48550/arXiv.2306.14565","CorpusId":263860779},"title":"Aligning Large Multi-Modal Model with Robust Instruction Tuning"},{"paperId":"b3e6c1cf9b1f67d93b2b4778c64ad265844f353e","externalIds":{"MAG":"3133431537","DBLP":"journals/tmm/TanLYL22","DOI":"10.1109/TMM.2021.3060291","CorpusId":234160722},"title":"Cross-Modal Semantic Matching Generative Adversarial Networks for Text-to-Image Synthesis"},{"paperId":"92d72ea8e690725d1f344657760149eeb22d5bd1","externalIds":{"DBLP":"conf/isic2/ThakareW22","CorpusId":254235325},"title":"Automatic Caption Generation from Image: A Comprehensive Survey"},{"paperId":"a1b1c24c111d0a6a4d2a657a0ba8d739547e4494","externalIds":{"DBLP":"conf/pkdd/LiZWZ22","DOI":"10.1007/978-3-031-26409-2_24","CorpusId":257633358},"title":"Fine-Grained Bidirectional Attention-Based Generative Networks for Image-Text Matching"},{"paperId":"ecce44df1956db4ec486539c6543345344809958","externalIds":{"DBLP":"journals/corr/abs-2202-03052","CorpusId":265040027},"title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"c85512839054d20b4c3b6c70053ec94d90ca0bd2","externalIds":{"DOI":"10.32470/ccn.2022.1140-0","CorpusId":251603979},"title":"Neural replay as context-driven memory reactivation"},{"paperId":"88601c22bb7176e8b6f1d351c79a1ed1fa6c1584","externalIds":{"DBLP":"conf/nips/YinYL21","CorpusId":245117081},"title":"Mitigating Forgetting in Online Continual Learning with Neuron Calibration"},{"paperId":"33a6bbb2b36da34b503b7fa79b6e084a45d992cf","externalIds":{"DBLP":"conf/acl/ZhaoLJ20","ACL":"2021.acl-long.203","DOI":"10.18653/v1/2021.acl-long.203","CorpusId":236459819},"title":"Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities"},{"paperId":"6a95aa51f68cec4c7d853eca7f34ac8739e8188e","externalIds":{"DBLP":"conf/clef/ChenGL20","CorpusId":225074033},"title":"HCP-MIC at VQA-Med 2020: Effective Visual Representation for Medical Visual Question Answering"},{"paperId":"ef9bbc83dea84df3711de01de56f2b7f91bae068","externalIds":{"DBLP":"series/lncs/MontavonBLSM19","MAG":"2973136764","DOI":"10.1007/978-3-030-28954-6_10","CorpusId":202579539},"title":"Layer-Wise Relevance Propagation: An Overview"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","externalIds":{"CorpusId":53712941},"title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"ba6d8ef9bb44ac03a15f732f468da913a7a7bb2e","externalIds":{"DOI":"10.1093/benz/9780199773787.article.b00002084","CorpusId":240758907},"title":"albert"},{"paperId":"f4b4a1f89046325c5f6526b3df3eb09284cb73e9","externalIds":{"MAG":"950853366","DOI":"10.1007/978-3-642-24797-2_2","CorpusId":60085539},"title":"Supervised Sequence Labelling"},{"paperId":"bea0e759434cd5f42b25f290f0b22011b972ffcc","externalIds":{"DOI":"10.1007/0-387-25739-x_8","CorpusId":12673704},"title":"BIOMEDICAL ONTOLOGIES"}]}