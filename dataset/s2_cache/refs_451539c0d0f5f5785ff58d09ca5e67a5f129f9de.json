{"references":[{"paperId":"8622315040be18cce64acbb733a503f336ac55aa","externalIds":{"DBLP":"conf/wacv/ZampokasBT24","DOI":"10.1109/WACVW60836.2024.00105","CorpusId":269190188},"title":"Latency Driven Spatially Sparse Optimization for Multi-Branch CNNs for Semantic Segmentation"},{"paperId":"2c9bdb69274cc4baf7d9df8ebfc86fc6bf35d0f1","externalIds":{"DBLP":"conf/wacv/ParkLKCPCLK24","DOI":"10.1109/WACVW60836.2024.00107","CorpusId":269191004},"title":"VLAAD: Vision and Language Assistant for Autonomous Driving"},{"paperId":"7c307803a9dc358ddd6c2a8cd382b348cd0b8602","externalIds":{"DBLP":"conf/wacv/PuligillaOZPK24","ArXiv":"2312.16648","DOI":"10.1109/WACVW60836.2024.00103","CorpusId":266573042},"title":"LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization"},{"paperId":"257eaf21e6e6926e9693e04e946ec2199b3afee1","externalIds":{"DBLP":"journals/corr/abs-2312-06352","ArXiv":"2312.06352","DOI":"10.1109/WACVW60836.2024.00104","CorpusId":266162771},"title":"NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations"},{"paperId":"c0cfb1bfe13bc3bf590a1ac3278359d249843ce1","externalIds":{"DBLP":"journals/aim/ZhengCTCSZLLZYM23","DOI":"10.1002/aaai.12139","CorpusId":265384998},"title":"High-definition map automatic annotation system based on active learning"},{"paperId":"0333cf45841955d19de5096d5fd0ee55fdcd15ad","externalIds":{"ArXiv":"2311.08206","DBLP":"journals/corr/abs-2311-08206","DOI":"10.1109/WACVW60836.2024.00108","CorpusId":265157758},"title":"Human-Centric Autonomous Systems With LLMs for User Command Reasoning"},{"paperId":"e7f2818d27829232f71628b70849826910b1fe16","externalIds":{"DBLP":"journals/corr/abs-2310-10103","ArXiv":"2310.10103","DOI":"10.48550/arXiv.2310.10103","CorpusId":264146047},"title":"Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning"},{"paperId":"10158879cdb64ce7d3f7bb5572c4617ea808602e","externalIds":{"ArXiv":"2310.08034","DBLP":"journals/corr/abs-2310-08034","DOI":"10.1109/MITS.2024.3381793","CorpusId":263908840},"title":"Receive, Reason, and React: Drive as You Say, With Large Language Models in Autonomous Vehicles"},{"paperId":"5d09c6a8bae91da2640f1926df2a94e940556d27","externalIds":{"ArXiv":"2310.08710","DBLP":"journals/corr/abs-2310-08710","DOI":"10.48550/arXiv.2310.08710","CorpusId":264128139},"title":"Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research"},{"paperId":"19933dd9e03058e686ef412262eef7696cce3e8f","externalIds":{"ArXiv":"2310.03026","DBLP":"journals/corr/abs-2310-03026","DOI":"10.48550/arXiv.2310.03026","CorpusId":263620279},"title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving"},{"paperId":"f01ff5acf9e086030c01beda6f433f99013ebbd4","externalIds":{"DBLP":"conf/icra/ChenSHKWBMS24","ArXiv":"2310.01957","DOI":"10.1109/ICRA57147.2024.10611018","CorpusId":263608168},"title":"Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"},{"paperId":"77693ca00a8ef775af96b5c450aa0afdb0e10a51","externalIds":{"DBLP":"journals/corr/abs-2310-02251","ArXiv":"2310.02251","DOI":"10.1109/ICRA57147.2024.10611485","CorpusId":263608641},"title":"Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving"},{"paperId":"555dd10c4dc8570d4069389a4da785a8ff73063d","externalIds":{"ArXiv":"2310.02324","DBLP":"journals/corr/abs-2310-02324","DOI":"10.48550/arXiv.2310.02324","CorpusId":263620315},"title":"ALT-Pilot: Autonomous navigation with Language augmented Topometric maps"},{"paperId":"958ed4830ae80a189ecb9b93ab75a6ce2e3926fc","externalIds":{"DBLP":"journals/corr/abs-2310-01415","ArXiv":"2310.01415","DOI":"10.48550/arXiv.2310.01415","CorpusId":263605637},"title":"GPT-Driver: Learning to Drive with GPT"},{"paperId":"3cbfe152220de84ecf8059fa50c47587a3134c86","externalIds":{"DBLP":"conf/iclr/WenF0C0CDS0024","ArXiv":"2309.16292","DOI":"10.48550/arXiv.2309.16292","CorpusId":263136146},"title":"DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models"},{"paperId":"7a76afd69e0df420d145767438579a71df021bf5","externalIds":{"DBLP":"conf/iccv/SeffCCNZNRAS23","ArXiv":"2309.16534","DOI":"10.1109/ICCV51070.2023.00788","CorpusId":263140658},"title":"MotionLM: Multi-Agent Motion Forecasting as Language Modeling"},{"paperId":"6066c3b06c8bd4cad5da055d2ac7826f7bcc81f1","externalIds":{"DBLP":"journals/corr/abs-2309-11745","ArXiv":"2309.11745","DOI":"10.48550/arXiv.2309.11745","CorpusId":262083852},"title":"PIE: Simulating Disease Progression via Progressive Image Editing"},{"paperId":"482665786ce1956fb9ea4b694d2d8e8cf92276fa","externalIds":{"ArXiv":"2309.10228","DBLP":"conf/wacv/CuiMCYW24","DOI":"10.1109/WACVW60836.2024.00101","CorpusId":262054629},"title":"Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles"},{"paperId":"85f163f55f0aa42664c0393325e56dfea05fe08e","externalIds":{"ArXiv":"2309.05186","DBLP":"journals/ijcv/DingHXZL25","DOI":"10.1007/s11263-025-02433-3","CorpusId":261682424},"title":"HiLM-D: Enhancing MLLMs with Multi-scale High-Resolution Details for Autonomous Driving"},{"paperId":"7679dc8534cb1dd65c63c50b38f56386228d32d1","externalIds":{"ArXiv":"2309.05282","DBLP":"journals/corr/abs-2309-05282","DOI":"10.48550/arXiv.2309.05282","CorpusId":261681760},"title":"Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving"},{"paperId":"29d262a66c57a9f819fe0325f184b91d48a4c2a4","externalIds":{"ArXiv":"2309.04379","DBLP":"journals/corr/abs-2309-04379","DOI":"10.48550/arXiv.2309.04379","CorpusId":261660217},"title":"Language Prompt for Autonomous Driving"},{"paperId":"5a9d4bcffa9989cac4139b2844358884ae023e8d","externalIds":{"DBLP":"journals/corr/abs-2309-04077","ArXiv":"2309.04077","DOI":"10.48550/arXiv.2309.04077","CorpusId":261660608},"title":"SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments"},{"paperId":"22ebfc211d184ed615729378a43fde175bf14478","externalIds":{"ArXiv":"2309.00615","DBLP":"journals/corr/abs-2309-00615","DOI":"10.48550/arXiv.2309.00615","CorpusId":261493787},"title":"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following"},{"paperId":"a5b1c02c0307a057940b3056d769fdeb5c3dcbb1","externalIds":{"DBLP":"conf/iccv/BelderVT23","ArXiv":"2308.13270","DOI":"10.1109/ICCV51070.2023.00774","CorpusId":261214592},"title":"A Game of Bundle Adjustment - Learning Efficient Convergence"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"11bca2cafe89e14dc733504f97e2489de697ceab","externalIds":{"ArXiv":"2307.07162","DBLP":"conf/wacv/FuLWDCSQ24","DOI":"10.1109/WACVW60836.2024.00102","CorpusId":259924488},"title":"Drive Like a Human: Rethinking Autonomous Driving with Large Language Models"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","externalIds":{"DBLP":"conf/corl/HuangWZL0023","ArXiv":"2307.05973","DOI":"10.48550/arXiv.2307.05973","CorpusId":259837330},"title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"8ac98f4ca139781a0b000c40fa6cdd2af7592b7f","externalIds":{"DBLP":"journals/corr/abs-2307-04370","ArXiv":"2307.04370","DOI":"10.1109/TIV.2023.3318070","CorpusId":259501050},"title":"Recent Advancements in End-to-End Autonomous Driving Using Deep Learning: A Survey"},{"paperId":"23be5ad6c40d3b59835edef7fe0b4a25315a77b3","externalIds":{"ArXiv":"2307.03073","DBLP":"journals/corr/abs-2307-03073","DOI":"10.1109/IROS58592.2024.10801660","CorpusId":259360864},"title":"Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"8f7297454d7f44365b9bcda5ebb9439a43daf5e6","externalIds":{"DBLP":"journals/corr/abs-2306-13063","ArXiv":"2306.13063","DOI":"10.48550/arXiv.2306.13063","CorpusId":259224389},"title":"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs"},{"paperId":"f0d72acf3369b28dc5f78ddd169ff5761fb80e82","externalIds":{"ArXiv":"2306.06849","DBLP":"journals/corr/abs-2306-06849","DOI":"10.48550/arXiv.2306.06849","CorpusId":259138470},"title":"Mitigating Transformer Overconfidence via Lipschitz Regularization"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"f197bf0fc2f228483f6af3285000d54d8d97f9eb","externalIds":{"ArXiv":"2305.16291","DBLP":"journals/tmlr/WangX0MXZFA24","DOI":"10.48550/arXiv.2305.16291","CorpusId":258887849},"title":"Voyager: An Open-Ended Embodied Agent with Large Language Models"},{"paperId":"ee156428803c5bd6e7372f6b27d74bcf88390db3","externalIds":{"ArXiv":"2305.14836","DBLP":"conf/aaai/QianCZJJ24","DOI":"10.48550/arXiv.2305.14836","CorpusId":258866014},"title":"NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario"},{"paperId":"08b273c6b975e38a74375ed00cb604d84e487826","externalIds":{"DBLP":"conf/nips/MontaliLMKRLGEY23","ArXiv":"2305.12032","DOI":"10.48550/arXiv.2305.12032","CorpusId":258832394},"title":"The Waymo Open Sim Agents Challenge"},{"paperId":"75f8f350f33bbbe7e0c2666d15004b6275277df7","externalIds":{"ArXiv":"2305.08877","DBLP":"conf/cvpr/MaYAHGLW23","DOI":"10.1109/CVPRW59228.2023.00557","CorpusId":258714890},"title":"M2DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer"},{"paperId":"bfc57243a1efa914ec6450747ca3c1b5a3a83c00","externalIds":{"ArXiv":"2305.07840","DBLP":"conf/itsc/MaYCAHGW23","DOI":"10.1109/ITSC57777.2023.10421798","CorpusId":258686791},"title":"CEMFormer: Learning to Predict Driver Intentions from in-Cabin and External Cameras via Spatial-Temporal Transformers"},{"paperId":"004c8f4f37ecbef5d3a347a7e6ba00ecaac733e0","externalIds":{"DBLP":"conf/eccv/LaiRJLR24","ArXiv":"2305.03907","DOI":"10.48550/arXiv.2305.03907","CorpusId":258557101},"title":"Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation"},{"paperId":"6f8b9192b1f215254ee7625d752710182c05d2f9","externalIds":{"DBLP":"journals/corr/abs-2305-02677","ArXiv":"2305.02677","DOI":"10.48550/arXiv.2305.02677","CorpusId":258479994},"title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls"},{"paperId":"0d043b0bd4a981f7e6135a79dac6d71a809af8cb","externalIds":{"ArXiv":"2305.02317","DBLP":"journals/corr/abs-2305-02317","DOI":"10.48550/arXiv.2305.02317","CorpusId":258461502},"title":"Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","externalIds":{"DBLP":"conf/nips/LuPCGCWZG23","ArXiv":"2304.09842","DOI":"10.48550/arXiv.2304.09842","CorpusId":258212542},"title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"a8680b3419f3cbe6650f72b1023aed0ad0becb9e","externalIds":{"ArXiv":"2304.07919","CorpusId":258180277},"title":"Chain of Thought Prompt Tuning in Vision Language Models"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"07db8d33845ca3034f623a5bddf4c4816056d3ba","externalIds":{"DBLP":"conf/cvpr/Xu0LLZTMXDSYZM23","ArXiv":"2303.07601","DOI":"10.1109/CVPR52729.2023.01318","CorpusId":257505115},"title":"V2V4Real: A Real-World Large-Scale Dataset for Vehicle-to-Vehicle Cooperative Perception"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","externalIds":{"DBLP":"journals/corr/abs-2303-04671","ArXiv":"2303.04671","DOI":"10.48550/arXiv.2303.04671","CorpusId":257404891},"title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"be7b764fe1c9c32cbe349bde1fbb19321fd1d71c","externalIds":{"DBLP":"journals/corr/abs-2303-02151","ArXiv":"2303.02151","DOI":"10.1109/CVPR52729.2023.01460","CorpusId":257353537},"title":"Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"0ba581718f294db1d7b3dbc159cc3d3380f74606","externalIds":{"DBLP":"journals/access/VempralaBBK24","ArXiv":"2306.17582","DOI":"10.1109/ACCESS.2024.3387941","CorpusId":259141622},"title":"ChatGPT for Robotics: Design Principles and Model Abilities"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","externalIds":{"DBLP":"journals/corr/abs-2302-00923","ArXiv":"2302.00923","DOI":"10.48550/arXiv.2302.00923","CorpusId":256504063},"title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"2842933f5f97d5c3a85f31f08e79a771070b1ac0","externalIds":{"ArXiv":"2303.17220","DBLP":"journals/corr/abs-2303-17220","DOI":"10.1109/TIV.2022.3223131","CorpusId":253914332},"title":"Milestones in Autonomous Driving and Intelligent Vehicles: Survey of Surveys"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"048056c0321876c0d582c3bf40b1883cda9260d5","externalIds":{"DBLP":"conf/nips/WilsonQALSKPKHP21","ArXiv":"2301.00493","DOI":"10.48550/arXiv.2301.00493","CorpusId":244906596},"title":"Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting"},{"paperId":"951016af892f5ecde12b2acd9374b92b66afb796","externalIds":{"DBLP":"journals/corr/abs-2212-09399","ArXiv":"2212.09399","DOI":"10.1007/s43503-023-00018-y","CorpusId":254853932},"title":"AI art in architecture"},{"paperId":"0ab5ed1a90cb3a0689894de8867737cb523a0388","externalIds":{"DBLP":"conf/aaai/TangCCZLLZLMSZ23","ArXiv":"2212.11123","DOI":"10.48550/arXiv.2212.11123","CorpusId":254926797},"title":"THMA: Tencent HD Map AI System for Creating HD Map Annotations"},{"paperId":"541ca024fc404a0c14ae280b18020d6540faa0d3","externalIds":{"DBLP":"conf/iccv/GeorgescuFILSA23","ArXiv":"2212.05922","DOI":"10.1109/ICCV51070.2023.01479","CorpusId":254563922},"title":"Audiovisual Masked Autoencoders"},{"paperId":"af1c871282ec122869d03f5420ef5d9143358a91","externalIds":{"DBLP":"conf/cvpr/GuptaK23","ArXiv":"2211.11559","DOI":"10.1109/CVPR52729.2023.01436","CorpusId":253734854},"title":"Visual Programming: Compositional visual reasoning without training"},{"paperId":"a15827c9a5d4be3f0a128ff7e5b184f4a484bde2","externalIds":{"DBLP":"journals/corr/abs-2210-16943","ArXiv":"2210.16943","DOI":"10.1109/ICASSP49357.2023.10094684","CorpusId":253237620},"title":"Vitasd: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"a55400d3def03879c6152387c210d2ce5f59f713","externalIds":{"DBLP":"journals/corr/abs-2210-08477","ArXiv":"2210.08477","DOI":"10.1145/3581641.3584078","CorpusId":252918751},"title":"Large-scale Text-to-Image Generation Models for Visual Artists’ Creative Works"},{"paperId":"197678d7a28be18b3de1fb03a6728dccef85d2b2","externalIds":{"ArXiv":"2210.03112","DBLP":"journals/corr/abs-2210-03112","DOI":"10.1109/CVPR52729.2023.01041","CorpusId":252735318},"title":"A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning"},{"paperId":"eb85c73e8bcfb66bcd5dde49e43c179e2c561ffe","externalIds":{"ArXiv":"2209.10767","DBLP":"journals/corr/abs-2209-10767","DOI":"10.1109/WACV56688.2023.00110","CorpusId":252439240},"title":"DRAMA: Joint Risk Localization and Captioning in Driving"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","externalIds":{"DBLP":"journals/corr/abs-2209-11302","ArXiv":"2209.11302","DOI":"10.1109/ICRA48891.2023.10161317","CorpusId":252519594},"title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"d98eb094dff70a51959b1665a0b804b94fed45c8","externalIds":{"DBLP":"conf/ivs/MaW24","ArXiv":"2209.09178","DOI":"10.1109/IV55156.2024.10588802","CorpusId":252367491},"title":"ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection"},{"paperId":"91deaf9d324c8feafc189da0da03e60a60287bca","externalIds":{"ArXiv":"2209.07753","DBLP":"conf/icra/LiangHXXHIFZ23","DOI":"10.1109/ICRA48891.2023.10160591","CorpusId":252355542},"title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"fe38342d754d76b68767660d36e653a08f409dbd","externalIds":{"DBLP":"journals/ijcv/LaiLRR24","PubMedCentral":"10873248","ArXiv":"2208.04464","DOI":"10.1007/s11263-023-01879-7","CorpusId":251442710,"PubMed":"38371492"},"title":"In the Eye of Transformer: Global–Local Correlation for Egocentric Gaze Estimation and Beyond"},{"paperId":"794c5d5ca20e71eae416da91cf1fed0a8ef15658","externalIds":{"ArXiv":"2207.06405","DBLP":"journals/corr/abs-2207-06405","DOI":"10.48550/arXiv.2207.06405","CorpusId":250491011},"title":"Masked Autoencoders that Listen"},{"paperId":"f3cf71c51b882fe3111d71c4bf104297d38197f8","externalIds":{"ArXiv":"2207.05608","DBLP":"conf/corl/HuangXXCLFZTMCS22","DOI":"10.48550/arXiv.2207.05608","CorpusId":250451569},"title":"Inner Monologue: Embodied Reasoning through Planning with Language Models"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","externalIds":{"ArXiv":"2207.04429","DBLP":"conf/corl/ShahOIL22","DOI":"10.48550/arXiv.2207.04429","CorpusId":250426345},"title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","externalIds":{"DBLP":"journals/corr/abs-2205-11916","ArXiv":"2205.11916","CorpusId":249017743},"title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"4c6c00c5a4e7fe0973befe47f280816a2c113ba1","externalIds":{"DBLP":"journals/corr/abs-2205-05277","ArXiv":"2205.05277","DOI":"10.24963/ijcai.2022/700","CorpusId":248693530},"title":"AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"a58b3f2ab75fdbda082e684d027ab4f552b0b5d3","externalIds":{"DBLP":"journals/corr/abs-2204-05186","ArXiv":"2204.05186","DOI":"10.48550/arXiv.2204.05186","CorpusId":248085271},"title":"Correcting Robot Plans with Natural Language Feedback"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","externalIds":{"DBLP":"conf/corl/IchterBCFHHHIIJ22","ArXiv":"2204.01691","CorpusId":247939706},"title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","externalIds":{"ArXiv":"2204.00598","DBLP":"journals/corr/abs-2204-00598","CorpusId":247922520},"title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"182b92dafafd619a472418818fda0947efc100a8","externalIds":{"ArXiv":"2203.09127","DBLP":"conf/kdd/Huang0SSHZF22","DOI":"10.1145/3534678.3539021","CorpusId":247997031},"title":"ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"f4df78183261538e718066331898ee5cad7cad05","externalIds":{"DBLP":"journals/corr/abs-2202-12837","ArXiv":"2202.12837","ACL":"2022.emnlp-main.759","DOI":"10.18653/v1/2022.emnlp-main.759","CorpusId":247155069},"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"1d803f07e4591bd67c358eef715bcd443e821894","externalIds":{"ArXiv":"2202.02005","DBLP":"journals/corr/abs-2202-02005","CorpusId":237257594},"title":"BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7478cbf46623440f99c0ae2f7daaad13a6be67b3","externalIds":{"DBLP":"journals/imwut/TongGL21","DOI":"10.1145/3494995","CorpusId":245568452},"title":"Zero-Shot Learning for IMU-Based Activity Recognition Using Video Embeddings"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"e77c484af99fc1eb3d3c36699ac81822e98cb74d","externalIds":{"DBLP":"conf/cvpr/LuddeckeE22","ArXiv":"2112.10003","DOI":"10.1109/CVPR52688.2022.00695","CorpusId":247794227},"title":"Image Segmentation Using Text and Image Prompts"},{"paperId":"69ee9b3a915951cc84b74599a3a2699a66d4004f","externalIds":{"DBLP":"conf/corl/ShridharMF21","ArXiv":"2109.12098","CorpusId":237396838},"title":"CLIPort: What and Where Pathways for Robotic Manipulation"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"20e6909ce6c5f12b61e5c9022d97134137360273","externalIds":{"DBLP":"conf/corl/NairM0ISF21","ArXiv":"2109.01115","CorpusId":237385309},"title":"Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","externalIds":{"DBLP":"journals/corr/abs-2108-10904","ArXiv":"2108.10904","CorpusId":237291550},"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"235430f9c86e5d78487b9dac602ff5788c629c7c","externalIds":{"DBLP":"journals/corr/abs-2108-05030","ArXiv":"2108.05030","DOI":"10.1109/TITS.2022.3184990","CorpusId":236976327},"title":"DQ-GAT: Towards Safe and Efficient Autonomous Driving With Deep Q-Learning and Graph Attention Networks"},{"paperId":"95c20f35d352f23b19c378c0758b8dc1d7622872","externalIds":{"MAG":"3192360612","DOI":"10.37282/991819.21.35","CorpusId":238655569},"title":"On Aspects of the Theory of Syntax"},{"paperId":"37278f60ce066f4a2ba2f4665c892c5e95a721f2","externalIds":{"DBLP":"journals/corr/abs-2107-07045","ArXiv":"2107.07045","CorpusId":235899040},"title":"Explainable AI: current status and future directions"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","externalIds":{"ArXiv":"2106.13884","DBLP":"journals/corr/abs-2106-13884","CorpusId":235658331},"title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"82b57e0ed286fd8bc591c77b5301c1414055244c","externalIds":{"DBLP":"conf/acl/MiaoMLZ020","ACL":"2021.acl-long.268","ArXiv":"2105.11098","DOI":"10.18653/v1/2021.acl-long.268","CorpusId":235166394},"title":"Prevent the Language Model from being Overconfident in Neural Machine Translation"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","externalIds":{"DBLP":"journals/corr/abs-2104-08786","ArXiv":"2104.08786","ACL":"2022.acl-long.556","DOI":"10.18653/v1/2022.acl-long.556","CorpusId":233296494},"title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":"df37e254a5e2d966680d9c4ef70b44d43a073eb9","externalIds":{"DBLP":"conf/cvpr/LeeKCKR21","ArXiv":"2104.00924","DOI":"10.1109/CVPR46437.2021.00307","CorpusId":233004334},"title":"Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"066a6c744803cb3322c073a5dfd9540d9a799e6d","externalIds":{"DBLP":"journals/corr/abs-2101-05043","ArXiv":"2101.05043","DOI":"10.1109/TIV.2022.3164507","CorpusId":231592413},"title":"Video Action Recognition for Lane-Change Classification and Prediction of Surrounding Vehicles"},{"paperId":"5a94aaa3ad624608e46a75de49452a03de568a07","externalIds":{"MAG":"3093241733","DBLP":"journals/corr/abs-2010-07954","ArXiv":"2010.07954","ACL":"2020.emnlp-main.356","DOI":"10.18653/v1/2020.emnlp-main.356","CorpusId":223953395},"title":"Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding"},{"paperId":"dc4e07576a9ecfee713b5f13ebc989410a5a7417","externalIds":{"DBLP":"conf/itsc/NiWXMYS20","DOI":"10.1109/ITSC45102.2020.9294501","CorpusId":229703352},"title":"A V2X-based Approach for Avoiding Potential Blind-zone Collisions between Right-turning Vehicles and Pedestrians at Intersections*"},{"paperId":"3d5aa58a32a459b433837140f471404113176749","externalIds":{"DBLP":"journals/ral/DingCLEZ21","ArXiv":"2009.08311","MAG":"3086616174","DOI":"10.1109/LRA.2021.3058873","CorpusId":221761353},"title":"Multimodal Safety-Critical Scenarios Generation for Decision-Making Algorithms Evaluation"},{"paperId":"65b7edbf053143d620d1c40563b1fde828f83116","externalIds":{"DBLP":"journals/tiv/OuK20","MAG":"2996542533","DOI":"10.1109/TIV.2019.2960930","CorpusId":214321398},"title":"Enhancing Driver Distraction Recognition Using Generative Adversarial Networks"},{"paperId":"b994a27a4776084136a6782ab7ae9198d9233078","externalIds":{"DOI":"10.37896/sr7.8/037","CorpusId":243254992},"title":"A SURVEY ON DEEP LEARNING TECHNIQUES"},{"paperId":"826b2efc9e3f563e9aa1845e5f88ea725bcefd9e","externalIds":{"MAG":"3046086546","DBLP":"conf/corl/GoyalNM20","ArXiv":"2007.15543","CorpusId":220870869},"title":"PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping Pixels to Rewards"},{"paperId":"10d11f0045dc7f217c7f01bc6cbb47929e9b8808","externalIds":{"DBLP":"journals/corr/abs-2006-16228","ArXiv":"2006.16228","MAG":"3100177202","CorpusId":220249786},"title":"Self-Supervised MultiModal Versatile Networks"},{"paperId":"8cda672bd5487ec2c67d5c217dc84ed8fb786640","externalIds":{"ArXiv":"2011.07231","DBLP":"conf/cvpr/ZhuY20a","MAG":"3104220704","DOI":"10.1109/cvpr42600.2020.00877","CorpusId":219617394},"title":"ActBERT: Learning Global-Local Video-Text Representations"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"1301e9d11b728268ed1ff3f1a9adc155308d5250","externalIds":{"ArXiv":"2005.07648","DBLP":"conf/rss/LynchS21","DOI":"10.15607/RSS.2021.XVII.047","CorpusId":235657751},"title":"Language Conditioned Imitation Learning Over Unstructured Data"},{"paperId":"bc23dfb70e109dbe666b979f995e2779b96e1073","externalIds":{"MAG":"3003205975","DBLP":"journals/arcras/TellexGKM20","DOI":"10.1146/annurev-control-101119-071628","CorpusId":213739887},"title":"Robots That Use Language"},{"paperId":"d3b770e127284f996741086ec6cbffafe635ccd7","externalIds":{"DBLP":"journals/itsm/WangBSWLB20","MAG":"2996811802","DOI":"10.1109/MITS.2019.2953562","CorpusId":210931452},"title":"A Survey on Cooperative Longitudinal Motion Control of Multiple Connected and Automated Vehicles"},{"paperId":"d846b06c98070ed1f9ffc4b5a10bd533640848ae","externalIds":{"MAG":"3034550906","ArXiv":"2003.06537","DBLP":"conf/cvpr/HanZXF20","DOI":"10.1109/cvpr42600.2020.00301","CorpusId":212725768},"title":"OccuSeg: Occupancy-Aware 3D Instance Segmentation"},{"paperId":"129983331ca874142a3e8eb2d93d820bdf1f9aca","externalIds":{"MAG":"3127561923","DBLP":"journals/tits/KiranSTMSYP22","ArXiv":"2002.00444","DOI":"10.1109/TITS.2021.3054625","CorpusId":211011033},"title":"Deep Reinforcement Learning for Autonomous Driving: A Survey"},{"paperId":"af5189707321e9b11165930d1530dd0e23c0c14e","externalIds":{"MAG":"2995697759","DBLP":"journals/corr/abs-1912-10773","ArXiv":"1912.10773","DOI":"10.1109/TITS.2019.2962338","CorpusId":209444730},"title":"A Survey of Deep Learning Applications to Autonomous Vehicle Control"},{"paperId":"8406903fd2f0eb25349bf071ccfaae3947e2a9cd","externalIds":{"MAG":"3035172746","DBLP":"conf/cvpr/SunKDCPTGZCCVHN20","ArXiv":"1912.04838","DOI":"10.1109/CVPR42600.2020.00252","CorpusId":209140225},"title":"Scalability in Perception for Autonomous Driving: Waymo Open Dataset"},{"paperId":"c8b041226ef3231d87297ac546bd1c2fd82b35ca","externalIds":{"MAG":"3003954087","DBLP":"conf/iros/NMKGBK19","DOI":"10.1109/IROS40897.2019.8967929","CorpusId":210971098},"title":"Talk to the Vehicle: Language Conditioned Autonomous Navigation of Self Driving Cars"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"a595767fa35bcc84362f629fbc4d2d9b05d7342a","externalIds":{"MAG":"2981207549","DBLP":"journals/jfr/GrigorescuTCM20","ArXiv":"1910.07738","DOI":"10.1002/rob.21918","CorpusId":204744017},"title":"A survey of deep learning techniques for autonomous driving"},{"paperId":"ecf2a5496c765c8a0133c45952e82e3756961a11","externalIds":{"DBLP":"journals/corr/abs-1909-10838","MAG":"2970603850","ArXiv":"1909.10838","ACL":"D19-1215","DOI":"10.18653/v1/D19-1215","CorpusId":202734592},"title":"Talk2Car: Taking Control of Your Self-Driving Car"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","externalIds":{"ArXiv":"1908.03557","MAG":"2968124245","DBLP":"journals/corr/abs-1908-03557","CorpusId":199528533},"title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","externalIds":{"MAG":"2966715458","DBLP":"journals/corr/abs-1908-02265","ArXiv":"1908.02265","CorpusId":199453025},"title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"63cb6c387770ba5a134da3dad93f8f34621b5b21","externalIds":{"MAG":"2904393593","DBLP":"journals/adhoc/CuiLSZ19","DOI":"10.1016/J.ADHOC.2018.12.006","CorpusId":69657049},"title":"A review on safety failures, security attacks, and available countermeasures for autonomous vehicles"},{"paperId":"c2c8482c713b94073f3d59895b373db4398ddfbb","externalIds":{"DBLP":"conf/nips/JiangGMF19","ArXiv":"1906.07343","MAG":"2951725892","CorpusId":189998275},"title":"Language as an Abstraction for Hierarchical Deep Reinforcement Learning"},{"paperId":"d9f1f536c67e650992ec4afd66166740f860c99b","externalIds":{"MAG":"2953303875","DBLP":"journals/access/YurtseverLCT20","ArXiv":"1906.05113","DOI":"10.1109/ACCESS.2020.2983149","CorpusId":186206717},"title":"A Survey of Autonomous Driving: Common Practices and Emerging Technologies"},{"paperId":"7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50","externalIds":{"DBLP":"conf/ijcai/LuketinaNFFAGWR19","MAG":"2948380112","ArXiv":"1906.03926","DOI":"10.24963/ijcai.2019/880","CorpusId":182952502},"title":"A Survey of Reinforcement Learning Informed by Natural Language"},{"paperId":"8e66c7e494476eb0dee846349df1bd705ceac6c3","externalIds":{"DBLP":"conf/cvpr/ChangLSSBHW0LRH19","MAG":"2983397630","ArXiv":"1911.02620","DOI":"10.1109/CVPR.2019.00895","CorpusId":198162846},"title":"Argoverse: 3D Tracking and Forecasting With Rich Maps"},{"paperId":"a7231cc5f146c37e4bf558ae8ca7a8e9a8226a56","externalIds":{"DBLP":"conf/icra/Abeysirigoonawardena19","MAG":"2968176166","DOI":"10.1109/ICRA.2019.8793740","CorpusId":54033845},"title":"Generating Adversarial Driving Scenarios in High-Fidelity Simulators"},{"paperId":"fe55041b55851074a95536540d5f03fa7dc9d93c","externalIds":{"DOI":"10.1142/9789811203909_0018","CorpusId":242573075},"title":"Baidu"},{"paperId":"21cc0c84033940460a0697d567dfb799e89ca760","externalIds":{"MAG":"2934625602","DBLP":"journals/tvt/XingLWCVW19","DOI":"10.1109/TVT.2019.2908425","CorpusId":132992248},"title":"Driver Activity Recognition for Intelligent Vehicles: A Deep Learning Approach"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"3bb322718d64a34b91b29c8230c5978de5d7fb7a","externalIds":{"MAG":"2905076052","ArXiv":"1812.05784","DBLP":"conf/cvpr/LangVCZYB19","DOI":"10.1109/CVPR.2019.01298","CorpusId":55701967},"title":"PointPillars: Fast Encoders for Object Detection From Point Clouds"},{"paperId":"c6f33988f72ab7ea1549962bc5d1f7bc5d2fd750","externalIds":{"MAG":"2904904015","DBLP":"conf/itst/LiuMO18","DOI":"10.1109/ITST.2018.8566704","CorpusId":54462554},"title":"Cooperation of V2I/P2I Communication and Roadside Radar Perception for the Safety of Vulnerable Road Users"},{"paperId":"9d0907770cd4619aa6a36139a859e8f09bc9f0ef","externalIds":{"MAG":"2885138528","ArXiv":"1807.11546","DBLP":"conf/eccv/KimRDCA18","DOI":"10.1007/978-3-030-01216-8_35","CorpusId":51887402},"title":"Textual Explanations for Self-Driving Vehicles"},{"paperId":"c09a440abdc92d187c53812d8aa29ed7de29a37b","externalIds":{"ArXiv":"1807.00412","MAG":"2810677299","DBLP":"journals/corr/abs-1807-00412","DOI":"10.1109/ICRA.2019.8793742","CorpusId":49558891},"title":"Learning to Drive in a Day"},{"paperId":"49c0769b9100ed7148648ce11740055f04ca864d","externalIds":{"DBLP":"conf/iccps/KatoTMMHKMAFA18","MAG":"2798786289","DOI":"10.1109/ICCPS.2018.00035","CorpusId":13792468},"title":"Autoware on Board: Enabling Autonomous Vehicles with Embedded Systems"},{"paperId":"bd79a394b3eb055a0c73a34f3428704ca6404cbd","externalIds":{"DOI":"10.1007/s41321-018-0013-3","CorpusId":40213299},"title":"Autonomous Driving"},{"paperId":"5383ee00a5c0c3372081fccfeceb812b7854c9ea","externalIds":{"MAG":"2780740184","DBLP":"journals/corr/abs-1712-06080","ArXiv":"1712.06080","DOI":"10.1609/aaai.v32i1.12301","CorpusId":9164115},"title":"Spatial As Deep: Spatial CNN for Traffic Scene Understanding"},{"paperId":"e854977a9c4c2effc42f2e24064726fb6307b6f5","externalIds":{"MAG":"2951984500","DBLP":"journals/corr/abs-1711-00482","ArXiv":"1711.00482","ACL":"N18-1197","DOI":"10.18653/v1/N18-1197","CorpusId":37390552},"title":"Learning with Latent Language"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"9b5f696f73c1264ccb8e97d3b738a2342ecd6bee","externalIds":{"MAG":"2619697695","DBLP":"journals/corr/ArandjelovicZ17","ArXiv":"1705.08168","DOI":"10.1109/ICCV.2017.73","CorpusId":10769575},"title":"Look, Listen and Learn"},{"paperId":"572785b5d6f6fa4b174d79725f82c056b0fb4565","externalIds":{"MAG":"2609532991","ArXiv":"1704.05519","DBLP":"journals/corr/JanaiGBG17","DOI":"10.1561/0600000079","CorpusId":23267374},"title":"Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art"},{"paperId":"d997beefc0922d97202789d2ac307c55c2c52fba","externalIds":{"MAG":"2950642167","DBLP":"conf/cvpr/QiSMG17","ArXiv":"1612.00593","DOI":"10.1109/CVPR.2017.16","CorpusId":5115938},"title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"},{"paperId":"4cdcf2ae5e1fafebd9b3613247a7b1962584da34","externalIds":{"MAG":"2339077268","DBLP":"journals/corr/QiSNDYG16","ArXiv":"1604.03265","DOI":"10.1109/CVPR.2016.609","CorpusId":1009127},"title":"Volumetric and Multi-view CNNs for Object Classification on 3D Data"},{"paperId":"86c35b55ee582b331edbac166084554b15b72a0c","externalIds":{"DBLP":"journals/micro/KatoTINTH15","MAG":"2206222117","DOI":"10.1109/MM.2015.133","CorpusId":206473388},"title":"An Open Approach to Autonomous Vehicles"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","externalIds":{"DBLP":"journals/corr/AntolALMBZP15","MAG":"1933349210","ArXiv":"1505.00468","DOI":"10.1007/s11263-016-0966-6","CorpusId":3180429},"title":"VQA: Visual Question Answering"},{"paperId":"dc1c4a97ad967ad3df6dfd11128562a0befbefe7","externalIds":{"MAG":"1985261475","DOI":"10.1109/TNSRE.2015.2415520","CorpusId":20133108,"PubMed":"25850090"},"title":"EEG-Based Attention Tracking During Distracted Driving"},{"paperId":"eaf474b729604fb83a68afb2220e289dc9829ecc","externalIds":{"MAG":"2000840273","DBLP":"journals/tits/WangZWDC15","DOI":"10.1109/TITS.2014.2330979","CorpusId":5568620},"title":"Online Prediction of Driver Distraction Based on Brain Activity Patterns"},{"paperId":"2b91c03e7be00b2eee610a73ef2becc669e01781","externalIds":{"DBLP":"conf/icassp/MrouehMG15","MAG":"2949547965","ArXiv":"1501.05396","DOI":"10.1109/ICASSP.2015.7178347","CorpusId":351326},"title":"Deep multimodal learning for Audio-Visual Speech Recognition"},{"paperId":"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745","externalIds":{"ArXiv":"1412.6632","MAG":"1811254738","DBLP":"journals/corr/MaoXYWY14a","CorpusId":3509328},"title":"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)"},{"paperId":"ac3ee98020251797c2b401e1389461df88e52e62","externalIds":{"MAG":"1924770834","DBLP":"journals/corr/ChungGCB14","ArXiv":"1412.3555","CorpusId":5201925},"title":"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"},{"paperId":"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","externalIds":{"ArXiv":"1412.2306","DBLP":"journals/corr/KarpathyF14","MAG":"1905882502","DOI":"10.1109/CVPR.2015.7298932","CorpusId":8517067},"title":"Deep visual-semantic alignments for generating image descriptions"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","externalIds":{"MAG":"1895577753","DBLP":"journals/corr/VinyalsTBE14","ArXiv":"1411.4555","DOI":"10.1109/CVPR.2015.7298935","CorpusId":1169492},"title":"Show and tell: A neural image caption generator"},{"paperId":"3ed2c33dd7d8289cd973a5fe4c9e7c9176bfffcb","externalIds":{"DBLP":"journals/itsm/BenglerDFMSW14","MAG":"2047030012","DOI":"10.1109/MITS.2014.2336271","CorpusId":14390213},"title":"Three Decades of Driver Assistance Systems: Review and Future Perspectives"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","externalIds":{"MAG":"2130942839","DBLP":"conf/nips/SutskeverVL14","ArXiv":"1409.3215","CorpusId":7961699},"title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","externalIds":{"MAG":"2950577311","DBLP":"journals/corr/abs-1301-3781","ArXiv":"1301.3781","CorpusId":5959482},"title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42","externalIds":{"DBLP":"conf/cvpr/GeigerLU12","MAG":"2150066425","DOI":"10.1109/CVPR.2012.6248074","CorpusId":6724907},"title":"Are we ready for autonomous driving? The KITTI vision benchmark suite"},{"paperId":"28c41e35948d0aca47497cbdff1d9486004093e5","externalIds":{"MAG":"2139137304","DBLP":"conf/ivs/LevinsonABDHKKLPPSSSTWT11","DOI":"10.1109/IVS.2011.5940562","CorpusId":10519397},"title":"Towards fully autonomous driving: Systems and algorithms"},{"paperId":"a5b1bf6584f24fe3ccc8d67e5bba369783f817f0","externalIds":{"DBLP":"conf/hri/KollarTRR10","MAG":"2069809153","DOI":"10.1145/1734454.1734553","CorpusId":276090},"title":"Toward understanding natural language directions"},{"paperId":"9e938cf8a74c2e7415bfd10d66a9ea4b1c3e0e15","externalIds":{"MAG":"2154844948","DBLP":"journals/jfr/ThrunMDSADFGHHLOPPSSDJKMRNJABDEKNM06","DOI":"10.1002/rob.20147","CorpusId":1438204},"title":"Stanley: The robot that won the DARPA Grand Challenge"},{"paperId":"fa0f137fb007d169d368873484633ef7c45de003","externalIds":{"MAG":"1636244751","DBLP":"journals/ml/FineST98","DOI":"10.1023/A:1007469218079","CorpusId":3465810},"title":"The Hierarchical Hidden Markov Model: Analysis and Applications"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"e23c34414e66118ecd9b08cf0cd4d016f59b0b85","externalIds":{"DBLP":"journals/tsp/SchusterP97","MAG":"2131774270","DOI":"10.1109/78.650093","CorpusId":18375389},"title":"Bidirectional recurrent neural networks"},{"paperId":"3de5d40b60742e3dfa86b19e7f660962298492af","externalIds":{"MAG":"2121227244","ACL":"J92-4003","DBLP":"journals/coling/BrownPdLM92","CorpusId":10986188},"title":"Class-Based n-gram Models of Natural Language"},{"paperId":"89de521e6a64430a31844dd7a9f2f3f8794a0c1d","externalIds":{"DBLP":"conf/acm/KanadeTW86","MAG":"2094430142","DOI":"10.1145/324634.325197","CorpusId":2308303},"title":"Autonomous land vehicle project at CMU"},{"paperId":"10f7507b8408bf35125b8e04254ad890c8d45e1d","externalIds":{"MAG":"1525482321","CorpusId":54114373},"title":"Procedures As A Representation For Data In A Computer Program For Understanding Natural Language"},{"paperId":"3d9ea7086ceef1af211996ae4d4213d824811b97","externalIds":{"MAG":"2076379374","DOI":"10.4159/HARVARD.9780674421974","CorpusId":62697326},"title":"Automatic language translation : lexical and technical aspects, with particular reference to Russian"},{"paperId":"704676f0ade49ec4ff11489d9e88f649e949acab","externalIds":{"MAG":"2050047175","DBLP":"conf/aieeire/HoltT60","DOI":"10.1145/1460361.1460405","CorpusId":15299738},"title":"Man-to-machine communication and automatic code translation"},{"paperId":"825298ea1933f01f1cfbff0c5e9bca4c60fdb5cd","externalIds":{"DBLP":"journals/corr/abs-2309-13193","DOI":"10.48550/arXiv.2309.13193","CorpusId":262460657},"title":"SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","externalIds":{"ArXiv":"2303.17580","DBLP":"journals/corr/abs-2303-17580","DOI":"10.48550/arXiv.2303.17580","CorpusId":257833781},"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"cc5abf7957e0038028b17f5fa5e8e8562f6369f0","externalIds":{"DBLP":"conf/emnlp/ZhengYJLLSW23","DOI":"10.18653/v1/2023.findings-emnlp.734","CorpusId":266167093},"title":"Chain-of-Thought Reasoning in Tabular Language Models"},{"paperId":"dccd764ec820c13369e91c53890dfc8cd0334355","externalIds":{"DBLP":"journals/corr/abs-2211-11682","DOI":"10.48550/arXiv.2211.11682","CorpusId":253735373},"title":"PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning"},{"paperId":"4a761322413c40d482bce9f3477137ca08956861","externalIds":{"DBLP":"conf/eccv/MindererGSNWDMA22","DOI":"10.1007/978-3-031-20080-9_42","CorpusId":253304326},"title":"Simple Open-Vocabulary Object Detection"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"0c0a778e6fdf7e36b1750c533dcc916f86608607","externalIds":{"MAG":"2527310337","DBLP":"journals/tkde/XunJGZ17","DOI":"10.1109/TKDE.2016.2614508","CorpusId":13490401},"title":"A Survey on Context Learning"},{"paperId":"7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3","externalIds":{"CorpusId":18420840},"title":"ALVINN, an autonomous land vehicle in a neural network"},{"paperId":"a20788a28ef55c8105237a7aa8af45418828c74f","externalIds":{"CorpusId":218560134},"title":"at Road"},{"paperId":"a3ee0f8c5efa5605182d0fdc93cdb8df3c5f4b37","externalIds":{"MAG":"2070183171","DBLP":"journals/ai/SlaneyT01","DOI":"10.1016/S0004-3702(00)00079-5","CorpusId":40952164},"title":"Blocks World revisited"},{"paperId":"68c2dd3954fca8dad215696e9afa375ffc212ffd","externalIds":{"DBLP":"journals/ac/Bar-Hillel60","MAG":"1608789752","DOI":"10.1016/S0065-2458(08)60607-5","CorpusId":18822284},"title":"The Present Status of Automatic Translation of Languages"},{"paperId":"875b416e3c0420a46f47920d5ae3a504974174fa","externalIds":{"DOI":"10.4271/j3016_201609","CorpusId":53007988},"title":"Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles"},{"paperId":"943faccf34106e7ed579820cd0c746015dded70c","externalIds":{"CorpusId":282040330},"title":"Imagined Subgoals for Hierarchical Goal-Conditioned Policies"}]}