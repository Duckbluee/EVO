{"paperId":"d21d09ca270dd27baa8c34ba09662c847663a517","externalIds":{"DBLP":"journals/tacl/BergerSAF25","ArXiv":"2408.04909","DOI":"10.1162/tacl.a.52","CorpusId":276741430},"title":"Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy, Trends, and Metrics Analysis","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2052147700","name":"Uri Berger"},{"authorId":"2126417012","name":"Gabriel Stanovsky"},{"authorId":"2769805","name":"Omri Abend"},{"authorId":"2285459587","name":"Lea Frermann"}],"abstract":"\n The task of image captioning has recently been gaining popularity, and with it the complex task of evaluating the quality of image captioning models. In this work, we present the first survey and taxonomy of over 70 different image captioning metrics and their usage in hundreds of papers, specifically designed to help users select the most suitable metric for their needs. We find that despite the diversity of proposed metrics, the vast majority of studies rely on only five popular metrics, which we show to be weakly correlated with human ratings. We hypothesize that combining a diverse set of metrics can enhance correlation with human ratings. As an initial step, we demonstrate that a linear regression-based ensemble method, which we call EnsembEval, trained on one human ratings dataset, achieves improved correlation across five additional datasets, showing there is a lot of room for improvement by leveraging a diverse set of metrics.1"}