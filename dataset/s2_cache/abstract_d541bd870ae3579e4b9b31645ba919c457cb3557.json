{"abstract":"Asynchronous training is widely used for scaling DNN training over large-scale distributed deep learning systems using the parameter server architecture. Communication has been identified as the bottleneck, since large volumes of data are exchanged during the training, which greatly slows down the training procedure. Recent studies try to reduce the communication traffic through gradients sparsification and quantitation approaches. We identify three limitations in previous studies. First, the fundamental guideline for gradient sparsification of their work is the magnitude of the gradients. However, the gradients' magnitude cannot fully indicate the significance of the parameters. The significance of the parameters involves their contribution to the prediction accuracy of DNN models. Second, their gradient quantitation method often leads to error accumulation for gradients aggregation and affects convergence behaviors. Third, previous quantitation approaches are CPU intensive, which generate strong overhead for the server. We propose MIPD, an adaptive and layer-wised gradients sparsification framework that compresses the gradients based on model interpretability and probability distribution of gradients. MIPD compresses the gradients according to the corresponding significance of its parameters, which is defined by model interpretability. In order to increase the robustness of our approach, we also propose an Exponential Smoothing method to further identify the significant parameters and to compensate for the dropped gradients on the server. For reducing the CPU overhead of the server, MIDP proposes to update half of the parameters for each training step. Furthermore, it encodes the gradients based on their probability distribution, thereby minimizing their approximated errors. Extensive experimental results generated on the GPU cluster indicate that the proposed framework effectively improves the training performance of DNNs by up to 36.2%, which ensures high accuracy as compared to state-of-art solutions. Accordingly, the CPU and network usage of the server dropped by up to 42.0% and 32.7% respectively."}