{"abstract":"Interpreting decisions made by deep neural networks (DNNs) has recently received wide attention. Specifically, this field is advanced to reveal the black box of the decisionmaking process of DNNs to facilitate reliable real applications. One recent method, U-Noise, realizes this by introducing an additional model to interpret the image segmentation process. By assuming that important pixels for segmentation should not be hindered by noise, such a model learns a noise mask as an interpretability map to identify which pixels can be added with noise. However, U-Noise regards all pixels independently during noise mask learning, which can cause the interpretability map to be less smooth and continuous. In this study, we propose a smoothing loss to better guide interpretability learning. It works by introducing a new assumption that important pixels for segmentation are also likely to be spatially close. We draw inspiration from the bilateral filter to design the smoothing loss, which enables a two-fold smoothing strategy with regard to the spatial location and pixel intensity. Experiments on a medical image segmentation dataset demonstrate that our method can generate a smoother yet more accurate interpretability map than prior methods."}