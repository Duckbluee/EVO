{"references":[{"paperId":"44d16a076c00ecada3d425203377e4ec951c4ed0","externalIds":{"DBLP":"journals/corr/abs-2311-10537","ArXiv":"2311.10537","DOI":"10.48550/arXiv.2311.10537","CorpusId":265281260},"title":"MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning"},{"paperId":"575a215f1080dfc0e9f8d055775c294e765aac84","externalIds":{"PubMedCentral":"10656647","DOI":"10.1001/jamanetworkopen.2023.43689","CorpusId":265277446,"PubMed":"37976064"},"title":"Leveraging Large Language Models for Decision Support in Personalized Oncology"},{"paperId":"f1731ac89acecc0f3f6a531df49e79d3e50003c5","externalIds":{"DBLP":"journals/corr/abs-2310-18729","ArXiv":"2310.18729","DOI":"10.48550/arXiv.2310.18729","CorpusId":264590595},"title":"Using Large Language Models to Support Thematic Analysis in Empirical Legal Studies"},{"paperId":"df2ed9f2d994cc91a710261398ff04b01d1a9f7c","externalIds":{"ArXiv":"2310.13345","DBLP":"journals/corr/abs-2310-13345","DOI":"10.48550/arXiv.2310.13345","CorpusId":264406064},"title":"An LLM can Fool Itself: A Prompt-Based Adversarial Attack"},{"paperId":"0f92d5a01baa449edc5592716dd639ec7868c44f","externalIds":{"DBLP":"journals/corr/abs-2310-11207","ArXiv":"2310.11207","DOI":"10.48550/arXiv.2310.11207","CorpusId":264172366},"title":"Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations"},{"paperId":"4f63c5a89c7299a864c6c48aa1844fb0fe8c9437","externalIds":{"ArXiv":"2310.10844","DBLP":"journals/corr/abs-2310-10844","DOI":"10.48550/arXiv.2310.10844","CorpusId":264172191},"title":"Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"},{"paperId":"9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632","externalIds":{"ArXiv":"2310.07984","DBLP":"journals/corr/abs-2310-07984","DOI":"10.48550/arXiv.2310.07984","CorpusId":263908992},"title":"Large Language Models for Scientific Synthesis, Inference and Explanation"},{"paperId":"dc976bca01f879f18c9302c9119d7f1ba41332b7","externalIds":{"DOI":"10.56726/irjmets45213","CorpusId":264082112},"title":"Contribution and performance of ChatGPT and other Large Language Models (LLM) for scientific and research advancements: a double-edged sword"},{"paperId":"ee7c66ffc882f6eaa639bd95c30c61b30d3d4dd5","externalIds":{"DOI":"10.1007/s00405-023-08267-4","CorpusId":263907057,"PubMed":"37819549"},"title":"Correction: Exploring the potential of Chat-GPT as a supportive tool for sialendoscopy clinical decision making and patient information support"},{"paperId":"78a48c296267ae59b4be60ebbbf96b5d7f6c693b","externalIds":{"DBLP":"journals/corr/abs-2310-05421","ArXiv":"2310.05421","DOI":"10.48550/arXiv.2310.05421","CorpusId":263830717},"title":"Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations"},{"paperId":"faab24bc6cd4a4dea6e82420d145f08445c05fc7","externalIds":{"ArXiv":"2310.05175","DBLP":"conf/icml/0006W0HWJLJPLBW24","DOI":"10.48550/arXiv.2310.05175","CorpusId":263829692},"title":"Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"},{"paperId":"5f5b57728562e43cb564eb6ebd19a81a39120387","externalIds":{"ArXiv":"2311.06981","DBLP":"journals/corr/abs-2311-06981","DOI":"10.48550/arXiv.2311.06981","CorpusId":265150467},"title":"Using Generative AI for Literature Searches and Scholarly Writing: Is the Integrity of the Scientific Discourse in Jeopardy?"},{"paperId":"a8aef5a15dc2b486b3bf01205d2687a1140a41bb","externalIds":{"ArXiv":"2310.02374","DBLP":"journals/corr/abs-2310-02374","DOI":"10.48550/arXiv.2310.02374","CorpusId":263622205},"title":"Conversational Health Agents: A Personalized LLM-Powered Agent Framework"},{"paperId":"5432b77bfb1dced97c5b1fc684b0fa7d0d84c424","externalIds":{"ArXiv":"2311.10723","DBLP":"journals/corr/abs-2311-10723","DOI":"10.1145/3604237.3626869","CorpusId":265294420},"title":"Large Language Models in Finance: A Survey"},{"paperId":"3041dc1dc5db0c332b8ee55b11db47e817f4527a","externalIds":{"DOI":"10.1038/s41433-023-02759-7","CorpusId":262749079,"PubMed":"37749374"},"title":"Large language model (LLM)-driven chatbots for neuro-ophthalmic medical education"},{"paperId":"b6346f9fa093b8e85df712485a2b851b9f680dac","externalIds":{"DBLP":"journals/corr/abs-2309-12307","ArXiv":"2309.12307","DOI":"10.48550/arXiv.2309.12307","CorpusId":262084134},"title":"LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"},{"paperId":"8b37d13e9ba87792d949e8af5f13ef8f83ee0bf9","externalIds":{"DOI":"10.1681/ASN.0000000000000237","CorpusId":262085414,"PubMed":"37731175"},"title":"Does ChatGPT Help Us Understand the Medical Literature?"},{"paperId":"755853c6b30f5a186131e23a63c68a3f2737068e","externalIds":{"ArXiv":"2309.10062","DBLP":"conf/iros/KannanVM24","DOI":"10.1109/IROS58592.2024.10802322","CorpusId":262055166},"title":"SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models"},{"paperId":"0c72450890a54b68d63baa99376131fda8f06cf9","externalIds":{"ArXiv":"2309.07864","DBLP":"journals/corr/abs-2309-07864","DOI":"10.48550/arXiv.2309.07864","CorpusId":261817592},"title":"The Rise and Potential of Large Language Model Based Agents: A Survey"},{"paperId":"5a9d4bcffa9989cac4139b2844358884ae023e8d","externalIds":{"DBLP":"journals/corr/abs-2309-04077","ArXiv":"2309.04077","DOI":"10.48550/arXiv.2309.04077","CorpusId":261660608},"title":"SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments"},{"paperId":"a7de810b4787e6a453b48d3c83e14c489e8fdd8c","externalIds":{"DBLP":"conf/goodit/MontagnaFKFP23","DOI":"10.1145/3582515.3609536","CorpusId":260900725},"title":"Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease Self-Management"},{"paperId":"d00735241af700d21762d2f3ca00d920241a15a4","externalIds":{"DBLP":"journals/corr/abs-2309-01219","ArXiv":"2309.01219","DOI":"10.1162/coli.a.16","CorpusId":261530162},"title":"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"},{"paperId":"26089bdfdbca1e6eaaceca71e3116b715bec6d47","externalIds":{"DBLP":"journals/corr/abs-2309-01029","ArXiv":"2309.01029","DOI":"10.1145/3639372","CorpusId":261530292},"title":"Explainability for Large Language Models: A Survey"},{"paperId":"819bbdc2dac9e13d9ca3e2508a6e063186ce5e40","externalIds":{"DBLP":"conf/iclr/PengQFS24","ArXiv":"2309.00071","DOI":"10.48550/arXiv.2309.00071","CorpusId":261493986},"title":"YaRN: Efficient Context Window Extension of Large Language Models"},{"paperId":"24d836bbc35413d76c3c69cb30bfc0f1449f5207","externalIds":{"DBLP":"journals/arobots/SinghBMGXTFTG23","DOI":"10.1007/s10514-023-10135-3","CorpusId":261332464},"title":"ProgPrompt: program generation for situated robot task planning using large language models"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","externalIds":{"DBLP":"journals/fcsc/WangMFZYZCTCLZWW24","ArXiv":"2308.11432","DOI":"10.1007/s11704-024-40231-1","CorpusId":261064713},"title":"A survey on large language model based autonomous agents"},{"paperId":"2dfb9171e180dcb0af23d305e024d43d311708ab","externalIds":{"DBLP":"journals/corr/abs-2308-10882","ArXiv":"2308.10882","DOI":"10.48550/arXiv.2308.10882","CorpusId":261048876},"title":"Giraffe: Adventures in Expanding Context Lengths in LLMs"},{"paperId":"338d8f3b199abcebc85f34016b0162ab3a9d5310","externalIds":{"DBLP":"journals/corr/abs-2308-07633","ArXiv":"2308.07633","DOI":"10.1162/tacl_a_00704","CorpusId":260900101},"title":"A Survey on Model Compression for Large Language Models"},{"paperId":"81b10e64133e775dab53153cc82277d276efe1f7","externalIds":{"DBLP":"conf/iclr/YaoHNLFXNC0AXMW24","ArXiv":"2308.02151","DOI":"10.48550/arXiv.2308.02151","CorpusId":260611249},"title":"Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization"},{"paperId":"02d4096c030d052e1866d52fbc3b83480e1ed9f5","externalIds":{"DBLP":"conf/kdd/DongMXM023","DOI":"10.1145/3580305.3599572","CorpusId":260499677},"title":"Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques"},{"paperId":"91206346edbe28abb606d7b3425cd455d4019d4f","externalIds":{"DBLP":"journals/corr/abs-2308-01825","ArXiv":"2308.01825","DOI":"10.48550/arXiv.2308.01825","CorpusId":260438790},"title":"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"},{"paperId":"e1ea0e1614baf4e247ae4e38e9dd033c8af8070a","externalIds":{"ArXiv":"2308.02678","DBLP":"journals/corr/abs-2308-02678","DOI":"10.48550/arXiv.2308.02678","CorpusId":260683113},"title":"Ethical Considerations and Policy Implications for Large Language Models: Guiding Responsible Development and Deployment"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","externalIds":{"ArXiv":"2308.00675","DBLP":"journals/corr/abs-2308-00675","DOI":"10.48550/arXiv.2308.00675","CorpusId":260351459},"title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"703035b483c181953de1b55b5fd59cd4cd4cf211","externalIds":{"DBLP":"journals/corr/abs-2308-00352","ArXiv":"2308.00352","DOI":"10.48550/arXiv.2308.00352","CorpusId":260351380},"title":"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","externalIds":{"DBLP":"journals/corr/abs-2307-16789","ArXiv":"2307.16789","DOI":"10.48550/arXiv.2307.16789","CorpusId":260334759},"title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"af6d0ba799213cbbcbfceb1fb9b78d2858486308","externalIds":{"DBLP":"journals/corr/abs-2307-14535","ArXiv":"2307.14535","DOI":"10.48550/arXiv.2307.14535","CorpusId":260203080},"title":"Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"ae22f7c57916562e2729a1a7f34298e4220b77a7","externalIds":{"DBLP":"conf/eacl/WangYW24","ACL":"2024.eacl-long.105","ArXiv":"2307.07164","DOI":"10.48550/arXiv.2307.07164","CorpusId":259924840},"title":"Learning to Retrieve In-Context Examples for Large Language Models"},{"paperId":"c5d18dbb92d0cd5393baa1e69de33d6922ac3e57","externalIds":{"DBLP":"conf/icra/MandiJS24","ArXiv":"2307.04738","DOI":"10.1109/ICRA57147.2024.10610855","CorpusId":259501567},"title":"RoCo: Dialectic Multi-Robot Collaboration with Large Language Models"},{"paperId":"467c49fd4f69638ddeda1ff3241fe4495151cf0e","externalIds":{"DOI":"10.1007/s10439-023-03306-x","CorpusId":259500055,"PubMed":"37428337"},"title":"A Domain-Specific Next-Generation Large Language Model (LLM) or ChatGPT is Required for Biomedical Engineering and Research"},{"paperId":"0893549771094fac547432cb4f84e9605c911a86","externalIds":{"PubMedCentral":"10326069","DBLP":"journals/npjdm/MeskoT23","DOI":"10.1038/s41746-023-00873-0","CorpusId":259357970,"PubMed":"37414860"},"title":"The imperative for regulatory oversight of large language models (or generative AI) in healthcare"},{"paperId":"929305892d4ddae575a0fc23227a8139f7681632","externalIds":{"DBLP":"journals/corr/abs-2307-02483","ArXiv":"2307.02483","DOI":"10.48550/arXiv.2307.02483","CorpusId":259342528},"title":"Jailbroken: How Does LLM Safety Training Fail?"},{"paperId":"c12db2c60e8989f646a29ad4f4d24475e860ad91","externalIds":{"ArXiv":"2307.02486","DBLP":"journals/corr/abs-2307-02486","CorpusId":259341682},"title":"LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"},{"paperId":"ae2d35aeb5dd7043ac0056d60002e2e3c240bd1a","externalIds":{"DOI":"10.1007/s00405-023-08104-8","CorpusId":259333388,"PubMed":"37405455"},"title":"Exploring the potential of Chat-GPT as a supportive tool for sialendoscopy clinical decision making and patient information support"},{"paperId":"065970427ce71b73a99e520e868625fe05664295","externalIds":{"PubMedCentral":"10386180","DOI":"10.3390/vaccines11071217","CorpusId":259710889,"PubMed":"37515033"},"title":"Artificial Intelligence and Public Health: Evaluating ChatGPT Responses to Vaccination Myths and Misconceptions"},{"paperId":"3c996ace0fc6c54dbd9035d92d731bd9a57f8c6d","externalIds":{"DBLP":"conf/icalt/DaiLJLTGC23","DOI":"10.1109/ICALT58122.2023.00100","CorpusId":263231102},"title":"Can Large Language Models Provide Feedback to Students? A Case Study on ChatGPT"},{"paperId":"19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e","externalIds":{"DBLP":"conf/aaai/00010LYHLW24","ArXiv":"2306.17492","DOI":"10.48550/arXiv.2306.17492","CorpusId":259308873},"title":"Preference Ranking Optimization for Human Alignment"},{"paperId":"f5afaccfe90268485a9961c5771ec5e71e9b806c","externalIds":{"ArXiv":"2306.15595","DBLP":"journals/corr/abs-2306-15595","DOI":"10.48550/arXiv.2306.15595","CorpusId":259262376},"title":"Extending Context Window of Large Language Models via Positional Interpolation"},{"paperId":"87875a07976c26f82705de1fc70041169e5d652b","externalIds":{"DBLP":"conf/nips/YangSGCSYGPA23","ArXiv":"2306.15626","DOI":"10.48550/arXiv.2306.15626","CorpusId":259262077},"title":"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"cbbc2cc774c50b0b19922185b80e9ce90b7cd2f6","externalIds":{"DBLP":"journals/tacl/RubinB24a","ArXiv":"2306.13421","DOI":"10.1162/tacl_a_00693","CorpusId":259243694},"title":"Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval"},{"paperId":"1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642","externalIds":{"ArXiv":"2306.09442","DBLP":"journals/corr/abs-2306-09442","DOI":"10.48550/arXiv.2306.09442","CorpusId":259187620},"title":"Explore, Establish, Exploit: Red Teaming Language Models from Scratch"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","externalIds":{"ArXiv":"2306.09093","DBLP":"journals/corr/abs-2306-09093","DOI":"10.48550/arXiv.2306.09093","CorpusId":259165461},"title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"2a68cfffde314b717ca3fc4bd3ffab597f1b6ea9","externalIds":{"ArXiv":"2306.09525","DBLP":"journals/corr/abs-2306-09525","DOI":"10.48550/arXiv.2306.09525","CorpusId":259187949},"title":"Explaining Legal Concepts with Augmented Large Language Models (GPT-4)"},{"paperId":"454c8fef2957aa2fb13eb2c7a454393a2ee83805","externalIds":{"DBLP":"journals/corr/abs-2306-08568","ArXiv":"2306.08568","CorpusId":259164815},"title":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","externalIds":{"ArXiv":"2306.08640","DBLP":"journals/corr/abs-2306-08640","DOI":"10.48550/arXiv.2306.08640","CorpusId":259164559},"title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"94bcf0390d5acb1b92323bd15cc1dc311314122c","externalIds":{"DBLP":"conf/corl/0003GFKLACEHHIX23","ArXiv":"2306.08647","DOI":"10.48550/arXiv.2306.08647","CorpusId":259164906},"title":"Language to Rewards for Robotic Skill Synthesis"},{"paperId":"80980cd10d19f021c14a6b7eee871b6a5d328024","externalIds":{"ArXiv":"2306.07174","DBLP":"conf/nips/Wang0CLYGW23","DOI":"10.48550/arXiv.2306.07174","CorpusId":259137816},"title":"Augmenting Language Models with Long-Term Memory"},{"paperId":"5dea206e2a36e672f197252bdd27d156d058f48c","externalIds":{"DBLP":"journals/corr/abs-2306-06031","ArXiv":"2306.06031","DOI":"10.48550/arXiv.2306.06031","CorpusId":259129734},"title":"FinGPT: Open-Source Financial Large Language Models"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","externalIds":{"ArXiv":"2306.05424","DBLP":"journals/corr/abs-2306-05424","DOI":"10.48550/arXiv.2306.05424","CorpusId":259108333},"title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"eebb4a3162c1251b51e50ccd83797babc5b776c0","externalIds":{"ArXiv":"2306.05171","DBLP":"journals/corr/abs-2306-05171","DOI":"10.48550/arXiv.2306.05171","CorpusId":259108727},"title":"Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures"},{"paperId":"6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8","externalIds":{"DBLP":"journals/corr/abs-2306-04387","ArXiv":"2306.04387","DOI":"10.48550/arXiv.2306.04387","CorpusId":259095896},"title":"M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"},{"paperId":"50f44ef10335d59cec145b15effae20ff22c1fdb","externalIds":{"ArXiv":"2306.03901","DBLP":"journals/corr/abs-2306-03901","DOI":"10.48550/arXiv.2306.03901","CorpusId":259088875},"title":"ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"88b9a3e5882e5dc6dc56d3476948d1c5be67d798","externalIds":{"ArXiv":"2306.01694","DBLP":"journals/corr/abs-2306-01694","DOI":"10.48550/arXiv.2306.01694","CorpusId":259063728},"title":"Evaluating Language Models for Mathematics through Interactions"},{"paperId":"6f6e2e0311589a9af045f6acd00b7dee6d19fce4","externalIds":{"DBLP":"conf/nips/KazemnejadPRDR23","ArXiv":"2305.19466","DOI":"10.48550/arXiv.2305.19466","CorpusId":258987259},"title":"The Impact of Positional Encoding on Length Generalization in Transformers"},{"paperId":"73b2dee720ebc9014dfe57d9b73da60ca7645c86","externalIds":{"DBLP":"journals/corr/abs-2305-19352","ArXiv":"2305.19352","DOI":"10.1109/FLLM63129.2024.10852491","CorpusId":258987995},"title":"LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","externalIds":{"ArXiv":"2305.18752","DBLP":"conf/nips/YangSLZGLS23","DOI":"10.48550/arXiv.2305.18752","CorpusId":258967184},"title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"80be1426825288ff876acb8cc0babcc6629fa644","externalIds":{"ArXiv":"2305.18898","DBLP":"journals/corr/abs-2305-18898","DOI":"10.48550/arXiv.2305.18898","CorpusId":258967880},"title":"AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation"},{"paperId":"6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2","externalIds":{"DBLP":"conf/acl/LiuO0CSMSKC24","ArXiv":"2305.17888","DOI":"10.48550/arXiv.2305.17888","CorpusId":258959117},"title":"LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"ce913026f693101e54d3ab9152e107034d81fce1","externalIds":{"DBLP":"journals/tmlr/LiangBLTSYZNWKN23","DOI":"10.1111/nyas.15007","CorpusId":253553585,"PubMed":"37230490"},"title":"Holistic Evaluation of Language Models"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","externalIds":{"ArXiv":"2305.16103","DBLP":"journals/corr/abs-2305-16103","DOI":"10.48550/arXiv.2305.16103","CorpusId":258887944},"title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"f0888b9c0ef63e68c7758e6aec2370961c0eede9","externalIds":{"ArXiv":"2305.16504","DBLP":"journals/corr/abs-2305-16504","DOI":"10.48550/arXiv.2305.16504","CorpusId":258947425},"title":"On the Tool Manipulation Capability of Open-source Large Language Models"},{"paperId":"7d8905a1fd288068f12c8347caeabefd36d0dd6c","externalIds":{"DBLP":"journals/corr/abs-2305-15334","ArXiv":"2305.15334","DOI":"10.52202/079017-4020","CorpusId":258865184},"title":"Gorilla: Large Language Model Connected with Massive APIs"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","externalIds":{"ArXiv":"2305.14985","DBLP":"conf/emnlp/YouSW0WACC23","DOI":"10.48550/arXiv.2305.14985","CorpusId":258865952},"title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","externalIds":{"DBLP":"journals/corr/abs-2305-15023","ArXiv":"2305.15023","DOI":"10.48550/arXiv.2305.15023","CorpusId":258865326},"title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"5dbffedcabe3fa43060ebbe2b1789500edfd871f","externalIds":{"DBLP":"conf/emnlp/HaoGMHWWH23","ArXiv":"2305.14992","DOI":"10.48550/arXiv.2305.14992","CorpusId":258865812},"title":"Reasoning with Language Model is Planning with World Model"},{"paperId":"5193003d574eff310742e6ce94612fc82851fee0","externalIds":{"DBLP":"journals/corr/abs-2305-15212","ArXiv":"2305.15212","ACL":"2023.acl-short.107","DOI":"10.48550/arXiv.2305.15212","CorpusId":258865689},"title":"Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning"},{"paperId":"0744580e75a74357e466a57082c85cb42f548aa9","externalIds":{"DBLP":"conf/emnlp/KimJKJYSS23","ArXiv":"2305.14045","DOI":"10.48550/arXiv.2305.14045","CorpusId":258841149},"title":"The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning"},{"paperId":"7742233d33da13910d0303e4ec8814a4e26e96e9","externalIds":{"DBLP":"journals/corr/abs-2305-14327","ArXiv":"2305.14327","DOI":"10.48550/arXiv.2305.14327","CorpusId":258841263},"title":"Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","externalIds":{"ArXiv":"2305.14201","DBLP":"journals/corr/abs-2305-14201","DOI":"10.48550/arXiv.2305.14201","CorpusId":258840942},"title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"5b8f0460d408a8688d9ee0cba127c779d3291d99","externalIds":{"ArXiv":"2305.13735","DBLP":"journals/corr/abs-2305-13735","DOI":"10.48550/arXiv.2305.13735","CorpusId":258841835},"title":"Aligning Large Language Models through Synthetic Feedback"},{"paperId":"2ad8183c72a90511383a32ccaeea313eb85f4085","externalIds":{"DBLP":"journals/corr/abs-2305-14167","ArXiv":"2305.14167","DOI":"10.48550/arXiv.2305.14167","CorpusId":258841764},"title":"DetGPT: Detect What You Need via Reasoning"},{"paperId":"a22f3398ea865426c89ee66f4824ec626e56a864","externalIds":{"DBLP":"journals/corr/abs-2305-14322","ArXiv":"2305.14322","DOI":"10.48550/arXiv.2305.14322","CorpusId":258841042},"title":"RET-LLM: Towards a General Read-Write Memory for Large Language Models"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"a10843d1349fff8d2a7d9722f800802187fef67f","externalIds":{"DBLP":"conf/nips/KimLKPYKL23","ArXiv":"2305.14152","DOI":"10.48550/arXiv.2305.14152","CorpusId":258841104},"title":"Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"},{"paperId":"cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa","externalIds":{"DBLP":"journals/corr/abs-2305-14387","ArXiv":"2305.14387","DOI":"10.48550/arXiv.2305.14387","CorpusId":258865545},"title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"},{"paperId":"6783b17fe4328f48403f57009a73f784de09f645","externalIds":{"DBLP":"journals/corr/abs-2305-12002","ArXiv":"2305.12002","DOI":"10.1145/3583780.3615285","CorpusId":258833440},"title":"XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters"},{"paperId":"c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c","externalIds":{"DBLP":"journals/corr/abs-2305-11554","ArXiv":"2305.11554","DOI":"10.48550/arXiv.2305.11554","CorpusId":258823133},"title":"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings"},{"paperId":"017010b941d902a467f6d329ae5e74fd67e67912","externalIds":{"DBLP":"journals/corr/abs-2305-11627","ArXiv":"2305.11627","DOI":"10.48550/arXiv.2305.11627","CorpusId":258823276},"title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","externalIds":{"DBLP":"conf/nips/WangCCWZZLLZQD23","ArXiv":"2305.11175","DOI":"10.48550/arXiv.2305.11175","CorpusId":258762579},"title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"c3a59e1e405e7c28319e5a1c5b5241f9b340cf63","externalIds":{"DBLP":"journals/corr/abs-2305-10250","ArXiv":"2305.10250","DOI":"10.48550/arXiv.2305.10250","CorpusId":258741194},"title":"MemoryBank: Enhancing Large Language Models with Long-Term Memory"},{"paperId":"b6d6c33298b852cf63edac233deca70530d69a2a","externalIds":{"ArXiv":"2305.10403","DBLP":"journals/corr/abs-2305-10403","CorpusId":258740735},"title":"PaLM 2 Technical Report"},{"paperId":"256d20b96fa0ec65a373bfe64f128eb56b4ea508","externalIds":{"DBLP":"journals/corr/abs-2306-05539","ArXiv":"2306.05539","DOI":"10.48550/arXiv.2306.05539","CorpusId":259129868},"title":"Instruction Tuned Models are Quick Learners"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"5c7aaee5651221893ea0e67c363cab4c4be53b83","externalIds":{"ArXiv":"2305.09246","DBLP":"journals/corr/abs-2305-09246","DOI":"10.48550/arXiv.2305.09246","CorpusId":258715090},"title":"Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning"},{"paperId":"9ada8fa11b1cdece31f253acae50b62df8d5f823","externalIds":{"DBLP":"journals/corr/abs-2305-07922","ArXiv":"2305.07922","DOI":"10.48550/arXiv.2305.07922","CorpusId":258685677},"title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation"},{"paperId":"ac47bd3b512301371fc87c68416befce6589912e","externalIds":{"ArXiv":"2305.07716","DBLP":"journals/corr/abs-2305-07716","PubMedCentral":"10464606","DOI":"10.3389/frobt.2023.1221739","CorpusId":258686595,"PubMed":"37649810"},"title":"Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning"},{"paperId":"88884b8806262a4095036041e3567d450dba39f7","externalIds":{"DBLP":"journals/corr/abs-2305-06983","ArXiv":"2305.06983","DOI":"10.48550/arXiv.2305.06983","CorpusId":258615731},"title":"Active Retrieval Augmented Generation"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","externalIds":{"DBLP":"journals/corr/abs-2305-06355","ArXiv":"2305.06355","DOI":"10.1007/s11432-024-4321-9","CorpusId":258588306},"title":"VideoChat: chat-centric video understanding"},{"paperId":"3e4085e5869f1b7959707a1e1d7d273b6057eb4e","externalIds":{"DBLP":"journals/tmlr/LiAZMKMMALCLZZW23","ArXiv":"2305.06161","CorpusId":258588247},"title":"StarCoder: may the source be with you!"},{"paperId":"e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a","externalIds":{"ArXiv":"2305.05658","DBLP":"journals/arobots/WuAKLZSBRF23","DOI":"10.1007/s10514-023-10139-z","CorpusId":258564887},"title":"TidyBot: Personalized Robot Assistance with Large Language Models"},{"paperId":"e01515c6138bc525f7aec30fc85f2adf028d4156","externalIds":{"DBLP":"journals/corr/abs-2305-03047","ArXiv":"2305.03047","DOI":"10.48550/arXiv.2305.03047","CorpusId":258479665},"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"},{"paperId":"6f8b9192b1f215254ee7625d752710182c05d2f9","externalIds":{"DBLP":"journals/corr/abs-2305-02677","ArXiv":"2305.02677","DOI":"10.48550/arXiv.2305.02677","CorpusId":258479994},"title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls"},{"paperId":"8f831f341e959955a495730d81996e62c57cc0bd","externalIds":{"ArXiv":"2305.03111","DBLP":"conf/nips/LiHQYLLWQGHZ0LC23","DOI":"10.48550/arXiv.2305.03111","CorpusId":258547040},"title":"Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs"},{"paperId":"c79852e9c9cc6734c9150847deb5449e489354ea","externalIds":{"DBLP":"journals/corr/abs-2305-01711","ArXiv":"2305.01711","DOI":"10.48550/arXiv.2305.01711","CorpusId":258461014},"title":"Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","externalIds":{"ArXiv":"2304.15010","DBLP":"journals/corr/abs-2304-15010","DOI":"10.48550/arXiv.2304.15010","CorpusId":258418343},"title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","externalIds":{"DBLP":"journals/corr/abs-2304-14178","ArXiv":"2304.14178","DOI":"10.48550/arXiv.2304.14178","CorpusId":258352455},"title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"9c963e11a0a48f946f00095100dd3a303ba65949","externalIds":{"ArXiv":"2304.12529","DBLP":"journals/access/YeYD23","DOI":"10.1109/ACCESS.2023.3282111","CorpusId":258309818},"title":"Improved Trust in Human-Robot Collaboration With ChatGPT"},{"paperId":"12594b6afe01461384d2856d2bf44f1cf8533e3e","externalIds":{"PubMedCentral":"10166793","DOI":"10.3389/fpubh.2023.1166120","CorpusId":256756338,"PubMed":"37181697"},"title":"ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health"},{"paperId":"4c8ef2db0c77aba453783f5211ebafc6695d3835","externalIds":{"DBLP":"journals/tnn/ZhongPZWYLWLYMJSHZ25","ArXiv":"2304.11107","DOI":"10.1109/TNNLS.2025.3567945","CorpusId":258291600,"PubMed":"40526555"},"title":"ChatABL: Abductive Learning via Natural Language Interaction With ChatGPT"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"dbac86036cb5ed4dd6bbdda4a8613b163e20ec90","externalIds":{"DBLP":"journals/corr/abs-2304-11082","ArXiv":"2304.11082","DOI":"10.48550/arXiv.2304.11082","CorpusId":258291526},"title":"Fundamental Limitations of Alignment in Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","externalIds":{"DBLP":"conf/nips/LuPCGCWZG23","ArXiv":"2304.09842","DOI":"10.48550/arXiv.2304.09842","CorpusId":258212542},"title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"5be9a64df5f8d7e5a33fcc2c7bdfcde1fbbd085a","externalIds":{"PubMedCentral":"10273039","DOI":"10.2196/48291","CorpusId":258775775,"PubMed":"37261894"},"title":"Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions"},{"paperId":"90dd829f3d64dda19092b6e26909803bea5c37c1","externalIds":{"ArXiv":"2304.07995","DBLP":"journals/corr/abs-2304-07995","DOI":"10.48550/arXiv.2304.07995","CorpusId":258179750},"title":"From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"a8680b3419f3cbe6650f72b1023aed0ad0becb9e","externalIds":{"ArXiv":"2304.07919","CorpusId":258180277},"title":"Chain of Thought Prompt Tuning in Vision Language Models"},{"paperId":"e92a5332390f0ba94615935541da4da9bed56512","externalIds":{"ArXiv":"2304.07493","DBLP":"conf/isca/0003THL00LG023","DOI":"10.1145/3579371.3589038","CorpusId":258179335},"title":"OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization"},{"paperId":"302ee27524a717ddc21f332ca634b9211c6ec6aa","externalIds":{"ArXiv":"2304.06975","DBLP":"journals/corr/abs-2304-06975","DOI":"10.48550/arXiv.2304.06975","CorpusId":258170497},"title":"HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"},{"paperId":"3ab661db57d924f4ff1706e05ac807873ca00e0a","externalIds":{"DBLP":"journals/corr/abs-2304-06767","ArXiv":"2304.06767","DOI":"10.48550/arXiv.2304.06767","CorpusId":258170300},"title":"RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"},{"paperId":"b63e97330154acece935ffa6901e3f36518e5703","externalIds":{"DBLP":"journals/corr/abs-2304-06762","ArXiv":"2304.06762","DOI":"10.48550/arXiv.2304.06762","CorpusId":258170263},"title":"Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study"},{"paperId":"748698bd4387afd08594e0dc8150c2afa210d9ae","externalIds":{"DBLP":"conf/nips/YuanYTWHH23","ArXiv":"2304.05302","DOI":"10.48550/arXiv.2304.05302","CorpusId":258059818},"title":"RRHF: Rank Responses to Align Language Models with Human Feedback without tears"},{"paperId":"ae6a4cd221684be6ca3082b6f526a7901281490b","externalIds":{"ArXiv":"2304.05332","DBLP":"journals/corr/abs-2304-05332","DOI":"10.48550/arXiv.2304.05332","CorpusId":258059651},"title":"Emergent autonomous scientific research capabilities of large language models"},{"paperId":"38179848e2d6a3ad373b1793848816111428ac36","externalIds":{"DBLP":"journals/corr/abs-2304-04370","ArXiv":"2304.04370","DOI":"10.48550/arXiv.2304.04370","CorpusId":258049306},"title":"OpenAGI: When LLM Meets Domain Experts"},{"paperId":"9e8cb8c91a0acb6e661b58ad724aa758490f2bea","externalIds":{"ArXiv":"2304.03277","DBLP":"journals/corr/abs-2304-03277","CorpusId":257985497},"title":"Instruction Tuning with GPT-4"},{"paperId":"bdb68c5e2369633b20e733774ac66eb4600c34d1","externalIds":{"DBLP":"journals/corr/abs-2304-01933","ArXiv":"2304.01933","DOI":"10.48550/arXiv.2304.01933","CorpusId":257921386},"title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"},{"paperId":"51a0bba0c5fb4257e843040615bb23f712fed4e6","externalIds":{"ArXiv":"2304.01852","DOI":"10.1016/j.metrad.2023.100017","CorpusId":257921533},"title":"Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models"},{"paperId":"8f4773974bd2c27919e2a36d9aa6c2331dc34ca8","externalIds":{"DOI":"10.1016/j.rbmo.2023.04.009","CorpusId":258268211,"PubMed":"37142479"},"title":"Artificial intelligence in scientific writing: a friend or a foe?"},{"paperId":"a98862ffe4c18634a67a3df8a965a35e5e0d7ec8","externalIds":{"DOI":"10.1016/j.lindif.2023.102274","CorpusId":257445349},"title":"ChatGPT for good? On opportunities and challenges of large language models for education"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"3dfed62c61f650eb114f0f0aa26b4e7d37b963a6","externalIds":{"DBLP":"journals/corr/abs-2303-17395","ArXiv":"2303.17395","DOI":"10.1109/TASLP.2024.3419446","CorpusId":257834090},"title":"WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","externalIds":{"DBLP":"journals/corr/abs-2303-17564","ArXiv":"2303.17564","CorpusId":257833842},"title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","externalIds":{"DBLP":"journals/corr/abs-2303-16434","ArXiv":"2303.16434","DOI":"10.48550/arXiv.2303.16434","CorpusId":257804802},"title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"a757999ed260d7bc45484dc6b4456bf33fe6f679","externalIds":{"ArXiv":"2303.16199","DBLP":"journals/corr/abs-2303-16199","DOI":"10.48550/arXiv.2303.16199","CorpusId":257771811},"title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"},{"paperId":"af5c7848417882012203ac21399977ebda695a2b","externalIds":{"DBLP":"conf/emnlp/ZhangCZKLZMLC23","ArXiv":"2303.12570","DOI":"10.48550/arXiv.2303.12570","CorpusId":257663528},"title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"0671fd553dd670a4e820553a974bc48040ba0819","externalIds":{"DBLP":"conf/nips/ShinnCGNY23","ArXiv":"2303.11366","CorpusId":258833055},"title":"Reflexion: language agents with verbal reinforcement learning"},{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","externalIds":{"ArXiv":"2303.10845","DBLP":"journals/corr/abs-2303-10845","CorpusId":257666647},"title":"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","externalIds":{"ArXiv":"2303.11381","DBLP":"journals/corr/abs-2303-11381","DOI":"10.48550/arXiv.2303.11381","CorpusId":257637012},"title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"27d391d65ab42c30dc35595213ba6585633afa5d","externalIds":{"ArXiv":"2303.09752","DBLP":"journals/corr/abs-2303-09752","DOI":"10.48550/arXiv.2303.09752","CorpusId":257622671},"title":"CoLT5: Faster Long-Range Transformers with Conditional Computation"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","externalIds":{"ArXiv":"2303.09014","DBLP":"journals/corr/abs-2303-09014","DOI":"10.48550/arXiv.2303.09014","CorpusId":257557449},"title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","externalIds":{"DBLP":"conf/iccv/SurisMV23","ArXiv":"2303.08128","DOI":"10.1109/ICCV51070.2023.01092","CorpusId":257505358},"title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"6c767695b841c52e2d953db893c4140a2b4da429","externalIds":{"DBLP":"conf/hri/BillingRL23","DOI":"10.1145/3568294.3580040","CorpusId":257406366},"title":"Language Models for Human-Robot Interaction"},{"paperId":"e4be613cc875e61b8c1c6c60d958f1c20d12d6c0","externalIds":{"ArXiv":"2303.06247","DBLP":"conf/iros/Ding0P023","DOI":"10.1109/IROS55552.2023.10342169","CorpusId":257496672},"title":"Task and Motion Planning with Large Language Models for Object Rearrangement"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","externalIds":{"DBLP":"journals/corr/abs-2303-04671","ArXiv":"2303.04671","DOI":"10.48550/arXiv.2303.04671","CorpusId":257404891},"title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6","externalIds":{"DBLP":"journals/corr/abs-2303-03915","ArXiv":"2303.03915","DOI":"10.48550/arXiv.2303.03915","CorpusId":257378329},"title":"The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"},{"paperId":"32524aa3ae8522542753ed7e6f4cca3970e4acab","externalIds":{"ArXiv":"2303.03480","DBLP":"journals/ral/DorbalaMM24","DOI":"10.1109/LRA.2023.3346800","CorpusId":257378363},"title":"Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"fdb03aa9c310fa61df0be724705fb6f4ab20d37e","externalIds":{"DBLP":"journals/corr/abs-2303-03548","ArXiv":"2303.03548","DOI":"10.1109/IROS55552.2023.10341488","CorpusId":257378614},"title":"Large Language Models as Zero-Shot Human Models for Human-Robot Interaction"},{"paperId":"b626560f19f815808a289ef5c24a17c57320da70","externalIds":{"DBLP":"journals/corr/abs-2303-05398","ACL":"2023.acl-industry.4","ArXiv":"2303.05398","DOI":"10.48550/arXiv.2303.05398","CorpusId":257427208},"title":"MathPrompter: Mathematical Reasoning using Large Language Models"},{"paperId":"be7b764fe1c9c32cbe349bde1fbb19321fd1d71c","externalIds":{"DBLP":"journals/corr/abs-2303-02151","ArXiv":"2303.02151","DOI":"10.1109/CVPR52729.2023.01460","CorpusId":257353537},"title":"Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners"},{"paperId":"4b3d5da6da9c0b6b61d9672a0374da89b0da1ad3","externalIds":{"PubMedCentral":"10020064","DOI":"10.3946/kjme.2023.253","CorpusId":257282206,"PubMed":"36858381"},"title":"The impending impacts of large language models on medical education"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"3599a236f285af48782fc30b1341d13ec7320735","externalIds":{"ArXiv":"2302.09419","DBLP":"journals/corr/abs-2302-09419","DOI":"10.48550/arXiv.2302.09419","CorpusId":257039063},"title":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"},{"paperId":"22e2f488ecd88bd2adf79092d0d390d8f7b06a0f","externalIds":{"DBLP":"journals/aiethics/MokanderSKF24","ArXiv":"2302.08500","DOI":"10.1007/s43681-023-00289-2","CorpusId":256901111},"title":"Auditing large language models: a three-layered approach"},{"paperId":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","externalIds":{"ArXiv":"2302.07842","DBLP":"journals/tmlr/MialonDLNPRRSDC23","CorpusId":256868474},"title":"Augmented Language Models: a Survey"},{"paperId":"6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552","externalIds":{"DBLP":"journals/corr/abs-2302-07459","ArXiv":"2302.07459","DOI":"10.48550/arXiv.2302.07459","CorpusId":256868727},"title":"The Capacity for Moral Self-Correction in Large Language Models"},{"paperId":"873a581320d928249609d3c07229d5af182a379c","externalIds":{"DBLP":"conf/emnlp/QinZ0CYY23","ArXiv":"2302.06476","DOI":"10.18653/v1/2023.emnlp-main.85","CorpusId":256827430},"title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"},{"paperId":"d05ba0c40f3408aab7bb594628f24d9f04bf2831","externalIds":{"DOI":"10.1101/2023.02.02.23285399","CorpusId":256626649,"PubMed":"36798292"},"title":"Evaluating ChatGPT as an Adjunct for Radiologic Decision-Making"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","externalIds":{"DBLP":"journals/corr/abs-2302-00923","ArXiv":"2302.00923","DOI":"10.48550/arXiv.2302.00923","CorpusId":256504063},"title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"0b1c6e2f2a04496dac42bd562f518a0f7415b31f","externalIds":{"PubMedCentral":"9931307","DOI":"10.1371/journal.pdig.0000205","CorpusId":256747912,"PubMed":"36812618"},"title":"ChatGPT passing USMLE shines a spotlight on the flaws of medical education"},{"paperId":"465471bb5bf1a945549d6291c2d23367966b4957","externalIds":{"ArXiv":"2302.00083","DBLP":"journals/corr/abs-2302-00083","DOI":"10.1162/tacl_a_00605","CorpusId":256459451},"title":"In-Context Retrieval-Augmented Language Models"},{"paperId":"07b14c24833400b79978b0a5f084803337e30a15","externalIds":{"DBLP":"conf/naacl/ShiMYS0LZY24","ACL":"2024.naacl-long.463","ArXiv":"2301.12652","DOI":"10.48550/arXiv.2301.12652","CorpusId":256389797},"title":"REPLUG: Retrieval-Augmented Black-Box Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"86478f285356b5c8d27423e6b939634d9e010fba","externalIds":{"DBLP":"journals/corr/abs-2301-12314","ArXiv":"2301.12314","DOI":"10.48550/arXiv.2301.12314","CorpusId":256390383},"title":"Progressive Prompts: Continual Learning for Language Models"},{"paperId":"7ec58d26c4dddb4bc3b6829fa0654a22cc26fdfe","externalIds":{"ArXiv":"2301.04589","DBLP":"journals/corr/abs-2301-04589","DOI":"10.48550/arXiv.2301.04589","CorpusId":255595513},"title":"Memory Augmented Large Language Models are Computationally Universal"},{"paperId":"b0f615d4d300778d87122ce10621db2bbc196cad","externalIds":{"DOI":"10.5260/chara.24.3.41","CorpusId":256138459},"title":"Linguistic Data Consortium"},{"paperId":"e965e93e76a9e6c4e4863d145b5c007b540d575d","externalIds":{"ArXiv":"2212.12017","DBLP":"journals/corr/abs-2212-12017","CorpusId":255096269},"title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"},{"paperId":"0c0300f53c01ae609c97395c98de4c9d85d92876","externalIds":{"DBLP":"conf/acl/XuSH23","ACL":"2023.acl-long.641","ArXiv":"2212.10773","DOI":"10.48550/arXiv.2212.10773","CorpusId":254926784},"title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"},{"paperId":"980e55d9226cac302d0fae7732da4e67b8bc952c","externalIds":{"DBLP":"conf/acl/RatnerLBRMAKSLS23","ACL":"2023.acl-long.352","ArXiv":"2212.10947","DOI":"10.18653/v1/2023.acl-long.352","CorpusId":258686160},"title":"Parallel Context Windows for Large Language Models"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","externalIds":{"ArXiv":"2212.10403","DBLP":"conf/acl/0009C23","DOI":"10.48550/arXiv.2212.10403","CorpusId":254877753},"title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","externalIds":{"ACL":"2023.acl-long.806","ArXiv":"2212.09689","DBLP":"conf/acl/HonovichSLS23","DOI":"10.48550/arXiv.2212.09689","CorpusId":254853659},"title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"},{"paperId":"3cbffab9d7981da6662d474aaa056dcbd3c1701e","externalIds":{"ArXiv":"2212.09196","DBLP":"journals/corr/abs-2212-09196","DOI":"10.1038/s41562-023-01659-w","CorpusId":254854575,"PubMed":"37524930"},"title":"Emergent analogical reasoning in large language models"},{"paperId":"b1b8c3e47f44158d22fb70bb453d2494ed013b70","externalIds":{"ACL":"2023.acl-long.244","ArXiv":"2212.08061","DBLP":"journals/corr/abs-2212-08061","DOI":"10.48550/arXiv.2212.08061","CorpusId":254686088},"title":"On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning"},{"paperId":"3936fd3c6187f606c6e4e2e20b196dbc41cc4654","externalIds":{"DBLP":"journals/corr/abs-2212-08073","ArXiv":"2212.08073","DOI":"10.48550/arXiv.2212.08073","CorpusId":254823489},"title":"Constitutional AI: Harmlessness from AI Feedback"},{"paperId":"8ee45aeb7c97e3346cc62f216f673b91277ac718","externalIds":{"DBLP":"conf/iccv/SongSWCW023","ArXiv":"2212.04088","DOI":"10.1109/ICCV51070.2023.00280","CorpusId":254408960},"title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"},{"paperId":"a02fbaf22237a1aedacb1320b6007cd70c1fe6ec","externalIds":{"DBLP":"journals/corr/abs-2212-04356","ArXiv":"2212.04356","CorpusId":252923993},"title":"Robust Speech Recognition via Large-Scale Weak Supervision"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","externalIds":{"DBLP":"journals/corr/abs-2211-15533","ArXiv":"2211.15533","DOI":"10.48550/arXiv.2211.15533","CorpusId":254044610},"title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","externalIds":{"DBLP":"journals/corr/abs-2211-11501","ArXiv":"2211.11501","DOI":"10.48550/arXiv.2211.11501","CorpusId":253734939},"title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"2c994fadbb84fb960d8306ee138dbeef41a5b323","externalIds":{"ArXiv":"2211.10438","DBLP":"conf/icml/XiaoLSWDH23","DOI":"10.48550/arXiv.2211.10438","CorpusId":253708271},"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"},{"paperId":"af1c871282ec122869d03f5420ef5d9143358a91","externalIds":{"DBLP":"conf/cvpr/GuptaK23","ArXiv":"2211.11559","DOI":"10.1109/CVPR52729.2023.01436","CorpusId":253734854},"title":"Visual Programming: Compositional visual reasoning without training"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","externalIds":{"DBLP":"journals/corr/abs-2211-09085","ArXiv":"2211.09085","CorpusId":253553203},"title":"Galactica: A Large Language Model for Science"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6","externalIds":{"DBLP":"journals/corr/abs-2210-17323","ArXiv":"2210.17323","CorpusId":253237200},"title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"1bb6d5761903c7ac978188ae36e2648905e95dc5","externalIds":{"ArXiv":"2210.11399","DBLP":"conf/emnlp/TayWC0SSGZRCZMP23","DOI":"10.48550/arXiv.2210.11399","CorpusId":253018395},"title":"Transcending Scaling Laws with 0.1% Extra Compute"},{"paperId":"c8d594f09413b1555970f43e68847c211235d60f","externalIds":{"DBLP":"journals/corr/abs-2210-09150","ArXiv":"2210.09150","DOI":"10.48550/arXiv.2210.09150","CorpusId":252917981},"title":"Prompting GPT-3 To Be Reliable"},{"paperId":"c305ab1bdba79442bec72ec7f5c5ee7c49c2a566","externalIds":{"DBLP":"journals/corr/abs-2210-05714","ArXiv":"2210.05714","DOI":"10.1109/ICRA48891.2023.10160969","CorpusId":252846548},"title":"Visual Language Maps for Robot Navigation"},{"paperId":"dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc","externalIds":{"DBLP":"journals/corr/abs-2210-03858","ArXiv":"2210.03858","DOI":"10.48550/arXiv.2210.03858","CorpusId":252780574},"title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","externalIds":{"DBLP":"conf/iclr/ShiSF0SVCTRZ0W23","ArXiv":"2210.03057","DOI":"10.48550/arXiv.2210.03057","CorpusId":252735112},"title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"fc74fe92fb9e34d3ae1237bf9a6e718c723f4f3e","externalIds":{"DBLP":"journals/corr/abs-2209-14290","ArXiv":"2209.14290","DOI":"10.1145/3539618.3591687","CorpusId":252568176},"title":"FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","externalIds":{"DBLP":"journals/corr/abs-2209-14375","ArXiv":"2209.14375","DOI":"10.48550/arXiv.2209.14375","CorpusId":252596089},"title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","externalIds":{"DBLP":"journals/corr/abs-2209-11302","ArXiv":"2209.11302","DOI":"10.1109/ICRA48891.2023.10161317","CorpusId":252519594},"title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"30a7390ec0103684eba9fb6bde1983d706fb57b3","externalIds":{"DBLP":"journals/corr/abs-2208-11580","ArXiv":"2208.11580","DOI":"10.48550/arXiv.2208.11580","CorpusId":251765570},"title":"Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"},{"paperId":"17bcb1edbe068e8fe6a97da552c70a77a15bbce7","externalIds":{"DBLP":"journals/corr/abs-2209-07858","ArXiv":"2209.07858","DOI":"10.48550/arXiv.2209.07858","CorpusId":252355458},"title":"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"},{"paperId":"916be31cbf847faa65cad0549e153f0c25b9f424","externalIds":{"ArXiv":"2208.03299","DBLP":"journals/jmlr/IzacardLLHPSDJRG23","CorpusId":251371732},"title":"Few-shot Learning with Retrieval Augmented Language Models"},{"paperId":"914254fac74a2da051cccf6ca16afcaad416a079","externalIds":{"DBLP":"journals/corr/abs-2208-01448","ArXiv":"2208.01448","DOI":"10.48550/arXiv.2208.01448","CorpusId":251253416},"title":"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"},{"paperId":"f3cf71c51b882fe3111d71c4bf104297d38197f8","externalIds":{"ArXiv":"2207.05608","DBLP":"conf/corl/HuangXXCLFZTMCS22","DOI":"10.48550/arXiv.2207.05608","CorpusId":250451569},"title":"Inner Monologue: Embodied Reasoning through Planning with Language Models"},{"paperId":"b17cc18e4130505b939f7d527082eb6be2a7fd5b","externalIds":{"DBLP":"journals/corr/abs-2207-00747","ArXiv":"2207.00747","DOI":"10.48550/arXiv.2207.00747","CorpusId":250264890},"title":"Rationale-Augmented Ensembles in Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","externalIds":{"ArXiv":"2206.04615","DBLP":"journals/corr/abs-2206-04615","CorpusId":263625818},"title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3","externalIds":{"ACL":"2022.emnlp-main.410","ArXiv":"2205.12393","DBLP":"conf/emnlp/ScialomCM22","DOI":"10.18653/v1/2022.emnlp-main.410","CorpusId":252815378},"title":"Fine-tuned Language Models are Continual Learners"},{"paperId":"354bf043179e3e9f05df73e3f04517e53c326d1f","externalIds":{"ArXiv":"2205.12255","DBLP":"journals/corr/abs-2205-12255","DOI":"10.48550/arXiv.2205.12255","CorpusId":249017698},"title":"TALM: Tool Augmented Language Models"},{"paperId":"eb4d54651c4f610749caf2bf401af3ce28ddc439","externalIds":{"ArXiv":"2210.17451","DBLP":"conf/emnlp/WangAM00AG22","ACL":"2022.emnlp-main.388","DOI":"10.48550/arXiv.2210.17451","CorpusId":253153886},"title":"AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"},{"paperId":"b21670e8061a06ab97e7d6052c9345a326e84ff8","externalIds":{"ArXiv":"2205.05131","DBLP":"conf/iclr/Tay00GW0CBSZZHM23","CorpusId":252780443},"title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96","externalIds":{"ArXiv":"2205.05198","DBLP":"journals/corr/abs-2205-05198","DOI":"10.48550/arXiv.2205.05198","CorpusId":248693351},"title":"Reducing Activation Recomputation in Large Transformer Models"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"39a45eba627199ee12c168dffcead45e138e9a01","externalIds":{"DOI":"10.1162/daed_a_01909","CorpusId":248377874},"title":"Do Large Language Models Understand Us?"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"597cad6c7b9de94eecc153c7cdcaf824905fe915","externalIds":{"DBLP":"journals/corr/abs-2204-09391","ArXiv":"2204.09391","DOI":"10.48550/arXiv.2204.09391","CorpusId":248266789},"title":"You Are What You Write: Preserving Privacy in the Era of Large Language Models"},{"paperId":"06d7cb8c8816360feb33c3367073e0ef66d7d0b0","externalIds":{"ACL":"2022.emnlp-main.340","ArXiv":"2204.07705","DBLP":"conf/emnlp/WangMAKMNADASPK22","DOI":"10.18653/v1/2022.emnlp-main.340","CorpusId":253098274},"title":"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","externalIds":{"ACL":"2022.bigscience-1.9","DBLP":"journals/corr/abs-2204-06745","ArXiv":"2204.06745","DOI":"10.48550/arXiv.2204.06745","CorpusId":248177957},"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"15190e8b459bd85d546286f7d7da61b4f4f3f58a","externalIds":{"DBLP":"conf/icml/WangRHSCBLR22","ArXiv":"2204.05832","DOI":"10.48550/arXiv.2204.05832","CorpusId":248118752},"title":"What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","externalIds":{"DBLP":"conf/corl/IchterBCFHHHIIJ22","ArXiv":"2204.01691","CorpusId":247939706},"title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","externalIds":{"DBLP":"journals/corr/abs-2203-15556","ArXiv":"2203.15556","CorpusId":247778764},"title":"Training Compute-Optimal Large Language Models"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","externalIds":{"DBLP":"conf/iclr/0002WSLCNCZ23","ArXiv":"2203.11171","CorpusId":247595263},"title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"8666f9f379389a5dff31e72fb0f992a37763ba41","externalIds":{"ArXiv":"2203.11147","DBLP":"journals/corr/abs-2203-11147","DOI":"10.48550/arXiv.2203.11147","CorpusId":247594830},"title":"Teaching language models to support answers with verified quotes"},{"paperId":"e4f82c0a13cae6739239ae0c25a554b6daff35af","externalIds":{"ArXiv":"2203.10705","DBLP":"journals/corr/abs-2203-10705","ACL":"2022.acl-long.331","DOI":"10.48550/arXiv.2203.10705","CorpusId":247593909},"title":"Compression of Generative Pre-trained Language Models via Quantization"},{"paperId":"081edae651e709e448bdd8a1f1b5760c7c7e1f53","externalIds":{"ACL":"2022.findings-acl.207","DBLP":"journals/corr/abs-2203-05797","ArXiv":"2203.05797","DOI":"10.48550/arXiv.2203.05797","CorpusId":247411350},"title":"Long Time No See! Open-Domain Conversation with Long-Term Persona Memory"},{"paperId":"c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd","externalIds":{"DBLP":"journals/corr/abs-2203-05115","ArXiv":"2203.05115","DOI":"10.48550/arXiv.2203.05115","CorpusId":247362809},"title":"Internet-augmented language models through few-shot prompting for open-domain question answering"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"e0995bad59c8638ea8c319bb7220c0f0b1ed5dca","externalIds":{"ArXiv":"2203.00555","DBLP":"journals/pami/WangMDHZW24","DOI":"10.1109/TPAMI.2024.3386927","CorpusId":247187905},"title":"DeepNet: Scaling Transformers to 1,000 Layers"},{"paperId":"4c09ac7b09628aa2aad12aea8dd6c2aef6c83aa0","externalIds":{"DBLP":"journals/corr/abs-2202-07962","ArXiv":"2202.07962","ACL":"2022.emnlp-main.168","DOI":"10.18653/v1/2022.emnlp-main.168","CorpusId":246867041},"title":"Revisiting Parameter-Efficient Tuning: Are We Really There Yet?"},{"paperId":"62d17b6f6ad77fd71ef9954c7784700d5e316f1f","externalIds":{"ArXiv":"2202.05520","DBLP":"conf/fat/BrownLMST22","DOI":"10.1145/3531146.3534642","CorpusId":246823897},"title":"What Does it Mean for a Language Model to Preserve Privacy?"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","externalIds":{"ArXiv":"2203.07814","DBLP":"journals/corr/abs-2203-07814","DOI":"10.1126/science.abq1158","CorpusId":246527904,"PubMed":"36480631"},"title":"Competition-level code generation with AlphaCode"},{"paperId":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","externalIds":{"DBLP":"journals/corr/abs-2202-03286","ACL":"2022.emnlp-main.225","ArXiv":"2202.03286","DOI":"10.18653/v1/2022.emnlp-main.225","CorpusId":246634238},"title":"Red Teaming Language Models with Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","externalIds":{"ArXiv":"2201.11990","DBLP":"journals/corr/abs-2201-11990","CorpusId":246411325},"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","externalIds":{"DBLP":"journals/corr/abs-2201-08239","ArXiv":"2201.08239","CorpusId":246063428},"title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","externalIds":{"DBLP":"journals/corr/abs-2201-07207","ArXiv":"2201.07207","CorpusId":246035276},"title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"79950179d60ba39a74d5fe2aedc47a57c0bf4c03","externalIds":{"DBLP":"journals/corr/abs-2201-05966","ACL":"2022.emnlp-main.39","ArXiv":"2201.05966","DOI":"10.18653/v1/2022.emnlp-main.39","CorpusId":246016124},"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"b587204402ceb03dd85d00b0e8cc3286408b7cf2","externalIds":{"ArXiv":"2112.13610","DBLP":"journals/corr/abs-2112-13610","CorpusId":245502210},"title":"CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark"},{"paperId":"a3184d40d390793232c99c89b57b8f65c16320b2","externalIds":{"DBLP":"journals/corr/abs-2112-12731","ArXiv":"2112.12731","CorpusId":245425057},"title":"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","externalIds":{"ArXiv":"2112.10508","DBLP":"journals/corr/abs-2112-10508","CorpusId":245335281},"title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","externalIds":{"DBLP":"journals/corr/abs-2112-09332","ArXiv":"2112.09332","CorpusId":245329531},"title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","externalIds":{"DBLP":"journals/corr/abs-2112-08633","ACL":"2022.naacl-main.191","ArXiv":"2112.08633","DOI":"10.18653/v1/2022.naacl-main.191","CorpusId":245218561},"title":"Learning To Retrieve Prompts for In-Context Learning"},{"paperId":"3dfb1f50f2a34a699c339dabaa6f9b3a977973de","externalIds":{"ArXiv":"2112.07916","DBLP":"conf/naacl/GuoAUONSY22","DOI":"10.18653/v1/2022.findings-naacl.55","CorpusId":245144820},"title":"LongT5: Efficient Text-To-Text Transformer for Long Sequences"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"53c3940f35b8b45d55ed49056282e1961954513d","externalIds":{"ArXiv":"2112.05682","CorpusId":245117555},"title":"Self-attention Does Not Need $O(n^2)$ Memory"},{"paperId":"002c256d30d6be4b23d365a8de8ae0e67e4c9641","externalIds":{"DBLP":"journals/corr/abs-2112-04426","ArXiv":"2112.04426","CorpusId":244954723},"title":"Improving language models by retrieving from trillions of tokens"},{"paperId":"68f141724814839d556a989646194be88641b143","externalIds":{"ArXiv":"2112.11446","DBLP":"journals/corr/abs-2112-11446","CorpusId":245353475},"title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","externalIds":{"ArXiv":"2112.00861","DBLP":"journals/corr/abs-2112-00861","CorpusId":244799619},"title":"A General Language Assistant as a Laboratory for Alignment"},{"paperId":"cbf98ebe967e0f3f3236e7932f37013b98244e94","externalIds":{"ArXiv":"2111.10952","DBLP":"journals/corr/abs-2111-10952","CorpusId":244478674},"title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"},{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","externalIds":{"ArXiv":"2111.01243","DBLP":"journals/corr/abs-2111-01243","DOI":"10.1145/3605943","CorpusId":240420063},"title":"Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"},{"paperId":"ee8984a6712791d4e0f2c776dad8119a3b893dd9","externalIds":{"ArXiv":"2110.14883","DBLP":"conf/icpp/LiLBFHLW023","DOI":"10.1145/3605573.3605613","CorpusId":240070340},"title":"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"2582a04918f6fe62dc142f2fca9ca0bb0b1d7895","externalIds":{"ArXiv":"2110.09456","DBLP":"journals/corr/abs-2110-09456","CorpusId":239016890},"title":"NormFormer: Improved Transformer Pretraining with Extra Normalization"},{"paperId":"7d5c661fa9a4255ee087e861f820564ea2e2bd6b","externalIds":{"DBLP":"journals/corr/abs-2110-08193","ArXiv":"2110.08193","ACL":"2022.findings-acl.165","DOI":"10.18653/v1/2022.findings-acl.165","CorpusId":239010011},"title":"BBQ: A hand-built bias benchmark for question answering"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","externalIds":{"ArXiv":"2110.08207","DBLP":"journals/corr/abs-2110-08207","CorpusId":239009562},"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"0ab41d455d676542b37ca1499bb19ea6a5d1cf79","externalIds":{"DBLP":"journals/corr/abs-2110-04725","ArXiv":"2110.04725","CorpusId":238582964},"title":"Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"},{"paperId":"43a87867fe6bf4eb920f97fc753be4b727308923","externalIds":{"DBLP":"journals/corr/abs-2110-04366","ArXiv":"2110.04366","CorpusId":238583580},"title":"Towards a Unified View of Parameter-Efficient Transfer Learning"},{"paperId":"e1227daa4877599e13de41a5207a222e1b197456","externalIds":{"DBLP":"conf/nips/AlexLTTMRHASCNS21","ArXiv":"2109.14076","CorpusId":238215290},"title":"RAFT: A Real-World Few-Shot Text Classification Benchmark"},{"paperId":"a6d8d04962f84ae6225e72723869a002b9fc8036","externalIds":{"DBLP":"conf/emnlp/KimKLLKJ0KKSLJL21","ACL":"2021.emnlp-main.274","ArXiv":"2109.04650","DOI":"10.18653/v1/2021.emnlp-main.274","CorpusId":237485423},"title":"What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","externalIds":{"DBLP":"journals/corr/abs-2109-07958","ACL":"2022.acl-long.229","ArXiv":"2109.07958","DOI":"10.18653/v1/2022.acl-long.229","CorpusId":237532606},"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"9ba50f992ccd92f428503ea6246157260a26cd77","externalIds":{"ArXiv":"2109.01247","DBLP":"conf/naacl/WebsonP22","ACL":"2022.naacl-main.167","DOI":"10.18653/v1/2022.naacl-main.167","CorpusId":237416760},"title":"Do Prompt-Based Models Really Understand the Meaning of Their Prompts?"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","externalIds":{"DBLP":"conf/emnlp/0034WJH21","ACL":"2021.emnlp-main.685","ArXiv":"2109.00859","DOI":"10.18653/v1/2021.emnlp-main.685","CorpusId":237386541},"title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"7668b23aadf43bebe5e2d3abf37938b44bd16200","externalIds":{"DBLP":"conf/cvpr/ChangCNGSB22","ArXiv":"2109.00590","DOI":"10.1109/CVPR52688.2022.01600","CorpusId":237385768},"title":"WebQA: Multihop and Multimodal QA"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","externalIds":{"ArXiv":"2108.12409","DBLP":"journals/corr/abs-2108-12409","CorpusId":237347130},"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","externalIds":{"DBLP":"journals/corr/abs-2108-07732","ArXiv":"2108.07732","CorpusId":237142385},"title":"Program Synthesis with Large Language Models"},{"paperId":"de549c1592a62c129b8d49c8c0137aa6859b103f","externalIds":{"DBLP":"journals/corr/abs-2107-07566","ACL":"2022.acl-long.579","ArXiv":"2107.07566","DOI":"10.18653/v1/2022.acl-long.579","CorpusId":236034557},"title":"Internet-Augmented Dialogue Generation"},{"paperId":"4237cbebe788a97174f48dc398082739bbffe95b","externalIds":{"ArXiv":"2107.07498","DBLP":"journals/corr/abs-2107-07498","CorpusId":235899009},"title":"FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"319b84be7a843250bc81d7086f79a4126d550277","externalIds":{"DBLP":"journals/corr/abs-2107-02137","ArXiv":"2107.02137","CorpusId":235731579},"title":"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"},{"paperId":"64902a5077ee68011cd467398dbb66511e8e891a","externalIds":{"DBLP":"journals/corr/abs-2106-12066","ArXiv":"2106.12066","ACL":"2021.findings-acl.310","DOI":"10.18653/v1/2021.findings-acl.310","CorpusId":235606305},"title":"It’s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning"},{"paperId":"00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d","externalIds":{"ArXiv":"2106.10715","DBLP":"journals/aiopen/ZhangGHCXSYQGKC21","DOI":"10.1016/j.aiopen.2021.12.003","CorpusId":235490263},"title":"CPM-2: Large-scale Cost-effective Pre-trained Language Models"},{"paperId":"339b2b711fb5b228d097b03ebc3e62a521779235","externalIds":{"DBLP":"conf/acl/ZakenGR22","ACL":"2022.acl-short.1","ArXiv":"2106.10199","DOI":"10.18653/v1/2022.acl-short.1","CorpusId":231672601},"title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"5aab57cc0530560d82c74c055f664280619d7e81","externalIds":{"ArXiv":"2106.03634","ACL":"2021.findings-acl.404","DBLP":"journals/corr/abs-2106-03634","DOI":"10.18653/v1/2021.findings-acl.404","CorpusId":235358436},"title":"PROST: Physical Reasoning about Objects through Space and Time"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","externalIds":{"ArXiv":"2106.03193","DBLP":"journals/tacl/GoyalGCCWJKRGF22","ACL":"2022.tacl-1.30","DOI":"10.1162/tacl_a_00474","CorpusId":235358129},"title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"a870f4abeef3d245641479b9d4c9f626a7178167","externalIds":{"DBLP":"journals/corr/abs-2106-01979","ArXiv":"2106.01979","CorpusId":235313954},"title":"CCPM: A Chinese Classical Poetry Matching Dataset"},{"paperId":"7547680408358916e66917d03436fca7540a7528","externalIds":{"ArXiv":"2105.12655","DBLP":"conf/nips/Puri0JZDZD0CDTB21","CorpusId":235195915},"title":"CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","externalIds":{"ArXiv":"2105.09938","DBLP":"conf/nips/HendrycksBKMAGB21","CorpusId":234790100},"title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"28459083ba624020c8f1c1ed7c3a075f48b4e709","externalIds":{"DBLP":"journals/corr/abs-2105-09680","ArXiv":"2105.09680","CorpusId":234790338},"title":"KLUE: Korean Language Understanding Evaluation"},{"paperId":"78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","externalIds":{"DBLP":"journals/corr/abs-2104-12369","MAG":"3158631574","ArXiv":"2104.12369","CorpusId":233394012},"title":"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","externalIds":{"DBLP":"journals/ijon/SuALPBL24","ArXiv":"2104.09864","DOI":"10.1016/j.neucom.2023.127063","CorpusId":233307138},"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","externalIds":{"ACL":"2021.emnlp-main.572","ArXiv":"2104.08835","DBLP":"journals/corr/abs-2104-08835","DOI":"10.18653/v1/2021.emnlp-main.572","CorpusId":233296709},"title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"},{"paperId":"eec41f0659b11d39fd606ac6cb8721c512a2ea6f","externalIds":{"DBLP":"conf/acl/TanzerRR22","ACL":"2022.acl-long.521","ArXiv":"2105.00828","DOI":"10.18653/v1/2022.acl-long.521","CorpusId":247450508},"title":"Memorisation versus Generalisation in Pre-trained Language Models"},{"paperId":"1e5b05838e16244310db554b04ff6541f05acb0b","externalIds":{"DBLP":"conf/naacl/XuZGXMW21","ACL":"2021.naacl-main.172","ArXiv":"2104.02704","MAG":"3166642164","DOI":"10.18653/V1/2021.NAACL-MAIN.172","CorpusId":233033383},"title":"Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge"},{"paperId":"238eb420c472bfdb1b4d34f9f53abec51f307a6b","externalIds":{"DBLP":"journals/corr/abs-2103-13262","ArXiv":"2103.13262","CorpusId":232335691},"title":"FastMoE: A Fast Mixture-of-Expert Training System"},{"paperId":"bc37c6bdb8f39929a58b30464f72d6aa46cddc17","externalIds":{"DBLP":"journals/corr/abs-2103-10385","ArXiv":"2103.10385","DOI":"10.1016/j.aiopen.2023.08.012","CorpusId":232269696},"title":"GPT Understands, Too"},{"paperId":"50796b0f3edf9cb5ff1e447c298b33755378aa4f","externalIds":{"DBLP":"conf/acl/DuQLDQY022","ACL":"2022.acl-long.26","ArXiv":"2103.10360","DOI":"10.18653/v1/2022.acl-long.26","CorpusId":247519241},"title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","externalIds":{"MAG":"3170403598","ArXiv":"2103.07191","DBLP":"conf/naacl/PatelBG21","ACL":"2021.naacl-main.168","DOI":"10.18653/V1/2021.NAACL-MAIN.168","CorpusId":232223322},"title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","externalIds":{"DBLP":"conf/nips/HendrycksBKABTS21","ArXiv":"2103.03874","CorpusId":232134851},"title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"ca2f1088d3e581b2c6c75cf0ebc96506d620f64d","externalIds":{"DBLP":"conf/fat/BenderGMS21","DOI":"10.1145/3442188.3445922","CorpusId":262580630},"title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c","externalIds":{"DBLP":"journals/cacm/ZhangBHRV21","DOI":"10.1145/3446776","CorpusId":231991101},"title":"Understanding deep learning (still) requires rethinking generalization"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","externalIds":{"DBLP":"conf/acl-deelio/LiuSZDCC22","ArXiv":"2101.06804","ACL":"2022.deelio-1.10","DOI":"10.18653/v1/2022.deelio-1.10","CorpusId":231632658},"title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"346081161bdc8f18e2a4c4af7f51d35452b5cb01","externalIds":{"ArXiv":"2101.02235","DBLP":"journals/tacl/GevaKSKRB21","DOI":"10.1162/tacl_a_00370","CorpusId":230799347},"title":"Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","externalIds":{"DBLP":"journals/corr/abs-2101-00027","ArXiv":"2101.00027","CorpusId":230435736},"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"470735385073e6b378717a886dcc8f1014e69e8a","externalIds":{"DBLP":"journals/talip/NaseemRKP21","ArXiv":"2010.15036","MAG":"3095760691","DOI":"10.1145/3434237","CorpusId":225094551},"title":"A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","externalIds":{"ArXiv":"2010.11934","DBLP":"conf/naacl/XueCRKASBR21","MAG":"3169483174","ACL":"2021.naacl-main.41","DOI":"10.18653/V1/2021.NAACL-MAIN.41","CorpusId":225040574},"title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"645bd6eadc247989abc5e0b0aa0be79ec8b11ea6","externalIds":{"MAG":"3089430725","DBLP":"journals/corr/abs-2010-00133","ArXiv":"2010.00133","ACL":"2020.emnlp-main.154","DOI":"10.18653/v1/2020.emnlp-main.154","CorpusId":222090785},"title":"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","externalIds":{"MAG":"3088599783","ACL":"2020.findings-emnlp.301","DBLP":"journals/corr/abs-2009-11462","ArXiv":"2009.11462","DOI":"10.18653/v1/2020.findings-emnlp.301","CorpusId":221878771},"title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"4dee6b82c7e59973ccd1520ff83f6b66f4d4bed4","externalIds":{"MAG":"3087082845","ArXiv":"2009.09372","DBLP":"journals/corr/abs-2009-09372","CorpusId":221818756},"title":"Softmax Tempering for Training Neural Machine Translation Models"},{"paperId":"6048cbb88d9a6691bfade0d46b41650533ac42bd","externalIds":{"MAG":"3094590205","CorpusId":225054352},"title":"Real-Time Execution of Large-scale Language Models on Mobile."},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","externalIds":{"MAG":"3081168214","DBLP":"conf/kdd/RasleyRRH20","DOI":"10.1145/3394486.3406703","CorpusId":221191193},"title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"a942cc84412d6a52adab325909b52296c017df48","externalIds":{"MAG":"3096835037","DBLP":"journals/corr/abs-2010-15875","ArXiv":"2010.15875","DOI":"10.24963/ijcai.2020/505","CorpusId":220483148},"title":"Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","externalIds":{"ArXiv":"2106.15772","DBLP":"conf/acl/MiaoLS20","MAG":"3034643750","ACL":"2020.acl-main.92","DOI":"10.18653/v1/2020.acl-main.92","CorpusId":220047831},"title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"c014f8bc3b521453a93a13bb2c90700fcf462738","externalIds":{"MAG":"3102835887","DBLP":"journals/corr/abs-2006-12467","ArXiv":"2006.12467","CorpusId":219965648},"title":"Limits to Depth Efficiencies of Self-Attention"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"d97e7561fa7710213ccd4f8128044ea6849be377","externalIds":{"DBLP":"journals/corr/abs-2005-00333","MAG":"3099771192","ACL":"2020.emnlp-main.185","ArXiv":"2005.00333","DOI":"10.18653/v1/2020.emnlp-main.185","CorpusId":218470125},"title":"XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"},{"paperId":"24e4d3370dc366d6b353d1d6818a0df266bb31b9","externalIds":{"MAG":"3023935112","ACL":"2020.emnlp-main.647","ArXiv":"2004.14900","DBLP":"journals/corr/abs-2004-14900","DOI":"10.18653/v1/2020.emnlp-main.647","CorpusId":216868259},"title":"MLSUM: The Multilingual Summarization Corpus"},{"paperId":"414f232eda907f7fe7eb5b56f0efcde6c78b0d0b","externalIds":{"MAG":"3017462938","DBLP":"journals/corr/abs-2004-11142","CorpusId":216080804},"title":"DuReaderrobust: A Chinese Dataset Towards Evaluating the Robustness of Machine Reading Comprehension Models"},{"paperId":"babeda48b10a4d638252118f2238d05a06f4ec55","externalIds":{"ACL":"2021.acl-long.416","DBLP":"journals/corr/abs-2004-09456","MAG":"3019416653","ArXiv":"2004.09456","DOI":"10.18653/v1/2021.acl-long.416","CorpusId":215828184},"title":"StereoSet: Measuring stereotypical bias in pretrained language models"},{"paperId":"2ffcf8352223c95ae8cef4daaec995525ecc926b","externalIds":{"ArXiv":"2004.08994","MAG":"3017003177","DBLP":"journals/corr/abs-2004-08994","CorpusId":215828407},"title":"Adversarial Training for Large Neural Language Models"},{"paperId":"71017cc6d270d28d9edcd47550450dc05edd65f4","externalIds":{"DBLP":"conf/acl/SmithWSWB20","MAG":"3034600233","ArXiv":"2004.08449","ACL":"2020.acl-main.183","DOI":"10.18653/v1/2020.acl-main.183","CorpusId":215827653},"title":"Can You Put it All Together: Evaluating Conversational Agents’ Ability to Blend Skills"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","externalIds":{"ArXiv":"2004.05986","ACL":"2020.coling-main.419","DBLP":"conf/coling/XuHZLCLXSYYTDLS20","MAG":"3114651185","DOI":"10.18653/V1/2020.COLING-MAIN.419","CorpusId":215745536},"title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"2081ac22151c1075fcc6533f0935c29d486bfa6f","externalIds":{"MAG":"3114361292","DBLP":"conf/coling/CuiLYCMCWH20","ArXiv":"2004.03116","ACL":"2020.coling-main.589","DOI":"10.18653/v1/2020.coling-main.589","CorpusId":215238329},"title":"A Sentence Cloze Dataset for Chinese Machine Reading Comprehension"},{"paperId":"4980093337f6b4a3a960ba95a54689ee491bc8ca","externalIds":{"ACL":"2020.acl-main.635","ArXiv":"2004.04100","DBLP":"journals/corr/abs-2004-04100","MAG":"3015364215","DOI":"10.18653/v1/2020.acl-main.635","CorpusId":215415968},"title":"KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation"},{"paperId":"725d5acdbdf0a11677f785a16e1722b92c55a47f","externalIds":{"MAG":"3019858811","DBLP":"conf/acl/XuPWLL20","ACL":"2020.acl-main.330","ArXiv":"2004.12302","DOI":"10.18653/v1/2020.acl-main.330","CorpusId":216553243},"title":"MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","externalIds":{"ArXiv":"2003.05002","MAG":"3045462440","DBLP":"journals/tacl/ClarkPNCGCK20","DOI":"10.1162/tacl_a_00317","CorpusId":212657414},"title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","externalIds":{"MAG":"3006439205","DBLP":"journals/corr/abs-2002-05202","ArXiv":"2002.05202","CorpusId":211096588},"title":"GLU Variants Improve Transformer"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","externalIds":{"ArXiv":"2002.08909","MAG":"3034671305","DBLP":"conf/icml/GuuLTPC20","CorpusId":211204736},"title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"3fa61dbb52424e7de78cb193b2460b8d38fac351","externalIds":{"DOI":"10.1007/978-1-4020-6754-9_3553","CorpusId":220162798},"title":"Constitutional"},{"paperId":"d08463bd665589d04619f04dbde84183ffcf2e63","externalIds":{"ArXiv":"2001.09977","DBLP":"journals/corr/abs-2001-09977","MAG":"3000779003","CorpusId":210920238},"title":"Towards a Human-like Open-Domain Chatbot"},{"paperId":"1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6","externalIds":{"MAG":"3002330681","DBLP":"journals/corr/abs-2001-08435","ArXiv":"2001.08435","DOI":"10.5281/ZENODO.3608135","CorpusId":210868223},"title":"The Pushshift Reddit Dataset"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","externalIds":{"MAG":"2970971581","DBLP":"journals/corr/abs-1912-01703","ArXiv":"1912.01703","CorpusId":202786778},"title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"04f4e55e14150b7c48b0287ba77c7443df76ed45","externalIds":{"DBLP":"conf/aaai/BiskZLGC20","MAG":"2998617917","ArXiv":"1911.11641","DOI":"10.1609/AAAI.V34I05.6239","CorpusId":208290939},"title":"PIQA: Reasoning about Physical Commonsense in Natural Language"},{"paperId":"f51497f463566581874c941353dd9d80069c5b77","externalIds":{"DBLP":"conf/iclr/RaePJHL20","MAG":"2995575179","ArXiv":"1911.05507","CorpusId":207930593},"title":"Compressive Transformers for Long-Range Sequence Modelling"},{"paperId":"2aef70dc36ce8c2aba1cc5e823e20a59db3f7326","externalIds":{"ArXiv":"1911.06191","DBLP":"journals/corr/abs-1911-06191","MAG":"2970752831","ACL":"W19-5348","DOI":"10.18653/v1/W19-5348","CorpusId":201739018},"title":"Microsoft Research Asia’s Systems for WMT19"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","externalIds":{"DBLP":"journals/corr/abs-1911-00359","MAG":"2989539713","ACL":"2020.lrec-1.494","ArXiv":"1911.00359","CorpusId":207870323},"title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","externalIds":{"MAG":"3034850762","ACL":"2020.acl-main.441","DBLP":"conf/acl/NieWDBWK20","ArXiv":"1910.14599","DOI":"10.18653/v1/2020.acl-main.441","CorpusId":207756753},"title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","externalIds":{"MAG":"2982399380","ACL":"2020.acl-main.703","DBLP":"journals/corr/abs-1910-13461","ArXiv":"1910.13461","DOI":"10.18653/v1/2020.acl-main.703","CorpusId":204960716},"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"faa5f1130ff9d05a8ae5b9ba79d675bd4d917c5d","externalIds":{"ACL":"2020.acl-main.653","MAG":"3035497479","ArXiv":"1910.07475","DBLP":"conf/acl/LewisORRS20","DOI":"10.18653/v1/2020.acl-main.653","CorpusId":204734128},"title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"10eda4521c032adabaa8e70d6569e17370b29dcd","externalIds":{"ArXiv":"1910.07467","DBLP":"conf/nips/ZhangS19a","MAG":"2981040094","DOI":"10.5167/UZH-177483","CorpusId":113405151},"title":"Root Mean Square Layer Normalization"},{"paperId":"703685e969fed715e13937c11d7ecc5cc7c4dfd0","externalIds":{"DBLP":"journals/corr/abs-1910-05895","ACL":"2019.iwslt-1.17","MAG":"2979636403","ArXiv":"1910.05895","DOI":"10.5281/zenodo.3525484","CorpusId":204512247},"title":"Transformers without Tears: Improving the Normalization of Self-Attention"},{"paperId":"83b8108014e3db4f46354a28ae68193f143c4e7e","externalIds":{"MAG":"3104216863","ACL":"2020.emnlp-main.496","ArXiv":"1910.04732","DBLP":"journals/corr/abs-1910-04732","DOI":"10.18653/v1/2020.emnlp-main.496","CorpusId":204009154},"title":"Structured Pruning of Large Language Models"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","externalIds":{"MAG":"3025935268","DBLP":"conf/sc/RajbhandariRRH20","ArXiv":"1910.02054","DOI":"10.1109/SC41405.2020.00024","CorpusId":269617042},"title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","externalIds":{"MAG":"2973529529","DBLP":"journals/corr/abs-1909-09436","ArXiv":"1909.09436","CorpusId":202712680},"title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"7a15950dc71079285a4eaf195de5aadd87c41b40","externalIds":{"MAG":"2973379954","DBLP":"journals/corr/abs-1909-08593","ArXiv":"1909.08593","CorpusId":202660943},"title":"Fine-Tuning Language Models from Human Preferences"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","externalIds":{"MAG":"2973727699","ArXiv":"1909.08053","DBLP":"journals/corr/abs-1909-08053","CorpusId":202660670},"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"6ffb1cc32ddde6ae01e2fc0286eafa116ade0ffb","externalIds":{"MAG":"2972340899","DBLP":"journals/corr/abs-1909-07005","ArXiv":"1909.07005","CorpusId":202577996},"title":"KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"},{"paperId":"d8cb11d4be955f9869387a18967dee366eb851d9","externalIds":{"DBLP":"journals/corr/abs-1909-03242","ACL":"D19-1475","MAG":"2971911738","ArXiv":"1909.03242","DOI":"10.18653/v1/D19-1475","CorpusId":202541363},"title":"MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","externalIds":{"ACL":"Q19-1026","MAG":"2912924812","DBLP":"journals/tacl/KwiatkowskiPRCP19","DOI":"10.1162/tacl_a_00276","CorpusId":86611921},"title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","externalIds":{"DBLP":"journals/corr/abs-1908-11828","MAG":"2970413168","ArXiv":"1908.11828","ACL":"D19-1382","DOI":"10.18653/v1/D19-1382","CorpusId":201698093},"title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"9ec95c1130a6ac4238ac2e5c7b2b66047511ea92","externalIds":{"ACL":"D19-1321","DBLP":"journals/corr/abs-1908-06605","MAG":"2968258309","ArXiv":"1908.06605","DOI":"10.18653/v1/D19-1321","CorpusId":201070608},"title":"Long and Diverse Text Generation with Planning-based Hierarchical Variational Model"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"39e801ca0dbc69c3697f118e24dac964abb63d4a","externalIds":{"MAG":"3026404337","DOI":"10.18148/SUB/2019.V23I2.601","CorpusId":203595067},"title":"The CommitmentBank: Investigating projection in naturally occurring discourse"},{"paperId":"401dc39c2c8c910253d47980cfa3b4d2f7790d9b","externalIds":{"MAG":"2963259903","ArXiv":"1907.10641","DBLP":"conf/aaai/SakaguchiBBC20","DOI":"10.1145/3474381","CorpusId":198893658},"title":"WinoGrande"},{"paperId":"ebf59587f8f170ff4241c42263bbfb9da5bd2135","externalIds":{"MAG":"2964040452","DBLP":"conf/acl/FanJPGWA19","ACL":"P19-1346","ArXiv":"1907.09190","DOI":"10.18653/v1/P19-1346","CorpusId":196170479},"title":"ELI5: Long Form Question Answering"},{"paperId":"7334f45c06555d4b6bf7e6b4437574c11369697e","externalIds":{"DBLP":"conf/acl/LiDLZS19","ACL":"P19-1430","MAG":"2950118387","DOI":"10.18653/v1/P19-1430","CorpusId":196193355},"title":"Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge"},{"paperId":"04234cd1cad396f76b96042227041abc9e525b0a","externalIds":{"DBLP":"conf/acl/VilaresG19","MAG":"2950372330","ACL":"P19-1092","ArXiv":"1906.04701","DOI":"10.18653/v1/P19-1092","CorpusId":184487171},"title":"HEAD-QA: A Healthcare Dataset for Complex Reasoning"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","externalIds":{"ArXiv":"1906.02243","MAG":"2963809228","DBLP":"journals/corr/abs-1906-02243","ACL":"P19-1355","DOI":"10.18653/v1/P19-1355","CorpusId":174802812},"title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"e278e072774f23675266881750e20bca74804cb9","externalIds":{"MAG":"2949884065","DBLP":"conf/acl/ZhengHS19","ArXiv":"1906.01265","ACL":"P19-1075","DOI":"10.18653/v1/P19-1075","CorpusId":174798153},"title":"ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"},{"paperId":"8a1744da011375d711ed75fc2d160c6fdca2cf89","externalIds":{"MAG":"2954861308","DBLP":"journals/corr/abs-1906-10770","ArXiv":"1906.10770","DOI":"10.1109/CVPR.2019.00644","CorpusId":195657908},"title":"Deep Modular Co-Attention Networks for Visual Question Answering"},{"paperId":"ad7129af0644dbcafa9aa2f111cb76526ea444a1","externalIds":{"MAG":"2971008823","DBLP":"conf/nips/ZellersHRBFRC19","ArXiv":"1905.12616","CorpusId":168169824},"title":"Defending Against Neural Fake News"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","externalIds":{"MAG":"2971274815","ArXiv":"1905.03197","DBLP":"journals/corr/abs-1905-03197","CorpusId":147704286},"title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","externalIds":{"MAG":"2943552823","ArXiv":"1905.00537","DBLP":"conf/nips/WangPNSMHLB19","CorpusId":143424870},"title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"9770fff7379a7ab9006b48939462354dda9a2053","externalIds":{"MAG":"2953271402","DBLP":"journals/corr/abs-1905-10044","ArXiv":"1905.10044","ACL":"N19-1300","DOI":"10.18653/v1/N19-1300","CorpusId":165163607},"title":"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"},{"paperId":"8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad","externalIds":{"MAG":"2946609015","DBLP":"journals/corr/abs-1905-07830","ACL":"P19-1472","ArXiv":"1905.07830","DOI":"10.18653/v1/P19-1472","CorpusId":159041722},"title":"HellaSwag: Can a Machine Really Finish Your Sentence?"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","externalIds":{"ArXiv":"1904.10509","DBLP":"journals/corr/abs-1904-10509","MAG":"2940744433","CorpusId":129945531},"title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"1b07a24b81834116f6ad1d0232485ba81b9445f3","externalIds":{"ArXiv":"1904.09679","MAG":"2995238850","ACL":"2020.tacl-1.10","DBLP":"journals/tacl/SunYYC20","DOI":"10.1162/tacl_a_00305","CorpusId":214233456},"title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"},{"paperId":"fc09d6486be1c9bbfbef4165ce3c1ab664e5d084","externalIds":{"MAG":"2954508062","DBLP":"conf/naacl/ZhangBH19","ArXiv":"1904.01130","ACL":"N19-1131","DOI":"10.18653/v1/N19-1131","CorpusId":91184042},"title":"PAWS: Paraphrase Adversaries from Word Scrambling"},{"paperId":"b611a8095630557229dc5fb6b07c272f1cd614da","externalIds":{"MAG":"2920807444","DBLP":"conf/www/BorkanDSTV19","ArXiv":"1903.04561","DOI":"10.1145/3308560.3317593","CorpusId":75135222},"title":"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","externalIds":{"MAG":"2919420119","DBLP":"conf/naacl/DuaWDSS019","ACL":"N19-1246","ArXiv":"1903.00161","DOI":"10.18653/v1/N19-1246","CorpusId":67855846},"title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","externalIds":{"MAG":"2911435132","ACL":"P19-1334","DBLP":"journals/corr/abs-1902-01007","ArXiv":"1902.01007","DOI":"10.18653/v1/P19-1334","CorpusId":59599752},"title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"9ae17b09c59f06f02ef824b856a440de663471d0","externalIds":{"ArXiv":"1902.00098","MAG":"2913443447","DBLP":"journals/corr/abs-1902-00098","DOI":"10.1007/978-3-030-29135-8_7","CorpusId":59553505},"title":"The Second Conversational Intelligence Challenge (ConvAI2)"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","externalIds":{"ArXiv":"1901.02860","DBLP":"conf/acl/DaiYYCLS19","MAG":"2964110616","ACL":"P19-1285","DOI":"10.18653/v1/P19-1285","CorpusId":57759363},"title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"eefa0df7c5678fa6004f8b48dbbc1c2696702fee","externalIds":{"DBLP":"journals/corr/abs-1812-06162","ArXiv":"1812.06162","MAG":"2903697572","CorpusId":56262183},"title":"An Empirical Model of Large-Batch Training"},{"paperId":"e9b13731027418ed38103d1dfc8a70f6881bc684","externalIds":{"MAG":"2903867219","ArXiv":"1812.05252","DBLP":"conf/cvpr/GaoJYLHWL19","DOI":"10.1109/CVPR.2019.00680","CorpusId":54700454},"title":"Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering"},{"paperId":"889ad3c713bd7f1b3a8e9b07e136ec4a88651893","externalIds":{"MAG":"2903314293","DBLP":"journals/access/ZhangZWGL18","DOI":"10.1109/ACCESS.2018.2883637","CorpusId":56598900},"title":"Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection"},{"paperId":"be2e66b8b28bfad2cbfa3087176b79ec5ab1ec04","externalIds":{"DBLP":"conf/acml/LiLLLSW18","MAG":"2912689473","CorpusId":53586918},"title":"Character-based BiLSTM-CRF Incorporating POS and Dictionaries for Chinese Opinion Target Extraction"},{"paperId":"a33a06ddc762fb855b6954c08d5aca603080b011","externalIds":{"MAG":"2951583236","DBLP":"conf/acl/RashkinSLB19","ACL":"P19-1534","DOI":"10.18653/v1/P19-1534","CorpusId":195069365},"title":"Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset"},{"paperId":"a5b66ee341cb990f7f70a124b5fab3316d3b7e27","externalIds":{"DBLP":"journals/corr/abs-1810-12885","MAG":"2898662126","ArXiv":"1810.12885","CorpusId":53116244},"title":"ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"},{"paperId":"227458886343b86bd15adf58c769be326b4b058a","externalIds":{"MAG":"2898875342","DBLP":"conf/iclr/DinanRSFAW19","ArXiv":"1811.01241","CorpusId":53218829},"title":"Wizard of Wikipedia: Knowledge-Powered Conversational agents"},{"paperId":"d170bd486e4c0fe82601e322b0e9e0dde63ab299","externalIds":{"MAG":"2894175714","DBLP":"conf/iclr/BaevskiA19","ArXiv":"1809.10853","CorpusId":52892477},"title":"Adaptive Input Representations for Neural Language Modeling"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","externalIds":{"ArXiv":"1809.05053","ACL":"D18-1269","MAG":"2891555348","DBLP":"conf/emnlp/ConneauRLWBSS18","DOI":"10.18653/v1/D18-1269","CorpusId":52271711},"title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"84a6d47676c2d2c1414d3893d09e47d33906fb1c","externalIds":{"MAG":"2889235221","DBLP":"journals/corr/abs-1808-09121","CorpusId":52109970},"title":"WiC: 10, 000 Example Pairs for Evaluating Context-Sensitive Representations"},{"paperId":"305b2cf37e5dece81e95c92883d5a6e28ac93b22","externalIds":{"DBLP":"conf/emnlp/NarayanCL18","MAG":"2888482885","ArXiv":"1808.08745","ACL":"D18-1206","DOI":"10.18653/v1/D18-1206","CorpusId":215768182},"title":"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"},{"paperId":"990a7b4eceedb6e053e6386269481bdfc42a1094","externalIds":{"MAG":"2888296173","DBLP":"journals/tacl/ReddyCM19","ACL":"Q19-1016","ArXiv":"1808.07042","DOI":"10.1162/tacl_a_00266","CorpusId":52055325},"title":"CoQA: A Conversational Question Answering Challenge"},{"paperId":"39e734da43eb8c72e9549b42e96760545036f8e5","externalIds":{"DBLP":"journals/corr/abs-1808-07036","ACL":"D18-1241","MAG":"2951831170","ArXiv":"1808.07036","DOI":"10.18653/v1/D18-1241","CorpusId":52057510},"title":"QuAC: Question Answering in Context"},{"paperId":"1536e8958697c5364f68b2e2448905dbbeb3a0ca","externalIds":{"DBLP":"journals/corr/abs-1809-02789","MAG":"2952396187","ACL":"D18-1260","ArXiv":"1809.02789","DOI":"10.18653/v1/D18-1260","CorpusId":52183757},"title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"},{"paperId":"549c1a581b61f9ea47afc6f6871845392eaebbc4","externalIds":{"ACL":"C18-1166","MAG":"2876111955","DBLP":"conf/coling/LiuCDZCLT18","CorpusId":52012862},"title":"LCQMC:A Large-scale Chinese Question Matching Corpus"},{"paperId":"2e29be79de2bb255784b65f4ecd59824b8cc21fe","externalIds":{"DBLP":"journals/corr/abs-1807-02478","ArXiv":"1807.02478","MAG":"2858159822","CorpusId":49652844},"title":"CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","externalIds":{"MAG":"2963323070","ACL":"P18-2124","ArXiv":"1806.03822","DBLP":"journals/corr/abs-1806-03822","DOI":"10.18653/v1/P18-2124","CorpusId":47018994},"title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"d7b6753a2d4a2b286c396854063bde3a91b75535","externalIds":{"ArXiv":"1806.02847","DBLP":"journals/corr/abs-1806-02847","MAG":"2805206884","CorpusId":47015717},"title":"A Simple Method for Commonsense Reasoning"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","externalIds":{"MAG":"2806081754","ArXiv":"1806.00920","DBLP":"journals/corr/abs-1806-00920","CorpusId":46932369},"title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"99ad0533f84c110da2d0713d5798e6e14080b159","externalIds":{"DBLP":"conf/naacl/KhashabiCRUR18","ACL":"N18-1023","MAG":"2804897457","DOI":"10.18653/v1/N18-1023","CorpusId":5112038},"title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","externalIds":{"MAG":"2950919220","DBLP":"conf/acl/Kudo18","ArXiv":"1804.10959","ACL":"P18-1007","DOI":"10.18653/v1/P18-1007","CorpusId":13753208},"title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","externalIds":{"MAG":"2963457723","DBLP":"conf/naacl/RudingerNLD18","ArXiv":"1804.09301","ACL":"N18-2002","DOI":"10.18653/v1/N18-2002","CorpusId":13756572},"title":"Gender Bias in Coreference Resolution"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"0be19fd9896e5d40222c690cc3ff553adc7c0e27","externalIds":{"MAG":"2963526187","DBLP":"conf/naacl/ZhaoWYOC18","ACL":"N18-2003","ArXiv":"1804.06876","DOI":"10.18653/v1/N18-2003","CorpusId":4952494},"title":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","externalIds":{"ArXiv":"1803.05457","DBLP":"journals/corr/abs-1803-05457","MAG":"2794325560","CorpusId":3922816},"title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"b1d24e8e08435b7c52335485a0d635abf9bc604c","externalIds":{"MAG":"2789566302","ACL":"N18-1074","ArXiv":"1803.05355","DBLP":"conf/naacl/ThorneVCM18","DOI":"10.18653/v1/N18-1074","CorpusId":4711425},"title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification"},{"paperId":"c0fdddc750f58373ad6b1e30660812ef9903b7fe","externalIds":{"MAG":"2963099470","ACL":"P19-1632","DBLP":"conf/acl/LiuNWGHLX19","DOI":"10.18653/v1/P19-1632","CorpusId":167217689},"title":"Matching Article Pairs with Graphical Decomposition and Convolutions"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","externalIds":{"DBLP":"journals/corr/abs-1802-05365","MAG":"2949856395","ArXiv":"1802.05365","ACL":"N18-1202","DOI":"10.18653/v1/N18-1202","CorpusId":3626819},"title":"Deep Contextualized Word Representations"},{"paperId":"8691706ad0cf5e83969658b2e6bfffdc379440c9","externalIds":{"ArXiv":"1801.10198","MAG":"2950355077","DBLP":"conf/iclr/LiuSPGSKS18","CorpusId":3608234},"title":"Generating Wikipedia by Summarizing Long Sequences"},{"paperId":"9589244bbff8c5b5e57f52f99776cda332e6ba48","externalIds":{"ArXiv":"1711.07010","MAG":"2768172751","DBLP":"journals/corr/abs-1711-07010","CorpusId":34327624},"title":"A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text"},{"paperId":"995b7affd684b910d5a1c520c3af00fd20cc39b0","externalIds":{"DBLP":"journals/corr/abs-1711-05073","ACL":"W18-2605","ArXiv":"1711.05073","MAG":"2952341389","DOI":"10.18653/v1/W18-2605","CorpusId":3662564},"title":"DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications"},{"paperId":"e7fd6848cb29ca221a7e17d823e06fb566f1f135","externalIds":{"DBLP":"journals/corr/abs-1710-03740","ArXiv":"1710.03740","MAG":"2963112338","CorpusId":3297437},"title":"Mixed Precision Training"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","externalIds":{"ACL":"D17-1088","DBLP":"conf/emnlp/WangLS17","MAG":"2757276219","DOI":"10.18653/v1/D17-1088","CorpusId":910689},"title":"Deep Neural Solver for Math Word Problems"},{"paperId":"96bc7e517759afa2972278ef206796154a295c98","externalIds":{"MAG":"2740205663","DOI":"10.3390/APP7080767","CorpusId":33716308},"title":"Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs"},{"paperId":"932a5de79d8a8ebb75ea0c43493450fd9922e738","externalIds":{"DBLP":"journals/corr/WelblLG17","MAG":"2963123047","ACL":"W17-4413","ArXiv":"1707.06209","DOI":"10.18653/v1/W17-4413","CorpusId":1553193},"title":"Crowdsourcing Multiple Choice Science Questions"},{"paperId":"d02d98da59475cc0e671edb290bee74d4d82656b","externalIds":{"MAG":"2591536208","DOI":"10.1016/j.annemergmed.2017.01.007","CorpusId":20383938,"PubMed":"28238502"},"title":"Know What You Don't Know."},{"paperId":"531a7f2c659787165df4fd5b4580590b953448e4","externalIds":{"MAG":"2963912046","ACL":"W17-5525","DBLP":"conf/sigdial/NovikovaDR17","ArXiv":"1706.09254","DOI":"10.18653/v1/W17-5525","CorpusId":19662556},"title":"The E2E Dataset: New Challenges For End-to-End Generation"},{"paperId":"ea738439b880ad033ff01602ea52d04b366d0d37","externalIds":{"MAG":"3099479658","ArXiv":"1706.06613","DBLP":"conf/sigir/XiongDCLP17","DOI":"10.1145/3077136.3080809","CorpusId":5878197},"title":"End-to-End Neural Ad-hoc Ranking with Kernel Pooling"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"40fbb2a926e46f59341b8aa7c4359a9602a9f5b5","externalIds":{"ArXiv":"1706.02021","DBLP":"journals/corr/GuoYZC17","MAG":"2950095934","DOI":"10.1109/CVPR.2017.430","CorpusId":11244259},"title":"Network Sketching: Exploiting Binary Structure in Deep CNNs"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","externalIds":{"ACL":"P17-1015","DBLP":"conf/acl/LingYDB17","ArXiv":"1705.04146","MAG":"2613312549","DOI":"10.18653/v1/P17-1015","CorpusId":12777818},"title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","externalIds":{"MAG":"2612431505","ArXiv":"1705.03551","ACL":"P17-1147","DBLP":"journals/corr/JoshiCWZ17","DOI":"10.18653/v1/P17-1147","CorpusId":26501419},"title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","externalIds":{"DBLP":"journals/corr/WilliamsNB17","MAG":"2963846996","ArXiv":"1704.05426","ACL":"N18-1101","DOI":"10.18653/v1/N18-1101","CorpusId":3432876},"title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"636a79420d838eabe4af7fb25d6437de45ab64e8","externalIds":{"MAG":"2606964149","DBLP":"journals/corr/LaiXLYH17","ArXiv":"1704.04683","ACL":"D17-1082","DOI":"10.18653/v1/D17-1082","CorpusId":6826032},"title":"RACE: Large-scale ReAding Comprehension Dataset From Examinations"},{"paperId":"510e26733aaff585d65701b9f1be7ca9d5afc586","externalIds":{"DBLP":"journals/corr/ShazeerMMDLHD17","MAG":"2952339051","ArXiv":"1701.06538","CorpusId":12462234},"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"},{"paperId":"88caa4a0253a8b0076176745ebc072864eab66e1","externalIds":{"MAG":"2567070169","DBLP":"conf/icml/DauphinFAG17","ArXiv":"1612.08083","CorpusId":16119010},"title":"Language Modeling with Gated Convolutional Networks"},{"paperId":"efbd381493bb9636f489b965a2034d529cd56bcd","externalIds":{"ArXiv":"1609.07843","MAG":"2525332836","DBLP":"journals/corr/MerityXBS16","CorpusId":16299141},"title":"Pointer Sentinel Mixture Models"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","externalIds":{"DBLP":"conf/wmt/BojarCFGHHJKLMN16","MAG":"2512924740","ACL":"W16-2301","DOI":"10.18653/v1/W16-2301","CorpusId":14421595},"title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","externalIds":{"ACL":"D15-1202","ArXiv":"1608.01413","MAG":"2951624407","DBLP":"journals/corr/RoyR16","DOI":"10.18653/v1/D15-1202","CorpusId":560565},"title":"Solving General Arithmetic Word Problems"},{"paperId":"7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c","externalIds":{"MAG":"2508354372","DBLP":"conf/emnlp/BlodgettGO16","ArXiv":"1608.08868","ACL":"D16-1120","DOI":"10.18653/v1/D16-1120","CorpusId":1066490},"title":"Demographic Dialectal Variation in Social Media: A Case Study of African-American English"},{"paperId":"0199888fe2945829b004d7253c3876b7909b3808","externalIds":{"MAG":"2492957416","DBLP":"journals/bise/MaedcheMSWK16","DOI":"10.1007/S12599-016-0444-2","CorpusId":27949558},"title":"Advanced User Assistance Systems"},{"paperId":"bdf28e3cadbabda3261bd904c37edea66ab84766","externalIds":{"MAG":"2495998536","DBLP":"journals/corr/LiLHWCZX16","ArXiv":"1607.06275","CorpusId":6901603},"title":"Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","externalIds":{"MAG":"3037932933","ArXiv":"1607.06450","DBLP":"journals/corr/BaKH16","CorpusId":8236317},"title":"Layer Normalization"},{"paperId":"de5e7320729f5d3cbb6709eb6329ec41ace8c95d","externalIds":{"ArXiv":"1606.08415","MAG":"2899663614","CorpusId":125617073},"title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"5ed791f810da580c78df6a052c6b9f2e258f6b0a","externalIds":{"MAG":"2952915793","ArXiv":"1606.06031","DBLP":"conf/acl/PapernoKLPBPBBF16","ACL":"P16-1144","DOI":"10.18653/v1/P16-1144","CorpusId":2381275},"title":"The LAMBADA dataset: Word prediction requiring a broad discourse context"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","externalIds":{"DBLP":"journals/corr/RajpurkarZLL16","MAG":"2963748441","ACL":"D16-1264","ArXiv":"1606.05250","DOI":"10.18653/v1/D16-1264","CorpusId":11816014},"title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","externalIds":{"DBLP":"conf/naacl/Koncel-Kedziorski16","MAG":"2475046758","ACL":"N16-1136","DOI":"10.18653/v1/N16-1136","CorpusId":2228719},"title":"MAWPS: A Math Word Problem Repository"},{"paperId":"9f0687bcd0a7d7fc91b8c5d36c003a38b8853105","externalIds":{"ArXiv":"1606.01305","MAG":"2409027918","DBLP":"conf/iclr/KruegerMKPBKGBC17","CorpusId":12200521},"title":"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"},{"paperId":"69e76e16740ed69f4dc55361a3d319ac2f1293dd","externalIds":{"MAG":"2964043796","DBLP":"journals/corr/MnihBMGLHSK16","ArXiv":"1602.01783","CorpusId":6875312},"title":"Asynchronous Methods for Deep Reinforcement Learning"},{"paperId":"62df84d6a4d26f95e4714796c2337c9848cc13b5","externalIds":{"ArXiv":"1512.01274","DBLP":"journals/corr/ChenLLLWWXXZZ15","MAG":"2186615578","CorpusId":1507815},"title":"MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems"},{"paperId":"d64561879a2fbd3d39a5e876a667ffa4561eed80","externalIds":{"DBLP":"conf/emnlp/PengD15","MAG":"2250709962","ACL":"D15-1064","DOI":"10.18653/v1/D15-1064","CorpusId":12266201},"title":"Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","externalIds":{"DBLP":"conf/acl/SennrichHB16a","ACL":"P16-1162","MAG":"1816313093","ArXiv":"1508.07909","DOI":"10.18653/v1/P16-1162","CorpusId":1114678},"title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"b122a828f5fee3c6afc54e70f41b00184d6383fc","externalIds":{"ArXiv":"1506.05865","ACL":"D15-1229","MAG":"2951457522","DBLP":"journals/corr/HuCZ15","DOI":"10.18653/v1/D15-1229","CorpusId":11597846},"title":"LCSTS: A Large Scale Chinese Short Text Summarization Dataset"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"88789ee88311acef28475ad33dbcd6b3c4be8358","externalIds":{"MAG":"1978012692","DOI":"10.1177/1527476412450193","CorpusId":46997949},"title":"Wikipedia"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","externalIds":{"ACL":"D13-1160","MAG":"2252136820","DBLP":"conf/emnlp/BerantCFL13","DOI":"10.18653/v1/d13-1160","CorpusId":6401679},"title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"00beaaa3cc933ecafd815727eb082c28f1338a2e","externalIds":{"DBLP":"conf/clef/PenasHFRSM13","MAG":"204995840","DOI":"10.1007/978-3-642-40802-1_29","CorpusId":40290810},"title":"QA4MRE 2011-2013: Overview of Question Answering for Machine Reading Evaluation"},{"paperId":"296094909b3a3524b8265410b6f9c4c63ebc9de8","externalIds":{"DBLP":"conf/emnlp/Boyd-GraberSHD12","ACL":"D12-1118","MAG":"1934849960","CorpusId":215514151},"title":"Besting the Quiz Master: Crowdsourcing Incremental Classification Games"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","externalIds":{"DBLP":"conf/icassp/SchusterN12","MAG":"2121879602","DOI":"10.1109/ICASSP.2012.6289079","CorpusId":22320655},"title":"Japanese and Korean voice search"},{"paperId":"128cb6b891aee1b5df099acb48e2efecfcff689f","externalIds":{"DBLP":"conf/aaaiss/Levesque11","MAG":"2267020232","CorpusId":15710851},"title":"The Winograd Schema Challenge"},{"paperId":"5cfbbf3cdff0f905874589bcd21b2646340a5447","externalIds":{"MAG":"2145755360","DBLP":"conf/aaaiss/RoemmeleBG11","CorpusId":434646},"title":"Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"},{"paperId":"a538b05ebb01a40323997629e171c91aa28b8e2f","externalIds":{"MAG":"1665214252","DBLP":"conf/icml/NairH10","CorpusId":15539264},"title":"Rectified Linear Units Improve Restricted Boltzmann Machines"},{"paperId":"47ced790a563344efae66588b5fb7fe6cca29ed3","externalIds":{"MAG":"2155482025","DBLP":"journals/ftir/RobertsonZ09","DOI":"10.1561/1500000019","CorpusId":207178704},"title":"The Probabilistic Relevance Framework: BM25 and Beyond"},{"paperId":"3e4bc1aa55c752918ae99b1a125f6adef61afad2","externalIds":{"DBLP":"journals/ai/CampbellHH02","MAG":"2911296969","DOI":"10.1016/S0004-3702(01)00129-1","CorpusId":662187},"title":"Deep Blue"},{"paperId":"db6ae486a695efc02910b1dc08eeba13b50d5ca8","externalIds":{"ACL":"C92-4173","MAG":"2037450062","DBLP":"conf/coling/WebsterK92","DOI":"10.3115/992424.992434","CorpusId":3170394},"title":"Tokenization as the Initial Phase in NLP"},{"paperId":"f22f6972e66bdd2e769fa64b0df0a13063c0c101","externalIds":{"DBLP":"journals/nn/HornikSW89","MAG":"2137983211","DOI":"10.1016/0893-6080(89)90020-8","CorpusId":2757547},"title":"Multilayer feedforward networks are universal approximators"},{"paperId":"c38b4734b0c2da90b3053b79619455ef2ad91145","externalIds":{"MAG":"2070018906","DOI":"10.3758/BF03210367","CorpusId":144603191},"title":"Some characteristics of selective attention in visual perception determined by vocal reaction time"},{"paperId":"8aa98fbfb6f1e979dead13ce24075503fe47658e","externalIds":{"DBLP":"journals/corr/abs-2301-00234","CorpusId":263886074},"title":"A Survey for In-context Learning"},{"paperId":"4972b88f8f324a4fa18e921f62a9857af2b5fc7b","externalIds":{"DBLP":"conf/acl/MuennighoffWSRB23","ACL":"2023.acl-long.891","DOI":"10.18653/v1/2023.acl-long.891","CorpusId":253264914},"title":"Crosslingual Generalization through Multitask Finetuning"},{"paperId":"4ebf49a7c053bf1d22fcce17bc8c80db827e8f99","externalIds":{"CorpusId":259282629},"title":"Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning"},{"paperId":"e3aa232577bb427b1f3a34acbdef84bd85734042","externalIds":{"DBLP":"journals/corr/abs-2308-16137","DOI":"10.48550/arXiv.2308.16137","CorpusId":261339508},"title":"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","externalIds":{"ArXiv":"2303.17580","DBLP":"journals/corr/abs-2303-17580","DOI":"10.48550/arXiv.2303.17580","CorpusId":257833781},"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"ed9943d73eb42116fe33564b5065c78b5ca0b16e","externalIds":{"DBLP":"journals/corr/abs-2306-06624","DOI":"10.48550/arXiv.2306.06624","CorpusId":259138886},"title":"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs"},{"paperId":"ac771182d1780c863954243809d1e144433919f9","externalIds":{"DBLP":"journals/corr/abs-2307-12966","DOI":"10.48550/arXiv.2307.12966","CorpusId":260356605},"title":"Aligning Large Language Models with Human: A Survey"},{"paperId":"3221d2373e2f501a623d72eb65a91694e9e162ac","externalIds":{"DOI":"10.14569/ijacsa.2023.0140607","CorpusId":259311318},"title":"Investigating OpenAI’s ChatGPT Potentials in Generating Chatbot's Dialogue for English as a Foreign Language Learning"},{"paperId":"343d24c4dcfaff2132373d218561a23fbd53e934","externalIds":{"DBLP":"journals/corr/abs-2306-02272","DOI":"10.48550/arXiv.2306.02272","CorpusId":259076427},"title":"OWQ: Lessons learned from activation outliers for weight quantization in large language models"},{"paperId":"81051b830a4f5606106765902a51ba281c9230f9","externalIds":{"DBLP":"journals/corr/abs-2304-09145","DOI":"10.48550/arXiv.2304.09145","CorpusId":258187503},"title":"Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"},{"paperId":"bb318b579cf47eddb9e120353653ef4cad87c4ed","externalIds":{"DBLP":"journals/corr/abs-2309-04198","DOI":"10.48550/arXiv.2309.04198","CorpusId":274981296},"title":"The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature"},{"paperId":"cc1f72c2334703be01063536705b116ba49341cd","externalIds":{"DBLP":"journals/corr/abs-2307-12950","DOI":"10.48550/arXiv.2307.12950","CorpusId":274712366},"title":"RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment"},{"paperId":"ec64e324ce1210fe5245dfd0fb5a92058732e5b9","externalIds":{"DBLP":"journals/corr/abs-2204-07705","CorpusId":248227391},"title":"Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks"},{"paperId":"dccd764ec820c13369e91c53890dfc8cd0334355","externalIds":{"DBLP":"journals/corr/abs-2211-11682","DOI":"10.48550/arXiv.2211.11682","CorpusId":253735373},"title":"PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning"},{"paperId":"7e2530784eeae241e997627795819cf42ba8562f","externalIds":{"DBLP":"journals/corr/abs-2205-12410","DOI":"10.48550/arXiv.2205.12410","CorpusId":249063002},"title":"AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models"},{"paperId":"ec936b808e0fab9281c050ad4010cddec92c8cbe","externalIds":{"ACL":"2022.acl-short.8","DBLP":"conf/acl/LiuJFTDY022","DOI":"10.18653/v1/2022.acl-short.8","CorpusId":248780177},"title":"P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"b9478e237b58160c65acd2c41894493d27e2c277","externalIds":{"MAG":"3169113923","DBLP":"journals/aiopen/YuanZDDLCZYT21","DOI":"10.1016/J.AIOPEN.2021.06.001","CorpusId":236712622},"title":"WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"},{"paperId":"3ede1108e6cbad247b875aa46b4541967a83b980","externalIds":{"CorpusId":247234889},"title":"Limits to Depth Efﬁciencies of Self-Attention Supplementary Material"},{"paperId":"e8f297e161f57e461ede2d4e0c26573981cad077","externalIds":{"ACL":"2020.wmt-1.1","DBLP":"conf/wmt/BarraultBBCFGGH20","DOI":"10.18653/v1/2020.wmt-1.1","CorpusId":229365773},"title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"paperId":"3ee38da21d8cf9cb7d4077b729e57f68e9c8d671","externalIds":{"ArXiv":"2009.07839","MAG":"3085932930","DBLP":"journals/corr/abs-2009-07839","CorpusId":221738927},"title":"Text Generation by Learning from Demonstrations"},{"paperId":"310b8117ae5ce3df8aa6304ad382525b9b46937e","externalIds":{"ACL":"2020.webnlg-papers.7","CorpusId":230091790},"title":"The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task: Overview and Evaluation Results (WebNLG+ 2020)"},{"paperId":"f73a99c817a22c9aa0c4ffd38d0a1f08d541b2ef","externalIds":{"DOI":"10.1007/978-3-030-29135-8","CorpusId":263984013},"title":"The NeurIPS '18 Competition: From Machine Learning to Intelligent Conversations"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","externalIds":{"MAG":"2970277902","DBLP":"journals/corr/abs-1810-07366","ACL":"D19-1600","ArXiv":"1810.07366","DOI":"10.18653/v1/D19-1600","CorpusId":52984852},"title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"92e121c6e114fe3cfb89370df03847c66a9b4e28","externalIds":{"CorpusId":199370376},"title":"An Adversarial Winograd Schema Challenge at Scale"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","externalIds":{"MAG":"2952331680","ACL":"N19-1421","DBLP":"journals/corr/abs-1811-00937","ArXiv":"1811.00937","DOI":"10.18653/v1/N19-1421","CorpusId":53296520},"title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"8ce1512e77fa6a646513a60d78e0081afe870c07","externalIds":{"MAG":"2962934410","DBLP":"conf/lrec/CuiLCMWH18","ArXiv":"1709.08299","ACL":"L18-1431","CorpusId":3935253},"title":"Dataset for the First Evaluation on Chinese Machine Reading Comprehension"},{"paperId":"7afb83134d5b7914131e10b229d30dc2593266f6","externalIds":{"ACL":"D18-1536","MAG":"2889968917","DBLP":"conf/emnlp/ChenCLYLT18","DOI":"10.18653/v1/D18-1536","CorpusId":53081318},"title":"The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","externalIds":{"DBLP":"conf/acl/PanZMNKJ17","MAG":"2742113707","ACL":"P17-1178","DOI":"10.18653/v1/P17-1178","CorpusId":29939583},"title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"0c0a778e6fdf7e36b1750c533dcc916f86608607","externalIds":{"MAG":"2527310337","DBLP":"journals/tkde/XunJGZ17","DOI":"10.1109/TKDE.2016.2614508","CorpusId":13490401},"title":"A Survey on Context Learning"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","externalIds":{"DBLP":"journals/jmlr/SrivastavaHKSS14","MAG":"2095705004","DOI":"10.5555/2627435.2670313","CorpusId":6844431},"title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"5e537c4d988d55f74d0bd5bb5015208977fc52e6","externalIds":{"CorpusId":126210996},"title":"FWDselect : Variable selection algorithm in regression models"},{"paperId":"e808f28d411a958c5db81ceb111beb2638698f47","externalIds":{"MAG":"111307969","DBLP":"conf/mlcw/DaganGM05","CorpusId":8587959},"title":"The PASCAL Recognising Textual Entailment Challenge"},{"paperId":"4954fa180728932959997a4768411ff9136aac81","externalIds":{"DBLP":"journals/corr/AbadiBCCDDDGIIK16","MAG":"2402144811","ArXiv":"1605.08695","CorpusId":6287870},"title":"TensorFlow: A system for large-scale machine learning"},{"paperId":"24de1048791bac4972ecc16d1c3c1de23691407d","externalIds":{"CorpusId":266378240},"title":"Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"},{"paperId":"22391aa6619fd72e688dad51c846be67c810a6d0","externalIds":{"CorpusId":273457870},"title":"Fine-tuning a LLM using Reinforcement Learning from Human Feedback for a Therapy Chatbot Application"}]}