{"references":[{"paperId":"c3f1fae241a3c2449e675ab750873d800f95513c","externalIds":{"DBLP":"journals/corr/abs-2405-14734","ArXiv":"2405.14734","DOI":"10.48550/arXiv.2405.14734","CorpusId":269983560},"title":"SimPO: Simple Preference Optimization with a Reference-Free Reward"},{"paperId":"2052a297586451686bbf959c47254cc3db13abab","externalIds":{"DBLP":"conf/acl/HuaQZTL0025","ArXiv":"2405.11870","DOI":"10.48550/arXiv.2405.11870","CorpusId":269922026},"title":"Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process"},{"paperId":"4a597a081721e436e20b4e85197072e22aaecfad","externalIds":{"ArXiv":"2404.12358","CorpusId":269214194},"title":"From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"},{"paperId":"973814cd535facbf4f27c3de477b05bf19366030","externalIds":{"DBLP":"journals/corr/abs-2403-07691","ArXiv":"2403.07691","ACL":"2024.emnlp-main.626","DOI":"10.48550/arXiv.2403.07691","CorpusId":268363309},"title":"ORPO: Monolithic Preference Optimization without Reference Model"},{"paperId":"66e7edf09589527ebb58418632418758cee668cd","externalIds":{"ArXiv":"2403.04204","DBLP":"conf/ijcai/0001DYYZ00X0024","DOI":"10.48550/arXiv.2403.04204","CorpusId":268264604},"title":"On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"},{"paperId":"d27da1ba65fa958e45837120fad1c25e7017d80c","externalIds":{"DBLP":"journals/corr/abs-2402-17135","ArXiv":"2402.17135","DOI":"10.48550/arXiv.2402.17135","CorpusId":268033694},"title":"Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings"},{"paperId":"8be81d531dfc4a1145474a1bb2f9c0cf15e19f45","externalIds":{"ArXiv":"2402.16030","DBLP":"journals/corr/abs-2402-16030","DOI":"10.48550/arXiv.2402.16030","CorpusId":267938516},"title":"Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration"},{"paperId":"6e3eed2b1ec12cf13bcc3a7427113b832193265a","externalIds":{"ArXiv":"2402.06853","DBLP":"journals/aiethics/WangCDNYZ25","DOI":"10.1007/s43681-024-00583-7","CorpusId":267627936},"title":"History, development, and principles of large language models: an introductory survey"},{"paperId":"bf40c8f88875f7c591dddc0936542918f4083b22","externalIds":{"ArXiv":"2402.05070","DBLP":"journals/corr/abs-2402-05070","DOI":"10.48550/arXiv.2402.05070","CorpusId":267523348},"title":"A Roadmap to Pluralistic Alignment"},{"paperId":"f977dac98cc603bfccae6ea991cf4b1f83bf139c","externalIds":{"DBLP":"conf/naacl/LiuQWSKJZSBLLW25","ArXiv":"2402.01878","DOI":"10.48550/arXiv.2402.01878","CorpusId":267411871},"title":"LiPO: Listwise Preference Optimization through Learning-to-Rank"},{"paperId":"c0d8e5ee66c279299012cc3b8d0519011b3f4998","externalIds":{"DBLP":"journals/corr/abs-2402-01306","ArXiv":"2402.01306","CorpusId":267406810},"title":"KTO: Model Alignment as Prospect Theoretic Optimization"},{"paperId":"3a589ce2b38da3083ce1b63b2785646536a364f9","externalIds":{"DBLP":"conf/icml/Ji0NKW00H24","ArXiv":"2402.00856","CorpusId":267365004},"title":"Towards Efficient Exact Optimization of Language Model Alignment"},{"paperId":"e360eb07461f2741793f99ece8b97a6c04fb2b68","externalIds":{"ArXiv":"2401.06838","DBLP":"conf/acl/SheZHZLGC24","DOI":"10.48550/arXiv.2401.06838","CorpusId":266998982},"title":"MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization"},{"paperId":"59084df7203c6be33838ba3e3854eb9bda053ed2","externalIds":{"DBLP":"journals/corr/abs-2401-06081","ArXiv":"2401.06081","DOI":"10.48550/arXiv.2401.06081","CorpusId":266933254},"title":"Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint"},{"paperId":"411114f989a3d1083d90afd265103132fee94ebe","externalIds":{"DBLP":"journals/corr/abs-2401-04088","ArXiv":"2401.04088","DOI":"10.48550/arXiv.2401.04088","CorpusId":266844877},"title":"Mixtral of Experts"},{"paperId":"ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5","externalIds":{"DBLP":"conf/icml/ChenDYJG24","ArXiv":"2401.01335","DOI":"10.48550/arXiv.2401.01335","CorpusId":266725672},"title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"},{"paperId":"ec639bacdf18bdd2e715c55b6ecd3471bd72f240","externalIds":{"DBLP":"journals/corr/abs-2312-16430","ArXiv":"2312.16430","DOI":"10.48550/arXiv.2312.16430","CorpusId":266573420},"title":"Preference as Reward, Maximum Preference Optimization with Importance Sampling"},{"paperId":"aee47d4f45d5c02f79fff62ce4147f0d382cd87e","externalIds":{"ArXiv":"2312.15997","DBLP":"conf/acl/LiuWWLLLZZZ024","DOI":"10.48550/arXiv.2312.15997","CorpusId":266551232},"title":"Aligning Large Language Models with Human Preferences through Representation Engineering"},{"paperId":"485f8a429cf5f70c558181187f2d62e31784deaa","externalIds":{"DBLP":"conf/acl/Xu0ZL024","ArXiv":"2312.14591","DOI":"10.48550/arXiv.2312.14591","CorpusId":266521165},"title":"Reasons to Reject? Aligning Language Models with Judgments"},{"paperId":"867c82da010e0cb2c69e7d8fe12f94ba6a49ee74","externalIds":{"ArXiv":"2312.14925","DBLP":"journals/corr/abs-2312-14925","DOI":"10.48550/arXiv.2312.14925","CorpusId":266521540},"title":"A Survey of Reinforcement Learning from Human Feedback"},{"paperId":"6b97aa78bcdb88548c44e7e1671c0ed37ed37976","externalIds":{"ArXiv":"2312.09390","DBLP":"conf/icml/BurnsIKBGACEJLS24","DOI":"10.48550/arXiv.2312.09390","CorpusId":266312608},"title":"Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"},{"paperId":"d1f925c65d56ff4de5d317a54d47d6df34b17d4e","externalIds":{"DBLP":"conf/cvpr/JiangXDCYYYZHZ24","ArXiv":"2312.06968","DOI":"10.1109/CVPR52733.2024.02553","CorpusId":266174200},"title":"Hallucination Augmented Contrastive Learning for Multimodal Large Language Model"},{"paperId":"1f1638ca845881545f364ca23b1fae46f729e72f","externalIds":{"ArXiv":"2312.02554","CorpusId":265659430},"title":"ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference"},{"paperId":"84259db14b725853ecfe425fe85ca375b32983c2","externalIds":{"DBLP":"conf/acl/ChengYLDHCDL24","ArXiv":"2311.08045","DOI":"10.48550/arXiv.2311.08045","CorpusId":265157873},"title":"Adversarial Preference Optimization"},{"paperId":"e51f20efb872d0ba99a8b501259948bbb2f6963f","externalIds":{"DBLP":"conf/iclr/GuoZT0W24","ArXiv":"2311.04072","DOI":"10.48550/arXiv.2311.04072","CorpusId":265043685},"title":"Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment"},{"paperId":"07cdf957a11506f87fbc030dcfaaa6399847648c","externalIds":{"ArXiv":"2310.20246","DBLP":"journals/corr/abs-2310-20246","DOI":"10.48550/arXiv.2310.20246","CorpusId":264770810},"title":"Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations"},{"paperId":"a3d1954a57110f199ad58c24a6e588ee73135170","externalIds":{"ArXiv":"2310.19852","DBLP":"journals/corr/abs-2310-19852","CorpusId":264743032},"title":"AI Alignment: A Comprehensive Survey"},{"paperId":"9262e201ecaf678429bf86481b1c1d1ae6332b44","externalIds":{"ArXiv":"2310.16955","DBLP":"journals/tmlr/SinhaBBACB24","DOI":"10.48550/arXiv.2310.16955","CorpusId":264490789},"title":"Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks"},{"paperId":"8dc334f0dd716585eee05d2daaff330de4916a9e","externalIds":{"DBLP":"journals/corr/abs-2310-16763","ArXiv":"2310.16763","DOI":"10.48550/arXiv.2310.16763","CorpusId":264451639},"title":"SuperHF: Supervised Iterative Learning from Human Feedback"},{"paperId":"378a51082ddf7430f928b7dde59186c041eb4b6c","externalIds":{"DBLP":"conf/emnlp/LiLZLW23","ArXiv":"2310.13385","DOI":"10.48550/arXiv.2310.13385","CorpusId":264405905},"title":"Tuna: Instruction Tuning using Feedback from Large Language Models"},{"paperId":"0f7308fbcae43d22813f70c334c2425df0b1cce1","externalIds":{"ArXiv":"2310.12773","DBLP":"conf/iclr/DaiPSJXL0024","DOI":"10.48550/arXiv.2310.12773","CorpusId":264306078},"title":"Safe RLHF: Safe Reinforcement Learning from Human Feedback"},{"paperId":"a84d6b82947f27bc6bf7f42d69f48b40adcfb6c3","externalIds":{"ArXiv":"2310.11971","DBLP":"journals/corr/abs-2310-11971","DOI":"10.48550/arXiv.2310.11971","CorpusId":264289051},"title":"Improving Generalization of Alignment with Human Preferences through Group Invariant Learning"},{"paperId":"f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f","externalIds":{"DBLP":"journals/corr/abs-2310-12036","ArXiv":"2310.12036","DOI":"10.48550/arXiv.2310.12036","CorpusId":264288854},"title":"A General Theoretical Paradigm to Understand Learning from Human Preferences"},{"paperId":"2be910eb19f2f8f2e8038d2a835bc48f868ccbf1","externalIds":{"DBLP":"journals/corr/abs-2310-11523","ArXiv":"2310.11523","DOI":"10.48550/arXiv.2310.11523","CorpusId":264289064},"title":"Group Preference Optimization: Few-Shot Alignment of Large Language Models"},{"paperId":"d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855","externalIds":{"ArXiv":"2310.10505","DBLP":"journals/corr/abs-2310-10505","DOI":"10.48550/arXiv.2310.10505","CorpusId":264146066},"title":"ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"},{"paperId":"dd7a74a09fc29cadcd47fafc4f7812bb8d2d7208","externalIds":{"DBLP":"journals/corr/abs-2310-07629","ArXiv":"2310.07629","DOI":"10.48550/arXiv.2310.07629","CorpusId":263834741},"title":"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values"},{"paperId":"5001630bcc65e8e0e621b19625629a2689724743","externalIds":{"DBLP":"conf/iclr/LiSYF0024","ArXiv":"2310.05470","DOI":"10.48550/arXiv.2310.05470","CorpusId":263829791},"title":"Generative Judge for Evaluating Alignment"},{"paperId":"4118c8bca76b4bfafc379d80bf91455df88614b6","externalIds":{"DBLP":"journals/corr/abs-2310-05782","ArXiv":"2310.05782","DOI":"10.48550/arXiv.2310.05782","CorpusId":263831305},"title":"Aligning Language Models with Human Preferences via a Bayesian Approach"},{"paperId":"e6776f5f293c18f4b2322b1479f083cb24d33343","externalIds":{"ArXiv":"2310.05344","DBLP":"journals/corr/abs-2310-05344","DOI":"10.48550/arXiv.2310.05344","CorpusId":263830508},"title":"SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF"},{"paperId":"af7669dc48c70d8cf6fccdf1322d6056a6b39dc8","externalIds":{"DBLP":"journals/corr/abs-2310-04373","ArXiv":"2310.04373","DOI":"10.48550/arXiv.2310.04373","CorpusId":263829192},"title":"Confronting Reward Model Overoptimization with Constrained RLHF"},{"paperId":"023d462ec6ff84cee0d0716a34d11efc7cde8534","externalIds":{"DBLP":"conf/iclr/CosteAK024","ArXiv":"2310.02743","DOI":"10.48550/arXiv.2310.02743","CorpusId":263620686},"title":"Reward Model Ensembles Help Mitigate Overoptimization"},{"paperId":"311b5c770738fabc940b3b630664d562916df83c","externalIds":{"ArXiv":"2310.01045","DBLP":"journals/corr/abs-2310-01045","DOI":"10.48550/arXiv.2310.01045","CorpusId":263605400},"title":"Tool-Augmented Reward Modeling"},{"paperId":"f279b73a1f6034be4261009f2810a01b9f2fb6e3","externalIds":{"ArXiv":"2310.00212","DBLP":"journals/corr/abs-2310-00212","DOI":"10.48550/arXiv.2310.00212","CorpusId":263334045},"title":"Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"},{"paperId":"e8df1cf6742b50a15500b8dd3dde3942e9c91418","externalIds":{"DBLP":"conf/icml/WanFWM00024","ArXiv":"2309.17179","DOI":"10.48550/arXiv.2309.17179","CorpusId":263310590},"title":"Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"},{"paperId":"860c8de4fdac38695ff6860dd15312f1079c6117","externalIds":{"DBLP":"conf/iclr/WangJYLC24","ArXiv":"2309.16240","DOI":"10.48550/arXiv.2309.16240","CorpusId":263142109},"title":"Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"},{"paperId":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","externalIds":{"DBLP":"journals/corr/abs-2309-16609","ArXiv":"2309.16609","DOI":"10.48550/arXiv.2309.16609","CorpusId":263134555},"title":"Qwen Technical Report"},{"paperId":"749d59f887c8ac83fd4f5178465e8b03e463358c","externalIds":{"DBLP":"journals/corr/abs-2309-15025","ArXiv":"2309.15025","DOI":"10.48550/arXiv.2309.15025","CorpusId":262824801},"title":"Large Language Model Alignment: A Survey"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"29f032fc875576b5c3c6b1c2d76af8639bacfb88","externalIds":{"ArXiv":"2309.11235","DBLP":"journals/corr/abs-2309-11235","DOI":"10.48550/arXiv.2309.11235","CorpusId":262064307},"title":"OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"},{"paperId":"c96297261467b5daa2d01227496a70d444602434","externalIds":{"DBLP":"journals/corr/abs-2309-10305","ArXiv":"2309.10305","DOI":"10.48550/arXiv.2309.10305","CorpusId":261951743},"title":"Baichuan 2: Open Large-scale Language Models"},{"paperId":"19a0777498ec3ef1e11e8349df3eb336cc19698d","externalIds":{"ACL":"2023.sigdial-1.27","DBLP":"conf/sigdial/RichardsonH23","ArXiv":"2309.10015","DOI":"10.48550/arXiv.2309.10015","CorpusId":262053831},"title":"Syndicom: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback"},{"paperId":"22ab4219371366a4e890382bc0ca606130840ca7","externalIds":{"ArXiv":"2309.06657","DBLP":"conf/iclr/0002ZJKSLL24","DOI":"10.48550/arXiv.2309.06657","CorpusId":261705578},"title":"Statistical Rejection Sampling Improves Preference Optimization"},{"paperId":"74b4b993babe99bc5f5c589c27fef0f1baba606b","externalIds":{"DBLP":"journals/corr/abs-2309-02144","ArXiv":"2309.02144","DOI":"10.48550/arXiv.2309.02144","CorpusId":261558535},"title":"Making Large Language Models Better Reasoners with Alignment"},{"paperId":"600ff4c4ae9fc506c86673c5ecce4fa90803e987","externalIds":{"DBLP":"conf/icml/0001PMMFLBHCRP24","ArXiv":"2309.00267","CorpusId":261493811},"title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"},{"paperId":"c12db2e67d1fb289266faa5507ff112c9a062465","externalIds":{"DBLP":"journals/corr/abs-2309-00754","ArXiv":"2309.00754","DOI":"10.48550/arXiv.2309.00754","CorpusId":261530070},"title":"Efficient RLHF: Reducing the Memory Usage of PPO"},{"paperId":"78b0c5d96a05b4be6a00702fba24c9174e8173af","externalIds":{"ArXiv":"2308.12050","CorpusId":261076271},"title":"Aligning Language Models with Offline Learning from Human Feedback"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","externalIds":{"DBLP":"journals/corr/abs-2308-09583","ArXiv":"2308.09583","DOI":"10.48550/arXiv.2308.09583","CorpusId":261030818},"title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"f2ba9e7d9624bd94a786ea5e3161a9425a21a475","externalIds":{"DBLP":"conf/iclr/LiYZSLZWL24","ArXiv":"2308.06259","DOI":"10.48550/arXiv.2308.06259","CorpusId":260866107},"title":"Self-Alignment with Instruction Backtranslation"},{"paperId":"7f55ef29a6f8b2771c5435bbeba29c87264fdc88","externalIds":{"ArXiv":"2308.04592","DBLP":"journals/corr/abs-2308-04592","DOI":"10.48550/arXiv.2308.04592","CorpusId":260735852},"title":"Shepherd: A Critic for Language Model Generation"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"92930ed3560ea6c86d53cf52158bc793b089054d","externalIds":{"ArXiv":"2307.04657","DBLP":"conf/nips/JiLDPZB0SW023","DOI":"10.48550/arXiv.2307.04657","CorpusId":259501579},"title":"BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"},{"paperId":"19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e","externalIds":{"DBLP":"conf/aaai/00010LYHLW24","ArXiv":"2306.17492","DOI":"10.48550/arXiv.2306.17492","CorpusId":259308873},"title":"Preference Ranking Optimization for Human Alignment"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"ccd94602e3acecf999d0c9ba62b1a8bc02e9f696","externalIds":{"ArXiv":"2306.05087","DBLP":"journals/corr/abs-2306-05087","DOI":"10.48550/arXiv.2306.05087","CorpusId":259108266},"title":"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization"},{"paperId":"0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8","externalIds":{"DBLP":"conf/nips/RameCDGSSC23","ArXiv":"2306.04488","DOI":"10.48550/arXiv.2306.04488","CorpusId":259096117},"title":"Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"},{"paperId":"e2e52461194bc81351da7caa978ac42e9e9549cc","externalIds":{"DBLP":"conf/nips/WuHSDSASOH23","ArXiv":"2306.01693","DOI":"10.48550/arXiv.2306.01693","CorpusId":259064099},"title":"Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"},{"paperId":"be8db99310602d66bba64bcf41a572c45816fbfc","externalIds":{"ArXiv":"2305.20050","DBLP":"conf/iclr/LightmanKBEBLLS24","DOI":"10.48550/arXiv.2305.20050","CorpusId":258987659},"title":"Let's Verify Step by Step"},{"paperId":"38d64919ba526868a850a0e5f6239d4c474b7e7e","externalIds":{"DBLP":"journals/corr/abs-2305-17926","ArXiv":"2305.17926","DOI":"10.48550/arXiv.2305.17926","CorpusId":258960339},"title":"Large Language Models are not Fair Evaluators"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"13fd4277388cc2a9da75e8b772e5efcf6ebe2d32","externalIds":{"ArXiv":"2305.16960","DBLP":"conf/iclr/LiuYJZYV24","CorpusId":264590778},"title":"Training Socially Aligned Language Models on Simulated Social Interactions"},{"paperId":"5b8f0460d408a8688d9ee0cba127c779d3291d99","externalIds":{"ArXiv":"2305.13735","DBLP":"journals/corr/abs-2305-13735","DOI":"10.48550/arXiv.2305.13735","CorpusId":258841835},"title":"Aligning Large Language Models through Synthetic Feedback"},{"paperId":"cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa","externalIds":{"DBLP":"journals/corr/abs-2305-14387","ArXiv":"2305.14387","DOI":"10.48550/arXiv.2305.14387","CorpusId":258865545},"title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"58af2d4fcca54c14334d1efd975554b4eb78cd4d","externalIds":{"ArXiv":"2305.10425","DBLP":"journals/corr/abs-2305-10425","DOI":"10.48550/arXiv.2305.10425","CorpusId":258741082},"title":"SLiC-HF: Sequence Likelihood Calibration with Human Feedback"},{"paperId":"b6d6c33298b852cf63edac233deca70530d69a2a","externalIds":{"ArXiv":"2305.10403","DBLP":"journals/corr/abs-2305-10403","CorpusId":258740735},"title":"PaLM 2 Technical Report"},{"paperId":"ebf35cef5c249d90b40043fffa41f8802c27f132","externalIds":{"DBLP":"conf/acl/AkyurekAKCWT23","ArXiv":"2305.08844","ACL":"2023.acl-long.427","DOI":"10.48550/arXiv.2305.08844","CorpusId":258685337},"title":"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"e01515c6138bc525f7aec30fc85f2adf028d4156","externalIds":{"DBLP":"journals/corr/abs-2305-03047","ArXiv":"2305.03047","DOI":"10.48550/arXiv.2305.03047","CorpusId":258479665},"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"},{"paperId":"74b05bba46db21e589a2cc0f916f81069b0368ef","externalIds":{"DBLP":"journals/corr/abs-2305-00955","ArXiv":"2305.00955","DOI":"10.48550/arXiv.2305.00955","CorpusId":258426970},"title":"Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"ae736662f64d56f3ab1894fbd9c45f8f37251843","externalIds":{"ArXiv":"2304.07327","DBLP":"conf/nips/KopfKRATSBNSNES23","DOI":"10.48550/arXiv.2304.07327","CorpusId":258179434},"title":"OpenAssistant Conversations - Democratizing Large Language Model Alignment"},{"paperId":"68c834c19cd126bbd6d25a3572d7205cfed76271","externalIds":{"DBLP":"journals/corr/abs-2304-06364","ArXiv":"2304.06364","DOI":"10.48550/arXiv.2304.06364","CorpusId":258108259},"title":"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"},{"paperId":"3ab661db57d924f4ff1706e05ac807873ca00e0a","externalIds":{"DBLP":"journals/corr/abs-2304-06767","ArXiv":"2304.06767","DOI":"10.48550/arXiv.2304.06767","CorpusId":258170300},"title":"RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"},{"paperId":"748698bd4387afd08594e0dc8150c2afa210d9ae","externalIds":{"DBLP":"conf/nips/YuanYTWHH23","ArXiv":"2304.05302","DOI":"10.48550/arXiv.2304.05302","CorpusId":258059818},"title":"RRHF: Rank Responses to Align Language Models with Human Feedback without tears"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"d2170504c4ad9403bea118ae8debdfda95978546","externalIds":{"ArXiv":"2302.05206","DBLP":"journals/corr/abs-2302-05206","DOI":"10.48550/arXiv.2302.05206","CorpusId":256808689},"title":"The Wisdom of Hindsight Makes Language Models Better Instruction Followers"},{"paperId":"cb3125e4f63f3d058a2a39270ecb585e86c3d1ff","externalIds":{"DBLP":"journals/corr/abs-2302-02676","ArXiv":"2302.02676","DOI":"10.48550/arXiv.2302.02676","CorpusId":257038005},"title":"Chain of Hindsight Aligns Language Models with Feedback"},{"paperId":"2cd72e71299c5d62d5cdb1164df5236172d418c4","externalIds":{"DBLP":"journals/corr/abs-2301-00355","ArXiv":"2301.00355","DOI":"10.48550/arXiv.2301.00355","CorpusId":253306024},"title":"Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"3936fd3c6187f606c6e4e2e20b196dbc41cc4654","externalIds":{"DBLP":"journals/corr/abs-2212-08073","ArXiv":"2212.08073","DOI":"10.48550/arXiv.2212.08073","CorpusId":254823489},"title":"Constitutional AI: Harmlessness from AI Feedback"},{"paperId":"3eed4de25636ac90f39f6e1ef70e3507ed61a2a6","externalIds":{"DBLP":"journals/cacm/Shanahan24","ArXiv":"2212.03551","DOI":"10.1145/3624724","CorpusId":254366666},"title":"Talking about Large Language Models"},{"paperId":"de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a","externalIds":{"DBLP":"journals/corr/abs-2211-15006","ArXiv":"2211.15006","DOI":"10.48550/arXiv.2211.15006","CorpusId":254043997},"title":"Fine-tuning language models to find agreement among humans with diverse preferences"},{"paperId":"99ca5162211a895a5dfbff9d7e36e21e09ca646e","externalIds":{"ArXiv":"2211.03540","DBLP":"journals/corr/abs-2211-03540","DOI":"10.48550/arXiv.2211.03540","CorpusId":253384413},"title":"Measuring Progress on Scalable Oversight for Large Language Models"},{"paperId":"fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b","externalIds":{"DBLP":"conf/icml/GaoSH23","ArXiv":"2210.10760","CorpusId":252992904},"title":"Scaling Laws for Reward Model Overoptimization"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","externalIds":{"DBLP":"conf/acl/SuzgunSSGTCCLCZ23","ArXiv":"2210.09261","DOI":"10.48550/arXiv.2210.09261","CorpusId":252917648},"title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"912a39c2e0e4a35747531669cfa952d2c5627729","externalIds":{"ArXiv":"2210.01241","DBLP":"journals/corr/abs-2210-01241","DOI":"10.48550/arXiv.2210.01241","CorpusId":252693405},"title":"Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"},{"paperId":"06d7cb8c8816360feb33c3367073e0ef66d7d0b0","externalIds":{"ACL":"2022.emnlp-main.340","ArXiv":"2204.07705","DBLP":"conf/emnlp/WangMAKMNADASPK22","DOI":"10.18653/v1/2022.emnlp-main.340","CorpusId":253098274},"title":"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a3184d40d390793232c99c89b57b8f65c16320b2","externalIds":{"DBLP":"journals/corr/abs-2112-12731","ArXiv":"2112.12731","CorpusId":245425057},"title":"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","externalIds":{"DBLP":"journals/corr/abs-2112-09332","ArXiv":"2112.09332","CorpusId":245329531},"title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf","externalIds":{"ArXiv":"2112.04359","DBLP":"journals/corr/abs-2112-04359","CorpusId":244954639},"title":"Ethical and social risks of harm from Language Models"},{"paperId":"68f141724814839d556a989646194be88641b143","externalIds":{"ArXiv":"2112.11446","DBLP":"journals/corr/abs-2112-11446","CorpusId":245353475},"title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","externalIds":{"ArXiv":"2112.00861","DBLP":"journals/corr/abs-2112-00861","CorpusId":244799619},"title":"A General Language Assistant as a Laboratory for Alignment"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"6c4bd57f7e70b9c037e44a96840e5ead0513abb0","externalIds":{"DBLP":"conf/icml/EthayarajhCS22","ArXiv":"2110.08420","CorpusId":250340652},"title":"Understanding Dataset Difficulty with V-Usable Information"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","externalIds":{"DBLP":"journals/corr/abs-2109-07958","ACL":"2022.acl-long.229","ArXiv":"2109.07958","DOI":"10.18653/v1/2022.acl-long.229","CorpusId":237532606},"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","externalIds":{"ACL":"2022.acl-long.577","DBLP":"journals/corr/abs-2107-06499","ArXiv":"2107.06499","DOI":"10.18653/v1/2022.acl-long.577","CorpusId":235829052},"title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"49f905eb03958c7cfae52ac759ea8978b8b2a6ea","externalIds":{"DBLP":"journals/corr/abs-2103-14659","ArXiv":"2103.14659","CorpusId":232404883},"title":"Alignment of Language Agents"},{"paperId":"7e38476342ce1fcc8ef0dcd23686539395961769","externalIds":{"MAG":"3108981297","DBLP":"journals/corr/abs-2011-15091","ArXiv":"2011.15091","DOI":"10.1098/rspa.2021.0068","CorpusId":227239152},"title":"Inductive biases for deep learning of higher-level cognition"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","externalIds":{"MAG":"3088599783","ACL":"2020.findings-emnlp.301","DBLP":"journals/corr/abs-2009-11462","ArXiv":"2009.11462","DOI":"10.18653/v1/2020.findings-emnlp.301","CorpusId":221878771},"title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"f9ef88bfc78baeb24e697b05c307cf019f8a3630","externalIds":{"MAG":"3036973371","ArXiv":"2002.05058","DBLP":"journals/corr/abs-2002-05058","DOI":"10.1609/aaai.v34i05.6521","CorpusId":211082630},"title":"Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models"},{"paperId":"ec55f76812cacc12d29ec632d924377524a13022","externalIds":{"ArXiv":"1911.03860","ACL":"2020.acl-main.428","DBLP":"conf/acl/LiRKWBCW20","MAG":"2988615798","DOI":"10.18653/v1/2020.acl-main.428","CorpusId":207853191},"title":"Don’t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a15950dc71079285a4eaf195de5aadd87c41b40","externalIds":{"MAG":"2973379954","DBLP":"journals/corr/abs-1909-08593","ArXiv":"1909.08593","CorpusId":202660943},"title":"Fine-Tuning Language Models from Human Preferences"},{"paperId":"90d8f96e2cd71a50b40992020cb65bc75f352ea1","externalIds":{"ArXiv":"1909.01214","MAG":"2972005891","ACL":"D19-1307","DBLP":"journals/corr/abs-1909-01214","DOI":"10.18653/v1/D19-1307","CorpusId":202540345},"title":"Better Rewards Yield Better Summaries: Learning to Summarise Without References"},{"paperId":"53a77e8f73f2ca422d6e38fa9ecc490231ac044c","externalIds":{"MAG":"2968297680","DBLP":"conf/iclr/WelleckKRDCW20","ArXiv":"1908.04319","CorpusId":199551982},"title":"Neural Text Generation with Unlikelihood Training"},{"paperId":"04babf1157c27c2d8abe24be5c45c6e99078daf0","externalIds":{"DBLP":"conf/inlg/YiGKCCHVGH19","MAG":"2943766373","ArXiv":"1904.13015","ACL":"W19-8608","DOI":"10.18653/v1/W19-8608","CorpusId":140246560},"title":"Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators"},{"paperId":"b0b96270a9bbeb9f3ec040e70114d565fbcaaed9","externalIds":{"DBLP":"journals/corr/abs-1901-05415","ArXiv":"1901.05415","MAG":"2951998460","ACL":"P19-1358","DOI":"10.18653/v1/P19-1358","CorpusId":58007087},"title":"Learning from Dialogue after Deployment: Feed Yourself, Chatbot!"},{"paperId":"c6f913e4baa7f2c85363c0625c87003ad3b3a14c","externalIds":{"MAG":"2901707424","ArXiv":"1811.07871","DBLP":"journals/corr/abs-1811-07871","CorpusId":53745764},"title":"Scalable agent alignment via reward modeling: a research direction"},{"paperId":"3e6cde685fdf321d7edf9319f7b07c01ff79c11a","externalIds":{"MAG":"2889659926","DBLP":"journals/corr/abs-1811-06521","ArXiv":"1811.06521","CorpusId":53424488},"title":"Reward learning from human preferences and demonstrations in Atari"},{"paperId":"32bfc242d0e85f1b6f2ff838c37287f8cfddf7c2","externalIds":{"MAG":"2963325985","ACL":"W19-2401","DOI":"10.18653/v1/W19-2401","CorpusId":168169995},"title":"Towards Coherent and Cohesive Long-form Text Generation"},{"paperId":"0052b31f07eda7737b5e0e2bf3803c3a32f3f728","externalIds":{"MAG":"2896930824","DBLP":"journals/corr/abs-1810-08575","ArXiv":"1810.08575","CorpusId":53041432},"title":"Supervising strong learners by amplifying weak experts"},{"paperId":"e77213a99f05ff8525fa4ee59a3b066164b994da","externalIds":{"DBLP":"conf/atal/BarlierLP18","MAG":"2808007596","DOI":"10.65109/miek6215","CorpusId":51871965},"title":"Training Dialogue Systems With Human Advice"},{"paperId":"15919637566348de8ca1e054151c24cc864b0f0e","externalIds":{"MAG":"2950095508","DBLP":"journals/corr/abs-1805-10627","ACL":"P18-1165","ArXiv":"1805.10627","DOI":"10.18653/v1/P18-1165","CorpusId":44062452},"title":"Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","externalIds":{"DBLP":"journals/corr/abs-1802-05365","MAG":"2949856395","ArXiv":"1802.05365","ACL":"N18-1202","DOI":"10.18653/v1/N18-1202","CorpusId":3626819},"title":"Deep Contextualized Word Representations"},{"paperId":"f5265e346382354887340c7b520d639162e2f598","externalIds":{"ACL":"W17-4508","DBLP":"conf/emnlp/VolskePSS17","MAG":"2760781482","DOI":"10.18653/v1/W17-4508","CorpusId":2204603},"title":"TL;DR: Mining Reddit to Learn Automatic Summarization"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"13d156f37ecea807706fd117547ac1b805d5c5aa","externalIds":{"DBLP":"conf/acl/WangLM16","MAG":"2964193163","ArXiv":"1606.02447","ACL":"P16-1224","DOI":"10.18653/v1/P16-1224","CorpusId":2705742},"title":"Learning Language Games through Interaction"},{"paperId":"53fcb4a07f6825439a311e61b98c007c5641e2f0","externalIds":{"DBLP":"conf/atal/AsriPGLP16","MAG":"2486334580","DOI":"10.5555/2936924.2936991","CorpusId":15719006},"title":"Score-based Inverse Reinforcement Learning"},{"paperId":"f37076f426023241f19cdc2fb0a0fd733a6fa7fa","externalIds":{"ArXiv":"1602.06023","DBLP":"conf/conll/NallapatiZSGX16","MAG":"2341401723","ACL":"K16-1028","DOI":"10.18653/v1/K16-1028","CorpusId":8928715},"title":"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"},{"paperId":"f009a64f87841b79b1e2ea1748366270455ef9c0","externalIds":{"MAG":"2573393487","DBLP":"conf/aaai/WirthFN16","DOI":"10.1609/aaai.v30i1.10269","CorpusId":17740071},"title":"Model-Free Preference-Based Reinforcement Learning"},{"paperId":"21c05d606d6899c42ae02e3b671e92faaaf130a7","externalIds":{"MAG":"1155403144","DBLP":"journals/arobots/DanielKVM015","DOI":"10.1007/s10514-015-9454-z","CorpusId":14757449},"title":"Active reward learning with a novel acquisition function"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","externalIds":{"DBLP":"conf/emnlp/PenningtonSM14","ACL":"D14-1162","MAG":"2250539671","DOI":"10.3115/v1/D14-1162","CorpusId":1957433},"title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","externalIds":{"MAG":"2130942839","DBLP":"conf/nips/SutskeverVL14","ArXiv":"1409.3215","CorpusId":7961699},"title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"16302319d910a1da77656133727f081c65995635","externalIds":{"MAG":"1566123042","DBLP":"conf/icml/SchoenauerASS14","CorpusId":77838},"title":"Programming by Feedback"},{"paperId":"ad4122205be956ea90930df42507cdc0079acbc0","externalIds":{"MAG":"120470225","DBLP":"conf/ida/WirthF13","DOI":"10.1007/978-3-642-41398-8_37","CorpusId":33245848},"title":"A Policy Iteration Algorithm for Learning from Preference-Based Feedback"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","externalIds":{"ArXiv":"1310.4546","MAG":"2950133940","DBLP":"conf/nips/MikolovSCCD13","CorpusId":16447573},"title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"1ee3d855069c31b7990a1ac554be9e20f408aefb","externalIds":{"DBLP":"conf/iui/KnoxS13","MAG":"2157726050","DOI":"10.1145/2449396.2449422","CorpusId":1750056},"title":"Learning non-myopically from human-generated reward"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","externalIds":{"MAG":"2950577311","DBLP":"journals/corr/abs-1301-3781","ArXiv":"1301.3781","CorpusId":5959482},"title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"b3af2e367d7297775c71fa9a61b0b49fb888bc38","externalIds":{"MAG":"2116671302","DBLP":"conf/nips/WilsonFT12","CorpusId":6019958},"title":"A Bayesian Approach for Policy Learning from Trajectory Preference Queries"},{"paperId":"80128eac4063571da71f8e731ac6b137dd208e24","externalIds":{"MAG":"2114864106","CorpusId":62656736},"title":"Learning from human-generated reward"},{"paperId":"b8014d4af46d9b26cf434240dced24294b2110b6","externalIds":{"DBLP":"journals/ml/FurnkranzHCP12","MAG":"2154023516","DOI":"10.1007/s10994-012-5313-8","CorpusId":10373780},"title":"Preference-based reinforcement learning: a formal framework and a policy iteration algorithm"},{"paperId":"781cfd4e09dc6d721844391f415518dfa2774f1d","externalIds":{"MAG":"1583953806","ArXiv":"1208.0984","DBLP":"conf/pkdd/AkrourSS12","DOI":"10.1007/978-3-642-33486-3_8","CorpusId":47518713},"title":"APRIL: Active Preference-learning based Reinforcement Learning"},{"paperId":"31936551c1f219abd7e202a1775c9b6d756912bb","externalIds":{"DBLP":"conf/aamas/KnoxS12","MAG":"2294422333","DOI":"10.65109/pajo3896","CorpusId":320141},"title":"Reinforcement learning from simultaneous human and MDP reward"},{"paperId":"6efd7b0e85f735f3aa750df521a40fd5caa5a77c","externalIds":{"DBLP":"conf/pkdd/ChengFHP11","MAG":"2129297552","DOI":"10.1007/978-3-642-23780-5_30","CorpusId":17361173},"title":"Preference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning"},{"paperId":"f1c72d92275e460d142db923faa6a9ecbb0a8346","externalIds":{"MAG":"2129659607","DBLP":"conf/ro-man/SuayC11","DOI":"10.1109/ROMAN.2011.6005223","CorpusId":1104128},"title":"Effect of human guidance and state space size on Interactive Reinforcement Learning"},{"paperId":"c54174bd1a98b1ae1fb111b32950fce538f32007","externalIds":{"MAG":"2098584016","DOI":"10.1109/ICORR.2011.5975338","CorpusId":341617,"PubMed":"22275543"},"title":"Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning"},{"paperId":"1c61f9ef06fe74505775a833ff849185757199e7","externalIds":{"MAG":"2113459411","ACL":"P11-1015","DBLP":"conf/acl/MaasDPHNP11","CorpusId":1428702},"title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"bc1022b031dc6c7019696492e8116598097a8c12","externalIds":{"MAG":"2158899491","DBLP":"journals/jmlr/CollobertWBKKK11","ArXiv":"1103.0398","DOI":"10.5555/1953048.2078186","CorpusId":351666},"title":"Natural Language Processing (Almost) from Scratch"},{"paperId":"20e4ba526a5bcca493e1a67f54719a3284539071","externalIds":{"DBLP":"conf/atal/KnoxS10","MAG":"2116157560","DOI":"10.65109/mcue9477","CorpusId":6676959},"title":"Combining manual feedback with subsequent MDP reward signals for reinforcement learning"},{"paperId":"256c3bd45ab7452bb51721eb25d3367bb654225e","externalIds":{"MAG":"2156869222","DBLP":"conf/kcap/KnoxS09","DOI":"10.1145/1597735.1597738","CorpusId":2994241},"title":"Interactively shaping agents via human reinforcement: the TAMER framework"},{"paperId":"5fc5c5a4e489e781de434567d946e6eb65c44f60","externalIds":{"MAG":"1973435495","DBLP":"journals/ftir/Liu09","DOI":"10.1007/978-3-642-14267-3","CorpusId":28826624},"title":"Learning to rank for information retrieval"},{"paperId":"fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f","externalIds":{"MAG":"2110064869","DOI":"10.1109/DEVLRN.2008.4640845","CorpusId":5613334},"title":"TAMER: Training an Agent Manually via Evaluative Reinforcement"},{"paperId":"9dc1748099dd4321d42fb84bc7ee1f71e7814459","externalIds":{"DBLP":"journals/talip/GaoL04","MAG":"2073147434","DOI":"10.1145/1034780.1034781","CorpusId":322920},"title":"Introduction to the special issue on statistical language modeling"},{"paperId":"7d986dac610e20441adb9161e5466c88932626e9","externalIds":{"DBLP":"journals/tois/ZhaiL04","MAG":"1972594981","DOI":"10.1145/984321.984322","CorpusId":207670589},"title":"A study of smoothing methods for language models applied to information retrieval"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","externalIds":{"MAG":"2140679639","DBLP":"conf/nips/BengioDV00","CorpusId":221275765},"title":"A Neural Probabilistic Language Model"},{"paperId":"109186b81a0575936297ae3a0ff41491124a4bf2","externalIds":{"DBLP":"conf/agents/IsbellSKSS01","MAG":"2118756286","DOI":"10.1145/375735.376334","CorpusId":462880},"title":"A social reinforcement learning agent"},{"paperId":"3c1d0b29b2c2217d9e17b38043fb73033d3efa49","externalIds":{"DBLP":"conf/nips/IsbellSKSS01","MAG":"2116372557","CorpusId":8689480},"title":"Cobot: A Social Reinforcement Learning Agent"},{"paperId":"c6586e7c73cc1c9e9a251947425c54c5051be626","externalIds":{"DBLP":"journals/pieee/Rosenfeld00","MAG":"2100506586","DOI":"10.1109/5.880083","CorpusId":10959945},"title":"Two decades of statistical language modeling: where do we go from here?"},{"paperId":"1e62711ba0e6385ee204b475a238c4e82811fc22","externalIds":{"DBLP":"conf/aaai/IsbellKKSS00","MAG":"2120804913","CorpusId":2025191},"title":"Cobot in LambdaMOO: A Social Statistics Agent"},{"paperId":"d71c82fcb1fa2d8bb79bbd848b20f6ec53b076a9","externalIds":{"MAG":"2174230203","DBLP":"journals/ml/MaclinS96","DOI":"10.1023/A:1018020625251","CorpusId":8469001},"title":"Creating Advice-Taking Reinforcement Learners"},{"paperId":"d4e8bed3b50a035e1eabad614fe4218a34b3b178","externalIds":{"MAG":"2158195707","ACL":"P96-1041","DBLP":"journals/csl/ChenG99","ArXiv":"cmp-lg/9606011","DOI":"10.3115/981863.981904","CorpusId":215842252},"title":"An Empirical Study of Smoothing Techniques for Language Modeling"},{"paperId":"c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58","externalIds":{"DBLP":"journals/pami/BahlJM83","MAG":"2151844356","DOI":"10.1109/TPAMI.1983.4767370","CorpusId":14789841,"PubMed":"21869099"},"title":"A Maximum Likelihood Approach to Continuous Speech Recognition"},{"paperId":"2894125ea5f8a3300bd098e7be2331f7789ff91b","externalIds":{"MAG":"2069277547","DOI":"10.1287/OPRE.11.3.399","CorpusId":122381976},"title":"Generalized Lagrange Multiplier Method for Solving Problems of Optimum Allocation of Resources"},{"paperId":"f4b5ed96ca25c0f61778a26d03cdd6c4b946b5ea","externalIds":{"DBLP":"conf/iclr/YangKCPT24","CorpusId":271745706},"title":"RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment"},{"paperId":"c2f1fbc5e829a8b667470bb4f61698f25210442b","externalIds":{"DBLP":"conf/iclr/0025L000WX24","CorpusId":271746234},"title":"CPPO: Continual Learning for Reinforcement Learning with Human Feedback"},{"paperId":"4c8cc2383cec93bd9ea0758692f01b98a035215b","externalIds":{"DBLP":"journals/corr/abs-2310-01377","DOI":"10.48550/arXiv.2310.01377","CorpusId":263605623},"title":"UltraFeedback: Boosting Language Models with High-quality Feedback"},{"paperId":"ac771182d1780c863954243809d1e144433919f9","externalIds":{"DBLP":"journals/corr/abs-2307-12966","DOI":"10.48550/arXiv.2307.12966","CorpusId":260356605},"title":"Aligning Large Language Models with Human: A Survey"},{"paperId":"4972b88f8f324a4fa18e921f62a9857af2b5fc7b","externalIds":{"DBLP":"conf/acl/MuennighoffWSRB23","ACL":"2023.acl-long.891","DOI":"10.18653/v1/2023.acl-long.891","CorpusId":253264914},"title":"Crosslingual Generalization through Multitask Finetuning"},{"paperId":"24df244bf7a6e8c93c5f183d3f62d39c0f773c68","externalIds":{"DBLP":"journals/corr/abs-2310-05910","DOI":"10.48550/arXiv.2310.05910","CorpusId":280526392},"title":"SALMON: Self-Alignment with Principle-Following Reward Models"},{"paperId":"7c363f962654392d67a3323cfeba4ae9cf1dec32","externalIds":{"MAG":"2322584079","DOI":"10.4135/9781071812082.n626","CorpusId":147342723},"title":"“Technique for the Measurement of Attitudes, A”"},{"paperId":"bb0656031cb17adf6bac5fd0fe8d53dd9c291508","externalIds":{"DBLP":"conf/nips/HoffmannBMBCRCH22","CorpusId":258509679},"title":"An empirical analysis of compute-optimal large language model training"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"84082634110fcedaaa32632f6cc16a034eedb2a0","externalIds":{"DBLP":"journals/jmlr/WirthANF17","MAG":"3006334608","CorpusId":703818},"title":"A Survey of Preference-Based Reinforcement Learning Methods"},{"paperId":"5e537c4d988d55f74d0bd5bb5015208977fc52e6","externalIds":{"CorpusId":126210996},"title":"FWDselect : Variable selection algorithm in regression models"},{"paperId":"cbbcf7b0ae2d7503836a5a1db42dd3104579972d","externalIds":{"CorpusId":16505586},"title":"Author manuscript, published in \"European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (2011)\" Preference-based Policy Learning"},{"paperId":"d37d6adbd1212140fc5e1bcba1f119ab7db85bf5","externalIds":{"MAG":"1667312704","DBLP":"books/daglib/p/FurnkranzH10","DOI":"10.1007/978-3-642-14125-6_1","CorpusId":15436717},"title":"Preference Learning: An Introduction"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","externalIds":{"DOI":"10.1023/A:1022672621406","CorpusId":2332513},"title":"Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"},{"paperId":"05d24d6f34197430d0387ad507dba0c90201364f","externalIds":{"CorpusId":115227571},"title":"Published online:"}]}