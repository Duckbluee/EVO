{"references":[{"paperId":"1df7661b39c278d1ae9991aee6e2c5d8be42f326","externalIds":{"ArXiv":"2503.05096","DBLP":"conf/cloud/HuangWSZYS25","DOI":"10.1145/3772052.3772239","CorpusId":276885099},"title":"AdaSpec: Adaptive Speculative Decoding for Fast, SLO-Aware Large Language Model Serving"},{"paperId":"d627537b4abc4aa69d0200d38915c9f438e72c02","externalIds":{"ArXiv":"2502.02789","DBLP":"journals/corr/abs-2502-02789","DOI":"10.48550/arXiv.2502.02789","CorpusId":276116935},"title":"Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation"},{"paperId":"47a92bbf83058bd4eb562ce469268559a74cc46e","externalIds":{"DBLP":"conf/icml/JiangFYHMK0YY25","ArXiv":"2502.00722","DOI":"10.48550/arXiv.2502.00722","CorpusId":276094253},"title":"Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs"},{"paperId":"2eed1fad9bbf887d4395de40f20144c4fafefd7f","externalIds":{"PubMedCentral":"12443585","DBLP":"journals/nature/GuoYZSWZXZMBZY025","ArXiv":"2501.12948","DOI":"10.1038/s41586-025-09422-z","CorpusId":275789950,"PubMed":"40962978"},"title":"DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"},{"paperId":"7d30bdf6b895144cf2eda29dbe674cdf84595240","externalIds":{"DBLP":"journals/corr/abs-2412-12488","ArXiv":"2412.12488","DOI":"10.48550/arXiv.2412.12488","CorpusId":274788484},"title":"A System for Microserving of LLMs"},{"paperId":"165e1b5ec12ab524641298e5f536e79fdbb552b7","externalIds":{"DBLP":"journals/corr/abs-2412-05496","ArXiv":"2412.05496","DOI":"10.48550/arXiv.2412.05496","CorpusId":274598006},"title":"Flex Attention: A Programming Model for Generating Optimized Attention Kernels"},{"paperId":"b5102a8f01ab3a5540cc1123bf30faa50870df46","externalIds":{"DBLP":"conf/mlsys/0003WJKZDWN25","ArXiv":"2411.19379","DOI":"10.48550/arXiv.2411.19379","CorpusId":274367849},"title":"Marconi: Prefix Caching for the Era of Hybrid LLMs"},{"paperId":"cebc329fa381a1f5055ec5dfa6352b7b30689430","externalIds":{"DBLP":"journals/corr/abs-2411-03519","ArXiv":"2411.03519","DOI":"10.48550/arXiv.2411.03519","CorpusId":273850266},"title":"AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution"},{"paperId":"c738c5630f454d1722f936cdaeec053744695c96","externalIds":{"DBLP":"journals/corr/abs-2411-01783","ArXiv":"2411.01783","DOI":"10.48550/arXiv.2411.01783","CorpusId":273812148},"title":"Context Parallelism for Scalable Million-Token Inference"},{"paperId":"757f47a920ea54ef7c88e3225263b1f80cdf0446","externalIds":{"ArXiv":"2411.01438","DBLP":"conf/eurosys/MaoXWCGBYSS25","DOI":"10.1145/3689031.3717459","CorpusId":273812235},"title":"SkyServe: Serving AI Models across Regions and Clouds with Spot Instances"},{"paperId":"c6ae89489e2846913d07c9dc85fb8a2a3c358a69","externalIds":{"DBLP":"journals/corr/abs-2411-00136","ArXiv":"2411.00136","DOI":"10.1109/SCW63240.2024.00178","CorpusId":273798387},"title":"LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators"},{"paperId":"6027d9b7f875a42a36029e2d3b7308be6fa88bb1","externalIds":{"DBLP":"conf/iclr/ChenSYZZNTDBJC25","ArXiv":"2410.16179","DOI":"10.48550/arXiv.2410.16179","CorpusId":273507777},"title":"MagicPIG: LSH Sampling for Efficient LLM Generation"},{"paperId":"205af0351e2ec16720201d43eabefb79735e3ed6","externalIds":{"DBLP":"conf/eurosys/GaoCS25","ArXiv":"2410.05004","DOI":"10.1145/3689031.3696072","CorpusId":273186152},"title":"Fast State Restoration in LLM Serving with HCache"},{"paperId":"b41ae48f3389d7aa1f7ef39b823ea587b3232c50","externalIds":{"DBLP":"conf/iclr/Jang0YJY0KY25","ArXiv":"2410.03355","DOI":"10.48550/arXiv.2410.03355","CorpusId":273162636},"title":"LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding"},{"paperId":"f2d0f3d47ae850f49a58f4977393bd0025af4bec","externalIds":{"ArXiv":"2409.19256","DBLP":"conf/eurosys/ShengZYWZZPL025","DOI":"10.1145/3689031.3696075","CorpusId":272987758},"title":"HybridFlow: A Flexible and Efficient RLHF Framework"},{"paperId":"0adf7d0d104f59189ee082442d595c4eaa094904","externalIds":{"DBLP":"conf/naacl/ChristopherBBCKF25","ArXiv":"2408.05636","DOI":"10.48550/arXiv.2408.05636","CorpusId":271855366},"title":"Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion"},{"paperId":"022f386eb66fc5532dd6f439e7a356fd33ebb9a2","externalIds":{"DBLP":"journals/pacmmod/ZhangJCFMNCC25","ArXiv":"2407.12820","DOI":"10.1145/3725338","CorpusId":271270110},"title":"PQCache: Product Quantization-based KVCache for Long Context LLM Inference"},{"paperId":"964440957c030504f6bcab11f514635ece1bf6b2","externalIds":{"DBLP":"conf/osdi/LeeLSS24","ArXiv":"2406.19707","DOI":"10.48550/arXiv.2406.19707","CorpusId":270845627},"title":"InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management"},{"paperId":"9b1ef16bb9c7a8e8ce952e724cdaa0478f4b5681","externalIds":{"DBLP":"journals/corr/abs-2406-17145","ArXiv":"2406.17145","DOI":"10.1145/3669940.3707220","CorpusId":270711123},"title":"GraphPipe: Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism"},{"paperId":"f3d401f01aa5cb2eb3974196efda8895d06610c7","externalIds":{"ArXiv":"2407.00079","DBLP":"journals/corr/abs-2407-00079","DOI":"10.48550/arXiv.2407.00079","CorpusId":270869762},"title":"Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"},{"paperId":"af35e5cd1b22ae7d3c5f8a95c3d5ebc308fabe72","externalIds":{"DBLP":"conf/osdi/SunHZXZL024","ArXiv":"2406.03243","DOI":"10.48550/arXiv.2406.03243","CorpusId":270258417},"title":"Llumnix: Dynamic Scheduling for Large Language Model Serving"},{"paperId":"6a111d13b9c272459fb3d7628a7b0307c8ebd0f7","externalIds":{"ArXiv":"2406.01566","DBLP":"conf/asplos/Mei0MYJV25","DOI":"10.1145/3669940.3707215","CorpusId":270217361},"title":"Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow"},{"paperId":"6fc96ad9fd3613de70a7a877fe9a9205c5af1bdf","externalIds":{"ArXiv":"2405.19888","DBLP":"journals/corr/abs-2405-19888","DOI":"10.48550/arXiv.2405.19888","CorpusId":270123549},"title":"Parrot: Efficient Serving of LLM-based Applications with Semantic Variable"},{"paperId":"7be8750e3b3d1d004b9dd91a11e0c508495969a2","externalIds":{"ArXiv":"2405.05751","DBLP":"conf/osdi/WuCLSJAVMPJ25","CorpusId":274981984},"title":"Mirage: A Multi-Level Superoptimizer for Tensor Programs"},{"paperId":"e03827ea5638ae9f9f987b73b2017fd115f9f79b","externalIds":{"DBLP":"conf/asplos/PrabhuNMRP25","ArXiv":"2405.04437","DOI":"10.1145/3669940.3707256","CorpusId":269614548},"title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention"},{"paperId":"53a803388e83ae89261624099d7be4287ace67cb","externalIds":{"ArXiv":"2405.04434","DBLP":"journals/corr/abs-2405-04434","DOI":"10.48550/arXiv.2405.04434","CorpusId":269613809},"title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"paperId":"b397af271897328a3111ea406c63553fb963e883","externalIds":{"ArXiv":"2404.16283","DBLP":"journals/corr/abs-2404-16283","DOI":"10.48550/arXiv.2404.16283","CorpusId":269362094},"title":"Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services"},{"paperId":"d0deaec3e1f74701ac43600d9e64c5c969be7391","externalIds":{"ArXiv":"2404.11912","DBLP":"journals/corr/abs-2404-11912","DOI":"10.48550/arXiv.2404.11912","CorpusId":269214125},"title":"TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding"},{"paperId":"eb06e95dd3eb5a916e52d2e463f474ef4967d8ca","externalIds":{"DBLP":"conf/sosp/WuLZ0L024","ArXiv":"2404.09526","DOI":"10.1145/3694715.3695948","CorpusId":269149599},"title":"LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism"},{"paperId":"dee2bf14c31f876e7cf2c43b816dacbac666268f","externalIds":{"ArXiv":"2403.19708","DBLP":"conf/usenix/GaoHSKJDYYZ24","CorpusId":268793498},"title":"Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention"},{"paperId":"d2421cffac277e230cb97fc2355b32e03dd8bb1f","externalIds":{"DBLP":"journals/corr/abs-2404-07947","ArXiv":"2404.07947","DOI":"10.1145/3620665.3640383","CorpusId":269042826},"title":"ExeGPT: Constraint-Aware Resource Scheduling for LLM Inference"},{"paperId":"20f090e35ad598fba2404e550c2462dc9da03a10","externalIds":{"DBLP":"journals/corr/abs-2403-02310","ArXiv":"2403.02310","DOI":"10.48550/arXiv.2403.02310","CorpusId":268249103},"title":"Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve"},{"paperId":"4623901ec1f8311b32e27bd627b2bce56161c6bb","externalIds":{"DBLP":"conf/emnlp/ZhaoH0XXZFZL024","ACL":"2024.emnlp-main.742","ArXiv":"2402.13720","DOI":"10.18653/v1/2024.emnlp-main.742","CorpusId":270738205},"title":"Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding"},{"paperId":"d0b049018c9e21b7b95c179d33e1e2ac9113c85b","externalIds":{"DBLP":"journals/corr/abs-2402-05109","ArXiv":"2402.05109","DOI":"10.48550/arXiv.2402.05109","CorpusId":267523522},"title":"Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding"},{"paperId":"f1a9e0830bc36c048fa4659beaa62609869895b5","externalIds":{"DBLP":"conf/icml/FuBS024","ArXiv":"2402.02057","DOI":"10.48550/arXiv.2402.02057","CorpusId":267412730},"title":"Break the Sequential Dependency of LLM Inference Using Lookahead Decoding"},{"paperId":"90d5a2cb82d277586704bba3c843511e9547cf1b","externalIds":{"DBLP":"conf/kdd/0003CLKFZZT0GW025","ArXiv":"2401.17644","DOI":"10.1145/3711896.3737413","CorpusId":267334913},"title":"BurstGPT: A Real-World Workload Dataset to Optimize LLM Serving Systems"},{"paperId":"1b5db3170c195508ff24fee8eda0d4987e806f0b","externalIds":{"DBLP":"journals/corr/abs-2401-15077","ArXiv":"2401.15077","DOI":"10.48550/arXiv.2401.15077","CorpusId":267301131},"title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"},{"paperId":"21e53e51ff77a5f34f43cb8ca029909c3ad9f71e","externalIds":{"ArXiv":"2401.11181","DBLP":"journals/corr/abs-2401-11181","DOI":"10.48550/arXiv.2401.11181","CorpusId":267068930},"title":"Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads"},{"paperId":"72f77a393079431e4207b3afe678ee80b420e6f8","externalIds":{"DBLP":"conf/osdi/ZhongLCHZL0024","ArXiv":"2401.09670","DOI":"10.48550/arXiv.2401.09670","CorpusId":267034664},"title":"DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"},{"paperId":"81711474e3f37a72d90c42dd10f46e431330f890","externalIds":{"DBLP":"journals/corr/abs-2312-12682","ArXiv":"2312.12682","DOI":"10.48550/arXiv.2312.12682","CorpusId":266375109},"title":"Mini-GPTs: Efficient Large Language Models through Contextual Pruning"},{"paperId":"d7eb6ff1007fbdad402719eba8494283ec575016","externalIds":{"DBLP":"journals/corr/abs-2312-13211","ArXiv":"2312.13211","DOI":"10.48550/arXiv.2312.13211","CorpusId":266375072},"title":"DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization"},{"paperId":"ddacee7382548fd9976e846c92500cfa3b6741db","externalIds":{"DBLP":"conf/sosp/SongMX024","ArXiv":"2312.12456","DOI":"10.1145/3694715.3695964","CorpusId":266375171},"title":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"},{"paperId":"a3dd3adc37cdb8991936f3feb109c20b6f892f3d","externalIds":{"DBLP":"conf/acl/FeiNZH0D024","ArXiv":"2312.09571","DOI":"10.48550/arXiv.2312.09571","CorpusId":266335580},"title":"Extending Context Window of Large Language Models via Semantic Compression"},{"paperId":"f56fd8eee26d28111ba0e8dd812ee5fc813f666f","externalIds":{"ArXiv":"2312.07987","DBLP":"conf/nips/CsordasPIS24","DOI":"10.48550/arXiv.2312.07987","CorpusId":266191825},"title":"SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention"},{"paperId":"2b05686607991a39aead43f371fd7ea2b08195f5","externalIds":{"DBLP":"journals/corr/abs-2312-08361","ArXiv":"2312.08361","DOI":"10.48550/arXiv.2312.08361","CorpusId":266191568},"title":"Distributed Inference and Fine-tuning of Large Language Models Over The Internet"},{"paperId":"00e18c603e60d861c4e99c541e4d65ef442d5945","externalIds":{"DBLP":"conf/acl/AlizadehMBKCMRF24","ArXiv":"2312.11514","DOI":"10.48550/arXiv.2312.11514","CorpusId":266362016},"title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory"},{"paperId":"9f5b740169b1557852e945ba745a7a2c05c56dcc","externalIds":{"DBLP":"conf/eurosys/YuL025","ArXiv":"2312.05516","DOI":"10.1145/3689031.3696086","CorpusId":266163159},"title":"Stateful Large Language Model Serving with Pensieve"},{"paperId":"7bbc7595196a0606a07506c4fb1473e5e87f6082","externalIds":{"ArXiv":"2312.00752","DBLP":"journals/corr/abs-2312-00752","CorpusId":265551773},"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"},{"paperId":"ad9146d98ae95bbeeef460abe083ecc2c4798672","externalIds":{"DBLP":"journals/corr/abs-2311-18677","ArXiv":"2311.18677","DOI":"10.1109/ISCA59077.2024.00019","CorpusId":265506047},"title":"Splitwise: Efficient Generative LLM Inference Using Phase Splitting"},{"paperId":"929ed6412136fe42e6ef1eeb7ea0b4da693dee37","externalIds":{"DBLP":"journals/corr/abs-2311-15566","ArXiv":"2311.15566","DOI":"10.1145/3620665.3640411","CorpusId":265456256},"title":"SpotServe: Serving Generative Large Language Models on Preemptible Instances"},{"paperId":"7a2cc2fdb0f0df3752bb4668224489535bdd9a06","externalIds":{"ArXiv":"2311.15436","DBLP":"journals/corr/abs-2311-15436","DOI":"10.48550/arXiv.2311.15436","CorpusId":265456419},"title":"Learning to Skip for Language Modeling"},{"paperId":"05e0fac90e8134707d2241214c5a5a8bb9a5440f","externalIds":{"DBLP":"conf/aaai/DordevicBTCS24","ArXiv":"2311.10642","DOI":"10.48550/arXiv.2311.10642","CorpusId":265281402},"title":"Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers"},{"paperId":"678aa50088b78fb4fd4d17de89fc472ef62b272a","externalIds":{"DBLP":"journals/corr/abs-2311-09550","ArXiv":"2311.09550","DOI":"10.48550/arXiv.2311.09550","CorpusId":265221029},"title":"A Speed Odyssey for Deployable Quantization of LLMs"},{"paperId":"c4ae033f14a89db8b6b6b4da598375f177a1fac4","externalIds":{"ArXiv":"2311.10768","DBLP":"conf/naacl/SantosLNCU24","ACL":"2024.naacl-long.249","DOI":"10.48550/arXiv.2311.10768","CorpusId":265295488},"title":"Memory Augmented Language Models through Mixture of Word Experts"},{"paperId":"ade22704be8a0fc3730d320cc7934b2ccbcd97e4","externalIds":{"ArXiv":"2311.09431","DBLP":"journals/corr/abs-2311-09431","DOI":"10.48550/arXiv.2311.09431","CorpusId":265220849},"title":"Striped Attention: Faster Ring Attention for Causal Transformers"},{"paperId":"bc5c73c101da795cfa44e4ac7751cdedca9b6d93","externalIds":{"ArXiv":"2311.04934","DBLP":"journals/corr/abs-2311-04934","DOI":"10.48550/arXiv.2311.04934","CorpusId":265067391},"title":"Prompt Cache: Modular Attention Reuse for Low-Latency Inference"},{"paperId":"34cd58ce4da403f75a2b9107d649cf117c9a64ee","externalIds":{"ArXiv":"2311.03687","DBLP":"journals/corr/abs-2311-03687","DOI":"10.48550/arXiv.2311.03687","CorpusId":265043165},"title":"Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models"},{"paperId":"faa4c46e1cbd99e486c7dc2881e024b79967961b","externalIds":{"DBLP":"journals/corr/abs-2311-03285","ArXiv":"2311.03285","DOI":"10.48550/arXiv.2311.03285","CorpusId":265033787},"title":"S-LoRA: Serving Thousands of Concurrent LoRA Adapters"},{"paperId":"4d76206515d6b33903937474273885476fc2771e","externalIds":{"DBLP":"journals/corr/abs-2311-01282","ArXiv":"2311.01282","DOI":"10.48550/arXiv.2311.01282","CorpusId":264935058},"title":"FlashDecoding++: Faster Large Language Model Inference on GPUs"},{"paperId":"40e8037ea711c90f43aa24ea089719fc35df578b","externalIds":{"DBLP":"conf/asplos/LaiSFLHL0JJ0JCJ25","ArXiv":"2311.02103","DOI":"10.1145/3676641.3716249","CorpusId":265033332},"title":"Relax: Composable Abstractions for End-to-End Dynamic Machine Learning"},{"paperId":"9529e50807f36acf3d2e4af994b5803c47e4746a","externalIds":{"DBLP":"conf/mlsys/ZhaoLZYC0CK0K24","ArXiv":"2310.19102","DOI":"10.48550/arXiv.2310.19102","CorpusId":264828796},"title":"Atom: Low-bit Quantization for Efficient and Accurate LLM Serving"},{"paperId":"c2314751d367b34239a537fe27e2bd51a8b84528","externalIds":{"DBLP":"conf/mlsys/Chen0WZCK24","ArXiv":"2310.18547","DOI":"10.48550/arXiv.2310.18547","CorpusId":264590197},"title":"Punica: Multi-Tenant LoRA Serving"},{"paperId":"95240dda409e28acccdc5cf619ad0c036cf4292d","externalIds":{"DBLP":"conf/icml/LiuWDZY0S0TRC23","ArXiv":"2310.17157","DOI":"10.48550/arXiv.2310.17157","CorpusId":260815690},"title":"Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"},{"paperId":"92aed22dcf8626f79d9aa3a5a23d28b799ca341e","externalIds":{"DBLP":"conf/sosp/NgD023","DOI":"10.1145/3600006.3613163","CorpusId":263609593},"title":"Paella: Low-latency Model Serving with Software-defined GPU Scheduling"},{"paperId":"a27dced654158b905c7447aae1aa294ebc8ecaf0","externalIds":{"DBLP":"journals/corr/abs-2310-12442","ArXiv":"2310.12442","DOI":"10.48550/arXiv.2310.12442","CorpusId":264306349},"title":"Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer"},{"paperId":"f206d34afed6a5705757f96ea97c7bfb9e1a83cd","externalIds":{"ArXiv":"2310.12072","DBLP":"journals/corr/abs-2310-12072","DOI":"10.48550/arXiv.2310.12072","CorpusId":264289340},"title":"SPEED: Speculative Pipelined Execution for Efficient Decoding"},{"paperId":"908dad62c0e43d80e3e3cb3c0402f7c71c70499c","externalIds":{"ArXiv":"2310.08560","DBLP":"journals/corr/abs-2310-08560","DOI":"10.48550/arXiv.2310.08560","CorpusId":263909014},"title":"MemGPT: Towards LLMs as Operating Systems"},{"paperId":"4c0428917aeee6aa7bd434f337d039f35996b736","externalIds":{"DBLP":"journals/corr/abs-2310-06839","ArXiv":"2310.06839","DOI":"10.48550/arXiv.2310.06839","CorpusId":263830692},"title":"LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"},{"paperId":"db633c6b1c286c0386f0078d8a2e6224e03a6227","externalIds":{"ArXiv":"2310.06825","DBLP":"journals/corr/abs-2310-06825","DOI":"10.48550/arXiv.2310.06825","CorpusId":263830494},"title":"Mistral 7B"},{"paperId":"564855d475ed9197dd7516594557ff886ff623e5","externalIds":{"ArXiv":"2310.05424","DBLP":"journals/corr/abs-2310-05424","DOI":"10.48550/arXiv.2310.05424","CorpusId":263830054},"title":"Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"},{"paperId":"a941874be310395c5778d4f1710ad0635795ad6b","externalIds":{"DBLP":"journals/corr/abs-2310-04607","ArXiv":"2310.04607","DOI":"10.48550/arXiv.2310.04607","CorpusId":263830290},"title":"A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators"},{"paperId":"00cccb9065f0a59e845d5b4d360ce31cf25036be","externalIds":{"DBLP":"journals/corr/abs-2310-03094","ArXiv":"2310.03094","DOI":"10.48550/arXiv.2310.03094","CorpusId":263671564},"title":"Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning"},{"paperId":"02ad9f3fefe33cb9ca546591bec65dbdf7766c80","externalIds":{"ArXiv":"2310.01889","DBLP":"conf/iclr/0055ZA24","DOI":"10.48550/arXiv.2310.01889","CorpusId":263608461},"title":"Ring Attention with Blockwise Transformers for Near-Infinite Context"},{"paperId":"6c323c535365e1c7cbfd9703cbec3b5650a3346b","externalIds":{"DBLP":"conf/iclr/Ge0LZ0024","ArXiv":"2310.01801","DOI":"10.48550/arXiv.2310.01801","CorpusId":263609075},"title":"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"},{"paperId":"4e13ecf80443a4135d516b7ba77eca82b5c6d347","externalIds":{"DBLP":"journals/corr/abs-2310-01382","ArXiv":"2310.01382","DOI":"10.48550/arXiv.2310.01382","CorpusId":263605754},"title":"Compressing LLMs: The Truth is Rarely Pure and Never Simple"},{"paperId":"fdc53c2c10742464087c0525f77e32604827a21d","externalIds":{"DBLP":"conf/iclr/XiaoTCHL24","ArXiv":"2309.17453","DOI":"10.48550/arXiv.2309.17453","CorpusId":263310483},"title":"Efficient Streaming Language Models with Attention Sinks"},{"paperId":"a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9","externalIds":{"ArXiv":"2309.14509","DBLP":"journals/corr/abs-2309-14509","DOI":"10.48550/arXiv.2309.14509","CorpusId":262826014},"title":"DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"},{"paperId":"c96297261467b5daa2d01227496a70d444602434","externalIds":{"DBLP":"journals/corr/abs-2309-10305","ArXiv":"2309.10305","DOI":"10.48550/arXiv.2309.10305","CorpusId":261951743},"title":"Baichuan 2: Open Large-scale Language Models"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"00e889fcfaf4396a20f37f681cf8b14f3e878879","externalIds":{"ArXiv":"2309.04255","DBLP":"journals/corr/abs-2309-04255","DOI":"10.48550/arXiv.2309.04255","CorpusId":261660737},"title":"LLMCad: Fast and Scalable On-device Large Language Model Inference"},{"paperId":"660380b17d3a37d8132f2e6dcb5cb47092e5b7d1","externalIds":{"ArXiv":"2309.01172","DBLP":"journals/corr/abs-2309-01172","DOI":"10.48550/arXiv.2309.01172","CorpusId":261530813},"title":"FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs"},{"paperId":"a9caf21a845cb0b1b1d453c052188de118006093","externalIds":{"DBLP":"journals/corr/abs-2308-16369","ArXiv":"2308.16369","DOI":"10.48550/arXiv.2308.16369","CorpusId":261395577},"title":"SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"},{"paperId":"b31a5884a8ebe96b6300839b28608b97f8f8ef76","externalIds":{"DBLP":"journals/corr/abs-2308-14508","ArXiv":"2308.14508","DOI":"10.48550/arXiv.2308.14508","CorpusId":261245264},"title":"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"},{"paperId":"7d95aae54152e157c35dfc0de01caa9ce4b85440","externalIds":{"DBLP":"journals/taco/DuJZZHL23","DOI":"10.1145/3617689","CorpusId":261125712},"title":"Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs"},{"paperId":"c60116a51bf66bc363d11b797d97eba84b13cfd7","externalIds":{"DBLP":"journals/corr/abs-2308-04945","ACL":"2024.eacl-demo.23","ArXiv":"2308.04945","DOI":"10.48550/arXiv.2308.04945","CorpusId":260735890},"title":"LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking"},{"paperId":"43e624ddeed82df944a6cae0dedec3372438e243","externalIds":{"DBLP":"journals/corr/abs-2308-04623","ArXiv":"2308.04623","DOI":"10.48550/arXiv.2308.04623","CorpusId":260735640},"title":"Accelerating LLM Inference with Staged Speculative Decoding"},{"paperId":"02d4096c030d052e1866d52fbc3b83480e1ed9f5","externalIds":{"DBLP":"conf/kdd/DongMXM023","DOI":"10.1145/3580305.3599572","CorpusId":260499677},"title":"Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"240103933ffe3dac2179cc160a2bd91299357a53","externalIds":{"DBLP":"journals/corr/abs-2307-08621","ArXiv":"2307.08621","CorpusId":259937453},"title":"Retentive Network: A Successor to Transformer for Large Language Models"},{"paperId":"a72975eb88eb31f193e9587e7415cb04e7bcdbee","externalIds":{"ACL":"2024.eacl-long.4","DBLP":"conf/eacl/MuhlgayRMLRBALSS24","ArXiv":"2307.06908","DOI":"10.48550/arXiv.2307.06908","CorpusId":259847758},"title":"Generating Benchmarks for Factuality Evaluation of Language Models"},{"paperId":"d988eb48e8b4e471f5df9d081bfc32db0781e6bf","externalIds":{"DBLP":"journals/tmlr/YangLCP024","ArXiv":"2307.05908","DOI":"10.48550/arXiv.2307.05908","CorpusId":259837553},"title":"Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding"},{"paperId":"1733eb7792f7a43dd21f51f4d1017a1bffd217b5","externalIds":{"DBLP":"journals/tacl/LiuLHPBPL24","ArXiv":"2307.03172","ACL":"2024.tacl-1.9","DOI":"10.1162/tacl_a_00638","CorpusId":259360665},"title":"Lost in the Middle: How Language Models Use Long Contexts"},{"paperId":"a595271dd931673f06b184b68c5bf73830d9dc91","externalIds":{"ArXiv":"2307.02666","DBLP":"journals/corr/abs-2307-02666","DOI":"10.48550/arXiv.2307.02666","CorpusId":259360916},"title":"Chiplet Cloud: Building AI Supercomputers for Serving Large Generative Language Models"},{"paperId":"c12db2c60e8989f646a29ad4f4d24475e860ad91","externalIds":{"ArXiv":"2307.02486","DBLP":"journals/corr/abs-2307-02486","CorpusId":259341682},"title":"LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"},{"paperId":"ce9435c82dc9b576f2037aa2f4357a520be9b2aa","externalIds":{"DBLP":"journals/corr/abs-2307-02628","ArXiv":"2307.02628","DOI":"10.48550/arXiv.2307.02628","CorpusId":259360560},"title":"SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference"},{"paperId":"f5afaccfe90268485a9961c5771ec5e71e9b806c","externalIds":{"ArXiv":"2306.15595","DBLP":"journals/corr/abs-2306-15595","DOI":"10.48550/arXiv.2306.15595","CorpusId":259262376},"title":"Extending Context Window of Large Language Models via Positional Interpolation"},{"paperId":"bc8428e270a5474cabfaff578d44955f757ccacd","externalIds":{"ArXiv":"2306.11222","DBLP":"journals/corr/abs-2306-11222","DOI":"10.48550/arXiv.2306.11222","CorpusId":259203385},"title":"LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation"},{"paperId":"6796b7db48bc8d617e71d5e3a1c9bce763f260dc","externalIds":{"DBLP":"conf/ijcnn/YemmeG23","DOI":"10.1109/IJCNN54540.2023.10191067","CorpusId":260388254},"title":"A Scalable GPT-2 Inference Hardware Architecture on FPGA"},{"paperId":"9afa0c3227fd0ec3a76928784e59c4205cbace24","externalIds":{"DBLP":"journals/tmlr/TornedeDEGMRSTT24","ArXiv":"2306.08107","DOI":"10.48550/arXiv.2306.08107","CorpusId":259164600},"title":"AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks"},{"paperId":"0423fc7bc1880b850d07aec8ebd9217a70626572","externalIds":{"ArXiv":"2306.06000","DBLP":"conf/nips/JinW0W23","DOI":"10.48550/arXiv.2306.06000","CorpusId":259129329},"title":"S3: Increasing GPU Utilization during Generative Inference for Higher Throughput"},{"paperId":"51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df","externalIds":{"DBLP":"journals/corr/abs-2306-03078","ArXiv":"2306.03078","DOI":"10.48550/arXiv.2306.03078","CorpusId":259076379},"title":"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"},{"paperId":"983f6e8e64e6838aa39336c62730b4a7ad445c35","externalIds":{"DBLP":"journals/corr/abs-2306-02003","ArXiv":"2306.02003","DOI":"10.48550/arXiv.2306.02003","CorpusId":259075212},"title":"On Optimal Caching and Model Multiplexing for Large Model Inference"},{"paperId":"db9507cdd3e2d7d9c90ed185bd831e55c62dcec9","externalIds":{"DBLP":"journals/sigmobile/LinTTYXH24","ArXiv":"2306.00978","DOI":"10.1145/3714983.3714987","CorpusId":258999941},"title":"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"},{"paperId":"d203c764fb5dec2b053be667c8b06e516ea6ef10","externalIds":{"DBLP":"journals/corr/abs-2306-01160","ArXiv":"2306.01160","DOI":"10.48550/arXiv.2306.01160","CorpusId":259063695},"title":"Faster Causal Attention Over Large Sequences Through Sparse Flash Attention"},{"paperId":"d6eeb2898bd9bd34744194ef543062dda6c4531a","externalIds":{"DBLP":"conf/nips/LiuDLWXXKS23","ArXiv":"2305.17118","DOI":"10.48550/arXiv.2305.17118","CorpusId":258947558},"title":"Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"},{"paperId":"60b35c6d68acced19b0c66edcfc0ee0a2c11efed","externalIds":{"DBLP":"journals/corr/abs-2305-16300","ArXiv":"2305.16300","DOI":"10.48550/arXiv.2305.16300","CorpusId":258887482},"title":"Landmark Attention: Random-Access Infinite Context Length for Transformers"},{"paperId":"c193eb176985a81ae64f63c5e50b2f11cfb7c4e6","externalIds":{"DBLP":"journals/corr/abs-2305-15805","ArXiv":"2305.15805","DOI":"10.48550/arXiv.2305.15805","CorpusId":258888224},"title":"Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"},{"paperId":"2f7364d8e5cf94315bf8905f57de9c5543e9a4bf","externalIds":{"DBLP":"journals/corr/abs-2305-14788","ArXiv":"2305.14788","DOI":"10.48550/arXiv.2305.14788","CorpusId":258865249},"title":"Adapting Language Models to Compress Contexts"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"a10843d1349fff8d2a7d9722f800802187fef67f","externalIds":{"DBLP":"conf/nips/KimLKPYKL23","ArXiv":"2305.14152","DOI":"10.48550/arXiv.2305.14152","CorpusId":258841104},"title":"Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"},{"paperId":"026b3396a63ed5772329708b7580d633bb86bec9","externalIds":{"DBLP":"conf/emnlp/PengAAAABCCCDDG23","ArXiv":"2305.13048","DOI":"10.18653/v1/2023.findings-emnlp.936","CorpusId":258832459},"title":"RWKV: Reinventing RNNs for the Transformer Era"},{"paperId":"5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200","externalIds":{"DBLP":"journals/corr/abs-2305-13245","ArXiv":"2305.13245","DOI":"10.48550/arXiv.2305.13245","CorpusId":258833177},"title":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"},{"paperId":"3556722b4703a21abafd2f9388743202943f4503","externalIds":{"ACL":"2023.acl-long.689","DBLP":"journals/corr/abs-2305-10427","ArXiv":"2305.10427","DOI":"10.18653/v1/2023.acl-long.689","CorpusId":258741236},"title":"Accelerating Transformer Inference for Translation via Parallel Decoding"},{"paperId":"5f187af087ebbaf1ce4bca686a4b1c2afee92b6d","externalIds":{"DBLP":"journals/corr/abs-2305-11186","ArXiv":"2305.11186","DOI":"10.48550/arXiv.2305.11186","CorpusId":258823240},"title":"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"},{"paperId":"f0c31511134abdd23f990310e8a2f2eb3a629b62","externalIds":{"ArXiv":"2305.09781","DBLP":"conf/asplos/MiaoOZCWZWZYSSC24","DOI":"10.1145/3620666.3651335","CorpusId":267094734},"title":"SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification"},{"paperId":"585f8b9725f5f5e5495c3508d39f70d1c053e190","externalIds":{"DBLP":"journals/tmlr/ChenZ024","ArXiv":"2305.05176","DOI":"10.48550/arXiv.2305.05176","CorpusId":258564349},"title":"FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance"},{"paperId":"b9870e130f61ff900fe00dbcc5782c9b31773d32","externalIds":{"DBLP":"journals/corr/abs-2304-08467","ArXiv":"2304.08467","DOI":"10.48550/arXiv.2304.08467","CorpusId":258179012},"title":"Learning to Compress Prompts with Gist Tokens"},{"paperId":"dbbc5003af690799fa4fe6330fb795311cde106f","externalIds":{"DBLP":"journals/pacmmod/NieMWYXMC023","ArXiv":"2304.03946","DOI":"10.1145/3588964","CorpusId":258048524},"title":"FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"},{"paperId":"ece77610adfb0fb162dd22ef694f2777393c319a","externalIds":{"DBLP":"journals/corr/abs-2304-03208","ArXiv":"2304.03208","DOI":"10.48550/arXiv.2304.03208","CorpusId":257985427},"title":"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster"},{"paperId":"7c25adf2ddb35df05a61c697da97efb8583d77df","externalIds":{"DBLP":"conf/isca/JouppiK0MNNPSST23","ArXiv":"2304.01433","DOI":"10.1145/3579371.3589350","CorpusId":257921908},"title":"TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"42a14d824caa3348046eb34c37e2ab7985faa7a3","externalIds":{"ArXiv":"2303.06865","DBLP":"journals/corr/abs-2303-06865","DOI":"10.48550/arXiv.2303.06865","CorpusId":257495837},"title":"High-throughput Generative Inference of Large Language Models with a Single GPU"},{"paperId":"f393aff1593c2d370ec0ae004910d18e40524967","externalIds":{"ArXiv":"2303.06349","DBLP":"journals/corr/abs-2303-06349","CorpusId":257496654},"title":"Resurrecting Recurrent Neural Networks for Long Sequences"},{"paperId":"174ae9800f6359520d900d19890acfcf46709107","externalIds":{"ArXiv":"2303.06182","DBLP":"journals/corr/abs-2303-06182","DOI":"10.48550/arXiv.2303.06182","CorpusId":257496628},"title":"Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference"},{"paperId":"0a6906bd6f026d3da3031c641ed03081bd0b574e","externalIds":{"ArXiv":"2302.14017","DBLP":"journals/corr/abs-2302-14017","DOI":"10.48550/arXiv.2302.14017","CorpusId":257219934},"title":"Full Stack Optimization of Transformer Inference: a Survey"},{"paperId":"7ba5c3953550b2a4c3abe1dcfc031a9674e64465","externalIds":{"DBLP":"conf/cgo/Wu23","DOI":"10.1145/3579990.3583093","CorpusId":257051536},"title":"PyTorch 2.0: The Journey to Bringing Compiler Technologies to the Core of PyTorch (Keynote)"},{"paperId":"ae3ac5509c445327a23431409624a1333aa825b0","externalIds":{"DBLP":"journals/corr/abs-2302-03773","ArXiv":"2302.03773","DOI":"10.48550/arXiv.2302.03773","CorpusId":256662734},"title":"What Matters In The Structured Pruning of Generative Language Models?"},{"paperId":"a1f8082505c7e90b0a033e1b9da0a97d67aad66c","externalIds":{"DBLP":"journals/corr/abs-2302-01318","ArXiv":"2302.01318","DOI":"10.48550/arXiv.2302.01318","CorpusId":256503945},"title":"Accelerating Large Language Model Decoding with Speculative Sampling"},{"paperId":"32fbc1821ef4a6daee1f9e9f140056ff9cda238a","externalIds":{"DBLP":"conf/sosp/ZhengJZHM0YZQYZ23","ArXiv":"2301.10936","DOI":"10.1145/3600006.3613139","CorpusId":256274793},"title":"PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation"},{"paperId":"909ad57ce8caa6b390a65ae09db352d27d8f3996","externalIds":{"DBLP":"journals/corr/abs-2301-00774","ArXiv":"2301.00774","DOI":"10.48550/arXiv.2301.00774","CorpusId":255372747},"title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"},{"paperId":"5a77b508302771fc083bf24e0bcda8553c9b5421","externalIds":{"DBLP":"journals/corr/abs-2212-14052","ArXiv":"2212.14052","DOI":"10.48550/arXiv.2212.14052","CorpusId":255340454},"title":"Hungry Hungry Hippos: Towards Language Modeling with State Space Models"},{"paperId":"53535d38fe259a3aa7c911edd8048d764e09e8e1","externalIds":{"DBLP":"journals/corr/abs-2212-09720","ArXiv":"2212.09720","DOI":"10.48550/arXiv.2212.09720","CorpusId":254853733},"title":"The case for 4-bit precision: k-bit Inference Scaling Laws"},{"paperId":"d8e9f8c8a37cb4cd26b92ad0d942d641cd512644","externalIds":{"DBLP":"journals/corr/abs-2211-17192","ArXiv":"2211.17192","DOI":"10.48550/arXiv.2211.17192","CorpusId":254096365},"title":"Fast Inference from Transformers via Speculative Decoding"},{"paperId":"43014fc85c4860487336579ec98f509fec1803f7","externalIds":{"DBLP":"conf/mlsys/GaleNYZ23","ArXiv":"2211.15841","DOI":"10.48550/arXiv.2211.15841","CorpusId":254069783},"title":"MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","externalIds":{"DBLP":"journals/tmlr/ChenM0C23","ArXiv":"2211.12588","CorpusId":253801709},"title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"17a8bd6a5763f6607863348ce1757ac2ad3417fd","externalIds":{"DBLP":"conf/iiswc/ChoiLKHA22","DOI":"10.1109/IISWC55918.2022.00018","CorpusId":254640088},"title":"Accelerating Transformer Networks through Recomposing Softmax Layers"},{"paperId":"e2df6ae1b3485449364ce2a5356ab09600fc3632","externalIds":{"DBLP":"journals/corr/abs-2211-13878","ArXiv":"2211.13878","DOI":"10.14778/3570690.3570697","CorpusId":254017888},"title":"Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"},{"paperId":"6d088e8d785a57b50c2b0e465e2460e09ced48d7","externalIds":{"DBLP":"conf/usenix/LiJZ0X23","ArXiv":"2210.17223","CorpusId":259859076},"title":"Accelerating Distributed MoE Training and Inference with Lina"},{"paperId":"7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6","externalIds":{"DBLP":"journals/corr/abs-2210-17323","ArXiv":"2210.17323","CorpusId":253237200},"title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"},{"paperId":"22b58dce1a13382418b8372bbd50ed3b2533f899","externalIds":{"DBLP":"journals/corr/abs-2210-03052","ArXiv":"2210.03052","DOI":"10.1109/IPDPS54959.2023.00042","CorpusId":252734710},"title":"ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"},{"paperId":"6c673f92ac808d9ccbaf3fc35e8da9cd66caf847","externalIds":{"ACL":"2023.acl-demo.54","DBLP":"journals/corr/abs-2209-01188","ArXiv":"2209.01188","DOI":"10.48550/arXiv.2209.01188","CorpusId":252070588},"title":"Petals: Collaborative Inference and Fine-tuning of Large Models"},{"paperId":"fd7e88a2313e176315d99fc299277e752d7703b7","externalIds":{"ArXiv":"2209.00099","DBLP":"journals/tacl/TrevisoLJACCHHH23","DOI":"10.1162/tacl_a_00577","CorpusId":251979721},"title":"Efficient Methods for Natural Language Processing: A Survey"},{"paperId":"4be7d1524edb0137599a5cc95f72844b85a52fe1","externalIds":{"DBLP":"journals/corr/abs-2208-07339","ArXiv":"2208.07339","DOI":"10.48550/arXiv.2208.07339","CorpusId":251564521},"title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"},{"paperId":"2f7c95f834b4e41d9f66843c109e1d57dccef49a","externalIds":{"ArXiv":"2207.05022","DBLP":"conf/asplos/GuoCL23","DOI":"10.1145/3575693.3575698","CorpusId":256391165},"title":"STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining"},{"paperId":"c142bfce3b6849ee9d6aa0d5637ab28b170fd507","externalIds":{"ArXiv":"2207.04606","DBLP":"conf/asplos/YeLSCC23","DOI":"10.1145/3582016.3582047","CorpusId":250426494},"title":"SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning"},{"paperId":"6d5c5e0df5dc4bd0bcd524d2a22d65d672bd0b74","externalIds":{"DBLP":"journals/corr/abs-2207-04296","ArXiv":"2207.04296","DOI":"10.1145/3575693.3576933","CorpusId":250425865},"title":"TensorIR: An Abstraction for Automatic Tensorized Program Optimization"},{"paperId":"c022f75b00d795c6297d6a9ea948856ea4d365a1","externalIds":{"DBLP":"conf/sc/AminabadiRALLZRSZRH22","ArXiv":"2207.00032","DOI":"10.1109/SC41404.2022.00051","CorpusId":250243681},"title":"DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"},{"paperId":"eaef083b9d661f42cc0d89d9d8156218f33a91d9","externalIds":{"DBLP":"journals/corr/abs-2206-13947","ArXiv":"2206.13947","DOI":"10.48550/arXiv.2206.13947","CorpusId":250089125},"title":"Long Range Language Modeling via Gated State Spaces"},{"paperId":"5eeb828685e44ca5b8ebafb34a9fa4d51c9186df","externalIds":{"ArXiv":"2206.09557","DBLP":"conf/iclr/ParkPKLKKKKLL24","CorpusId":258180013},"title":"LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models"},{"paperId":"2e700ff36108119f5ed19a53bd2eaa22b42ec3d8","externalIds":{"ArXiv":"2206.03382","DBLP":"journals/corr/abs-2206-03382","DOI":"10.48550/arXiv.2206.03382","CorpusId":249431713},"title":"Tutel: Adaptive Mixture-of-Experts at Scale"},{"paperId":"e03609f2587f690867e7ea0bedaf0db25282c548","externalIds":{"DBLP":"conf/nips/YaoAZWLH22","ArXiv":"2206.01861","DOI":"10.48550/arXiv.2206.01861","CorpusId":249395624},"title":"ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"},{"paperId":"ae06a01845948c86210f88dfca9abdd02e4506cf","externalIds":{"ArXiv":"2205.10350","DBLP":"journals/corr/abs-2205-10350","DOI":"10.48550/arXiv.2205.10350","CorpusId":248964928},"title":"Lossless Acceleration for Seq2seq Generation with Aggressive Decoding"},{"paperId":"bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96","externalIds":{"ArXiv":"2205.05198","DBLP":"journals/corr/abs-2205-05198","DOI":"10.48550/arXiv.2205.05198","CorpusId":248693351},"title":"Reducing Activation Recomputation in Large Transformer Models"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"67dc33f02c8b5f24cd213b6b5fb5c74cead581aa","externalIds":{"DBLP":"journals/pami/XiaoWGLZQL23","ArXiv":"2204.09269","DOI":"10.1109/TPAMI.2023.3277122","CorpusId":248266379,"PubMed":"37200120"},"title":"A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"f5edad3a50ba8a6329d202f31677d988f1d0cf87","externalIds":{"DBLP":"conf/hpca/ZhouXKR22","DOI":"10.1109/HPCA53966.2022.00082","CorpusId":248865393},"title":"TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer"},{"paperId":"d98bfe7181fa81602e0c414202639fa2ebb81d80","externalIds":{"ACL":"2023.iwslt-1.47","ArXiv":"2203.16266","DBLP":"conf/iwslt/ZhanCCWBG23","DOI":"10.18653/v1/2023.iwslt-1.47","CorpusId":260379236},"title":"DePA: Improving Non-autoregressive Translation with Dependency-Aware Decoder"},{"paperId":"72600c2aee2ef4e3c229b5e72512fad2b5ad6450","externalIds":{"DBLP":"conf/cc/KatelKB22","DOI":"10.1145/3497776.3517770","CorpusId":247522110},"title":"MLIR-based code generation for GPU tensor cores"},{"paperId":"397f12152afb61196659505d7a9897b8ba079e44","externalIds":{"DBLP":"journals/corr/abs-2203-08991","ArXiv":"2203.08991","ACL":"2022.acl-long.1","DOI":"10.48550/arXiv.2203.08991","CorpusId":247518565},"title":"AdapLeR: Speeding up Inference by Adaptive Length Reduction"},{"paperId":"bbc57e1b3cf90e09b64377f13de455793bc81ad5","externalIds":{"DBLP":"journals/corr/abs-2202-09368","ArXiv":"2202.09368","CorpusId":247011948},"title":"Mixture-of-Experts with Expert Choice Routing"},{"paperId":"7d1e512888a2fa4e838c12a02ae7fce867d322a8","externalIds":{"DBLP":"conf/icml/RajbhandariLYZA22","ArXiv":"2201.05596","CorpusId":245986500},"title":"DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"},{"paperId":"cf4f4b69b76dc58dc8c0d443ab88ceb461eec719","externalIds":{"ArXiv":"2112.14397","CorpusId":252780015},"title":"EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"53c3940f35b8b45d55ed49056282e1961954513d","externalIds":{"ArXiv":"2112.05682","CorpusId":245117555},"title":"Self-attention Does Not Need $O(n^2)$ Memory"},{"paperId":"002c256d30d6be4b23d365a8de8ae0e67e4c9641","externalIds":{"DBLP":"journals/corr/abs-2112-04426","ArXiv":"2112.04426","CorpusId":244954723},"title":"Improving language models by retrieving from trillions of tokens"},{"paperId":"57150ca7d793d6f784cf82da1c349edf7beb6bc2","externalIds":{"DBLP":"conf/cvpr/YuLZSZWFY22","ArXiv":"2111.11418","DOI":"10.1109/CVPR52688.2022.01055","CorpusId":244478080},"title":"MetaFormer is Actually What You Need for Vision"},{"paperId":"b7b3343b45c785ccab1c94beecc28ea91e041685","externalIds":{"DBLP":"conf/sc/ChenHPLG0D021","DOI":"10.1145/3458817.3476138","CorpusId":239037005},"title":"E.T.: Re-Thinking Self-Attention for Transformer Models on GPUs"},{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","externalIds":{"DBLP":"conf/iclr/GuGR22","ArXiv":"2111.00396","CorpusId":240354066},"title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"217913c84a4bdbe5cee3630d70480fda8d44bfb0","externalIds":{"ArXiv":"2111.00230","DBLP":"journals/corr/abs-2111-00230","CorpusId":240354698},"title":"Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning"},{"paperId":"592d8b30525a5ed14d56cb2e72a791f28a0981aa","externalIds":{"DBLP":"conf/mlsys/FegadeCGM22","ArXiv":"2110.10221","CorpusId":239049450},"title":"The CoRa Tensor Compiler: Compilation for Ragged Tensors with Minimal Padding"},{"paperId":"8ae292cbd9144acbf4b42b7ead82b079faf33192","externalIds":{"DBLP":"conf/emnlp/KuduguntaHBKLLF21","ArXiv":"2110.03742","DOI":"10.18653/v1/2021.findings-emnlp.304","CorpusId":238531628},"title":"Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","externalIds":{"ArXiv":"2108.12409","DBLP":"journals/corr/abs-2108-12409","CorpusId":237347130},"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"c1a4278f969acfc6682a924e31b95e1ade9703ee","externalIds":{"DBLP":"conf/emnlp/GuptaDGCB21","ArXiv":"2106.06899","ACL":"2021.sustainlp-1.5","DOI":"10.18653/v1/2021.sustainlp-1.5","CorpusId":235422257},"title":"Memory-efficient Transformers via Top-k Attention"},{"paperId":"0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe","externalIds":{"DBLP":"conf/nips/RollerSSW21","ArXiv":"2106.04426","CorpusId":235367626},"title":"Hash Layers For Large Sparse Models"},{"paperId":"03662672662f49e6b06148e94b407b60b0bb72f3","externalIds":{"DBLP":"conf/naacl/LiaoZRSSH21","ACL":"2021.naacl-main.162","MAG":"3170113752","DOI":"10.18653/V1/2021.NAACL-MAIN.162","CorpusId":235097407},"title":"A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models"},{"paperId":"d5e999aae76d5270ef272076979c809817458212","externalIds":{"DBLP":"journals/corr/abs-2105-14103","ArXiv":"2105.14103","CorpusId":235254329},"title":"An Attention Free Transformer"},{"paperId":"6da4b231148bf26677233d1f778d08a5d26f4313","externalIds":{"DBLP":"conf/naacl/YeLHS21","MAG":"3171750540","ACL":"2021.naacl-main.463","ArXiv":"2105.11618","DOI":"10.18653/V1/2021.NAACL-MAIN.463","CorpusId":235097557},"title":"TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference"},{"paperId":"67571d29190faea9fbd104acd16274f8c4edf254","externalIds":{"ArXiv":"2105.01601","DBLP":"conf/nips/TolstikhinHKBZU21","CorpusId":233714958},"title":"MLP-Mixer: An all-MLP Architecture for Vision"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","externalIds":{"DBLP":"journals/ijon/SuALPBL24","ArXiv":"2104.09864","DOI":"10.1016/j.neucom.2023.127063","CorpusId":233307138},"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d","externalIds":{"ACL":"2021.emnlp-main.406","DBLP":"journals/corr/abs-2104-08803","ArXiv":"2104.08803","DOI":"10.18653/v1/2021.emnlp-main.406","CorpusId":233296246},"title":"Consistent Accelerated Inference via Confident Adaptive Transformers"},{"paperId":"90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb","externalIds":{"ArXiv":"2104.08378","DBLP":"journals/corr/abs-2104-08378","CorpusId":233296249},"title":"Accelerating Sparse Deep Neural Networks"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"0486a4811ea15f975fa375d1b0c6a8e7b87176c2","externalIds":{"ArXiv":"2101.00542","DBLP":"conf/aaai/LiLXZ21","DOI":"10.1609/aaai.v35i15.17572","CorpusId":230437609},"title":"An Efficient Transformer Decoder with Compressed Sub-layers"},{"paperId":"1784fa1b5ac0b9f9fefe5e0508f91033f5952177","externalIds":{"DBLP":"journals/corr/abs-2012-15833","ACL":"2021.findings-acl.11","ArXiv":"2012.15833","DOI":"10.18653/v1/2021.findings-acl.11","CorpusId":229923438},"title":"Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade"},{"paperId":"14c26c4f14c9cad4d805583eb36b7ba52120423e","externalIds":{"DBLP":"conf/emnlp/LiLCRLZS21","ArXiv":"2012.14682","DOI":"10.18653/v1/2021.findings-emnlp.43","CorpusId":237414575},"title":"CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade"},{"paperId":"5b9d8bcc46b766b47389c912a8e026f81b91b0d8","externalIds":{"MAG":"3111507638","ArXiv":"2012.07436","DBLP":"conf/aaai/ZhouZPZLXZ21","DOI":"10.1609/aaai.v35i12.17325","CorpusId":229156802},"title":"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"},{"paperId":"47c2ab0159a743e45848e0b0375c632d4dc9bc76","externalIds":{"DBLP":"conf/sc/AliP0S20","DOI":"10.1109/SC41405.2020.00073","CorpusId":231792654},"title":"BATCH: Machine Learning Inference Serving on Serverless Platforms with Adaptive Batching"},{"paperId":"0c775d7ed34fb4690b4291490778649ae75c48d2","externalIds":{"DBLP":"journals/corr/abs-2010-05680","MAG":"3093091803","ArXiv":"2010.05680","DOI":"10.1145/3437801.3441578","CorpusId":222291613},"title":"TurboTransformers: an efficient GPU serving system for transformer models"},{"paperId":"9baab08fbe37369856688b2abe5b3c90cce1682c","externalIds":{"DBLP":"journals/tkdd/GuptaA22","ArXiv":"2008.05221","MAG":"3048823912","DOI":"10.1145/3487045","CorpusId":221112343},"title":"Compression of Deep Learning Models for Text: A Survey"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","externalIds":{"ArXiv":"2007.14062","DBLP":"journals/corr/abs-2007-14062","MAG":"3045733172","CorpusId":220831004},"title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","externalIds":{"MAG":"3040573126","DBLP":"conf/iclr/LepikhinLXCFHKS21","ArXiv":"2006.16668","CorpusId":220265858},"title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"6f68e1bb253925d8431588555d3010419f322e04","externalIds":{"DBLP":"conf/icml/KatharopoulosV020","MAG":"3037798801","ArXiv":"2006.16236","CorpusId":220250819},"title":"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"},{"paperId":"ac6535d096fc79dde2d9ce0329e0626b79ede7f0","externalIds":{"ArXiv":"2006.10369","DBLP":"conf/iclr/Kasai0PCS21","CorpusId":235613631},"title":"Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"},{"paperId":"ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e","externalIds":{"DBLP":"journals/corr/abs-2006-09503","ArXiv":"2006.09503","MAG":"3036879053","CorpusId":219720945},"title":"Memory-Efficient Pipeline-Parallel DNN Training"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","externalIds":{"MAG":"3033529678","DBLP":"journals/corr/abs-2006-04768","ArXiv":"2006.04768","CorpusId":219530577},"title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8","externalIds":{"MAG":"3101163004","DBLP":"conf/nips/ZhouXGM0W20","ArXiv":"2006.04152","CorpusId":219531455},"title":"BERT Loses Patience: Fast and Robust Inference with Early Exit"},{"paperId":"63857190aaf5aab1d94b54bb257b7b03b8cb5a50","externalIds":{"DBLP":"journals/corr/abs-2006-03274","MAG":"3033182847","ArXiv":"2006.03274","CorpusId":219401765},"title":"GMAT: Global Memory Augmentation for Transformers"},{"paperId":"66f0f35fc78bdf2af9de46093d49a428970cde2e","externalIds":{"MAG":"3099715410","DBLP":"conf/nips/Sanh0R20","ArXiv":"2005.07683","CorpusId":218665313},"title":"Movement Pruning: Adaptive Sparsity by Fine-Tuning"},{"paperId":"90a1491ac32e732c93773354e4e665794ed4d490","externalIds":{"MAG":"3035038672","DBLP":"journals/corr/abs-2004-12993","ArXiv":"2004.12993","ACL":"2020.acl-main.204","DOI":"10.18653/v1/2020.acl-main.204","CorpusId":216552850},"title":"DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","externalIds":{"DBLP":"journals/corr/abs-2004-05150","MAG":"3015468748","ArXiv":"2004.05150","CorpusId":215737171},"title":"Longformer: The Long-Document Transformer"},{"paperId":"5d34881ff68bd203ff790187e7e5c9e034389cfa","externalIds":{"MAG":"3014568172","DBLP":"journals/corr/abs-2004-02178","ArXiv":"2004.02178","ACL":"2020.acl-main.537","DOI":"10.18653/v1/2020.acl-main.537","CorpusId":214802887},"title":"FastBERT: a Self-distilling BERT with Adaptive Inference Time"},{"paperId":"657329c633709dd1ac34a30d57341b186b1a47c2","externalIds":{"ACL":"2021.tacl-1.4","MAG":"2997517014","DBLP":"journals/tacl/RoySVG21","ArXiv":"2003.05997","DOI":"10.1162/tacl_a_00353","CorpusId":212718077},"title":"Efficient Content-Based Sparse Attention with Routing Transformers"},{"paperId":"34a4e6818d680875ff0bef9a76de0376118446d1","externalIds":{"MAG":"3034609440","ArXiv":"2002.11296","DBLP":"journals/corr/abs-2002-11296","CorpusId":211505992},"title":"Sparse Sinkhorn Attention"},{"paperId":"c6c734e16f66fbfcefac7625cc64599e83292c1e","externalIds":{"MAG":"3101045333","DBLP":"conf/nips/WangW0B0020","ArXiv":"2002.10957","CorpusId":211296536},"title":"MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"},{"paperId":"94f94e8892261d0377159379ca5a166ceae19a14","externalIds":{"DBLP":"conf/icml/GoyalCRCSV20","MAG":"3034742519","CorpusId":219792793},"title":"PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"},{"paperId":"7b28577ed96bd1b76bbe79859b3222604b2dc369","externalIds":{"DBLP":"journals/corr/abs-2001-08785","MAG":"3000840023","ArXiv":"2001.08785","CorpusId":210911571},"title":"Semi-Autoregressive Training Improves Mask-Predict Decoding"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","externalIds":{"DBLP":"journals/corr/abs-2001-04451","MAG":"2994673210","ArXiv":"2001.04451","CorpusId":209315300},"title":"Reformer: The Efficient Transformer"},{"paperId":"594a89505eb803280628198920e87cfc2bb82d94","externalIds":{"MAG":"2985232804","ArXiv":"1911.02549","DBLP":"conf/isca/ReddiCKMSWABCCC20","DOI":"10.1109/ISCA45697.2020.00045","CorpusId":207880425},"title":"MLPerf Inference Benchmark"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"80b362efee95c1759c6dab9219eb77ca3ee44475","externalIds":{"DBLP":"conf/sosp/JiaPTWZA19","MAG":"2981758446","DOI":"10.1145/3341301.3359630","CorpusId":202726856},"title":"TASO: optimizing deep learning computation with automatic generation of graph substitutions"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","externalIds":{"DBLP":"journals/corr/abs-1910-01108","ArXiv":"1910.01108","MAG":"2978017171","CorpusId":203626972},"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1","externalIds":{"MAG":"2996159613","DBLP":"conf/iclr/FanGJ20","ArXiv":"1909.11556","CorpusId":202750230},"title":"Reducing Transformer Depth on Demand with Structured Dropout"},{"paperId":"0cbf97173391b0430140117027edcaf1a37968c7","externalIds":{"MAG":"3105966348","ACL":"2020.findings-emnlp.372","DBLP":"conf/emnlp/JiaoYSJCL0L20","ArXiv":"1909.10351","DOI":"10.18653/v1/2020.findings-emnlp.372","CorpusId":202719327},"title":"TinyBERT: Distilling BERT for Natural Language Understanding"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","externalIds":{"MAG":"2973727699","ArXiv":"1909.08053","DBLP":"journals/corr/abs-1909-08053","CorpusId":202660670},"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"75acc731bdd2b626edc74672a30da3bc51010ae8","externalIds":{"ArXiv":"1909.05858","MAG":"2973049837","DBLP":"journals/corr/abs-1909-05858","CorpusId":202573071},"title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},{"paperId":"80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","externalIds":{"DBLP":"conf/emnlp/SunCGL19","MAG":"2969515962","ACL":"D19-1441","ArXiv":"1908.09355","DOI":"10.18653/v1/D19-1441","CorpusId":201670719},"title":"Patient Knowledge Distillation for BERT Model Compression"},{"paperId":"f6390beca54411b06f3bde424fb983a451789733","externalIds":{"MAG":"2970777192","ArXiv":"1909.00015","DBLP":"journals/corr/abs-1909-00015","ACL":"D19-1223","DOI":"10.18653/v1/D19-1223","CorpusId":202538495},"title":"Adaptively Sparse Transformers"},{"paperId":"6b2704fd8517a9917cfd9d3735930be48717d3de","externalIds":{"MAG":"2965046076","ArXiv":"1906.11024","DBLP":"journals/corr/abs-1906-11024","DOI":"10.24963/ijcai.2019/735","CorpusId":195657947},"title":"Sharing Attention Weights for Fast Transformer"},{"paperId":"661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f","externalIds":{"MAG":"2954698171","DBLP":"conf/pldi/TilletKC19","DOI":"10.1145/3315508.3329973","CorpusId":184488182},"title":"Triton: an intermediate language and compiler for tiled neural network computations"},{"paperId":"b03c7ff961822183bab66b2e594415e585d3fd09","externalIds":{"ArXiv":"1905.10650","MAG":"2945767825","DBLP":"conf/nips/MichelLN19","CorpusId":166227946},"title":"Are Sixteen Heads Really Better than One?"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","externalIds":{"ArXiv":"1904.10509","DBLP":"journals/corr/abs-1904-10509","MAG":"2940744433","CorpusId":129945531},"title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","externalIds":{"DBLP":"journals/corr/abs-1904-09751","MAG":"2938704169","ArXiv":"1904.09751","CorpusId":127986954},"title":"The Curious Case of Neural Text Degeneration"},{"paperId":"5efadc9019ce3378a0eb6c8f939cdde6c8918b1e","externalIds":{"MAG":"2988975212","DBLP":"conf/emnlp/GhazvininejadLL19","ACL":"D19-1633","DOI":"10.18653/v1/D19-1633","CorpusId":202538740},"title":"Mask-Predict: Parallel Decoding of Conditional Masked Language Models"},{"paperId":"6b39bea0ae1720dcbc1ed19ffa697114c4d356c4","externalIds":{"MAG":"3004495293","ArXiv":"1903.11314","DBLP":"journals/corr/abs-1903-11314","DOI":"10.1145/3363554","CorpusId":219875251},"title":"Scalable Deep Learning on Distributed Infrastructures"},{"paperId":"2a31319e73d4486716168b65cdf7559baeda18ce","externalIds":{"ACL":"N19-1133","MAG":"2915716523","DBLP":"journals/corr/abs-1902-09113","ArXiv":"1902.09113","DOI":"10.18653/v1/N19-1133","CorpusId":209009596},"title":"Star-Transformer"},{"paperId":"9d9c4a3b27dab2293ed915ad69662ea09901ccab","externalIds":{"DBLP":"journals/corr/abs-1812-09664","MAG":"2962969034","ArXiv":"1812.09664","DOI":"10.1609/aaai.v33i01.33013723","CorpusId":56895270},"title":"Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input"},{"paperId":"5e04881e91bff952d102d967c4ffb498ec30d4af","externalIds":{"DBLP":"journals/corr/abs-1811-03115","MAG":"2890152612","ArXiv":"1811.03115","CorpusId":53208380},"title":"Blockwise Parallel Decoding for Deep Autoregressive Models"},{"paperId":"f971658ab845d7573c4bbb760d5e7e5332025254","externalIds":{"DBLP":"journals/corr/abs-1807-05358","MAG":"2884700152","ArXiv":"1807.05358","CorpusId":49868726},"title":"Beyond Data and Model Parallelism for Deep Neural Networks"},{"paperId":"ec1f582446aa24f3f0920123ee6f05feea0b5e0a","externalIds":{"DBLP":"journals/corr/abs-1805-02867","ArXiv":"1805.02867","MAG":"2801655600","CorpusId":21731006},"title":"Online normalizer calculation for softmax"},{"paperId":"29de7c0fb3c09eaf55b20619bceaeafe72fd87a6","externalIds":{"MAG":"2963096510","DBLP":"conf/acl/LewisDF18","ArXiv":"1805.04833","ACL":"P18-1082","DOI":"10.18653/v1/P18-1082","CorpusId":44134226},"title":"Hierarchical Neural Story Generation"},{"paperId":"fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299","externalIds":{"ACL":"P18-1027","ArXiv":"1805.04623","DBLP":"journals/corr/abs-1805-04623","MAG":"2963951265","DOI":"10.18653/v1/P18-1027","CorpusId":21700944},"title":"Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"},{"paperId":"d8c09661b1bebfb690f0566167c87d64c5628d73","externalIds":{"MAG":"2788193959","ArXiv":"1802.09941","DBLP":"journals/csur/Ben-NunH19","DOI":"10.1145/3320060","CorpusId":220247313},"title":"Demystifying Parallel and Distributed Deep Learning"},{"paperId":"9c5c89199114858eafbe50b46d77d38ffd03b28a","externalIds":{"DBLP":"journals/corr/abs-1802-06901","ArXiv":"1802.06901","ACL":"D18-1149","MAG":"2953322184","DOI":"10.18653/v1/D18-1149","CorpusId":3438497},"title":"Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement"},{"paperId":"15e81c8d1c21f9e928c72721ac46d458f3341454","externalIds":{"DBLP":"journals/corr/abs-1711-02281","MAG":"2767206889","ArXiv":"1711.02281","CorpusId":3480671},"title":"Non-Autoregressive Neural Machine Translation"},{"paperId":"5c317ad50a192e47acd3a5fd0569a5ef8a4f7a8f","externalIds":{"MAG":"2739738036","ACL":"P17-2091","DBLP":"conf/acl/ShiK17","DOI":"10.18653/v1/P17-2091","CorpusId":3293347},"title":"Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"96f78f409e4e3f25276d9d98977ef67f2a801abf","externalIds":{"MAG":"2951974986","ArXiv":"1703.00381","DBLP":"journals/corr/OlivaPS17","CorpusId":12201880},"title":"The Statistical Recurrent Unit"},{"paperId":"510e26733aaff585d65701b9f1be7ca9d5afc586","externalIds":{"DBLP":"journals/corr/ShazeerMMDLHD17","MAG":"2952339051","ArXiv":"1701.06538","CorpusId":12462234},"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"},{"paperId":"88caa4a0253a8b0076176745ebc072864eab66e1","externalIds":{"MAG":"2567070169","DBLP":"conf/icml/DauphinFAG17","ArXiv":"1612.08083","CorpusId":16119010},"title":"Language Modeling with Gated Convolutional Networks"},{"paperId":"896de8418884f4aab1ae4a60027500c9e8baffc3","externalIds":{"MAG":"2610140147","DBLP":"conf/icpr/Teerapittayanon16","ArXiv":"1709.01686","DOI":"10.1109/ICPR.2016.7900006","CorpusId":2916466},"title":"BranchyNet: Fast inference via early exiting from deep neural networks"},{"paperId":"6326e8ab3d2c2b179c26b1aa368fae7e42a34ed0","externalIds":{"MAG":"2056999868","DBLP":"journals/concurrency/GeijnW97","DOI":"10.1002/(SICI)1096-9128(199704)9:4%3C255::AID-CPE250%3E3.0.CO;2-2","CorpusId":611285},"title":"SUMMA: scalable universal matrix multiplication algorithm"},{"paperId":"944adc818b89a543282f9be3a356df67a09faccd","externalIds":{"MAG":"2158992088","DBLP":"journals/tc/Burton85","DOI":"10.1109/TC.1985.6312218","CorpusId":27560619},"title":"Speculative computation, parallelism, and functional programming"},{"paperId":"ea313326f4dbb4d80aad9f92acc969129bf3d98b","externalIds":{"DBLP":"journals/corr/abs-2501-12162","DOI":"10.48550/arXiv.2501.12162","CorpusId":278741242},"title":"AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative Decoding"},{"paperId":"da6f13c10f1675fce96398da0c83c39f798aef66","externalIds":{"DBLP":"conf/nips/ChenMSHRJC24","DOI":"10.52202/079017-4116","CorpusId":276318223},"title":"Sequoia: Scalable and Robust Speculative Decoding"},{"paperId":"f7f7586d901242624a6f33f129e2074fd853eaa8","externalIds":{"DBLP":"conf/nips/LiuT0NT0024","DOI":"10.52202/079017-0381","CorpusId":276117179},"title":"Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting"},{"paperId":"d3389f9e362c74d50ee8021d67b9711805d7054d","externalIds":{"DBLP":"conf/icml/ZhangZYXLPJ24","CorpusId":272330520},"title":"Accelerating Iterative Retrieval-augmented Language Model Serving with Speculation"},{"paperId":"47101e6a62f1a26bfe713017e9fe888026445c20","externalIds":{"DBLP":"conf/osdi/WuZZSL024","CorpusId":271217621},"title":"dLoRA: Dynamically Orchestrating Requests and Adapters for LoRA LLM Serving"},{"paperId":"1c77e47c50ee92bbd0b49fe04a850fa337784dcf","externalIds":{"DBLP":"journals/corr/abs-2411-04975","DOI":"10.48550/arXiv.2411.04975","CorpusId":273877636},"title":"SuffixDecoding: A Model-Free Approach to Speeding Up Large Language Model Inference"},{"paperId":"d1a6b3a5efde3783b53f822dc8dd00aaac934b95","externalIds":{"DBLP":"journals/corr/abs-2305-09781","DOI":"10.48550/arXiv.2305.09781","CorpusId":258740799},"title":"SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification"},{"paperId":"1a4c6856292b8c64d19a812a77f0aa6fd47cb96c","externalIds":{"DBLP":"journals/corr/abs-2308-08155","CorpusId":260925901},"title":"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework"},{"paperId":"33489921dac9fed095332ea13326043c9911a6d3","externalIds":{"DBLP":"conf/osdi/00010XMXMG0Z23","CorpusId":259858911},"title":"Welder: Scheduling Deep Learning Memory Access via Tile-graph"},{"paperId":"6a128a8b9078469dfd28dcfee1b9fdce54e6c31d","externalIds":{"DBLP":"conf/ndss/LuHGL000WC25","DOI":"10.14722/ndss.2025.230057","CorpusId":265068718},"title":"BumbleBee: Secure Two-party Inference Framework for Large Transformers"},{"paperId":"000cc7bff1a286193286f095f1668eacedbefc1a","externalIds":{"DBLP":"journals/corr/abs-2309-17157","DOI":"10.48550/arXiv.2309.17157","CorpusId":263310377},"title":"LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud"},{"paperId":"363668677c459ebc0ff494655f993a93a0251009","externalIds":{"DBLP":"conf/iclr/FrantarAHA23","CorpusId":259298689},"title":"OPTQ: Accurate Quantization for Generative Pre-trained Transformers"},{"paperId":"8264257f573696fc0a1ef7531c825041832197f8","externalIds":{"DBLP":"journals/corr/abs-2301-12017","DOI":"10.48550/arXiv.2301.12017","CorpusId":256390278},"title":"Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases"},{"paperId":"f4d546b9cd5681430de63e7d8739dc2d50045fb4","externalIds":{"DBLP":"journals/corr/abs-2310-07240","DOI":"10.48550/arXiv.2310.07240","CorpusId":263834986},"title":"CacheGen: Fast Context Loading for Language Model Applications"},{"paperId":"f2352a48f86fc783bb2537a1b5c97cdc4ddd852e","externalIds":{"DBLP":"conf/osdi/ZhengWZHMWHMTHJ23","CorpusId":265910753},"title":"EINNET: Optimizing Tensor Programs with Derivation-Based Transformations"},{"paperId":"eec73c865db0f3e882ccb5951879de0eff423266","externalIds":{"DBLP":"conf/nips/NarayananS0BLL23","CorpusId":268064255},"title":"Cheaply Estimating Inference Efficiency Metrics for Autoregressive Transformer Models"},{"paperId":"2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c","externalIds":{"DBLP":"journals/corr/abs-2306-00978","DOI":"10.48550/arXiv.2306.00978","CorpusId":271271084},"title":"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"},{"paperId":"ccf15b75d3ed3287c0ac524666578ed785bff1a3","externalIds":{"DBLP":"journals/corr/abs-2302-07863","DOI":"10.48550/arXiv.2302.07863","CorpusId":263869576},"title":"Big Little Transformer Decoder"},{"paperId":"e327d6794179139d4a3564effa26ab13a7f7b6c3","externalIds":{"ACL":"2022.coling-1.453","DBLP":"conf/coling/WuZHZ22","CorpusId":252819285},"title":"Speeding up Transformer Decoding via an Attention Refinement Network"},{"paperId":"104f7a96eba307056e1038e183ee8c24d009ba13","externalIds":{"DBLP":"journals/corr/abs-2206-09557","DOI":"10.48550/arXiv.2206.09557","CorpusId":249890271},"title":"nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"},{"paperId":"afb989488eb1801111553006c01e99441b205bc7","externalIds":{"DBLP":"conf/usenix/ZhouWZS22","CorpusId":266233970},"title":"PetS: A Unified Framework for Parameter-Efficient Transformers Serving"},{"paperId":"4a984ec8286b19bb0c033e6e4df198a0421b0c17","externalIds":{"ACL":"2022.coling-1.414","DBLP":"conf/coling/Kong0YZ22","CorpusId":252818912},"title":"Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting"},{"paperId":"da37fb0a9071d1ceda0c7e359a049c6c7e627a01","externalIds":{"DBLP":"conf/osdi/UngerJ0LBNRPMML22","CorpusId":267834645},"title":"Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization"},{"paperId":"4b2137280915ccc0e06e97b604778b05876a34ad","externalIds":{"CorpusId":247456179},"title":"Evaluating Large Language Models"},{"paperId":"1554887c6bd76c443a477b27dbcab35877787b27","externalIds":{"DBLP":"conf/naacl/WangXWWL21","ACL":"2021.naacl-industry.15","DOI":"10.18653/v1/2021.naacl-industry.15","CorpusId":235097440},"title":"LightSeq: A High Performance Inference Library for Transformers"},{"paperId":"6fe1587be4ff23e45169f129cfd4d5575b767da0","externalIds":{"DOI":"10.1007/978-1-4842-6919-0_2","CorpusId":241893528},"title":"Databricks"},{"paperId":"832e87496369c91c4fad3084029ef4321ee57aaf","externalIds":{"ACL":"2021.wmt-1.77","DBLP":"conf/wmt/WuHJ21","CorpusId":245855702},"title":"TenTrans High-Performance Inference Toolkit for WMT2021 Efficiency Task"},{"paperId":"96acd6b1c1528d3f50f71083c88d84d619b5b8b8","externalIds":{"DBLP":"conf/osdi/BaiZZJ20","MAG":"3096484587","CorpusId":227177645},"title":"PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications"},{"paperId":"5772d98f4b336d650cb9b9de6e27095002fb218f","externalIds":{"MAG":"2956461999","DBLP":"conf/usenix/ZhangYWY19","CorpusId":196810567},"title":"MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving"},{"paperId":"e399090e4dd760e8f09bd3d4de61b13b057c7740","externalIds":{"MAG":"2899971035","CorpusId":4625928},"title":"Compiling machine learning programs via high-level tracing"},{"paperId":"067e07b725ab012c80aa2f87857f6791c1407f6d","externalIds":{"DBLP":"conf/interspeech/SakSB14","MAG":"2293634267","DOI":"10.21437/Interspeech.2014-80","CorpusId":6263878},"title":"Long short-term memory recurrent neural network architectures for large scale acoustic modeling"},{"paperId":"0c646a8923d70ae9a93ee8945c05e47246707d80","externalIds":{"CorpusId":5378522},"title":"SUBMITTED BY"},{"paperId":"146ac1952faf171a1c198b2023200be1d1fcd14a","externalIds":{"CorpusId":249059077},"title":"This paper is included in the Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation."}]}