{"abstract":"Identifying an appropriate underlying graph kernel that reflects pairwise similarities is critical in many recent graph spectral signal restoration schemes, including image denoising, dequantization, and contrast enhancement. Existing graph learning algorithms compute the most likely entries of a properly defined graph Laplacian matrix <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {L}$</tex-math></inline-formula>, but require a large number of signal observations <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {z}$</tex-math></inline-formula>'s for a stable estimate. In this work, we assume instead the availability of a relevant feature vector <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {f}_i$</tex-math></inline-formula> per node <inline-formula><tex-math notation=\"LaTeX\">$i$</tex-math></inline-formula>, from which we compute an optimal feature graph via optimization of a feature metric. Specifically, we alternately optimize the diagonal and off-diagonal entries of a Mahalanobis distance matrix <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {M}$</tex-math></inline-formula> by minimizing the graph Laplacian regularizer (GLR) <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {z}^{\\top } \\mathbf {L} \\mathbf {z}$</tex-math></inline-formula>, where edge weight is <inline-formula><tex-math notation=\"LaTeX\">$w_{i,j} = \\exp \\lbrace - (\\mathbf {f}_i - \\mathbf {f}_j)^{\\top } \\mathbf {M} (\\mathbf {f}_i - \\mathbf {f}_j) \\rbrace$</tex-math></inline-formula>, given a single observation <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {z}$</tex-math></inline-formula>. We optimize diagonal entries via proximal gradient (PG), where we constrain <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {M}$</tex-math></inline-formula> to be positive definite (PD) via linear inequalities derived from the Gershgorin circle theorem. To optimize off-diagonal entries, we design a block descent algorithm that iteratively optimizes one row and column of <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {M}$</tex-math></inline-formula>. To keep <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {M}$</tex-math></inline-formula> PD, we constrain the Schur complement of sub-matrix <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {M}_{2,2}$</tex-math></inline-formula> of <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {M}$</tex-math></inline-formula> to be PD when optimizing via PG. Our algorithm mitigates full eigen-decomposition of <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {M}$</tex-math></inline-formula>, thus ensuring fast computation speed even when feature vector <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {f}_i$</tex-math></inline-formula> has high dimension. To validate its usefulness, we apply our feature graph learning algorithm to the problem of 3D point cloud denoising, resulting in state-of-the-art performance compared to competing schemes in extensive experiments."}