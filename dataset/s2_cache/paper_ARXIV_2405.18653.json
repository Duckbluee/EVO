{"paperId":"eaac29467de2dd223d32cc3d3a77b637ef2bc4b3","externalIds":{"DBLP":"journals/corr/abs-2405-18653","ArXiv":"2405.18653","DOI":"10.1145/3705725","CorpusId":270095224},"title":"Recent Advances of Foundation Language Models-based Continual Learning: A Survey","openAccessPdf":{"url":"https://dl.acm.org/doi/pdf/10.1145/3705725","status":"HYBRID","license":"other-oa","disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2303799141","name":"Yutao Yang"},{"authorId":"145558445","name":"Jie Zhou"},{"authorId":"2301111695","name":"Xuanwen Ding"},{"authorId":"2231570984","name":"Tianyu Huai"},{"authorId":"2286433951","name":"Shunyu Liu"},{"authorId":"2152518131","name":"Qin Chen"},{"authorId":"2268703214","name":"Liang He"},{"authorId":"2303903840","name":"Yuan Xie"}],"abstract":"Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing and computer vision. Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich common sense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. Despite these capabilities, LMs still struggle with catastrophic forgetting, hindering their ability to learn continuously like humans. To address this, continual learning (CL) methodologies have been introduced, allowing LMs to adapt to new tasks while retaining learned knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking. In this article, we delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models, large language models, and vision-language models. We divide these studies into offline and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning."}