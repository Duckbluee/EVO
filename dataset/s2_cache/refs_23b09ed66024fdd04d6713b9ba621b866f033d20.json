{"references":[{"paperId":"2c0312c604f9f7638bb4533b39e0ae81e7f6ab12","externalIds":{"ArXiv":"2311.16867","DBLP":"journals/corr/abs-2311-16867","DOI":"10.48550/arXiv.2311.16867","CorpusId":265466629},"title":"The Falcon Series of Open Language Models"},{"paperId":"908dad62c0e43d80e3e3cb3c0402f7c71c70499c","externalIds":{"ArXiv":"2310.08560","DBLP":"journals/corr/abs-2310-08560","DOI":"10.48550/arXiv.2310.08560","CorpusId":263909014},"title":"MemGPT: Towards LLMs as Operating Systems"},{"paperId":"4c0428917aeee6aa7bd434f337d039f35996b736","externalIds":{"DBLP":"journals/corr/abs-2310-06839","ArXiv":"2310.06839","DOI":"10.48550/arXiv.2310.06839","CorpusId":263830692},"title":"LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"},{"paperId":"368fb35a07076eba01c2e4700499323cd4524513","externalIds":{"ArXiv":"2310.01352","DBLP":"conf/iclr/Lin0CSL00KSLZY24","DOI":"10.48550/arXiv.2310.01352","CorpusId":263605962},"title":"RA-DIT: Retrieval-Augmented Dual Instruction Tuning"},{"paperId":"c4d3b9a87295db0b9f6466b7f3ce2f175c6d3157","externalIds":{"DBLP":"journals/corr/abs-2310-00576","ArXiv":"2310.00576","DOI":"10.48550/arXiv.2310.00576","CorpusId":263334451},"title":"GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length"},{"paperId":"73290ecbec2f38d1d647ddef1ada69cee41725b3","externalIds":{"DBLP":"conf/iclr/Zhu00SWWL24","ArXiv":"2309.10400","DOI":"10.48550/arXiv.2309.10400","CorpusId":262053659},"title":"PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"},{"paperId":"c96297261467b5daa2d01227496a70d444602434","externalIds":{"DBLP":"journals/corr/abs-2309-10305","ArXiv":"2309.10305","DOI":"10.48550/arXiv.2309.10305","CorpusId":261951743},"title":"Baichuan 2: Open Large-scale Language Models"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"b31a5884a8ebe96b6300839b28608b97f8f8ef76","externalIds":{"DBLP":"journals/corr/abs-2308-14508","ArXiv":"2308.14508","DOI":"10.48550/arXiv.2308.14508","CorpusId":261245264},"title":"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"},{"paperId":"0b0debb710366cdff461938c80763eace1651af6","externalIds":{"DBLP":"journals/corr/abs-2308-12950","ArXiv":"2308.12950","DOI":"10.48550/arXiv.2308.12950","CorpusId":261100919},"title":"Code Llama: Open Foundation Models for Code"},{"paperId":"2dfb9171e180dcb0af23d305e024d43d311708ab","externalIds":{"DBLP":"journals/corr/abs-2308-10882","ArXiv":"2308.10882","DOI":"10.48550/arXiv.2308.10882","CorpusId":261048876},"title":"Giraffe: Adventures in Expanding Context Lengths in LLMs"},{"paperId":"823ca4778e1027f2f0b356df051d762dcecaaba0","externalIds":{"ArXiv":"2307.08691","DBLP":"journals/corr/abs-2307-08691","DOI":"10.48550/arXiv.2307.08691","CorpusId":259936734},"title":"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"},{"paperId":"1733eb7792f7a43dd21f51f4d1017a1bffd217b5","externalIds":{"DBLP":"journals/tacl/LiuLHPBPL24","ArXiv":"2307.03172","ACL":"2024.tacl-1.9","DOI":"10.1162/tacl_a_00638","CorpusId":259360665},"title":"Lost in the Middle: How Language Models Use Long Contexts"},{"paperId":"f5afaccfe90268485a9961c5771ec5e71e9b806c","externalIds":{"ArXiv":"2306.15595","DBLP":"journals/corr/abs-2306-15595","DOI":"10.48550/arXiv.2306.15595","CorpusId":259262376},"title":"Extending Context Window of Large Language Models via Positional Interpolation"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"60b35c6d68acced19b0c66edcfc0ee0a2c11efed","externalIds":{"DBLP":"journals/corr/abs-2305-16300","ArXiv":"2305.16300","DOI":"10.48550/arXiv.2305.16300","CorpusId":258887482},"title":"Landmark Attention: Random-Access Infinite Context Length for Transformers"},{"paperId":"2f7364d8e5cf94315bf8905f57de9c5543e9a4bf","externalIds":{"DBLP":"journals/corr/abs-2305-14788","ArXiv":"2305.14788","DOI":"10.48550/arXiv.2305.14788","CorpusId":258865249},"title":"Adapting Language Models to Compress Contexts"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"eb511ae6b9f04e4936891d26787f274b48b99d57","externalIds":{"DBLP":"conf/emnlp/0002IEBL23","ArXiv":"2305.14196","DOI":"10.48550/arXiv.2305.14196","CorpusId":258841877},"title":"ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding"},{"paperId":"d9964ab436eefd21f923a4bc833c6b66692c7f00","externalIds":{"DBLP":"journals/corr/abs-2305-13304","ArXiv":"2305.13304","DOI":"10.48550/arXiv.2305.13304","CorpusId":258832617},"title":"RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text"},{"paperId":"c8dd0fe00f9a71ea68b6856b36590b8daa316139","externalIds":{"DBLP":"journals/corr/abs-2305-04859","ArXiv":"2305.04859","DOI":"10.1007/978-3-031-44696-2_24","CorpusId":258557836},"title":"A Frustratingly Easy Improvement for Position Embeddings via Random Padding"},{"paperId":"dbc368bc8b49347dd27679894524fa62f88492c9","externalIds":{"DBLP":"journals/corr/abs-2305-01625","ArXiv":"2305.01625","DOI":"10.48550/arXiv.2305.01625","CorpusId":258436892},"title":"Unlimiformer: Long-Range Transformers with Unlimited Length Input"},{"paperId":"f711aae062ae30c0888910b2bdcc5be6c1d1c340","externalIds":{"DBLP":"conf/dasfaa/WangLYHWWML25","ArXiv":"2304.13343","DOI":"10.1007/978-981-95-4158-4_12","CorpusId":258331553},"title":"SCM: Enhancing Large Language Model with Self-Controlled Memory Framework"},{"paperId":"0a5af6c39fe47901e8a69ec538d6ebb95a30a23a","externalIds":{"DBLP":"journals/corr/abs-2304-12102","ArXiv":"2304.12102","DOI":"10.48550/arXiv.2304.12102","CorpusId":258298489},"title":"Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering"},{"paperId":"b9870e130f61ff900fe00dbcc5782c9b31773d32","externalIds":{"DBLP":"journals/corr/abs-2304-08467","ArXiv":"2304.08467","DOI":"10.48550/arXiv.2304.08467","CorpusId":258179012},"title":"Learning to Compress Prompts with Gist Tokens"},{"paperId":"5278a8eb2ba2429d4029745caf4e661080073c81","externalIds":{"DBLP":"conf/uist/ParkOCMLB23","ArXiv":"2304.03442","DOI":"10.1145/3586183.3606763","CorpusId":258040990},"title":"Generative Agents: Interactive Simulacra of Human Behavior"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","externalIds":{"DBLP":"journals/corr/abs-2303-17564","ArXiv":"2303.17564","CorpusId":257833842},"title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"27d391d65ab42c30dc35595213ba6585633afa5d","externalIds":{"ArXiv":"2303.09752","DBLP":"journals/corr/abs-2303-09752","DOI":"10.48550/arXiv.2303.09752","CorpusId":257622671},"title":"CoLT5: Faster Long-Range Transformers with Conditional Computation"},{"paperId":"68adb03744692247fb834406798894db9fe77010","externalIds":{"DBLP":"journals/corr/abs-2302-14502","ArXiv":"2302.14502","DOI":"10.48550/arXiv.2302.14502","CorpusId":257232619},"title":"A Survey on Long Text Modeling with Transformers"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"0a6906bd6f026d3da3031c641ed03081bd0b574e","externalIds":{"ArXiv":"2302.14017","DBLP":"journals/corr/abs-2302-14017","DOI":"10.48550/arXiv.2302.14017","CorpusId":257219934},"title":"Full Stack Optimization of Transformer Inference: a Survey"},{"paperId":"3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e","externalIds":{"DBLP":"journals/corr/abs-2302-00093","ArXiv":"2302.00093","DOI":"10.48550/arXiv.2302.00093","CorpusId":256459776},"title":"Large Language Models Can Be Easily Distracted by Irrelevant Context"},{"paperId":"465471bb5bf1a945549d6291c2d23367966b4957","externalIds":{"ArXiv":"2302.00083","DBLP":"journals/corr/abs-2302-00083","DOI":"10.1162/tacl_a_00605","CorpusId":256459451},"title":"In-Context Retrieval-Augmented Language Models"},{"paperId":"3d5922d71a370f32b7f232a596def914f67eebd1","externalIds":{"ACL":"2023.acl-long.79","DBLP":"journals/corr/abs-2212-10375","ArXiv":"2212.10375","DOI":"10.48550/arXiv.2212.10375","CorpusId":254877590},"title":"Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering"},{"paperId":"87126a964ed14d0d2207747fc732b197e2fc9493","externalIds":{"ACL":"2022.emnlp-main.149","DBLP":"journals/corr/abs-2212-02027","ArXiv":"2212.02027","DOI":"10.48550/arXiv.2212.02027","CorpusId":254246471},"title":"Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","externalIds":{"DBLP":"journals/corr/abs-2211-15533","ArXiv":"2211.15533","DOI":"10.48550/arXiv.2211.15533","CorpusId":254044610},"title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"1dff6b1b35e2d45d4db57c8b4e4395486c3e365f","externalIds":{"ArXiv":"2210.09461","DBLP":"conf/iclr/BolyaFDZFH23","DOI":"10.48550/arXiv.2210.09461","CorpusId":252968113},"title":"Token Merging: Your ViT But Faster"},{"paperId":"b8ad6ba5a367dc44aeebc85baded2af54d28255d","externalIds":{"DBLP":"conf/acl/JainZAWNLTNRBMX23","ACL":"2023.acl-long.355","ArXiv":"2210.01185","DOI":"10.18653/v1/2023.acl-long.355","CorpusId":258461112},"title":"ContraCLM: Contrastive Learning For Causal Language Model"},{"paperId":"70e91e16eb321067d9402710e14a40cf28311f73","externalIds":{"DBLP":"conf/iclr/MaZKHGNMZ23","ArXiv":"2209.10655","DOI":"10.48550/arXiv.2209.10655","CorpusId":252439127},"title":"Mega: Moving Average Equipped Gated Attention"},{"paperId":"4be7d1524edb0137599a5cc95f72844b85a52fe1","externalIds":{"DBLP":"journals/corr/abs-2208-07339","ArXiv":"2208.07339","DOI":"10.48550/arXiv.2208.07339","CorpusId":251564521},"title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"},{"paperId":"916be31cbf847faa65cad0549e153f0c25b9f424","externalIds":{"ArXiv":"2208.03299","DBLP":"journals/jmlr/IzacardLLHPSDJRG23","CorpusId":251371732},"title":"Few-shot Learning with Retrieval Augmented Language Models"},{"paperId":"c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617","externalIds":{"ArXiv":"2207.02098","DBLP":"journals/corr/abs-2207-02098","DOI":"10.48550/arXiv.2207.02098","CorpusId":250280065},"title":"Neural Networks and the Chomsky Hierarchy"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"da1d6445b6b64ce9eb4587ba8abbdc490f648ec1","externalIds":{"DBLP":"journals/corr/abs-2205-12674","ArXiv":"2205.12674","ACL":"2022.emnlp-main.382","DOI":"10.48550/arXiv.2205.12674","CorpusId":249062699},"title":"Training Language Models with Memory Augmentation"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","externalIds":{"ACL":"2022.bigscience-1.9","DBLP":"journals/corr/abs-2204-06745","ArXiv":"2204.06745","DOI":"10.48550/arXiv.2204.06745","CorpusId":248177957},"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"a2fc77f075f666b462d9350e7576f0ba9845c61b","externalIds":{"DBLP":"journals/corr/abs-2203-16634","ArXiv":"2203.16634","DOI":"10.48550/arXiv.2203.16634","CorpusId":247839823},"title":"Transformer Language Models without Positional Encodings Still Learn Positional Information"},{"paperId":"397f12152afb61196659505d7a9897b8ba079e44","externalIds":{"DBLP":"journals/corr/abs-2203-08991","ArXiv":"2203.08991","ACL":"2022.acl-long.1","DOI":"10.48550/arXiv.2203.08991","CorpusId":247518565},"title":"AdapLeR: Speeding up Inference by Adaptive Length Reduction"},{"paperId":"081edae651e709e448bdd8a1f1b5760c7c7e1f53","externalIds":{"ACL":"2022.findings-acl.207","DBLP":"journals/corr/abs-2203-05797","ArXiv":"2203.05797","DOI":"10.48550/arXiv.2203.05797","CorpusId":247411350},"title":"Long Time No See! Open-Domain Conversation with Long-Term Persona Memory"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","externalIds":{"DBLP":"journals/corr/abs-2112-09332","ArXiv":"2112.09332","CorpusId":245329531},"title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"3dfb1f50f2a34a699c339dabaa6f9b3a977973de","externalIds":{"ArXiv":"2112.07916","DBLP":"conf/naacl/GuoAUONSY22","DOI":"10.18653/v1/2022.findings-naacl.55","CorpusId":245144820},"title":"LongT5: Efficient Text-To-Text Transformer for Long Sequences"},{"paperId":"2d82ee05b132d4681c3bd517afc17d608fe6e525","externalIds":{"DBLP":"journals/corr/abs-2112-07210","ArXiv":"2112.07210","ACL":"2022.naacl-main.144","DOI":"10.18653/v1/2022.naacl-main.144","CorpusId":245131261},"title":"Simple Local Attentions Remain Competitive for Long-Context Tasks"},{"paperId":"590432f953b6ce1b4b36bf66a2ac65eeee567515","externalIds":{"DBLP":"conf/naacl/SanthanamKSPZ22","ArXiv":"2112.01488","ACL":"2022.naacl-main.272","DOI":"10.18653/v1/2022.naacl-main.272","CorpusId":244799249},"title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","externalIds":{"DBLP":"journals/corr/abs-2110-13711","ArXiv":"2110.13711","DOI":"10.18653/v1/2022.findings-naacl.117","CorpusId":239885427},"title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"a6fdb277d0a4b09899f802bda3359f5c2021a156","externalIds":{"ArXiv":"2109.10862","DBLP":"journals/corr/abs-2109-10862","CorpusId":237593001},"title":"Recursively Summarizing Books with Human Feedback"},{"paperId":"64522a5b3476e9f201f6a5b3e312ef0005c562f1","externalIds":{"DBLP":"journals/corr/abs-2109-05644","ArXiv":"2109.05644","ACL":"2021.emnlp-main.266","DOI":"10.18653/v1/2021.emnlp-main.266","CorpusId":237491629},"title":"SHAPE: Shifted Absolute Position Embedding for Transformers"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","externalIds":{"ArXiv":"2108.12409","DBLP":"journals/corr/abs-2108-12409","CorpusId":237347130},"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"88064de690af282dbdf222774f03ff070b9df22b","externalIds":{"ArXiv":"2107.07567","DBLP":"journals/corr/abs-2107-07567","ACL":"2022.acl-long.356","DOI":"10.18653/v1/2022.acl-long.356","CorpusId":236034497},"title":"Beyond Goldfish Memory: Long-Term Open-Domain Conversation"},{"paperId":"5d032bd2632b6f5847767f39ce247098c6bbc563","externalIds":{"DBLP":"conf/nips/RenDDYLSD21","ArXiv":"2107.05768","CorpusId":235829099},"title":"Combiner: Full Attention Transformer with Sparse Computation Cost"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"7509c66a666e2e3f14bc8676b969b945ee6e136f","externalIds":{"DBLP":"conf/nips/LikhomanenkoXSC21","ArXiv":"2106.03143","CorpusId":235358538},"title":"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"},{"paperId":"d8e7bad2681ce70277c900c77a22181d4b03d705","externalIds":{"DBLP":"journals/corr/abs-2106-01950","ArXiv":"2106.01950","ACL":"2021.acl-short.18","DOI":"10.18653/v1/2021.acl-short.18","CorpusId":235313860},"title":"The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","externalIds":{"DBLP":"journals/ijon/SuALPBL24","ArXiv":"2104.09864","DOI":"10.1016/j.neucom.2023.127063","CorpusId":233307138},"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"9dc624d7258d1a56117ca720aea953ce46b66b21","externalIds":{"MAG":"3170490008","ArXiv":"2104.02112","DBLP":"journals/corr/abs-2104-02112","ACL":"2021.naacl-main.112","DOI":"10.18653/V1/2021.NAACL-MAIN.112","CorpusId":233033613},"title":"Efficient Attentions for Long Document Summarization"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"46c585ee9abf76779ea4b863d2da4358efd0d1d3","externalIds":{"DBLP":"journals/tacl/YogatamadK21","ArXiv":"2102.02557","DOI":"10.1162/tacl_a_00371","CorpusId":231717977},"title":"Adaptive Semiparametric Language Models"},{"paperId":"0822f8d7e6a72a65e65f147d3a8d8fccd485da40","externalIds":{"DBLP":"journals/corr/abs-2012-15832","ACL":"2021.acl-long.427","ArXiv":"2012.15832","DOI":"10.18653/v1/2021.acl-long.427","CorpusId":229924221},"title":"Shortformer: Better Language Modeling using Shorter Inputs"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","externalIds":{"DBLP":"journals/corr/abs-2101-00027","ArXiv":"2101.00027","CorpusId":230435736},"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"8b28d9e3ca408b8a41d32f8bd4da7fbbd4f12a4b","externalIds":{"ACL":"2021.emnlp-main.526","DBLP":"conf/emnlp/RashidLGR21","ArXiv":"2012.15495","DOI":"10.18653/v1/2021.emnlp-main.526","CorpusId":229923739},"title":"Towards Zero-Shot Knowledge Distillation for Natural Language Processing"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"67ee20536c30a225b86902af2f091e28e5e19b40","externalIds":{"DBLP":"conf/ijcnlp/WuLQGGY22","ArXiv":"2010.06891","DOI":"10.18653/v1/2022.findings-aacl.29","CorpusId":248157404},"title":"Memformer: A Memory-Augmented Transformer for Sequence Modeling"},{"paperId":"a50d31c082521817a1e74cae584963a63163ca70","externalIds":{"ArXiv":"2010.07003","DBLP":"journals/corr/abs-2010-07003","ACL":"2021.acl-long.508","MAG":"3092746498","DOI":"10.18653/v1/2021.acl-long.508","CorpusId":222341845},"title":"Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","externalIds":{"DBLP":"conf/iclr/ChoromanskiLDSG21","ArXiv":"2009.14794","MAG":"3091156754","CorpusId":222067132},"title":"Rethinking Attention with Performers"},{"paperId":"84476fdf6ead3553f4493dff8e02308439d6222b","externalIds":{"DBLP":"conf/emnlp/HuangLXX20","ArXiv":"2009.13658","ACL":"2020.findings-emnlp.298","MAG":"3089892483","DOI":"10.18653/v1/2020.findings-emnlp.298","CorpusId":221995630},"title":"Improve Transformer Models with Better Relative Position Embeddings"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","externalIds":{"ArXiv":"2007.14062","DBLP":"journals/corr/abs-2007-14062","MAG":"3045733172","CorpusId":220831004},"title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"030d7d7ae48a9f81700b2c1f7cf835235777b8e7","externalIds":{"DBLP":"journals/corr/abs-2007-00814","MAG":"3038831159","ArXiv":"2007.00814","DOI":"10.1162/tacl_a_00405","CorpusId":220302658},"title":"Relevance-guided Supervision for OpenQA with ColBERT"},{"paperId":"6f68e1bb253925d8431588555d3010419f322e04","externalIds":{"DBLP":"conf/icml/KatharopoulosV020","MAG":"3037798801","ArXiv":"2006.16236","CorpusId":220250819},"title":"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"},{"paperId":"8256f48f759cf85044db251cc512f965834945b3","externalIds":{"DBLP":"journals/corr/abs-2006-15595","ArXiv":"2006.15595","CorpusId":220249871},"title":"Rethinking Positional Encoding in Language Pre-training"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","externalIds":{"MAG":"3033529678","DBLP":"journals/corr/abs-2006-04768","ArXiv":"2006.04768","CorpusId":219530577},"title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"14b65a86c82e38fce0eb3506e0d4084ad5cdb583","externalIds":{"MAG":"3033187248","DBLP":"conf/iclr/HeLGC21","ArXiv":"2006.03654","CorpusId":219531210},"title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","externalIds":{"DBLP":"journals/corr/abs-2004-05150","MAG":"3015468748","ArXiv":"2004.05150","CorpusId":215737171},"title":"Longformer: The Long-Document Transformer"},{"paperId":"b26f2037f769d5ffc5f7bdcec2de8da28ec14bee","externalIds":{"MAG":"3015883388","ArXiv":"2004.04906","DBLP":"conf/emnlp/KarpukhinOMLWEC20","ACL":"2020.emnlp-main.550","DOI":"10.18653/v1/2020.emnlp-main.550","CorpusId":215737187},"title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"657329c633709dd1ac34a30d57341b186b1a47c2","externalIds":{"ACL":"2021.tacl-1.4","MAG":"2997517014","DBLP":"journals/tacl/RoySVG21","ArXiv":"2003.05997","DOI":"10.1162/tacl_a_00353","CorpusId":212718077},"title":"Efficient Content-Based Sparse Attention with Routing Transformers"},{"paperId":"7af72a461ed7cda180e7eab878efd5f35d79bbf4","externalIds":{"DBLP":"conf/icml/ChenK0H20","MAG":"3034978746","ArXiv":"2002.05709","CorpusId":211096730},"title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","externalIds":{"ArXiv":"2002.08909","MAG":"3034671305","DBLP":"conf/icml/GuuLTPC20","CorpusId":211204736},"title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"94f94e8892261d0377159379ca5a166ceae19a14","externalIds":{"DBLP":"conf/icml/GoyalCRCSV20","MAG":"3034742519","CorpusId":219792793},"title":"PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","externalIds":{"DBLP":"journals/corr/abs-2001-04451","MAG":"2994673210","ArXiv":"2001.04451","CorpusId":209315300},"title":"Reformer: The Efficient Transformer"},{"paperId":"4287533d12143cdbc4948b60ecece28b6c750f17","externalIds":{"MAG":"3042266831","DBLP":"conf/eccv/ZhangSZGM20","DOI":"10.1007/978-3-030-58580-8_41","CorpusId":220647041},"title":"Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks"},{"paperId":"509b4661ed74a24c2ffdbf131f9e1c6a1783752d","externalIds":{"MAG":"2995744795","DBLP":"conf/iclr/YunBRRK20","ArXiv":"1912.10077","CorpusId":209444410},"title":"Are Transformers universal approximators of sequence-to-sequence functions?"},{"paperId":"f51497f463566581874c941353dd9d80069c5b77","externalIds":{"DBLP":"conf/iclr/RaePJHL20","MAG":"2995575179","ArXiv":"1911.05507","CorpusId":207930593},"title":"Compressive Transformers for Long-Range Sequence Modelling"},{"paperId":"2cf3bd0cc1382f35384e259d99e4f9744eeaed28","externalIds":{"MAG":"3106298483","ArXiv":"1911.02972","ACL":"2020.findings-emnlp.232","DBLP":"journals/corr/abs-1911-02972","DOI":"10.18653/v1/2020.findings-emnlp.232","CorpusId":207847640},"title":"Blockwise Self-Attention for Long Document Understanding"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","externalIds":{"MAG":"2983040767","ArXiv":"1911.02116","ACL":"2020.acl-main.747","DBLP":"conf/acl/ConneauKGCWGGOZ20","DOI":"10.18653/v1/2020.acl-main.747","CorpusId":207880568},"title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"7be8c119dbe065c52125ee7716601751f3116844","externalIds":{"MAG":"2988841832","ArXiv":"1911.00172","DBLP":"journals/corr/abs-1911-00172","CorpusId":207870430},"title":"Generalization through Memorization: Nearest Neighbor Language Models"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","externalIds":{"MAG":"2996428491","DBLP":"journals/corr/abs-1909-11942","ArXiv":"1909.11942","CorpusId":202888986},"title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","externalIds":{"DBLP":"journals/corr/abs-1908-10084","MAG":"2970641574","ArXiv":"1908.10084","ACL":"D19-1410","DOI":"10.18653/v1/D19-1410","CorpusId":201646309},"title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","externalIds":{"MAG":"2950813464","DBLP":"journals/corr/abs-1906-08237","ArXiv":"1906.08237","CorpusId":195069387},"title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","externalIds":{"ArXiv":"1904.10509","DBLP":"journals/corr/abs-1904-10509","MAG":"2940744433","CorpusId":129945531},"title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","externalIds":{"MAG":"2952509486","DBLP":"journals/corr/abs-1904-01038","ArXiv":"1904.01038","ACL":"N19-4009","DOI":"10.18653/v1/N19-4009","CorpusId":91184134},"title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","externalIds":{"ArXiv":"1901.02860","DBLP":"conf/acl/DaiYYCLS19","MAG":"2964110616","ACL":"P19-1285","DOI":"10.18653/v1/P19-1285","CorpusId":57759363},"title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","externalIds":{"MAG":"2963925437","DBLP":"journals/corr/abs-1803-02155","ACL":"N18-2074","ArXiv":"1803.02155","DOI":"10.18653/v1/N18-2074","CorpusId":3725815},"title":"Self-Attention with Relative Position Representations"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","externalIds":{"MAG":"2950541952","DBLP":"conf/iclr/LoshchilovH19","CorpusId":53592270},"title":"Decoupled Weight Decay Regularization"},{"paperId":"b70e7049d1e0a6cf4e98ed0addd3faa7b484689e","externalIds":{"DBLP":"conf/nlpcc/MengH17","MAG":"2782096898","DOI":"10.1007/978-3-319-73618-1_4","CorpusId":42098520},"title":"Dialogue Intent Classification with Long Short-Term Memory Networks"},{"paperId":"9ef902f3c427d697f3579cd79844b44de99bc93c","externalIds":{"MAG":"2752127475","DBLP":"conf/semeval/Al-NatshehMMZ17","ACL":"S17-2013","DOI":"10.18653/v1/S17-2013","CorpusId":1222366},"title":"UdL at SemEval-2017 Task 1: Semantic Textual Similarity Estimation of English Sentence Pairs Using Regression Model over Pairwise Features"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","externalIds":{"ArXiv":"1705.03122","MAG":"2950686565","DBLP":"journals/corr/GehringAGYD17","CorpusId":3648736},"title":"Convolutional Sequence to Sequence Learning"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","externalIds":{"DBLP":"journals/corr/WilliamsNB17","MAG":"2963846996","ArXiv":"1704.05426","ACL":"N18-1101","DOI":"10.18653/v1/N18-1101","CorpusId":3432876},"title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"29092f0deaac3898e43b3f094bf15d82b6a99afd","externalIds":{"ArXiv":"1703.03129","DBLP":"journals/corr/KaiserNRB17","MAG":"2583010282","CorpusId":12122362},"title":"Learning to Remember Rare Events"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","externalIds":{"DBLP":"journals/corr/RajpurkarZLL16","MAG":"2963748441","ACL":"D16-1264","ArXiv":"1606.05250","DOI":"10.18653/v1/D16-1264","CorpusId":11816014},"title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2cd8e8f510c89c7c18268e8ad51c061e459ad321","externalIds":{"DBLP":"journals/corr/ParikhT0U16","ACL":"D16-1244","MAG":"2413794162","ArXiv":"1606.01933","DOI":"10.18653/v1/D16-1244","CorpusId":8495258},"title":"A Decomposable Attention Model for Natural Language Inference"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","externalIds":{"DBLP":"conf/acl/SennrichHB16a","ACL":"P16-1162","MAG":"1816313093","ArXiv":"1508.07909","DOI":"10.18653/v1/P16-1162","CorpusId":1114678},"title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"c214c715b90a0fff8d8c5fd2115a9234a9bdd49e","externalIds":{"MAG":"1999599827","DOI":"10.1016/J.LINGUA.2015.02.002","CorpusId":46254463},"title":"Polysemy: Current perspectives and approaches"},{"paperId":"c1126fbffd6b8547a44c58b192b36b08b18299de","externalIds":{"MAG":"2167839676","DBLP":"journals/corr/GravesWD14","ArXiv":"1410.5401","CorpusId":15299054},"title":"Neural Turing Machines"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","externalIds":{"ACL":"D13-1170","DBLP":"conf/emnlp/SocherPWCMNP13","MAG":"2251939518","DOI":"10.18653/v1/d13-1170","CorpusId":990233},"title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"8264257f573696fc0a1ef7531c825041832197f8","externalIds":{"DBLP":"journals/corr/abs-2301-12017","DOI":"10.48550/arXiv.2301.12017","CorpusId":256390278},"title":"Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases"},{"paperId":"147af99d852c516e16d90b128504a43c82ceffb8","externalIds":{"DBLP":"conf/acl/JiangYTTDL23","DOI":"10.18653/v1/2023.findings-acl.426","CorpusId":259858947},"title":"\"Low-Resource\" Text Classification: A Parameter-Free Classification Method with Compressors"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8ff46c88964a36985f2b45933a3d47b81bd87bd0","externalIds":{"CorpusId":233225749},"title":"Quora Question Pairs"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","externalIds":{"ACL":"I05-5002","DBLP":"conf/acl-iwp/DolanB05","MAG":"131533222","CorpusId":16639476},"title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":"6038d62f22be3162324d3cb5214512966fc6ddb0","externalIds":{"CorpusId":266690063},"title":"Music Transformer 기반 음악"}]}