{"references":[{"paperId":"1559681cb51d2362e2d9776a567667ae15302015","externalIds":{"DBLP":"journals/corr/abs-2407-20785","ArXiv":"2407.20785","DOI":"10.48550/arXiv.2407.20785","CorpusId":271544429},"title":"Retinex-Diffusion: On Controlling Illumination Conditions in Diffusion Models via Retinex Theory"},{"paperId":"bf20f64d7544d20953f117fb19d2d82489e6ca36","externalIds":{"DBLP":"journals/corr/abs-2407-15886","ArXiv":"2407.15886","DOI":"10.48550/arXiv.2407.15886","CorpusId":271334679},"title":"CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models"},{"paperId":"b956ba21aae4e75f102e53119754e5dab366e8da","externalIds":{"DBLP":"conf/cvpr/ZhaoCHCN24","DOI":"10.1109/CVPR52733.2024.00423","CorpusId":272724984},"title":"Vector Graphics Generation via Mutually Impulsed Dual-Domain Diffusion"},{"paperId":"ab01f985e771145e4de5a51742319e7de4e5fc4c","externalIds":{"DBLP":"journals/corr/abs-2406-07520","ArXiv":"2406.07520","DOI":"10.48550/arXiv.2406.07520","CorpusId":270379694},"title":"Neural Gaffer: Relighting Any Object via Diffusion"},{"paperId":"dde98c39ca1a1c681ae7c5295fa70272a85c5b8e","externalIds":{"ArXiv":"2405.00448","DBLP":"journals/corr/abs-2405-00448","DOI":"10.48550/arXiv.2405.00448","CorpusId":269484238},"title":"MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion Generation"},{"paperId":"db8d88f2398f27285a4d6f036c3b6b8d6780eafc","externalIds":{"DBLP":"journals/corr/abs-2403-10983","ArXiv":"2403.10983","DOI":"10.48550/arXiv.2403.10983","CorpusId":268512816},"title":"OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models"},{"paperId":"2a179caf53e90cb32586f8c12689f1b362b0d4bf","externalIds":{"DBLP":"journals/corr/abs-2403-10701","ArXiv":"2403.10701","DOI":"10.1109/CVPR52733.2024.00769","CorpusId":268512724},"title":"IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation"},{"paperId":"e5dd95af05d946e790da293190dd41eb53d979b8","externalIds":{"DBLP":"conf/cvpr/0001PSNH24","ArXiv":"2403.10615","DOI":"10.1109/CVPR52733.2024.00894","CorpusId":268512690},"title":"LightIt: Illumination Modeling and Control for Diffusion Models"},{"paperId":"be5396aad21ced6d637e5f678bea6524c12453ac","externalIds":{"ArXiv":"2403.05053","DBLP":"journals/corr/abs-2403-05053","DOI":"10.1145/3664647.3680848","CorpusId":268296977},"title":"PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering"},{"paperId":"2e99a953a34b703658cdf6a510e2bf89b823771a","externalIds":{"DBLP":"conf/siggraph/00010PKW024","ArXiv":"2402.11929","DOI":"10.1145/3641519.3657396","CorpusId":267750463},"title":"DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation"},{"paperId":"3a63ae4086248e4ba4bd106839a26a08256909c4","externalIds":{"ArXiv":"2401.13627","DBLP":"conf/cvpr/YuGLHKWH0024","DOI":"10.1109/CVPR52733.2024.02425","CorpusId":267199774},"title":"Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild"},{"paperId":"30d31dd034fd65c0bf4df6f24c6225eab39317f1","externalIds":{"DBLP":"journals/tnn/MoserSRFPD25","ArXiv":"2401.00736","DOI":"10.1109/TNNLS.2024.3476671","CorpusId":266693843,"PubMed":"39471123"},"title":"Diffusion Models, Image Super-Resolution, and Everything: A Survey"},{"paperId":"05eb2ad3af471c05a24abbf70258688e579cdf22","externalIds":{"DBLP":"journals/corr/abs-2312-16794","ArXiv":"2312.16794","DOI":"10.1109/CVPR52733.2024.00598","CorpusId":266573944},"title":"ZONE: Zero-Shot Instruction-Guided Local Editing"},{"paperId":"ed526b03334bfaa372f153e69f652eb736186895","externalIds":{"DBLP":"conf/aaai/DuanCK0FFH24","ArXiv":"2312.14611","DOI":"10.48550/arXiv.2312.14611","CorpusId":266521621},"title":"Tuning-Free Inversion-Enhanced Control for Consistent Image Editing"},{"paperId":"b30052880e4784ab7ce82612ef549129b52f7f40","externalIds":{"DBLP":"conf/iclr/0001SAWNS25","ArXiv":"2312.14091","DOI":"10.48550/arXiv.2312.14091","CorpusId":266435854},"title":"HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models"},{"paperId":"2dbaf84d12e40c9dcedb577c2be7a68faf5715d7","externalIds":{"ArXiv":"2312.10113","DBLP":"conf/cvpr/GuoL24","DOI":"10.1109/CVPR52733.2024.00667","CorpusId":266348293},"title":"Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation"},{"paperId":"0e8f1bb91bb4502966fa5e91e0610832dfe4240e","externalIds":{"ArXiv":"2401.08639","DBLP":"journals/corr/abs-2401-08639","DOI":"10.48550/arXiv.2401.08639","CorpusId":267028569},"title":"One-Step Diffusion Distillation via Deep Equilibrium Models"},{"paperId":"388b0f44faf0a14cc402c2554ec36a868cf59129","externalIds":{"DBLP":"journals/corr/abs-2312-06739","ArXiv":"2312.06739","DOI":"10.1109/CVPR52733.2024.00799","CorpusId":266174392},"title":"SmartEdit: Exploring Complex Instruction-Based Image Editing with Multimodal Large Language Models"},{"paperId":"6efdfc765553f2557bbac1f29ec6e7ee4e4b8253","externalIds":{"DBLP":"conf/cvpr/Ren0YSZJG024","ArXiv":"2312.06886","DOI":"10.1109/CVPR52733.2024.00617","CorpusId":266173984},"title":"Relightful Harmonization: Lighting-Aware Portrait Background Replacement"},{"paperId":"514f7237a57909aef36479f6bea8a727dd00b1b9","externalIds":{"ArXiv":"2312.06738","DBLP":"journals/corr/abs-2312-06738","DOI":"10.48550/arXiv.2312.06738","CorpusId":266174467},"title":"InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following"},{"paperId":"99216956376efb229d4e1a9beeb047e9bfce246e","externalIds":{"ArXiv":"2312.05039","DBLP":"conf/cvpr/SinghZLSL024","DOI":"10.1109/CVPR52733.2024.00621","CorpusId":266149485},"title":"SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control"},{"paperId":"999b48ef5551e550c89fba97d7f66347efee8030","externalIds":{"DBLP":"conf/eccv/ZhuangZLYC24","ArXiv":"2312.03594","DOI":"10.48550/arXiv.2312.03594","CorpusId":265674753},"title":"A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting"},{"paperId":"1093328a179820b4475b501a00e05223d774d95d","externalIds":{"DBLP":"journals/corr/abs-2312-03556","ArXiv":"2312.03556","DOI":"10.1109/WACV57701.2024.00535","CorpusId":265696143},"title":"Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention"},{"paperId":"eae3b2c1b7f8b802ef6d5a66a8e397ec8e0db132","externalIds":{"DBLP":"journals/corr/abs-2312-03667","ArXiv":"2312.03667","DOI":"10.48550/arXiv.2312.03667","CorpusId":265714588},"title":"WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on"},{"paperId":"5322ba0056794e0d4b7bc519b92061492d3cecb4","externalIds":{"ArXiv":"2312.03771","DBLP":"journals/corr/abs-2312-03771","DOI":"10.48550/arXiv.2312.03771","CorpusId":266053929},"title":"DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models"},{"paperId":"05c7eabf85ab246fa30beb191c6b13cf1181d1ba","externalIds":{"ArXiv":"2312.01725","DBLP":"conf/cvpr/KimG00C24","DOI":"10.1109/CVPR52733.2024.00781","CorpusId":265609458},"title":"Stable VITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On"},{"paperId":"8c974ff7dd7a7b9cf7c615b81b092896af4f6c5e","externalIds":{"DBLP":"conf/cvpr/NamKPY24","ArXiv":"2311.18608","DOI":"10.1109/CVPR52733.2024.00878","CorpusId":265506264},"title":"Contrastive Denoising Score for Text-Guided Latent Diffusion Image Editing"},{"paperId":"aee0f3c37fb52117379e6df2ebe869745c74fcc6","externalIds":{"DBLP":"journals/corr/abs-2312-00833","ArXiv":"2312.00833","DOI":"10.48550/arXiv.2312.00833","CorpusId":265608717},"title":"Lasagna: Layered Score Distillation for Disentangled Object Relighting"},{"paperId":"654c66239fd00174c22d9fb0dc7e6060022632f3","externalIds":{"DBLP":"conf/cvpr/BrackFKTSKP24","ArXiv":"2311.16711","DOI":"10.1109/CVPR52733.2024.00846","CorpusId":265466786},"title":"LEDITS++: Limitless Image Editing Using Text-to-Image Models"},{"paperId":"606f236b88fa79cadd3e5e0b87f73ae5705621d4","externalIds":{"ArXiv":"2311.16432","DBLP":"conf/cvpr/LinCT0024","DOI":"10.1109/CVPR52733.2024.00674","CorpusId":265466876},"title":"Text-Driven Image Editing via Learnable Regions"},{"paperId":"d730d42bb655b3b44727d71c147f9758612043a8","externalIds":{"DBLP":"journals/corr/abs-2311-17042","ArXiv":"2311.17042","DOI":"10.48550/arXiv.2311.17042","CorpusId":265466173},"title":"Adversarial Diffusion Distillation"},{"paperId":"038c5d91b952464d413defcd7a3b695a1306f650","externalIds":{"DBLP":"conf/cvpr/Sun0L0PZYY24","ArXiv":"2311.16512","DOI":"10.1109/CVPR52733.2024.02444","CorpusId":265466453},"title":"CoSeR: Bridging Image and Language for Cognitive Super-Resolution"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"66facda0b11bae0bac6f0f1828d3b10e7d9b2047","externalIds":{"DBLP":"conf/cvpr/LvHYHLLWCC22","ArXiv":"2311.12631","DOI":"10.1109/CVPRW63382.2024.00150","CorpusId":265308987},"title":"GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning"},{"paperId":"85b10400864187230714506412c85610c786b5c3","externalIds":{"ArXiv":"2311.10709","DBLP":"conf/eccv/GirdharSBDARSYPM24","DOI":"10.48550/arXiv.2311.10709","CorpusId":265281059},"title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning"},{"paperId":"5bcb0153dd0840113eb27d4d6f753414ef656a03","externalIds":{"ArXiv":"2311.10089","DBLP":"conf/cvpr/SheyninPSKZAPT24","DOI":"10.1109/CVPR52733.2024.00847","CorpusId":265221391},"title":"Emu Edit: Precise Image Editing via Recognition and Generation Tasks"},{"paperId":"63a77d3494d98f06ee7f9e30ea18801457152350","externalIds":{"DBLP":"journals/corr/abs-2311-04811","ArXiv":"2311.04811","DOI":"10.1007/s11263-024-02305-2","CorpusId":265050510},"title":"Image-Based Virtual Try-On: A Survey"},{"paperId":"1d8019f49b319aea98acd572f4ebb7fdeeddedb4","externalIds":{"DBLP":"journals/corr/abs-2311-01410","ArXiv":"2311.01410","DOI":"10.48550/arXiv.2311.01410","CorpusId":264935182},"title":"The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing"},{"paperId":"8f8673c61af83790b56a7a8dca4ab970013659a0","externalIds":{"DBLP":"conf/nips/KotarTYY023","ArXiv":"2311.00750","DOI":"10.48550/arXiv.2311.00750","CorpusId":264935263},"title":"Are These the Same Apple? Comparing Images Based on Object Intrinsics"},{"paperId":"8ac96b1a5196011381a7ee52738ba7eaebcb44a0","externalIds":{"ArXiv":"2310.19784","DBLP":"journals/corr/abs-2310-19784","DOI":"10.48550/arXiv.2310.19784","CorpusId":264815845},"title":"CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"},{"paperId":"d406b90a31297f853ab45099f6ed5d4a44703abb","externalIds":{"ArXiv":"2310.19145","DBLP":"conf/emnlp/ChakrabartySSM23","DOI":"10.48550/arXiv.2310.19145","CorpusId":264803981},"title":"Learning to Follow Object-Centric Image Editing Instructions Faithfully"},{"paperId":"2d895c3153a80d281428a14d14ae121536fe790d","externalIds":{"ArXiv":"2310.13165","DBLP":"conf/nips/XuMHLC23","DOI":"10.48550/arXiv.2310.13165","CorpusId":264405630},"title":"CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation"},{"paperId":"592f36d0783d7543c2ab35069be23c97ff8b5a23","externalIds":{"DBLP":"conf/iclr/0009DW0ZS24","ArXiv":"2310.12149","DOI":"10.48550/arXiv.2310.12149","CorpusId":264288824},"title":"Object-aware Inversion and Reassembly for Image Editing"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","externalIds":{"ArXiv":"2310.10647","DBLP":"journals/corr/abs-2310-10647","DOI":"10.1145/3696415","CorpusId":264172934},"title":"A Survey on Video Diffusion Models"},{"paperId":"6487ec82f6d8082a5b402a5416ea03009acb1679","externalIds":{"DBLP":"journals/cgf/PoYGABBCDHKLLMNOTWW24","ArXiv":"2310.07204","DOI":"10.1111/cgf.15063","CorpusId":263835355},"title":"State of the Art on Diffusion Models for Visual Computing"},{"paperId":"a63d56908f7d960343821f4acbd6efb47a4e6296","externalIds":{"ArXiv":"2310.07222","DBLP":"conf/mm/YangC023","DOI":"10.1145/3581783.3612200","CorpusId":263835196},"title":"Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model"},{"paperId":"8b7cce220c3b19f9b2d4a6c531907ed3b592b55e","externalIds":{"DBLP":"journals/corr/abs-2310-04378","ArXiv":"2310.04378","CorpusId":263831037},"title":"Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference"},{"paperId":"6b6a5b29e45648045d641dcba47506a6617c6d76","externalIds":{"DBLP":"journals/corr/abs-2310-02848","ArXiv":"2310.02848","DOI":"10.48550/arXiv.2310.02848","CorpusId":263620873},"title":"Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models"},{"paperId":"40417133ef6de4a3bdc825d46d683f12063cd418","externalIds":{"ArXiv":"2310.02426","DBLP":"journals/corr/abs-2310-02426","DOI":"10.48550/arXiv.2310.02426","CorpusId":263622002},"title":"EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods"},{"paperId":"76579c1180efb539e27d9b5468a51217b85721ad","externalIds":{"DBLP":"conf/iccv/WuT23","DOI":"10.1109/ICCV51070.2023.00678","CorpusId":266648419},"title":"A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance"},{"paperId":"8fafd95a6ffbecf9c1b5f4542ac4b78a00602551","externalIds":{"DBLP":"journals/corr/abs-2310-00426","ArXiv":"2310.00426","DOI":"10.48550/arXiv.2310.00426","CorpusId":263334265},"title":"PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","externalIds":{"ArXiv":"2309.17102","DBLP":"journals/corr/abs-2309-17102","DOI":"10.48550/arXiv.2309.17102","CorpusId":263310303},"title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"fbfc96dc9e8fde0def80198956ff8cb879899b6b","externalIds":{"DBLP":"conf/prcv/HuangLQC23","ArXiv":"2309.16608","DOI":"10.48550/arXiv.2309.16608","CorpusId":263134058},"title":"KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing"},{"paperId":"a5b7fc1bff0910ff31975ec0a15ed30c41f0a968","externalIds":{"DBLP":"journals/ijcv/ZhangWLZRGGS25","ArXiv":"2309.15818","DOI":"10.1007/s11263-024-02271-9","CorpusId":263151295},"title":"Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation"},{"paperId":"8d531cb8cf51eec3b8f1106d189295fa3c81c02a","externalIds":{"ArXiv":"2309.15664","DBLP":"conf/nips/0060YYB023","DOI":"10.48550/arXiv.2309.15664","CorpusId":263151874},"title":"Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing"},{"paperId":"e04da3c945aae8e2211222d373e7bf771d6412a7","externalIds":{"DBLP":"journals/corr/abs-2309-15807","ArXiv":"2309.15807","DOI":"10.48550/arXiv.2309.15807","CorpusId":263151865},"title":"Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack"},{"paperId":"ed4603ea341acc26cab24f41aa40524fb7779917","externalIds":{"DBLP":"journals/ijcv/WangCMZHWYHYYGWSJCLDLQL25","ArXiv":"2309.15103","DOI":"10.1007/s11263-024-02295-1","CorpusId":262823915},"title":"LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models"},{"paperId":"a6c5179a3498ffeccf656d4282fd4d007558a377","externalIds":{"ArXiv":"2309.14709","DBLP":"journals/corr/abs-2309-14709","DOI":"10.48550/arXiv.2309.14709","CorpusId":262825263},"title":"Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement"},{"paperId":"54de506c54905ee4f62ab800137b9509f4cc4ad7","externalIds":{"ArXiv":"2309.14934","DBLP":"journals/corr/abs-2309-14934","DOI":"10.1109/ICICML60161.2023.10424833","CorpusId":262826063},"title":"FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing"},{"paperId":"42f43de1558c8fc006bc6b0d0911ea4a44ef0bd5","externalIds":{"DBLP":"conf/bmvc/ChenL23","ArXiv":"2309.11321","DOI":"10.48550/arXiv.2309.11321","CorpusId":262066278},"title":"Face Aging via Diffusion-based Editing"},{"paperId":"212f5c5101598ffd6ce25532f051ff5e391afe4b","externalIds":{"DBLP":"journals/corr/abs-2309-10556","ArXiv":"2309.10556","DOI":"10.48550/arXiv.2309.10556","CorpusId":262055174},"title":"Forgedit: Text Guided Image Editing via Learning and Forgetting"},{"paperId":"045402ecd9dfebeeea9f348f2bd19dd44c11eead","externalIds":{"DBLP":"journals/corr/abs-2309-09614","ArXiv":"2309.09614","DOI":"10.48550/arXiv.2309.09614","CorpusId":262045903},"title":"Gradpaint: Gradient-Guided Inpainting with Diffusion Models"},{"paperId":"da3a188c227d817b90203ab5294685d8424ad1e2","externalIds":{"DBLP":"conf/iccv/0007GXH23","ArXiv":"2309.04907","DOI":"10.1109/ICCV51070.2023.01458","CorpusId":261682188},"title":"Effective Real Image Editing with Accelerated Iterative Diffusion Inversion"},{"paperId":"6deb76b8ac8d35dded7dd76d768a245bb69ffe91","externalIds":{"DBLP":"journals/corr/abs-2309-04372","ArXiv":"2309.04372","DOI":"10.48550/arXiv.2309.04372","CorpusId":261660692},"title":"MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers"},{"paperId":"75ca0ce6493c94bb48766b042243023a5439beeb","externalIds":{"ArXiv":"2309.03895","DBLP":"conf/cvpr/GengYHLGZBZLHCG24","DOI":"10.1109/CVPR52733.2024.01208","CorpusId":261582721},"title":"InstructDiffusion: A Generalist Modeling Interface for Vision Tasks"},{"paperId":"f9d6afa4c38981307e7579d3469ad81147b222e4","externalIds":{"DBLP":"journals/corr/abs-2309-00613","ArXiv":"2309.00613","DOI":"10.1109/WACV57701.2024.00792","CorpusId":261494286},"title":"Iterative Multi-granular Image Editing using Diffusion Models"},{"paperId":"156fc6ed50e9873814dd8554526eedf9841a1be0","externalIds":{"ArXiv":"2309.00398","DBLP":"journals/corr/abs-2309-00398","DOI":"10.48550/arXiv.2309.00398","CorpusId":261494083},"title":"VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation"},{"paperId":"858f0643110ccccb6a9103747f2169fecfb98668","externalIds":{"DBLP":"journals/corr/abs-2308-15070","ArXiv":"2308.15070","DOI":"10.48550/arXiv.2308.15070","CorpusId":261276317},"title":"DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior"},{"paperId":"49faa5c9bf6459a256f68872fb3b51df6b0a2dd8","externalIds":{"DBLP":"journals/corr/abs-2308-13142","ArXiv":"2308.13142","DOI":"10.48550/arXiv.2308.13142","CorpusId":261214460},"title":"A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions"},{"paperId":"70ba95384664ab7becfd8196d20a46fe66112c91","externalIds":{"DBLP":"journals/ijcv/LiRJLWZWC25","ArXiv":"2308.09388","DOI":"10.1007/s11263-025-02570-9","CorpusId":261031011},"title":"Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey"},{"paperId":"737ad8905228cd410e3342b5cceefd4feb57d166","externalIds":{"DBLP":"conf/eccv/MirzaeiABKLDG24","ArXiv":"2308.08947","DOI":"10.48550/arXiv.2308.08947","CorpusId":261031162},"title":"Watch Your Steps: Local Image and Scene Editing by Text Instructions"},{"paperId":"e8411b1b42ca26939a42d9c8cd5a3a8ae0a4a15a","externalIds":{"DBLP":"journals/corr/abs-2308-07863","ArXiv":"2308.07863","DOI":"10.1109/ICCV51070.2023.00706","CorpusId":260900064},"title":"StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models"},{"paperId":"2854e5bab8e6f36e54c64456628a9559bf67019e","externalIds":{"ArXiv":"2308.06721","DBLP":"journals/corr/abs-2308-06721","DOI":"10.48550/arXiv.2308.06721","CorpusId":260886966},"title":"IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models"},{"paperId":"84f0a99d0f0015a6145c94468870d43ab1d166fd","externalIds":{"DBLP":"journals/corr/abs-2308-06571","ArXiv":"2308.06571","DOI":"10.48550/arXiv.2308.06571","CorpusId":260887737},"title":"ModelScope Text-to-Video Technical Report"},{"paperId":"9e99648c5d4d9ce4fba73007291bbd3f804c83ea","externalIds":{"DBLP":"conf/nips/SunYPSYHQK23","ArXiv":"2308.00906","DOI":"10.48550/arXiv.2308.00906","CorpusId":260379053},"title":"ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation"},{"paperId":"35eb7fe8c6130ce422639dbbe42294228bc42735","externalIds":{"DBLP":"journals/corr/abs-2307-12493","ArXiv":"2307.12493","DOI":"10.1109/ICCV51070.2023.00218","CorpusId":260125230},"title":"TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition"},{"paperId":"9b4e61dda9db6317afae1bd4a12356d00769d9f3","externalIds":{"DBLP":"conf/cvpr/ChenHLSZZ24","ArXiv":"2307.09481","DOI":"10.1109/CVPR52733.2024.00630","CorpusId":259951373},"title":"AnyDoor: Zero-shot Object-level Image Customization"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"276f6117b8b8549a47461653b95e657278260ee3","externalIds":{"DBLP":"conf/cvpr/RuizLJWHPWRA24","ArXiv":"2307.06949","DOI":"10.1109/CVPR52733.2024.00624","CorpusId":259847576},"title":"HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models"},{"paperId":"2cfaa5b3571d3b75f040f6d639359a3c673f5561","externalIds":{"DBLP":"journals/corr/abs-2307-02421","ArXiv":"2307.02421","DOI":"10.48550/arXiv.2307.02421","CorpusId":259342813},"title":"DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models"},{"paperId":"d7890d1906d95c4ae4c430b350455156d6d8aed9","externalIds":{"DBLP":"conf/iclr/PodellELBDMPR24","ArXiv":"2307.01952","CorpusId":259341735},"title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"},{"paperId":"463222658c4df87f149d8a968aa5760f0d399606","externalIds":{"ArXiv":"2307.00522","DBLP":"journals/corr/abs-2307-00522","DOI":"10.48550/arXiv.2307.00522","CorpusId":259316918},"title":"LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance"},{"paperId":"10c9a466cdf400bf01cabbcb1bc9a14555730195","externalIds":{"DBLP":"journals/nn/HuangTX25","ArXiv":"2306.16894","DOI":"10.48550/arXiv.2306.16894","CorpusId":259287333,"PubMed":"39423497"},"title":"PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing"},{"paperId":"03a281a176413ed4d140293edc5bf04a3ad7a1f1","externalIds":{"DBLP":"conf/cvpr/ShiXLPYZTB24","ArXiv":"2306.14435","DOI":"10.1109/CVPR52733.2024.00844","CorpusId":259252555},"title":"DragDiffusion: Harnessing Diffusion Models for Interactive Point-Based Image Editing"},{"paperId":"34509b045b625bab87f5d3747fcae0736ea4f880","externalIds":{"DBLP":"journals/corr/abs-2306-14685","ArXiv":"2306.14685","DOI":"10.48550/arXiv.2306.14685","CorpusId":259252217},"title":"DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models"},{"paperId":"8745157f991013b23fbb79d300ba560f9005c8d4","externalIds":{"DBLP":"journals/corr/abs-2306-09869","ArXiv":"2306.09869","DOI":"10.48550/arXiv.2306.09869","CorpusId":259187874},"title":"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models"},{"paperId":"ecf38d702bea560c1f9d372645e2be7661a71f37","externalIds":{"DBLP":"conf/nips/ZhangMCSS23","ArXiv":"2306.10012","DOI":"10.48550/arXiv.2306.10012","CorpusId":259187796},"title":"MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing"},{"paperId":"de374dc9bb0b443ef399fc36587aa1e192447466","externalIds":{"ArXiv":"2306.09344","DBLP":"journals/corr/abs-2306-09344","DOI":"10.48550/arXiv.2306.09344","CorpusId":259171761},"title":"DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data"},{"paperId":"0b4fd518368afd639912a3425004d5ccc348c4d4","externalIds":{"DBLP":"journals/corr/abs-2306-07596","ArXiv":"2306.07596","DOI":"10.48550/arXiv.2306.07596","CorpusId":259144837},"title":"Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model"},{"paperId":"45549f8845f1fe6daea0f300e52d5df7061632d4","externalIds":{"DBLP":"journals/corr/abs-2306-05414","ArXiv":"2306.05414","DOI":"10.1109/WACV57701.2024.00424","CorpusId":259287564},"title":"ProxEdit: Improving Tuning-Free Real Image Editing with Proximal Guidance"},{"paperId":"43f46d6d6ddc25ec6e0015df8b3276a450b486ba","externalIds":{"ArXiv":"2306.04139","CorpusId":259501520},"title":"A Comprehensive Survey on Generative Diffusion Models for Structured Data"},{"paperId":"0809c278fcdec2ce297da3a9d6e031fc192263f6","externalIds":{"ArXiv":"2306.02717","DBLP":"journals/corr/abs-2306-02717","DOI":"10.48550/arXiv.2306.02717","CorpusId":259076409},"title":"User-friendly Image Editing with Minimal Text Input: Leveraging Captioning and Injection Techniques"},{"paperId":"6e5760e5d4b468bbf01a95a6f64bd65c3aa3d798","externalIds":{"DBLP":"journals/corr/abs-2306-00950","ArXiv":"2306.00950","DOI":"10.1111/cgf.70040","CorpusId":258999295},"title":"Differential Diffusion: Giving Each Pixel Its Strength"},{"paperId":"861370f7c2d18bed09905fde334a19cc96e83e14","externalIds":{"ArXiv":"2306.00983","DBLP":"journals/corr/abs-2306-00983","DOI":"10.48550/arXiv.2306.00983","CorpusId":258999204},"title":"StyleDrop: Text-to-Image Generation in Any Style"},{"paperId":"fbebb1a5d72aec2a6b13fc909f781f6ba9b04925","externalIds":{"DBLP":"conf/nips/EpsteinJPEH23","ArXiv":"2306.00986","DOI":"10.48550/arXiv.2306.00986","CorpusId":258999106},"title":"Diffusion Self-Guidance for Controllable Image Generation"},{"paperId":"9b504916f0e8fbeb1891f0db299dcbbc118f4898","externalIds":{"ArXiv":"2306.00980","DBLP":"conf/nips/LiWJHC0WTR23","DOI":"10.48550/arXiv.2306.00980","CorpusId":258999690},"title":"SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds"},{"paperId":"ea720398e02286b1ae55b2a21b52f45a00dca1ba","externalIds":{"DBLP":"conf/cvpr/SongZLCPZKA23","DOI":"10.1109/CVPR52729.2023.01756","CorpusId":260005035},"title":"ObjectStitch: Object Compositing with Diffusion Model"},{"paperId":"6eeabcc30f8ae13746220685a58ad6249705e732","externalIds":{"ArXiv":"2306.00783","DBLP":"conf/nips/ZhangDXTT23","CorpusId":258999493},"title":"FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models"},{"paperId":"dcf1a0693de432b477add54d95a1d67637cbe4d4","externalIds":{"DBLP":"conf/cvpr/LuTWNWS23","DOI":"10.1109/CVPR52729.2023.01371","CorpusId":261081082},"title":"Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style"},{"paperId":"0896c05cd6e2705338a714591af24cb8f2fa35d5","externalIds":{"DBLP":"journals/corr/abs-2306-08276","ArXiv":"2306.08276","DOI":"10.1109/CVPR52729.2023.00447","CorpusId":259164412},"title":"TryOnDiffusion: A Tale of Two UNets"},{"paperId":"e4afe0c8cf553fb115ba428f20015ac6dbf83c11","externalIds":{"DBLP":"journals/corr/abs-2306-00219","ArXiv":"2306.00219","DOI":"10.48550/arXiv.2306.00219","CorpusId":258999925},"title":"Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images"},{"paperId":"b34e36a9c8f7de7a4813c9c447dfa085d67cca11","externalIds":{"ArXiv":"2305.18676","DBLP":"journals/corr/abs-2305-18676","DOI":"10.1145/3610543.3626172","CorpusId":258967646},"title":"LayerDiffusion: Layered Controlled Image Editing with Diffusion Models"},{"paperId":"37e6f0f3c204a8094c9351e3b36570c4770cae70","externalIds":{"ArXiv":"2305.18007","DBLP":"conf/nips/LeeKH23","DOI":"10.48550/arXiv.2305.18007","CorpusId":258960627},"title":"Conditional Score Guidance for Text-Driven Image-to-Image Translation"},{"paperId":"a08e36ca94f702a6916010dabb09d8e8235ba6b7","externalIds":{"ArXiv":"2305.18047","DBLP":"journals/corr/abs-2305-18047","DOI":"10.48550/arXiv.2305.18047","CorpusId":258959425},"title":"InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions"},{"paperId":"477ae5324c206c35d4bde4fe3fad21c74349a723","externalIds":{"DBLP":"conf/nips/GuWZFXLZ00JW23","ArXiv":"2305.18286","DOI":"10.48550/arXiv.2305.18286","CorpusId":258960099},"title":"Photoswap: Personalized Subject Swapping in Images"},{"paperId":"5728ecb3a11c1586c4ae53e11ab395a0263eb5f4","externalIds":{"ArXiv":"2305.18292","DBLP":"conf/nips/GuWWSCFXZCWGSS23","DOI":"10.48550/arXiv.2305.18292","CorpusId":258960192},"title":"Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models"},{"paperId":"4c8504f06a0063bd32e7e78efd168549e1f6a7c6","externalIds":{"ArXiv":"2305.17489","DBLP":"journals/corr/abs-2305-17489","DOI":"10.1109/WACV57701.2024.00515","CorpusId":258960274},"title":"Text-to-image Editing by Image Information Removal"},{"paperId":"ae7a5b62dd71c356f7e98f3708959bb433dfc0ff","externalIds":{"DBLP":"journals/corr/abs-2305-17423","ArXiv":"2305.17423","DOI":"10.48550/arXiv.2305.17423","CorpusId":258960309},"title":"FISEdit: Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference"},{"paperId":"3a2424025d3c25a829e0a66b6e5698427745ca37","externalIds":{"DBLP":"journals/corr/abs-2305-16807","ArXiv":"2305.16807","DOI":"10.1109/WACV61041.2025.00207","CorpusId":258947366},"title":"Negative-Prompt Inversion: Fast Image Inversion for Editing with Text-Guided Diffusion Models"},{"paperId":"7ee8487178a0bb8927e97aff94d338d3b31097fe","externalIds":{"DBLP":"conf/cvpr/XuGWHES24","ArXiv":"2305.16223","DOI":"10.1109/CVPR52733.2024.00829","CorpusId":258887939},"title":"Prompt-Free Diffusion: Taking “Text” Out of Text-to-Image Diffusion Models"},{"paperId":"7657e124622858a970815a96991727884088d648","externalIds":{"ArXiv":"2305.15779","DBLP":"journals/corr/abs-2305-15779","DOI":"10.48550/arXiv.2305.15779","CorpusId":258888143},"title":"Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models"},{"paperId":"a483ff9557f29d21fe780b3dd969a037a3ffc3ed","externalIds":{"DBLP":"journals/corr/abs-2305-16322","ArXiv":"2305.16322","DOI":"10.48550/arXiv.2305.16322","CorpusId":258888112},"title":"Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models"},{"paperId":"7c4f6fd4c7eadcc7189a6797db215895340f93c7","externalIds":{"ArXiv":"2305.14742","DBLP":"journals/corr/abs-2305-14742","DOI":"10.48550/arXiv.2305.14742","CorpusId":258865947},"title":"ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation"},{"paperId":"0261949e6cb167d6dd1c2192302be4bb26addb44","externalIds":{"DBLP":"journals/tmm/HuangHLYDLCC24","ArXiv":"2305.13819","DOI":"10.1109/TMM.2024.3359769","CorpusId":258841180},"title":"WaveDM: Wavelet-Based Diffusion Models for Image Restoration"},{"paperId":"ea2feb30a758519672e876bd1ff6f05b859e308e","externalIds":{"DBLP":"journals/corr/abs-2305-12966","ArXiv":"2305.12966","DOI":"10.48550/arXiv.2305.12966","CorpusId":258832675},"title":"Hierarchical Integration Diffusion Model for Realistic Image Deblurring"},{"paperId":"e665e77f3996c3f3f71a421965db5927c6706c95","externalIds":{"ArXiv":"2305.13501","DBLP":"conf/mm/MorelliBCC0C23","DOI":"10.1145/3581783.3612137","CorpusId":258840871},"title":"LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On"},{"paperId":"05b15934d837dc84afa96824742d3dcc7ec88e09","externalIds":{"DBLP":"journals/corr/abs-2305-10973","ArXiv":"2305.10973","DOI":"10.1145/3588432.3591500","CorpusId":258762550},"title":"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold"},{"paperId":"f6fdac9b5e771394d22bfd5fbaf8147a52b6e792","externalIds":{"ArXiv":"2305.11147","DBLP":"conf/nips/QinZYFYZWNXSE0X23","DOI":"10.48550/arXiv.2305.11147","CorpusId":258762776},"title":"UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild"},{"paperId":"02bc11de9d2f75bad48166098aa6b30fffee4d70","externalIds":{"ArXiv":"2305.10474","DBLP":"journals/corr/abs-2305-10474","DOI":"10.1109/ICCV51070.2023.02096","CorpusId":258762178},"title":"Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models"},{"paperId":"315d7a58fada47c5729645f0af8ddfaa0743f82f","externalIds":{"ArXiv":"2305.07015","DBLP":"journals/ijcv/WangYZCL24","DOI":"10.1007/s11263-024-02168-7","CorpusId":258615282},"title":"Exploiting Diffusion Prior for Real-World Image Super-Resolution"},{"paperId":"9666180b59e6414f546ca4156a756c43d06207ab","externalIds":{"DBLP":"conf/mm/ZhaoZWLH023","ArXiv":"2305.06710","DOI":"10.1145/3581783.3612588","CorpusId":258615416},"title":"Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator"},{"paperId":"f1fac000b77ee817bc2f431fbc2626ed302131c7","externalIds":{"DBLP":"journals/corr/abs-2305-05947","ArXiv":"2305.05947","DOI":"10.1109/CVPRW63382.2024.00738","CorpusId":258587826},"title":"iEdit: Localised Text-guided Image Editing with Weak Supervision"},{"paperId":"cf7f48b90ae00a70122a2b7ab4de715821ff35e0","externalIds":{"DBLP":"journals/corr/abs-2305-06077","ArXiv":"2305.06077","DOI":"10.1109/ICCV51070.2023.00809","CorpusId":258588229},"title":"Relightify: Relightable 3D Faces from a Single Image via Diffusion Models"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","externalIds":{"DBLP":"journals/corr/abs-2305-05665","ArXiv":"2305.05665","DOI":"10.1109/CVPR52729.2023.01457","CorpusId":258564264},"title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"7d13ffca85dbda3ab943797d19b48559e385f7e8","externalIds":{"ArXiv":"2305.04441","DBLP":"conf/iccv/DongXDH23","DOI":"10.1109/ICCV51070.2023.00683","CorpusId":258556958},"title":"Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models"},{"paperId":"d4b68fffb7a060c08609443deb40c5758af84bfe","externalIds":{"DBLP":"journals/corr/abs-2305-04651","ArXiv":"2305.04651","DOI":"10.48550/arXiv.2305.04651","CorpusId":258557880},"title":"ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation"},{"paperId":"314047a5aad780af9efef7ebd4a41e6995666543","externalIds":{"DBLP":"conf/siggraph/TewelGCA23","ArXiv":"2305.01644","DOI":"10.1145/3588432.3591506","CorpusId":258436985},"title":"Key-Locked Rank One Editing for Text-to-Image Personalization"},{"paperId":"fb69dd274948fdb1565abcdc2ad74fd9ef3e84f2","externalIds":{"ArXiv":"2304.11829","DBLP":"journals/corr/abs-2304-11829","DOI":"10.1109/WACV57701.2024.00529","CorpusId":258298850},"title":"Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation"},{"paperId":"f5a0c57f90c6abe31482e9f320ccac5ee789b135","externalIds":{"ArXiv":"2304.08818","DBLP":"journals/corr/abs-2304-08818","DOI":"10.1109/CVPR52729.2023.02161","CorpusId":258187553},"title":"Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"},{"paperId":"8ad199f11f386319ebd2706c372562677c98fae3","externalIds":{"DBLP":"journals/corr/abs-2304-08477","ArXiv":"2304.08477","DOI":"10.48550/arXiv.2304.08477","CorpusId":258180320},"title":"Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation"},{"paperId":"9b8b9c4f742998d3730cc13b30313a95fc0077fd","externalIds":{"DBLP":"conf/cvpr/LuoGZSS23","ArXiv":"2304.08291","DOI":"10.1109/CVPRW59228.2023.00169","CorpusId":258179052},"title":"Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models"},{"paperId":"85963807c11abe38e9a2797d9860e012238607ef","externalIds":{"DBLP":"conf/iccv/CaoWQSQZ23","ArXiv":"2304.08465","DOI":"10.1109/ICCV51070.2023.02062","CorpusId":258179432},"title":"MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"398e91b99a4d988a39cadaa2a2c8ad9307615a5e","externalIds":{"DBLP":"conf/iccv/HertzAC23","ArXiv":"2304.07090","DOI":"10.1109/ICCV51070.2023.00221","CorpusId":258170014},"title":"Delta Denoising Score"},{"paperId":"72570016bbb28e8fb15ab4667eb84887f5dd35ad","externalIds":{"ArXiv":"2304.06790","DBLP":"journals/corr/abs-2304-06790","DOI":"10.48550/arXiv.2304.06790","CorpusId":258170322},"title":"Inpaint Anything: Segment Anything Meets Image Inpainting"},{"paperId":"940da2b97449e3aaa138570cebb7064a53210b11","externalIds":{"ArXiv":"2304.06720","DBLP":"journals/corr/abs-2304-06720","DOI":"10.1109/ICCV51070.2023.00694","CorpusId":258108187},"title":"Expressive Text-to-Image Generation with Rich Text"},{"paperId":"e966b71579cec41aa98d24d45ea2f6f76aceb7e2","externalIds":{"DBLP":"conf/cvpr/DingZXJTZ23","ArXiv":"2304.06711","DOI":"10.1109/CVPR52729.2023.01225","CorpusId":258108166},"title":"DiffusionRig: Learning Personalized Priors for Facial Appearance Editing"},{"paperId":"d1c33172c2ffbc038f0598f3ac56bb04af79c904","externalIds":{"DBLP":"conf/cvpr/Huberman-Spiegelglas24","ArXiv":"2304.06140","DOI":"10.1109/CVPR52733.2024.01185","CorpusId":258108162},"title":"An Edit Friendly DDPM Noise Space: Inversion and Manipulations"},{"paperId":"14fd4a6f0b9d6ac526427bbb4a1c86bc61d4e4e0","externalIds":{"ArXiv":"2304.04344","DBLP":"journals/corr/abs-2304-04344","DOI":"10.48550/arXiv.2304.04344","CorpusId":258049148},"title":"Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models"},{"paperId":"9733025aea2ba71792be10c18d635e8fc1455e31","externalIds":{"DBLP":"journals/corr/abs-2304-03411","ArXiv":"2304.03411","DOI":"10.1109/CVPR52733.2024.00816","CorpusId":258041269},"title":"InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning"},{"paperId":"e846aa9a93d706e942a685bd483f6efd0dd320ee","externalIds":{"ArXiv":"2304.03246","DBLP":"journals/corr/abs-2304-03246","DOI":"10.48550/arXiv.2304.03246","CorpusId":257985241},"title":"Inst-Inpaint: Instructing to Remove Objects with Diffusion Models"},{"paperId":"6bff8eb5205cae5c1916e51f54bab5d43c05bf77","externalIds":{"DBLP":"conf/icml/ZhangJZYJC23","ArXiv":"2304.03322","DOI":"10.48550/arXiv.2304.03322","CorpusId":258041305},"title":"Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"5549dc3ceff07561d9fb59610c0f78c71617901a","externalIds":{"DBLP":"conf/cvpr/FeiLPZYLZ023","ArXiv":"2304.01247","DOI":"10.1109/CVPR52729.2023.00958","CorpusId":257921922},"title":"Generative Diffusion Prior for Unified Image Restoration and Enhancement"},{"paperId":"83b8e18488d8f31dd017ec0b26531cef4b635b36","externalIds":{"DBLP":"conf/nips/ChenHLRJCC23","ArXiv":"2304.00186","DOI":"10.48550/arXiv.2304.00186","CorpusId":257913352},"title":"Subject-driven Text-to-Image Generation via Apprenticeship Learning"},{"paperId":"7397eb511e866e31c49ae53e4949780f3e5cab66","externalIds":{"ArXiv":"2304.09748","DBLP":"journals/corr/abs-2304-09748","DOI":"10.48550/arXiv.2304.09748","CorpusId":258212902},"title":"Reference-based Image Composition with Sketch via Structure-aware Diffusion Model"},{"paperId":"8e1ad8e0f4b86db4ac986f1c493d4ae673b63fed","externalIds":{"ArXiv":"2303.17905","DBLP":"journals/corr/abs-2303-17905","DOI":"10.1109/ICCV51070.2023.00226","CorpusId":257900655},"title":"3D-aware Image Generation using 2D Diffusion Models"},{"paperId":"a0a62943b8c462225844823110db8f2544feb30f","externalIds":{"DBLP":"journals/ijcv/LiuGZZWLZ25","ArXiv":"2303.16491","DOI":"10.1007/s11263-025-02462-y","CorpusId":257804739},"title":"Implicit Diffusion Models for Continuous Super-Resolution"},{"paperId":"ce13af4d467c4b01c4af570bd154317ae25ec892","externalIds":{"ArXiv":"2303.15649","DBLP":"journals/corr/abs-2303-15649","DOI":"10.26599/CVM.2025.9450462","CorpusId":257771440},"title":"StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing"},{"paperId":"6bdfd57293e318bd07d2d3b6929f4a57ccbfeb46","externalIds":{"DBLP":"conf/wacv/JeongKU24","ArXiv":"2303.15403","DOI":"10.1109/WACV57701.2024.00507","CorpusId":257766537},"title":"Training-free Content Injection using h-space in Diffusion Models"},{"paperId":"6f27c9ee95c249f36597daaeaca41dd3f3f2c769","externalIds":{"ArXiv":"2303.14353","DBLP":"journals/corr/abs-2303-14353","DOI":"10.48550/arXiv.2303.14353","CorpusId":257767119,"PubMed":"39416230"},"title":"DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency"},{"paperId":"91db703044115e686a3109532c08ce35592d7ce9","externalIds":{"DBLP":"journals/corr/abs-2303-13756","ArXiv":"2303.13756","DOI":"10.1109/CVPR52729.2023.02255","CorpusId":257757040},"title":"GP-VTON: Towards General Purpose Virtual Try-On via Collaborative Local-Flow Global-Parsing Learning"},{"paperId":"0c7a877e7952785216767ad9fa3b850fe77cce52","externalIds":{"ACL":"2023.acl-long.73","DBLP":"conf/acl/YinWYWWNYLL0FGW23","ArXiv":"2303.12346","DOI":"10.48550/arXiv.2303.12346","CorpusId":257663639},"title":"NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation"},{"paperId":"1f02ba1c6fae779ec3d003340e72eaf82351cfb9","externalIds":{"DBLP":"conf/iccv/HuLKWOKS23","ArXiv":"2303.11897","DOI":"10.1109/ICCV51070.2023.01866","CorpusId":257636562},"title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering"},{"paperId":"f335f50b2f1979ec151caecb33cb8cdc5afd48aa","externalIds":{"ArXiv":"2303.11306","DBLP":"conf/iccv/PatashnikGAAC23","DOI":"10.1109/ICCV51070.2023.02107","CorpusId":257632209},"title":"Localizing Object-level Shape Variations with Text-to-Image Diffusion Models"},{"paperId":"6dc7300018597a0c9e01dfade23591e89b038adf","externalIds":{"DBLP":"journals/corr/abs-2303-10073","ArXiv":"2303.10073","DOI":"10.48550/arXiv.2303.10073","CorpusId":257623108},"title":"DialogPaint: A Dialog-based Image Editing Model"},{"paperId":"372bc41602bbd21f192305775f0a58de9880e454","externalIds":{"DBLP":"conf/cvpr/0007YFQCYC0SEXX24","ArXiv":"2303.09618","DOI":"10.1109/CVPR52733.2024.00862","CorpusId":257622925},"title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing"},{"paperId":"02231a0d3f0e29f0bb69b74e2634e86c5753fb48","externalIds":{"DBLP":"journals/corr/abs-2303-09642","ArXiv":"2303.09642","DOI":"10.48550/arXiv.2303.09642","CorpusId":257622938},"title":"SUD2: Supervision by Denoising Diffusion Models for Image Reconstruction"},{"paperId":"67909a17f9c9467de536aa2cf7b0864dc6215e96","externalIds":{"DBLP":"journals/corr/abs-2303-09472","ArXiv":"2303.09472","DOI":"10.1109/ICCV51070.2023.01204","CorpusId":257557425},"title":"DiffIR: Efficient Diffusion Model for Image Restoration"},{"paperId":"1b8452be196d943241c76c170e93ea1d49aa07ea","externalIds":{"ArXiv":"2303.08714","DBLP":"journals/corr/abs-2303-08714","DOI":"10.48550/arXiv.2303.08714","CorpusId":257532290},"title":"ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution"},{"paperId":"35ccd924de9e8483bdcf144cbf2edf09be157b7e","externalIds":{"ArXiv":"2303.07909","DBLP":"journals/corr/abs-2303-07909","DOI":"10.48550/arXiv.2303.07909","CorpusId":257505012},"title":"Text-to-image Diffusion Models in Generative AI: A Survey"},{"paperId":"ba84fa079c2e2e16a255f65367b75ecba2e806b7","externalIds":{"DBLP":"journals/corr/abs-2303-05125","ArXiv":"2303.05125","DOI":"10.48550/arXiv.2303.05125","CorpusId":257427549},"title":"Cones: Concept Neurons in Diffusion Models for Customized Generation"},{"paperId":"ac974291d7e3a152067382675524f3e3c2ded11b","externalIds":{"DBLP":"conf/icml/SongD0S23","ArXiv":"2303.01469","DOI":"10.1007/978-1-4842-1329-2_9","CorpusId":257280191},"title":"Consistency Models"},{"paperId":"e4c15256a8e0ff8df225e76465bb875b56875868","externalIds":{"DBLP":"journals/corr/abs-2303-00262","ArXiv":"2303.00262","DOI":"10.1109/WACV57701.2024.00416","CorpusId":257255250},"title":"Collage Diffusion"},{"paperId":"e15900cf7c93d4b6e45a12fe3534840c910467e1","externalIds":{"DBLP":"journals/corr/abs-2302-13848","ArXiv":"2302.13848","DOI":"10.1109/ICCV51070.2023.01461","CorpusId":257219968},"title":"ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation"},{"paperId":"cdd53edde3c989381bb4e71aec4bea0a2755d3fe","externalIds":{"ArXiv":"2302.11797","DBLP":"journals/corr/abs-2302-11797","DOI":"10.48550/arXiv.2302.11797","CorpusId":257102889},"title":"Region-Aware Diffusion for Zero-shot Text-driven Image Editing"},{"paperId":"26e5b933b8f60bd749d428b5ff813b2abcd765d8","externalIds":{"ArXiv":"2302.09778","DBLP":"conf/icml/HuangC0SZZ23","DOI":"10.48550/arXiv.2302.09778","CorpusId":257038979},"title":"Composer: Creative and Controllable Image Synthesis with Composable Conditions"},{"paperId":"58842cdca3ea68f7b9e638b288fc247a6f26dafc","externalIds":{"DBLP":"conf/aaai/MouWXW0QS24","ArXiv":"2302.08453","DOI":"10.48550/arXiv.2302.08453","CorpusId":256900833},"title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models"},{"paperId":"7dd7e87772799a62a6d4dbe26616c3b5eb7ae72c","externalIds":{"ArXiv":"2302.07979","DBLP":"journals/corr/abs-2302-07979","DOI":"10.48550/arXiv.2302.07979","CorpusId":256900814},"title":"PRedItOR: Text Guided Image Editing with Diffusion Prior"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"3821c2d78758084cfbdd5071e7d6d31a151c10e8","externalIds":{"ArXiv":"2302.04841","DBLP":"conf/nips/VoronovKBR23","CorpusId":259262648},"title":"Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics"},{"paperId":"07be0ec1f45e21a1032616535d0290ee6bfe0f6b","externalIds":{"DBLP":"conf/iccv/EsserCAGG23","ArXiv":"2302.03011","DOI":"10.1109/ICCV51070.2023.00675","CorpusId":256615582},"title":"Structure and Content-Guided Video Synthesis with Diffusion Models"},{"paperId":"daf61010eee0fbf6f9bab7db71c395ffca6f3ff3","externalIds":{"ArXiv":"2302.03027","DBLP":"conf/siggraph/ParmarS0LLZ23","DOI":"10.1145/3588432.3591513","CorpusId":256616002},"title":"Zero-shot Image-to-Image Translation"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"1a984acf57d7d6dd2aa3da0ea1e563598ffad9bd","externalIds":{"DBLP":"journals/corr/abs-2301-12247","DOI":"10.48550/arXiv.2301.12247","CorpusId":256390079},"title":"SEGA: Instructing Diffusion using Semantic Dimensions"},{"paperId":"c9217c50acce84e417bbc13a4ecaa06db0c77026","externalIds":{"ArXiv":"2301.11699","DBLP":"conf/icml/LuoGZSS23","DOI":"10.48550/arXiv.2301.11699","CorpusId":256358842},"title":"Image Restoration with Mean-Reverting Stochastic Differential Equations"},{"paperId":"994a1ce6677b496bd3c0c63aceafc6556005e994","externalIds":{"DBLP":"conf/cvpr/LiLWMYGLL23","ArXiv":"2301.07093","DOI":"10.1109/CVPR52729.2023.02156","CorpusId":255942528},"title":"GLIGEN: Open-Set Grounded Text-to-Image Generation"},{"paperId":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","externalIds":{"DBLP":"conf/iccv/WuGWLGSHSQS23","ArXiv":"2212.11565","DOI":"10.1109/ICCV51070.2023.00701","CorpusId":254974187},"title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"b916302feec5be76fc4aa935202008f6ae638efa","externalIds":{"ArXiv":"2212.08698","DBLP":"journals/corr/abs-2212-08698","DOI":"10.1109/CVPR52729.2023.00189","CorpusId":254854155},"title":"Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models"},{"paperId":"32e96b5e0472f00a9a8895f15b272541ca91553f","externalIds":{"DBLP":"journals/corr/abs-2212-07603","ArXiv":"2212.07603","DOI":"10.1109/ICME55011.2023.00473","CorpusId":254685897},"title":"Text-Guided Mask-Free Local Image Retouching"},{"paperId":"0f4349d90feed96087c6a7b627617bedce0cfb12","externalIds":{"DBLP":"journals/pami/YueL24","ArXiv":"2212.06512","DOI":"10.1109/TPAMI.2024.3432651","CorpusId":254591838,"PubMed":"39042531"},"title":"DifFace: Blind Face Restoration With Diffused Error Contraction"},{"paperId":"f1c39410893794ee3643efa85be1816964aa85ea","externalIds":{"DBLP":"conf/cvpr/0001SMPNPOLFSB023","ArXiv":"2212.06909","DOI":"10.1109/CVPR52729.2023.01761","CorpusId":254636532},"title":"Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting"},{"paperId":"cf90b380e0ad61d4e980e674255b6f6dc7e3a071","externalIds":{"DBLP":"journals/corr/abs-2212-06013","ArXiv":"2212.06013","DOI":"10.48550/arXiv.2212.06013","CorpusId":254564528},"title":"The Stable Artist: Steering Semantics in Diffusion Latent Space"},{"paperId":"afd674940791fb2daeecb0801a5068d80d9f77a7","externalIds":{"DBLP":"conf/cvpr/GuoWYHWPW23","ArXiv":"2212.04711","DOI":"10.1109/CVPR52729.2023.01350","CorpusId":254535902},"title":"ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal"},{"paperId":"95ca7eb71b2aea74d7bbe23252e41c03070b807d","externalIds":{"DBLP":"journals/corr/abs-2212-05034","ArXiv":"2212.05034","DOI":"10.1109/CVPR52729.2023.02148","CorpusId":254535802},"title":"SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model"},{"paperId":"a6ad30123bef4b19ee40c3d63cfabf00d211f0ef","externalIds":{"DBLP":"conf/cvpr/ZhangHGMR23","ArXiv":"2212.04489","DOI":"10.1109/CVPR52729.2023.00584","CorpusId":254408758},"title":"SINE: SINgle Image Editing with Text-to-Image Diffusion Models"},{"paperId":"144eca44e250cc462f6fc3a172abb865978f66f5","externalIds":{"DBLP":"conf/cvpr/KumariZ0SZ23","ArXiv":"2212.04488","DOI":"10.1109/CVPR52729.2023.00192","CorpusId":254408780},"title":"Multi-Concept Customization of Text-to-Image Diffusion"},{"paperId":"201eee156cf6b4bbd0b1be04840f984457b2d646","externalIds":{"DBLP":"journals/corr/abs-2212-02024","ArXiv":"2212.02024","DOI":"10.48550/arXiv.2212.02024","CorpusId":254246927},"title":"Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models"},{"paperId":"074c9b84e86b70c02cd1fa0d65204fe07ecdd849","externalIds":{"DBLP":"journals/corr/abs-2212-00210","ArXiv":"2212.00210","DOI":"10.1109/WACV57701.2024.00415","CorpusId":254125549},"title":"Shape-Guided Diffusion with Inside-Outside Attention"},{"paperId":"3a75ed3e9e81c9db573ef73d20e2c66c12aaedf8","externalIds":{"ArXiv":"2212.00490","DBLP":"journals/corr/abs-2212-00490","DOI":"10.48550/arXiv.2212.00490","CorpusId":254125609},"title":"Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model"},{"paperId":"cff3337f669d615c554b6fb1806e4a84fa0bdee6","externalIds":{"DBLP":"journals/corr/abs-2211-16152","ArXiv":"2211.16152","DOI":"10.1109/CVPR52729.2023.00983","CorpusId":254069706},"title":"Wavelet Diffusion Models are fast and scalable Image Generators"},{"paperId":"35c7586457f8a158a967a3d73207ee8ecd1e8eb6","externalIds":{"DBLP":"journals/corr/abs-2211-14305","ArXiv":"2211.14305","DOI":"10.1109/CVPR52729.2023.01762","CorpusId":254018089},"title":"SpaText: Spatio-Textual Representation for Controllable Image Generation"},{"paperId":"7c12d091ea5c16938943559fb0f3d430ba040c08","externalIds":{"DBLP":"journals/corr/abs-2211-13524","ArXiv":"2211.13524","DOI":"10.48550/arXiv.2211.13524","CorpusId":254018290},"title":"GAN Prior based Null-Space Learning for Consistent Super-Resolution"},{"paperId":"4f1502111d35aa6651dfaedfeb1184b3c3dd2fcb","externalIds":{"DBLP":"journals/corr/abs-2211-13227","ArXiv":"2211.13227","DOI":"10.1109/CVPR52729.2023.01763","CorpusId":253802085},"title":"Paint by Example: Exemplar-based Image Editing with Diffusion Models"},{"paperId":"831c240d7725b8e4ba3e4039f16a693253fab2ab","externalIds":{"DBLP":"conf/cvpr/ZhangHTHMDX23","ArXiv":"2211.13203","DOI":"10.1109/CVPR52729.2023.00978","CorpusId":257427673},"title":"Inversion-based Style Transfer with Diffusion Models"},{"paperId":"7e80f79472d9b5aaa109075910d3ff9f9149f4c9","externalIds":{"DBLP":"journals/corr/abs-2211-12446","ArXiv":"2211.12446","DOI":"10.1109/CVPR52729.2023.02158","CorpusId":253761481},"title":"EDICT: Exact Diffusion Inversion via Coupled Transformations"},{"paperId":"b000d6865db824af1563708fb7a545ddd65c6b3a","externalIds":{"DBLP":"conf/cvpr/TumanyanGBD23","ArXiv":"2211.12572","DOI":"10.1109/CVPR52729.2023.00191","CorpusId":253801961},"title":"Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation"},{"paperId":"ac2d6f80ad17f638010bd09d30ade52f52803951","externalIds":{"ArXiv":"2211.11319","DBLP":"journals/corr/abs-2211-11319","DOI":"10.1109/CVPR52729.2023.00190","CorpusId":253734791},"title":"VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models"},{"paperId":"94b690162ead76af6a487d6e10998ea585c035d1","externalIds":{"DBLP":"journals/corr/abs-2211-11018","ArXiv":"2211.11018","DOI":"10.48550/arXiv.2211.11018","CorpusId":253735209},"title":"MagicVideo: Efficient Video Generation With Latent Diffusion Models"},{"paperId":"662ca5c9ddaae1e5accaa69f588122249a156740","externalIds":{"DBLP":"journals/tnn/HuangZTMHDX25","ArXiv":"2211.10682","DOI":"10.1109/TNNLS.2023.3342645","CorpusId":253735322,"PubMed":"38198263"},"title":"DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization"},{"paperId":"a2d2bbe4c542173662a444b33b76c66992697830","externalIds":{"DBLP":"conf/cvpr/BrooksHE23","ArXiv":"2211.09800","DOI":"10.1109/CVPR52729.2023.01764","CorpusId":253581213},"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions"},{"paperId":"97029b53d0252ea68472423dea33e5aa2316926d","externalIds":{"DBLP":"conf/iccv/XuWZWS23","ArXiv":"2211.08332","DOI":"10.1109/ICCV51070.2023.00713","CorpusId":253523371},"title":"Versatile Diffusion: Text, Images and Variations All in One Diffusion Model"},{"paperId":"a4be3489d7dba5d286cf00b698f267b65ebd7aaf","externalIds":{"ArXiv":"2211.07825","DBLP":"journals/corr/abs-2211-07825","DOI":"10.48550/arXiv.2211.07825","CorpusId":253523448},"title":"Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models"},{"paperId":"0231f2aed9a96cb516242fb57f2cb63f5651c4d8","externalIds":{"DBLP":"journals/corr/abs-2211-05105","ArXiv":"2211.05105","DOI":"10.1109/CVPR52729.2023.02157","CorpusId":253420366},"title":"Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models"},{"paperId":"e24f4b28167b05fbf7d29000490fc0a4e4c109c7","externalIds":{"ArXiv":"2211.01324","DBLP":"journals/corr/abs-2211-01324","DOI":"10.48550/arXiv.2211.01324","CorpusId":253254800},"title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers"},{"paperId":"6eed3f71e6274d7083b0c193561f0699db76800b","externalIds":{"DBLP":"journals/corr/abs-2210-12965","ArXiv":"2210.12965","DOI":"10.48550/arXiv.2210.12965","CorpusId":253098221},"title":"High-Resolution Image Editing via Multi-Stage Blended Diffusion"},{"paperId":"a02313d56a6f71be9aafe43628e0f3a1d0cb858e","externalIds":{"DBLP":"journals/corr/abs-2210-10960","ArXiv":"2210.10960","DOI":"10.48550/arXiv.2210.10960","CorpusId":253018703},"title":"Diffusion Models already have a Semantic Latent Space"},{"paperId":"064ccebc03d3afabaae30fe29a457c1cfcdff7e3","externalIds":{"ArXiv":"2210.11427","DBLP":"conf/iclr/CouaironVSC23","DOI":"10.48550/arXiv.2210.11427","CorpusId":253018768},"title":"DiffEdit: Diffusion-based semantic image editing with mask guidance"},{"paperId":"23e261a20a315059b4de5492ed071c97a20c12e7","externalIds":{"ArXiv":"2210.09276","DBLP":"journals/corr/abs-2210-09276","DOI":"10.1109/CVPR52729.2023.00582","CorpusId":252918469},"title":"Imagic: Text-Based Real Image Editing with Diffusion Models"},{"paperId":"f170754f8ab3187514292c12b1cbb431c0a8a634","externalIds":{"ArXiv":"2210.09292","DBLP":"journals/corr/abs-2210-09292","DOI":"10.48550/arXiv.2210.09292","CorpusId":252918532},"title":"Efficient Diffusion Models for Vision: A Survey"},{"paperId":"625d57bd52c60cd79aa4add6c4420dc2ad3b808a","externalIds":{"DBLP":"journals/corr/abs-2210-03142","ArXiv":"2210.03142","DOI":"10.1109/CVPR52729.2023.01374","CorpusId":252762155},"title":"On Distillation of Guided Diffusion Models"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"52472459ea81b6ebc65d16a0c80005f749542cba","externalIds":{"DBLP":"conf/iclr/KwonY23","ArXiv":"2209.15264","DOI":"10.48550/arXiv.2209.15264","CorpusId":252668838},"title":"Diffusion-based Image Translation using Disentangled Style and Content Representation"},{"paperId":"61e46884567be7cad12e999365b16a8d3414b678","externalIds":{"DBLP":"conf/iclr/ChungKMKY23","ArXiv":"2209.14687","DOI":"10.48550/arXiv.2209.14687","CorpusId":252596252},"title":"Diffusion Posterior Sampling for General Noisy Inverse Problems"},{"paperId":"1e33716e8820b867d5a8aaebab44c2d3135ea4ac","externalIds":{"DBLP":"conf/iclr/SingerPH00ZHYAG23","ArXiv":"2209.14792","CorpusId":252595919},"title":"Make-A-Video: Text-to-Video Generation without Text-Video Data"},{"paperId":"efa1647594b236361610a20d507127f0586a379b","externalIds":{"DBLP":"journals/corr/abs-2209-04747","ArXiv":"2209.04747","DOI":"10.1109/TPAMI.2023.3261988","CorpusId":252199918,"PubMed":"37030794"},"title":"Diffusion Models in Vision: A Survey"},{"paperId":"35a29c47d5292e8967e5a9a8a21b23d8637b7d07","externalIds":{"DBLP":"journals/tkde/CaoTGXCHL24","ArXiv":"2209.02646","DOI":"10.1109/TKDE.2024.3361474","CorpusId":265039918},"title":"A Survey on Generative Diffusion Models"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","externalIds":{"DBLP":"journals/corr/abs-2209-00796","ArXiv":"2209.00796","DOI":"10.1145/3626235","CorpusId":252070859},"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744","externalIds":{"ArXiv":"2208.12242","DBLP":"conf/cvpr/RuizLJPRA23","DOI":"10.1109/CVPR52729.2023.02155","CorpusId":251800180},"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"paperId":"525f459f369032e2f2fa3eb1d60da34ab99191bc","externalIds":{"ArXiv":"2208.09392","DBLP":"journals/corr/abs-2208-09392","DOI":"10.48550/arXiv.2208.09392","CorpusId":251710469},"title":"Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise"},{"paperId":"c0eebe98b3ca4fbccfeb7fea51d4d92b7ef4cbfd","externalIds":{"DBLP":"conf/nips/GuthCBM22","ArXiv":"2208.05003","DOI":"10.48550/arXiv.2208.05003","CorpusId":251468170},"title":"Wavelet Score-Based Generative Modeling"},{"paperId":"5406129d9d7d00dc310671c43597101b0ee93629","externalIds":{"ArXiv":"2208.01618","DBLP":"journals/corr/abs-2208-01618","DOI":"10.48550/arXiv.2208.01618","CorpusId":251253049},"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"},{"paperId":"04e541391e8dce14d099d00fb2c21dbbd8afe87f","externalIds":{"DBLP":"journals/corr/abs-2208-01626","ArXiv":"2208.01626","DOI":"10.48550/arXiv.2208.01626","CorpusId":251252882},"title":"Prompt-to-Prompt Image Editing with Cross Attention Control"},{"paperId":"69beb616ffdc42afe86b7487c5db82b6d88638b8","externalIds":{"DBLP":"journals/pami/OzdenizciL23","ArXiv":"2207.14626","DOI":"10.1109/TPAMI.2023.3238179","CorpusId":251197000,"PubMed":"37021892"},"title":"Restoring Vision in Adverse Weather Conditions With Patch-Based Denoising Diffusion Models"},{"paperId":"0270ec4bc946b59c5cf6204be2553682dee0346c","externalIds":{"DBLP":"journals/corr/abs-2207-13038","ArXiv":"2207.13038","DOI":"10.48550/arXiv.2207.13038","CorpusId":251066705},"title":"Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models"},{"paperId":"af9f365ed86614c800f082bd8eb14be76072ad16","externalIds":{"DBLP":"journals/corr/abs-2207-12598","ArXiv":"2207.12598","DOI":"10.48550/arXiv.2207.12598","CorpusId":249145348},"title":"Classifier-Free Diffusion Guidance"},{"paperId":"f9583f36414287bd4b7f34a9b178aa9cc3cd471a","externalIds":{"ArXiv":"2207.08208","DBLP":"journals/corr/abs-2207-08208","DOI":"10.1109/TMI.2023.3290149","CorpusId":250627054,"PubMed":"37379177"},"title":"Unsupervised Medical Image Translation With Adversarial Diffusion Models"},{"paperId":"dd7b8d097cf6ee0d42d200b2019912af582b31dd","externalIds":{"DBLP":"journals/corr/abs-2207-06635","ArXiv":"2207.06635","DOI":"10.48550/arXiv.2207.06635","CorpusId":250526607},"title":"EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations"},{"paperId":"ef669bb2d0a3e957a91c1dde85ce01c6984ad7d6","externalIds":{"DBLP":"journals/tog/AvrahamiFL23","ArXiv":"2206.02779","DOI":"10.1145/3592450","CorpusId":249394540},"title":"Blended Latent Diffusion"},{"paperId":"b3f5cf32178bcbed91aa5303b70963c6463f48a2","externalIds":{"DBLP":"conf/nips/ChungSRY22","ArXiv":"2206.00941","DOI":"10.48550/arXiv.2206.00941","CorpusId":249282628},"title":"Improving Diffusion Models for Inverse Problems using Manifold Constraints"},{"paperId":"4530c25da949bb2185c50663158ef19d52e3c6b5","externalIds":{"DBLP":"conf/nips/0011ZB0L022","ArXiv":"2206.00927","DOI":"10.48550/arXiv.2206.00927","CorpusId":249282317},"title":"DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps"},{"paperId":"32b3553d7dc8a263c63d32eeec2916d1647ab178","externalIds":{"DBLP":"journals/corr/abs-2206-00386","ArXiv":"2206.00386","DOI":"10.48550/arXiv.2206.00386","CorpusId":249240430},"title":"DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder"},{"paperId":"2f4c451922e227cbbd4f090b74298445bbd900d0","externalIds":{"DBLP":"journals/corr/abs-2206-00364","ArXiv":"2206.00364","DOI":"10.48550/arXiv.2206.00364","CorpusId":249240415},"title":"Elucidating the Design Space of Diffusion-Based Generative Models"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","externalIds":{"DBLP":"conf/nips/HoSGC0F22","ArXiv":"2204.03458","DOI":"10.48550/arXiv.2204.03458","CorpusId":248006185},"title":"Video Diffusion Models"},{"paperId":"183aec8376b39ae2a1707a436266cddaf8f05596","externalIds":{"DBLP":"conf/iclr/ChaoSCLCLCCL22","ArXiv":"2203.14206","DOI":"10.48550/arXiv.2203.14206","CorpusId":247763065},"title":"Denoising Likelihood Score Matching for Conditional Score-based Data Generation"},{"paperId":"15e234a67f30d6761f1d7670d501095d1697b69c","externalIds":{"DBLP":"conf/eccv/GafniPASPT22","ArXiv":"2203.13131","DOI":"10.48550/arXiv.2203.13131","CorpusId":247628171},"title":"Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors"},{"paperId":"9dc481ec44178e797466bbad968071917842156b","externalIds":{"DBLP":"journals/corr/abs-2203-03605","ArXiv":"2203.03605","DOI":"10.48550/arXiv.2203.03605","CorpusId":247292561},"title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"},{"paperId":"7f3ceb7e47038336344eb90471f97e83fffd16a5","externalIds":{"DBLP":"journals/pami/YoungDFGMFI25","ArXiv":"2202.02952","DOI":"10.1109/TPAMI.2023.3299789","CorpusId":258236522,"PubMed":"37505997"},"title":"Supervision by Denoising"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7","externalIds":{"ArXiv":"2201.11793","DBLP":"journals/corr/abs-2201-11793","CorpusId":246411364},"title":"Denoising Diffusion Restoration Models"},{"paperId":"1e91fa21b890a8f5d615578f4ddf46c3cb394691","externalIds":{"ArXiv":"2201.09865","DBLP":"journals/corr/abs-2201-09865","DOI":"10.1109/CVPR52688.2022.01117","CorpusId":246240274},"title":"RePaint: Inpainting using Denoising Diffusion Probabilistic Models"},{"paperId":"9b7b218b0f4e14f97260b6192add37da5e9ae2c5","externalIds":{"DBLP":"journals/corr/abs-2201-06503","ArXiv":"2201.06503","CorpusId":246016304},"title":"Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models"},{"paperId":"86b42cac364985919987789795be7c3a577ee3de","externalIds":{"DBLP":"journals/corr/abs-2201-02605","ArXiv":"2201.02605","DOI":"10.1007/978-3-031-20077-9_21","CorpusId":245827815},"title":"Detecting Twenty-thousand Classes using Image-level Supervision"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"7002ae048e4b8c9133a55428441e8066070995cb","externalIds":{"ArXiv":"2112.10741","DBLP":"journals/corr/abs-2112-10741","CorpusId":245335086},"title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"},{"paperId":"e77c484af99fc1eb3d3c36699ac81822e98cb74d","externalIds":{"DBLP":"conf/cvpr/LuddeckeE22","ArXiv":"2112.10003","DOI":"10.1109/CVPR52688.2022.00695","CorpusId":247794227},"title":"Image Segmentation Using Text and Image Prompts"},{"paperId":"dc563ae2179a79a3b58749bc142a719e009d6cc5","externalIds":{"DBLP":"journals/corr/abs-2112-05744","ArXiv":"2112.05744","DOI":"10.1109/WACV56688.2023.00037","CorpusId":245117331},"title":"More Control for Free! Image Synthesis with Semantic Diffusion Guidance"},{"paperId":"b582edb16f5425642767cb6c26839111f867f4dc","externalIds":{"DBLP":"journals/corr/abs-2111-15640","ArXiv":"2111.15640","DOI":"10.1109/CVPR52688.2022.01036","CorpusId":244729224},"title":"Diffusion Autoencoders: Toward a Meaningful and Decodable Representation"},{"paperId":"414e554d281d529401c873cb9c97186365ec5dd8","externalIds":{"ArXiv":"2111.14822","DBLP":"journals/corr/abs-2111-14822","DOI":"10.1109/CVPR52688.2022.01043","CorpusId":244714856},"title":"Vector Quantized Diffusion Model for Text-to-Image Synthesis"},{"paperId":"88e8801e4daf404d3d40f1648ef29faeb8e6d58a","externalIds":{"ArXiv":"2111.14818","DBLP":"conf/cvpr/AvrahamiLF22","DOI":"10.1109/CVPR52688.2022.01767","CorpusId":244714366},"title":"Blended Diffusion for Text-driven Editing of Natural Images"},{"paperId":"35356feaaf1a739a7db2b76f32e3e5a71ec74eb5","externalIds":{"DBLP":"journals/corr/abs-2111-13606","ArXiv":"2111.13606","CorpusId":244709128},"title":"Conditional Image Generation with Score-Based Diffusion Models"},{"paperId":"37c9c4e7648f639c0b36f150fc6c6c90b3682f4a","externalIds":{"DBLP":"journals/corr/abs-2111-05826","ArXiv":"2111.05826","DOI":"10.1145/3528233.3530757","CorpusId":243938678},"title":"Palette: Image-to-Image Diffusion Models"},{"paperId":"8f8dedb511c0324d1cb7f9750560109ca9290b5f","externalIds":{"ArXiv":"2110.02711","DBLP":"conf/cvpr/KimKY22a","DOI":"10.1109/CVPR52688.2022.00246","CorpusId":244909410},"title":"DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation"},{"paperId":"2f1103a039c4511a111b506fdbe980a4f34b6709","externalIds":{"DBLP":"conf/iccv/Zeng0LP21","DOI":"10.1109/ICCV48922.2021.01390","CorpusId":244072324},"title":"CR-Fill: Generative Image Inpainting with Auxiliary Contextual Reconstruction"},{"paperId":"fdf7012ebe9d4c4d2d93004613e7a19f69a83a93","externalIds":{"ArXiv":"2109.07161","DBLP":"journals/corr/abs-2109-07161","DOI":"10.1109/WACV51458.2022.00323","CorpusId":237513361},"title":"Resolution-robust Large Mask Inpainting with Fourier Convolutions"},{"paperId":"cda3fbbac6734b603bee363b0938e9baa924aa78","externalIds":{"DBLP":"journals/corr/abs-2108-02938","ArXiv":"2108.02938","DOI":"10.1109/iccv48922.2021.01410","CorpusId":236950721},"title":"ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4","externalIds":{"DBLP":"journals/jmlr/HoSCFNS22","ArXiv":"2106.15282","CorpusId":235619773},"title":"Cascaded Diffusion Models for High Fidelity Image Generation"},{"paperId":"64ea8f180d0682e6c18d1eb688afdb2027c02794","externalIds":{"ArXiv":"2105.05233","DBLP":"journals/corr/abs-2105-05233","CorpusId":234357997},"title":"Diffusion Models Beat GANs on Image Synthesis"},{"paperId":"d0cefc4621a55540c77e8ade9b8ae1bfeb530f42","externalIds":{"DBLP":"journals/corr/abs-2104-14951","ArXiv":"2104.14951","DOI":"10.1016/j.neucom.2022.01.029","CorpusId":233476433},"title":"SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models"},{"paperId":"38b0567e83386ddc294d6c81b541deacbd8e3c2a","externalIds":{"DBLP":"conf/emnlp/HesselHFBC21","ACL":"2021.emnlp-main.595","ArXiv":"2104.08718","DOI":"10.18653/v1/2021.emnlp-main.595","CorpusId":233296711},"title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning"},{"paperId":"bc7e6165b00f0c39d40ca2c7a4eb33fcc0e3200d","externalIds":{"DBLP":"journals/pami/SahariaHCSFN23","ArXiv":"2104.07636","DOI":"10.1109/TPAMI.2022.3204461","CorpusId":233241040,"PubMed":"36094974"},"title":"Image Super-Resolution via Iterative Refinement"},{"paperId":"bc5b3be970609bf44d8fd51db199406e1aa69473","externalIds":{"ArXiv":"2104.05358","DBLP":"journals/corr/abs-2104-05358","CorpusId":233210328},"title":"UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models"},{"paperId":"aaa99de83292370a964fcaa51e6e866a96726bb2","externalIds":{"ArXiv":"2103.17249","DBLP":"conf/iccv/PatashnikWSCL21","DOI":"10.1109/ICCV48922.2021.00209","CorpusId":232428282},"title":"StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery"},{"paperId":"0887d1edc0a9c7a0e0ed60e8c4d22406af679b64","externalIds":{"DBLP":"conf/cvpr/GeSZG0021","ArXiv":"2103.04559","DOI":"10.1109/CVPR46437.2021.00838","CorpusId":232147187},"title":"Parser-Free Virtual Try-on via Distilling Appearance Flows"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"de18baa4964804cf471d85a5a090498242d2e79f","externalIds":{"ArXiv":"2102.09672","DBLP":"conf/icml/NicholD21","CorpusId":231979499},"title":"Improved Denoising Diffusion Probabilistic Models"},{"paperId":"47f7ec3d0a5e6e83b6768ece35206a94dc81919c","externalIds":{"ArXiv":"2012.09841","MAG":"3111551570","DBLP":"journals/corr/abs-2012-09841","DOI":"10.1109/CVPR46437.2021.01268","CorpusId":229297973},"title":"Taming Transformers for High-Resolution Image Synthesis"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"014576b866078524286802b1d0e18628520aa886","externalIds":{"ArXiv":"2010.02502","DBLP":"journals/corr/abs-2010-02502","MAG":"3092442149","CorpusId":222140788},"title":"Denoising Diffusion Implicit Models"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"1156e277fa7ec195b043161d3c5c97715da17658","externalIds":{"DBLP":"conf/nips/0011E20","ArXiv":"2006.09011","MAG":"3035384201","CorpusId":219708245},"title":"Improved Techniques for Training Score-Based Generative Models"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"9b539d413393047b28bb7be9b195f142aaf7a80e","externalIds":{"ACL":"2021.eacl-main.24","MAG":"3023786569","DBLP":"conf/eacl/RollerDGJWLXOSB21","ArXiv":"2004.13637","DOI":"10.18653/v1/2021.eacl-main.24","CorpusId":216562425},"title":"Recipes for Building an Open-Domain Chatbot"},{"paperId":"5474ddca920f59c4ec3c243345a5b9248e64065b","externalIds":{"DBLP":"conf/cvpr/ChoiUYH20","ArXiv":"1912.01865","MAG":"2991997773","DOI":"10.1109/cvpr42600.2020.00821","CorpusId":208617800},"title":"StarGAN v2: Diverse Image Synthesis for Multiple Domains"},{"paperId":"56e3b48e72f9452cb862de1b76c51ade2b681c43","externalIds":{"DBLP":"journals/corr/abs-1911-11544","ArXiv":"1911.11544","MAG":"3035355202","DOI":"10.1109/cvpr42600.2020.00832","CorpusId":208290975},"title":"Image2StyleGAN++: How to Edit the Embedded Images?"},{"paperId":"52f3c46066eb88ff2f0a3c7d48fe36206870f985","externalIds":{"DBLP":"conf/iccv/HanHHS19","MAG":"2991079364","DOI":"10.1109/ICCV.2019.01057","CorpusId":204959889},"title":"ClothFlow: A Flow-Based Model for Clothed Person Generation"},{"paperId":"d8d89a0a1eca983512247af701a9e5596c903a16","externalIds":{"DBLP":"conf/cvpr/ShenGTZ20","ArXiv":"1907.10786","MAG":"2963577681","DOI":"10.1109/CVPR42600.2020.00926","CorpusId":198897678},"title":"Interpreting the Latent Space of GANs for Semantic Face Editing"},{"paperId":"965359b3008ab50dd04e171551220ec0e7f83aba","externalIds":{"MAG":"2971034910","ArXiv":"1907.05600","DBLP":"conf/nips/SongE19","CorpusId":196470871},"title":"Generative Modeling by Estimating Gradients of the Data Distribution"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"22aab110058ebbd198edb1f1e7b4f69fb13c0613","externalIds":{"ArXiv":"1809.11096","MAG":"2893749619","DBLP":"conf/iclr/BrockDS19","CorpusId":52889459},"title":"Large Scale GAN Training for High Fidelity Natural Image Synthesis"},{"paperId":"eff3ccb8a50e69d5839055dd1f3ddf21661ec60d","externalIds":{"MAG":"2883309205","DBLP":"journals/corr/abs-1807-07688","ArXiv":"1807.07688","DOI":"10.1007/978-3-030-01261-8_36","CorpusId":49901141},"title":"Toward Characteristic-Preserving Image-based Virtual Try-On Network"},{"paperId":"954c88092b3edffa1768f4a77b94677e2781d2ad","externalIds":{"MAG":"2808122338","ArXiv":"1806.06137","DOI":"10.1088/1361-6420/aaf14a","CorpusId":119152310},"title":"Deep null space learning for inverse problems: convergence analysis and rates"},{"paperId":"dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4","externalIds":{"MAG":"2949261999","ArXiv":"1801.04381","DBLP":"conf/cvpr/SandlerHZZC18","DOI":"10.1109/CVPR.2018.00474","CorpusId":4555207},"title":"MobileNetV2: Inverted Residuals and Linear Bottlenecks"},{"paperId":"c468bbde6a22d961829e1970e6ad5795e05418d1","externalIds":{"ArXiv":"1801.03924","MAG":"2783879794","DBLP":"journals/corr/abs-1801-03924","DOI":"10.1109/CVPR.2018.00068","CorpusId":4766599},"title":"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"},{"paperId":"473993aad08d6e1967ad692513e7c539f07b780e","externalIds":{"MAG":"2952288014","DBLP":"journals/corr/abs-1711-08447","ArXiv":"1711.08447","DOI":"10.1109/CVPR.2018.00787","CorpusId":4532827},"title":"VITON: An Image-Based Virtual Try-on Network"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"744fe47157477235032f7bb3777800f9f2f45e52","externalIds":{"MAG":"2766527293","DBLP":"conf/iclr/KarrasALL18","ArXiv":"1710.10196","CorpusId":3568073},"title":"Progressive Growing of GANs for Improved Quality, Stability, and Variation"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","externalIds":{"MAG":"2963981733","DBLP":"conf/nips/HeuselRUNH17","CorpusId":326772},"title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"585bf7bea8fa5267738bc465611d6f197e0f87dd","externalIds":{"MAG":"2953020559","DBLP":"conf/nips/PapamakariosMP17","ArXiv":"1705.07057","CorpusId":7166013},"title":"Masked Autoregressive Flow for Density Estimation"},{"paperId":"8acbe90d5b852dadea7810345451a99608ee54c7","externalIds":{"MAG":"2963073614","DBLP":"conf/cvpr/IsolaZZE17","ArXiv":"1611.07004","DOI":"10.1109/CVPR.2017.632","CorpusId":6200260},"title":"Image-to-Image Translation with Conditional Adversarial Networks"},{"paperId":"2ba23d9b46027e47b4483243871760e315213ffe","externalIds":{"ArXiv":"1609.03126","MAG":"2521028896","DBLP":"conf/iclr/ZhaoML17","CorpusId":15876696},"title":"Energy-based Generative Adversarial Network"},{"paperId":"41f1d50c85d3180476c4c7b3eea121278b0d8474","externalIds":{"MAG":"2953318193","ArXiv":"1601.06759","DBLP":"conf/icml/OordKK16","CorpusId":8142135},"title":"Pixel Recurrent Neural Networks"},{"paperId":"4dcdae25a5e33682953f0853ee4cf7ca93be58a9","externalIds":{"MAG":"967544008","DBLP":"journals/corr/YuZSSX15","ArXiv":"1506.03365","CorpusId":8317437},"title":"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop"},{"paperId":"0f899b92b7fb03b609fee887e4b6f3b633eaf30d","externalIds":{"MAG":"299440670","ArXiv":"1505.05770","DBLP":"journals/corr/RezendeM15","CorpusId":12554042},"title":"Variational Inference with Normalizing Flows"},{"paperId":"2dcef55a07f8607a819c21fe84131ea269cc2e3c","externalIds":{"MAG":"2129069237","DBLP":"journals/corr/Sohl-DicksteinW15","ArXiv":"1503.03585","CorpusId":14888175},"title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"},{"paperId":"353ecf7b66b3e9ff5e9f41145a147e899a2eea5c","externalIds":{"DBLP":"journals/corr/MirzaO14","ArXiv":"1411.1784","MAG":"2125389028","CorpusId":12803511},"title":"Conditional Generative Adversarial Nets"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"872bae24c109f7c30e052ac218b17a8b028d08a0","externalIds":{"MAG":"2013035813","DBLP":"journals/neco/Vincent11","DOI":"10.1162/NECO_a_00142","CorpusId":5560643,"PubMed":"21492012"},"title":"A Connection Between Score Matching and Denoising Autoencoders"},{"paperId":"79ad463104c7b7afeab11c2046fe7c18d5108ac6","externalIds":{"DOI":"10.1080/10131750485310161","CorpusId":218497666},"title":"Pattern"},{"paperId":"9966e890f2eedb4577e11b9d5a66380a4d9341fe","externalIds":{"DBLP":"journals/jmlr/Hyvarinen05","MAG":"1505878979","CorpusId":1152227},"title":"Estimation of Non-Normalized Statistical Models by Score Matching"},{"paperId":"d9fbcafa9aa98af9e36f3669a0e7b1ee4795407d","externalIds":{"DBLP":"journals/access/IslamM0024","DOI":"10.1109/ACCESS.2024.3368612","CorpusId":267960812},"title":"Deep Learning in Virtual Try-On: A Comprehensive Survey"},{"paperId":"2c355786be51f0176be581f319f1359d26f2b12f","externalIds":{"DBLP":"journals/corr/abs-2311-16567","DOI":"10.48550/arXiv.2311.16567","CorpusId":265466277},"title":"MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices"},{"paperId":"90428f3a8caa5082f825ebf3138514ddf273dae3","externalIds":{"CorpusId":253581838},"title":"Supplementary Materials for: NULL-text Inversion for Editing Real Images using Guided Diffusion Models"},{"paperId":"9c81be0c478bfc0a48eedb8769326fe289a11acc","externalIds":{"DBLP":"conf/iclr/SongVMK23","CorpusId":259298715},"title":"Pseudoinverse-Guided Diffusion Models for Inverse Problems"},{"paperId":"a31a6dc544174e83f6bd55fcc648e2c693f995a9","externalIds":{"DBLP":"journals/corr/abs-2303-17546","DOI":"10.48550/arXiv.2303.17546","CorpusId":257834185},"title":"PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models"},{"paperId":"973b4ccaf1e37553f4587374dc354196f0b20b6c","externalIds":{"DBLP":"journals/corr/abs-2211-07804","DOI":"10.48550/arXiv.2211.07804","CorpusId":260888585},"title":"Diffusion Models for Medical Image Analysis: A Comprehensive Survey"},{"paperId":"8eb00104fbc6f9bed30d0d01d8b29a5a10004766","externalIds":{"DBLP":"journals/corr/abs-2210-09477","DOI":"10.48550/arXiv.2210.09477","CorpusId":252968124},"title":"UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image"},{"paperId":"71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f","externalIds":{"DBLP":"journals/corr/abs-2209-02646","DOI":"10.48550/arXiv.2209.02646","CorpusId":252090040},"title":"A Survey on Generative Diffusion Model"},{"paperId":"735fcf085059f419112b76f7217e7f1407efcbb0","externalIds":{"DBLP":"journals/corr/abs-2211-13221","DOI":"10.48550/arXiv.2211.13221","CorpusId":253802030},"title":"Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths"},{"paperId":"0af6e4b23091b863258176d5765759926a4c85de","externalIds":{"DBLP":"journals/corr/abs-2108-01073","CorpusId":236772794},"title":"SDEdit: Image Synthesis and Editing with Stochastic Differential Equations"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"cfee1826dd4743eab44c6e27a0cc5970effa4d80","externalIds":{"CorpusId":264403242},"title":"Improving Image Generation with Better Captions"}]}