{"abstract":"Previous research has typically concentrated on leveraging the internal knowledge of Large Language Models (LLMs) to answer known questions (i.e., \\textit{internal reasoning such as generate-then-read}). In contrast, for questions that fall outside their known scope, these models rely on external knowledge retrieval to provide accurate responses (i.e., \\textit{external acting such as retrieve-then-read}). However, few previous works consider the \\textit{compositional questions}, which consist of several known and unknown sub-questions, necessitating the dynamic combination of previous two methods (i.e., \\textit{internal reasoning and external acting}) to achieve a better trade-off between effectiveness and efficiency. To this end, we introduce a \\textbf{Self} \\textbf{D}ivide-and-\\textbf{C}onquer (\\textit{\\texttt{Self-DC}}) framework, accompanying with the first \\textbf{C}ompositional \\textbf{u}nknown \\textbf{Q}uestion-\\textbf{A}nswering dataset (CuQA). This framework enables LLMs to adaptively choose between using internal knowledge and retrieving external knowledge as needed, resulting in a better trade-off between effectiveness and efficiency. Experimental results on two datasets demonstrate that \\textit{\\texttt{Self-DC}} can achieve comparable or even better performance with much fewer external calls compared with several strong baselines."}