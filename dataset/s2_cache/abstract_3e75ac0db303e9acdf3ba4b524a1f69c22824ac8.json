{"abstract":"The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile foundational models. However, evaluating MLLMs in <italic>low-level visual perception and understanding</italic> remains a yet-to-explore domain. To this end, we design benchmark settings to <italic>emulate human language responses</italic> related to low-level vision: the low-level visual <italic>perception</italic> (<underline>A1</underline>) <italic>via</italic> visual question answering related to low-level attributes (<italic>e.g. clarity, lighting</italic>); and the low-level visual <italic>description</italic> (<underline>A2</underline>), on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related question-answering and description evaluations of MLLMs from single images to <italic>image pairs</italic>. Specifically, for <italic>perception</italic> (A1), we carry out the LLVisionQA<inline-formula><tex-math notation=\"LaTeX\">$^{+}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3445770.gif\"/></alternatives></inline-formula> dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended question about its low-level features; for <bold/><italic>description</italic><bold/> (A2), we propose the LLDescribe<inline-formula><tex-math notation=\"LaTeX\">$^{+}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3445770.gif\"/></alternatives></inline-formula> dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on <bold/><italic>assessment</italic><bold/> (A3) ability, <italic>i.e.</italic> predicting score, by employing a softmax-based approach to enable all MLLMs to generate <italic>quantifiable</italic> quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level visual competencies on single images, but only GPT-4V exhibits higher accuracy on pairwise comparisons than single image evaluations (<italic>like humans</italic>). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs."}