{"abstract":"Microservices architecture is popular due to its scalability and flexibility. However, managing and troubleshooting distributed microservices-based systems can be challenging and time consuming. Auto-remediation of anomalies, that is the automated detection and root-causes generation and execution of repair scripts, can reduce the down-times and increase the availability of systems. This thesis will explore the potential and effectiveness of using large language models (LLMs) in auto-remediation. It will develop an auto-remediation framework to mitigate the effects of performance-based anomalies in self-adaptive microservice architectures. Multiple sample microservice applications as test-bed will be rigorously studied, and a dataset will be created to evaluate LLM-based codegeneration models using semantic, lexical, and correctness metrics in zero-shot and few-shot scenarios. Additionally, we will develop reliable prompts for automated Ansible runbook generation and assess their efficiency for orchestrating the auto-remediation process, including deployment, configuration changes, and system recovery to improve application reliability and operational efficiency."}