{"abstract":"Given a large amount of data in the base classes and a small number of data in the new classes, meta-learning can learn prior experience from the base classes and transfer knowledge to the new classes by generating network parameters for data generation. In this paper, we propose a novel generative adversarial network called MetaCGAN for generating high quality and diversity images to achieve data augmentation for the new classes with few data. In particular, MetaCGAN consists of two modules, the conditional GAN (CGAN) and MetaNet modules. The CGAN module is our skeleton network that is applied to generate images, while the MetaNet module is our auxiliary network that is applied to provide deconvolutional weights for the generator of CGAN. Experimental results on the MNIST, Fashion MNIST and CelebA data sets demonstrate the superiority of MetaCGAN over baseline models. Both qualitative and quantitative results show that the MetaNet module can learn prior knowledge and transfer it from the base classes to the new classes, which is beneficial for generating high quality and diversity images to the new classes with few images."}