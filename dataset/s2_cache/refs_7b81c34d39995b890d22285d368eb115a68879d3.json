{"references":[{"paperId":"665d1777159bb66d50d6e7e7d0537e156e07b415","externalIds":{"DBLP":"journals/corr/abs-2504-00527","ArXiv":"2504.00527","DOI":"10.1109/CVPR52734.2025.00790","CorpusId":277468534},"title":"SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning"},{"paperId":"0561c4c325287bb3269d8c7dbd7b36943ebe653f","externalIds":{"ArXiv":"2410.12490","DBLP":"conf/nips/0003LZLXB24","DOI":"10.48550/arXiv.2410.12490","CorpusId":273375651},"title":"Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective"},{"paperId":"f631ff4f13b6b85f67d06f5c050c5ef270f89267","externalIds":{"DBLP":"journals/corr/abs-2408-00759","ArXiv":"2408.00759","DOI":"10.48550/arXiv.2408.00759","CorpusId":271600745},"title":"Text-Guided Video Masked Autoencoder"},{"paperId":"29150a04fa6d3ddc81e9968a1307c9c5b54f8cc0","externalIds":{"DBLP":"conf/eccv/WeiGM24","ArXiv":"2407.15837","DOI":"10.48550/arXiv.2407.15837","CorpusId":271329419},"title":"Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning"},{"paperId":"f00664971b2d42437859a3af07c64c7310d1dad1","externalIds":{"ArXiv":"2407.13036","DBLP":"conf/eccv/HinojosaLG24","DOI":"10.48550/arXiv.2407.13036","CorpusId":271270426},"title":"ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders"},{"paperId":"f5d93ba63ba3e1b3991ae87cadfee5b1f407decf","externalIds":{"DBLP":"journals/corr/abs-2407-05193","ArXiv":"2407.05193","DOI":"10.48550/arXiv.2407.05193","CorpusId":271051051},"title":"CBM: Curriculum by Masking"},{"paperId":"72617ead144bbbe762cd8a2edf83db6419500687","externalIds":{"DBLP":"conf/cvpr/GuoSMZBMZZ24","DOI":"10.1109/CVPR52733.2024.02523","CorpusId":271163136},"title":"CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training"},{"paperId":"dab14fd51fcc53aa5592c682e6f0b365b7801e21","externalIds":{"DBLP":"journals/corr/abs-2404-10242","ArXiv":"2404.10242","DOI":"10.1109/CVPR52733.2024.01117","CorpusId":269157464},"title":"Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology"},{"paperId":"3273395b2c37cb96d3229ea193066879581743bb","externalIds":{"DBLP":"journals/corr/abs-2403-10052","ArXiv":"2403.10052","DOI":"10.1109/CVPR52733.2024.01427","CorpusId":268510417},"title":"T4P: Test-Time Training of Trajectory Prediction via Masked Autoencoder and Actor-Specific Token Memory"},{"paperId":"a86b749640a2f45eccf41a555406d9a983c953be","externalIds":{"DBLP":"conf/cvpr/BasuGM0024","ArXiv":"2403.08848","DOI":"10.1109/CVPR52733.2024.01113","CorpusId":268384907},"title":"FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders"},{"paperId":"9d10c3a3160f06782641052ce6f6e58431e30308","externalIds":{"DBLP":"conf/cvpr/00080GL0L24","ArXiv":"2403.07692","DOI":"10.1109/CVPR52733.2024.01342","CorpusId":268363729},"title":"Masked AutoDecoder is Effective Multi-Task Vision Generalist"},{"paperId":"ca5be911381489014d8abb5ef3f56705f62129aa","externalIds":{"ArXiv":"2402.19082","DBLP":"journals/corr/abs-2402-19082","DOI":"10.1109/CVPR52733.2024.02145","CorpusId":268063320},"title":"VideoMAC: Video Masked Autoencoders Meet ConvNets"},{"paperId":"025c76e135c2cf2b40561dd9f6d0e3337f078999","externalIds":{"DBLP":"journals/corr/abs-2402-05382","ArXiv":"2402.05382","DOI":"10.48550/arXiv.2402.05382","CorpusId":259298822},"title":"Task-customized Masked Autoencoder via Mixture of Cluster-conditional Experts"},{"paperId":"40df5568bfcb158a326b6c753d740a3c6fcdb4b5","externalIds":{"DBLP":"conf/icml/Pham0Y24","ArXiv":"2402.01516","DOI":"10.48550/arXiv.2402.01516","CorpusId":267406384},"title":"Cross-view Masked Diffusion Transformers for Person Image Synthesis"},{"paperId":"5670e59032e9b70f5af366ef8641282c204b555e","externalIds":{"ArXiv":"2401.00897","DBLP":"journals/corr/abs-2401-00897","DOI":"10.48550/arXiv.2401.00897","CorpusId":266725657},"title":"Masked Modeling for Self-supervised Representation Learning on Vision and Beyond"},{"paperId":"13925d7d5952b1ba5960dfe2c44d977be109b636","externalIds":{"ArXiv":"2312.06647","DBLP":"journals/corr/abs-2312-06647","DOI":"10.48550/arXiv.2312.06647","CorpusId":266162752},"title":"4M: Massively Multimodal Masked Modeling"},{"paperId":"ce73bca81ed4ff96efad709c2fbcd451d610bd10","externalIds":{"DBLP":"conf/nips/FullerMG23","ArXiv":"2311.00566","DOI":"10.48550/arXiv.2311.00566","CorpusId":264833082},"title":"CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders"},{"paperId":"5a35a6d11e527b3671acd875569f4842c02e262d","externalIds":{"DBLP":"conf/nips/YuanZZ0QSZLKYHD23","ArXiv":"2310.20695","DOI":"10.48550/arXiv.2310.20695","CorpusId":264820501},"title":"HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception"},{"paperId":"3f12da8f29e89579f5c133567955bbc9485bd9ec","externalIds":{"ArXiv":"2310.16318","DBLP":"journals/corr/abs-2310-16318","DOI":"10.48550/arXiv.2310.16318","CorpusId":264487231},"title":"Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder"},{"paperId":"e3c990c31e6a83a8fc19eca53bf22f6a1afd4bb3","externalIds":{"DBLP":"conf/iccv/ZhouLQLPZZGL23","DOI":"10.1109/ICCV51070.2023.01482","CorpusId":267024786},"title":"SparseMAE: Sparse Training Meets Masked Autoencoders"},{"paperId":"639c9b6ea55029d06ea7d5f55bc08bcc78d0734b","externalIds":{"DBLP":"conf/iccv/ZhaoWCLYPL23","DOI":"10.1109/ICCV51070.2023.01745","CorpusId":267026508},"title":"Masked Retraining Teacher-Student Framework for Domain Adaptive Object Detection"},{"paperId":"a08da472572f244101f60db53f46b67aa2d54f81","externalIds":{"DBLP":"conf/iccv/LaoSLLY23","DOI":"10.1109/ICCV51070.2023.00587","CorpusId":267022515},"title":"Masked Autoencoders Are Stronger Knowledge Distillers"},{"paperId":"f4b5ed58491de52904c5eeb4b0c616dac507792f","externalIds":{"DBLP":"conf/iccv/LiWH0WW023","DOI":"10.1109/ICCV51070.2023.00157","CorpusId":267027060},"title":"UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding"},{"paperId":"852174d28620dfbf261288c70a41ee967c69cdc8","externalIds":{"DBLP":"conf/iccv/YangLLY23","DOI":"10.1109/ICCV51070.2023.01961","CorpusId":267023203},"title":"MRM: Masked Relation Modeling for Medical Image Pre-Training with Genetics"},{"paperId":"920cc8f167407e4b17db974c527a3cc6fe87230c","externalIds":{"DBLP":"journals/corr/abs-2309-14136","ArXiv":"2309.14136","DOI":"10.48550/arXiv.2309.14136","CorpusId":262464780},"title":"Masked Image Residual Learning for Scaling Deeper Vision Transformers"},{"paperId":"2d3ee3327e7c70196a0ea5e271e1e30da5551588","externalIds":{"ArXiv":"2309.01005","DBLP":"journals/corr/abs-2309-01005","DOI":"10.48550/arXiv.2309.01005","CorpusId":261530214},"title":"RevColV2: Exploring Disentangled Representations in Masked Image Modeling"},{"paperId":"e710eccd39374ee9e558a461b109552325864a58","externalIds":{"DBLP":"journals/corr/abs-2308-16572","ArXiv":"2308.16572","DOI":"10.1109/WACV57701.2024.00248","CorpusId":261394835},"title":"CL-MAE: Curriculum-Learned Masked Autoencoders"},{"paperId":"bf268f80ccc33920062bf91da7fcff659e6a9893","externalIds":{"DBLP":"journals/tip/WalshOS23","DOI":"10.1109/TIP.2023.3306916","CorpusId":261099166,"PubMed":"37616141"},"title":"Masked Embedding Modeling With Rapid Domain Adjustment for Few-Shot Image Classification"},{"paperId":"70327e3a7d666ba581a50e68816b4388aa2fd377","externalIds":{"DBLP":"conf/iccv/ZhaiLBLC23","ArXiv":"2308.12510","DOI":"10.1109/ICCV51070.2023.01750","CorpusId":261100768},"title":"Masked Autoencoders are Efficient Class Incremental Learners"},{"paperId":"23698ecdb7fc23f222d6cfed41447bddf3317dd3","externalIds":{"DBLP":"conf/eccv/ZouHYGLFZ24","ArXiv":"2308.10421","DOI":"10.48550/arXiv.2308.10421","CorpusId":261048943},"title":"UniM2AE: Multi-modal Masked Autoencoders with Unified 3D Representation for 3D Perception in Autonomous Driving"},{"paperId":"54f5ffa912dfa12fa77dae9b205cf25f51b74748","externalIds":{"ArXiv":"2308.10794","DBLP":"conf/iccv/HuangZZQW23","DOI":"10.1109/ICCV51070.2023.01241","CorpusId":261049343},"title":"MGMAE: Motion Guided Masking for Video Masked Autoencoding"},{"paperId":"e6a690d40d1811140ef12b4977e43e9fd2908e18","externalIds":{"DBLP":"journals/corr/abs-2308-10315","ArXiv":"2308.10315","DOI":"10.1109/ICCV51070.2023.00154","CorpusId":261049741},"title":"Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting"},{"paperId":"9781a56c0fcbc2cae640e0c7d30517cb8b16105e","externalIds":{"DBLP":"journals/corr/abs-2308-09245","ArXiv":"2308.09245","DOI":"10.1109/ICCV51070.2023.01520","CorpusId":261030701},"title":"Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos"},{"paperId":"94a307c64fe0b49b2e019dfb6241aee78d9379f7","externalIds":{"DBLP":"conf/iccv/MaoDZFOL23","ArXiv":"2308.07092","DOI":"10.1109/ICCV51070.2023.00934","CorpusId":260887435},"title":"Masked Motion Predictors are Strong 3D Action Representation Learners"},{"paperId":"5ae1996ece0323839595ee50d3c64766b9cb53ff","externalIds":{"DBLP":"conf/ijcai/ZhangZSYK23","DOI":"10.24963/ijcai.2023/762","CorpusId":260860855},"title":"A Survey on Masked Autoencoder for Visual Self-supervised Learning"},{"paperId":"30f9b1fd46e9192b41d2b0d877e06688d0b2e9b5","externalIds":{"ArXiv":"2307.15254","DBLP":"conf/iccv/TangHZZZ023","DOI":"10.1109/ICCV51070.2023.00377","CorpusId":260315910},"title":"Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification"},{"paperId":"c84e03c8ba7feaa2679a90b1637f3b079be15aa9","externalIds":{"DBLP":"conf/miccai/WangLMWLWSYXZ23","ArXiv":"2307.12591","DOI":"10.48550/arXiv.2307.12591","CorpusId":260125020},"title":"SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation"},{"paperId":"b301f0dced17a3c8dde6e2d7d4919e0f6192d518","externalIds":{"ArXiv":"2307.12672","DBLP":"conf/miccai/PanSTHLAKHR23","DOI":"10.48550/arXiv.2307.12672","CorpusId":260125463},"title":"Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling"},{"paperId":"1dd4b5c3c91b0345f6b955a226d229300e6476e1","externalIds":{"DBLP":"journals/corr/abs-2307-12721","ArXiv":"2307.12721","DOI":"10.48550/arXiv.2307.12721","CorpusId":260125529},"title":"AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays"},{"paperId":"6143d56afcd6d7dd815762cafb2b16f848dca4a7","externalIds":{"DBLP":"journals/tmi/WangLT23","DOI":"10.1109/TMI.2023.3290700","CorpusId":259284307,"PubMed":"37379178"},"title":"autoSMIM: Automatic Superpixel-Based Masked Image Modeling for Skin Lesion Segmentation"},{"paperId":"e29734b4945611c4eeb49bd176086881e71ba25a","externalIds":{"DBLP":"conf/aaai/YaoZL0L23","DOI":"10.1609/aaai.v37i4.25604","CorpusId":259739068},"title":"One-for-All: Proposal Masked Cross-Class Anomaly Detection"},{"paperId":"4ba78edb48d729339b109b1ae707daafcca8e005","externalIds":{"DBLP":"conf/aaai/0001ZDWWGX23","DOI":"10.1609/aaai.v37i4.25674","CorpusId":259716837},"title":"Yet Another Traffic Classifier: A Masked Autoencoder Based Traffic Transformer with Multi-Level Flow Representation"},{"paperId":"dc430bba79178483c254a072c2151f0b3e7111af","externalIds":{"DBLP":"journals/corr/abs-2307-04231","ArXiv":"2307.04231","DOI":"10.1609/aaai.v37i3.25448","CorpusId":259501669},"title":"Mx2M: Masked Cross-Modality Modeling in Domain Adaptation for 3D Semantic Segmentation"},{"paperId":"812b24eaaa550d9c1a5bb480b847e1b5479c5289","externalIds":{"DBLP":"conf/aaai/LiuGL23","DOI":"10.48550/arXiv.2211.15362","CorpusId":274142137},"title":"Good helper is around you: Attention-driven Masked Image Modeling"},{"paperId":"7ad25dbb5022c119144990cde0d364e2519df728","externalIds":{"ArXiv":"2306.12041","DBLP":"conf/cvpr/RisteaCIPKS24","DOI":"10.1109/CVPR52733.2024.01513","CorpusId":259212303},"title":"Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors"},{"paperId":"6be443bdce0b2f92c4d720a660de15b1967acd95","externalIds":{"ArXiv":"2306.08249","DBLP":"journals/corr/abs-2306-08249","DOI":"10.48550/arXiv.2306.08249","CorpusId":259164436},"title":"Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition"},{"paperId":"f83d3a91b28a9b7dfc77f82eff6402a7e06670a6","externalIds":{"ArXiv":"2306.02006","DBLP":"conf/ijcai/SongFZL23","DOI":"10.48550/arXiv.2306.02006","CorpusId":259076469},"title":"MA2CL: Masked Attentive Contrastive Learning for Multi-Agent Reinforcement Learning"},{"paperId":"dcb4f2b9b0e6da0d629878d1ad0469aee3df2020","externalIds":{"DBLP":"journals/corr/abs-2306-04898","ArXiv":"2306.04898","DOI":"10.1109/CVPR52729.2023.00765","CorpusId":259108864},"title":"Understanding Masked Autoencoders via Hierarchical Latent Variable Models"},{"paperId":"2460f3e3949f426a867048b7df7a14b7cf654d94","externalIds":{"DBLP":"conf/cvpr/FeiFZHWW23","DOI":"10.1109/CVPR52729.2023.02342","CorpusId":260891405},"title":"Masked Auto-Encoders Meet Generative Adversarial Networks and Beyond"},{"paperId":"2c600b2829a608ce366614dc4911ef672f2e4255","externalIds":{"DBLP":"journals/corr/abs-2306-00520","ArXiv":"2306.00520","CorpusId":258999862},"title":"On Masked Pre-training and the Marginal Likelihood"},{"paperId":"b9cacf2710f292c9e3edaf45b92f7faa1a9928c8","externalIds":{"DBLP":"conf/cvpr/ZhaoWL23","DOI":"10.1109/CVPR52729.2023.01793","CorpusId":260061227},"title":"Representation Learning for Visual Object Tracking by Masked Appearance Transfer"},{"paperId":"6a3f9e15119e197b86cc59d1d880d85e89501db5","externalIds":{"DBLP":"conf/cvpr/XiePBW23","DOI":"10.1109/CVPR52729.2023.00321","CorpusId":261080761},"title":"MAESTER: Masked Autoencoder Guided Segmentation at Pixel Resolution for Accurate, Self-Supervised Subcellular Structure Recognition"},{"paperId":"f04a8d11f8752a230d58ff792d529fe342f75104","externalIds":{"ArXiv":"2305.14344","DBLP":"conf/nips/Gupta00023","DOI":"10.48550/arXiv.2305.14344","CorpusId":258841721},"title":"Siamese Masked Autoencoders"},{"paperId":"c6e62cb30d344f67ed32443c7c17a86cc268458c","externalIds":{"DBLP":"journals/corr/abs-2305-08808","ArXiv":"2305.08808","DOI":"10.1109/CVPR52729.2023.01304","CorpusId":258686501},"title":"GeoMAE: Masked Geometric Target Prediction for Self-supervised Point Cloud Pre-Training"},{"paperId":"af9e69ec120e94b5542edf88b392ee3769c1379b","externalIds":{"ArXiv":"2305.08932","DBLP":"conf/nips/LinJ23","DOI":"10.48550/arXiv.2305.08932","CorpusId":258714642},"title":"MIMEx: Intrinsic Rewards from Masked Input Modeling"},{"paperId":"9474128c5040c38b4570b85895a76c21985fa3da","externalIds":{"ArXiv":"2305.05026","DBLP":"journals/corr/abs-2305-05026","DOI":"10.1109/CVPR52729.2023.00119","CorpusId":258564652},"title":"Self-Supervised Pre-Training with Masked Shape Prediction for 3D Scene Understanding"},{"paperId":"0d49e9fab5a02f4274e4d824d36b8fbe52d539d6","externalIds":{"DBLP":"journals/corr/abs-2304-13166","ArXiv":"2304.13166","DOI":"10.1109/CVPR52729.2023.01754","CorpusId":258332133},"title":"LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization"},{"paperId":"0a32ba84d37bdcfab87791be8c162e8725dbb214","externalIds":{"DBLP":"conf/cvpr/YanZCTZSGZ23","ArXiv":"2304.06914","DOI":"10.1109/CVPR52729.2023.00559","CorpusId":258170018},"title":"SMAE: Few-shot Learning for HDR Deghosting with Saturation-Aware Masked Autoencoders"},{"paperId":"1dc6c476e6a48cf9c11dc1307de2b844fb1824ba","externalIds":{"DBLP":"journals/corr/abs-2304-05919","ArXiv":"2304.05919","DOI":"10.1109/CVPR52729.2023.01000","CorpusId":258079193},"title":"Hard Patches Mining for Masked Image Modeling"},{"paperId":"b032f324a0d4a24fd917551345bd100dc368e41a","externalIds":{"DBLP":"conf/iccv/0005M0L00WXYF23","ArXiv":"2304.03283","DOI":"10.1109/ICCV51070.2023.01492","CorpusId":257985028},"title":"Diffusion Models as Masked Autoencoders"},{"paperId":"2eeabf899f0fe92c5e67178478fe983f71c2d28b","externalIds":{"DBLP":"journals/corr/abs-2303-17342","ArXiv":"2303.17342","DOI":"10.1109/CVPR52729.2023.02098","CorpusId":257833673},"title":"PMatch: Paired Masked Image Modeling for Dense Geometric Matching"},{"paperId":"9fac3d0728a8c833a593446e3e176e90d856df04","externalIds":{"DBLP":"journals/corr/abs-2303-16727","ArXiv":"2303.16727","DOI":"10.1109/CVPR52729.2023.01398","CorpusId":257805127},"title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking"},{"paperId":"0810afe92ec3b0b31183648aae2b4dc77dea734f","externalIds":{"DBLP":"conf/iccv/Li00WHWQ23","ArXiv":"2303.16058","DOI":"10.1109/ICCV51070.2023.01826","CorpusId":257771777},"title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models"},{"paperId":"d7adb2e6c8e381ca6c7a9a74ec5c54061573f9e1","externalIds":{"DBLP":"journals/corr/abs-2303-14865","ArXiv":"2303.14865","DOI":"10.1109/CVPR52729.2023.01449","CorpusId":257766798},"title":"Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens"},{"paperId":"362e7184fcec5e3daf52cf5fb993799e457ba1c5","externalIds":{"ArXiv":"2303.15466","DBLP":"conf/cvpr/LinHMH0C23","DOI":"10.1109/CVPR52729.2023.01882","CorpusId":257771519},"title":"Supervised Masked Knowledge Distillation for Few-Shot Transformers"},{"paperId":"ca21f28e2a0a8205038301d8385151ab7ca2a050","externalIds":{"DBLP":"conf/iccv/GaoZCY23","ArXiv":"2303.14389","DOI":"10.1109/ICCV51070.2023.02117","CorpusId":257767316},"title":"Masked Diffusion Transformer is a Strong Image Synthesizer"},{"paperId":"0ca3852a9b2df57db93b1efe8fdd78cd89f4159a","externalIds":{"DBLP":"journals/corr/abs-2303-14191","ArXiv":"2303.14191","DOI":"10.1109/CVPR52729.2023.00908","CorpusId":257757096},"title":"Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning"},{"paperId":"13da3ee5237de2f94d63640789f2c9685e70ba68","externalIds":{"ArXiv":"2303.13132","DBLP":"journals/corr/abs-2303-13132","DOI":"10.1109/CVPR52729.2023.00169","CorpusId":257687687},"title":"Masked Image Training for Generalizable Deep Image Denoising"},{"paperId":"67317a73151316933c3943ff68b5f7cfcbc7e4c7","externalIds":{"ArXiv":"2303.12208","DBLP":"journals/corr/abs-2303-12208","DOI":"10.1109/CVPR52729.2023.02235","CorpusId":257663583},"title":"MAGVLT: Masked Generative Vision-and-Language Transformer"},{"paperId":"f01910dbab8238b574825a11cb88a7ddf2a95fc7","externalIds":{"ArXiv":"2303.12001","CorpusId":257636974},"title":"ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders"},{"paperId":"7e4b65a2858f402a986187d5edd3e5e353985358","externalIds":{"DBLP":"conf/iccv/LiuWLZL023","ArXiv":"2303.11325","DOI":"10.1109/ICCV51070.2023.01635","CorpusId":257632135},"title":"GeoMIM: Towards Better 3D Knowledge Transfer via Masked Image Modeling for Multi-view 3D Understanding"},{"paperId":"d54af4757b6fc8cd980bb720960072b72c100ec1","externalIds":{"ArXiv":"2303.09373","DBLP":"conf/cvpr/ZhangWA0GROWJLP24","DOI":"10.1109/CVPR52733.2024.00559","CorpusId":257557662,"PubMed":"39479533"},"title":"MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling"},{"paperId":"fd8b40bd4aaf87256be8e933253f6c5c40c647f4","externalIds":{"ArXiv":"2303.08129","DBLP":"conf/cvpr/ChenZZWLGZ23","DOI":"10.1109/CVPR52729.2023.00512","CorpusId":257505114},"title":"PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection"},{"paperId":"63f94bf821927a736805077a8cfe00c48a6b00c8","externalIds":{"DBLP":"journals/corr/abs-2303-05251","ArXiv":"2303.05251","DOI":"10.1109/CVPR52729.2023.00211","CorpusId":257427476},"title":"Masked Image Modeling with Local Multi-Scale Reconstruction"},{"paperId":"05e73f7da4b0a52b409459b69fcaed950b0b09f3","externalIds":{"DBLP":"conf/iclr/0001GGS23","ArXiv":"2303.03679","DOI":"10.48550/arXiv.2303.03679","CorpusId":257378217},"title":"MAST: Masked Augmentation Subspace Training for Generalizable Self-Supervised Priors"},{"paperId":"0e906de5615f17bb5fef275aa8c73c4409a7815c","externalIds":{"DBLP":"journals/corr/abs-2303-03052","ArXiv":"2303.03052","DOI":"10.1109/CVPR52729.2023.01944","CorpusId":257364775},"title":"Masked Images Are Counterfactual Samples for Robust Fine-Tuning"},{"paperId":"7ed32e4a8e7ee87b2c441acd2b88ad012159eccf","externalIds":{"DBLP":"conf/iclr/YuLZZGQYHD023","ArXiv":"2303.00289","DOI":"10.48550/arXiv.2303.00289","CorpusId":257255242},"title":"StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training"},{"paperId":"9b00f31be5f5c7e00318ba4ddda01a73560fa476","externalIds":{"DBLP":"conf/cvpr/HouDHDN23","ArXiv":"2302.14746","DOI":"10.1109/CVPR52729.2023.01298","CorpusId":257232638},"title":"Mask3D: Pretraining 2D Vision Transformers by Learning Masked 3D Priors"},{"paperId":"f7b0894f9c8431eb127e02f7af83d16979a88d28","externalIds":{"ArXiv":"2302.14771","DBLP":"conf/cvpr/HuangP0WJY23","DOI":"10.1109/CVPR52729.2023.01535","CorpusId":257232448},"title":"Generic-to-Specific Distillation of Masked Autoencoders"},{"paperId":"2e27b1355389e897ccd38cd8f086eeec3b29d391","externalIds":{"ArXiv":"2302.14138","DBLP":"journals/corr/abs-2302-14138","DOI":"10.48550/arXiv.2302.14138","CorpusId":257232949},"title":"Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations"},{"paperId":"39cfe9b0f4ff8a325e83049e3e87b7f16384b910","externalIds":{"DBLP":"conf/ijcai/GuoZQLH23","ArXiv":"2302.14007","DOI":"10.48550/arXiv.2302.14007","CorpusId":257220161},"title":"Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training"},{"paperId":"3d743910cded9a81147a73b57c5654233053825b","externalIds":{"DBLP":"conf/cvpr/BashkirovaLSSE23","ArXiv":"2302.05496","DOI":"10.1109/CVPR52729.2023.00187","CorpusId":254496812},"title":"MaskSketch: Unpaired Structure-guided Masked Image Generation"},{"paperId":"0efa4d128dd140a2d3ad36b9f452fc3b80223667","externalIds":{"DBLP":"journals/corr/abs-2302-02615","ArXiv":"2302.02615","DOI":"10.1109/CVPR52729.2023.01114","CorpusId":256615563},"title":"Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need"},{"paperId":"d1177de3e533a40257c44e5ed39ba24eefcc8090","externalIds":{"DBLP":"conf/icml/SeoKJLSA23","ArXiv":"2302.02408","DOI":"10.48550/arXiv.2302.02408","CorpusId":256615177},"title":"Multi-View Masked World Models for Visual Robotic Manipulation"},{"paperId":"81620597ffafb4368cf0fe4fab7b7cd4506e09cd","externalIds":{"DBLP":"journals/corr/abs-2301-13155","ArXiv":"2301.13155","DOI":"10.48550/arXiv.2301.13155","CorpusId":256389841},"title":"Advancing Radiograph Representation Learning with Masked Record Modeling"},{"paperId":"bbb9a9c2ee37ba8ff0aed2ad3b2c5db91e27e74e","externalIds":{"DBLP":"journals/data/PanchalNKPNPB23","DOI":"10.3390/data8020029","CorpusId":256437093},"title":"Retinal Fundus Multi-Disease Image Dataset (RFMiD) 2.0: A Dataset of Frequently and Rarely Identified Diseases"},{"paperId":"b8e6e17ca2288c26b79cedff0a666e2549441ac1","externalIds":{"DBLP":"journals/corr/abs-2301-10938","ArXiv":"2301.10938","DOI":"10.48550/arXiv.2301.10938","CorpusId":256275013},"title":"Compact Transformer Tracker with Correlative Masked Modeling"},{"paperId":"fcaafd1064c6aa32d6517002ff7c5ebd878c90ed","externalIds":{"DBLP":"conf/cvpr/WeersSKYG23","ArXiv":"2301.07836","DOI":"10.1109/CVPR52729.2023.02244","CorpusId":258309125},"title":"Masked Autoencoding Does Not Help Natural Language Supervision at Scale"},{"paperId":"d6b1d1bad791f5b80244b2c015c817ef53829182","externalIds":{"DBLP":"journals/corr/abs-2301-06958","ArXiv":"2301.06958","DOI":"10.1109/CVPR52729.2023.02232","CorpusId":257233129},"title":"RILS: Masked Visual Reconstruction in Language Semantic Space"},{"paperId":"e45c0a415b01b13d25e7dbced9f651261be0feaf","externalIds":{"ArXiv":"2301.03580","DBLP":"journals/corr/abs-2301-03580","DOI":"10.48550/arXiv.2301.03580","CorpusId":255546299},"title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling"},{"paperId":"2218f1713d7f721ab76801063416ec9b11c7646f","externalIds":{"DBLP":"journals/corr/abs-2301-00808","ArXiv":"2301.00808","DOI":"10.1109/CVPR52729.2023.01548","CorpusId":255372693},"title":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"},{"paperId":"d3d079e711308bd278c270b621dde66aa311625c","externalIds":{"DBLP":"conf/iccv/ReedGLBFCKCUD23","ArXiv":"2212.14532","DOI":"10.1109/ICCV51070.2023.00378","CorpusId":255340927},"title":"Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning"},{"paperId":"007323e9a19faa7be415eb2122dd331b11a54989","externalIds":{"ArXiv":"2212.11696","DBLP":"conf/iclr/CaiZHSKLZ23","DOI":"10.48550/arXiv.2212.11696","CorpusId":254974335},"title":"Reversible Column Networks"},{"paperId":"28b171bfb721605ef25fd65df7a09560aa054d8f","externalIds":{"ArXiv":"2212.09948","DBLP":"conf/cvpr/XuX0O00023","DOI":"10.1109/CVPR52729.2023.00426","CorpusId":254876995},"title":"MM-3DScene: 3D Scene Understanding by Customizing Masked Modeling with Informative-Preserved Reconstruction and Self-Distilled Consistency"},{"paperId":"de46871d33c5ceab19bcf05db31e6a16b3d5ba03","externalIds":{"DBLP":"conf/nips/0001S0R0L0GMF23","ArXiv":"2212.08071","DOI":"10.48550/arXiv.2212.08071","CorpusId":254685493},"title":"MAViL: Masked Audio-Video Learners"},{"paperId":"64d0de48e288056320216f7905b2f4690e994840","externalIds":{"DBLP":"journals/corr/abs-2212-06785","ArXiv":"2212.06785","DOI":"10.1109/CVPR52729.2023.02085","CorpusId":254591457},"title":"Learning 3D Representations from 2D Pre-Trained Models via Image-to-Point Masked Autoencoders"},{"paperId":"fe34137e5cc07235eae65ce53a54cd226b9f8b23","externalIds":{"DBLP":"journals/corr/abs-2212-05199","ArXiv":"2212.05199","DOI":"10.1109/CVPR52729.2023.01008","CorpusId":254563906},"title":"MAGVIT: Masked Generative Video Transformer"},{"paperId":"541ca024fc404a0c14ae280b18020d6540faa0d3","externalIds":{"DBLP":"conf/iccv/GeorgescuFILSA23","ArXiv":"2212.05922","DOI":"10.1109/ICCV51070.2023.01479","CorpusId":254563922},"title":"Audiovisual Masked Autoencoders"},{"paperId":"715c34474a6272b643103d0ca7f56c064faf2099","externalIds":{"DBLP":"journals/corr/abs-2212-04500","ArXiv":"2212.04500","DOI":"10.1109/CVPR52729.2023.00611","CorpusId":254408955},"title":"Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning"},{"paperId":"f2db26387e696d12028d41663ed5c6e439794206","externalIds":{"ArXiv":"2211.12824","DBLP":"conf/cvpr/FuYZFSWB23","DOI":"10.1109/CVPR52729.2023.01029","CorpusId":253801972},"title":"Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation"},{"paperId":"65be7f0b01cf6c2a1cded708b809182fbcb43548","externalIds":{"DBLP":"journals/corr/abs-2211-11210","ArXiv":"2211.11210","DOI":"10.48550/arXiv.2211.11210","CorpusId":253734237},"title":"Contrastive Masked Autoencoders for Self-Supervised Video Hashing"},{"paperId":"e6f49ff1f18e8b8dd74e6477af2e728d0db1d388","externalIds":{"ArXiv":"2211.11432","DBLP":"journals/corr/abs-2211-11432","DOI":"10.1109/ICCV51070.2023.01532","CorpusId":253734858},"title":"MATE: Masked Autoencoders are Online 3D Test-Time Learners"},{"paperId":"210f6ffbed4bf3a0f020cfcb48dab9d6a9939cdb","externalIds":{"DBLP":"journals/corr/abs-2211-11446","ArXiv":"2211.11446","DOI":"10.1109/ICCV51070.2023.00233","CorpusId":253735003},"title":"SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training"},{"paperId":"2f68d3934b006fcd01732adbc1ab459b2485fc8e","externalIds":{"DBLP":"journals/corr/abs-2211-09117","ArXiv":"2211.09117","DOI":"10.1109/CVPR52729.2023.00213","CorpusId":253553243},"title":"MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis"},{"paperId":"afee4f9658c04d31640190881f64c1efcea72e26","externalIds":{"DBLP":"journals/corr/abs-2211-08887","ArXiv":"2211.08887","DOI":"10.1109/CVPR52729.2023.02177","CorpusId":253553682},"title":"Stare at What You See: Masked Image Modeling without Reconstruction"},{"paperId":"a135632a05cc1f3311859fdebcd1350b4e9e1ee7","externalIds":{"ArXiv":"2211.09120","DBLP":"conf/cvpr/BandaraPGNA023","DOI":"10.1109/CVPR52729.2023.01394","CorpusId":253553494},"title":"AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","externalIds":{"DBLP":"conf/cvpr/FangWXSWW0WC23","ArXiv":"2211.07636","DOI":"10.1109/CVPR52729.2023.01855","CorpusId":253510587},"title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"c668b80885784a2c02b7837978ee95fefd108f1d","externalIds":{"DBLP":"journals/corr/abs-2211-06956","ArXiv":"2211.06956","DOI":"10.1109/CVPR52729.2023.02175","CorpusId":253510456},"title":"Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding"},{"paperId":"a45913b5af0bb1f661dd0019462686174de070ff","externalIds":{"DBLP":"journals/corr/abs-2211-06627","ArXiv":"2211.06627","DOI":"10.1109/CVPR52729.2023.00150","CorpusId":253510593},"title":"MARLIN: Masked Autoencoder for facial video Representation LearnINg"},{"paperId":"249e00445585586214e27d1f4ade032533132d0a","externalIds":{"ArXiv":"2210.12843","DBLP":"journals/corr/abs-2210-12843","DOI":"10.1109/WACV56688.2023.00358","CorpusId":253098023},"title":"Delving into Masked Autoencoders for Multi-Label Thorax Disease Classification"},{"paperId":"9984126ddc153d6b6c74102a952c2a6d83851a4d","externalIds":{"DBLP":"journals/corr/abs-2210-11404","ArXiv":"2210.11404","DOI":"10.1109/WACV56688.2023.00555","CorpusId":253018882},"title":"Self-Supervised Learning with Masked Image Modeling for Teeth Numbering, Detection of Dental Restorations, and Instance Segmentation in Dental Panoramic Radiographs"},{"paperId":"b58d5388fd408af2437ac173cb457e8ff01598a7","externalIds":{"DBLP":"conf/nips/ZhangWW22a","ArXiv":"2210.08344","DOI":"10.48550/arXiv.2210.08344","CorpusId":252918861},"title":"How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders"},{"paperId":"ffce78c90f35d1d06c151ad34ec8fb47891957d9","externalIds":{"ArXiv":"2210.06096","DBLP":"conf/cvpr/SunCCLLTG23","DOI":"10.1109/CVPR52729.2023.00222","CorpusId":257687898},"title":"Masked Motion Encoding for Self-Supervised Video Representation Learning"},{"paperId":"f7227e206e11923e5d29a84ecc7dbd623a168f25","externalIds":{"ArXiv":"2210.06983","DBLP":"conf/iclr/WuYGZ0H23","CorpusId":257378676},"title":"Denoising Masked Autoencoders Help Robust Classification"},{"paperId":"429e68167c418d7aea7379083599834d5e36606f","externalIds":{"ArXiv":"2210.02077","DBLP":"journals/corr/abs-2210-02077","DOI":"10.48550/arXiv.2210.02077","CorpusId":252715443},"title":"Exploring The Role of Mean Teachers in Self-supervised Masked Auto-Encoders"},{"paperId":"3fa1505ec327b82416987a7db3dadab8b12601ea","externalIds":{"DBLP":"conf/iclr/GongRLHKKG23","ArXiv":"2210.07839","DOI":"10.48550/arXiv.2210.07839","CorpusId":252907593},"title":"Contrastive Audio-Visual Masked Autoencoder"},{"paperId":"7f12319e9b0cdbef09f720bd34dacb8422504df6","externalIds":{"DBLP":"journals/corr/abs-2209-12148","ArXiv":"2209.12148","DOI":"10.1109/TPAMI.2023.3322604","CorpusId":252531780,"PubMed":"37801379"},"title":"Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","externalIds":{"ArXiv":"2209.07098","DBLP":"conf/miccai/ChenDHLLWC22","DOI":"10.1007/978-3-031-16443-9_65","CorpusId":252280670},"title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"9bc2c47859d90a26e7f3fcedc1dbfe45d7ec9528","externalIds":{"DBLP":"conf/nips/GandelsmanSCE22","ArXiv":"2209.07522","DOI":"10.48550/arXiv.2209.07522","CorpusId":252283956},"title":"Test-Time Training with Masked Autoencoders"},{"paperId":"e875667d1ae8fd8f3b760eee6feb6c8a79497e8c","externalIds":{"DBLP":"conf/eccv/LezamaCJE22","ArXiv":"2209.04439","DOI":"10.48550/arXiv.2209.04439","CorpusId":252185410},"title":"Improved Masked Image Generation with Token-Critic"},{"paperId":"bac146e4f52df49ded741e4b31102b97c8b5847f","externalIds":{"DBLP":"journals/corr/abs-2209-01540","ArXiv":"2209.01540","DOI":"10.1109/CVPR52729.2023.02193","CorpusId":252089828},"title":"An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling"},{"paperId":"1114863be2a713a14771ccacb5c9436fb4a375e2","externalIds":{"DBLP":"journals/corr/abs-2208-12262","ArXiv":"2208.12262","DOI":"10.1109/CVPR52729.2023.01058","CorpusId":251799827},"title":"MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining"},{"paperId":"a7cd547c539d69f99f17855242cb07bd80047f9a","externalIds":{"DBLP":"journals/corr/abs-2208-12256","ArXiv":"2208.12256","DOI":"10.1109/CVPR52729.2023.02323","CorpusId":251800257},"title":"Masked Autoencoders Enable Efficient Knowledge Distillers"},{"paperId":"5d967afa1e837fc9fb6f2c15ee37c013e2b9e3c7","externalIds":{"ArXiv":"2208.04164","DBLP":"journals/corr/abs-2208-04164","DOI":"10.1109/CVPR52729.2023.00604","CorpusId":251402519},"title":"Understanding Masked Image Modeling via Learning Occlusion Invariant Feature"},{"paperId":"620369d6ed3ed68c3e4374d6ddf282e0b036d2f8","externalIds":{"DBLP":"journals/corr/abs-2208-02131","ArXiv":"2208.02131","DOI":"10.48550/arXiv.2208.02131","CorpusId":251280143},"title":"Masked Vision and Language Modeling for Multi-modal Representation Learning"},{"paperId":"8a9a9679656d97c326faa5375405c744e9989bcb","externalIds":{"DBLP":"journals/corr/abs-2208-00449","ArXiv":"2208.00449","DOI":"10.48550/arXiv.2208.00449","CorpusId":251223971},"title":"SdAE: Self-distillated Masked Autoencoder"},{"paperId":"8b56eec0ffdd4ead040e3af7a5fef2069a57dbdd","externalIds":{"ArXiv":"2207.13532","DBLP":"journals/corr/abs-2207-13532","DOI":"10.1109/TPAMI.2023.3336525","CorpusId":251105242,"PubMed":"38015699"},"title":"Contrastive Masked Autoencoders are Stronger Vision Learners"},{"paperId":"e77a95958d391ad1a587e5e70771c80f721cd6b7","externalIds":{"DBLP":"journals/corr/abs-2207-10228","ArXiv":"2207.10228","DOI":"10.48550/arXiv.2207.10228","CorpusId":250918098},"title":"MeshMAE: Masked Autoencoders for 3D Mesh Data Analysis"},{"paperId":"12cf36617c2f8bd4fd6ffba2d0344551c53db1df","externalIds":{"ArXiv":"2207.07116","DBLP":"journals/corr/abs-2207-07116","DOI":"10.48550/arXiv.2207.07116","CorpusId":250526543},"title":"Bootstrapped Masked Autoencoders for Vision BERT Pretraining"},{"paperId":"93b2d0804bfd0908cce5b04b1e3a763bf09311fe","externalIds":{"DBLP":"conf/ijcai/YuZJ22","DOI":"10.24963/ijcai.2022/513","CorpusId":250635192},"title":"Masked Feature Generation Network for Few-Shot Learning"},{"paperId":"df6c704ce133a00475c2c58f2ed705620734816c","externalIds":{"DBLP":"conf/ijcai/JingXL22","DOI":"10.24963/ijcai.2022/432","CorpusId":250634844},"title":"Graph Masked Autoencoder Enhanced Predictor for Neural Architecture Search"},{"paperId":"4f3888ea13bef9403d6c7a96c14a85575a79bce0","externalIds":{"ArXiv":"2206.13156","DBLP":"conf/miccai/ZhengLSXJ22","DOI":"10.48550/arXiv.2206.13156","CorpusId":250073035},"title":"Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification"},{"paperId":"a86fe34e17cfc4847a39ab54a2f3adda534eb43d","externalIds":{"DBLP":"conf/iclr/GuptaTZ0M023","ArXiv":"2206.11894","DOI":"10.48550/arXiv.2206.11894","CorpusId":249953817},"title":"MaskViT: Masked Visual Pre-Training for Video Prediction"},{"paperId":"8af37e55e7994860e6eeb839fd06ec271619a241","externalIds":{"DBLP":"journals/corr/abs-2206-10207","ArXiv":"2206.10207","DOI":"10.48550/arXiv.2206.10207","CorpusId":249889288},"title":"SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders"},{"paperId":"d2425b430fbf5b8ddf9cf2309c36a80a71e5a449","externalIds":{"DBLP":"journals/corr/abs-2206-08356","ArXiv":"2206.08356","DOI":"10.1109/CVPR52729.2023.01003","CorpusId":249712367},"title":"OmniMAE: Single Model Masked Pretraining on Images and Videos"},{"paperId":"2a7aee4ff519a7c0938bd397b6610707002836b3","externalIds":{"DBLP":"journals/corr/abs-2206-07706","ArXiv":"2206.07706","DOI":"10.48550/arXiv.2206.07706","CorpusId":249674757},"title":"Masked Frequency Modeling for Self-Supervised Visual Pre-Training"},{"paperId":"8215f4e7dc7ea6f588bcbc9b0f4383672545594f","externalIds":{"DBLP":"conf/cvpr/XieZCLWD023","ArXiv":"2206.04664","DOI":"10.1109/CVPR52729.2023.00999","CorpusId":249538409},"title":"On Data Scaling in Masked Image Modeling"},{"paperId":"9d4c2fc60b5c60c3639974aebdbf9743f27df785","externalIds":{"DBLP":"journals/corr/abs-2206-03826","ArXiv":"2206.03826","DOI":"10.48550/arXiv.2206.03826","CorpusId":249461729},"title":"Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks"},{"paperId":"98473b4f5bf852e09fe8acb906e74989bc573ed9","externalIds":{"DBLP":"conf/iclr/0001SH23","ArXiv":"2206.02967","CorpusId":257482844},"title":"Masked Unsupervised Self-training for Label-free Image Classification"},{"paperId":"18c92da1b7f7f8ea6877e5b3a0d5a6df14a09e00","externalIds":{"DBLP":"journals/corr/abs-2205-14540","ArXiv":"2205.14540","DOI":"10.48550/arXiv.2205.14540","CorpusId":249192001},"title":"SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners"},{"paperId":"4331838d6a6bf1baf7e6c740f8fa3ff86a64eb8d","externalIds":{"DBLP":"conf/icml/LiWWZL23","ArXiv":"2205.13943","DOI":"10.48550/arXiv.2205.13943","CorpusId":249151944},"title":"Architecture-Agnostic Masked Image Modeling - From ViT back to CNN"},{"paperId":"b34fc5871af9226d56f2a4f5d98a339a6aa3a1f4","externalIds":{"DBLP":"journals/corr/abs-2205-13543","ArXiv":"2205.13543","DOI":"10.1109/CVPR52729.2023.01391","CorpusId":249097401},"title":"Revealing the Dark Secrets of Masked Image Modeling"},{"paperId":"36e473b905bf78f9f0e22ae8e132d679f1212607","externalIds":{"DBLP":"conf/cvpr/LiuHZL023","ArXiv":"2205.13137","DOI":"10.1109/CVPR52729.2023.00605","CorpusId":257900866},"title":"MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers"},{"paperId":"d541c22a5ee930a082ba9af2f9e37cd330247be9","externalIds":{"DBLP":"journals/corr/abs-2205-13515","ArXiv":"2205.13515","DOI":"10.48550/arXiv.2205.13515","CorpusId":249097898},"title":"Green Hierarchical Vision Transformer for Masked Image Modeling"},{"paperId":"ed4e8f54074ac075148d29cf7650d0bff2ca95a8","externalIds":{"ArXiv":"2205.12551","DBLP":"conf/cvpr/RenLSBCS023","DOI":"10.1109/CVPR52729.2023.01952","CorpusId":257365045},"title":"Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers"},{"paperId":"41ec4928e1d0f802e0f4b3ee43f908ae0a25818e","externalIds":{"DBLP":"conf/miccai/JiangTTCV22","ArXiv":"2205.10342","DOI":"10.1007/978-3-031-16440-8_53","CorpusId":248964999,"PubMed":"36468915"},"title":"Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT)"},{"paperId":"32e606846f5396162294055fafd3632757e35ba2","externalIds":{"ArXiv":"2205.09616","DBLP":"journals/corr/abs-2205-09616","DOI":"10.48550/arXiv.2205.09616","CorpusId":248887665},"title":"Masked Image Modeling with Denoising Contrast"},{"paperId":"3890d82362d07064687a4b5e9024fc4c92921998","externalIds":{"DBLP":"journals/corr/abs-2205-09853","ArXiv":"2205.09853","DOI":"10.48550/arXiv.2205.09853","CorpusId":248965384},"title":"MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation"},{"paperId":"a3eeaf02dbd05ffaddf71cc6e42b97bd8b51e827","externalIds":{"DBLP":"conf/eccv/YangLSSYY22","ArXiv":"2205.01529","DOI":"10.48550/arXiv.2205.01529","CorpusId":248506080},"title":"Masked Generative Distillation"},{"paperId":"fad8dd8918a59b63f0f6df8aac31f23a0c7ad26a","externalIds":{"DBLP":"conf/wacv/ChenAASBB23","ArXiv":"2204.11716","DOI":"10.1109/WACV56688.2023.00201","CorpusId":248376910},"title":"Masked Image Modeling Advances 3D Medical Image Analysis"},{"paperId":"a6e231db70a6774abc995afc0201671e0dab4d3a","externalIds":{"ArXiv":"2204.07141","DBLP":"journals/corr/abs-2204-07141","DOI":"10.48550/arXiv.2204.07141","CorpusId":248178208},"title":"Masked Siamese Networks for Label-Efficient Learning"},{"paperId":"fb3761d27765536b204191d2a8bca2898055cb95","externalIds":{"DBLP":"conf/iccv/FangYWGSW23","ArXiv":"2204.02964","DOI":"10.1109/ICCV51070.2023.00574","CorpusId":247996501},"title":"Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection"},{"paperId":"1dfc7c6cb0f19d3384eaeb98698d64a67162c99c","externalIds":{"ArXiv":"2203.15371","DBLP":"journals/corr/abs-2203-15371","DOI":"10.48550/arXiv.2203.15371","CorpusId":247778800},"title":"mc-BEiT: Multi-choice Discretization for Image BERT Pre-training"},{"paperId":"b01a1fb77188bf3a53d642b80d1373a11527418c","externalIds":{"ArXiv":"2203.12719","DBLP":"conf/eccv/KakogeorgiouGPA22","DOI":"10.1007/978-3-031-20056-4_18","CorpusId":247627906},"title":"What to Hide from Your Students: Attention-Guided Masked Image Modeling"},{"paperId":"4990f7542f0600e0501a7e7a931b32eb7cb804d5","externalIds":{"DBLP":"journals/corr/abs-2203-12602","ArXiv":"2203.12602","DOI":"10.48550/arXiv.2203.12602","CorpusId":247619234},"title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"},{"paperId":"c96c551ece333d6e7f95f77176cedef07b3b1b18","externalIds":{"ArXiv":"2203.11183","DBLP":"journals/corr/abs-2203-11183","DOI":"10.48550/arXiv.2203.11183","CorpusId":247594448},"title":"Masked Discrimination for Self-Supervised Learning on Point Clouds"},{"paperId":"8bc61fdb50fdf05d2d99da5b276f9b5cabae13d8","externalIds":{"DBLP":"conf/eccv/YanLWZLY22","ArXiv":"2203.09855","DOI":"10.48550/arXiv.2203.09855","CorpusId":247594944},"title":"Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion"},{"paperId":"a601e71127d95bdae30fd818d2a0cc34b80b13f7","externalIds":{"DBLP":"journals/corr/abs-2203-06604","ArXiv":"2203.06604","DOI":"10.48550/arXiv.2203.06604","CorpusId":247447528},"title":"Masked Autoencoders for Point Cloud Self-supervised Learning"},{"paperId":"52d12c7c89e2ab092fdc309a99c99f1dbdfce224","externalIds":{"DBLP":"journals/corr/abs-2203-04614","ArXiv":"2203.04614","DOI":"10.48550/arXiv.2203.04614","CorpusId":247319014},"title":"Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification"},{"paperId":"7c597874535c1537d7ddff3b3723015b4dc79d30","externalIds":{"DBLP":"journals/corr/abs-2202-04200","ArXiv":"2202.04200","DOI":"10.1109/CVPR52688.2022.01103","CorpusId":246680316},"title":"MaskGIT: Masked Generative Image Transformer"},{"paperId":"dc9a76b7cb690e6a95f0f07bb3d4fabb7181b68d","externalIds":{"DBLP":"journals/corr/abs-2201-02184","ArXiv":"2201.02184","CorpusId":245769552},"title":"Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction"},{"paperId":"008a428e049003fe768068a0f1fa1416af5c4982","externalIds":{"DBLP":"journals/corr/abs-2112-09133","ArXiv":"2112.09133","DOI":"10.1109/CVPR52688.2022.01426","CorpusId":245218767},"title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training"},{"paperId":"dd2819016c6bf244c39b3e6707b60389bbdbcd21","externalIds":{"DBLP":"conf/cvpr/YuTR00L22","ArXiv":"2111.14819","DOI":"10.1109/CVPR52688.2022.01871","CorpusId":244714512},"title":"Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling"},{"paperId":"ba9d736006b897d06f75586ad46e28e00a5e566e","externalIds":{"DBLP":"journals/corr/abs-2111-12681","ArXiv":"2111.12681","CorpusId":244527662},"title":"VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling"},{"paperId":"9c4753ef43d2928866dc5bf6cec53d03373ec2fa","externalIds":{"DBLP":"conf/cvpr/Xie00LBYD022","ArXiv":"2111.09886","DOI":"10.1109/CVPR52688.2022.00943","CorpusId":244346275},"title":"SimMIM: a Simple Framework for Masked Image Modeling"},{"paperId":"1a7a28740c6eec40f9d8a8c3d987a8fa1b1437af","externalIds":{"ArXiv":"2111.09099","DBLP":"journals/corr/abs-2111-09099","DOI":"10.1109/CVPR52688.2022.01321","CorpusId":244270482},"title":"Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection"},{"paperId":"9653c070724e44f023e8cc3ec79f0b9e6d59480d","externalIds":{"DBLP":"journals/corr/abs-2111-07832","ArXiv":"2111.07832","CorpusId":244117494},"title":"iBOT: Image BERT Pre-Training with Online Tokenizer"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","externalIds":{"DBLP":"conf/cvpr/HeCXLDG22","ArXiv":"2111.06377","DOI":"10.1109/CVPR52688.2022.01553","CorpusId":243985980},"title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"2e1c5a97adfa29b2cbe203d190bf72bffe0e5511","externalIds":{"DBLP":"journals/corr/abs-2108-07954","ArXiv":"2108.07954","DOI":"10.1109/ICCV48922.2021.01000","CorpusId":237194938},"title":"Self-Supervised Visual Representations Learning by Contrastive Mask Prediction"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","externalIds":{"DBLP":"conf/iclr/Bao0PW22","ArXiv":"2106.08254","CorpusId":235436185},"title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"36b9d0f8610a82fd25854889d9327a04da4ff8fd","externalIds":{"DBLP":"journals/corr/abs-2106-05656","ArXiv":"2106.05656","CorpusId":235390630},"title":"MST: Masked Self-Supervised Transformer for Visual Representation"},{"paperId":"8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01","externalIds":{"ArXiv":"2106.05974","DBLP":"conf/nips/RiquelmePMNJPKH21","CorpusId":235417196},"title":"Scaling Vision with Sparse Mixture of Experts"},{"paperId":"0d0cf5f64c052aa7edc5bb638203616a620557f6","externalIds":{"DBLP":"journals/corr/abs-2105-04906","ArXiv":"2105.04906","CorpusId":234357520},"title":"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"0d5406775fab3e71848908327fb5504df5f60f92","externalIds":{"DBLP":"conf/nips/RidnikBNZ21","ArXiv":"2104.10972","CorpusId":233347018},"title":"ImageNet-21K Pretraining for the Masses"},{"paperId":"5f769c5df8de29d0a2cd9c020f78047013a87b34","externalIds":{"ArXiv":"2104.01027","DBLP":"journals/corr/abs-2104-01027","DOI":"10.21437/interspeech.2021-236","CorpusId":233004449},"title":"Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"47f7ec3d0a5e6e83b6768ece35206a94dc81919c","externalIds":{"ArXiv":"2012.09841","MAG":"3111551570","DBLP":"journals/corr/abs-2012-09841","DOI":"10.1109/CVPR46437.2021.01268","CorpusId":229297973},"title":"Taming Transformers for High-Resolution Image Synthesis"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"5f72c4f7a0991e7035ccc87b69410f4b3e6244ce","externalIds":{"DBLP":"journals/corr/abs-2010-07470","ArXiv":"2010.07470","MAG":"3093210455","DOI":"10.1109/TPAMI.2022.3176413","CorpusId":222377666,"PubMed":"35594229"},"title":"Masked Contrastive Representation Learning for Reinforcement Learning"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","externalIds":{"DBLP":"journals/corr/abs-2010-06000","ArXiv":"2010.06000","ACL":"2020.findings-emnlp.191","MAG":"3104050923","DOI":"10.18653/v1/2020.findings-emnlp.191","CorpusId":222310689},"title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"bd3e894559bbcbd114ab08108767b6efefbeb25e","externalIds":{"MAG":"3049757379","PubMedCentral":"7429815","DOI":"10.1038/s41467-020-17971-2","CorpusId":221129131,"PubMed":"32796848"},"title":"Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets"},{"paperId":"d84e7557bdcf540484ba305e1a06ad2d18ab2c38","externalIds":{"MAG":"3021385015","ArXiv":"2005.00214","DBLP":"journals/corr/abs-2005-00214","CorpusId":218470050},"title":"The AVA-Kinetics Localized Human Actions Video Dataset"},{"paperId":"7af72a461ed7cda180e7eab878efd5f35d79bbf4","externalIds":{"DBLP":"conf/icml/ChenK0H20","MAG":"3034978746","ArXiv":"2002.05709","CorpusId":211096730},"title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","externalIds":{"DBLP":"conf/cvpr/He0WXG20","MAG":"2987283559","ArXiv":"1911.05722","DOI":"10.1109/cvpr42600.2020.00975","CorpusId":207930212},"title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"b2b0c31d036941cb557be4afb7101dc1b72f17cb","externalIds":{"MAG":"2968609420","DBLP":"conf/iccv/UyPHNY19","ArXiv":"1908.04616","DOI":"10.1109/ICCV.2019.00167","CorpusId":199552106},"title":"Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data"},{"paperId":"4a2aa43f5fd9b7526635d78438cd942be25aaf6f","externalIds":{"ArXiv":"1908.01238","DBLP":"journals/tip/TangTFLT21","MAG":"2967296738","DOI":"10.1109/TIP.2020.3040528","CorpusId":199441899,"PubMed":"33290217"},"title":"Learning Guided Convolutional Network for Depth Completion"},{"paperId":"84c0528cb2aa4bdacad989b5b43441161dd4ecda","externalIds":{"ArXiv":"1907.06987","DBLP":"journals/corr/abs-1907-06987","MAG":"2961193895","CorpusId":196831809},"title":"A Short Note on the Kinetics-700 Human Action Dataset"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"4654aa505e5bcdb089d0df202cd7ceabc9d2d41f","externalIds":{"MAG":"2915126261","ArXiv":"1902.09063","DBLP":"journals/corr/abs-1902-09063","CorpusId":67855790},"title":"A large annotated medical image dataset for the development and evaluation of segmentation algorithms"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","externalIds":{"DBLP":"conf/miccai/PelkaKRNF18","MAG":"2897980926","DOI":"10.1007/978-3-030-01364-6_20","CorpusId":53087891},"title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"62dccab9ab715f33761a5315746ed02e48eed2a0","externalIds":{"DBLP":"journals/corr/abs-1808-01340","MAG":"2887051120","ArXiv":"1808.01340","CorpusId":51927456},"title":"A Short Note about Kinetics-600"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"7b058f72fba3b0a75deafa0fcbacdf413eea356c","externalIds":{"MAG":"2951449942","DBLP":"journals/corr/abs-1806-05104","ArXiv":"1806.05104","DOI":"10.1007/978-3-030-00931-1_76","CorpusId":49183911},"title":"Improving Cytoarchitectonic Segmentation of Human Brain Areas with Self-supervised Siamese Networks"},{"paperId":"aab368284210c1bb917ec2d31b84588e3d2d7eb4","externalIds":{"MAG":"2962742544","DBLP":"journals/corr/abs-1803-07728","ArXiv":"1803.07728","CorpusId":4009713},"title":"Unsupervised Representation Learning by Predicting Image Rotations"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d","externalIds":{"MAG":"2737258237","DBLP":"conf/cvpr/ZhouZPFB017","DOI":"10.1109/CVPR.2017.544","CorpusId":5636055},"title":"Scene Parsing through ADE20K Dataset"},{"paperId":"b68811a9b5cafe4795a11c1048541750068b7ad0","externalIds":{"MAG":"2949901290","ArXiv":"1706.04261","DBLP":"conf/iccv/GoyalKMMWKHFYMH17","DOI":"10.1109/ICCV.2017.622","CorpusId":834612},"title":"The Something Something Video Database for Learning and Evaluating Visual Common Sense"},{"paperId":"86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6","externalIds":{"DBLP":"journals/corr/KayCSZHVVGBNSZ17","ArXiv":"1705.06950","MAG":"2619947201","CorpusId":27300853},"title":"The Kinetics Human Action Video Dataset"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"35d181da0b939bdf3bdf579969e5fe69e277e03e","externalIds":{"ArXiv":"1612.06370","DBLP":"conf/cvpr/PathakGDDH17","MAG":"2575671312","DOI":"10.1109/CVPR.2017.638","CorpusId":10054272},"title":"Learning Features by Watching Objects Move"},{"paperId":"2a94c84383ee3de5e6211d43d16e7de387f68878","externalIds":{"ArXiv":"1612.03144","DBLP":"conf/cvpr/LinDGHHB17","MAG":"2565639579","DOI":"10.1109/CVPR.2017.106","CorpusId":10716717},"title":"Feature Pyramid Networks for Object Detection"},{"paperId":"729d5c7dc6bfb32e47b5bd24cdb01ccaaf62bba5","externalIds":{"DBLP":"journals/tog/YiKCSYSLHSG16","MAG":"2553307952","DOI":"10.1145/2980179.2980238","CorpusId":2880712},"title":"A scalable active framework for region annotation in 3D shape collections"},{"paperId":"7d0effebfa4bed19b6ba41f3af5b7e5b6890de87","externalIds":{"DBLP":"conf/cvpr/PathakKDDE16","MAG":"2963420272","ArXiv":"1604.07379","DOI":"10.1109/CVPR.2016.278","CorpusId":2202933},"title":"Context Encoders: Feature Learning by Inpainting"},{"paperId":"2ec8f7e0257a07d3914322b36072d1bbcd58a1e0","externalIds":{"ArXiv":"1603.09246","DBLP":"journals/corr/NorooziF16","MAG":"2949497014","DOI":"10.1007/978-3-319-46466-4_5","CorpusId":187547},"title":"Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles"},{"paperId":"8201e6e687f2de477258e9be53ba7b73ee30d7de","externalIds":{"MAG":"2326925005","DBLP":"journals/corr/ZhangIE16","ArXiv":"1603.08511","DOI":"10.1007/978-3-319-46487-9_40","CorpusId":50698},"title":"Colorful Image Colorization"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"9b686d76914befea66377ec79c1f9258d70ea7e3","externalIds":{"MAG":"2190691619","ArXiv":"1512.03012","DBLP":"journals/corr/ChangFGHHLSSSSX15","CorpusId":2554264},"title":"ShapeNet: An Information-Rich 3D Model Repository"},{"paperId":"fc1b1c9364c58ec406f494dd944b609a6a038ba6","externalIds":{"MAG":"343636949","ArXiv":"1505.05192","DBLP":"conf/iccv/DoerschGE15","DOI":"10.1109/ICCV.2015.167","CorpusId":9062671},"title":"Unsupervised Visual Representation Learning by Context Prediction"},{"paperId":"8e3f12804882b60ad5f59aad92755c5edb34860e","externalIds":{"MAG":"12634471","DBLP":"conf/eccv/BossardGG14","DOI":"10.1007/978-3-319-10599-4_29","CorpusId":12726540},"title":"Food-101 - Mining Discriminative Components with Random Forests"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","externalIds":{"ArXiv":"1409.0575","DBLP":"journals/corr/RussakovskyDSKSMHKKBBF14","MAG":"2546241758","DOI":"10.1007/s11263-015-0816-y","CorpusId":2930547},"title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"da9e411fcf740569b6b356f330a1d0fc077c8d7c","externalIds":{"MAG":"24089286","ArXiv":"1212.0402","DBLP":"journals/corr/abs-1212-0402","CorpusId":7197134},"title":"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"},{"paperId":"b3c785b99ec147049caa47f707f337b717705970","externalIds":{"MAG":"2118246710","DBLP":"journals/pami/AchantaSSLFS12","DOI":"10.1109/TPAMI.2012.120","CorpusId":1806278,"PubMed":"22641706"},"title":"SLIC Superpixels Compared to State-of-the-Art Superpixel Methods"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","externalIds":{"DBLP":"conf/nips/OrdonezKB11","MAG":"2109586012","CorpusId":14579301},"title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","externalIds":{"MAG":"2997574889","DBLP":"journals/jmlr/VincentLLBM10","DOI":"10.5555/1756006.1953039","CorpusId":17804904},"title":"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"cfc3cf1210832f8e81e75a8d776407d0d2792440","externalIds":{"DOI":"10.1017/s1049096513001224","CorpusId":219564812},"title":"International"},{"paperId":"13e0ec34998ede0c82e11780dcaa321fd4efc9ff","externalIds":{"DBLP":"conf/pakdd/GhitaI24","DOI":"10.1007/978-981-97-2253-2_4","CorpusId":269565829},"title":"A New Loss for Image Retrieval: Class Anchor Margin"},{"paperId":"33fb0bd2e7f296ff0f349499e441cb228ea02c2a","externalIds":{"DBLP":"conf/miccai/XieGHZXW23","DOI":"10.1007/978-3-031-43907-0_2","CorpusId":263673496},"title":"MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking"},{"paperId":"ffe19b9ec6292a762a49c55cb5768e9088142e03","externalIds":{"DBLP":"conf/miccai/ChenZWLL23","DOI":"10.1007/978-3-031-43904-9_48","CorpusId":263673298},"title":"Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning"},{"paperId":"1054668810e6925ed0530aa0978430f73dde937b","externalIds":{"DBLP":"conf/iclr/XiangTZ23","CorpusId":259298839},"title":"MIMT: Masked Image Modeling Transformer for Video Compression"},{"paperId":"c726f9d377409738aeacbcaa13c5c85a88db2bd8","externalIds":{"DBLP":"conf/miccai/ZhaoHOKM23","DOI":"10.1007/978-3-031-43907-0_63","CorpusId":263673438},"title":"Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images"},{"paperId":"80df05f30e2faa0f12993d4761c305f7fd131a6a","externalIds":{"DBLP":"conf/miccai/WuZSXJ23","DOI":"10.1007/978-3-031-43987-2_69","CorpusId":263673293},"title":"Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning"},{"paperId":"76f15b72b2b58c608e17bb4c75041d2b29b033f7","externalIds":{"DBLP":"conf/iclr/YangLT0WC00O23","CorpusId":259298181},"title":"Cycle-consistent Masked AutoEncoder for Unsupervised Domain Generalization"},{"paperId":"bec88d4b3ff29a52d154dc9261475fe0637a8680","externalIds":{"DBLP":"conf/miccai/LuWX23","DOI":"10.1007/978-3-031-43987-2_44","CorpusId":263673147},"title":"Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis"},{"paperId":"3dc68067b8ca726360497ddd489b81bcc8d729cf","externalIds":{"DBLP":"conf/iclr/Zhang00Y23","CorpusId":259298446},"title":"Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining"},{"paperId":"7487a10ad46622897e7dc3c3dfc65bbfc44c045b","externalIds":{"DBLP":"journals/access/ZhouL23","DOI":"10.1109/ACCESS.2023.3323383","CorpusId":264046030},"title":"Masked Autoencoders in Computer Vision: A Comprehensive Survey"},{"paperId":"8b919acdc2477f63333549b78a051145708ea2ad","externalIds":{"DBLP":"conf/nips/HuangCGXZLLX22","CorpusId":258509091},"title":"Masked Generative Adversarial Networks are Data-Efficient Generation Learners"},{"paperId":"4d90f6a0c79f62bf9a607766c9c0e7a8d36215b9","externalIds":{"DBLP":"conf/nips/GaoMLLDQ22","CorpusId":258509178},"title":"MCMAE: Masked Convolution Meets Masked Autoencoders"},{"paperId":"95178397fcc3571ba9881ec09d13fce979a18028","externalIds":{"DBLP":"conf/nips/LiuHRR22","CorpusId":258509601},"title":"Masked Prediction: A Parameter Identifiability View"},{"paperId":"bf2888234fdd3c10c76e32d1c93f4d7d7622c9cc","externalIds":{"DBLP":"conf/eccv/LiangFFWCCW22","DOI":"10.1007/978-3-031-20062-5_10","CorpusId":253523771},"title":"Point Cloud Domain Adaptation via Masked Local 3D Structure Prediction"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","externalIds":{"ArXiv":"2103.14030","DBLP":"conf/iccv/LiuL00W0LG21","DOI":"10.1109/ICCV48922.2021.00986","CorpusId":232352874},"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","externalIds":{"MAG":"2965818302","DBLP":"conf/clef/AbachaHDLDM19","CorpusId":198489641},"title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","externalIds":{"CorpusId":53712941},"title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"6c11626ae08706e6185fceff0a6d05e4bfd6bd06","externalIds":{"CorpusId":2057504},"title":"Unsupervised Learning of Visual Representations using Videos"},{"paperId":"d655f580c209f366de6a830547ee0f09aed1a8fe","externalIds":{"MAG":"152007144","DBLP":"conf/dagstuhl/GeyerHM10","DOI":"10.1007/978-3-642-14866-8_2","CorpusId":62775471},"title":"Modeling"},{"paperId":"5d90f06bb70a0a3dced62413346235c02b1aa086","externalIds":{"MAG":"2945315962","CorpusId":18268744},"title":"Learning Multiple Layers of Features from Tiny Images"}]}