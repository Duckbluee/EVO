{"references":[{"paperId":"391eaeb1092c2b145ff0e5a2fa61637a42921fce","externalIds":{"DBLP":"conf/cvpr/ChenSCJD24","ArXiv":"2311.10081","DOI":"10.1109/CVPR52733.2024.01350","CorpusId":265221232},"title":"DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"},{"paperId":"97d24f9f0d81007d57cc43e61bf2b0c9081fe184","externalIds":{"ACL":"2024.naacl-long.394","ArXiv":"2311.09677","DBLP":"conf/naacl/ZhangDLFL0CJZ24","DOI":"10.18653/v1/2024.naacl-long.394","CorpusId":265220839},"title":"R-Tuning: Instructing Large Language Models to Say ‘I Don’t Know’"},{"paperId":"d2d3960bd1e4e84cbb0051945ea1419005e75e07","externalIds":{"ArXiv":"2311.09800","DBLP":"journals/corr/abs-2311-09800","DOI":"10.48550/arXiv.2311.09800","CorpusId":265221328},"title":"Dial BeInfo for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning"},{"paperId":"b7394bf9a5e265aae6278641067f0503e24675a4","externalIds":{"DBLP":"conf/naacl/QiuECH24","ArXiv":"2311.09467","DOI":"10.48550/arXiv.2311.09467","CorpusId":265221182},"title":"Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation"},{"paperId":"b10482ab3dd1d340c3c926d92c3e617c24ee3949","externalIds":{"ArXiv":"2311.09114","DBLP":"journals/corr/abs-2311-09114","DOI":"10.48550/arXiv.2311.09114","CorpusId":265213353},"title":"Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification"},{"paperId":"3a89e289e2dd29f5e52a2bf354a637762b661257","externalIds":{"DBLP":"journals/corr/abs-2311-08401","ArXiv":"2311.08401","DOI":"10.48550/arXiv.2311.08401","CorpusId":265158181},"title":"Fine-tuning Language Models for Factuality"},{"paperId":"7a147a745f69329afb1c86becdba7b3029a169ca","externalIds":{"ArXiv":"2311.08390","DBLP":"conf/acl/Yan0CS0YLKRLB24","DOI":"10.18653/v1/2024.acl-long.541","CorpusId":265157560},"title":"Predicting Text Preference Via Structured Comparative Reasoning"},{"paperId":"f543f1d36e4556b50b160c68fd18da3d7db40867","externalIds":{"ArXiv":"2311.07424","DBLP":"journals/corr/abs-2311-07424","DOI":"10.48550/arXiv.2311.07424","CorpusId":265150344},"title":"Hallucination Augmented Recitations for Language Models"},{"paperId":"7a0081f6464de0ce95b71137b76bd2aa773b6b32","externalIds":{"DBLP":"journals/corr/abs-2310-16964","ArXiv":"2310.16964","DOI":"10.48550/arXiv.2310.16964","CorpusId":264490708},"title":"Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation"},{"paperId":"03764434729b83d4f04a8bd02f99f2500cd5bbae","externalIds":{"ArXiv":"2310.06827","DBLP":"conf/iclr/JonesPSCMMAK24","DOI":"10.48550/arXiv.2310.06827","CorpusId":263830994},"title":"Teaching Language Models to Hallucinate Less with Synthetic Tasks"},{"paperId":"cd2e04598909158494e556823d9de8baa692cee2","externalIds":{"DBLP":"journals/corr/abs-2310-06271","ArXiv":"2310.06271","DOI":"10.48550/arXiv.2310.06271","CorpusId":263828949},"title":"Towards Mitigating Hallucination in Large Language Models via Self-Reflection"},{"paperId":"99bfe503743c5ec8e16e50ab8438159cdb533a89","externalIds":{"ArXiv":"2310.04988","DBLP":"journals/corr/abs-2310-04988","DOI":"10.48550/arXiv.2310.04988","CorpusId":263831293},"title":"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations"},{"paperId":"79429814fd4d967b9277af2805c53f370e52ebb5","externalIds":{"ArXiv":"2310.03951","DBLP":"journals/corr/abs-2310-03951","DOI":"10.48550/arXiv.2310.03951","CorpusId":263831527},"title":"Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations"},{"paperId":"be177300487b6d0f25e6cade9a31900454b13281","externalIds":{"DBLP":"journals/corr/abs-2310-03214","ArXiv":"2310.03214","DOI":"10.48550/arXiv.2310.03214","CorpusId":263672149},"title":"FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"1827dd28ef866eaeb929ddf4bcfa492880aba4c7","externalIds":{"DBLP":"journals/corr/abs-2307-03987","ArXiv":"2307.03987","DOI":"10.48550/arXiv.2307.03987","CorpusId":263699899},"title":"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation"},{"paperId":"90a51ca64fc9bb3b84eb20a8c9d68ad78b49d4b7","externalIds":{"DBLP":"journals/corr/abs-2306-06085","ArXiv":"2306.06085","DOI":"10.48550/arXiv.2306.06085","CorpusId":259129531},"title":"Trapping LLM Hallucinations Using Tagged Context Prompts"},{"paperId":"405f8f5f1c6df1b3343c812832479aad5180b65f","externalIds":{"ArXiv":"2306.03341","DBLP":"journals/corr/abs-2306-03341","CorpusId":259088877},"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"},{"paperId":"0d5124d1fb7f21aa2efc0ae234feab97e8a23208","externalIds":{"DBLP":"journals/corr/abs-2305-13632","ArXiv":"2305.13632","DOI":"10.48550/arXiv.2305.13632","CorpusId":258841008},"title":"Detecting and Mitigating Hallucinations in Multilingual Summarisation"},{"paperId":"e01515c6138bc525f7aec30fc85f2adf028d4156","externalIds":{"DBLP":"journals/corr/abs-2305-03047","ArXiv":"2305.03047","DOI":"10.48550/arXiv.2305.03047","CorpusId":258479665},"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"},{"paperId":"08a80cb34d785258c770acecd302ab41ead46eed","externalIds":{"DBLP":"conf/iclr/XuSZG0FTLJ24","ArXiv":"2304.12244","CorpusId":258298159},"title":"WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"},{"paperId":"3aaf6a2cbad5850ad81ab5c163599cb3d523436f","externalIds":{"DBLP":"journals/corr/abs-2303-17651","ArXiv":"2303.17651","DOI":"10.48550/arXiv.2303.17651","CorpusId":257900871},"title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"f7212245d3787c66b8dc1e9fa4bc48349cef1155","externalIds":{"DBLP":"conf/emnlp/ChengHBZLW0WDZ23","ArXiv":"2303.08518","DOI":"10.48550/arXiv.2303.08518","CorpusId":257532394},"title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation"},{"paperId":"08b85bce712168998004ee80ce4e475390413c74","externalIds":{"ArXiv":"2302.11382","DBLP":"journals/corr/abs-2302-11382","CorpusId":257079092},"title":"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT"},{"paperId":"e965e93e76a9e6c4e4863d145b5c007b540d575d","externalIds":{"ArXiv":"2212.12017","DBLP":"journals/corr/abs-2212-12017","CorpusId":255096269},"title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"c6784d18a77176f58b75e639d0b99d797a0b4f84","externalIds":{"DBLP":"journals/corr/abs-2212-05765","ACL":"2022.emnlp-main.280","ArXiv":"2212.05765","DOI":"10.18653/v1/2022.emnlp-main.280","CorpusId":254563972},"title":"Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue"},{"paperId":"4e53b481beabba42aac027e5a8c69fed26ab4062","externalIds":{"DBLP":"conf/acl/JiLLYWZF23","ArXiv":"2212.01588","DOI":"10.48550/arXiv.2212.01588","CorpusId":254246604},"title":"RHO ($ρ$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e","externalIds":{"ArXiv":"2210.08726","DBLP":"conf/acl/GaoDPCCFZLLJG23","ACL":"2023.acl-long.910","DOI":"10.18653/v1/2023.acl-long.910","CorpusId":254247260},"title":"RARR: Researching and Revising What Language Models Say, Using Language Models"},{"paperId":"c8d594f09413b1555970f43e68847c211235d60f","externalIds":{"DBLP":"journals/corr/abs-2210-09150","ArXiv":"2210.09150","DOI":"10.48550/arXiv.2210.09150","CorpusId":252917981},"title":"Prompting GPT-3 To Be Reliable"},{"paperId":"311e48e1c4a0dcd65a6699376ffc85a24a333a56","externalIds":{"ACL":"2021.emnlp-main.645","DBLP":"conf/emnlp/ChrysostomouA21","ArXiv":"2108.13759","DOI":"10.18653/v1/2021.emnlp-main.645","CorpusId":237364609},"title":"Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","externalIds":{"DBLP":"journals/corr/abs-2006-06195","MAG":"3102995547","ArXiv":"2006.06195","CorpusId":219573512},"title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"659bf9ce7175e1ec266ff54359e2bd76e0b7ff31","externalIds":{"DBLP":"conf/nips/LewisPPPKGKLYR020","MAG":"3027879771","ArXiv":"2005.11401","CorpusId":218869575},"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","externalIds":{"DBLP":"journals/corr/abs-1910-01108","ArXiv":"1910.01108","MAG":"2978017171","CorpusId":203626972},"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","externalIds":{"MAG":"2996428491","DBLP":"journals/corr/abs-1909-11942","ArXiv":"1909.11942","CorpusId":202888986},"title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}]}