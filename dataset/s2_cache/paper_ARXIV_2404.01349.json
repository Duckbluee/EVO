{"paperId":"38f94f882255732b4ad4a154777382a8bf0d0648","externalIds":{"DBLP":"journals/corr/abs-2404-01349","ArXiv":"2404.01349","DOI":"10.1145/3682112.3682117","CorpusId":268856702},"title":"Fairness in Large Language Models: A Taxonomic Survey","openAccessPdf":{"url":"https://arxiv.org/pdf/2404.01349","status":"GREEN","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01349, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2283849596","name":"Zhibo Chu"},{"authorId":"2206777360","name":"Zichong Wang"},{"authorId":"2243674300","name":"Wenbin Zhang"}],"abstract":"Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed."}