{"references":[{"paperId":"f1f63620e87facef02234e82864c4b8adee081ec","externalIds":{"DBLP":"journals/corr/abs-2408-06327","ArXiv":"2408.06327","DOI":"10.48550/arXiv.2408.06327","CorpusId":271854812},"title":"VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents"},{"paperId":"99bc6bbeb0b8d7057a97d361ad0cbc9669d24235","externalIds":{"DBLP":"conf/nips/ChenYWLDLLDHSW024","ArXiv":"2408.03361","DOI":"10.48550/arXiv.2408.03361","CorpusId":271744921},"title":"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI"},{"paperId":"b98d9ed58fb08ec471c13c279af5c31c9c8772ff","externalIds":{"DBLP":"journals/corr/abs-2408-02718","ArXiv":"2408.02718","DOI":"10.48550/arXiv.2408.02718","CorpusId":271720013},"title":"MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models"},{"paperId":"42e02d1b10408ed81396f47c17ab3faf17964468","externalIds":{"ArXiv":"2408.01337","DBLP":"conf/ismir/WeckMBQFB24","DOI":"10.48550/arXiv.2408.01337","CorpusId":271693734},"title":"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models"},{"paperId":"1b4cf4485a37088e45e3ced97a005b27bad8e556","externalIds":{"ArXiv":"2407.16837","DBLP":"conf/nips/KilMLCWCWLC24","DOI":"10.52202/079017-0906","CorpusId":271404485},"title":"MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs"},{"paperId":"d97d7c977da22d94c7c82b57e5fa43d379c26ced","externalIds":{"ArXiv":"2407.08733","DBLP":"conf/iclr/Zhou0NLWWHWH25","DOI":"10.48550/arXiv.2407.08733","CorpusId":271098025},"title":"Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist"},{"paperId":"40b420cad2fa52491d0d001351ce18764d20eec1","externalIds":{"ArXiv":"2407.04973","DBLP":"journals/corr/abs-2407-04973","DOI":"10.48550/arXiv.2407.04973","CorpusId":271050597},"title":"LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts"},{"paperId":"0fd5fb95058d42a7566d6cf69f6112b23e3adcc1","externalIds":{"DBLP":"journals/corr/abs-2407-01511","ArXiv":"2407.01511","DOI":"10.48550/arXiv.2407.01511","CorpusId":270869926},"title":"CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents"},{"paperId":"bd3b3e8197d56aa8c20d18e61fae48753d618c5e","externalIds":{"DBLP":"conf/nips/WangXH0LZLWLMCA24","ArXiv":"2406.18521","DOI":"10.48550/arXiv.2406.18521","CorpusId":270737638},"title":"CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs"},{"paperId":"58ee9e1c426166a5451a1ce13e1186f7d6baacfd","externalIds":{"DBLP":"journals/corr/abs-2406-16338","ArXiv":"2406.16338","DOI":"10.48550/arXiv.2406.16338","CorpusId":270703034},"title":"VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models"},{"paperId":"02f41dc0b9dd7361aa90ff160952ba77fe9550cd","externalIds":{"ArXiv":"2406.17126","DBLP":"journals/corr/abs-2406-17126","DOI":"10.1145/3770854.3785678","CorpusId":270711541},"title":"MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs"},{"paperId":"94773f22b5befd0e167a7de525d29bec2b09937a","externalIds":{"ArXiv":"2406.16860","DBLP":"journals/corr/abs-2406-16860","DOI":"10.48550/arXiv.2406.16860","CorpusId":270703300},"title":"Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs"},{"paperId":"d02b9420df66330620f8853d19de610c98d2e1c1","externalIds":{"DBLP":"journals/corr/abs-2406-14515","ArXiv":"2406.14515","DOI":"10.48550/arXiv.2406.14515","CorpusId":270620008},"title":"MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding"},{"paperId":"8efecf2a2192c81be47fb0214c14b76aa9472ced","externalIds":{"DBLP":"conf/acl/0004ZYHZ0025","ArXiv":"2406.13219","DOI":"10.48550/arXiv.2406.13219","CorpusId":270620471},"title":"MC-MKE: A Fine-Grained Multimodal Knowledge Editing Benchmark Emphasizing Modality Consistency"},{"paperId":"7c752d0a7b2cb33295b50efc7a03afb88bed77b5","externalIds":{"DBLP":"journals/corr/abs-2406-13246","ArXiv":"2406.13246","DOI":"10.48550/arXiv.2406.13246","CorpusId":270619607},"title":"GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs"},{"paperId":"0257ea7eee5afb66d8394f5fdad9fd87d3ef04a2","externalIds":{"DBLP":"conf/cvpr/LiGGWWZS24","DOI":"10.1109/CVPR52733.2024.01263","CorpusId":271963485},"title":"SEED-Bench: Benchmarking Multimodal Large Language Models"},{"paperId":"920731b589af90a5b79236f4939ac117bbb939f2","externalIds":{"DBLP":"conf/cvpr/MajumdarA0PYHSM24","DOI":"10.1109/CVPR52733.2024.01560","CorpusId":268066655},"title":"OpenEQA: Embodied Question Answering in the Era of Foundation Models"},{"paperId":"583eb562bd5f210b305154b2235d668d3949a195","externalIds":{"DBLP":"conf/cvpr/LiuLWWTHL0YLZ25","ArXiv":"2406.10638","DOI":"10.1109/CVPR52734.2025.00849","CorpusId":270559998},"title":"Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly"},{"paperId":"cb98545d6ff56f4d0f8b639d3e778b7dcbf029cc","externalIds":{"DBLP":"journals/corr/abs-2406-10424","ArXiv":"2406.10424","DOI":"10.48550/arXiv.2406.10424","CorpusId":270559280},"title":"What is the Visual Cognition Gap between Humans and Multimodal LLMs?"},{"paperId":"fdb3864b5c8914adedfce0245648604191e6e38e","externalIds":{"ArXiv":"2406.09411","DBLP":"conf/iclr/WangFH0LLMXZZYM25","DOI":"10.48550/arXiv.2406.09411","CorpusId":270440454},"title":"MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding"},{"paperId":"62e7ee3cc8bc0ad865bcd1c02928d9d4263530bc","externalIds":{"ArXiv":"2406.09121","CorpusId":270440929},"title":"MMRel: Benchmarking Relation Understanding in Multi-Modal Large Language Models"},{"paperId":"e28f145beea9b3b43c13d38522d77ad13dd12406","externalIds":{"ArXiv":"2406.07057","DBLP":"conf/nips/ZhangHSLZFWCYWS24","DOI":"10.52202/079017-1561","CorpusId":270379776},"title":"MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models"},{"paperId":"fcfbf158468d35a3f339e525e6810f8950edd2b2","externalIds":{"DBLP":"conf/nips/WangZRDLLH0ZLZL24","ArXiv":"2406.07230","DOI":"10.48550/arXiv.2406.07230","CorpusId":270379788},"title":"Needle In A Multimodal Haystack"},{"paperId":"3aabd69e13f64f10fd210e4e9e6b2e75c0e734d1","externalIds":{"DBLP":"conf/nips/RomeroLWGMPOVBJ24","ArXiv":"2406.05967","DOI":"10.48550/arXiv.2406.05967","CorpusId":270371898},"title":"CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark"},{"paperId":"064dfc54498e60266132c0ccf5ab22ae849f99a0","externalIds":{"DBLP":"conf/nips/LiuFFD0WBZFGLLN24","ArXiv":"2406.05862","DOI":"10.48550/arXiv.2406.05862","CorpusId":270371872},"title":"II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models"},{"paperId":"899d30a3796b377e01866fac1e9be037590471bb","externalIds":{"ArXiv":"2406.05343","DBLP":"journals/corr/abs-2406-05343","DOI":"10.48550/arXiv.2406.05343","CorpusId":270370866},"title":"M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark"},{"paperId":"0072019b934df4a802c2f948ed090c7745b32dd5","externalIds":{"DBLP":"journals/corr/abs-2406-01584","ArXiv":"2406.01584","DOI":"10.48550/arXiv.2406.01584","CorpusId":270215984},"title":"SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model"},{"paperId":"acb53707ebc5ce954d9821fc721382d997befbe7","externalIds":{"DBLP":"journals/corr/abs-2405-16473","ArXiv":"2405.16473","DOI":"10.48550/arXiv.2405.16473","CorpusId":270062772},"title":"M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought"},{"paperId":"0abc653fe34ce293b6a2db3bd59341e6fd1b3dc6","externalIds":{"DBLP":"conf/nips/LiWZQD0BL24","ArXiv":"2405.12523","DOI":"10.48550/arXiv.2405.12523","CorpusId":269930051},"title":"Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models"},{"paperId":"08f3a096bc3f950d09f194b8659f2862b6dc2bd9","externalIds":{"DBLP":"journals/corr/abs-2405-11985","ArXiv":"2405.11985","DOI":"10.48550/arXiv.2405.11985","CorpusId":269921884},"title":"MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering"},{"paperId":"8e236f3f44ffedfa4b58570fc02b91c59b871018","externalIds":{"ArXiv":"2405.09713","DBLP":"conf/cvpr/WangWCCGLLG24","DOI":"10.1109/CVPR52733.2024.01271","CorpusId":269791023},"title":"SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge"},{"paperId":"e852785ce2acc2b79aad22f7b2585585a9c88bfc","externalIds":{"DBLP":"conf/nips/00040HA24","ArXiv":"2405.08807","DOI":"10.48550/arXiv.2405.08807","CorpusId":269761780},"title":"SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation"},{"paperId":"b9a64f44a2de148a8db8aa6c91584b00d4717cf0","externalIds":{"DBLP":"conf/acl/ZouSZ0FSYC24","ArXiv":"2404.15592","DOI":"10.48550/arXiv.2404.15592","CorpusId":269330205},"title":"ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction"},{"paperId":"ff05de1241fd5a5a2e05f7ed298e5ba9a123d7ba","externalIds":{"DBLP":"conf/nips/000100SAMIP24","ArXiv":"2404.13591","DOI":"10.48550/arXiv.2404.13591","CorpusId":269293027},"title":"MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning"},{"paperId":"c50dd4cc8affd2cc53ac825905d54dd6b3a4a4f9","externalIds":{"ArXiv":"2404.12966","DBLP":"conf/mm/LiTJQZ00025","DOI":"10.1145/3746027.3754720","CorpusId":269282799},"title":"Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive Reasoning"},{"paperId":"0e376e8f3ea918ba696cdd2d0c8d0215e94cf0a8","externalIds":{"ArXiv":"2404.09619","DBLP":"journals/corr/abs-2404-09619","DOI":"10.48550/arXiv.2404.09619","CorpusId":269149019},"title":"UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark"},{"paperId":"1b93eba7e1f1ca764d94909cd7a936ab12002eb4","externalIds":{"ArXiv":"2404.07917","DBLP":"journals/jcise/DorisGTAACA25","DOI":"10.48550/arXiv.2404.07917","CorpusId":269042942},"title":"DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation"},{"paperId":"6339ec6b3a53d3832b3e9b6896bdf3ee7ddb3663","externalIds":{"ArXiv":"2404.07922","DBLP":"journals/corr/abs-2404-07922","DOI":"10.48550/arXiv.2404.07922","CorpusId":269042849},"title":"LaVy: Vietnamese Multimodal Large Language Model"},{"paperId":"3687e55cf21c4d0041d9bf0b74988319bfe3402b","externalIds":{"DBLP":"journals/corr/abs-2404-05719","ArXiv":"2404.05719","DOI":"10.48550/arXiv.2404.05719","CorpusId":269005503},"title":"Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs"},{"paperId":"f019c9661b253ddb611e930348e20ddcd350a952","externalIds":{"ArXiv":"2404.03027","CorpusId":268889385},"title":"JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks"},{"paperId":"2af15dd679b9c3c91030e2bf2047abbca0194d94","externalIds":{"ArXiv":"2404.00578","DBLP":"journals/corr/abs-2404-00578","DOI":"10.48550/arXiv.2404.00578","CorpusId":268819811},"title":"M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models"},{"paperId":"3c026fbab985ad0af6bfe4818b94c29a286fa4b7","externalIds":{"ArXiv":"2403.20271","DBLP":"journals/corr/abs-2403-20271","DOI":"10.48550/arXiv.2403.20271","CorpusId":268793990},"title":"Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want"},{"paperId":"8a9e11addba791860a9dbf15de75cc28d2cf844c","externalIds":{"DBLP":"conf/nips/ChenLDZZCDWQLZ24","ArXiv":"2403.20330","DOI":"10.48550/arXiv.2403.20330","CorpusId":268793433},"title":"Are We on the Right Way for Evaluating Large Vision-Language Models?"},{"paperId":"6d017adda6b2b1ea627dde2f0e85401ebb9fe566","externalIds":{"DBLP":"journals/corr/abs-2403-14624","ArXiv":"2403.14624","DOI":"10.48550/arXiv.2403.14624","CorpusId":268554279},"title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?"},{"paperId":"d5dc38ed5a3e386a2f4303ef858796f94bbb44d0","externalIds":{"DBLP":"conf/acl/AlwajihNBMA24","ArXiv":"2403.01031","DOI":"10.48550/arXiv.2403.01031","CorpusId":268230575},"title":"Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks"},{"paperId":"437fbf13aa6c141c26d328921fae1dc40a2f234d","externalIds":{"DBLP":"conf/eccv/WangRLLYCWLLZQD24","ArXiv":"2402.19474","DOI":"10.48550/arXiv.2402.19474","CorpusId":268091186},"title":"The All-Seeing Project V2: Towards General Relation Comprehension of the Open World"},{"paperId":"4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2","externalIds":{"DBLP":"conf/acl/Huang0G024","ArXiv":"2402.14683","DOI":"10.48550/arXiv.2402.14683","CorpusId":267782634},"title":"Visual Hallucinations of Multi-modal Large Language Models"},{"paperId":"3fd4a8f70f60c9cf945762d1a4c3f70b90496e7f","externalIds":{"ArXiv":"2402.14154","DBLP":"journals/corr/abs-2402-14154","DOI":"10.48550/arXiv.2402.14154","CorpusId":267782833},"title":"MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms"},{"paperId":"73d57469550b92e1dce5debdddae725f2a582581","externalIds":{"DBLP":"journals/corr/abs-2402-13220","ArXiv":"2402.13220","DOI":"10.48550/arXiv.2402.13220","CorpusId":267760238},"title":"How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts"},{"paperId":"9ab82096689de59b108a0bf2667ab12cac2f8271","externalIds":{"DBLP":"journals/corr/abs-2402-12185","ArXiv":"2402.12185","DOI":"10.1109/TIP.2025.3607618","CorpusId":267751223,"PubMed":"41082427"},"title":"ChartX and ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning"},{"paperId":"1ecc1583a38fed4fe232b2d76d5c60086f9fa36a","externalIds":{"DBLP":"journals/corr/abs-2402-14835","ArXiv":"2402.14835","DOI":"10.48550/arXiv.2402.14835","CorpusId":267897663},"title":"MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing"},{"paperId":"e9b462fce328853292a7b6d5a71f4f80610ad190","externalIds":{"DBLP":"journals/corr/abs-2402-11217","ArXiv":"2402.11217","DOI":"10.48550/arXiv.2402.11217","CorpusId":267751054},"title":"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"},{"paperId":"7dddcbb769ce827cc7154af50cb72658fa847d88","externalIds":{"DBLP":"conf/icassp/ChaLLY24","ArXiv":"2402.09717","DOI":"10.1109/ICASSP48485.2024.10446658","CorpusId":267681788},"title":"Visually Dehallucinative Instruction Generation"},{"paperId":"57a16e741016638dfd5b5f4c8a839c65ef83b817","externalIds":{"ArXiv":"2402.07729","DBLP":"conf/acl/YangXLC0ZLLZZZ24","DOI":"10.48550/arXiv.2402.07729","CorpusId":267626820},"title":"AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension"},{"paperId":"3e75ac0db303e9acdf3ba4b524a1f69c22824ac8","externalIds":{"DBLP":"journals/pami/ZhangWZZL24","ArXiv":"2402.07116","DOI":"10.1109/TPAMI.2024.3445770","CorpusId":267627209,"PubMed":"39167507"},"title":"Q-Bench<inline-formula><tex-math notation=\"LaTeX\">$^+$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3445770.gif\"/></alternatives></inline-formula>: A Benchmark for Multi-Modal Foundation Models on Low-Level Visi"},{"paperId":"a1f76db91c0debcf93ae9889736bce8470902113","externalIds":{"DBLP":"journals/corr/abs-2402-06196","ArXiv":"2402.06196","DOI":"10.48550/arXiv.2402.06196","CorpusId":267617032},"title":"Large Language Models: A Survey"},{"paperId":"cdf6d0eb9673ba5b83735ff358743f4226ed88b7","externalIds":{"ArXiv":"2402.04178","DBLP":"journals/visintelligence/ShiGLWFHWCYC25","DOI":"10.1007/s44267-025-00079-w","CorpusId":267499747},"title":"SHIELD: an evaluation benchmark for face spoofing and forgery detection with multimodal large language models"},{"paperId":"21f1a99feff322b2c2af8c4239eb86ffe64b613b","externalIds":{"DBLP":"journals/corr/abs-2402-05138","ArXiv":"2402.05138","DOI":"10.48550/arXiv.2402.05138","CorpusId":267547996},"title":"SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark"},{"paperId":"9b86d92b01d923dbf386aeeb80ad4d79e530379e","externalIds":{"DBLP":"journals/corr/abs-2402-03173","ArXiv":"2402.03173","DOI":"10.1007/s11432-024-4602-x","CorpusId":267411732},"title":"MULTI: multimodal understanding leaderboard with text and images"},{"paperId":"19e909f88b8b9b0635bd6e441094e1738c3bba9a","externalIds":{"ArXiv":"2402.03190","DBLP":"conf/acl/ChenWXZYLSLGC24","DOI":"10.48550/arXiv.2402.03190","CorpusId":267412532},"title":"Unified Hallucination Detection for Multimodal Large Language Models"},{"paperId":"bce43cb9af37a0c8d90f8cadaebd6bb002685edd","externalIds":{"ArXiv":"2401.16420","DBLP":"journals/corr/abs-2401-16420","DOI":"10.48550/arXiv.2401.16420","CorpusId":267311889},"title":"InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model"},{"paperId":"c5db6c2726911b72d534f97bd4d1ed63f6431340","externalIds":{"ArXiv":"2401.16158","DBLP":"journals/corr/abs-2401-16158","DOI":"10.48550/arXiv.2401.16158","CorpusId":267311877},"title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception"},{"paperId":"1524b7ff78755a3445d22400a8d6c75ba8c0cd65","externalIds":{"DBLP":"journals/corr/abs-2401-12915","ArXiv":"2401.12915","DOI":"10.48550/arXiv.2401.12915","CorpusId":267094801},"title":"Red Teaming Visual Language Models"},{"paperId":"a35084f393108217938c393f450dc7aff4a7ceb1","externalIds":{"DBLP":"journals/corr/abs-2401-11944","ArXiv":"2401.11944","DOI":"10.48550/arXiv.2401.11944","CorpusId":267068665},"title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark"},{"paperId":"e0702a22e0841c54ab865b4996d7b07af192a3e1","externalIds":{"DBLP":"journals/corr/abs-2401-10529","ArXiv":"2401.10529","DOI":"10.48550/arXiv.2401.10529","CorpusId":267061347},"title":"Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"},{"paperId":"a07de765c59755fa9653276f0b5ccb68f77d7660","externalIds":{"ArXiv":"2401.08276","DBLP":"journals/corr/abs-2401-08276","DOI":"10.48550/arXiv.2401.08276","CorpusId":267027512},"title":"AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception"},{"paperId":"f48b95576bd50a69937d8bbff0cc42bdb20e49f5","externalIds":{"DBLP":"conf/acl/WangLLLWW24","ArXiv":"2401.07529","DOI":"10.48550/arXiv.2401.07529","CorpusId":266999192},"title":"MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception"},{"paperId":"ca00f4056f9039d3c1a4c3a113f5ee0527149b66","externalIds":{"ArXiv":"2401.06209","DBLP":"journals/corr/abs-2401-06209","DOI":"10.1109/CVPR52733.2024.00914","CorpusId":266976992},"title":"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs"},{"paperId":"1479196adc7f321b9b260e1ff06dddb7599b55df","externalIds":{"DBLP":"journals/corr/abs-2401-04471","ArXiv":"2401.04471","DOI":"10.48550/arXiv.2401.04471","CorpusId":266900046},"title":"TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models"},{"paperId":"f2b6323973955f9a1ebb9be76a616991de3d3a8f","externalIds":{"DBLP":"journals/corr/abs-2312-15915","ArXiv":"2312.15915","DOI":"10.48550/arXiv.2312.15915","CorpusId":266550948},"title":"ChartBench: A Benchmark for Complex Visual Reasoning in Charts"},{"paperId":"c672ec79f55cef8f7a32cd8dddfa981b893f1567","externalIds":{"DBLP":"journals/corr/abs-2312-14135","ArXiv":"2312.14135","DOI":"10.1109/CVPR52733.2024.01243","CorpusId":266436019},"title":"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"},{"paperId":"c2f1a101eb1c54cab321c47ce4f3b4b416f4786a","externalIds":{"DBLP":"journals/corr/abs-2312-10763","ArXiv":"2312.10763","DOI":"10.48550/arXiv.2312.10763","CorpusId":266359182},"title":"M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts"},{"paperId":"d77bc1a237b67c57b0c1b99b4802e703747a9688","externalIds":{"DBLP":"journals/corr/abs-2312-02896","ArXiv":"2312.02896","DOI":"10.48550/arXiv.2312.02896","CorpusId":265658792},"title":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"},{"paperId":"0ac8e8d765724e1cea5ea24c84772dffab961359","externalIds":{"ArXiv":"2312.00081","DBLP":"conf/cvpr/PengXYLW24","DOI":"10.1109/CVPR52733.2024.01261","CorpusId":265551993},"title":"Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding"},{"paperId":"1a5a79b393b3f00eb5a47243ee031ad799d2f641","externalIds":{"DBLP":"conf/eccv/LiuZGLYQ24","ArXiv":"2311.17600","DOI":"10.1007/978-3-031-72992-8_22","CorpusId":265498692},"title":"MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"},{"paperId":"ea3448eb86a233189631d914721e587d45931b64","externalIds":{"DBLP":"conf/cvpr/0002WH00LWX0L0024","ArXiv":"2311.17005","DOI":"10.1109/CVPR52733.2024.02095","CorpusId":265466214},"title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"},{"paperId":"b50d19c5c298f6562c3b3c6c3822a351bdc89260","externalIds":{"ArXiv":"2311.16502","DBLP":"journals/corr/abs-2311-16502","DOI":"10.1109/CVPR52733.2024.00913","CorpusId":265466525},"title":"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"},{"paperId":"0f993809c1fe00403ecea66d8f572832f075cfe4","externalIds":{"ArXiv":"2311.10774","ACL":"2024.naacl-long.70","DBLP":"journals/corr/abs-2311-10774","DOI":"10.48550/arXiv.2311.10774","CorpusId":265294419},"title":"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning"},{"paperId":"18940a4ccd955c72930ee0f8771ff710a9afeef3","externalIds":{"ArXiv":"2311.07397","DBLP":"journals/corr/abs-2311-07397","DOI":"10.48550/arXiv.2311.07397","CorpusId":265149533},"title":"An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation"},{"paperId":"ad13b213681b6f634bc83a264df246e83dd9a9d9","externalIds":{"DBLP":"conf/cvpr/YeXYYHL0Z024","ArXiv":"2311.04257","DOI":"10.1109/CVPR52733.2024.01239","CorpusId":265050943},"title":"mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"},{"paperId":"18a3ec6c7aca5fc6e21455db46b6aaff22bc1a35","externalIds":{"DBLP":"journals/corr/abs-2311-03287","ArXiv":"2311.03287","DOI":"10.48550/arXiv.2311.03287","CorpusId":265033982},"title":"Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges"},{"paperId":"1fa674b4e1c82cf648868be2568637fdfb5fb1b3","externalIds":{"DBLP":"conf/emnlp/KamathHC23a","ArXiv":"2310.19785","DOI":"10.48550/arXiv.2310.19785","CorpusId":264805345},"title":"What's \"up\" with vision-language models? Investigating their struggle with spatial reasoning"},{"paperId":"0b395ed1c8b284e551172b728e83cf257e33729a","externalIds":{"ArXiv":"2310.14566","DBLP":"conf/cvpr/GuanLWXLL0CHYM024","DOI":"10.1109/CVPR52733.2024.01363","CorpusId":265499116},"title":"Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models"},{"paperId":"458111ac5a0f73bb35a2acf55298268be25ccfa2","externalIds":{"ArXiv":"2310.07704","DBLP":"journals/corr/abs-2310-07704","DOI":"10.48550/arXiv.2310.07704","CorpusId":263834718},"title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"8946891e94831adc8cddb0d32311cce2445c96d2","externalIds":{"DBLP":"conf/iclr/LuBX0LH0CG024","ArXiv":"2310.02255","CorpusId":264491155},"title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"},{"paperId":"bee68767debbdc96d6f75947e544a8be98b869e3","externalIds":{"DBLP":"journals/corr/abs-2310-02071","ArXiv":"2310.02071","DOI":"10.48550/arXiv.2310.02071","CorpusId":263609022},"title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"},{"paperId":"556505a06b7d09fd4e8b415d56561d950a6790c2","externalIds":{"ArXiv":"2309.16499","DBLP":"journals/corr/abs-2309-16499","DOI":"10.48550/arXiv.2309.16499","CorpusId":263135324},"title":"Cross-City Matters: A Multimodal Remote Sensing Benchmark Dataset for Cross-City Semantic Segmentation using High-Resolution Domain Adaptation Networks"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"593b42c628b49937bcd7f7c4a7d54d5f97e6b414","externalIds":{"DBLP":"journals/corr/abs-2309-14181","ArXiv":"2309.14181","DOI":"10.48550/arXiv.2309.14181","CorpusId":262824606},"title":"Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision"},{"paperId":"786294f4008732a5dac9895a8507bc4c80450075","externalIds":{"DBLP":"conf/icassp/HuangLWHKWACSPS24","ArXiv":"2309.09510","DOI":"10.1109/ICASSP48485.2024.10448257","CorpusId":262046689},"title":"Dynamic-Superb: Towards a Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark For Speech"},{"paperId":"2928e5a5ee488104c5d7b636f577baef2d470310","externalIds":{"DBLP":"journals/corr/abs-2308-16890","ArXiv":"2308.16890","DOI":"10.48550/arXiv.2308.16890","CorpusId":261397179},"title":"TouchStone: Evaluating Vision-Language Models by Language Models"},{"paperId":"bb1083425517bdac8d9a6438fcf5032543acb20e","externalIds":{"DBLP":"journals/corr/abs-2308-15126","ArXiv":"2308.15126","DOI":"10.48550/arXiv.2308.15126","CorpusId":261276646},"title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models"},{"paperId":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","externalIds":{"ArXiv":"2308.12966","CorpusId":261101015},"title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"},{"paperId":"658cd67a91da86cf451e6f1b015f762b56015172","externalIds":{"DBLP":"conf/aaai/GunjalYB24","ArXiv":"2308.06394","DOI":"10.48550/arXiv.2308.06394","CorpusId":260887222},"title":"Detecting and Preventing Hallucinations in Large Vision Language Models"},{"paperId":"2bd1b8990db73b6495c11082bea2d5f925c5226f","externalIds":{"ArXiv":"2308.03349","DBLP":"journals/corr/abs-2308-03349","DOI":"10.48550/arXiv.2308.03349","CorpusId":260682715},"title":"SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","externalIds":{"ArXiv":"2308.02490","DBLP":"journals/corr/abs-2308-02490","DOI":"10.48550/arXiv.2308.02490","CorpusId":260611572},"title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"7fbc502441d66daf1f53765d5d86a8dfba9ab0ce","externalIds":{"DBLP":"journals/corr/abs-2308-01390","ArXiv":"2308.01390","DOI":"10.48550/arXiv.2308.01390","CorpusId":261043320},"title":"OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"},{"paperId":"4309d572a37d655779f9dce6a2c98c66334132de","externalIds":{"DBLP":"journals/corr/abs-2307-16125","ArXiv":"2307.16125","DOI":"10.48550/arXiv.2307.16125","CorpusId":260334888},"title":"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"},{"paperId":"872c111c4bed5aba086cc023ce6279edb469220a","externalIds":{"DBLP":"journals/corr/abs-2307-15266","ArXiv":"2307.15266","DOI":"10.48550/arXiv.2307.15266","CorpusId":260316172},"title":"RSGPT: A Remote Sensing Vision Language Model and Benchmark"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","externalIds":{"DBLP":"journals/corr/abs-2307-08581","ArXiv":"2307.08581","DOI":"10.48550/arXiv.2307.08581","CorpusId":259937702},"title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"b37b1dc72b1882858f5120f2cd6883134089a6ed","externalIds":{"ArXiv":"2307.06281","DBLP":"journals/corr/abs-2307-06281","DOI":"10.48550/arXiv.2307.06281","CorpusId":259837088},"title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","externalIds":{"ArXiv":"2307.02469","ACL":"2024.naacl-long.440","DBLP":"conf/naacl/ZengZZXWWZKS24","DOI":"10.48550/arXiv.2307.02469","CorpusId":259342291},"title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","externalIds":{"ArXiv":"2306.13394","DBLP":"journals/corr/abs-2306-13394","DOI":"10.48550/arXiv.2306.13394","CorpusId":259243928},"title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"948e8cfae92c2004f2dd5c9316f5972f8baaea21","externalIds":{"DBLP":"journals/corr/abs-2306-16527","ArXiv":"2306.16527","DOI":"10.48550/arXiv.2306.16527","CorpusId":259287020},"title":"OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","externalIds":{"DBLP":"journals/corr/abs-2306-07207","ArXiv":"2306.07207","DOI":"10.1145/3796716","CorpusId":259138706},"title":"Valley: Video Assistant with Large Language Model Enhanced Ability"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","externalIds":{"ArXiv":"2306.06687","DBLP":"conf/nips/YinWCSLLH0S0SO23","DOI":"10.48550/arXiv.2306.06687","CorpusId":259138958},"title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"89689059d0cdcb52d7fbb6007ab953db22936a90","externalIds":{"DBLP":"conf/nips/ZhangAGCB23","ArXiv":"2306.05179","DOI":"10.48550/arXiv.2306.05179","CorpusId":259108959},"title":"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","externalIds":{"DBLP":"journals/corr/abs-2306-00890","ArXiv":"2306.00890","DOI":"10.48550/arXiv.2306.00890","CorpusId":258999820},"title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","externalIds":{"DBLP":"journals/ijcv/ZangLHZL25","ArXiv":"2305.18279","DOI":"10.1007/s11263-024-02214-4","CorpusId":258959011},"title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","externalIds":{"DBLP":"journals/corr/abs-2305-16355","ArXiv":"2305.16355","ACL":"2023.tllm-1.2","DOI":"10.48550/arXiv.2305.16355","CorpusId":258947721},"title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"206400aba5f12f734cdd2e4ab48ef6014ea60773","externalIds":{"DBLP":"journals/corr/abs-2305-10355","ArXiv":"2305.10355","DOI":"10.48550/arXiv.2305.10355","CorpusId":258740697},"title":"Evaluating Object Hallucination in Large Vision-Language Models"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","externalIds":{"ArXiv":"2304.15010","DBLP":"journals/corr/abs-2304-15010","DOI":"10.48550/arXiv.2304.15010","CorpusId":258418343},"title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"690df0820f35a47e1ce44f90e6ddb4132aa09267","externalIds":{"DBLP":"journals/pami/ZhangHJL24","ArXiv":"2304.00685","DOI":"10.1109/TPAMI.2024.3369699","CorpusId":257913547,"PubMed":"38408000"},"title":"Vision-Language Models for Vision Tasks: A Survey"},{"paperId":"fc8988585c6846fdeee33b34779a6a87b92c3e86","externalIds":{"ArXiv":"2303.14465","DBLP":"journals/corr/abs-2303-14465","DOI":"10.1109/ICCV51070.2023.01102","CorpusId":257766245},"title":"Equivariant Similarity for Vision-Language Foundation Models"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","externalIds":{"ArXiv":"2303.11381","DBLP":"journals/corr/abs-2303-11381","DOI":"10.48550/arXiv.2303.11381","CorpusId":257637012},"title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"6c529e9ac309735a41c072a840f10ba652f9b909","externalIds":{"DOI":"10.5194/essd-15-113-2023","CorpusId":255233504},"title":"MDAS: a new multimodal benchmark dataset for remote sensing"},{"paperId":"74b860abb88f5a4ccd043e2da15ae40eb63625f6","externalIds":{"DBLP":"journals/tomccap/LeiCYDZ23","DOI":"10.1145/3573010","CorpusId":254294097},"title":"Learning the Userâ€™s Deeper Preferences for Multi-modal Recommendation Systems"},{"paperId":"1436ee75f8dded44de157cf778a96bdaf6b20a76","externalIds":{"ArXiv":"2212.05935","DBLP":"journals/corr/abs-2212-05935","DOI":"10.48550/arXiv.2212.05935","CorpusId":254563929},"title":"Hierarchical multimodal transformers for Multi-Page DocVQA"},{"paperId":"a02fbaf22237a1aedacb1320b6007cd70c1fe6ec","externalIds":{"DBLP":"journals/corr/abs-2212-04356","ArXiv":"2212.04356","CorpusId":252923993},"title":"Robust Speech Recognition via Large-Scale Weak Supervision"},{"paperId":"10667c1ae4b49808772b5a377c5b52196701267f","externalIds":{"DBLP":"conf/iclr/Yuksekgonul0KJ023","ArXiv":"2210.01936","DOI":"10.48550/arXiv.2210.01936","CorpusId":252734947},"title":"When and why vision-language models behave like bags-of-words, and what to do about it?"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"47a67e76ed84260ff19f7a948d764005d1edf1c9","externalIds":{"DBLP":"journals/corr/abs-2206-01718","ArXiv":"2206.01718","DOI":"10.48550/arXiv.2206.01718","CorpusId":249375629},"title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"},{"paperId":"354b48677e314ef2f47512c5a81723cfd17dd05d","externalIds":{"ACL":"2023.tacl-1.37","DBLP":"journals/corr/abs-2205-00363","ArXiv":"2205.00363","DOI":"10.1162/tacl_a_00566","CorpusId":248496506},"title":"Visual Spatial Reasoning"},{"paperId":"b611c501269224702d1a9942c8600a31ec66ab28","externalIds":{"ArXiv":"2203.10244","DBLP":"conf/acl/MasryLTJH22","ACL":"2022.findings-acl.177","DOI":"10.48550/arXiv.2203.10244","CorpusId":247593713},"title":"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"8f6c652a392995bd047a2f7b94474ab1e6e23ff0","externalIds":{"DBLP":"conf/cvpr/AzumaMKK22","ArXiv":"2112.10482","DOI":"10.1109/CVPR52688.2022.01854","CorpusId":245334889},"title":"ScanQA: 3D Question Answering for Spatial Scene Understanding"},{"paperId":"dd2819016c6bf244c39b3e6707b60389bbdbcd21","externalIds":{"DBLP":"conf/cvpr/YuTR00L22","ArXiv":"2111.14819","DOI":"10.1109/CVPR52688.2022.01871","CorpusId":244714512},"title":"Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling"},{"paperId":"ec8afc75ec219f2a5f9ed9d7c9dde0720f69b5a2","externalIds":{"DBLP":"conf/icml/ZengZL22","ArXiv":"2111.08276","CorpusId":244129883},"title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"fb1c90806fc5ec72987f58110aa255edbce6620d","externalIds":{"ACL":"2021.acl-long.528","MAG":"3176371077","ArXiv":"2105.04165","DBLP":"conf/acl/LuGJQHLZ20","DOI":"10.18653/v1/2021.acl-long.528","CorpusId":234337054},"title":"Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","externalIds":{"ArXiv":"2102.03334","DBLP":"journals/corr/abs-2102-03334","CorpusId":231839613},"title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"228e4d2b3afa1cdb9f6f87c5eae88cb93f6a0a6e","externalIds":{"DBLP":"journals/corr/abs-2008-08899","ArXiv":"2008.08899","MAG":"3077892130","CorpusId":221186646},"title":"Document Visual Question Answering Challenge 2020"},{"paperId":"33eadd4e666a894306a22ba0839c5e0cef77280e","externalIds":{"ArXiv":"2003.12462","MAG":"3106859150","DBLP":"journals/corr/abs-2003-12462","DOI":"10.1007/978-3-030-58536-5_44","CorpusId":214693197},"title":"TextCaps: a Dataset for Image Captioning with Reading Comprehension"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","externalIds":{"MAG":"2969862959","DBLP":"conf/emnlp/TanB19","ACL":"D19-1514","ArXiv":"1908.07490","DOI":"10.18653/v1/D19-1514","CorpusId":201103729},"title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"c4798919e74411d87f7745840e45b8bcf61128ff","externalIds":{"MAG":"2945761034","DBLP":"conf/naacl/KimKLK19","ACL":"N19-1011","DOI":"10.18653/v1/N19-1011","CorpusId":174799768},"title":"AudioCaps: Generating Captions for Audios in The Wild"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","externalIds":{"DBLP":"conf/cvpr/MarinoRFM19","ArXiv":"1906.00067","MAG":"2947312908","DOI":"10.1109/CVPR.2019.00331","CorpusId":173991173},"title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"af1f7739283bdbd2b7a94903041f6d6afd991907","externalIds":{"MAG":"2936135081","DBLP":"journals/corr/abs-1904-08920","ArXiv":"1904.08920","DOI":"10.1109/CVPR.2019.00851","CorpusId":85553602},"title":"Towards VQA Models That Can Read"},{"paperId":"d0bfd3cb732471a0843a39d2d047caf60a844466","externalIds":{"ArXiv":"1903.02741","DBLP":"conf/cvpr/ZhangGJZZ19","MAG":"2953612685","DOI":"10.1109/CVPR.2019.00546","CorpusId":71148268},"title":"RAVEN: A Dataset for Relational and Analogical Visual REasoNing"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c","externalIds":{"DBLP":"conf/cvpr/Gurari0SGLGLB18","MAG":"2788643321","ArXiv":"1802.08218","DOI":"10.1109/CVPR.2018.00380","CorpusId":3831582},"title":"VizWiz Grand Challenge: Answering Visual Questions from Blind People"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","externalIds":{"DBLP":"journals/corr/JohnsonHMFZG16","ArXiv":"1612.06890","MAG":"2953212746","DOI":"10.1109/CVPR.2017.215","CorpusId":15458100},"title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"d997beefc0922d97202789d2ac307c55c2c52fba","externalIds":{"MAG":"2950642167","DBLP":"conf/cvpr/QiSMG17","ArXiv":"1612.00593","DOI":"10.1109/CVPR.2017.16","CorpusId":5115938},"title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"},{"paperId":"b60630911d7746fba06de7c34abe98c9a61c6bcc","externalIds":{"MAG":"2964303913","ArXiv":"1606.05433","DBLP":"journals/corr/WangWSHD16","DOI":"10.1109/TPAMI.2017.2754246","CorpusId":7483388,"PubMed":"28945588"},"title":"FVQA: Fact-Based Visual Question Answering"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"44040913380206991b1991daf1192942e038fe31","externalIds":{"ACL":"Q14-1006","DBLP":"journals/tacl/YoungLHH14","MAG":"2185175083","DOI":"10.1162/tacl_a_00166","CorpusId":3104920},"title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"e04a79cfbba40950118a29beb10e4f4953fb57b2","externalIds":{"MAG":"2060976744","DOI":"10.1038/336722A0","CorpusId":5100095},"title":"Where and why?"},{"paperId":"4b6f83d69adeb44c6fe00bd3658f53395d8d154c","externalIds":{"DBLP":"journals/corr/abs-2403-16999","DOI":"10.48550/arXiv.2403.16999","CorpusId":268681119},"title":"Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models"},{"paperId":"fd614b1a2e97bfa1a449b888b9a6edde7a0f36df","externalIds":{"DBLP":"journals/corr/abs-2403-13164","DOI":"10.48550/arXiv.2403.13164","CorpusId":268536988},"title":"VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning"},{"paperId":"c41ff6575a0eb5942212de2cfdb09396bcb3601e","externalIds":{"DBLP":"journals/corr/abs-2403-01777","DOI":"10.48550/arXiv.2403.01777","CorpusId":268247824},"title":"NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models"},{"paperId":"0370f0e8459bc687c6adaaff2e34de35bb480d81","externalIds":{"DBLP":"journals/corr/abs-2402-03757","DOI":"10.48550/arXiv.2402.03757","CorpusId":267500239},"title":"The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs"},{"paperId":"848e690a62c327e1210532d58a6b914097cac763","externalIds":{"DBLP":"journals/corr/abs-2305-07895","DOI":"10.48550/arXiv.2305.07895","CorpusId":258685422},"title":"On the Hidden Mystery of OCR in Large Multimodal Models"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","externalIds":{"MAG":"2901466771","PubMedCentral":"6244189","DOI":"10.1038/sdata.2018.251","CorpusId":53712941,"PubMed":"31745191"},"title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"3e2734025037fc626873c56e05ddb43ccccd3858","externalIds":{"CorpusId":272333785},"title":"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage"}]}