{"references":[{"paperId":"935fb3a9cfd8b6e21c50e06db19a055669ea5f20","externalIds":{"DBLP":"conf/cikm/DaiZHTLX24","DOI":"10.1145/3627673.3680109","CorpusId":273497081},"title":"Enhancing E-Commerce Query Rewriting: A Large Language Model Approach with Domain-Specific Pre-Training and Reinforcement Learning"},{"paperId":"585e95a43f4ceb3b9fdd8408b7b0b5df468c1030","externalIds":{"ArXiv":"2410.02089","DBLP":"journals/corr/abs-2410-02089","DOI":"10.48550/arXiv.2410.02089","CorpusId":273098785},"title":"RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning"},{"paperId":"c4ff8bc44d88cd267baf18ac5d3a3a1fe86d08eb","externalIds":{"DBLP":"conf/iclr/KumarZASCSBIBRZ25","ArXiv":"2409.12917","DOI":"10.48550/arXiv.2409.12917","CorpusId":272753259},"title":"Training Language Models to Self-Correct via Reinforcement Learning"},{"paperId":"78c06d6c4eccce45f7c0fb24c4de3c05809c4993","externalIds":{"DBLP":"journals/corr/abs-2409-01369","ArXiv":"2409.01369","DOI":"10.48550/arXiv.2409.01369","CorpusId":272367433},"title":"Imitating Language via Scalable Inverse Reinforcement Learning"},{"paperId":"76eb8409fccfe15a4d7ef8a8bb5ad0eddd097bd4","externalIds":{"DBLP":"journals/tamd/McIntoshSLWH24","DOI":"10.1109/TCDS.2024.3377445","CorpusId":268597110},"title":"The Inadequacy of Reinforcement Learning From Human Feedbackâ€”Radicalizing Large Language Models via Semantic Vulnerabilities"},{"paperId":"a6bee5c982638049a70a2690697465cf058ef516","externalIds":{"ArXiv":"2407.01461","DBLP":"journals/corr/abs-2407-01461","DOI":"10.48550/arXiv.2407.01461","CorpusId":270869783},"title":"Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement"},{"paperId":"cd27f45bc760447fb4de3209e2381ea3493bbd57","externalIds":{"DBLP":"conf/nips/ZhangZHYD024","ArXiv":"2406.03816","DOI":"10.48550/arXiv.2406.03816","CorpusId":270285630},"title":"ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search"},{"paperId":"87912571f3df29464d3ccafae66f6e1eed581564","externalIds":{"ArXiv":"2405.19107","DBLP":"journals/corr/abs-2405-19107","DOI":"10.48550/arXiv.2405.19107","CorpusId":270095388},"title":"Offline Regularised Reinforcement Learning for Large Language Models Alignment"},{"paperId":"be992d914389d9e4462df1911e6ede7645b66f86","externalIds":{"DBLP":"conf/ecai/JhaJSA025","ArXiv":"2405.16661","DOI":"10.3233/FAIA250996","CorpusId":270062293},"title":"RLSF: Fine-tuning LLMs via Symbolic Feedback"},{"paperId":"6366cb50a5e2043b2bca11a8f03005c42b036c3e","externalIds":{"ArXiv":"2405.14655","DBLP":"journals/corr/abs-2405-14655","DOI":"10.48550/arXiv.2405.14655","CorpusId":269982866},"title":"Multi-turn Reinforcement Learning from Preference Human Feedback"},{"paperId":"f7749635a5fc0492ef4705bee963ffa887bb2865","externalIds":{"DBLP":"conf/nips/ZhaiBLPTZSXL0L24","ArXiv":"2405.10292","DOI":"10.48550/arXiv.2405.10292","CorpusId":269790773},"title":"Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning"},{"paperId":"c93a21e063a1685558f18c9cd075947f1c1e660a","externalIds":{"DBLP":"conf/cvpr/0001GS0C24","ArXiv":"2404.04627","DOI":"10.1109/CVPR52733.2024.01360","CorpusId":269005074},"title":"Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement"},{"paperId":"dd8e0baf97a805c9de242f32c9c2f42bbb863aeb","externalIds":{"DBLP":"conf/naacl/LiWXXCC24","ArXiv":"2403.11558","DOI":"10.48550/arXiv.2403.11558","CorpusId":268531118},"title":"Reinforcement Learning with Token-level Feedback for Controllable Text Generation"},{"paperId":"c78350e81298ca87bc1d59b466fa40081232caaa","externalIds":{"DBLP":"journals/corr/abs-2403-04642","ArXiv":"2403.04642","DOI":"10.48550/arXiv.2403.04642","CorpusId":268264399},"title":"Teaching Large Language Models to Reason with Reinforcement Learning"},{"paperId":"668858489bbec3ce45f7a84a6a557b329f9ec91a","externalIds":{"DBLP":"conf/icml/ZhouZPLK24","ArXiv":"2402.19446","DOI":"10.48550/arXiv.2402.19446","CorpusId":268091206},"title":"ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL"},{"paperId":"08e84c939b88fc50aaa74ef76e202e61a1ad940b","externalIds":{"ArXiv":"2402.01391","DBLP":"journals/corr/abs-2402-01391","DOI":"10.48550/arXiv.2402.01391","CorpusId":267406244},"title":"StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback"},{"paperId":"65fb348291de709a379a3f0d00b48726a1a674d2","externalIds":{"DBLP":"journals/corr/abs-2401-16635","ArXiv":"2401.16635","DOI":"10.48550/arXiv.2401.16635","CorpusId":267320613},"title":"Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble"},{"paperId":"59084df7203c6be33838ba3e3854eb9bda053ed2","externalIds":{"DBLP":"journals/corr/abs-2401-06081","ArXiv":"2401.06081","DOI":"10.48550/arXiv.2401.06081","CorpusId":266933254},"title":"Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint"},{"paperId":"bd0ed34897bcf69d482caf16e7baab1b725d8b88","externalIds":{"ArXiv":"2312.05657","CorpusId":266163427},"title":"PerfRL: A Small Language Model Framework for Efficient Code Optimization"},{"paperId":"d3ee9ab2b145e9e6a94441c5fff61a95bb875bb2","externalIds":{"DBLP":"conf/acl/Wang0CVX24","ArXiv":"2311.09641","DOI":"10.18653/v1/2024.acl-long.140","CorpusId":265220937},"title":"RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models"},{"paperId":"0f7308fbcae43d22813f70c334c2425df0b1cce1","externalIds":{"ArXiv":"2310.12773","DBLP":"conf/iclr/DaiPSJXL0024","DOI":"10.48550/arXiv.2310.12773","CorpusId":264306078},"title":"Safe RLHF: Safe Reinforcement Learning from Human Feedback"},{"paperId":"d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855","externalIds":{"ArXiv":"2310.10505","DBLP":"journals/corr/abs-2310-10505","DOI":"10.48550/arXiv.2310.10505","CorpusId":264146066},"title":"ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"},{"paperId":"f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0","externalIds":{"ArXiv":"2310.06147","DBLP":"journals/corr/abs-2310-06147","DOI":"10.48550/arXiv.2310.06147","CorpusId":263830826},"title":"Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond"},{"paperId":"73b54d0875c8c9b8f0120a29f8d7a924166a1e68","externalIds":{"DBLP":"conf/emnlp/ShenZZZDGZH23","ArXiv":"2310.05199","DOI":"10.48550/arXiv.2310.05199","CorpusId":263831112},"title":"Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"},{"paperId":"3714ed902e79dad5dcc93c5d033c8222d044f3c8","externalIds":{"DBLP":"journals/corr/abs-2310-02368","ArXiv":"2412.14308","DOI":"10.1109/DeepTest66595.2025.00011","CorpusId":263620542},"title":"Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation"},{"paperId":"fe583403c95c3e9b4148d6276f04bda5ace33660","externalIds":{"DBLP":"journals/corr/abs-2308-12030","ArXiv":"2308.12030","DOI":"10.48550/arXiv.2308.12030","CorpusId":261076002},"title":"Prompt-Based Length Controlled Generation with Reinforcement Learning"},{"paperId":"32608b3b06793a9b453fa742756b34c82afdb9d7","externalIds":{"DBLP":"conf/aaai/WangZHHLLXZ24","ArXiv":"2308.02223","DOI":"10.48550/arXiv.2308.02223","CorpusId":260611030},"title":"ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation"},{"paperId":"a53a747ed6c7c8090d9542ed51d4f65103ac35ad","externalIds":{"DBLP":"journals/corr/abs-2308-00031","ArXiv":"2308.00031","DOI":"10.1613/jair.1.15278","CorpusId":260351093},"title":"Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges"},{"paperId":"5da8d559955f0501ad6b2a97fe8a106f067cb5dd","externalIds":{"ArXiv":"2307.11922","DBLP":"journals/corr/abs-2307-11922","DOI":"10.48550/arXiv.2307.11922","CorpusId":260125969},"title":"Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","externalIds":{"DBLP":"conf/corl/HuangWZL0023","ArXiv":"2307.05973","DOI":"10.48550/arXiv.2307.05973","CorpusId":259837330},"title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"a669ea57529f4db630043c8c75d8f840c485d24d","externalIds":{"DBLP":"journals/tmlr/LiuZXF00Y23","ArXiv":"2307.04349","DOI":"10.48550/arXiv.2307.04349","CorpusId":259501019},"title":"RLTF: Reinforcement Learning from Unit Test Feedback"},{"paperId":"e2e52461194bc81351da7caa978ac42e9e9549cc","externalIds":{"DBLP":"conf/nips/WuHSDSASOH23","ArXiv":"2306.01693","DOI":"10.48550/arXiv.2306.01693","CorpusId":259064099},"title":"Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"f197bf0fc2f228483f6af3285000d54d8d97f9eb","externalIds":{"ArXiv":"2305.16291","DBLP":"journals/tmlr/WangX0MXZFA24","DOI":"10.48550/arXiv.2305.16291","CorpusId":258887849},"title":"Voyager: An Open-Ended Embodied Agent with Large Language Models"},{"paperId":"a553bf27d801d09f667fe121c0ba9632257f364b","externalIds":{"DBLP":"journals/corr/abs-2305-16381","ArXiv":"2305.16381","DOI":"10.48550/arXiv.2305.16381","CorpusId":258947323},"title":"DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"},{"paperId":"60d90e96e7c434861697194fa47f1978d86b9d28","externalIds":{"ArXiv":"2305.14718","DBLP":"conf/iclr/BahetiLB0SR24","CorpusId":258865581},"title":"Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models"},{"paperId":"c226a4acb42912054d498bcf771023b0ba2da001","externalIds":{"DBLP":"conf/iclr/PangWLC0Z024","ArXiv":"2305.14483","DOI":"10.48550/arXiv.2305.14483","CorpusId":258865735},"title":"Language Model Self-improvement by Reinforcement Learning Contemplation"},{"paperId":"5d32c364088733c6e8dadc9cf0baa26e10506d61","externalIds":{"DBLP":"conf/nips/ZhaoLH23","ArXiv":"2305.14078","DOI":"10.48550/arXiv.2305.14078","CorpusId":258841057},"title":"Large Language Models as Commonsense Knowledge for Large-Scale Task Planning"},{"paperId":"d8c78221e4366d6a72a6b3e41e35b706cc45c01d","externalIds":{"DBLP":"journals/corr/abs-2305-13301","ArXiv":"2305.13301","DOI":"10.48550/arXiv.2305.13301","CorpusId":258833251},"title":"Training Diffusion Models with Reinforcement Learning"},{"paperId":"76822be30e76b8f4f7dfdb15d6cca1ba1e8e617e","externalIds":{"DBLP":"journals/corr/abs-2305-12411","ArXiv":"2305.12411","DOI":"10.1109/ICCV51070.2023.01354","CorpusId":258833123},"title":"Synthesizing Diverse Human Motions in 3D Indoor Scenes"},{"paperId":"ebf35cef5c249d90b40043fffa41f8802c27f132","externalIds":{"DBLP":"conf/acl/AkyurekAKCWT23","ArXiv":"2305.08844","ACL":"2023.acl-long.427","DOI":"10.48550/arXiv.2305.08844","CorpusId":258685337},"title":"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs"},{"paperId":"a2e2652a81ac5ae0f9ec391593ac496248350a17","externalIds":{"DBLP":"journals/corr/abs-2305-03987","ArXiv":"2305.03987","DOI":"10.48550/arXiv.2305.03987","CorpusId":258557007},"title":"Replicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization"},{"paperId":"5a80cb485ff292ab06d8a2868c792175962cc106","externalIds":{"DBLP":"conf/sigir/WangCJY23","ArXiv":"2304.07920","DOI":"10.1145/3539618.3591648","CorpusId":258179835},"title":"Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning"},{"paperId":"85cc48276c69924d3e92ddb38facb7d92be9a4a6","externalIds":{"DBLP":"journals/corr/abs-2304-03439","ArXiv":"2304.03439","DOI":"10.48550/arXiv.2304.03439","CorpusId":258041354},"title":"Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4"},{"paperId":"09b2b77111900880585072d82ab272c9222ac9a1","externalIds":{"ArXiv":"2304.01662","DBLP":"conf/cvpr/DessiBGRFB23","DOI":"10.1109/CVPR52729.2023.00670","CorpusId":257921928},"title":"Cross-Domain Image Captioning with Discriminative Finetuning"},{"paperId":"5579b0dd263902ad6fe76f23db074dd533522f0e","externalIds":{"DOI":"10.1007/s00894-023-05523-6","CorpusId":257807667,"PubMed":"36991180"},"title":"De novo drug design based on Stack-RNN with multi-objective reward-weighted sum and reinforcement learning"},{"paperId":"0671fd553dd670a4e820553a974bc48040ba0819","externalIds":{"DBLP":"conf/nips/ShinnCGNY23","ArXiv":"2303.11366","CorpusId":258833055},"title":"Reflexion: language agents with verbal reinforcement learning"},{"paperId":"326d196706f5af85e2bde24b2ca50e48607ba9d7","externalIds":{"PubMedCentral":"10015005","DOI":"10.1038/s41467-023-37139-y","CorpusId":257498979,"PubMed":"36918561"},"title":"AlphaFlow: autonomous discovery and optimization of multi-step chemistry using a self-driven fluidic lab guided by reinforcement learning"},{"paperId":"e8e035f9768a4d4e7fe9a2e167cd93d170407b1b","externalIds":{"DBLP":"journals/corr/abs-2302-08215","ArXiv":"2302.08215","DOI":"10.48550/arXiv.2302.08215","CorpusId":256900692},"title":"Aligning Language Models with Preferences through f-divergence Minimization"},{"paperId":"c5120b546f1bd99df5bd2e2bf44db5c7c46d1545","externalIds":{"DBLP":"conf/icml/KorbakSCBBPBP23","ArXiv":"2302.08582","DOI":"10.48550/arXiv.2302.08582","CorpusId":257020046},"title":"Pretraining Language Models with Human Preferences"},{"paperId":"ecd0b23e4828fca585a05eff56563852d35858d9","externalIds":{"DOI":"10.1007/s00113-023-01296-y","CorpusId":256787765,"PubMed":"36763148"},"title":"ChatGPT"},{"paperId":"f72d3f83e8d946e5b9c016f19143a654d9807539","externalIds":{"DOI":"10.1016/j.sbi.2023.102537","CorpusId":256813401,"PubMed":"36774727"},"title":"Artificial intelligence in multi-objective drug design."},{"paperId":"0a6bc37a07a37e3573d36e10cc11669eca0ff903","externalIds":{"ArXiv":"2301.13816","DBLP":"journals/corr/abs-2301-13816","DOI":"10.48550/arXiv.2301.13816","CorpusId":256416258},"title":"Execution-based Code Generation using Deep Reinforcement Learning"},{"paperId":"c5434eef64f3275d821ba24bfa3818bfc10649fb","externalIds":{"DBLP":"conf/icml/Fan023","ArXiv":"2301.13362","DOI":"10.48550/arXiv.2301.13362","CorpusId":256415971},"title":"Optimizing DDPM Sampling with Shortcut Fine-Tuning"},{"paperId":"64daf0dd3cf9adef433d568627e2c3a784219878","externalIds":{"DBLP":"journals/corr/abs-2301-12729","ArXiv":"2301.12729","DOI":"10.1145/3543507.3583380","CorpusId":256389520},"title":"Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling"},{"paperId":"4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4","externalIds":{"DBLP":"conf/icml/ZhuJJ23","ArXiv":"2301.11270","DOI":"10.48550/arXiv.2301.11270","CorpusId":256274676},"title":"Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"},{"paperId":"3a2ce70ff9854a354006b5ea63aaa28b8a553dcb","externalIds":{"ArXiv":"2301.06687","DBLP":"journals/corr/abs-2301-06687","DOI":"10.48550/arXiv.2301.06687","CorpusId":255942448},"title":"DQNAS: Neural Architecture Search using Reinforcement Learning"},{"paperId":"c49a0912595a1cc70aab63524f64ed08c92194a8","externalIds":{"DOI":"10.1126/science.ade2574","CorpusId":253259177,"PubMed":"36927031"},"title":"Evolutionary-scale prediction of atomic level protein structure with a language model"},{"paperId":"3936fd3c6187f606c6e4e2e20b196dbc41cc4654","externalIds":{"DBLP":"journals/corr/abs-2212-08073","ArXiv":"2212.08073","DOI":"10.48550/arXiv.2212.08073","CorpusId":254823489},"title":"Constitutional AI: Harmlessness from AI Feedback"},{"paperId":"5c43607c7f10284003e0072b8632ef7427d3df06","externalIds":{"DBLP":"conf/wacv/HondaWM23","ArXiv":"2212.03230","DOI":"10.1109/WACV56688.2023.00118","CorpusId":254275009},"title":"Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning"},{"paperId":"a2d6e9091d3474392a6e2e9b91b0b657afaa4bf0","externalIds":{"ArXiv":"2211.16508","DBLP":"journals/corr/abs-2211-16508","DOI":"10.48550/arXiv.2211.16508","CorpusId":250640542},"title":"Reinforced Genetic Algorithm for Structure-based Drug Design"},{"paperId":"66c5a0fbde7e06bf0ef179e660b6a211bcd80aac","externalIds":{"DBLP":"journals/corr/abs-2211-11890","ArXiv":"2211.11890","DOI":"10.48550/arXiv.2211.11890","CorpusId":259298576},"title":"TEMPERA: Test-Time Prompting via Reinforcement Learning"},{"paperId":"379e42895f6d40ab9e9559609f505aba89145a5d","externalIds":{"DBLP":"conf/mlsys/PopeDCDBHXAD23","ArXiv":"2211.05102","DOI":"10.48550/arXiv.2211.05102","CorpusId":253420623},"title":"Efficiently Scaling Transformer Inference"},{"paperId":"fdc3c5b52e7c027654c05803e984bcbfafc4fac2","externalIds":{"DBLP":"journals/prl/ZhaoLSWCY23","DOI":"10.1016/j.patrec.2022.11.031","CorpusId":254331129},"title":"A multi-scenario text generation method based on meta reinforcement learning"},{"paperId":"e383ee065bb7b9ef798d5aff9db794691f4c0e38","externalIds":{"DBLP":"journals/eswa/Lin24","ArXiv":"2210.13623","DOI":"10.48550/arXiv.2210.13623","CorpusId":253107350},"title":"Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook"},{"paperId":"2247d23c62e5f3e3e48c67ba328d763f5d7e61ae","externalIds":{"PubMedCentral":"9814657","DOI":"10.1038/s42004-022-00733-0","CorpusId":252972804,"PubMed":"36697952"},"title":"Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds"},{"paperId":"bda379f36935e8a10d405418f4dbf0952c583e19","externalIds":{"DBLP":"journals/corr/abs-2210-07209","PubMedCentral":"9982302","ArXiv":"2210.07209","DOI":"10.1016/j.patter.2023.100678","CorpusId":252873335,"PubMed":"36873904"},"title":"Computer-aided multi-objective optimization in small molecule discovery"},{"paperId":"f32ae39f30181cd737af7b5ced35f6b7d854529c","externalIds":{"DBLP":"journals/corr/abs-2210-05891","ArXiv":"2210.05891","DOI":"10.1109/TPAMI.2023.3264449","CorpusId":252846243,"PubMed":"37018106"},"title":"Point Cloud Scene Completion With Joint Color and Semantic Estimation From Single RGB-D Image"},{"paperId":"912a39c2e0e4a35747531669cfa952d2c5627729","externalIds":{"ArXiv":"2210.01241","DBLP":"journals/corr/abs-2210-01241","DOI":"10.48550/arXiv.2210.01241","CorpusId":252693405},"title":"Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"},{"paperId":"52d5e69f8adda670f91cfda0b480eaadbc2ab05d","externalIds":{"PubMedCentral":"9531503","DBLP":"journals/jcheminf/ThomasOBG22","DOI":"10.1186/s13321-022-00646-z","CorpusId":251923231,"PubMed":"36192789"},"title":"Augmented Hill-Climb increases reinforcement learning efficiency for language-based de novo molecule generation"},{"paperId":"8607533bd0e3fa2509cf9839a4bad874b6fb360c","externalIds":{"PubMedCentral":"9472278","DBLP":"journals/jcisd/IshitaniKR22","DOI":"10.1021/acs.jcim.2c00366","CorpusId":251516258,"PubMed":"35960209"},"title":"Molecular Design Method Using a Reversible Tree Representation of Chemical Compounds and Deep Reinforcement Learning"},{"paperId":"28f8ac5fedd09594d4c28d2038bda46e44330cd4","externalIds":{"ArXiv":"2208.01232","DBLP":"journals/tvcg/DengWQW23","DOI":"10.1109/TVCG.2022.3209468","CorpusId":251253219,"PubMed":"36179003"},"title":"DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning"},{"paperId":"ca1490bf357a09a74d04c5172224af57aa6e56dc","externalIds":{"DBLP":"conf/kdd/SunXMWCZ22","DOI":"10.1145/3534678.3542676","CorpusId":251518153,"PubMed":"37056719"},"title":"MolSearch: Search-based Multi-objective Molecular Generation and Property Optimization"},{"paperId":"c009a959dd236c162e51703e3bfd4d2b0b751c17","externalIds":{"ArXiv":"2207.08583","DBLP":"journals/corr/abs-2207-08583","DOI":"10.48550/arXiv.2207.08583","CorpusId":250627127},"title":"MAD for Robust Reinforcement Learning in Machine Translation"},{"paperId":"700ea1b0cdbf41ce7bb2172a91d50fbedcf26714","externalIds":{"DBLP":"journals/information/GibsonO22","DOI":"10.3390/info13070331","CorpusId":250510701},"title":"A Reinforcement Learning Approach to Speech Coding"},{"paperId":"42ffe1139f97ce3809c9140e2d92e837fe5eb1e7","externalIds":{"DBLP":"conf/sigir/YeLF0C22","DOI":"10.1145/3477495.3532063","CorpusId":250340116},"title":"Structured and Natural Responses Co-generation for Conversational Search"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","externalIds":{"ArXiv":"2207.01780","DBLP":"journals/corr/abs-2207-01780","DOI":"10.48550/arXiv.2207.01780","CorpusId":250280117},"title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"f9a19688c5c1cfd278b919c07da1fdb138fe38ca","externalIds":{"DBLP":"journals/bioinformatics/ZhongLCLPHPW22","DOI":"10.1093/bioinformatics/btac408","CorpusId":250175853,"PubMed":"35775965"},"title":"Hierarchical reinforcement learning for automatic disease diagnosis"},{"paperId":"d53ef63ada540f20ebbf28aa47072e36c8faf024","externalIds":{"DBLP":"conf/sigmod/ZhangCZ022","DOI":"10.1145/3514221.3526155","CorpusId":249578943},"title":"LearnedSQLGen: Constraint-aware SQL Generation using Reinforcement Learning"},{"paperId":"a5cea6716378949a2b73f0401237d29791a6ee6c","externalIds":{"DBLP":"conf/iclr/SnellKSYL23","ArXiv":"2206.11871","DOI":"10.48550/arXiv.2206.11871","CorpusId":249954054},"title":"Offline RL for Natural Language Generation with Implicit Language Q Learning"},{"paperId":"64ba788e015b5fb9479a78fe925731e0a289f72e","externalIds":{"PubMedCentral":"9156766","DOI":"10.1038/s41598-022-12845-7","CorpusId":249233418,"PubMed":"35641549"},"title":"Synthesizing controlled microstructures of porous media using generative adversarial networks and reinforcement learning"},{"paperId":"07759a84f27e43cfa5bc8d579f8227c96e6ae1dc","externalIds":{"ArXiv":"2205.12548","DBLP":"journals/corr/abs-2205-12548","ACL":"2022.emnlp-main.222","DOI":"10.48550/arXiv.2205.12548","CorpusId":249062787},"title":"RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning"},{"paperId":"805748eec6be59ae0cd92a48400a902b3b7ed8e6","externalIds":{"DBLP":"conf/nips/HarveyNMWW22","ArXiv":"2205.11495","DOI":"10.48550/arXiv.2205.11495","CorpusId":248986725},"title":"Flexible Diffusion Modeling of Long Videos"},{"paperId":"9373f5d52237d0e54b3a48182b8b56ea0f7e00cf","externalIds":{"ArXiv":"2204.07696","DBLP":"journals/corr/abs-2204-07696","DOI":"10.48550/arXiv.2204.07696","CorpusId":248227346},"title":"Efficient Reinforcement Learning for Unsupervised Controlled Text Generation"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"85233f0e70e5c7ecc8da0d9839c3655a4ec61af9","externalIds":{"DBLP":"journals/corr/abs-2203-14500","ArXiv":"2203.14500","DOI":"10.48550/arXiv.2203.14500","CorpusId":247761965},"title":"MolGenSurvey: A Systematic Survey in Machine Learning Models for Molecule Design"},{"paperId":"45b0f30ee83f324ccdffd608818ffb2d50de4a8f","externalIds":{"DBLP":"journals/corr/abs-2203-13055","ArXiv":"2203.13055","DOI":"10.1109/CVPR52688.2022.01077","CorpusId":247627867},"title":"Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory"},{"paperId":"7a1069dafaeb484e22f2473d5545f1e45ce30656","externalIds":{"DBLP":"journals/corr/abs-2203-05132","ArXiv":"2203.05132","ACL":"2022.findings-acl.2","DOI":"10.48550/arXiv.2203.05132","CorpusId":247362946},"title":"Compilable Neural Code Generation with Compiler Feedback"},{"paperId":"36bb5573f4108a41fa559b8b068922fa4136707a","externalIds":{"DOI":"10.1002/wcms.1608","CorpusId":247248974},"title":"Generative models for molecular discovery: Recent advances and challenges"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"3921e7cd03c440c6d065a51b499c9f581d1d2f1f","externalIds":{"DBLP":"journals/symmetry/ZhangWZXTGL22","DOI":"10.3390/sym14030471","CorpusId":247239270},"title":"A Survey of Automatic Source Code Summarization"},{"paperId":"83ebb3a6f8c81492456d024d5dfb9a9bc0221434","externalIds":{"DBLP":"conf/icml/LamprierSCCKSP22","ArXiv":"2201.12320","CorpusId":246411268},"title":"Generative Cooperative Networks for Natural Language Generation"},{"paperId":"801981a7a071e5b76adcd54f4275006073fe4380","externalIds":{"MAG":"3011632945","DBLP":"journals/tse/WangZSWZWYX22","DOI":"10.1109/tse.2020.2979701","CorpusId":215880650},"title":"Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention"},{"paperId":"a03c3022c00b2011352a1b40e9194b92076dad1b","externalIds":{"DBLP":"journals/corr/abs-2206-02544","ArXiv":"2206.02544","DOI":"10.1109/WACV51458.2022.00278","CorpusId":246869703},"title":"RLSS: A Deep Reinforcement Learning Algorithm for Sequential Scene Generation"},{"paperId":"4ae9648bfa97dfe6a01b8bc2282ae363660856f5","externalIds":{"ArXiv":"2112.08593","DBLP":"journals/corr/abs-2112-08593","CorpusId":245218878},"title":"Goal-Directed Story Generation: Augmenting Generative Language Models with Reinforcement Learning"},{"paperId":"d2277d983e791e72179644ec16a128493fa16653","externalIds":{"DOI":"10.1109/iaecst54258.2021.9695533","CorpusId":246661610},"title":"A Symbolic-domain Music Generation Method Based on Leak-GAN"},{"paperId":"2d3c78f7be72deb7ad0d8b0ede1b20122286e915","externalIds":{"DBLP":"journals/jcheminf/LiuYVIW23","PubMedCentral":"9940339","DOI":"10.1186/s13321-023-00694-z","CorpusId":244942088,"PubMed":"36803659"},"title":"DrugEx v3: scaffold-constrained drug design with graph transformer-based reinforcement learning"},{"paperId":"601ab36b6f077ff57472f4a0cf2e061dd05b9b85","externalIds":{"ArXiv":"2111.10493","DBLP":"journals/corr/abs-2111-10493","CorpusId":244478155},"title":"Discrete Representations Strengthen Vision Transformer Robustness"},{"paperId":"49f6dbf4ead6a8a3d26f9cf218a654f2f3d1d896","externalIds":{"ArXiv":"2111.08005","DBLP":"conf/iclr/0011S0E22","CorpusId":244130146},"title":"Solving Inverse Problems in Medical Imaging with Score-Based Generative Models"},{"paperId":"c19e71fb84c9a2b06af2ae51e33f3b84cc6be507","externalIds":{"DBLP":"journals/pr/RosaP21","MAG":"3165906701","ArXiv":"2212.11119","DOI":"10.1016/j.patcog.2021.108098","CorpusId":236241277},"title":"A survey on text generation using generative adversarial networks"},{"paperId":"8404df59479984074a7a333b03638e2ab8441929","externalIds":{"MAG":"3205764667","DBLP":"journals/kbs/PangLG21","DOI":"10.1016/J.KNOSYS.2021.107585","CorpusId":244618301},"title":"RL-DARTS: Differentiable neural architecture search via reinforcement-learning-based meta-optimizer"},{"paperId":"1c7ef87a6a75187e2e2fdf4576e356cdd5fa5b76","externalIds":{"DBLP":"conf/mm/NieL0LL021","DOI":"10.1145/3474085.3475604","CorpusId":239011862},"title":"Triangle-Reward Reinforcement Learning: A Visual-Linguistic Semantic Alignment for Image Captioning"},{"paperId":"aeadd0c0ff70db38ce881c4653dd530025864b18","externalIds":{"ArXiv":"2111.01009","DBLP":"journals/corr/abs-2111-01009","DOI":"10.33774/chemrxiv-2021-fzxmk-v2","CorpusId":240354311},"title":"Fragment-based Sequential Translation for Molecular Optimization"},{"paperId":"807f377de905eda62e4cd2f0797153a59296adbb","externalIds":{"DBLP":"conf/iccv/ShiLW21","DOI":"10.1109/ICCV48922.2021.00219","CorpusId":244430079},"title":"Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning"},{"paperId":"a6fdb277d0a4b09899f802bda3359f5c2021a156","externalIds":{"ArXiv":"2109.10862","DBLP":"journals/corr/abs-2109-10862","CorpusId":237593001},"title":"Recursively Summarizing Books with Human Feedback"},{"paperId":"cfd2b572196324561362cca8061084320a7cd23b","externalIds":{"DBLP":"journals/corr/abs-2109-03540","ArXiv":"2109.03540","CorpusId":237440455},"title":"A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"},{"paperId":"251bdaff7521e60fe81fc375acfd34951c7f13ea","externalIds":{"DBLP":"journals/corr/abs-2108-12472","ArXiv":"2108.12472","ACL":"2021.emnlp-main.83","DOI":"10.18653/v1/2021.emnlp-main.83","CorpusId":237353321},"title":"ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models"},{"paperId":"8fe44af15f0e31c090c0dde4b606e91360a6fb74","externalIds":{"ArXiv":"2108.11510","DBLP":"journals/corr/abs-2108-11510","DOI":"10.1007/s10462-021-10061-9","CorpusId":237303796},"title":"Deep reinforcement learning in computer vision: a comprehensive survey"},{"paperId":"5c84ffe8bb0da3e359cc71a9a08d5379ff5eac12","externalIds":{"DOI":"10.3410/f.740477162.793587584","CorpusId":241280428},"title":"Faculty Opinions recommendation of Accurate prediction of protein structures and interactions using a three-track neural network."},{"paperId":"8e01ee3b48c5ef28bad27b40d0a150bdfc9e6d69","externalIds":{"DBLP":"journals/jcisd/GoelRLP21","DOI":"10.1021/acs.jcim.1c01341","CorpusId":237732209,"PubMed":"34866384"},"title":"MoleGuLAR: Molecule Generation Using Reinforcement Learning with Alternating Rewards"},{"paperId":"85da6c156e97eb5f731d69214684d948829477e9","externalIds":{"MAG":"3186474193","DBLP":"journals/jcisd/AtanceDEOM22","DOI":"10.33774/chemrxiv-2021-9w3tc","CorpusId":237698428,"PubMed":"36219571"},"title":"De Novo Drug Design Using Reinforcement Learning with Graph-Based Deep Generative Models"},{"paperId":"dc32a984b651256a8ec282be52310e6bd33d9815","externalIds":{"PubMedCentral":"8371605","DOI":"10.1038/s41586-021-03819-2","CorpusId":235959867,"PubMed":"34265844"},"title":"Highly accurate protein structure prediction with AlphaFold"},{"paperId":"d3f9b7c21197633c40684abc44f8c4d555f20150","externalIds":{"DOI":"10.1038/s41524-021-00535-3","CorpusId":235815389},"title":"Autonomous reinforcement learning agent for chemical vapor deposition synthesis of quantum materials"},{"paperId":"a7b82c7b89f447031ec5c66fa837cb6754d4c6b7","externalIds":{"DBLP":"journals/iacr/RijsdijkWPP21","DOI":"10.46586/tches.v2021.i3.677-707","CorpusId":231731688},"title":"Reinforcement Learning for Hyperparameter Tuning in Deep Learning-based Side-channel Analysis"},{"paperId":"4384ff4ac7459d3045ff660b1772c975512701d9","externalIds":{"DBLP":"journals/corr/abs-2106-15561","ArXiv":"2106.15561","CorpusId":235669930},"title":"A Survey on Neural Speech Synthesis"},{"paperId":"c719fa0e940a7d6ee8e89cf92123d4a519214171","externalIds":{"ArXiv":"2106.10528","DBLP":"journals/tip/LiuMHVRK22","DOI":"10.1109/TIP.2022.3143699","CorpusId":235490052,"PubMed":"35073266"},"title":"Video Summarization Through Reinforcement Learning With a 3D Spatio-Temporal U-Net"},{"paperId":"a109b995dfeb444417f66545c67bce210bd11650","externalIds":{"DBLP":"journals/corr/abs-2106-08942","ArXiv":"2106.08942","ACL":"2021.naacl-main.133","DOI":"10.18653/v1/2021.naacl-main.133","CorpusId":235097487},"title":"Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation"},{"paperId":"f3a2bc491bb9c96e14a728de0b0adf9228569ffd","externalIds":{"DBLP":"conf/emnlp/GuoT0XH22","ArXiv":"2106.07704","DOI":"10.18653/v1/2022.findings-emnlp.518","CorpusId":253098270},"title":"Efficient (Soft) Q-Learning for Text Generation with Limited Good Data"},{"paperId":"50f963a8ccaaa30822aed49a16d839d74a78703a","externalIds":{"DBLP":"journals/air/MoghaddamBJZDMK23","ArXiv":"2106.06976","DOI":"10.1007/s10462-023-10395-6","CorpusId":235422723},"title":"Games of GANs: game-theoretical models for generative adversarial networks"},{"paperId":"5d7d34abbc14739e40b53ec3c33a3c698a37e70e","externalIds":{"DBLP":"conf/nips/ScialomDSLP21","ArXiv":"2106.06363","CorpusId":235417446},"title":"To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs"},{"paperId":"7c2de353f3151a0f88736af9be354ab5e773bff0","externalIds":{"DBLP":"journals/corr/abs-2106-04985","ArXiv":"2106.04985","CorpusId":235377032},"title":"Energy-Based Models for Code Generation under Compilability Constraints"},{"paperId":"d705f010ceb3df03f53b4e85a316dc3437ddcb4a","externalIds":{"DBLP":"conf/kdd/MohankumarBS21","ArXiv":"2106.03816","DOI":"10.1145/3447548.3467202","CorpusId":235364004},"title":"Diversity driven Query Rewriting in Search Advertising"},{"paperId":"c75bedd4c64c1d95819915ff4c7fd553999c7c07","externalIds":{"ArXiv":"2105.15048","DOI":"10.1038/s42005-021-00684-3","CorpusId":235253825},"title":"Quantum compiling by deep reinforcement learning"},{"paperId":"dba6a03368481bec3fb8077bed59627cdf481e18","externalIds":{"ArXiv":"2105.09630","DBLP":"journals/corr/abs-2105-09630","DOI":"10.1016/j.neunet.2021.09.025","CorpusId":234790470,"PubMed":"34710788"},"title":"Enriching Query Semantics for Code Search with Reinforcement Learning"},{"paperId":"de148b6b12cb9e63ee90b66b62c835a162862933","externalIds":{"DBLP":"conf/ijcai/WangDZKCHW21","ArXiv":"2105.06631","DOI":"10.24963/ijcai.2021/491","CorpusId":234681065},"title":"Ordering-Based Causal Discovery with Reinforcement Learning"},{"paperId":"c8559021289f08eaf8cf2294e406bc1c6b506d19","externalIds":{"DBLP":"journals/corr/abs-2105-04387","ArXiv":"2105.04387","DOI":"10.1007/s10462-022-10248-8","CorpusId":234342691},"title":"Recent advances in deep learning based dialogue systems: a systematic survey"},{"paperId":"3803ea42e1fc773db3b1d0fa05f41b5ebf0a61d1","externalIds":{"DBLP":"journals/pieee/ScholkopfLBKKGB21","DOI":"10.1109/JPROC.2021.3058954","CorpusId":261325982},"title":"Toward Causal Representation Learning"},{"paperId":"4817ccfe1f4059c37a919ee30d49ae3eae95bd73","externalIds":{"ArXiv":"2104.07715","DBLP":"journals/corr/abs-2104-07715","CorpusId":233289667},"title":"Quantum Architecture Search via Deep Reinforcement Learning"},{"paperId":"a0c40c5bf41c7bbb95014132d25f99ac15b2d0d9","externalIds":{"ArXiv":"2104.05565","DBLP":"journals/corr/abs-2104-05565","DOI":"10.1007/s10462-022-10205-5","CorpusId":233210638},"title":"Survey on reinforcement learning for language processing"},{"paperId":"b6e836b131b2abe9aa82668d65c589aa8c56ea22","externalIds":{"DBLP":"journals/corr/abs-2104-01408","ArXiv":"2104.01408","DOI":"10.21437/interspeech.2021-1236","CorpusId":233025187},"title":"Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability"},{"paperId":"5fd907226d543ea5c74c4d3ed4387d376d6cff41","externalIds":{"DBLP":"conf/nips/OstaszewskiTMSD21","ArXiv":"2103.16089","CorpusId":229166765},"title":"Reinforcement learning for optimization of variational quantum circuit architectures"},{"paperId":"5d556dd3afff529de8cb694f88916b2d95fbdd3a","externalIds":{"ArXiv":"2101.03288","DBLP":"journals/corr/abs-2101-03288","CorpusId":231573444},"title":"How to Train Your Energy-Based Models"},{"paperId":"07fd366a8ebdefe54cdb57d87c81dcd22de25a91","externalIds":{"DBLP":"journals/corr/abs-2012-11635","ArXiv":"2012.11635","CorpusId":229348988},"title":"A Distributional Approach to Controlled Text Generation"},{"paperId":"be31a8281a70ead42ea86bfd8ead610bf7762c93","externalIds":{"DBLP":"journals/cce/PowellMQ20","MAG":"3081491576","DOI":"10.1016/j.compchemeng.2020.107077","CorpusId":225009738},"title":"Real-time optimization using reinforcement learning"},{"paperId":"e982f9a7e286423b8d3c421782adc6a7fc85ae3e","externalIds":{"MAG":"3089164705","DBLP":"journals/kbs/WangJHC20","DOI":"10.1016/J.KNOSYS.2020.106421","CorpusId":225016449},"title":"GRL: Knowledge graph completion with GAN-based reinforcement learning"},{"paperId":"e000fbc49a38e9780b3ad59a9f73adbf0e31945e","externalIds":{"DBLP":"conf/acl-spnlp/KreutzerRL21","ACL":"2021.spnlp-1.4","ArXiv":"2011.02511","DOI":"10.18653/v1/2021.spnlp-1.4","CorpusId":235377459},"title":"Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks"},{"paperId":"7bf03c12f74a83852fc831c468aa754e0008a6d7","externalIds":{"DBLP":"conf/acl/YangCN20","ACL":"2021.acl-long.56","ArXiv":"2010.09954","DOI":"10.18653/v1/2021.acl-long.56","CorpusId":235490180},"title":"Improving Dialog Systems for Negotiation with Personality Modeling"},{"paperId":"29df8ed16d3787c710e0128dc1948a95990cc9fb","externalIds":{"DBLP":"conf/mm/LiuXWDJT20","MAG":"3093385053","DOI":"10.1145/3394171.3413924","CorpusId":222278468},"title":"Cascade Reasoning Network for Text-based Visual Question Answering"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","externalIds":{"MAG":"3082115681","DBLP":"journals/corr/abs-2009-01325","ArXiv":"2009.01325","CorpusId":221665105},"title":"Learning to summarize from human feedback"},{"paperId":"46fd9996cd945983f5acc9ed073dc244d6d2f32e","externalIds":{"MAG":"3017628311","DBLP":"journals/kbs/ShenLZZL20","DOI":"10.1016/j.knosys.2020.105920","CorpusId":218997863},"title":"Remote sensing image captioning via Variational Autoencoder and Reinforcement Learning"},{"paperId":"c316f7012cc6a459cc6044b7ac384e3c41739fc3","externalIds":{"MAG":"3048023795","DBLP":"journals/corr/abs-2008-03096","ArXiv":"2008.03096","DOI":"10.21437/Interspeech.2020-1822","CorpusId":221083365},"title":"Incremental Text to Speech for Neural Sequence-to-Sequence Models using Reinforcement Learning"},{"paperId":"b480661fe147158fa6da46602fe38a4c053be2df","externalIds":{"DBLP":"journals/tnn/YangLSWZC21","MAG":"3044101393","DOI":"10.1109/TNNLS.2020.3008037","CorpusId":225464098,"PubMed":"32701451"},"title":"Hierarchical Human-Like Deep Neural Networks for Abstractive Text Summarization"},{"paperId":"bfb8d3300c67ee13585fd3ec67fd3fd445489a8e","externalIds":{"PubMedCentral":"7654024","MAG":"3103098434","DOI":"10.1186/s13321-020-00473-0","CorpusId":226286828,"PubMed":"33292554"},"title":"Memory-assisted reinforcement learning for diverse molecular de novo design"},{"paperId":"2c77172171ae31f5c4cd1f19e79e91b939d3d966","externalIds":{"ArXiv":"2007.09380","MAG":"3043770089","DBLP":"conf/eccv/ChenDCXCLZL20","DOI":"10.1007/978-3-030-58529-7_12","CorpusId":220646794},"title":"CATCH: Context-based Meta Reinforcement Learning for Transferrable Architecture Search"},{"paperId":"78ba6127fabb7056afc6f97924852bdd8b653b71","externalIds":{"ArXiv":"2007.09180","DBLP":"journals/corr/abs-2007-09180","MAG":"3043455042","DOI":"10.1007/978-3-030-58571-6_11","CorpusId":220647072},"title":"Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search"},{"paperId":"cb9fb10f604a196515e48ad90f217d33794f5991","externalIds":{"MAG":"3038528491","DBLP":"journals/tomccap/FanZYW20","DOI":"10.1145/3390891","CorpusId":218518087},"title":"Recurrent Attention Network with Reinforced Generator for Visual Dialog"},{"paperId":"276780906415581346d4d6ce81efbc99ed74429e","externalIds":{"DBLP":"conf/kdd/SiddiqueOH20","MAG":"3099309639","ArXiv":"2007.02244","DOI":"10.1145/3394486.3403231","CorpusId":220363883},"title":"Unsupervised Paraphrasing via Deep Reinforcement Learning"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"c6f9e20f574c37e2ef12fa41a5729df8e0499f6f","externalIds":{"ArXiv":"2006.09191","MAG":"3035236454","DBLP":"conf/nips/TrippDH20","CorpusId":219708904},"title":"Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining"},{"paperId":"eb78dc546fe64099f1fba742a12551772b97783f","externalIds":{"DBLP":"conf/nips/ScialomDLPS20","MAG":"3101141546","ArXiv":"2006.04643","CorpusId":219530971},"title":"ColdGANs: Taming Language GANs with Cautious Sampling Strategies"},{"paperId":"86c0107bb37599ab1f69c6a4695a03105e7b8bfb","externalIds":{"MAG":"3031617143","DBLP":"conf/sigmod/ElMS20","DOI":"10.1145/3318464.3389779","CorpusId":218981884},"title":"Automatically Generating Data Exploration Sessions Using Deep Reinforcement Learning"},{"paperId":"61ec6e5bfe0c5316b24620b94604ed21edd9b5b8","externalIds":{"MAG":"3026586341","DOI":"10.1103/physreva.101.052327","CorpusId":216642247},"title":"Quantum adiabatic algorithm design using reinforcement learning"},{"paperId":"cc22c4e54c0dd381a2ff22881d4fb570cf3761e8","externalIds":{"MAG":"2992005611","DBLP":"journals/pami/KobyzevPB21","DOI":"10.1109/TPAMI.2020.2992934","CorpusId":208910764,"PubMed":"32396070"},"title":"Normalizing Flows: An Introduction and Review of Current Methods"},{"paperId":"eccd7060c4f81e92d65601f5c7ac7cade2f68807","externalIds":{"MAG":"3034716028","DBLP":"conf/acl/CaiLXLHC20","ACL":"2020.acl-main.27","ArXiv":"2005.02835","DOI":"10.18653/v1/2020.acl-main.27","CorpusId":218516704},"title":"TAG : Type Auxiliary Guiding for Code Comment Generation"},{"paperId":"bc2673037d720154a1daf32e9ae77512f29e8290","externalIds":{"DBLP":"journals/corr/abs-2005-01618","ArXiv":"2005.01618","MAG":"3021164031","CorpusId":211559011},"title":"Reward Constrained Interactive Recommendation with Natural Language Feedback"},{"paperId":"65dd4ff4074812ef4123ecfca9609ef3db87f9de","externalIds":{"MAG":"3102841213","DBLP":"conf/emnlp/YaoTYSS20","ArXiv":"2005.00689","ACL":"2020.emnlp-main.559","DOI":"10.18653/v1/2020.emnlp-main.559","CorpusId":218487432},"title":"An Imitation Game for Learning Semantic Parsers from User Interaction"},{"paperId":"f7d29bd6226f184c1735476d8cbe7c612cb85f6a","externalIds":{"MAG":"3019734328","DBLP":"conf/icml/GottipatiSNPWLL20","ArXiv":"2004.12485","CorpusId":216552839},"title":"Learning To Navigate The Synthetically Accessible Chemical Space Using Reinforcement Learning"},{"paperId":"020bb2ba5f3923858cd6882ba5c5a44ea8041ab6","externalIds":{"MAG":"3015606043","DBLP":"journals/pami/HospedalesAMS22","ArXiv":"2004.05439","DOI":"10.1109/TPAMI.2021.3079209","CorpusId":215744839,"PubMed":"33974543"},"title":"Meta-Learning in Neural Networks: A Survey"},{"paperId":"2ff04294838863ec2d2087cbaa36bac48bf0e555","externalIds":{"ArXiv":"2004.13796","DBLP":"journals/corr/abs-2004-13796","MAG":"3023581195","DOI":"10.1609/aaai.v35i16.17656","CorpusId":216641636},"title":"TextGAIL: Generative Adversarial Imitation Learning for Text Generation"},{"paperId":"8dea634744e9601ab9f59de99796525f684e754f","externalIds":{"MAG":"2997088366","DBLP":"conf/aaai/ZhaoLHLLLS20","DOI":"10.1609/AAAI.V34I05.6514","CorpusId":212983851},"title":"Balancing Quality and Human Involvement: An Effective Approach to Interactive Neural Machine Translation"},{"paperId":"cb674948d44850fd17c27cc0e5d1d432dee8860f","externalIds":{"MAG":"3016221549","ArXiv":"2004.05757","DBLP":"conf/aaai/ZhaoWNW20","DOI":"10.1609/AAAI.V34I05.6513","CorpusId":211102338},"title":"Reinforced Curriculum Learning on Pre-trained Neural Machine Translation Models"},{"paperId":"3be5a9092b58fb028ea6387436c3845a57da70fd","externalIds":{"DBLP":"journals/corr/abs-2003-12397","ArXiv":"2003.12397","MAG":"3106998990","DOI":"10.1007/978-3-030-58607-2_32","CorpusId":214693165},"title":"Modeling 3D Shapes by Reinforcement Learning"},{"paperId":"424cae792846feb121cce393a52f0de22da4ed98","externalIds":{"MAG":"3006661938","DBLP":"conf/icml/SimmPH20","ArXiv":"2002.07717","CorpusId":211146558},"title":"Reinforcement Learning for Molecular Design Guided by Quantum Mechanics"},{"paperId":"c02d7e04de29286f9ad161482408174353a22c32","externalIds":{"MAG":"3016329018","DBLP":"conf/icml/JinBJ20a","CorpusId":215827485},"title":"Multi-Objective Molecule Generation using Interpretable Substructures"},{"paperId":"0a2a6ceb81855761e8e5d14ec43901714d455b92","externalIds":{"MAG":"2995028880","DBLP":"conf/iclr/ZhouGXW020","ArXiv":"2001.11691","CorpusId":211003742},"title":"Self-Adversarial Learning with Comparative Discrimination for Text Generation"},{"paperId":"83ed014fb3f0881ad1e61034e3cf12581624fa58","externalIds":{"ArXiv":"2001.11279","DOI":"10.1098/rspa.2021.0168","CorpusId":239890043},"title":"Goal-directed graph construction using reinforcement learning"},{"paperId":"5b5de122d508518ecaae7c9e4cc627c36c96f2a9","externalIds":{"DBLP":"journals/tip/ZhaoXSLXZ20","MAG":"3003991937","DOI":"10.1109/TIP.2020.2963950","CorpusId":211014818,"PubMed":"32011250"},"title":"Open-Ended Video Question Answering via Multi-Modal Conditional Adversarial Networks"},{"paperId":"0005ba64c6bbdc29fc88a4d13e40b4827afc2168","externalIds":{"DBLP":"journals/corr/abs-2001-09212","MAG":"3093908578","ArXiv":"2001.09212","DOI":"10.1609/aiide.v16i1.7416","CorpusId":210921153},"title":"PCGRL: Procedural Content Generation via Reinforcement Learning"},{"paperId":"3f8af4ad61f68c0c884fac71f98369863ebf5ed4","externalIds":{"MAG":"2995781845","ArXiv":"1912.08517","DBLP":"journals/corr/abs-1912-08517","CorpusId":209405007},"title":"Distributional Reinforcement Learning for Energy-Based Sequential Models"},{"paperId":"6d2013fe7cc029ff93b70dcc065b8187a60ca499","externalIds":{"ArXiv":"1912.03978","CorpusId":208910618},"title":"InfoCNF: An Efficient Conditional Continuous Normalizing Flow with Adaptive Solvers"},{"paperId":"b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5","externalIds":{"DBLP":"journals/corr/abs-1911-09753","MAG":"2990913319","ArXiv":"1911.09753","DOI":"10.1609/AAAI.V34I03.5655","CorpusId":208248199},"title":"Reinforcing an Image Caption Generator Using Off-Line Human Feedback"},{"paperId":"7a15950dc71079285a4eaf195de5aadd87c41b40","externalIds":{"MAG":"2973379954","DBLP":"journals/corr/abs-1909-08593","ArXiv":"1909.08593","CorpusId":202660943},"title":"Fine-Tuning Language Models from Human Preferences"},{"paperId":"20e127a1d27617b2b8545a54c016ccf2b5170b78","externalIds":{"MAG":"2997510038","ArXiv":"1909.07547","DBLP":"journals/corr/abs-1909-07547","DOI":"10.1609/AAAI.V34I05.6400","CorpusId":202583386},"title":"Hierarchical Reinforcement Learning for Open-Domain Dialog"},{"paperId":"5c2d8867055de9df951c29492b5cbffd68ec95be","externalIds":{"MAG":"2959334635","DBLP":"journals/ivc/JaafraLDN19","DOI":"10.1016/J.IMAVIS.2019.06.005","CorpusId":199129901},"title":"Reinforcement learning for neural architecture search: A review"},{"paperId":"dd3135390364ab34390176d8f9d032d54ed078ee","externalIds":{"ACL":"D19-1436","MAG":"2971089354","DBLP":"journals/corr/abs-1908-07195","ArXiv":"1908.07195","DOI":"10.18653/v1/D19-1436","CorpusId":201103950},"title":"ARAML: A Stable Adversarial Training Framework for Text Generation"},{"paperId":"c3172ea74996bc7d390a1bebdc53e373db903b1d","externalIds":{"ArXiv":"1907.01752","DBLP":"conf/iclr/ChoshenFAA20","MAG":"2995338136","CorpusId":195791459},"title":"On the Weaknesses of Reinforcement Learning for Neural Machine Translation"},{"paperId":"608a2a208e3ced44da309a8930ef713e4a06a2f2","externalIds":{"ACL":"P19-1194","MAG":"2949980515","DBLP":"conf/acl/LuoLYZTCSS19","DOI":"10.18653/v1/P19-1194","CorpusId":196192573},"title":"Towards Fine-grained Text Sentiment Transfer"},{"paperId":"57daffd65a5d73a439903f3e50950c21c9eba687","externalIds":{"MAG":"3017586084","ArXiv":"1907.00456","DBLP":"journals/corr/abs-1907-00456","CorpusId":195766797},"title":"Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog"},{"paperId":"d5390f4db61ada45a32bb255bcf14265e120dc2b","externalIds":{"MAG":"2990212132","DBLP":"journals/corr/abs-1906-02923","ArXiv":"1906.02923","DOI":"10.1007/s10791-019-09367-8","CorpusId":174801360},"title":"Preference-based interactive multi-document summarisation"},{"paperId":"be9aa06811f1438e8c6e2fcbed2f8aed06469f2c","externalIds":{"MAG":"2966067376","ArXiv":"1907.12894","DBLP":"journals/corr/abs-1907-12894","DOI":"10.24963/ijcai.2019/326","CorpusId":198985659},"title":"Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation"},{"paperId":"d8ba2965be9cc93439c5bb52961b504c39f16e80","externalIds":{"ArXiv":"1904.12304","MAG":"2951771597","DBLP":"conf/cvpr/SarmadLK19","DOI":"10.1109/CVPR.2019.00605","CorpusId":139103260},"title":"RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion"},{"paperId":"b9700322c4ccbf441373973f133e2aa95e5d73b2","externalIds":{"MAG":"3098323906","ArXiv":"1904.00720","DBLP":"conf/www/YaoPS19","DOI":"10.1145/3308558.3313632","CorpusId":86524089},"title":"CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning"},{"paperId":"8c1d1c1cb110e62cb807b0e56faf5a29d4671638","externalIds":{"MAG":"2970397832","ACL":"D19-1350","DBLP":"conf/emnlp/GuiLPZXH19","DOI":"10.18653/v1/D19-1350","CorpusId":202790039},"title":"Neural Topic Model with Reinforcement Learning"},{"paperId":"f8be3ede4a63e67dc2cf0c5a03cf7e3005f78782","externalIds":{"MAG":"2905150051","DBLP":"conf/cvpr/GuoZWLY19","ArXiv":"1812.05285","DOI":"10.1109/CVPR.2019.00923","CorpusId":54666001},"title":"IRLAS: Inverse Reinforcement Learning for Architecture Search"},{"paperId":"5bfeb6901db481c08874cfe0ae807d8564513765","externalIds":{"DBLP":"journals/jcisd/BrownFSV19","MAG":"2900694120","ArXiv":"1811.09621","DOI":"10.1021/acs.jcim.8b00839","CorpusId":53787096,"PubMed":"30887799"},"title":"GuacaMol: Benchmarking Models for De Novo Molecular Design"},{"paperId":"d1931b9fbfe657f855cb9bb4c2b84fa4d02bb538","externalIds":{"MAG":"3106528330","DBLP":"journals/corr/abs-1810-03779","ArXiv":"1810.03779","DOI":"10.1162/artl_a_00301","CorpusId":52945447,"PubMed":"31697584"},"title":"Reinforcement Learning for Improving Agent Design"},{"paperId":"7e27d44e3fac723ccb703e0a83b22711bd42efe8","externalIds":{"MAG":"2896348597","ArXiv":"1810.04020","DBLP":"journals/csur/HossainSSL19","DOI":"10.1145/3295748","CorpusId":52947736},"title":"A Comprehensive Survey of Deep Learning for Image Captioning"},{"paperId":"cdcdb52d89fb4c730c8ef360b41d3f19e1d20b9b","externalIds":{"ACL":"W18-2712","DBLP":"journals/corr/abs-1809-03182","ArXiv":"1809.03182","MAG":"2953385323","DOI":"10.18653/v1/W18-2712","CorpusId":51868339},"title":"Towards one-shot learning for rare-word translation with external experts"},{"paperId":"25bdcfdcdd9a944ce5adb8d2663856f242c580a1","externalIds":{"MAG":"2895088466","DBLP":"conf/eccv/ZhangWSZLH18","DOI":"10.1007/978-3-030-01228-1_12","CorpusId":263896250},"title":"Goal-Oriented Visual Question Generation via Intermediate Rewards"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","externalIds":{"ArXiv":"1811.07234","MAG":"2888557792","DBLP":"journals/corr/abs-1811-07234","DOI":"10.1145/3238147.3238206","CorpusId":52069701},"title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"f54b36edae733ab9cd7a748595947710bd28a2e3","externalIds":{"MAG":"2952950989","ArXiv":"1808.08866","DBLP":"conf/emnlp/WuTQLL18","ACL":"D18-1397","DOI":"10.18653/v1/D18-1397","CorpusId":52100616},"title":"A Study of Reinforcement Learning for Neural Machine Translation"},{"paperId":"d003b811b705ce00e4a2cc47719743656d8af6f4","externalIds":{"DBLP":"journals/corr/abs-1806-10332","MAG":"2810271953","CorpusId":264593933},"title":"MONAS: Multi-Objective Neural Architecture Search using Reinforcement Learning"},{"paperId":"f5e78eb72c3da3f1f6ed1afdd4686927518f85a2","externalIds":{"DBLP":"journals/isci/LiPWYC18","MAG":"2792593939","DOI":"10.1016/J.INS.2018.03.050","CorpusId":13795836},"title":"A Generative Model for category text generation"},{"paperId":"def1049b5aae96c8e1eab0ca58d77ac9c2f0e3e9","externalIds":{"MAG":"2806351858","ArXiv":"1805.11973","DBLP":"journals/corr/abs-1805-11973","CorpusId":44100802},"title":"MolGAN: An implicit generative model for small molecular graphs"},{"paperId":"41b3180745068934bd9f7f2fbc2efc00c64d534b","externalIds":{"MAG":"2952892526","ACL":"P18-1063","DBLP":"journals/corr/abs-1805-11080","ArXiv":"1805.11080","DOI":"10.18653/v1/P18-1063","CorpusId":44129061},"title":"Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"},{"paperId":"02fdca762652c50d46619a886cfc0754732b05c5","externalIds":{"MAG":"2963115556","DBLP":"journals/corr/abs-1805-10364","ArXiv":"1805.10364","DOI":"10.1109/SPW.2018.00022","CorpusId":44115116},"title":"Detecting Deceptive Reviews Using Generative Adversarial Networks"},{"paperId":"b4151c385bd30e352c18c0135981e56560ecf744","externalIds":{"DBLP":"journals/jcisd/PutinAIASAZ18","MAG":"2805002767","DOI":"10.1021/acs.jcim.7b00690","CorpusId":46890769,"PubMed":"29762023"},"title":"Reinforced Adversarial Neural Computer for de Novo Molecular Design"},{"paperId":"7b614fef7c94690b0cde89e965be946b283c2cef","externalIds":{"MAG":"2964032708","DBLP":"conf/ijcai/WangYTZLD18","ArXiv":"1805.03616","DOI":"10.24963/ijcai.2018/619","CorpusId":13663262},"title":"A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization"},{"paperId":"c47dd7808e95a9291174302fc782cfc9fbc5c5fe","externalIds":{"MAG":"2800233718","DBLP":"conf/eamt/LamKR18","ArXiv":"1805.01553","ACL":"2018.eamt-main.17","CorpusId":13682824},"title":"A Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation"},{"paperId":"6e187ded899fb4c0d02b55a711f3e3522a49f50e","externalIds":{"DBLP":"conf/ijcai/ShiCQH18","ArXiv":"1804.11258","MAG":"2963730239","DOI":"10.24963/ijcai.2018/606","CorpusId":46950555},"title":"Toward Diverse Text Generation with Inverse Reinforcement Learning"},{"paperId":"c72582122ff631117a05deb2aefa04b01362e3fa","externalIds":{"MAG":"2788139998","DBLP":"conf/aaai/WuH18","ArXiv":"1804.07036","DOI":"10.1609/aaai.v32i1.11987","CorpusId":4999752},"title":"Learning to Extract Coherent Summary via Deep Reinforcement Learning"},{"paperId":"0a983a9ac014489e6cc5457c8d5bada5aa31bec0","externalIds":{"MAG":"2793945656","DOI":"10.1021/acs.molpharmaceut.7b01137","CorpusId":4475537,"PubMed":"29569445"},"title":"Adversarial Threshold Neural Computer for Molecular de Novo Design."},{"paperId":"59562be2cf8e01e8b7bb7560cef56158ea171227","externalIds":{"MAG":"2788283780","ArXiv":"1802.08636","ACL":"N18-1158","DBLP":"journals/corr/abs-1802-08636","DOI":"10.18653/v1/N18-1158","CorpusId":3510042},"title":"Ranking Sentences for Extractive Summarization with Reinforcement Learning"},{"paperId":"fe9b8aac9fa3bfd9724db5a881a578e471e612d7","externalIds":{"ArXiv":"1802.03268","DBLP":"conf/icml/PhamGZLD18","MAG":"2953209204","CorpusId":3638969},"title":"Efficient Neural Architecture Search via Parameter Sharing"},{"paperId":"af55ef5f60e5a33e688b28fa518cfb5bef35342b","externalIds":{"DBLP":"journals/tcyb/ZhangPY20","MAG":"2962805368","ArXiv":"1802.02488","DOI":"10.1109/TCYB.2018.2868826","CorpusId":3625590,"PubMed":"30273169"},"title":"SCH-GAN: Semi-Supervised Cross-Modal Hashing by Generative Adversarial Network"},{"paperId":"5bb8c2a054bb98aef95c108b0a29ea078d53c65e","externalIds":{"MAG":"2892153332","DBLP":"conf/emnlp/XuRL018","ACL":"D18-1428","ArXiv":"1802.01345","DOI":"10.18653/v1/D18-1428","CorpusId":53081554},"title":"Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation"},{"paperId":"7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","externalIds":{"ArXiv":"1801.07736","DBLP":"journals/corr/abs-1801-07736","MAG":"2951433039","CorpusId":3655946},"title":"MaskGAN: Better Text Generation via Filling in the ______"},{"paperId":"811df72e210e20de99719539505da54762a11c6d","externalIds":{"MAG":"2962902376","ArXiv":"1801.01290","DBLP":"journals/corr/abs-1801-01290","CorpusId":28202810},"title":"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"},{"paperId":"92a0cf2085013da3fe1fea2090d1bbabcabbf5be","externalIds":{"MAG":"2962879001","DBLP":"journals/corr/abs-1712-05846","ArXiv":"1712.05846","CorpusId":22564279},"title":"Hierarchical Text Generation and Planning for Strategic Dialogue"},{"paperId":"4ab08b2f1193d770c241b41f5d9f1c841a3663d3","externalIds":{"PubMedCentral":"5746857","MAG":"2774977638","DOI":"10.1021/acscentsci.7b00492","CorpusId":1341709,"PubMed":"29296675"},"title":"Optimizing Chemical Reactions with Deep Reinforcement Learning"},{"paperId":"4edd98e3947d8406ec95518c294721757afffb5d","externalIds":{"MAG":"3100751385","DBLP":"journals/corr/abs-1711-10907","PubMedCentral":"6059760","ArXiv":"1711.10907","DOI":"10.1126/sciadv.aap7885","CorpusId":38125055,"PubMed":"30050984"},"title":"Deep reinforcement learning for de novo drug design"},{"paperId":"a6401e102c03a441992b3e45f7b63eec09d4b89d","externalIds":{"MAG":"2953126480","DBLP":"journals/sigkdd/ChenLYT17","ArXiv":"1711.01731","DOI":"10.1145/3166054.3166058","CorpusId":5523008},"title":"A Survey on Dialogue Systems: Recent Advances and New Frontiers"},{"paperId":"c27db32efa8137cbf654902f8f728f338e55cd1c","externalIds":{"MAG":"2766447205","DBLP":"journals/nature/SilverSSAHGHBLB17","DOI":"10.1038/nature24270","CorpusId":205261034,"PubMed":"29052630"},"title":"Mastering the game of Go without human knowledge"},{"paperId":"485552d2711868b54d5fcddc92c746b09afeab07","externalIds":{"DBLP":"journals/corr/abs-1709-08624","MAG":"2949061076","ArXiv":"1709.08624","DOI":"10.1609/aaai.v32i1.11957","CorpusId":3389583},"title":"Long Text Generation via Adversarial Training with Leaked Information"},{"paperId":"62c16289e72ffecfa68e8b3d8f42f2e1eb9be25b","externalIds":{"ArXiv":"1709.02349","MAG":"2751124354","DBLP":"journals/corr/abs-1709-02349","CorpusId":25163644},"title":"A Deep Reinforcement Learning Chatbot"},{"paperId":"8a1ce657dd41a4f49990a4769000dc8049b83404","externalIds":{"MAG":"2950142842","DBLP":"conf/cvpr/ZhongYWSL18","DOI":"10.1109/CVPR.2018.00257","CorpusId":3866935},"title":"Practical Block-Wise Neural Network Architecture Generation"},{"paperId":"b5615fb37a04c1faaae9868469a7d1f9407befdd","externalIds":{"DBLP":"conf/sigir/YangZZ0ZZC17","MAG":"2740309605","DOI":"10.1145/3077136.3080706","CorpusId":3441234},"title":"Personalized Response Generation via Domain adaptation"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"abcbfc9742e8f4825cfc536091fd414e08d03998","externalIds":{"MAG":"2737114849","DBLP":"journals/corr/NguyenDB17","ACL":"D17-1153","ArXiv":"1707.07402","DOI":"10.18653/v1/D17-1153","CorpusId":215824512},"title":"Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback"},{"paperId":"9e7eb89ade64a05d9df093aea60f3313ac08c1c0","externalIds":{"ArXiv":"1706.09601","MAG":"2729842244","DBLP":"journals/corr/ZhangSLXGYH17","CorpusId":32961368},"title":"Actor-Critic Sequence Training for Image Captioning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"6a8dbea5e40831bd6e987c03b76487f45ac49599","externalIds":{"ArXiv":"1706.05125","ACL":"D17-1259","DBLP":"journals/corr/LewisYDPB17","MAG":"2625113742","DOI":"10.18653/v1/D17-1259","CorpusId":2454882},"title":"Deal or No Deal? End-to-End Learning of Negotiation Dialogues"},{"paperId":"a88f86093e6f2d14761d4b8cbdcadfeff496c948","externalIds":{"ArXiv":"1705.11001","DBLP":"journals/corr/LinLHZS17","MAG":"2616969219","CorpusId":4857922},"title":"Adversarial Ranking for Language Generation"},{"paperId":"15d739e2c184a6844bdbd9a2550d007de6ddb085","externalIds":{"DBLP":"journals/corr/GuimaraesSFA17","ArXiv":"1705.10843","MAG":"2618625858","CorpusId":35911567},"title":"Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models"},{"paperId":"032274e57f7d8b456bd255fe76b909b2c1d7458e","externalIds":{"ArXiv":"1705.04304","DBLP":"journals/corr/PaulusXS17","MAG":"2612675303","CorpusId":21850704},"title":"A Deep Reinforced Model for Abstractive Summarization"},{"paperId":"d77bc27d16a362e5e1b727904c3789355dda6062","externalIds":{"DBLP":"journals/jcheminf/OlivecronaBEC17","MAG":"2949437520","ArXiv":"1704.07555","PubMedCentral":"5583141","DOI":"10.1186/s13321-017-0235-x","CorpusId":2978311,"PubMed":"29086083"},"title":"Molecular de-novo design through deep reinforcement learning"},{"paperId":"c689f73f8ea65c6e81c628f2b37feae09b29e46b","externalIds":{"DBLP":"journals/corr/RenWZLL17","MAG":"2607151106","ArXiv":"1704.03899","DOI":"10.1109/CVPR.2017.128","CorpusId":2899486},"title":"Deep Reinforcement Learning-Based Image Captioning with Embedding Reward"},{"paperId":"31f5864ada5fb08b69da74b6d5ad99e385dcc737","externalIds":{"MAG":"2605243085","ACL":"D17-1062","DBLP":"journals/corr/ZhangL17d","ArXiv":"1703.10931","DOI":"10.18653/v1/D17-1062","CorpusId":7473831},"title":"Sentence Simplification with Deep Reinforcement Learning"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","externalIds":{"MAG":"2604763608","DBLP":"journals/corr/FinnAL17","ArXiv":"1703.03400","CorpusId":6719686},"title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"661965662c2d5d744e8f556e79122d1aa9d13197","externalIds":{"ArXiv":"1703.01008","ACL":"I17-1074","MAG":"2949252816","DBLP":"conf/ijcnlp/LiCLGC17","CorpusId":18750779},"title":"End-to-End Task-Completion Neural Dialogue Systems"},{"paperId":"049c6e5736313374c6e594c34b9be89a3a09dced","externalIds":{"MAG":"2949267040","ArXiv":"1703.01161","DBLP":"conf/icml/VezhnevetsOSHJS17","CorpusId":6656096},"title":"FeUdal Networks for Hierarchical Reinforcement Learning"},{"paperId":"042116e805aa3b5171efaf0c822dc142310ceefe","externalIds":{"DBLP":"journals/corr/HjelmJCCB17","ArXiv":"1702.08431","MAG":"2592298275","CorpusId":646822},"title":"Boundary-Seeking Generative Adversarial Networks"},{"paperId":"9172cd6c253edf7c3a1568e03577db20648ad0c4","externalIds":{"MAG":"2594103415","ArXiv":"1702.08165","DBLP":"conf/icml/HaarnojaTAL17","CorpusId":11227891},"title":"Reinforcement Learning with Deep Energy-Based Policies"},{"paperId":"4fc0ea6db600850908264652e1a5d7904f66ca58","externalIds":{"ArXiv":"1702.07983","MAG":"2593383075","DBLP":"journals/corr/CheLZHLSB17","CorpusId":9803807},"title":"Maximum-Likelihood Augmented Discrete Generative Adversarial Networks"},{"paperId":"9c49763e37c20007ebbddbe405b02546705a2d83","externalIds":{"MAG":"2594726847","ACL":"P17-1062","DBLP":"journals/corr/WilliamsAZ17","ArXiv":"1702.03274","DOI":"10.18653/v1/P17-1062","CorpusId":13214003},"title":"Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning"},{"paperId":"a8e4580471908d17e279000d328f39654359bd6e","externalIds":{"ArXiv":"1702.01806","ACL":"W17-3207","MAG":"2587741066","DBLP":"conf/aclnmt/FreitagA17","DOI":"10.18653/v1/W17-3207","CorpusId":2229477},"title":"Beam Search Strategies for Neural Machine Translation"},{"paperId":"176f1d608b918eec8dc4b75e7b6e0acaba84a447","externalIds":{"ArXiv":"1701.06547","ACL":"D17-1230","DBLP":"journals/corr/LiMSRJ17","MAG":"2951520714","DOI":"10.18653/v1/D17-1230","CorpusId":98180},"title":"Adversarial Learning for Neural Dialogue Generation"},{"paperId":"6c8353697cdbb98dfba4f493875778c4286d3e3a","externalIds":{"DBLP":"conf/cvpr/RennieMMRG17","MAG":"2963084599","ArXiv":"1612.00563","DOI":"10.1109/CVPR.2017.131","CorpusId":206594923},"title":"Self-Critical Sequence Training for Image Captioning"},{"paperId":"bed7834ae7d371171977a590872f60d137c2f951","externalIds":{"ArXiv":"1611.08481","DBLP":"journals/corr/VriesSCPLC16","MAG":"2558809543","DOI":"10.1109/CVPR.2017.475","CorpusId":36417},"title":"GuessWhat?! Visual Object Discovery through Multi-modal Dialogue"},{"paperId":"a870df7e7d43c9144e2520ef4e4779f1672dd654","externalIds":{"DBLP":"conf/icml/JaquesGBHTE17","MAG":"2953113026","ArXiv":"1611.02796","DOI":"10.17863/CAM.21343","CorpusId":15636415},"title":"Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control"},{"paperId":"6cd5dfccd9f52538b19a415e00031d0ee4e5b181","externalIds":{"MAG":"2951886768","DBLP":"conf/iclr/BakerGNR17","ArXiv":"1611.02167","CorpusId":1740355},"title":"Designing Neural Network Architectures using Reinforcement Learning"},{"paperId":"67d968c7450878190e45ac7886746de867bf673d","externalIds":{"MAG":"2952431534","ArXiv":"1611.01578","DBLP":"journals/corr/ZophL16","CorpusId":12713052},"title":"Neural Architecture Search with Reinforcement Learning"},{"paperId":"696f708a5b8f0e5098340bd88ef435b72033b2c1","externalIds":{"MAG":"2963560082","DBLP":"conf/aaai/MoZLLY18","ArXiv":"1610.02891","DOI":"10.1609/aaai.v32i1.11938","CorpusId":2963092},"title":"Personalizing a Dialogue System With Transfer Reinforcement Learning"},{"paperId":"32c4e19f4a757f6c6984416b97d69e287d1d0ecd","externalIds":{"MAG":"2949149471","ArXiv":"1609.05473","DBLP":"conf/aaai/YuZWY17","DOI":"10.1609/aaai.v31i1.10804","CorpusId":3439214},"title":"SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"},{"paperId":"1d9600229a4cf8ba5e8f5ad4d05b41af9c8f80a6","externalIds":{"DBLP":"conf/nips/NorouziBCJSWS16","MAG":"2952166695","ArXiv":"1609.00150","CorpusId":3631537},"title":"Reward Augmented Maximum Likelihood for Neural Structured Prediction"},{"paperId":"ecd2c12415a507b2a28b9e66abd7f83e09970093","externalIds":{"ArXiv":"1606.02560","DBLP":"journals/corr/ZhaoE16","MAG":"2962776342","ACL":"W16-3601","DOI":"10.18653/v1/W16-3601","CorpusId":6179947},"title":"Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning"},{"paperId":"1298dae5751fb06184f6b067d1503bde8037bdb7","externalIds":{"DBLP":"conf/emnlp/LiMRJGG16","MAG":"2951559297","ACL":"D16-1127","ArXiv":"1606.01541","DOI":"10.18653/v1/D16-1127","CorpusId":3147007},"title":"Deep Reinforcement Learning for Dialogue Generation"},{"paperId":"2b10281297ee001a9f3f4ea1aa9bea6b638c27df","externalIds":{"ArXiv":"1606.01540","DBLP":"journals/corr/BrockmanCPSSTZ16","CorpusId":16099293},"title":"OpenAI Gym"},{"paperId":"69e76e16740ed69f4dc55361a3d319ac2f1293dd","externalIds":{"MAG":"2964043796","DBLP":"journals/corr/MnihBMGLHSK16","ArXiv":"1602.01783","CorpusId":6875312},"title":"Asynchronous Methods for Deep Reinforcement Learning"},{"paperId":"846aedd869a00c09b40f1f1f35673cb22bc87490","externalIds":{"DBLP":"journals/nature/SilverHMGSDSAPL16","MAG":"2257979135","DOI":"10.1038/nature16961","CorpusId":515925,"PubMed":"26819042"},"title":"Mastering the game of Go with deep neural networks and tree search"},{"paperId":"41f1d50c85d3180476c4c7b3eea121278b0d8474","externalIds":{"MAG":"2953318193","ArXiv":"1601.06759","DBLP":"conf/icml/OordKK16","CorpusId":8142135},"title":"Pixel Recurrent Neural Networks"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"b7aee9dfb027d6061c6a653684c0fa9a9bba750d","externalIds":{"ArXiv":"1511.06732","MAG":"2950469587","DBLP":"journals/corr/RanzatoCAZ15","CorpusId":7147309},"title":"Sequence Level Training with Recurrent Neural Networks"},{"paperId":"024006d4c2a89f7acacc6e4438d156525b60a98f","externalIds":{"MAG":"2173248099","DBLP":"journals/corr/LillicrapHPHETS15","ArXiv":"1509.02971","CorpusId":16326763},"title":"Continuous control with deep reinforcement learning"},{"paperId":"d316c82c12cf4c45f9e85211ef3d1fa62497bff8","externalIds":{"MAG":"1191599655","DBLP":"journals/corr/SchulmanMLJA15","ArXiv":"1506.02438","CorpusId":3075448},"title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"paperId":"0f899b92b7fb03b609fee887e4b6f3b633eaf30d","externalIds":{"MAG":"299440670","ArXiv":"1505.05770","DBLP":"journals/corr/RezendeM15","CorpusId":12554042},"title":"Variational Inference with Normalizing Flows"},{"paperId":"340f48901f72278f6bf78a04ee5b01df208cc508","externalIds":{"DBLP":"journals/nature/MnihKSRVBGRFOPB15","MAG":"2145339207","DOI":"10.1038/nature14236","CorpusId":205242740,"PubMed":"25719670"},"title":"Human-level control through deep reinforcement learning"},{"paperId":"449532187c94af3dd3aa55e16d2c50f7854d2199","externalIds":{"MAG":"1771410628","ArXiv":"1502.05477","DBLP":"conf/icml/SchulmanLAJM15","CorpusId":16046818},"title":"Trust Region Policy Optimization"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"dc8301b67f98accbb331190dd7bd987952a692af","externalIds":{"MAG":"2964020555","DBLP":"journals/corr/DinhKB14","ArXiv":"1410.8516","CorpusId":13995862},"title":"NICE: Non-linear Independent Components Estimation"},{"paperId":"687d0e59d5c35f022ce4638b3e3a6142068efc94","externalIds":{"MAG":"2643747386","DBLP":"conf/icml/SilverLHDWR14","CorpusId":13928442},"title":"Deterministic Policy Gradient Algorithms"},{"paperId":"484ad17c926292fbe0d5211540832a8c8a8e958b","externalIds":{"MAG":"1909320841","DBLP":"conf/icml/RezendeMW14","CorpusId":16895865},"title":"Stochastic Backpropagation and Approximate Inference in Deep Generative Models"},{"paperId":"79ab3c49903ec8cb339437ccf5cf998607fc313e","externalIds":{"MAG":"2962957031","DBLP":"journals/jmlr/RossGB11","ArXiv":"1011.0686","CorpusId":103456},"title":"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","externalIds":{"MAG":"2117130368","DBLP":"conf/icml/CollobertW08","DOI":"10.1145/1390156.1390177","CorpusId":2617020},"title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"8570302f7b63e8fcf87030f556b065fd8c260021","externalIds":{"DBLP":"conf/nips/Todorov06","MAG":"2145060720","DOI":"10.7551/mitpress/7503.003.0176","CorpusId":393501},"title":"Linearly-solvable Markov decision problems"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"dc9047917d1ceb3805d954c73899ddd2d40dd5eb","externalIds":{"MAG":"2056653303","DOI":"10.1073/PNAS.38.8.716","CorpusId":13247989,"PubMed":"16589166"},"title":"On the Theory of Dynamic Programming."},{"paperId":"5e35f78ce5f05d92c9be7aec325efb5bd4c99d29","externalIds":{"DBLP":"conf/emnlp/0003ZZCK0024","ACL":"2024.emnlp-main.565","DOI":"10.18653/v1/2024.emnlp-main.565","CorpusId":273901422},"title":"Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning"},{"paperId":"f1b1e7f751fed944d6025b42d67c5305d43883cc","externalIds":{"ACL":"2024.privatenlp-1.17","DOI":"10.18653/v1/2024.privatenlp-1.17","CorpusId":271769603},"title":"Reinforcement Learning-Driven LLM Agent for Automated Attacks on LLMs"},{"paperId":"57daf938ab134ba05a1bef4d596c2074d367e81e","externalIds":{"DBLP":"conf/emnlp/HavrillaZPTTBAC23","DOI":"10.18653/v1/2023.emnlp-main.530","CorpusId":266163891},"title":"trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback"},{"paperId":"2a83a92b08e0f3873d07162c73c67e533321112e","externalIds":{"DBLP":"conf/naacl/LiuZFV22","DOI":"10.18653/v1/2022.findings-naacl.18","CorpusId":250562745},"title":"Aligning Generative Language Models with Human Values"},{"paperId":"0c863681a8aaca2089aaac55e68535b0cda7482f","externalIds":{"DBLP":"conf/naacl/AnuchitanukulI22","ACL":"2022.naacl-main.334","DOI":"10.18653/v1/2022.naacl-main.334","CorpusId":250390936},"title":"SURF: Semantic-level Unsupervised Reward Function for Machine Translation"},{"paperId":"ee6b916faa77eabaceecedd9e80729ac161b98c0","externalIds":{"DBLP":"journals/tgrs/ZhangGLL22","DOI":"10.1109/TGRS.2022.3220198","CorpusId":253406164},"title":"QINet: Decision Surface Learning and Adversarial Enhancement for Quasi-Immune Completion of Diverse Corrupted Point Clouds"},{"paperId":"ddf305a275b6cb0e40045a2f5cbdefba24d4c64c","externalIds":{"ACL":"2022.naacl-main.2","DBLP":"conf/naacl/MartinQOCSP22","DOI":"10.18653/v1/2022.naacl-main.2","CorpusId":250390577},"title":"Learning Natural Language Generation with Truncated Reinforcement Learning"},{"paperId":"9a0ffb6156763be319a1ea39c9fac4080bc61182","externalIds":{"DBLP":"journals/corr/abs-2212-10409","DOI":"10.48550/arXiv.2212.10409","CorpusId":254877643},"title":"Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations"},{"paperId":"6ed675b4908ea2c75ae4827c834a38ee8ea502c3","externalIds":{"DBLP":"journals/taslp/WuZH22","DOI":"10.1109/taslp.2022.3155284","CorpusId":247205765},"title":"Automatic Math Word Problem Generation With Topic-Expression Co-Attention Mechanism and Reinforcement Learning"},{"paperId":"49f8a31e13998ae431dde8092973e6bd0f8385be","externalIds":{"DBLP":"conf/iclr/Jang0K22","CorpusId":251648921},"title":"GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems"},{"paperId":"e75fb417b54a6eae589ff382874de09d7f58a3de","externalIds":{"DBLP":"journals/corr/abs-2107-12808","ArXiv":"2107.12808","CorpusId":236447390},"title":"Open-Ended Learning Leads to Generally Capable Agents"},{"paperId":"9bb99e29c711d9b82295beaf4136c0afdc63a051","externalIds":{"DBLP":"conf/mtsummit/KumariCGE21","ACL":"2021.mtsummit-research.13","CorpusId":237010913},"title":"Sentiment Preservation in Review Translation using Curriculum-based Re-inforcement Framework"},{"paperId":"bf7e4870b8cc433921a90f3b8e74f2341a566de3","externalIds":{"MAG":"3195333045","DOI":"10.1007/7355_2021_124","CorpusId":238873835},"title":"Generative AI Models for Drug Discovery"},{"paperId":"03cd06fb1659daad820b6fb63b5d3195cae509a9","externalIds":{"DBLP":"journals/access/SantraHL21","DOI":"10.1109/ACCESS.2021.3090918","CorpusId":235749784},"title":"Gradient Descent Effects on Differential Neural Architecture Search: A Survey"},{"paperId":"7b20d003dcde3106ed8f178e0083963775d7c15a","externalIds":{"DBLP":"conf/iclr/Pang021","CorpusId":235614253},"title":"Text Generation by Learning from Demonstrations"},{"paperId":"3ee38da21d8cf9cb7d4077b729e57f68e9c8d671","externalIds":{"ArXiv":"2009.07839","MAG":"3085932930","DBLP":"journals/corr/abs-2009-07839","CorpusId":221738927},"title":"Text Generation by Learning from Demonstrations"},{"paperId":"70adc568fa1c051c1c8b45e53f32af3118589611","externalIds":{"MAG":"3106543748","DOI":"10.3929/ETHZ-B-000452958","CorpusId":235067876},"title":"Generative Modelling with Design Constraints - Reinforcement Learning for Object Generation"},{"paperId":"0e971b2821dc04f24b2e58546857a9044b91cef6","externalIds":{"DBLP":"journals/ipm/MiaoCLG20","MAG":"2974246429","DOI":"10.1016/j.ipm.2019.102123","CorpusId":203705443},"title":"Multi-modal product title compression"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"871d5f31e23f427ba229023ea10ec6873ec36a0d","externalIds":{"MAG":"3105009590","DBLP":"conf/nips/SaqurN20","CorpusId":227276318},"title":"Multimodal Graph Networks for Compositional Generalization in Visual Question Answering"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"61c05d7cab4448a1585596cc7e00ab271c442c2e","externalIds":{"MAG":"2746340587","DOI":"10.26434/CHEMRXIV.5309668","CorpusId":53511626},"title":"Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry (ORGANIC)"},{"paperId":"4a30e0193889fca9c34cbf4e4e42ac2fe3db9f79","externalIds":{"DBLP":"conf/wmt/Popovic17","ACL":"W17-4770","MAG":"2758950307","DOI":"10.18653/v1/W17-4770","CorpusId":12942757},"title":"chrF++: words helping character n-grams"},{"paperId":"4f8d648c52edf74e41b0996128aa536e13cc7e82","externalIds":{"DBLP":"journals/ijsc/HaoZM16","DOI":"10.1142/S1793351X16500045","CorpusId":1779661},"title":"Deep Learning"},{"paperId":"0e658618c9dad4d70dd7dcd5c519185ec4f845f5","externalIds":{"CorpusId":265038752},"title":"In Advances in Neural Information Processing Systems"},{"paperId":"7fc604e1a3e45cd2d2742f96d62741930a363efa","externalIds":{"MAG":"2161914416","CorpusId":8531544},"title":"A Tutorial on Energy-Based Learning"},{"paperId":"e270bfa5b662c531a61a5b274da636603c23a734","externalIds":{"DBLP":"conf/aistats/Carreira-Perpinan05","MAG":"66838807","CorpusId":17861266},"title":"On Contrastive Divergence Learning"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","externalIds":{"DOI":"10.1023/A:1022672621406","CorpusId":2332513},"title":"Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"},{"paperId":"97efafdb4a3942ab3efba53ded7413199f79c054","externalIds":{"MAG":"2121863487","DBLP":"journals/tnn/SuttonB98","DOI":"10.1109/TNN.1998.712192","CorpusId":60035920},"title":"Reinforcement Learning: An Introduction"},{"paperId":"bc6014884d291555d92b8dbef4635a1a9e192962","externalIds":{"DBLP":"conf/icml/Sutton90","DOI":"10.1016/b978-1-55860-141-3.50030-4","CorpusId":267896352},"title":"Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming"},{"paperId":"22069cd4504656d3bb85748a4d43be7a4d7d5545","externalIds":{"MAG":"1569296262","CorpusId":60564875},"title":"Temporal credit assignment in reinforcement learning"}]}