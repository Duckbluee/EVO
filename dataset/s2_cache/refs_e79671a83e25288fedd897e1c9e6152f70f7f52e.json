{"references":[{"paperId":"3283764bbbd7084c9e6995e20953dbf36b25a226","externalIds":{"DBLP":"conf/acl/ZhouLDL0O024","ArXiv":"2402.12343","DOI":"10.48550/arXiv.2402.12343","CorpusId":267750465},"title":"Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!"},{"paperId":"5759d7c7038bfb7b5f1d47539c8348b04f5e0ee7","externalIds":{"ArXiv":"2312.04730","DBLP":"journals/corr/abs-2312-04730","DOI":"10.48550/arXiv.2312.04730","CorpusId":266149413},"title":"DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions"},{"paperId":"ea07b2e1a910defdcac01a47a152b6ad87e02ed4","externalIds":{"DBLP":"journals/corr/abs-2312-04724","ArXiv":"2312.04724","DOI":"10.48550/arXiv.2312.04724","CorpusId":266149921},"title":"Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models"},{"paperId":"14e8cf5a5e6a7b35e618b08f5cf06f572b3a54e0","externalIds":{"ArXiv":"2312.02119","DBLP":"conf/nips/MehrotraZKNASK24","DOI":"10.48550/arXiv.2312.02119","CorpusId":265609901},"title":"Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"},{"paperId":"da89cdeb0014666f4024f797d0c67cd45d92a7c9","externalIds":{"DBLP":"journals/corr/abs-2311-18580","ArXiv":"2311.18580","DOI":"10.48550/arXiv.2311.18580","CorpusId":265506833},"title":"FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity"},{"paperId":"9045649163477319dafba4403dc915c3388dceda","externalIds":{"ArXiv":"2311.14876","DBLP":"conf/bigdataconf/SinghAN23","DOI":"10.1109/BigData59044.2023.10386814","CorpusId":265457196},"title":"Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles"},{"paperId":"c1d1842e08716cbf7250167969522a1705d8bcd3","externalIds":{"DBLP":"journals/corr/abs-2311-11509","ArXiv":"2311.11509","DOI":"10.48550/arXiv.2311.11509","CorpusId":265294544},"title":"Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information"},{"paperId":"263a58f4fd32caca1dad2351af4d711aec451fe6","externalIds":{"ArXiv":"2311.11855","DBLP":"journals/corr/abs-2311-11855","DOI":"10.48550/arXiv.2311.11855","CorpusId":265294905},"title":"Evil Geniuses: Delving into the Safety of LLM-based Agents"},{"paperId":"66af295963d35a203281b4e7bb902028d1c63077","externalIds":{"DBLP":"conf/naacl/KimKLPLJ24","ACL":"2024.naacl-short.60","ArXiv":"2311.09585","DOI":"10.48550/arXiv.2311.09585","CorpusId":265221131},"title":"LifeTox: Unveiling Implicit Toxicity in Life Advice"},{"paperId":"ecdfc556828dc59d8a0009016fcbe97a06cf7e23","externalIds":{"ArXiv":"2312.00029","DBLP":"journals/corr/abs-2312-00029","DOI":"10.48550/arXiv.2312.00029","CorpusId":265552022},"title":"Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework"},{"paperId":"b88535ea9368753c91967bb7e997c06b1ac6aaec","externalIds":{"DBLP":"conf/naacl/CaoCC24","ACL":"2024.naacl-long.276","ArXiv":"2312.00027","DOI":"10.48550/arXiv.2312.00027","CorpusId":265551434},"title":"Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections"},{"paperId":"e92e8ff1becb9a9e4a7dd09878eaacb2a62ffb6b","externalIds":{"ArXiv":"2311.09096","DBLP":"conf/acl/ZhangYKMWH24","DOI":"10.48550/arXiv.2311.09096","CorpusId":265212812},"title":"Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"},{"paperId":"598df44f1d21a5d1fe3940c0bb2a6128a62c1c15","externalIds":{"ArXiv":"2311.09433","DBLP":"conf/cikm/WangS24","DOI":"10.1145/3627673.3679821","CorpusId":265220823},"title":"Trojan Activation Attack: Red-Teaming Large Language Models using Steering Vectors for Safety-Alignment"},{"paperId":"c4ff1be5c254b60b96b7455eefcc4ec9583f82ed","externalIds":{"ACL":"2024.naacl-long.118","ArXiv":"2311.08268","DBLP":"conf/naacl/DingKMCXCH24","DOI":"10.48550/arXiv.2311.08268","CorpusId":265664913},"title":"A Wolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"},{"paperId":"709af143f78bc62413c50ea1a7ee75b0702c4f59","externalIds":{"ACL":"2024.naacl-long.107","ArXiv":"2311.07689","DBLP":"journals/corr/abs-2311-07689","DOI":"10.48550/arXiv.2311.07689","CorpusId":265157927},"title":"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming"},{"paperId":"bfc0e3e651cd4b715272fe68add8a180a112293c","externalIds":{"ArXiv":"2311.03348","DBLP":"journals/corr/abs-2311-03348","DOI":"10.48550/arXiv.2311.03348","CorpusId":265043220},"title":"Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation"},{"paperId":"99eee47dad469790042b973bc7cb40cb63a660b1","externalIds":{"DBLP":"journals/corr/abs-2311-03191","ArXiv":"2311.03191","DOI":"10.48550/arXiv.2311.03191","CorpusId":265033222},"title":"DeepInception: Hypnotize Large Language Model to Be Jailbreaker"},{"paperId":"512b3097ffcd6afcde6b58da1e656d5d9a635678","externalIds":{"DBLP":"journals/corr/abs-2311-00117","ArXiv":"2311.00117","DOI":"10.48550/arXiv.2311.00117","CorpusId":264832925},"title":"BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B"},{"paperId":"b5a624da64475d735f0e298dc6f2f6669b5bb697","externalIds":{"DBLP":"journals/corr/abs-2311-00172","ArXiv":"2311.00172","DOI":"10.48550/arXiv.2311.00172","CorpusId":264833136},"title":"Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield"},{"paperId":"40ee4949c1050a465d418deb6dd7ea6304a3bc29","externalIds":{"DBLP":"journals/corr/abs-2310-19737","ArXiv":"2310.19737","DOI":"10.48550/arXiv.2310.19737","CorpusId":264817712},"title":"Adversarial Attacks and Defenses in Large Language Models: Old and New Threats"},{"paperId":"f3de6ea08e2464190673c0ec8f78e5ec1cd08642","externalIds":{"ArXiv":"2311.16119","DBLP":"journals/corr/abs-2311-16119","DOI":"10.48550/arXiv.2311.16119","CorpusId":265466048},"title":"Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition"},{"paperId":"0f7308fbcae43d22813f70c334c2425df0b1cce1","externalIds":{"ArXiv":"2310.12773","DBLP":"conf/iclr/DaiPSJXL0024","DOI":"10.48550/arXiv.2310.12773","CorpusId":264306078},"title":"Safe RLHF: Safe Reinforcement Learning from Human Feedback"},{"paperId":"9ec29a26336f043a705ac99baa04c8d7f69fe4b4","externalIds":{"ArXiv":"2310.10477","DBLP":"journals/corr/abs-2310-10477","DOI":"10.48550/arXiv.2310.10477","CorpusId":264146264},"title":"Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis"},{"paperId":"b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283","externalIds":{"DBLP":"conf/emnlp/RebedeaDSPC23","ArXiv":"2310.10501","DOI":"10.48550/arXiv.2310.10501","CorpusId":264146531},"title":"NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails"},{"paperId":"4637f79ddfaf923ce569996ffa5b6cda1996faa1","externalIds":{"DBLP":"conf/satml/ChaoRDHP025","ArXiv":"2310.08419","DOI":"10.1109/SaTML64287.2025.00010","CorpusId":263908890},"title":"Jailbreaking Black Box Large Language Models in Twenty Queries"},{"paperId":"6b135e922a0c673aeb0b05c5aeecdb6c794791c6","externalIds":{"DBLP":"journals/corr/abs-2310-06387","ArXiv":"2310.06387","DOI":"10.48550/arXiv.2310.06387","CorpusId":263830179,"PubMed":"41628045"},"title":"Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations"},{"paperId":"8cf9b49698fdb1b754df2556576412a7b44929f6","externalIds":{"DBLP":"journals/tmlr/Robey0HP25","ArXiv":"2310.03684","DOI":"10.48550/arXiv.2310.03684","CorpusId":263671542},"title":"SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"},{"paperId":"59207e9d0cd4129b6ed665205105192dd3032ff3","externalIds":{"DBLP":"conf/acl/ZhouLS00O024","ArXiv":"2310.03708","DOI":"10.18653/v1/2024.findings-acl.630","CorpusId":264175263},"title":"Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization"},{"paperId":"84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed","externalIds":{"ArXiv":"2310.02949","DBLP":"journals/corr/abs-2310-02949","DOI":"10.48550/arXiv.2310.02949","CorpusId":263620436},"title":"Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models"},{"paperId":"a9c75cf664f675a1b4034b0256ec3c5168e293df","externalIds":{"ArXiv":"2309.08532","CorpusId":262012566},"title":"EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers"},{"paperId":"a4c921bdef167ae54cc3a40643e6e3ed13d49a61","externalIds":{"DBLP":"conf/iclr/0001SARJH024","ArXiv":"2309.07875","DOI":"10.48550/arXiv.2309.07875","CorpusId":261823321},"title":"Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions"},{"paperId":"b574245f3db22b5eb7fe64bd8b0a147dab467b60","externalIds":{"DBLP":"conf/iclr/LiWZ0024","ArXiv":"2309.07124","DOI":"10.48550/arXiv.2309.07124","CorpusId":261705563},"title":"RAIN: Your Language Models Can Align Themselves without Finetuning"},{"paperId":"1ab91d6ac7afc1a0121487a9089fa70edc1634d4","externalIds":{"DBLP":"journals/corr/abs-2309-02705","ArXiv":"2309.02705","DOI":"10.48550/arXiv.2309.02705","CorpusId":261557007},"title":"Certifying LLM Safety against Adversarial Prompting"},{"paperId":"3e30a7ac4886b28eb50151f58e14a1d698cccd0e","externalIds":{"ArXiv":"2309.00614","DBLP":"journals/corr/abs-2309-00614","DOI":"10.48550/arXiv.2309.00614","CorpusId":261494182},"title":"Baseline Defenses for Adversarial Attacks Against Aligned Language Models"},{"paperId":"b8b8d5655df1c6a71bbb713387863e34cc055332","externalIds":{"DBLP":"journals/corr/abs-2308-14132","ArXiv":"2308.14132","DOI":"10.48550/arXiv.2308.14132","CorpusId":261245172},"title":"Detecting Language Model Attacks with Perplexity"},{"paperId":"ac1788e9a168a6455beb6316f316950842297c11","externalIds":{"DBLP":"journals/corr/abs-2308-12833","ArXiv":"2308.12833","DOI":"10.48550/arXiv.2308.12833","CorpusId":261101245},"title":"Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities"},{"paperId":"9f859726b3d8dffd96a1f55de4122617751cc1b4","externalIds":{"ArXiv":"2308.09662","DBLP":"journals/corr/abs-2308-09662","DOI":"10.48550/arXiv.2308.09662","CorpusId":261030829},"title":"Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment"},{"paperId":"2cdd5c3dc42c0df40bc8839709869af3560d4bfe","externalIds":{"DBLP":"journals/corr/abs-2308-07308","ArXiv":"2308.07308","DOI":"10.48550/arXiv.2308.07308","CorpusId":260887487},"title":"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked"},{"paperId":"897940fb5dd4d739b88c4659c4565d05f48d06b8","externalIds":{"DBLP":"conf/iclr/YuanJW0H0T24","ArXiv":"2308.06463","DOI":"10.48550/arXiv.2308.06463","CorpusId":260887189},"title":"GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"},{"paperId":"7142e920b6b9355d9cbacc9450818f912eca138e","externalIds":{"ArXiv":"2308.05374","DBLP":"journals/corr/abs-2308-05374","DOI":"10.48550/arXiv.2308.05374","CorpusId":260775522},"title":"Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment"},{"paperId":"19443d48399d4fe89a4b0a96917c50c6fd9c5af1","externalIds":{"DBLP":"journals/corr/abs-2308-04265","ArXiv":"2308.04265","ACL":"2024.emnlp-main.41","DOI":"10.48550/arXiv.2308.04265","CorpusId":260704223},"title":"FLIRT: Feedback Loop In-context Red Teaming"},{"paperId":"47030369e97cc44d4b2e3cf1be85da0fd134904a","externalIds":{"DBLP":"journals/corr/abs-2307-15043","ArXiv":"2307.15043","CorpusId":260202961},"title":"Universal and Transferable Adversarial Attacks on Aligned Language Models"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"ace98e1e58bcc364afbb2feff6d136232f5f47da","externalIds":{"ArXiv":"2307.08487","DBLP":"journals/corr/abs-2307-08487","DOI":"10.48550/arXiv.2307.08487","CorpusId":259937347},"title":"Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models"},{"paperId":"92930ed3560ea6c86d53cf52158bc793b089054d","externalIds":{"ArXiv":"2307.04657","DBLP":"conf/nips/JiLDPZB0SW023","DOI":"10.48550/arXiv.2307.04657","CorpusId":259501579},"title":"BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"},{"paperId":"317ad53bea6fb603c20f692bb2f1a01e2dc86161","externalIds":{"ArXiv":"2307.00691","DBLP":"journals/access/GuptaAAPP23","DOI":"10.1109/ACCESS.2023.3300381","CorpusId":259316122},"title":"From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy"},{"paperId":"f5fa0b3c2ecbf17ba922932432bed46a1447ed23","externalIds":{"ArXiv":"2306.17194","DBLP":"journals/corr/abs-2306-17194","DOI":"10.48550/arXiv.2306.17194","CorpusId":259309096},"title":"On the Exploitability of Instruction Tuning"},{"paperId":"1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642","externalIds":{"ArXiv":"2306.09442","DBLP":"journals/corr/abs-2306-09442","DOI":"10.48550/arXiv.2306.09442","CorpusId":259187620},"title":"Explore, Establish, Exploit: Red Teaming Language Models from Scratch"},{"paperId":"e2e52461194bc81351da7caa978ac42e9e9549cc","externalIds":{"DBLP":"conf/nips/WuHSDSASOH23","ArXiv":"2306.01693","DOI":"10.48550/arXiv.2306.01693","CorpusId":259064099},"title":"Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"bf52c9d94fd61fae0d231a7e43d45d673584c282","externalIds":{"ArXiv":"2305.00944","DBLP":"journals/corr/abs-2305-00944","DOI":"10.48550/arXiv.2305.00944","CorpusId":258426823},"title":"Poisoning Language Models During Instruction Tuning"},{"paperId":"025ca4c125d6ecabc816a56f160e5c992abc76d9","externalIds":{"DBLP":"journals/corr/abs-2304-05197","ArXiv":"2304.05197","DOI":"10.48550/arXiv.2304.05197","CorpusId":258060250},"title":"Multi-step Jailbreaking Privacy Attacks on ChatGPT"},{"paperId":"8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c","externalIds":{"ArXiv":"2303.12712","DBLP":"journals/corr/abs-2303-12712","CorpusId":257663729},"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"2f94f03fdac62d05f0f416b7b3855d1f597afee9","externalIds":{"DBLP":"journals/corr/abs-2303-04381","ArXiv":"2303.04381","DOI":"10.48550/arXiv.2303.04381","CorpusId":257405439},"title":"Automatically Auditing Large Language Models via Discrete Optimization"},{"paperId":"705e49afd92130f2bc1e0d4d0b1f6cb14e88803f","externalIds":{"DBLP":"conf/ccs/AbdelnabiGMEHF23","ArXiv":"2302.12173","DOI":"10.1145/3605764.3623985","CorpusId":258546941},"title":"Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection"},{"paperId":"0cf694b8f85ab2e11d45595de211a15cfbadcd22","externalIds":{"DBLP":"journals/corr/abs-2302-05733","ArXiv":"2302.05733","DOI":"10.1109/SPW63631.2024.00018","CorpusId":256827239},"title":"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"},{"paperId":"cef330bacf014d60daabbd489647b2006af130ca","externalIds":{"DBLP":"journals/corr/abs-2212-09251","ArXiv":"2212.09251","DOI":"10.48550/arXiv.2212.09251","CorpusId":254854519},"title":"Discovering Language Model Behaviors with Model-Written Evaluations"},{"paperId":"9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6","externalIds":{"DBLP":"journals/corr/abs-2211-09527","ArXiv":"2211.09527","DOI":"10.48550/arXiv.2211.09527","CorpusId":253581710},"title":"Ignore Previous Prompt: Attack Techniques For Language Models"},{"paperId":"b69aa32ee52f5efe8e3196114581ac610da8a2b2","externalIds":{"DBLP":"journals/corr/abs-2208-03274","ArXiv":"2208.03274","DOI":"10.48550/arXiv.2208.03274","CorpusId":251371664},"title":"A Holistic Approach to Undesired Content Detection in the Real World"},{"paperId":"dffe2aef88a3925b249bfe14dd0f8c7645860d64","externalIds":{"DBLP":"journals/corr/abs-2205-01663","ArXiv":"2205.01663","DOI":"10.48550/arXiv.2205.01663","CorpusId":248506146},"title":"Adversarial Training for High-Stakes Reliability"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","externalIds":{"DBLP":"journals/corr/abs-2202-03286","ACL":"2022.emnlp-main.225","ArXiv":"2202.03286","DOI":"10.18653/v1/2022.emnlp-main.225","CorpusId":246634238},"title":"Red Teaming Language Models with Language Models"},{"paperId":"0429020abad6f22ea17681fa403aa591693bb607","externalIds":{"ArXiv":"2112.05224","DBLP":"conf/sp/BagdasaryanS22","DOI":"10.1109/SP46214.2022.9833572","CorpusId":248069679},"title":"Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures"},{"paperId":"fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf","externalIds":{"ArXiv":"2112.04359","DBLP":"journals/corr/abs-2112-04359","CorpusId":244954639},"title":"Ethical and social risks of harm from Language Models"},{"paperId":"972706306f85b1bfb40c7d35c796ad5174eb0c9c","externalIds":{"DBLP":"journals/corr/abs-2111-09543","ArXiv":"2111.09543","CorpusId":244346093},"title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"},{"paperId":"0794333779d775abc2053052d1e7009066cbd4f1","externalIds":{"DBLP":"journals/corr/abs-2110-07518","ArXiv":"2110.07518","ACL":"2022.acl-long.447","DOI":"10.18653/v1/2022.acl-long.447","CorpusId":238856730},"title":"SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","externalIds":{"DBLP":"journals/corr/abs-2109-07958","ACL":"2022.acl-long.229","ArXiv":"2109.07958","DOI":"10.18653/v1/2022.acl-long.229","CorpusId":237532606},"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"041c4a3932eae3931d90b1a0fc033cf5f491c267","externalIds":{"MAG":"3171850892","DBLP":"conf/naacl/XuJLBWD21","ACL":"2021.naacl-main.235","DOI":"10.18653/V1/2021.NAACL-MAIN.235","CorpusId":235097625},"title":"Bot-Adversarial Dialogue for Safe Conversational Agents"},{"paperId":"59c2b4ef91d4ce23cd4f270c8750a00de9054ec2","externalIds":{"ACL":"2021.emnlp-main.464","DBLP":"conf/emnlp/GuoSJK21","ArXiv":"2104.13733","DOI":"10.18653/v1/2021.emnlp-main.464","CorpusId":233423658},"title":"Gradient-based Adversarial Attacks against Text Transformers"},{"paperId":"098370508aaf56f718a472511987ac2072d0f917","externalIds":{"DBLP":"journals/corr/abs-2103-12407","ArXiv":"2103.12407","CorpusId":232320738},"title":"Detecting Hate Speech with GPT-3"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","externalIds":{"MAG":"3088599783","ACL":"2020.findings-emnlp.301","DBLP":"journals/corr/abs-2009-11462","ArXiv":"2009.11462","DOI":"10.18653/v1/2020.findings-emnlp.301","CorpusId":221878771},"title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","externalIds":{"MAG":"3082115681","DBLP":"journals/corr/abs-2009-01325","ArXiv":"2009.01325","CorpusId":221665105},"title":"Learning to summarize from human feedback"},{"paperId":"3caf34532597683c980134579b156cd0d7db2f40","externalIds":{"ArXiv":"1908.07125","DBLP":"conf/emnlp/WallaceFKGS19","MAG":"2970290563","ACL":"D19-1221","DOI":"10.18653/v1/D19-1221","CorpusId":201698258},"title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"ad7129af0644dbcafa9aa2f111cb76526ea444a1","externalIds":{"MAG":"2971008823","DBLP":"conf/nips/ZellersHRBFRC19","ArXiv":"1905.12616","CorpusId":168169824},"title":"Defending Against Neural Fake News"},{"paperId":"634c083444e11c89f30c93a2986cb43db35ca304","externalIds":{"MAG":"2966491090","ACL":"Q19-1029","DOI":"10.1162/tacl_a_00279","CorpusId":155100063},"title":"Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering"},{"paperId":"514e7fb769950dbe96eb519c88ca17e04dc829f6","externalIds":{"DBLP":"conf/acl/EbrahimiRLD18","MAG":"2799194071","ACL":"P18-2006","DOI":"10.18653/v1/P18-2006","CorpusId":21698802},"title":"HotFlip: White-Box Adversarial Examples for Text Classification"},{"paperId":"29e944711a354c396fad71936f536e83025b6ce0","externalIds":{"MAG":"2547875792","DBLP":"conf/iclr/JangGP17","ArXiv":"1611.01144","CorpusId":2428314},"title":"Categorical Reparameterization with Gumbel-Softmax"},{"paperId":"6f35b070c250507dbec8a7365cf01b25eb66d792","externalIds":{"MAG":"2949089361","DBLP":"conf/www/WulczynTD17","ArXiv":"1610.08914","DOI":"10.1145/3038912.3052591","CorpusId":6060248},"title":"Ex Machina: Personal Attacks Seen at Scale"},{"paperId":"e39b586e561b36a3b71fa3d9ee7cb15c35d84203","externalIds":{"MAG":"2340954483","DBLP":"conf/www/NobataTTMC16","DOI":"10.1145/2872427.2883062","CorpusId":11546523},"title":"Abusive Language Detection in Online User Content"},{"paperId":"6fa338adac35e556e49cbb6020f13dcef4956a74","externalIds":{"MAG":"2963177662","DBLP":"journals/corr/ChengDL15","ArXiv":"1504.00680","DOI":"10.1609/icwsm.v9i1.14583","CorpusId":1048832},"title":"Antisocial Behavior in Online Discussion Communities"},{"paperId":"3fa4d63e0194cdbd909c579456830e0a7c909242","externalIds":{"MAG":"1997326096","DBLP":"journals/jasis/SoodCA12","DOI":"10.1002/asi.21690","CorpusId":12996601},"title":"Automatic identification of personal insults on social news sites"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"b7c01c660055dc91ac8de8621d5f823f0aa3c46e","externalIds":{"DBLP":"journals/corr/abs-2310-03708","DOI":"10.48550/arXiv.2310.03708","CorpusId":270227321},"title":"Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization for Language Models"},{"paperId":"cd9455512c80f104f525d71c0ee8a624e150afa8","externalIds":{"DBLP":"journals/corr/abs-2311-04044","DOI":"10.48550/arXiv.2311.04044","CorpusId":279603773},"title":"P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models"}]}