{"paperId":"ea3ecb8b809e7d5ae1bbc267863c0c4e72401a68","externalIds":{"DBLP":"journals/debu/KakkadJS0M23","ArXiv":"2306.01958","DOI":"10.48550/arXiv.2306.01958","CorpusId":259075297},"title":"A Survey on Explainability of Graph Neural Networks","openAccessPdf":{"url":"http://arxiv.org/pdf/2306.01958","status":"CLOSED","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2306.01958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2219548757","name":"Jaykumar Kakkad"},{"authorId":"2219549243","name":"Jaspal Jannu"},{"authorId":"1571168324","name":"Kartik Sharma"},{"authorId":"1682418","name":"C. Aggarwal"},{"authorId":"3390598","name":"Sourav Medya"}],"abstract":"Graph neural networks (GNNs) are powerful graph-based deep-learning models that have gained significant attention and demonstrated remarkable performance in various domains, including natural language processing, drug discovery, and recommendation systems. However, combining feature information and combinatorial graph structures has led to complex non-linear GNN models. Consequently, this has increased the challenges of understanding the workings of GNNs and the underlying reasons behind their predictions. To address this, numerous explainability methods have been proposed to shed light on the inner mechanism of the GNNs. Explainable GNNs improve their security and enhance trust in their recommendations. This survey aims to provide a comprehensive overview of the existing explainability techniques for GNNs. We create a novel taxonomy and hierarchy to categorize these methods based on their objective and methodology. We also discuss the strengths, limitations, and application scenarios of each category. Furthermore, we highlight the key evaluation metrics and datasets commonly used to assess the explainability of GNNs. This survey aims to assist researchers and practitioners in understanding the existing landscape of explainability methods, identifying gaps, and fostering further advancements in interpretable graph-based machine learning."}