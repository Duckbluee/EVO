{"paperId":"5be7e6b04c5a240cff340034aae2b57c677e211f","externalIds":{"DBLP":"journals/corr/abs-2404-14294","ArXiv":"2404.14294","DOI":"10.48550/arXiv.2404.14294","CorpusId":269293007},"title":"A Survey on Efficient Inference for Large Language Models","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2404.14294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2112253370","name":"Zixuan Zhou"},{"authorId":"6636914","name":"Xuefei Ning"},{"authorId":"2241616962","name":"Ke Hong"},{"authorId":"48737592","name":"Tianyu Fu"},{"authorId":"2264991200","name":"Jiaming Xu"},{"authorId":"2242132627","name":"Shiyao Li"},{"authorId":"2333875507","name":"Yuming Lou"},{"authorId":"2289835082","name":"Luning Wang"},{"authorId":"2297998322","name":"Zhihang Yuan"},{"authorId":"2264969921","name":"Xiuhong Li"},{"authorId":"2283520504","name":"Shengen Yan"},{"authorId":"144290348","name":"Guohao Dai"},{"authorId":"2238345410","name":"Xiao-Ping Zhang"},{"authorId":"2287809786","name":"Yuhan Dong"},{"authorId":"2283814845","name":"Yu Wang"}],"abstract":"Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions."}