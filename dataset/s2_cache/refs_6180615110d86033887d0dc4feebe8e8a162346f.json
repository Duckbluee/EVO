{"references":[{"paperId":"fd303eaaed1c2e5f4fbb964bd43c82b94cb3a250","externalIds":{"ArXiv":"2508.05224","DBLP":"journals/corr/abs-2508-05224","DOI":"10.48550/arXiv.2508.05224","CorpusId":280546709},"title":"Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning"},{"paperId":"2f0914fe82e383f6f9d7d867c7b9d131dcd1fc43","externalIds":{"DBLP":"journals/tetci/LiQYWGWL25","DOI":"10.1109/TETCI.2025.3543389","CorpusId":277724848},"title":"A Survey of Multimodal Fake News Detection: A Cross-Modal Interaction Perspective"},{"paperId":"aa3cc9b2d17eae33b55bd90e43d188301c3414d8","externalIds":{"DBLP":"journals/corr/abs-2507-09876","ArXiv":"2507.09876","DOI":"10.1145/3746027.3755837","CorpusId":280232816},"title":"ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models"},{"paperId":"e0b206875de10e5f0a7a85f3b9727dbb86fc16f8","externalIds":{"DBLP":"journals/corr/abs-2507-01903","ArXiv":"2507.01903","DOI":"10.48550/arXiv.2507.01903","CorpusId":280048683},"title":"AI4Research: A Survey of Artificial Intelligence for Scientific Research"},{"paperId":"ebccd098dc1fcb0e97a359013aacab3475b0d405","externalIds":{"DBLP":"journals/corr/abs-2506-16393","ArXiv":"2506.16393","DOI":"10.48550/arXiv.2506.16393","CorpusId":279465450},"title":"From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling"},{"paperId":"18432c7bcc8ac6eeadb9e2f4ab4490257af2aae5","externalIds":{"ArXiv":"2506.03524","DBLP":"journals/corr/abs-2506-03524","DOI":"10.48550/arXiv.2506.03524","CorpusId":279154785},"title":"Seed-Coder: Let the Code Model Curate Data for Itself"},{"paperId":"4661b7e8f6bb4f0cc1d4a767a92534f1def344b8","externalIds":{"DBLP":"journals/corr/abs-2505-19108","ArXiv":"2505.19108","DOI":"10.48550/arXiv.2505.19108","CorpusId":278904919},"title":"CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models"},{"paperId":"b8e9b4da89f33288f45f5860321db37eb73eacb7","externalIds":{"ArXiv":"2505.17685","DBLP":"journals/corr/abs-2505-17685","DOI":"10.48550/arXiv.2505.17685","CorpusId":278886333},"title":"FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving"},{"paperId":"adefd193d5ffc687554f0debde8ec1057a015af5","externalIds":{"DBLP":"journals/corr/abs-2505-15734","ArXiv":"2505.15734","DOI":"10.48550/arXiv.2505.15734","CorpusId":278783156},"title":"DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning"},{"paperId":"a828f08bf2c9f7d74128c31fc1e2e9e7c7fa828d","externalIds":{"DBLP":"journals/corr/abs-2505-15372","ArXiv":"2505.15372","DOI":"10.48550/arXiv.2505.15372","CorpusId":278783059},"title":"X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System"},{"paperId":"f85161b63a540ffd1e41dd6071ebf1fed3e05a6f","externalIds":{"ArXiv":"2505.15110","DBLP":"conf/emnlp/ZhangWXZC25","DOI":"10.48550/arXiv.2505.15110","CorpusId":278782754},"title":"RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals"},{"paperId":"8f8c30b184da966e3824907610fbbfe6bb1dc91c","externalIds":{"DBLP":"journals/corr/abs-2505-15510","ArXiv":"2505.15510","DOI":"10.48550/arXiv.2505.15510","CorpusId":278783160},"title":"Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought"},{"paperId":"fe77fa323ee6b0e34aed4755517cb2a9022f9506","externalIds":{"ArXiv":"2505.13307","DBLP":"journals/corr/abs-2505-13307","DOI":"10.48550/arXiv.2505.13307","CorpusId":278769186},"title":"RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning"},{"paperId":"e42054d042e2e5b3efe6e96cd6dd1c76ab3ca358","externalIds":{"DBLP":"journals/corr/abs-2504-13958","ArXiv":"2504.13958","DOI":"10.48550/arXiv.2504.13958","CorpusId":277955419},"title":"ToolRL: Reward is All Tool Learning Needs"},{"paperId":"a98d89f8398e3d63721e7cb8ee96325f89833d23","externalIds":{"DBLP":"journals/tmlr/FengFMW25","ArXiv":"2504.10903","DOI":"10.48550/arXiv.2504.10903","CorpusId":277786677},"title":"Efficient Reasoning Models: A Survey"},{"paperId":"71bd93626ac1b3533f2cdbd82da40ceb2be969b1","externalIds":{"DBLP":"journals/corr/abs-2504-10160","ArXiv":"2504.10160","DOI":"10.48550/arXiv.2504.10160","CorpusId":277780438},"title":"MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning"},{"paperId":"6ade71e1bed2e0a949c88b1024b75ac95eabea80","externalIds":{"DBLP":"journals/corr/abs-2504-10559","ArXiv":"2504.10559","DOI":"10.48550/arXiv.2504.10559","CorpusId":277787083},"title":"Efficient Process Reward Model Training via Active Learning"},{"paperId":"d63547dee51ff232272f65135064eaeb2ed8b1c4","externalIds":{"DBLP":"conf/aaai/NandyB25","DOI":"10.1609/aaai.v39i23.34676","CorpusId":277765602},"title":"Language Models of Code Are Few-Shot Planners and Reasoners for Multi-Document Summarization with Attribution"},{"paperId":"760e635f56c24538826786ef9acc2f106c420d11","externalIds":{"DBLP":"conf/aaai/0001CZW0CL25","DOI":"10.1609/aaai.v39i23.34688","CorpusId":277754928},"title":"Divide-Solve-Combine: An Interpretable and Accurate Prompting Framework for Zero-shot Multi-Intent Detection"},{"paperId":"0e1f4e37a6c7a7de985e7602ca711b0a0bc6ed32","externalIds":{"DBLP":"conf/aaai/YunCJJJK25","ArXiv":"2601.08475","DOI":"10.1609/aaai.v39i28.35380","CorpusId":277763028},"title":"SummPilot: Bridging Efficiency and Customization for Interactive Summarization System"},{"paperId":"ae5f53fb587761e249aa1e8c84d0693cb6d86a8e","externalIds":{"DBLP":"conf/aaai/WangWSWGW25","DOI":"10.1609/aaai.v39i24.34727","CorpusId":277756631},"title":"Distilling Structured Rationale from Large Language Models to Small Language Models for Abstractive Summarization"},{"paperId":"94359a10120024b3b2dada5d3b4257edf4e083b3","externalIds":{"ArXiv":"2504.05599","DBLP":"journals/corr/abs-2504-05599","DOI":"10.48550/arXiv.2504.05599","CorpusId":277627012},"title":"Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought"},{"paperId":"d85788857fd230169e17638631b96335368043ed","externalIds":{"DBLP":"journals/corr/abs-2504-05118","ArXiv":"2504.05118","DOI":"10.48550/arXiv.2504.05118","CorpusId":277621526},"title":"VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks"},{"paperId":"2e30748aea59a54fa5e645433a44e43fec5eeebf","externalIds":{"DBLP":"journals/corr/abs-2504-04717","ArXiv":"2504.04717","DOI":"10.48550/arXiv.2504.04717","CorpusId":277621374},"title":"Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models"},{"paperId":"9b487024ccc5c575a6d72a597ad65f92cce79a04","externalIds":{"DBLP":"journals/corr/abs-2504-01296","ArXiv":"2504.01296","DOI":"10.48550/arXiv.2504.01296","CorpusId":277502250},"title":"ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning"},{"paperId":"4e7a2097a12e2c56ed73edfd87140eaaae260bd0","externalIds":{"PubMedCentral":"12042776","DBLP":"journals/corr/abs-2504-00025","ArXiv":"2504.00025","DOI":"10.1098/rsos.241776","CorpusId":277467360,"PubMed":"40309181"},"title":"Generalization bias in large language model summarization of scientific research"},{"paperId":"4b8adc782b50a3ffc2bb455a0a30f16496ccf952","externalIds":{"DBLP":"conf/acl/Yan0HYHW25","ArXiv":"2503.18132","DOI":"10.48550/arXiv.2503.18132","CorpusId":277272779},"title":"MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection"},{"paperId":"3997d41786918a150e15af8f22a4dec57f538fe6","externalIds":{"ArXiv":"2503.17793","DBLP":"journals/corr/abs-2503-17793","DOI":"10.48550/arXiv.2503.17793","CorpusId":277272463},"title":"Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM"},{"paperId":"891cc1397f949d4432a5a0602e3757e9e3610862","externalIds":{"DBLP":"journals/tmlr/SuiCWZZYLWZZCH25","ArXiv":"2503.16419","DOI":"10.48550/arXiv.2503.16419","CorpusId":277150783},"title":"Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models"},{"paperId":"dd4cfde3e135f799a9a71b4f57e13a29de89f7e3","externalIds":{"ArXiv":"2503.14476","DBLP":"journals/corr/abs-2503-14476","DOI":"10.48550/arXiv.2503.14476","CorpusId":277104124},"title":"DAPO: An Open-Source LLM Reinforcement Learning System at Scale"},{"paperId":"edd5d458e230bf36c2dddcc634995c454e2b9e93","externalIds":{"ArXiv":"2503.12605","DBLP":"journals/corr/abs-2503-12605","DOI":"10.48550/arXiv.2503.12605","CorpusId":277065932},"title":"Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey"},{"paperId":"4166938d9659161b31e7c04662bfb0b419d5f44d","externalIds":{"ArXiv":"2503.09648","DBLP":"journals/corr/abs-2503-09648","DOI":"10.1145/3711896.3736561","CorpusId":276960878},"title":"A Survey on Trustworthy LLM Agents: Threats and Countermeasures"},{"paperId":"0d320beb4a5304a8bd03bb83eba1a8196c601be1","externalIds":{"ArXiv":"2503.09567","DBLP":"journals/corr/abs-2503-09567","DOI":"10.48550/arXiv.2503.09567","CorpusId":276937570},"title":"Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models"},{"paperId":"0b6f8aceba367ad90b9b895b0e8ab49f1bc8ca6a","externalIds":{"DBLP":"conf/acl/HeLZH0HY025","ArXiv":"2503.04396","DOI":"10.48550/arXiv.2503.04396","CorpusId":276813196},"title":"TableLoRA: Low-rank Adaptation on Table Structure Understanding for Large Language Models"},{"paperId":"a3beb0b9036a5bb8f7404abeb190716e0781f49b","externalIds":{"ArXiv":"2503.04813","DBLP":"journals/corr/abs-2503-04813","DOI":"10.48550/arXiv.2503.04813","CorpusId":276885370},"title":"Self-Evolved Preference Optimization for Enhancing Mathematical Reasoning in Small Language Models"},{"paperId":"3046aa7a99b8f1cfe86944b997f9a99218e3f63f","externalIds":{"DBLP":"journals/inffus/RenLOYZHW25","DOI":"10.1016/j.inffus.2025.103081","CorpusId":276939359},"title":"DyLas: A dynamic label alignment strategy for large-scale multi-label text classification"},{"paperId":"497d1aaa0f80b90b72e1a7519f1e4d1aefa2e429","externalIds":{"DBLP":"journals/nature/YuksekgonulBBLLHGZ25","DOI":"10.1038/s41586-025-08661-4","CorpusId":277148007,"PubMed":"40108317"},"title":"Optimizing generative AI by backpropagating language model feedback"},{"paperId":"900cd128482bbab4d2752d01ce80c55498b78dd2","externalIds":{"ArXiv":"2502.18449","DBLP":"journals/corr/abs-2502-18449","DOI":"10.48550/arXiv.2502.18449","CorpusId":276580226},"title":"SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution"},{"paperId":"f4195d4e283e289665cfc7a65fde2fa7b8814091","externalIds":{"DBLP":"journals/corr/abs-2502-17419","ArXiv":"2502.17419","DOI":"10.1109/TPAMI.2025.3637037","CorpusId":276575321,"PubMed":"41289126"},"title":"From System 1 to System 2: A Survey of Reasoning Large Language Models"},{"paperId":"1137feeae870e782814c338343f870211ac72d59","externalIds":{"DBLP":"conf/naacl/LuHKPCXKZWTV25","ArXiv":"2502.17328","DOI":"10.48550/arXiv.2502.17328","CorpusId":276575997},"title":"Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization"},{"paperId":"40d6c9d96114e697bff6ff10991d5d0c5a821ee4","externalIds":{"ArXiv":"2502.12616","DBLP":"journals/corr/abs-2502-12616","DOI":"10.18653/v1/2025.acl-long.843","CorpusId":276421915},"title":"Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions"},{"paperId":"473b12a5616a3e0b0a5af09fff351462200496f0","externalIds":{"DBLP":"journals/corr/abs-2502-11475","ArXiv":"2502.11475","DOI":"10.48550/arXiv.2502.11475","CorpusId":276408515},"title":"Focused-DPO: Enhancing Code Generation Through Focused Preference Optimization on Error-Prone Points"},{"paperId":"3076cfba160fc4d64eec459e2f99b307b72cb12a","externalIds":{"DBLP":"conf/acl/AcikgozGDYZEKHT25","ArXiv":"2502.08820","DOI":"10.48550/arXiv.2502.08820","CorpusId":276317419},"title":"Can a Single Model Master Both Multi-turn Conversations and Tool Use? CoALM: A Unified Conversational Agentic Language Model"},{"paperId":"01f257a840d271d766d741f911bb1078240fbbdf","externalIds":{"DBLP":"conf/icml/ZhangNF00025","ArXiv":"2502.04180","DOI":"10.48550/arXiv.2502.04180","CorpusId":276161505},"title":"Multi-agent Architecture Search via Agentic Supernet"},{"paperId":"bdd117d8e09b0a5c5530f4469cabadabe41bb7d1","externalIds":{"ArXiv":"2502.03860","DBLP":"journals/corr/abs-2502-03860","DOI":"10.48550/arXiv.2502.03860","CorpusId":276161343},"title":"BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation"},{"paperId":"4923e0686c6dc09b3f314c0695a8533ba31cdf7f","externalIds":{"ArXiv":"2502.03325","CorpusId":276116733},"title":"Electronic Circuit Principles of Large Language Models"},{"paperId":"98bdf937d7eb831ff9a2a9b363a4e682638ee366","externalIds":{"DBLP":"conf/acl/ZengJ0NCC25","ArXiv":"2502.01718","DOI":"10.48550/arXiv.2502.01718","CorpusId":276107488},"title":"ACECODER: Acing Coder RL via Automated Test-Case Synthesis"},{"paperId":"7ad25d4e9c2e60bde200bb730c83126bb85def14","externalIds":{"ArXiv":"2502.01715","DBLP":"journals/corr/abs-2502-01715","DOI":"10.48550/arXiv.2502.01715","CorpusId":276106948},"title":"Process-Supervised Reinforcement Learning for Code Generation"},{"paperId":"2eed1fad9bbf887d4395de40f20144c4fafefd7f","externalIds":{"PubMedCentral":"12443585","DBLP":"journals/nature/GuoYZSWZXZMBZY025","ArXiv":"2501.12948","DOI":"10.1038/s41586-025-09422-z","CorpusId":275789950,"PubMed":"40962978"},"title":"DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"},{"paperId":"fb64f4f6319368781351926ab4b990586af6f5ac","externalIds":{"ArXiv":"2501.04341","DBLP":"journals/corr/abs-2501-04341","DOI":"10.48550/arXiv.2501.04341","CorpusId":275358014},"title":"Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting"},{"paperId":"687845184d09f5cc3fa59110e8c8f01a0ef0bbb2","externalIds":{"DBLP":"journals/corr/abs-2501-03226","ArXiv":"2501.03226","DOI":"10.48550/arXiv.2501.03226","CorpusId":275336642},"title":"BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning"},{"paperId":"742e8577558e579c0e46e1a2e6a6bfd6c6e3347a","externalIds":{"DOI":"10.1016/j.health.2024.100378","CorpusId":275496771},"title":"An Automated Information Extraction Model For Unstructured Discharge Letters Using Large Language Models and GPT-4"},{"paperId":"4ad9c3ffe11b172911ba70a8f952cfba0af5b3c3","externalIds":{"ArXiv":"2412.17408","DBLP":"conf/aaai/QoribHN25","DOI":"10.48550/arXiv.2412.17408","CorpusId":274982874},"title":"Just What You Desire: Constrained Timeline Summarization with Self-Reflection for Enhanced Relevance"},{"paperId":"004a1f368b01e11b37a765305e6966b2395c14af","externalIds":{"DBLP":"journals/corr/abs-2412-16964","ArXiv":"2412.16964","DOI":"10.48550/arXiv.2412.16964","CorpusId":274982574},"title":"System-2 Mathematical Reasoning via Enriched Instruction Tuning"},{"paperId":"f1b1dd002edce21ca241d5df7caff05567af37a9","externalIds":{"DBLP":"conf/aaai/ChengCZ0FCL025","ArXiv":"2412.12932","DOI":"10.48550/arXiv.2412.12932","CorpusId":274789454},"title":"CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models"},{"paperId":"e793a93aa63160999cc4cf1db6c353bd3d7681e7","externalIds":{"DBLP":"conf/aaai/Bajpai025","ArXiv":"2412.08090","DOI":"10.48550/arXiv.2412.08090","CorpusId":274638651},"title":"Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic Alignment for Low-Resource Languages"},{"paperId":"357e250b043441d320b777cda0cd7400cbf289d6","externalIds":{"DBLP":"conf/naacl/AlazrakiR25","ArXiv":"2411.04535","DOI":"10.18653/v1/2025.findings-naacl.440","CorpusId":273877373},"title":"Meta-Reasoning Improves Tool Use in Large Language Models"},{"paperId":"75e185aa6f5e248685d4f178293279c086c656f8","externalIds":{"DBLP":"journals/corr/abs-2411-02462","ArXiv":"2411.02462","DOI":"10.48550/arXiv.2411.02462","CorpusId":273821140},"title":"Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study"},{"paperId":"cb9b22a139902e5c98042dca807939877850f018","externalIds":{"DBLP":"journals/corr/abs-2410-20482","ArXiv":"2410.20482","DOI":"10.48550/arXiv.2410.20482","CorpusId":273653877},"title":"What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration"},{"paperId":"1176de467f5f6fca504e7b98da7fe6fb4414fcd3","externalIds":{"ArXiv":"2410.19056","DBLP":"journals/corr/abs-2410-19056","DOI":"10.48550/arXiv.2410.19056","CorpusId":273638122},"title":"ReasonAgain: Using Extractable Symbolic Programs to Evaluate Mathematical Reasoning"},{"paperId":"7613e3b52b5a66f0e229320d1b632a22883a3583","externalIds":{"PubMedCentral":"11681215","DBLP":"journals/corr/abs-2410-08598","ArXiv":"2410.08598","DOI":"10.1038/s41598-024-75599-4","CorpusId":273323811,"PubMed":"39730415"},"title":"Parameter-efficient fine-tuning of large language models using semantic knowledge tuning"},{"paperId":"3cda8bca2e9eb2dfdb2243f9a8584123e2ad4a51","externalIds":{"DBLP":"conf/nips/Chen0WZC24","ArXiv":"2410.05695","DOI":"10.48550/arXiv.2410.05695","CorpusId":273228754},"title":"Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought"},{"paperId":"ac82fbd7122ef5814f8bbff01d12819fc44c4d2c","externalIds":{"DBLP":"journals/corr/abs-2410-04463","ArXiv":"2410.04463","DOI":"10.48550/arXiv.2410.04463","CorpusId":273185750},"title":"Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information"},{"paperId":"1bf4d5ec29a1c1452971a110f5e0d72d254e85a1","externalIds":{"DBLP":"journals/corr/abs-2409-18454","ArXiv":"2409.18454","DOI":"10.48550/arXiv.2409.18454","CorpusId":272969413},"title":"Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications"},{"paperId":"11dd2fc88747d20629f5aafab72ba649d93e969c","externalIds":{"ArXiv":"2409.12186","DBLP":"journals/corr/abs-2409-12186","CorpusId":272707390},"title":"Qwen2.5-Coder Technical Report"},{"paperId":"3faa759bae4003a55c62618961158ac610902688","externalIds":{"DBLP":"journals/corr/abs-2409-04421","ArXiv":"2409.04421","DOI":"10.48550/arXiv.2409.04421","CorpusId":272463905},"title":"RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs"},{"paperId":"36dfe38c6638330c88a4ae1d31597964ba64354c","externalIds":{"ArXiv":"2409.01893","DBLP":"conf/acl/ChenC0GLZ00L25","DOI":"10.48550/arXiv.2409.01893","CorpusId":272368356},"title":"What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices"},{"paperId":"68d59fa860a993d4865271889f93ffdbce05d4ac","externalIds":{"DBLP":"journals/tomccap/WangZFCWSLLQ25","DOI":"10.1145/3690642","CorpusId":272226888},"title":"S3 Agent: Unlocking the Power of VLLM for Zero-Shot Multi-Modal Sarcasm Detection"},{"paperId":"a7fb4245b412f0e54ec26d5973f041d52c83c0ad","externalIds":{"DBLP":"conf/acl/HuCC0S025","ArXiv":"2408.09559","DOI":"10.48550/arXiv.2408.09559","CorpusId":271903203},"title":"HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model"},{"paperId":"22386388e46a1ddd5ed4dff5255055e5a0ef49c8","externalIds":{"PubMedCentral":"11773977","DOI":"10.1186/s12874-025-02470-z","CorpusId":271820859,"PubMed":"39871166"},"title":"Scalable information extraction from free text electronic health records using large language models"},{"paperId":"207f0c123e48b9e980f49e42cbc35711e14fea07","externalIds":{"ArXiv":"2406.17294","DBLP":"journals/corr/abs-2406-17294","DOI":"10.48550/arXiv.2406.17294","CorpusId":270710901},"title":"Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models"},{"paperId":"4a54ae690a0a1a2613031d31247d312545b86f04","externalIds":{"DBLP":"conf/ieeecai/DuXMC24","DOI":"10.1109/CAI59869.2024.00042","CorpusId":269604521},"title":"An Evaluation of Reasoning Capabilities of Large Language Models in Financial Sentiment Analysis"},{"paperId":"61ab67b5e87d897a98f0e51f79177d1e0c39335c","externalIds":{"DBLP":"conf/acl/ZhangCLC024","ArXiv":"2406.13940","DOI":"10.48550/arXiv.2406.13940","CorpusId":270621005},"title":"AutoCAP: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought"},{"paperId":"dd3268b6fb685b7e31ca576f2a629467da90d662","externalIds":{"ArXiv":"2406.11409","DBLP":"journals/corr/abs-2406-11409","DOI":"10.48550/arXiv.2406.11409","CorpusId":270560319},"title":"CodeGemma: Open Code Models Based on Gemma"},{"paperId":"9f89ee4993811866039cacfe475899763df49c20","externalIds":{"DBLP":"journals/corr/abs-2406-10505","ArXiv":"2406.10505","DOI":"10.1109/ICASSP49660.2025.10889329","CorpusId":270560763},"title":"CroPrompt: Cross-task Interactive Prompting for Zero-shot Spoken Language Understanding"},{"paperId":"f32bcc2155997110a7905da050df4c8404867b24","externalIds":{"DBLP":"journals/corr/abs-2406-06592","ArXiv":"2406.06592","CorpusId":270379625},"title":"Improve Mathematical Reasoning in Language Models by Automated Process Supervision"},{"paperId":"b8580112a7698527487dd17e459c0b30236e2c11","externalIds":{"DBLP":"journals/pacmmod/LiHYCGZF0C24","DOI":"10.1145/3654979","CorpusId":270163360},"title":"Table-GPT: Table Fine-tuned GPT for Diverse Table Tasks"},{"paperId":"acb53707ebc5ce954d9821fc721382d997befbe7","externalIds":{"DBLP":"journals/corr/abs-2405-16473","ArXiv":"2405.16473","DOI":"10.48550/arXiv.2405.16473","CorpusId":270062772},"title":"M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought"},{"paperId":"6a08ff3d0821654b1fa92b2b63106946813e41a3","externalIds":{"DBLP":"conf/icml/0001W0ZZLH24","ArXiv":"2501.03230","DOI":"10.48550/arXiv.2501.03230","CorpusId":270556477},"title":"Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition"},{"paperId":"be9b950c365b6998c2b22dfdefe9ee9715128485","externalIds":{"ArXiv":"2404.11553","DBLP":"conf/aaai/LiS0YPLD25","DOI":"10.1609/aaai.v39i27.35038","CorpusId":269187651},"title":"Language Ranker: A Metric for Quantifying LLM Performance Across High and Low-Resource Languages"},{"paperId":"be445964370b5ea8c15aa1bf6cf10f951fa90b9a","externalIds":{"ArXiv":"2404.04925","DBLP":"journals/corr/abs-2404-04925","DOI":"10.48550/arXiv.2404.04925","CorpusId":269005862},"title":"Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers"},{"paperId":"5760218e4635cc2841dc7fba1752427a023c2193","externalIds":{"DBLP":"journals/fcsc/XuHZQXYG25","ArXiv":"2404.00929","DOI":"10.1007/s11704-024-40579-4","CorpusId":268819377},"title":"A survey on multilingual large language models: corpora, alignment, and bias"},{"paperId":"1a98d909b65ab7e0034e28ed304a65b5cd910687","externalIds":{"ACL":"2024.lrec-main.218","DBLP":"journals/corr/abs-2403-18277","ArXiv":"2403.18277","DOI":"10.48550/arXiv.2403.18277","CorpusId":268724016},"title":"BlendX: Complex Multi-Intent Detection with Blended Patterns"},{"paperId":"543eb210bb6e317d535fe4b2e5996c361caab5e1","externalIds":{"ArXiv":"2403.14888","DBLP":"conf/acl/XueZD024","DOI":"10.48550/arXiv.2403.14888","CorpusId":268667584},"title":"AutoRE: Document-Level Relation Extraction with Large Language Models"},{"paperId":"af97a85f50693189fde8553ef121e6ca91f11541","externalIds":{"ArXiv":"2403.05720","DBLP":"journals/jamia/AaliVAHBRGCDTKC25","DOI":"10.1093/jamia/ocae312","CorpusId":268359475,"PubMed":"39786555"},"title":"A dataset and benchmark for hospital course summarization with adapted large language models"},{"paperId":"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7","externalIds":{"DBLP":"journals/corr/abs-2403-05530","ArXiv":"2403.05530","CorpusId":268297180},"title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"},{"paperId":"3f573716cadfe0bf5c548e2dd384ef2012beb4eb","externalIds":{"DBLP":"journals/corr/abs-2403-02178","ArXiv":"2403.02178","DOI":"10.48550/arXiv.2403.02178","CorpusId":268249090},"title":"Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models"},{"paperId":"2c4bf56c5b1a1f06ee3ca21ce964ba2c8c66cb2c","externalIds":{"DBLP":"journals/corr/abs-2402-17231","ArXiv":"2402.17231","ACL":"2024.naacl-long.54","DOI":"10.48550/arXiv.2402.17231","CorpusId":268032238},"title":"MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning"},{"paperId":"2dc0c0a00544cd306abec905a98e4701e2401ba2","externalIds":{"DBLP":"journals/corr/abs-2402-14361","ArXiv":"2402.14361","DOI":"10.48550/arXiv.2402.14361","CorpusId":267782454},"title":"OpenTab: Advancing Large Language Models as Open-domain Table Reasoners"},{"paperId":"da54bda4ef0ba0f013ac639fb6a3dea1e4745ad4","externalIds":{"PubMedCentral":"10869356","DOI":"10.1038/s41467-024-45563-x","CorpusId":267700596,"PubMed":"38360817"},"title":"Structured information extraction from scientific text with large language models"},{"paperId":"6f826d007e851f0c694fbe0c378d5e0097aa18a2","externalIds":{"DBLP":"journals/fcsc/ZhangWDZC25","ArXiv":"2402.08259","DOI":"10.1007/s11704-024-40330-z","CorpusId":267637152},"title":"A survey of table reasoning with large language models"},{"paperId":"35b142ea69598e6241f0011312128031df55895c","externalIds":{"ArXiv":"2402.03300","DBLP":"journals/corr/abs-2402-03300","DOI":"10.48550/arXiv.2402.03300","CorpusId":267412607},"title":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"},{"paperId":"a5cd606537a666e4807f1c9d342cfeb2b71c4a34","externalIds":{"DBLP":"conf/cscwd/ZhangS00024","ArXiv":"2402.02549","DOI":"10.1109/CSCWD61410.2024.10580146","CorpusId":267411938},"title":"Are Large Language Models Table-based Fact-Checkers?"},{"paperId":"d22e336171c7ac92167f9b03b0e660af922069b4","externalIds":{"DBLP":"journals/corr/abs-2402-02113","ACL":"2024.eacl-long.18","ArXiv":"2402.02113","DOI":"10.48550/arXiv.2402.02113","CorpusId":267412428},"title":"Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon"},{"paperId":"f53557c2d2a4c9b89b4d549bde6a512bc9aa8957","externalIds":{"DBLP":"conf/iclr/PatnaikCAB0K24","ArXiv":"2402.01155","DOI":"10.48550/arXiv.2402.01155","CorpusId":267406213},"title":"CABINET: Content Relevance based Noise Reduction for Table Question Answering"},{"paperId":"ee9fc3765894c843e696e77a38ff7ecb0e76aa0d","externalIds":{"DBLP":"conf/acml/YeD024","ArXiv":"2401.15463","DOI":"10.48550/arXiv.2401.15463","CorpusId":267312293},"title":"DataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure"},{"paperId":"1f2a20a6efaf83214861dddae4a38a83ae18fe32","externalIds":{"ArXiv":"2401.14196","DBLP":"journals/corr/abs-2401-14196","DOI":"10.48550/arXiv.2401.14196","CorpusId":267211867},"title":"DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"},{"paperId":"ecc3415b74717b3f786760e12934a31b37d98312","externalIds":{"ArXiv":"2401.13223","DBLP":"journals/corr/abs-2401-13223","DOI":"10.48550/arXiv.2401.13223","CorpusId":267200238},"title":"TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data"},{"paperId":"ebd1c04c61f73f46def3305ca11d038c46665b65","externalIds":{"DBLP":"conf/icml/XuSCTSDM024","ArXiv":"2401.08417","DOI":"10.48550/arXiv.2401.08417","CorpusId":267028540},"title":"Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"},{"paperId":"f7c89f1f83595257d6e2bc306d4deee4cf77f573","externalIds":{"ArXiv":"2401.06468","DBLP":"journals/corr/abs-2401-06468","DOI":"10.48550/arXiv.2401.06468","CorpusId":266976998},"title":"Adapting Large Language Models for Document-Level Machine Translation"},{"paperId":"a23a89855e3af2e6cec7fd4a01e12cacdf6c727f","externalIds":{"DBLP":"journals/corr/abs-2401-05072","ArXiv":"2401.05072","ACL":"2024.emnlp-main.289","DOI":"10.48550/arXiv.2401.05072","CorpusId":266902834},"title":"Aligning Translation-Specific Understanding to General Understanding in Large Language Models"},{"paperId":"d1679160211bcbedd46bf1826caad5a7e1877315","externalIds":{"DBLP":"journals/tacl/PangYWYSTW25","ArXiv":"2401.08350","DOI":"10.1162/tacl_a_00730","CorpusId":267028612},"title":"Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models"},{"paperId":"8554b7c4ec2466326f5bd55335082edd83183f94","externalIds":{"ArXiv":"2401.00788","DBLP":"journals/corr/abs-2401-00788","DOI":"10.48550/arXiv.2401.00788","CorpusId":266693763},"title":"Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models"},{"paperId":"351a2d50b4aff8e9754dc7074dd589b10a7465d4","externalIds":{"DBLP":"journals/corr/abs-2312-17617","ArXiv":"2312.17617","DOI":"10.1007/s11704-024-40555-y","CorpusId":266690657},"title":"Large language models for generative information extraction: a survey"},{"paperId":"f727986a460a33b9e62ac23f7d8901748e5c10c9","externalIds":{"DBLP":"journals/corr/abs-2312-17449","ArXiv":"2312.17449","DOI":"10.48550/arXiv.2312.17449","CorpusId":266690744},"title":"DB-GPT: Empowering Database Interactions with Private Large Language Models"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","externalIds":{"ArXiv":"2312.14238","DBLP":"journals/corr/abs-2312-14238","DOI":"10.1109/CVPR52733.2024.02283","CorpusId":266521410},"title":"Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"9831f8c43efff346d8bb00e9b4cec59ac96bc7d8","externalIds":{"DBLP":"journals/corr/abs-2312-12740","ArXiv":"2312.12740","DOI":"10.48550/arXiv.2312.12740","CorpusId":266375177},"title":"Fine-tuning Large Language Models for Adaptive Machine Translation"},{"paperId":"00a67af3b7dc785b4813b61d232cc76b4fb2b189","externalIds":{"DBLP":"journals/corr/abs-2312-09039","ArXiv":"2312.09039","DOI":"10.48550/arXiv.2312.09039","CorpusId":266210509},"title":"TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning"},{"paperId":"7e17ef56273063dfa838de30b7cc0546b2e5ee10","externalIds":{"ArXiv":"2312.01678","DBLP":"journals/corr/abs-2312-01678","DOI":"10.48550/arXiv.2312.01678","CorpusId":265609811},"title":"Jellyfish: A Large Language Model for Data Preprocessing"},{"paperId":"5eea245cc12c55905d4df827d0c9776c5ddfa743","externalIds":{"DBLP":"conf/cvpr/MitraHDH24","ArXiv":"2311.17076","DOI":"10.1109/CVPR52733.2024.01367","CorpusId":265498786},"title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models"},{"paperId":"20cc2537130651253ead98d7a7b4077af474d0e7","externalIds":{"ArXiv":"2311.10271","DBLP":"conf/asru/LiuCZOHF23","DOI":"10.1109/ASRU57964.2023.10389663","CorpusId":265281481},"title":"Prompt Pool Based Class-Incremental Continual Learning for Dialog State Tracking"},{"paperId":"0f5245e3a53f69f66b876173affe9309ee31e7d6","externalIds":{"ArXiv":"2311.09758","ACL":"2024.naacl-long.79","DBLP":"conf/naacl/00010O24","DOI":"10.48550/arXiv.2311.09758","CorpusId":265220970},"title":"OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking"},{"paperId":"0c26024316c823e326bed05fd1611de365c0ebb3","externalIds":{"DBLP":"conf/naacl/ZhangYL024","ACL":"2024.naacl-long.335","ArXiv":"2311.09206","DOI":"10.48550/arXiv.2311.09206","CorpusId":265213406},"title":"TableLlama: Towards Open Large Generalist Models for Tables"},{"paperId":"6a3e630eee6b36eb7bf9efeda2a709a953a3e503","externalIds":{"ArXiv":"2311.09193","DBLP":"journals/corr/abs-2311-09193","DOI":"10.48550/arXiv.2311.09193","CorpusId":265212798},"title":"The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task"},{"paperId":"eb8e86d504084531691ff31d0fc23c82fbcee36c","externalIds":{"ArXiv":"2311.06838","DBLP":"conf/ijcnn/GanZM25","DOI":"10.1109/IJCNN64981.2025.11227752","CorpusId":265150160},"title":"GIELLM: Japanese General Information Extraction Large Language Model Utilizing Mutual Reinforcement Effects"},{"paperId":"9218965ff29c4747033c26098c598ee1e9bfa8e1","externalIds":{"DBLP":"journals/corr/abs-2311-03748","ArXiv":"2311.03748","DOI":"10.48550/arXiv.2311.03748","CorpusId":265043702},"title":"Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning"},{"paperId":"2313afae52d98e569da2dedbf14daf9efc74e7cf","externalIds":{"DBLP":"journals/corr/abs-2311-03079","ArXiv":"2311.03079","DOI":"10.48550/arXiv.2311.03079","CorpusId":265034288},"title":"CogVLM: Visual Expert for Pretrained Language Models"},{"paperId":"281f8411654a8bbca3b15dd95fdba42e03d6d666","externalIds":{"DBLP":"journals/corr/abs-2310-15987","ArXiv":"2310.15987","DOI":"10.48550/arXiv.2310.15987","CorpusId":264439637},"title":"Dissecting In-Context Learning of Translations in GPTs"},{"paperId":"4a99be7d5e0fbbdb28914bd5e96df26949ecb75e","externalIds":{"DBLP":"conf/emnlp/0001CWHC23","ArXiv":"2310.14799","DOI":"10.48550/arXiv.2310.14799","CorpusId":264591444},"title":"Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages"},{"paperId":"f176d0d466d7c778a6435fe9a8d7e49508cb9059","externalIds":{"ArXiv":"2310.14628","DBLP":"journals/corr/abs-2310-14628","DOI":"10.48550/arXiv.2310.14628","CorpusId":264426101},"title":"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts"},{"paperId":"4f55797c559bfaae4597ba67550d5d38f09285a9","externalIds":{"DBLP":"journals/corr/abs-2310-14970","ArXiv":"2310.14970","DOI":"10.48550/arXiv.2310.14970","CorpusId":264436605},"title":"Towards LLM-driven Dialogue State Tracking"},{"paperId":"9af2b40a6d9edeb3d53d7e612018bdbff993ffd2","externalIds":{"ArXiv":"2310.13448","DBLP":"conf/emnlp/AlvesGAPRSCM23","DOI":"10.48550/arXiv.2310.13448","CorpusId":264405904},"title":"Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning"},{"paperId":"19e3e729e7521c9c67b3c7b7481dbea65d8a4158","externalIds":{"ArXiv":"2310.11113","DBLP":"journals/tosem/ZhangITL25","DOI":"10.1145/3697009","CorpusId":264172780},"title":"Revisiting Sentiment Analysis for Software Engineering in the Era of Large Language Models"},{"paperId":"ee96922369a8ee1eb7bda1501520bb7916cde733","externalIds":{"DBLP":"conf/acl/RavautSCJ24","ArXiv":"2310.10570","DOI":"10.18653/v1/2024.acl-long.153","CorpusId":264146949},"title":"On Context Utilization in Summarization with Large Language Models"},{"paperId":"a994b90a982e622dfb473a9c7a51b1993f12f511","externalIds":{"DBLP":"journals/corr/abs-2310-10358","ArXiv":"2310.10358","DOI":"10.48550/arXiv.2310.10358","CorpusId":264146587},"title":"Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs"},{"paperId":"3c55aa582a2d682eb53e9237589234854e0b92d8","externalIds":{"DBLP":"conf/emnlp/XieLZZLW23","ArXiv":"2310.10035","DOI":"10.48550/arXiv.2310.10035","CorpusId":264147089},"title":"Empirical Study of Zero-Shot NER with ChatGPT"},{"paperId":"2cbdd4ef5a14dbf3446e1d5b0ecf4b222ded820d","externalIds":{"DBLP":"journals/corr/abs-2310-10520","ArXiv":"2310.10520","DOI":"10.48550/arXiv.2310.10520","CorpusId":264146741},"title":"Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking"},{"paperId":"3f413dca2607d68301143770e599b59d461a569e","externalIds":{"DBLP":"journals/corr/abs-2310-09263","ArXiv":"2310.09263","DOI":"10.48550/arXiv.2310.09263","CorpusId":264127877},"title":"Table-GPT: Table-tuned GPT for Diverse Table Tasks"},{"paperId":"1fcffc771fdebb50682d3aad716b955ef0e72fe3","externalIds":{"ACL":"2023.nlint-1.1","DBLP":"journals/corr/abs-2310-08885","ArXiv":"2310.08885","DOI":"10.48550/arXiv.2310.08885","CorpusId":264128313},"title":"InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems"},{"paperId":"1e98a9532d4e1fcf947d5b215e2cfabbf6cc41e0","externalIds":{"ArXiv":"2310.08582","DBLP":"journals/corr/abs-2310-08582","DOI":"10.48550/arXiv.2310.08582","CorpusId":263909090},"title":"Tree-Planner: Efficient Close-loop Task Planning with Large Language Models"},{"paperId":"f84d6d6d58b836a64c4a96b062bfff769d08a595","externalIds":{"ArXiv":"2310.06502","DBLP":"journals/corr/abs-2310-06502","DOI":"10.48550/arXiv.2310.06502","CorpusId":263830120},"title":"The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis"},{"paperId":"3f40edfcafc018b2cb54612a9aaa9d6b43a11a26","externalIds":{"ArXiv":"2310.03668","DBLP":"conf/iclr/SainzGALRA24","DOI":"10.48550/arXiv.2310.03668","CorpusId":263671572},"title":"GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"8946891e94831adc8cddb0d32311cce2445c96d2","externalIds":{"DBLP":"conf/iclr/LuBX0LH0CG024","ArXiv":"2310.02255","CorpusId":264491155},"title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"},{"paperId":"58219d9826f9ddde448c73e7ecc690111f5698f4","externalIds":{"DBLP":"journals/pvldb/ZhangHFCDP24","ArXiv":"2310.00815","DOI":"10.48550/arXiv.2310.00815","CorpusId":263605799},"title":"ReAcTable: Enhancing ReAct for Table Question Answering"},{"paperId":"54814744b42b06c855c97b23de1366e0bcbe775a","externalIds":{"ArXiv":"2309.17421","DBLP":"journals/corr/abs-2309-17421","DOI":"10.48550/arXiv.2309.17421","CorpusId":263310951},"title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)"},{"paperId":"75ce9634d281cc12cbe434f86c737df8e10796fa","externalIds":{"ACL":"2024.lrec-main.1269","DBLP":"journals/corr/abs-2309-12940","ArXiv":"2309.12940","DOI":"10.48550/arXiv.2309.12940","CorpusId":262217024},"title":"Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models"},{"paperId":"74acec7abaa6193107c6e3dfc630758ad9474af5","externalIds":{"DBLP":"journals/corr/abs-2309-12669","ArXiv":"2309.12669","DOI":"10.48550/arXiv.2309.12669","CorpusId":262217366},"title":"HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering"},{"paperId":"166d1e5361465f8e235747d14641249cbb3b6fd2","externalIds":{"DBLP":"conf/iclr/Xu0SA24","ArXiv":"2309.11674","DOI":"10.48550/arXiv.2309.11674","CorpusId":262084016},"title":"A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models"},{"paperId":"5ca0a9e4c0277e379740f889f00c79ddf507569c","externalIds":{"ArXiv":"2309.11668","ACL":"2023.wmt-1.44","DBLP":"journals/corr/abs-2309-11668","DOI":"10.48550/arXiv.2309.11668","CorpusId":262083660},"title":"Towards Effective Disambiguation for Machine Translation with Large Language Models"},{"paperId":"bc9f29881c1d93d225f0a74fa700531202c7043a","externalIds":{"DBLP":"journals/corr/abs-2309-10706","ArXiv":"2309.10706","DOI":"10.1007/s11432-023-4128-3","CorpusId":262053463},"title":"OpenBA: an open-sourced 15B bilingual asymmetric Seq2Seq model pre-trained from scratch"},{"paperId":"034f1d77d832460a239072c81b5bb178b93c1e9f","externalIds":{"DBLP":"journals/corr/abs-2309-08827","ArXiv":"2309.08827","DOI":"10.48550/arXiv.2309.08827","CorpusId":262043409},"title":"S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs"},{"paperId":"cd7c9fbb2acab241b0b4c7837877a19335c7284c","externalIds":{"ACL":"2024.findings-eacl.90","DBLP":"conf/eacl/ChenJBKHH24","ArXiv":"2309.08958","DOI":"10.48550/arXiv.2309.08958","CorpusId":262053896},"title":"Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca"},{"paperId":"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","externalIds":{"DBLP":"conf/iclr/YueQZFH00C24","ArXiv":"2309.05653","DOI":"10.48550/arXiv.2309.05653","CorpusId":261696697},"title":"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"},{"paperId":"e26888285436bc7998e5c95102a9beb60144be5e","externalIds":{"DBLP":"journals/corr/abs-2309-05463","ArXiv":"2309.05463","DOI":"10.48550/arXiv.2309.05463","CorpusId":261696657},"title":"Textbooks Are All You Need II: phi-1.5 technical report"},{"paperId":"6d928d835c31963ff32c5b5ec6e0dbd1cbadf2c9","externalIds":{"ACL":"2023.newsum-1.7","ArXiv":"2309.04269","DBLP":"journals/corr/abs-2309-04269","DOI":"10.18653/v1/2023.newsum-1.7","CorpusId":261660381,"PubMed":"39315281"},"title":"From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting"},{"paperId":"8a1a8290f7d42b0ce60445a4c0130ef737b3ff69","externalIds":{"DBLP":"conf/sigdial/AddleseeSGGDL23","ACL":"2023.sigdial-1.22","ArXiv":"2308.15231","DOI":"10.48550/arXiv.2308.15231","CorpusId":261276294},"title":"Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering"},{"paperId":"0b0debb710366cdff461938c80763eace1651af6","externalIds":{"DBLP":"journals/corr/abs-2308-12950","ArXiv":"2308.12950","DOI":"10.48550/arXiv.2308.12950","CorpusId":261100919},"title":"Code Llama: Open Foundation Models for Code"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","externalIds":{"DBLP":"journals/fcsc/WangMFZYZCTCLZWW24","ArXiv":"2308.11432","DOI":"10.1007/s11704-024-40231-1","CorpusId":261064713},"title":"A survey on large language model based autonomous agents"},{"paperId":"9a7b9515b66bf83c9c808626206eabe9a8837c22","externalIds":{"DBLP":"journals/tosem/WeyssowZKLS25","ArXiv":"2308.10462","DOI":"10.1145/3714461","CorpusId":261048999},"title":"Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models"},{"paperId":"ba4aa83248a1d08b521392eb971e47d10b7c74e1","externalIds":{"ArXiv":"2308.08614","DBLP":"journals/corr/abs-2308-08614","DOI":"10.48550/arXiv.2308.08614","CorpusId":261030743},"title":"Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought"},{"paperId":"451a657dabf80ebc43f6a3be518250b2cd5dfe1a","externalIds":{"DBLP":"journals/corr/abs-2308-07902","ArXiv":"2308.07902","ACL":"2023.ccl-2.8","DOI":"10.48550/arXiv.2308.07902","CorpusId":260899983},"title":"Through the Lens of Core Competency: Survey on Evaluation of Large Language Models"},{"paperId":"9fec5cf2f06e6fd8c5e6f6028226082d1ecec5b7","externalIds":{"DBLP":"journals/corr/abs-2308-04948","ArXiv":"2308.04948","DOI":"10.48550/arXiv.2308.04948","CorpusId":260736143},"title":"Extrapolating Large Language Models to Non-English by Aligning Languages"},{"paperId":"51ad726fdb12fce1f09bda7347bbf251d5ce971b","externalIds":{"DBLP":"journals/taslp/FengFDKQ24","ArXiv":"2308.03275","DOI":"10.1109/TASLP.2024.3414313","CorpusId":260680370},"title":"Adapter-Based Selective Knowledge Distillation for Federated Multi-Domain Meeting Summarization"},{"paperId":"ebbffe5db352a10fde868843b8d5787b87843f09","externalIds":{"ArXiv":"2308.03656","DBLP":"journals/corr/abs-2308-03656","DOI":"10.48550/arXiv.2308.03656","CorpusId":260682960},"title":"Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench"},{"paperId":"76513f54fcecf7a380f77ad785f05c3bc869db4a","externalIds":{"DBLP":"journals/tacl/AdlakhaBLMR24a","ACL":"2024.tacl-1.38","ArXiv":"2307.16877","DOI":"10.1162/tacl_a_00667","CorpusId":260334056},"title":"Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","externalIds":{"DBLP":"journals/corr/abs-2307-16789","ArXiv":"2307.16789","DOI":"10.48550/arXiv.2307.16789","CorpusId":260334759},"title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"e01ab53663e5df5961a021506a9cb09f4efc3788","externalIds":{"DBLP":"journals/corr/abs-2307-10169","ArXiv":"2307.10169","DOI":"10.48550/arXiv.2307.10169","CorpusId":259982665},"title":"Challenges and Applications of Large Language Models"},{"paperId":"13a96e00be6e940ad87e4628eedac4f86d4a6de6","externalIds":{"ArXiv":"2307.07135","DBLP":"conf/acl/0001HCCZLCX23","DOI":"10.48550/arXiv.2307.07135","CorpusId":259858970},"title":"MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System"},{"paperId":"a72975eb88eb31f193e9587e7415cb04e7bcdbee","externalIds":{"ACL":"2024.eacl-long.4","DBLP":"conf/eacl/MuhlgayRMLRBALSS24","ArXiv":"2307.06908","DOI":"10.48550/arXiv.2307.06908","CorpusId":259847758},"title":"Generating Benchmarks for Factuality Evaluation of Language Models"},{"paperId":"3e664adb009dce373129a3563e4b2cb08731bc76","externalIds":{"ArXiv":"2307.06018","DBLP":"journals/corr/abs-2307-06018","DOI":"10.48550/arXiv.2307.06018","CorpusId":259837230},"title":"PolyLM: An Open Source Polyglot Large Language Model"},{"paperId":"82b5c48ef2fe6165e58d76e4ebbb42b0b95659fa","externalIds":{"ACL":"2023.acl-short.143","ArXiv":"2307.05567","DBLP":"conf/acl/LuRTJ23","DOI":"10.48550/arXiv.2307.05567","CorpusId":259370619},"title":"Event Extraction as Question Generation and Answering"},{"paperId":"2dae1ad9b0c5cafc870ca2c300d639fa6047fb00","externalIds":{"ArXiv":"2307.01453","DBLP":"journals/corr/abs-2307-01453","DOI":"10.48550/arXiv.2307.01453","CorpusId":259341613},"title":"Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking"},{"paperId":"e9388e699a7ef4d12ae425e341ff610c67cbf64b","externalIds":{"DBLP":"journals/corr/abs-2307-00186","ArXiv":"2307.00186","DOI":"10.48550/arXiv.2307.00186","CorpusId":259316136},"title":"How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain"},{"paperId":"7a6a298efb965ce9a351a3212f6f536e94dbbb03","externalIds":{"ArXiv":"2306.14050","DBLP":"conf/acl/LiHYRC023","ACL":"2023.acl-long.150","DOI":"10.48550/arXiv.2306.14050","CorpusId":259251773},"title":"Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step"},{"paperId":"454c8fef2957aa2fb13eb2c7a454393a2ee83805","externalIds":{"DBLP":"journals/corr/abs-2306-08568","ArXiv":"2306.08568","CorpusId":259164815},"title":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct"},{"paperId":"473eb062612a17c965eaa62136322f0dec6b1f8e","externalIds":{"DBLP":"journals/corr/abs-2306-07209","ArXiv":"2306.07209","DOI":"10.48550/arXiv.2306.07209","CorpusId":259137864},"title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow"},{"paperId":"214fbadc57e954e325dc055fee5ac0e224dfde11","externalIds":{"DBLP":"journals/corr/abs-2306-01386","ArXiv":"2306.01386","ACL":"2023.acl-short.81","DOI":"10.48550/arXiv.2306.01386","CorpusId":259063822},"title":"ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?"},{"paperId":"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","externalIds":{"DBLP":"journals/corr/abs-2305-19308","ArXiv":"2305.19308","DOI":"10.48550/arXiv.2305.19308","CorpusId":258987743},"title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"},{"paperId":"c695c4e68561347564ea0daa50dc339dff73d8c5","externalIds":{"DBLP":"journals/corr/abs-2305-17144","ArXiv":"2305.17144","DOI":"10.48550/arXiv.2305.17144","CorpusId":258959262},"title":"Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory"},{"paperId":"c589ddc6c6fb07189af7c1212f6eb15c5ff72cde","externalIds":{"DBLP":"journals/corr/abs-2305-15005","ArXiv":"2305.15005","DOI":"10.48550/arXiv.2305.15005","CorpusId":258866189},"title":"Sentiment Analysis in the Era of Large Language Models: A Reality Check"},{"paperId":"6942bde24d01c412bdd53414cc88459afa5fac7d","externalIds":{"ACL":"2024.tacl-1.32","ArXiv":"2305.15083","DBLP":"journals/tacl/LiZHCC24","DOI":"10.1162/tacl_a_00655","CorpusId":258865882},"title":"Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions"},{"paperId":"69335077fcacbff7a7cf25697da1949e6bdfa968","externalIds":{"ArXiv":"2305.14999","DBLP":"conf/emnlp/QiXSLJWH23","DOI":"10.18653/v1/2023.emnlp-main.255","CorpusId":264935025},"title":"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models"},{"paperId":"983adfc42734a7ec2595a6b62352de967ab9000a","externalIds":{"DBLP":"journals/corr/abs-2305-14336","ArXiv":"2305.14336","DOI":"10.48550/arXiv.2305.14336","CorpusId":258841398},"title":"Schema-Driven Information Extraction from Heterogeneous Tables"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"bd5deadc58ee45b5e004378ba1d54a96bc947b4a","externalIds":{"ArXiv":"2305.14251","DBLP":"conf/emnlp/MinKLLYKIZH23","DOI":"10.48550/arXiv.2305.14251","CorpusId":258841470},"title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"},{"paperId":"83152f52af32a2a2ad4843680c386ac996ee5e39","externalIds":{"ArXiv":"2305.13140","DBLP":"journals/corr/abs-2305-13140","DOI":"10.48550/arXiv.2305.13140","CorpusId":258832656},"title":"Extrapolating Multilingual Understanding Models as Multilingual Generators"},{"paperId":"2338d7c9ab07e6d0f4160335dce0e6e6a87c4749","externalIds":{"ACL":"2023.acl-long.482","DBLP":"journals/corr/abs-2305-13412","ArXiv":"2305.13412","DOI":"10.48550/arXiv.2305.13412","CorpusId":258841145},"title":"Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method"},{"paperId":"daf9e24adbba3d1aead91cbac26502d3043db069","externalIds":{"DBLP":"journals/corr/abs-2305-13512","ArXiv":"2305.13512","DOI":"10.48550/arXiv.2305.13512","CorpusId":258841217},"title":"Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding"},{"paperId":"744a98cc2736fa71d3984602e10b68319a47c65e","externalIds":{"DBLP":"conf/sigsoft/WanWHGBL23","ArXiv":"2305.12434","DOI":"10.1145/3611643.3616310","CorpusId":258833296},"title":"BiasAsker: Measuring the Bias in Conversational AI System"},{"paperId":"e01c2a01d54365c8833b816de8ef39a752ffa9b0","externalIds":{"DBLP":"conf/acl/ZhangG023","ArXiv":"2305.11159","DOI":"10.48550/arXiv.2305.11159","CorpusId":258762464},"title":"Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"e0f27336698c84709bd60b6b7f4ce588cbae66bf","externalIds":{"DBLP":"journals/corr/abs-2305-09645","ArXiv":"2305.09645","DOI":"10.48550/arXiv.2305.09645","CorpusId":258714753},"title":"StructGPT: A General Framework for Large Language Model to Reason over Structured Data"},{"paperId":"ec56f49bef8925dc8931cc261ab3aca4dd36ad2d","externalIds":{"DBLP":"journals/corr/abs-2305-09067","ArXiv":"2305.09067","DOI":"10.48550/arXiv.2305.09067","CorpusId":258715201},"title":"SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting"},{"paperId":"9ada8fa11b1cdece31f253acae50b62df8d5f823","externalIds":{"DBLP":"journals/corr/abs-2305-07922","ArXiv":"2305.07922","DOI":"10.48550/arXiv.2305.07922","CorpusId":258685677},"title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation"},{"paperId":"60c646f722d1f206561c716aaabb7a1206499eb6","externalIds":{"DBLP":"conf/comad/MundraDDKPK24","ArXiv":"2305.07491","DOI":"10.1145/3632410.3632463","CorpusId":258676175},"title":"A Comprehensive Analysis of Adapter Efficiency"},{"paperId":"97992c13baa6185c03d9e672f53185bc59822596","externalIds":{"DBLP":"journals/corr/abs-2305-06575","ACL":"2024.emnlp-main.55","ArXiv":"2305.06575","DOI":"10.48550/arXiv.2305.06575","CorpusId":258615369},"title":"Chain-of-Dictionary Prompting Elicits Translation in Large Language Models"},{"paperId":"0b29ff236bb8f547d017bf747ad74ad2b8303851","externalIds":{"DBLP":"conf/emnlp/HuangTZZSXW23","ArXiv":"2305.07004","DOI":"10.48550/arXiv.2305.07004","CorpusId":258615377},"title":"Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting"},{"paperId":"1fb5a5298747b8c7d60f98640a543f20d42ab053","externalIds":{"ACL":"2023.acl-long.346","DBLP":"journals/corr/abs-2305-05940","ArXiv":"2305.05940","DOI":"10.48550/arXiv.2305.05940","CorpusId":258588286},"title":"Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment"},{"paperId":"a86dd6c62d3dc9c7989c98a3e4ace3fd8000e515","externalIds":{"ACL":"2023.acl-long.855","ArXiv":"2305.05711","DBLP":"journals/corr/abs-2305-05711","DOI":"10.48550/arXiv.2305.05711","CorpusId":258588144},"title":"CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors"},{"paperId":"3e4085e5869f1b7959707a1e1d7d273b6057eb4e","externalIds":{"DBLP":"journals/tmlr/LiAZMKMMALCLZZW23","ArXiv":"2305.06161","CorpusId":258588247},"title":"StarCoder: may the source be with you!"},{"paperId":"f2cd02c03d0169374442d9bc227c9aed178f4b20","externalIds":{"ArXiv":"2305.02105","DBLP":"conf/emnlp/WanCMLSLK23","DOI":"10.48550/arXiv.2305.02105","CorpusId":258461040},"title":"GPT-RE: In-context Learning for Relation Extraction using Large Language Models"},{"paperId":"56fa65d8dc41708082f9b2ef7752c49cee9ebe01","externalIds":{"DBLP":"conf/acl/WangWLGYR23","ACL":"2023.acl-long.304","ArXiv":"2305.01879","DOI":"10.48550/arXiv.2305.01879","CorpusId":258461058},"title":"SCOTT: Self-Consistent Chain-of-Thought Distillation"},{"paperId":"83bc360fa9874564cf8ccbd3a00e6e14dbe4e4e6","externalIds":{"DBLP":"conf/emnlp/QiuHZLL24","ArXiv":"2305.00450","DOI":"10.18653/v1/2024.findings-emnlp.34","CorpusId":258427051},"title":"SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support"},{"paperId":"c47f0a5feb18b036004b5404ef78ac94a65fa489","externalIds":{"DBLP":"journals/corr/abs-2305-00118","ArXiv":"2305.00118","DOI":"10.48550/arXiv.2305.00118","CorpusId":258426273},"title":"Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","externalIds":{"DBLP":"journals/corr/abs-2304-13712","ArXiv":"2304.13712","DOI":"10.1145/3649506","CorpusId":258331833},"title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"59fc49dfd81b92661437eaf7e339c0792ccd8755","externalIds":{"ArXiv":"2304.10436","DBLP":"journals/corr/abs-2304-10436","DOI":"10.48550/arXiv.2304.10436","CorpusId":258236069},"title":"Safety Assessment of Chinese Large Language Models"},{"paperId":"616597b6c8cc3d24339d9f16bb4b195624046abe","externalIds":{"DBLP":"journals/corr/abs-2304-09582","ArXiv":"2304.09582","DOI":"10.48550/arXiv.2304.09582","CorpusId":258212863},"title":"Is ChatGPT Equipped with Emotional Dialogue Capabilities?"},{"paperId":"c4e4b72da211dbf2ab2fd5263d453cf22ee0cf44","externalIds":{"DBLP":"journals/talip/BiCJXGCZ24","ArXiv":"2304.09048","DOI":"10.1145/3641850","CorpusId":258187563},"title":"CodeKGC: Code Language Model for Generative Knowledge Graph Construction"},{"paperId":"bbb2fc6e95d24fb58ab6c25b216b14ac49a32fbe","externalIds":{"DBLP":"journals/corr/abs-2304-08085","ArXiv":"2304.08085","DOI":"10.48550/arXiv.2304.08085","CorpusId":258179792},"title":"InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction"},{"paperId":"1aeb3239735e28c7318af096044e48d919ea500b","externalIds":{"DBLP":"journals/corr/abs-2304-04339","ArXiv":"2304.04339","DOI":"10.48550/arXiv.2304.04339","CorpusId":258048703},"title":"Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study"},{"paperId":"1a01c982aa20c1a1ad1ad94866e3197da99a52a2","externalIds":{"ArXiv":"2304.04193","DBLP":"conf/emnlp/0011LZ23","DOI":"10.48550/arXiv.2304.04193","CorpusId":258048787},"title":"Extractive Summarization via ChatGPT for Faithful Summary Generation"},{"paperId":"8ce0d339896e658b124aee63532f085c55fee225","externalIds":{"ArXiv":"2304.04256","DBLP":"journals/corr/abs-2304-04256","DOI":"10.48550/arXiv.2304.04256","CorpusId":258049061},"title":"A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding"},{"paperId":"68850153b0210615c86f9a72624f34e2913bcddf","externalIds":{"DBLP":"journals/corr/abs-2304-02210","ArXiv":"2304.02210","DOI":"10.18653/v1/2023.emnlp-main.1036","CorpusId":257952312},"title":"Document-Level Machine Translation with Large Language Models"},{"paperId":"c715914c388fa64dd8686cd8755e5adfebbf2388","externalIds":{"DBLP":"journals/corr/abs-2304-01904","ArXiv":"2304.01904","ACL":"2024.eacl-long.67","DOI":"10.48550/arXiv.2304.01904","CorpusId":257921623},"title":"REFINER: Reasoning Feedback on Intermediate Representations"},{"paperId":"bdb68c5e2369633b20e733774ac66eb4600c34d1","externalIds":{"DBLP":"journals/corr/abs-2304-01933","ArXiv":"2304.01933","DOI":"10.48550/arXiv.2304.01933","CorpusId":257921386},"title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"3aaf6a2cbad5850ad81ab5c163599cb3d523436f","externalIds":{"DBLP":"journals/corr/abs-2303-17651","ArXiv":"2303.17651","DOI":"10.48550/arXiv.2303.17651","CorpusId":257900871},"title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"293499319bdd460cb3fca1f0f5eb330e64bf3ff9","externalIds":{"ArXiv":"2303.13780","DBLP":"conf/emnlp/Peng0ZS0ZOT23","DOI":"10.2139/ssrn.4390455","CorpusId":257704711},"title":"Towards Making the Most of ChatGPT for Machine Translation"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","externalIds":{"ArXiv":"2303.11381","DBLP":"journals/corr/abs-2303-11381","DOI":"10.48550/arXiv.2303.11381","CorpusId":257637012},"title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"0671fd553dd670a4e820553a974bc48040ba0819","externalIds":{"DBLP":"conf/nips/ShinnCGNY23","ArXiv":"2303.11366","CorpusId":258833055},"title":"Reflexion: language agents with verbal reinforcement learning"},{"paperId":"90e0a949da276b66d13920a185a6e35042337518","externalIds":{"ACL":"2023.newsum-1.2","ArXiv":"2302.14229","DOI":"10.18653/v1/2023.newsum-1.2","CorpusId":257985556},"title":"Zero-Shot Cross-Lingual Summarization via Large Language Models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"f4cba0db34aa0c389cec267ca1f3ba5255ea2645","externalIds":{"ArXiv":"2302.10205","DBLP":"journals/corr/abs-2302-10205","CorpusId":257050669},"title":"Zero-Shot Information Extraction via Chatting with ChatGPT"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","externalIds":{"DBLP":"journals/corr/abs-2302-04761","ArXiv":"2302.04761","DOI":"10.48550/arXiv.2302.04761","CorpusId":256697342},"title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","externalIds":{"DBLP":"journals/corr/abs-2302-00923","ArXiv":"2302.00923","DOI":"10.48550/arXiv.2302.00923","CorpusId":256504063},"title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"a4a41319d5805a29316f24ed9519f09db77d4c29","externalIds":{"ArXiv":"2301.13848","DBLP":"journals/tacl/ZhangLDLMH24a","ACL":"2024.tacl-1.3","DOI":"10.1162/tacl_a_00632","CorpusId":256416014},"title":"Benchmarking Large Language Models for News Summarization"},{"paperId":"5988806996e8d12f5d4aa911960d842cf7be0c24","externalIds":{"DBLP":"conf/sigir/YeHYLHL23","ArXiv":"2301.13808","DOI":"10.1145/3539618.3591708","CorpusId":256416408},"title":"Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning"},{"paperId":"b115c1e1e9e51f8ad7d47b745bc04e29a654b84d","externalIds":{"DBLP":"journals/corr/abs-2301-13379","ACL":"2023.ijcnlp-main.20","ArXiv":"2301.13379","DOI":"10.48550/arXiv.2301.13379","CorpusId":256416127},"title":"Faithful Chain-of-Thought Reasoning"},{"paperId":"0a6bc37a07a37e3573d36e10cc11669eca0ff903","externalIds":{"ArXiv":"2301.13816","DBLP":"journals/corr/abs-2301-13816","DOI":"10.48550/arXiv.2301.13816","CorpusId":256416258},"title":"Execution-based Code Generation using Deep Reinforcement Learning"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","externalIds":{"DBLP":"journals/corr/abs-2301-03988","ArXiv":"2301.03988","DOI":"10.48550/arXiv.2301.03988","CorpusId":255570209},"title":"SantaCoder: don't reach for the stars!"},{"paperId":"1278dc2e5077b9a0fe5266ae6850a59d8231b41f","externalIds":{"ArXiv":"2212.13465","DBLP":"journals/corr/abs-2212-13465","DOI":"10.48550/arXiv.2212.13465","CorpusId":255186530},"title":"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions"},{"paperId":"84d96ad6389c03299b596b962fcc76412b7013db","externalIds":{"ACL":"2023.acl-long.713","DBLP":"journals/corr/abs-2212-10449","ArXiv":"2212.10449","DOI":"10.48550/arXiv.2212.10449","CorpusId":254877060},"title":"Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization"},{"paperId":"f6b171486d0240fff7a464c0fbbfd84483945924","externalIds":{"DBLP":"journals/corr/abs-2212-10018","ArXiv":"2212.10018","ACL":"2023.acl-long.76","DOI":"10.48550/arXiv.2212.10018","CorpusId":254877347},"title":"DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","externalIds":{"ArXiv":"2212.10535","DBLP":"journals/corr/abs-2212-10535","ACL":"2023.acl-long.817","DOI":"10.48550/arXiv.2212.10535","CorpusId":254877175},"title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","externalIds":{"ACL":"2023.acl-long.830","DBLP":"conf/acl/HoSY23","ArXiv":"2212.10071","DOI":"10.48550/arXiv.2212.10071","CorpusId":254877399},"title":"Large Language Models Are Reasoning Teachers"},{"paperId":"44ebfdb670007b3949507be0d1a1fca93bc3d5d5","externalIds":{"DBLP":"journals/corr/abs-2212-09660","ArXiv":"2212.09660","DOI":"10.48550/arXiv.2212.09660","CorpusId":254854317},"title":"The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges"},{"paperId":"2dc07d25a22467c01b6d5088ea148e58eb18cc2a","externalIds":{"DBLP":"journals/corr/abs-2212-05901","ArXiv":"2212.05901","DOI":"10.48550/arXiv.2212.05901","CorpusId":254564456},"title":"Parameter-Efficient Finetuning of Transformers for Source Code"},{"paperId":"e6d7caa77eae7b64f78359669f4dd48fd2d177a1","externalIds":{"DBLP":"conf/acl/BhaskarFD23","ArXiv":"2211.15914","DOI":"10.18653/v1/2023.findings-acl.591","CorpusId":258841826},"title":"Prompted Opinion Summarization with GPT-3.5"},{"paperId":"1e983fccd65cbd39712fa360b92235e5497d81b6","externalIds":{"ACL":"2022.emnlp-main.243","DBLP":"journals/corr/abs-2211-16164","ArXiv":"2211.16164","DOI":"10.48550/arXiv.2211.16164","CorpusId":254070082},"title":"Few-shot Query-Focused Summarization with Prefix-Merging"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","externalIds":{"DBLP":"journals/tmlr/ChenM0C23","ArXiv":"2211.12588","CorpusId":253801709},"title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"f26d0419a3e8e0d3128279c0a1c962a67f30b729","externalIds":{"DBLP":"journals/corr/abs-2211-10986","ArXiv":"2211.10986","DOI":"10.48550/arXiv.2211.10986","CorpusId":253735130},"title":"UnifiedABSA: A Unified ABSA Framework Based on Multi-task Instruction Tuning"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","externalIds":{"ArXiv":"2211.10435","DBLP":"journals/corr/abs-2211-10435","DOI":"10.48550/arXiv.2211.10435","CorpusId":253708270},"title":"PAL: Program-aided Language Models"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"e66f0f822d4c4853b39b27daaafa2993005fd55e","externalIds":{"DBLP":"conf/eacl/Chen23","ACL":"2023.findings-eacl.83","ArXiv":"2210.06710","DOI":"10.48550/arXiv.2210.06710","CorpusId":252872943},"title":"Large Language Models are few(1)-shot Table Reasoners"},{"paperId":"c1614ab718dad97018ee34fd57864bb58b6ecaba","externalIds":{"DBLP":"journals/corr/abs-2210-06656","ArXiv":"2210.06656","DOI":"10.48550/arXiv.2210.06656","CorpusId":252872940},"title":"Knowledge-grounded Dialog State Tracking"},{"paperId":"5dbc2b2ee6e65e39fa3fc4bd5030be7a4a9f9a76","externalIds":{"ACL":"2023.wassa-1.3","DBLP":"journals/corr/abs-2210-06629","ArXiv":"2210.06629","DOI":"10.48550/arXiv.2210.06629","CorpusId":252873265},"title":"Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis"},{"paperId":"8add69e155596bc128df70e1ddd5a41c68698399","externalIds":{"DBLP":"conf/emnlp/LiCLWQY22","ArXiv":"2210.04185","DOI":"10.48550/arXiv.2210.04185","CorpusId":252780701},"title":"Controllable Dialogue Simulation with In-Context Learning"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","externalIds":{"DBLP":"journals/corr/abs-2210-03493","ArXiv":"2210.03493","CorpusId":252762275},"title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","externalIds":{"DBLP":"conf/iclr/ShiSF0SVCTRZ0W23","ArXiv":"2210.03057","DOI":"10.48550/arXiv.2210.03057","CorpusId":252735112},"title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","externalIds":{"DBLP":"journals/corr/abs-2210-02875","ArXiv":"2210.02875","CorpusId":252734772},"title":"Binding Language Models in Symbolic Languages"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","externalIds":{"DBLP":"conf/iclr/YaoZYDSN023","ArXiv":"2210.03629","CorpusId":252762395},"title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","externalIds":{"DBLP":"conf/iclr/Lu0CWZRCK23","ArXiv":"2209.14610","DOI":"10.48550/arXiv.2209.14610","CorpusId":252595921},"title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"83851f1a32d41975582ca62355858ab5e34738f7","externalIds":{"ArXiv":"2209.12356","DBLP":"journals/corr/abs-2209-12356","DOI":"10.48550/arXiv.2209.12356","CorpusId":252532176},"title":"News Summarization and Evaluation in the Era of GPT-3"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"17bcb1edbe068e8fe6a97da552c70a77a15bbce7","externalIds":{"DBLP":"journals/corr/abs-2209-07858","ArXiv":"2209.07858","DOI":"10.48550/arXiv.2209.07858","CorpusId":252355458},"title":"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","externalIds":{"DBLP":"journals/corr/abs-2207-11280","ArXiv":"2207.11280","DOI":"10.48550/arXiv.2207.11280","CorpusId":251040785},"title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"c3e0d2e7c6533556ad5f8d85efc53db22c660001","externalIds":{"ArXiv":"2207.05270","DBLP":"conf/ccks/JinSLC22","DOI":"10.48550/arXiv.2207.05270","CorpusId":250451509},"title":"A Survey on Table Question Answering: Recent Advances"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","externalIds":{"ArXiv":"2207.01780","DBLP":"journals/corr/abs-2207-01780","DOI":"10.48550/arXiv.2207.01780","CorpusId":250280117},"title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","externalIds":{"DBLP":"journals/corr/abs-2205-11916","ArXiv":"2205.11916","CorpusId":249017743},"title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"4f68042a0aa40f34027a49ceec64ad2bbe2211aa","externalIds":{"ACL":"2022.emnlp-main.540","DBLP":"journals/corr/abs-2205-11277","ArXiv":"2205.11277","DOI":"10.48550/arXiv.2205.11277","CorpusId":248986389},"title":"When does Parameter-Efficient Transfer Learning Work for Machine Translation?"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"5ecc001af7dcfde1a9eb0b7e96ea4ea1fb9631e5","externalIds":{"DBLP":"journals/corr/abs-2204-04362","ACL":"2022.naacl-main.357","ArXiv":"2204.04362","DOI":"10.48550/arXiv.2204.04362","CorpusId":248085567},"title":"Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization"},{"paperId":"a07fead63ca30fcd708f66de173c70f40262817c","externalIds":{"ArXiv":"2204.04327","DBLP":"journals/corr/abs-2204-04327","ACL":"2022.naacl-main.336","DOI":"10.18653/v1/2022.naacl-main.336","CorpusId":248085039},"title":"Show, Don’t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","externalIds":{"DBLP":"conf/iclr/0002WSLCNCZ23","ArXiv":"2203.11171","CorpusId":247595263},"title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"b645e706651391eca1f692e7f560051c21b3dea4","externalIds":{"ArXiv":"2203.08568","DBLP":"conf/emnlp/Hu0X0SO22","DOI":"10.48550/arXiv.2203.08568","CorpusId":247476332},"title":"In-Context Learning for Few-Shot Dialogue State Tracking"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"f4df78183261538e718066331898ee5cad7cad05","externalIds":{"DBLP":"journals/corr/abs-2202-12837","ArXiv":"2202.12837","ACL":"2022.emnlp-main.759","DOI":"10.18653/v1/2022.emnlp-main.759","CorpusId":247155069},"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"6fee6e61543a776dbf83ec78320191dcf07ca406","externalIds":{"DBLP":"journals/air/WankhadeRK22","DOI":"10.1007/s10462-022-10144-1","CorpusId":246663281},"title":"A survey on sentiment analysis methods, applications, and challenges"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7efb1788b5e0fa3b4d9932722286ba1753b42f91","externalIds":{"DBLP":"journals/corr/abs-2201-08904","ArXiv":"2201.08904","CorpusId":246240888},"title":"Description-Driven Task-Oriented Dialog Modeling"},{"paperId":"79950179d60ba39a74d5fe2aedc47a57c0bf4c03","externalIds":{"DBLP":"journals/corr/abs-2201-05966","ACL":"2022.emnlp-main.39","ArXiv":"2201.05966","DOI":"10.18653/v1/2022.emnlp-main.39","CorpusId":246016124},"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904","externalIds":{"ArXiv":"2112.10668","DBLP":"conf/emnlp/LinMAWCSOGBDPSK22","ACL":"2022.emnlp-main.616","DOI":"10.18653/v1/2022.emnlp-main.616","CorpusId":245334784},"title":"Few-shot Learning with Multilingual Generative Language Models"},{"paperId":"42fc019b2668c9d9d984154d4c57f6c6d5a91619","externalIds":{"ArXiv":"2109.07684","ACL":"2021.mrl-1.1","DBLP":"journals/corr/abs-2109-07684","DOI":"10.18653/v1/2021.mrl-1.1","CorpusId":237532173},"title":"Language Models are Few-shot Multilingual Learners"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","externalIds":{"DBLP":"conf/emnlp/0034WJH21","ACL":"2021.emnlp-main.685","ArXiv":"2109.00859","DOI":"10.18653/v1/2021.emnlp-main.685","CorpusId":237386541},"title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"c6bf48f25e0a65d64d658b47326de5922ea7dd44","externalIds":{"DBLP":"journals/corr/abs-2104-08704","ArXiv":"2104.08704","ACL":"2022.acl-long.464","DOI":"10.18653/v1/2022.acl-long.464","CorpusId":233296648},"title":"A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation"},{"paperId":"014d4ccf128b1ea741498cfa6ca5e1e5cdd2284c","externalIds":{"ArXiv":"2103.03095","DBLP":"journals/corr/abs-2103-03095","DOI":"10.24963/ijcai.2021/622","CorpusId":232110865},"title":"A Survey on Spoken Language Understanding: Recent Advances and New Frontiers"},{"paperId":"870ff1dde0c103c3d90be51880f984628e77a8d6","externalIds":{"DBLP":"conf/nips/LuGRHSBCDJTLZSZ21","ArXiv":"2102.04664","CorpusId":231855531},"title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"ce3b364b7e6358940ce97d8d5887a65e5024ca21","externalIds":{"DBLP":"conf/fat/DhamalaSKKPCG21","ArXiv":"2101.11718","DOI":"10.1145/3442188.3445924","CorpusId":231719337},"title":"BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","externalIds":{"ArXiv":"2010.11934","DBLP":"conf/naacl/XueCRKASBR21","MAG":"3169483174","ACL":"2021.naacl-main.41","DOI":"10.18653/V1/2021.NAACL-MAIN.41","CorpusId":225040574},"title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"64525c2d24dfe356bec68c680f502cc75e7d743b","externalIds":{"DBLP":"journals/iotj/HuangXNZW19","MAG":"2973126236","DOI":"10.1109/JIOT.2019.2940709","CorpusId":203153138},"title":"Multimodal Representation Learning for Recommendation in Internet of Things"},{"paperId":"d53733e06efd8fb53e584605ee601af0394d9c3d","externalIds":{"ACL":"D19-1214","MAG":"2971167298","DBLP":"conf/emnlp/QinCLWL19","ArXiv":"1909.02188","DOI":"10.18653/v1/D19-1214","CorpusId":202541352},"title":"A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"4e32deccbe723d87b247d1e9c71938e4bfeb204a","externalIds":{"MAG":"2902744721","ArXiv":"1812.02303","DBLP":"journals/corr/abs-1812-02303","DOI":"10.1145/3419106","CorpusId":54448559},"title":"Neural Abstractive Text Summarization with Sequence-to-Sequence Models"},{"paperId":"047a000fdd280d1953d230d162374971a32cd6b8","externalIds":{"MAG":"2562335618","DBLP":"conf/slt/SarikayaCMJRCKR16","DOI":"10.1109/SLT.2016.7846294","CorpusId":2981246},"title":"An overview of end-to-end language understanding and dialog management for personal digital assistants"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"3863c181a54465b2fbd7af7fdb6fb165309f5296","externalIds":{"MAG":"648947103","CorpusId":60003789},"title":"Spoken Language Understanding: Systems for Extracting Semantic Information from Speech"},{"paperId":"d0d3f4d1003db0fb637519ef5d8bb140e7df8355","externalIds":{"MAG":"2399631432","CorpusId":37645299,"PubMed":"14513624"},"title":"May the source be with you."},{"paperId":"151754ee59f95d44220b0a11f993bb887b711eb2","externalIds":{"DOI":"10.3934/mfc.2025017","CorpusId":277722496},"title":"Elevating large language model reasoning ability with auto-enhanced zero-shot prompts"},{"paperId":"74399047adf8d76477ef97e9228c725d137688d8","externalIds":{"DBLP":"journals/corr/abs-2502-09674","DOI":"10.48550/arXiv.2502.09674","CorpusId":276394674},"title":"The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis"},{"paperId":"0446902557c72325db09cbdf2a80fc2b6e00b7e8","externalIds":{"DBLP":"conf/coling/DongCY25","CorpusId":275821073},"title":"ProTOD: Proactive Task-oriented Dialogue System Based on Large Language Model"},{"paperId":"0f2fbed561e37da842c98633faf8fe1de9e2e174","externalIds":{"DBLP":"journals/corr/abs-2402-02242","DOI":"10.48550/arXiv.2402.02242","CorpusId":267412110},"title":"Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey"},{"paperId":"b1d2ac6c68bfa2929b7f9d4fcf4067bee1c4e5db","externalIds":{"DBLP":"conf/nips/XinLL0ZCLDWCLHW24","DOI":"10.52202/079017-2560","CorpusId":276260025},"title":"V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark"},{"paperId":"3d4ad4e49ef71fece47e679d83dac4be2663ca6f","externalIds":{"ACL":"2024.icnlsp-1.47","DBLP":"conf/icnlsp/FornasiereB0C24","CorpusId":273819697},"title":"Medical Information Extraction with Large Language Models"},{"paperId":"30017e6ae47ee0b676dc3c68c95608fc26c76fc5","externalIds":{"DBLP":"journals/corr/abs-2412-13801","DOI":"10.48550/arXiv.2412.13801","CorpusId":279261423},"title":"A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on Method-Level Code Smell Detection"},{"paperId":"985298bd0b6f4c84ab34bb15cfed0b092f43b83a","externalIds":{"DBLP":"journals/corr/abs-2412-16720","DOI":"10.48550/arXiv.2412.16720","CorpusId":272648256},"title":"OpenAI o1 System Card"},{"paperId":"3607b92705cd1fba04eaf295f57be4b085791063","externalIds":{"CorpusId":263613825},"title":"Dialogue Distillery: Crafting Interpolable, Interpretable, and Introspectable Dialogue from LLMs"},{"paperId":"faabae8c5e8bb77cdde3b71c4900893ec878e7cc","externalIds":{"DBLP":"conf/ranlp/BelkhirS23","ACL":"2023.ranlp-1.18","DOI":"10.26615/978-954-452-092-2_018","CorpusId":265068407},"title":"Beyond Information: Is ChatGPT Empathetic Enough?"},{"paperId":"bc73bc1d7f63a33e686d0bb61f64b10ae31f1a69","externalIds":{"CorpusId":259325710},"title":"SEAGULL: An Embodied Agent for Instruction Following through Situated Dialog"},{"paperId":"f41f07a857c867498465e3024d1c3beb59b3f43e","externalIds":{"DBLP":"conf/emnlp/YangL23","DOI":"10.18653/v1/2023.findings-emnlp.403","CorpusId":266176158},"title":"Visual Elements Mining as Prompts for Instruction Learning for Target-Oriented Multimodal Sentiment Classification"},{"paperId":"879c7942118b781645d183b05ba9a1b6986133fd","externalIds":{"ACL":"2023.acl-long.16","DBLP":"conf/acl/LiangWJQHH23","DOI":"10.18653/v1/2023.acl-long.16","CorpusId":259370523},"title":"Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning"},{"paperId":"a38dc45c9f74e600bab0b1a6b9ff2d9ba59cd3d5","externalIds":{"DBLP":"conf/emnlp/ZhangL0L23","DOI":"10.18653/v1/2023.emnlp-main.132","CorpusId":266163859},"title":"CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data"},{"paperId":"918b08a07d0578b600a624c9a120bebde7f47d5c","externalIds":{"DBLP":"conf/emnlp/LiLZ0XZ23","DOI":"10.18653/v1/2023.emnlp-main.532","CorpusId":266163974},"title":"MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning"},{"paperId":"fb81264ac2fde6fecbdaaa2b96b9fa9e35bb2e08","externalIds":{"ACL":"2023.newsum-1.6","DOI":"10.18653/v1/2023.newsum-1.6","CorpusId":265607950},"title":"In-context Learning of Large Language Models for Controlled Dialogue Summarization: A Holistic Benchmark and Empirical Analysis"},{"paperId":"3f3ee90e464c637b88054cd55161ef409d0142cf","externalIds":{"DBLP":"conf/acl/0001WCC23","DOI":"10.18653/v1/2023.findings-acl.69","CorpusId":259859125},"title":"CLIPText: A New Paradigm for Zero-shot Text Classification"},{"paperId":"b555fbe8d4e30fc1312c615ab7dfe6ac030dbcff","externalIds":{"DBLP":"journals/corr/abs-2305-13627","DOI":"10.48550/arXiv.2305.13627","CorpusId":258841457},"title":"Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction"},{"paperId":"deb6dd39a68ea0d949f580a7944abf595185e150","externalIds":{"DBLP":"journals/corr/abs-2304-06556","DOI":"10.48550/arXiv.2304.06556","CorpusId":260631934},"title":"Are LLMs All You Need for Task-Oriented Dialogue?"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","externalIds":{"DBLP":"journals/corr/abs-2303-17568","DOI":"10.48550/arXiv.2303.17568","CorpusId":271089099},"title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"4f8e75d10cc9bcbd20e6aefea972b6d419a688cc","externalIds":{"DBLP":"conf/emnlp/2021f","DOI":"10.18653/v1/2021.findings-emnlp","CorpusId":244119161},"title":"Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021"},{"paperId":"24de1048791bac4972ecc16d1c3c1de23691407d","externalIds":{"CorpusId":266378240},"title":"Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"}]}