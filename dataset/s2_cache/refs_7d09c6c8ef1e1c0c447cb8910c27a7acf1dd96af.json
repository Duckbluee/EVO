{"references":[{"paperId":"eaac29467de2dd223d32cc3d3a77b637ef2bc4b3","externalIds":{"DBLP":"journals/corr/abs-2405-18653","ArXiv":"2405.18653","DOI":"10.1145/3705725","CorpusId":270095224},"title":"Recent Advances of Foundation Language Models-based Continual Learning: A Survey"},{"paperId":"8ce5f6e28d49e1bc00804fa2fa3e917deb203388","externalIds":{"ArXiv":"2405.14768","DBLP":"conf/nips/0104L0XY0X0C24","DOI":"10.48550/arXiv.2405.14768","CorpusId":269982715},"title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models"},{"paperId":"99f6acfc14ee633787f987768b31b240a87c2c0f","externalIds":{"DBLP":"journals/corr/abs-2402-12749","PubMedCentral":"11142305","DOI":"10.21203/rs.3.rs-4240043/v1","CorpusId":267759846,"PubMed":"38826372"},"title":"Me-LLaMA: Foundation Large Language Models for Medical Applications"},{"paperId":"89e13c80ff90c6e2f2dda700b5dd6c3be1aabf7d","externalIds":{"ArXiv":"2404.17790","DBLP":"journals/corr/abs-2404-17790","DOI":"10.48550/arXiv.2404.17790","CorpusId":269449465},"title":"Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities"},{"paperId":"cd2277d7d5776972287b586b67e3d290af60d5d6","externalIds":{"ArXiv":"2404.16621","DBLP":"journals/corr/abs-2404-16621","DOI":"10.48550/arXiv.2404.16621","CorpusId":269362560},"title":"Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare"},{"paperId":"cafe794035266eaef5d53e5d37aa486c71db9703","externalIds":{"ArXiv":"2404.07965","DBLP":"journals/corr/abs-2404-07965","DOI":"10.48550/arXiv.2404.07965","CorpusId":269042762},"title":"Rho-1: Not All Tokens Are What You Need"},{"paperId":"8ec57d7830c55261a18851471cb60d09f393a12d","externalIds":{"DBLP":"journals/corr/abs-2404-05875","ArXiv":"2404.05875","DOI":"10.48550/arXiv.2404.05875","CorpusId":269009666},"title":"CodecLM: Aligning Language Models with Tailored Synthetic Data"},{"paperId":"1a0c866c373bc68e17163c27ba85d77412414e31","externalIds":{"DBLP":"journals/corr/abs-2404-03264","ArXiv":"2404.03264","DOI":"10.1109/RBME.2024.3496744","CorpusId":268889461,"PubMed":"39531565"},"title":"Foundation Model for Advancing Healthcare: Challenges, Opportunities and Future Directions"},{"paperId":"4c7b9e38306221d3586cb106544e77602ec0459c","externalIds":{"DBLP":"journals/corr/abs-2404-03608","ACL":"2024.emnlp-demo.45","ArXiv":"2404.03608","DOI":"10.48550/arXiv.2404.03608","CorpusId":268889543},"title":"Sailor: Open Language Models for South-East Asia"},{"paperId":"ae0b7357da82cdb63427ffa6bd89e8da425306b6","externalIds":{"ArXiv":"2403.19137","DBLP":"conf/nips/JhaG024","DOI":"10.48550/arXiv.2403.19137","CorpusId":268733226},"title":"CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models"},{"paperId":"5b46ebad43c6bbd45249fd1490b9bd7a86d1acda","externalIds":{"ArXiv":"2403.18365","DBLP":"journals/corr/abs-2403-18365","DOI":"10.48550/arXiv.2403.18365","CorpusId":268723886},"title":"BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models"},{"paperId":"72cdf7586442a61fcc94a165282c5d9f5f0d6f5c","externalIds":{"ArXiv":"2403.18383","DBLP":"conf/cvpr/CaoLHLC24","DOI":"10.1109/CVPR52733.2024.02712","CorpusId":268724307},"title":"Generative Multi-modal Models are Good Class-Incremental Learners"},{"paperId":"9115ac5452dd4867ac52b024d0e60343b6868740","externalIds":{"ArXiv":"2403.11549","DBLP":"conf/cvpr/YuZ0H0LH24","DOI":"10.1109/CVPR52733.2024.02191","CorpusId":268532523},"title":"Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters"},{"paperId":"5372f4364fe5b7991c81ea19ee944c717afe6e0a","externalIds":{"ArXiv":"2403.11901","DBLP":"conf/icml/DasCNMSDLKC0DC24","DOI":"10.48550/arXiv.2403.11901","CorpusId":268532114},"title":"Larimar: Large Language Models with Episodic Memory Control"},{"paperId":"76b65c248677314865a110424542c220886dbb67","externalIds":{"DBLP":"journals/corr/abs-2403-11435","ACL":"2024.naacl-long.37","ArXiv":"2403.11435","DOI":"10.48550/arXiv.2403.11435","CorpusId":268513020},"title":"InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions"},{"paperId":"ac4114f174c72e06419941617b4929028051b3a5","externalIds":{"ArXiv":"2403.11373","DBLP":"journals/corr/abs-2403-11373","DOI":"10.48550/arXiv.2403.11373","CorpusId":268513563},"title":"Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration"},{"paperId":"e044799b6ecae8ff0e1ea549a01dfa35ccac530e","externalIds":{"DBLP":"journals/corr/abs-2403-10056","ArXiv":"2403.10056","DOI":"10.48550/arXiv.2403.10056","CorpusId":268510177},"title":"Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning"},{"paperId":"71009d6a45307b2e265f1a575d22704c41c794a8","externalIds":{"DBLP":"journals/corr/abs-2403-10245","ArXiv":"2403.10245","DOI":"10.1109/TNNLS.2025.3547882","CorpusId":268509875,"PubMed":"40193261"},"title":"CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning"},{"paperId":"8fe40a6de15979eb5e33ac72bdd8e80ac898c39e","externalIds":{"DBLP":"journals/corr/abs-2403-09613","ArXiv":"2403.09613","DOI":"10.48550/arXiv.2403.09613","CorpusId":268385313},"title":"Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training"},{"paperId":"ddcbdb923f0f76f7b64e3bc8adee78240cfe03ad","externalIds":{"DBLP":"conf/eccv/YuHCCLYW24","ArXiv":"2403.09296","DOI":"10.48550/arXiv.2403.09296","CorpusId":268384815},"title":"Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models"},{"paperId":"d0075dd5603c9d477edf0a41f59ab6dcf0e91976","externalIds":{"ArXiv":"2403.08763","DBLP":"journals/corr/abs-2403-08763","DOI":"10.48550/arXiv.2403.08763","CorpusId":268379604},"title":"Simple and Scalable Strategies to Continually Pre-train Large Language Models"},{"paperId":"e198314dbbe1c98957b6e6eea3c91ab47f7decfd","externalIds":{"DBLP":"journals/corr/abs-2403-08350","ArXiv":"2403.08350","DOI":"10.48550/arXiv.2403.08350","CorpusId":268379670},"title":"CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model"},{"paperId":"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7","externalIds":{"DBLP":"journals/corr/abs-2403-05530","ArXiv":"2403.05530","CorpusId":268297180},"title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"},{"paperId":"cf841544a9cb18f9fee3ed632862e08f6667815e","externalIds":{"ArXiv":"2403.03894","DBLP":"journals/corr/abs-2403-03894","DOI":"10.48550/arXiv.2403.03894","CorpusId":268253688},"title":"IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators"},{"paperId":"13f44206745d20971ca271401eff6772aa80de80","externalIds":{"DBLP":"journals/corr/abs-2403-03883","ArXiv":"2403.03883","DOI":"10.48550/arXiv.2403.03883","CorpusId":268253108},"title":"SaulLM-7B: A pioneering Large Language Model for Law"},{"paperId":"140858a7e8998c4b8c86cdc8f7c9b84dd6f7b736","externalIds":{"ArXiv":"2403.01554","DBLP":"journals/corr/abs-2403-01554","DOI":"10.48550/arXiv.2403.01554","CorpusId":268248289},"title":"Transformers for Supervised Online Continual Learning"},{"paperId":"015f62d7a59f7a4301c0cdbe997460c38148d07b","externalIds":{"DBLP":"conf/acl/HuangCWYLSYS24","ArXiv":"2403.01244","DOI":"10.48550/arXiv.2403.01244","CorpusId":268230393},"title":"Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal"},{"paperId":"12358df20ccf4085e6c8a45d3ab5fa15714abcd6","externalIds":{"DBLP":"journals/corr/abs-2402-17400","ArXiv":"2402.17400","DOI":"10.48550/arXiv.2402.17400","CorpusId":268032887},"title":"Investigating Continual Pretraining in Large Language Models: Insights and Implications"},{"paperId":"fd21fa3b285b85f2d508baf0d5d77db2cd2c60ca","externalIds":{"ArXiv":"2402.14270","DBLP":"journals/corr/abs-2402-14270","DOI":"10.48550/arXiv.2402.14270","CorpusId":267782519},"title":"Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization"},{"paperId":"d496eced38a50d5f1e0926d9e6dac71ccfab0bb3","externalIds":{"DBLP":"conf/acl/SongGZXWSZG0QL24","ArXiv":"2402.13013","DOI":"10.48550/arXiv.2402.13013","CorpusId":267760221},"title":"Code Needs Comments: Enhancing Code LLMs with Comment Augmentation"},{"paperId":"1c0b3679919cd0531973fced1a1eb49745d9332d","externalIds":{"ArXiv":"2402.12847","DBLP":"journals/corr/abs-2402-12847","DOI":"10.48550/arXiv.2402.12847","CorpusId":267759882},"title":"Instruction-tuned Language Models are Better Knowledge Learners"},{"paperId":"a8c422a624ad846ac4d2074a98645d9eb2f364a8","externalIds":{"DBLP":"conf/icml/Zhu000YD0K24","ArXiv":"2402.12048","DOI":"10.48550/arXiv.2402.12048","CorpusId":267751282},"title":"Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models"},{"paperId":"59b4b5b1e2198f264536d83e33d96b0a45ed3bac","externalIds":{"ArXiv":"2402.11260","DBLP":"journals/corr/abs-2402-11260","DOI":"10.48550/arXiv.2402.11260","CorpusId":267751418},"title":"MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning"},{"paperId":"dfbbc9373a575fb3031cd195c7aa616e183f0b60","externalIds":{"DBLP":"journals/corr/abs-2402-10987","ArXiv":"2402.10987","DOI":"10.48550/arXiv.2402.10987","CorpusId":267751068},"title":"WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing"},{"paperId":"b900b97d7657bdac9be8badb948a18e5eacefb9c","externalIds":{"DBLP":"conf/icml/ShenTHAF24","ArXiv":"2402.05140","DOI":"10.48550/arXiv.2402.05140","CorpusId":267547938},"title":"Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains"},{"paperId":"50c9dc9907d19afcafc082332e53b5a0ac58956d","externalIds":{"DBLP":"conf/icml/Jin024","ArXiv":"2402.01865","DOI":"10.48550/arXiv.2402.01865","CorpusId":267411970},"title":"What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement"},{"paperId":"ad1bb59e3e18a0dd8503c3961d6074f162baf710","externalIds":{"DBLP":"conf/acl/SoldainiKBSAABC24","ArXiv":"2402.00159","DOI":"10.48550/arXiv.2402.00159","CorpusId":267364861},"title":"Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research"},{"paperId":"1f2a20a6efaf83214861dddae4a38a83ae18fe32","externalIds":{"ArXiv":"2401.14196","DBLP":"journals/corr/abs-2401-14196","DOI":"10.48550/arXiv.2401.14196","CorpusId":267211867},"title":"DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"},{"paperId":"9579fd37922c25cf841041e158d7b9a81c4acd6b","externalIds":{"DBLP":"journals/corr/abs-2401-10716","ArXiv":"2401.10716","DOI":"10.48550/arXiv.2401.10716","CorpusId":267060969},"title":"Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models"},{"paperId":"645284e103010343446ebaa39f36a32b168652e6","externalIds":{"ArXiv":"2401.09646","DBLP":"journals/corr/abs-2401-09646","DOI":"10.48550/arXiv.2401.09646","CorpusId":267034871},"title":"ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change"},{"paperId":"bd7271fd7f595f66c47eaee28d5f556731882a18","externalIds":{"ArXiv":"2401.09181","DBLP":"journals/corr/abs-2401-09181","DOI":"10.48550/arXiv.2401.09181","CorpusId":267028471},"title":"Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer"},{"paperId":"da217117909eb5ef310160795271f88bf606f0ab","externalIds":{"DBLP":"conf/acl/ZhaoWHZQZYXC24","ArXiv":"2401.08295","DOI":"10.18653/v1/2024.acl-long.625","CorpusId":267027714},"title":"SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models"},{"paperId":"c064c79e3026f81e5043cd5b0f4264b4d43336e6","externalIds":{"ArXiv":"2401.06199","DBLP":"journals/corr/abs-2401-06199","DOI":"10.1101/2023.07.05.547496","CorpusId":259502990},"title":"xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein"},{"paperId":"f352a10baaf1b23b020c95f9a64a808bce5342e2","externalIds":{"DBLP":"journals/corr/abs-2401-03129","ArXiv":"2401.03129","DOI":"10.48550/arXiv.2401.03129","CorpusId":266844262},"title":"Examining Forgetting in Continual Pre-training of Aligned Large Language Models"},{"paperId":"7260442ef9c0448f07ce3803efd49cebaffcebe9","externalIds":{"ArXiv":"2401.02954","DBLP":"journals/corr/abs-2401-02954","CorpusId":266818336},"title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"},{"paperId":"c3d1832ed0444f75d44116fabbdda891aebc4b01","externalIds":{"ArXiv":"2401.02415","DBLP":"conf/acl/WuGGLWFSL24","DOI":"10.48550/arXiv.2401.02415","CorpusId":266755997},"title":"LLaMA Pro: Progressive LLaMA with Block Expansion"},{"paperId":"60f47874beb5f00364f8621410d1d34c37d11007","externalIds":{"DBLP":"journals/corr/abs-2401-01916","ArXiv":"2401.01916","DOI":"10.3847/2515-5172/ad1abe","CorpusId":266755857},"title":"AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets"},{"paperId":"04a340b15945c70e642227bf249639b171beb3f8","externalIds":{"DBLP":"journals/corr/abs-2401-01600","ArXiv":"2401.01600","DOI":"10.48550/arXiv.2401.01600","CorpusId":266741610},"title":"PLLaMa: An Open-source Large Language Model for Plant Science"},{"paperId":"8554b7c4ec2466326f5bd55335082edd83183f94","externalIds":{"ArXiv":"2401.00788","DBLP":"journals/corr/abs-2401-00788","DOI":"10.48550/arXiv.2401.00788","CorpusId":266693763},"title":"Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models"},{"paperId":"0b9eea26e5818981c173d03209adb472b8e3a38e","externalIds":{"ArXiv":"2312.11795","DBLP":"journals/corr/abs-2312-11795","DOI":"10.48550/arXiv.2312.11795","CorpusId":266362196},"title":"MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA"},{"paperId":"3713112311efbcf785de17fa86e5bf42e4360f77","externalIds":{"ArXiv":"2312.11370","DBLP":"conf/iclr/GaoPZYZ0HHXLK25","DOI":"10.48550/arXiv.2312.11370","CorpusId":266359733},"title":"G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model"},{"paperId":"6d8557383facc7610fda9703ff3ae8785135ace3","externalIds":{"DBLP":"journals/corr/abs-2311-17601","ArXiv":"2311.17601","DOI":"10.48550/arXiv.2311.17601","CorpusId":265498940},"title":"Continual Learning with Low Rank Adaptation"},{"paperId":"2b3554a8fea6f123fc04bd3e120f2293f227e1b2","externalIds":{"ArXiv":"2311.16208","DBLP":"journals/corr/abs-2311-16208","DOI":"10.48550/arXiv.2311.16208","CorpusId":265466509},"title":"InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery"},{"paperId":"fe92a35c51ebba91ed99f7da0e0124434229a469","externalIds":{"DBLP":"journals/corr/abs-2311-16206","ArXiv":"2311.16206","DOI":"10.48550/arXiv.2311.16206","CorpusId":265466199},"title":"Continual Instruction Tuning for Large Multimodal Models"},{"paperId":"3d7e5485fae2965ddf081dc64be6ab52f5834cf8","externalIds":{"ArXiv":"2311.11908","DBLP":"journals/corr/abs-2311-11908","DOI":"10.48550/arXiv.2311.11908","CorpusId":265294809},"title":"Continual Learning: Applications and the Road Forward"},{"paperId":"2a86d281bef364e2ea2d4fc61fde46ca25b955f1","externalIds":{"ArXiv":"2311.09774","DBLP":"journals/corr/abs-2311-09774","DOI":"10.48550/arXiv.2311.09774","CorpusId":265221365},"title":"HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs"},{"paperId":"739cf040ed2c2af49077db48d489a46be5fb6157","externalIds":{"DBLP":"conf/acl/XieAA24","ArXiv":"2311.08545","DOI":"10.18653/v1/2024.findings-acl.606","CorpusId":265213147},"title":"Efficient Continual Pre-training for Building Domain Specific Large Language Models"},{"paperId":"0a29191d66a129709980cbe3c937aa9e98707dc8","externalIds":{"DBLP":"conf/sc/YinDWS23","DOI":"10.1145/3581784.3613215","CorpusId":264591559},"title":"FORGE: Pre-Training Open Foundation Models for Science"},{"paperId":"5cd2bf358a36ed729297da8269af5594eb1bb9dd","externalIds":{"DBLP":"journals/corr/abs-2311-00204","ArXiv":"2311.00204","DOI":"10.48550/arXiv.2311.00204","CorpusId":264832958},"title":"Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering"},{"paperId":"30f04f34c3794bc6d8be403de55d733141afc55b","externalIds":{"DBLP":"conf/acl/LiHZCXLSH24","ArXiv":"2310.20329","DOI":"10.18653/v1/2024.acl-srw.6","CorpusId":264803741},"title":"InstructCoder: Instruction Tuning Large Language Models for Code Editing"},{"paperId":"e798b7cbf5663dc3ab801f4b5a8b4365098157b8","externalIds":{"ArXiv":"2310.20348","DBLP":"journals/corr/abs-2310-20348","DOI":"10.48550/arXiv.2310.20348","CorpusId":264820281},"title":"Class Incremental Learning with Pre-trained Vision-Language Models"},{"paperId":"1145c42897cc70b4401b0b52c490c794a09789d8","externalIds":{"DBLP":"journals/tnn/MazziaPCRB25","ArXiv":"2310.19704","DOI":"10.1109/TNNLS.2024.3498935","CorpusId":264820150,"PubMed":"40030360"},"title":"A Survey on Knowledge Editing of Neural Networks"},{"paperId":"21f005c5b15f5c41a45b76b733cb928dfc8e9b05","externalIds":{"DBLP":"conf/iclr/GargFPVMTSF24","ArXiv":"2310.16226","DOI":"10.48550/arXiv.2310.16226","CorpusId":264487212},"title":"TiC-CLIP: Continual Training of CLIP Models"},{"paperId":"15a2682ba1b479dea284062dd097a9a349a2eceb","externalIds":{"ArXiv":"2310.14558","DBLP":"journals/corr/abs-2310-14558","DOI":"10.48550/arXiv.2310.14558","CorpusId":264426685},"title":"AlpaCare: Instruction-tuned Large Language Models for Medical Application"},{"paperId":"75a85d74433d03a78a07bb95f6f261323a79eb80","externalIds":{"DBLP":"journals/corr/abs-2310-14510","ArXiv":"2310.14510","DOI":"10.48550/arXiv.2310.14510","CorpusId":264426357},"title":"CITB: A Benchmark for Continual Instruction Tuning"},{"paperId":"28fde851680a40fbbc5c6a44bd3ac6f5ca4ad284","externalIds":{"ArXiv":"2310.14152","DBLP":"conf/emnlp/WangCGXBZZGH23","DOI":"10.48550/arXiv.2310.14152","CorpusId":264426441},"title":"Orthogonal Subspace Learning for Language Model Continual Learning"},{"paperId":"525d4aee811dcfdfd11afe7d0ae9204f03c8a74e","externalIds":{"ArXiv":"2310.14029","DBLP":"journals/corr/abs-2310-14029","DOI":"10.48550/arXiv.2310.14029","CorpusId":264426172},"title":"LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions"},{"paperId":"938d1028b3c3cd4e3d34eed20b622bdc33453f6e","externalIds":{"ArXiv":"2310.13596","DBLP":"journals/corr/abs-2310-13596","DOI":"10.48550/arXiv.2310.13596","CorpusId":264405801},"title":"MarineGPT: Unlocking Secrets of Ocean to the Public"},{"paperId":"1dea0427ced025fb722bc42e08075aae1e1d6fea","externalIds":{"DBLP":"conf/nips/ShiW23","ArXiv":"2310.12244","DOI":"10.48550/arXiv.2310.12244","CorpusId":264305893},"title":"A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm"},{"paperId":"b16c7d45183b9d595ab64301be019741b1528860","externalIds":{"DBLP":"conf/iclr/AzerbayevSPSMJD24","ArXiv":"2310.10631","DOI":"10.48550/arXiv.2310.10631","CorpusId":264172303},"title":"Llemma: An Open Language Model For Mathematics"},{"paperId":"57eddf6eff27193984314141d04c333c11532fc6","externalIds":{"DBLP":"journals/corr/abs-2310-06266","ArXiv":"2310.06266","DOI":"10.1145/3639477.3639719","CorpusId":263830089},"title":"CodeFuse-13B: A Pretrained Multi-Lingual Code Large Language Model"},{"paperId":"a64067c6c4286fc60f4430829ae6b18519c088e3","externalIds":{"DBLP":"journals/corr/abs-2310-06762","ArXiv":"2310.06762","DOI":"10.48550/arXiv.2310.06762","CorpusId":263830425},"title":"TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models"},{"paperId":"fd183daf4870ec6abd5a5d997c0f9b88d1483f84","externalIds":{"ArXiv":"2310.04801","DBLP":"journals/corr/abs-2310-04801","DOI":"10.48550/arXiv.2310.04801","CorpusId":263829616},"title":"Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing"},{"paperId":"bd09391fbd124dc0c0a6be5d0ab2eb5d9c43fbac","externalIds":{"DBLP":"journals/corr/abs-2310-04793","ArXiv":"2310.04793","DOI":"10.48550/arXiv.2310.04793","CorpusId":263829590},"title":"FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets"},{"paperId":"d6354e91d8dcf73bff50097b76a81de874f7bd7a","externalIds":{"ArXiv":"2310.02031","DBLP":"journals/corr/abs-2310-02031","DOI":"10.48550/arXiv.2310.02031","CorpusId":263608392},"title":"OceanGPT: A Large Language Model for Ocean Science Tasks"},{"paperId":"b272513916b45c8517d289d7abee4a53e6832187","externalIds":{"DBLP":"conf/iclr/GouSGSYHDC24","ArXiv":"2309.17452","DOI":"10.48550/arXiv.2309.17452","CorpusId":263310365},"title":"ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"},{"paperId":"352244ac7602f13e16a08424db322364d0a2cef1","externalIds":{"DBLP":"journals/corr/abs-2309-14763","ArXiv":"2309.14763","DOI":"10.48550/arXiv.2309.14763","CorpusId":262822572},"title":"ConPET: Continual Parameter-Efficient Tuning for Large Language Models"},{"paperId":"6806ecad90a778aaa7f6a3cd3a539582d823066c","externalIds":{"DBLP":"journals/corr/abs-2309-11325","ArXiv":"2309.11325","DOI":"10.48550/arXiv.2309.11325","CorpusId":262064568},"title":"DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services"},{"paperId":"a281094d05e96b7cca044fdd87ff7c3c65649e20","externalIds":{"DBLP":"journals/corr/abs-2309-10313","ArXiv":"2309.10313","DOI":"10.48550/arXiv.2309.10313","CorpusId":262055661},"title":"Investigating the Catastrophic Forgetting in Multimodal Large Language Models"},{"paperId":"844bc3b26b5c63ec3b251ae634c194dcfb41a7d2","externalIds":{"DBLP":"journals/corr/abs-2309-13064","ArXiv":"2309.13064","DOI":"10.48550/arXiv.2309.13064","CorpusId":262459267},"title":"InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning"},{"paperId":"6f8c4311e65efebb9da5a542c0405684e82a77cc","externalIds":{"ACL":"2024.emnlp-main.35","DBLP":"conf/emnlp/LinL0DLZP00ZDPZ24","ArXiv":"2309.06256","DOI":"10.48550/arXiv.2309.06256","CorpusId":261697277},"title":"Mitigating the Alignment Tax of RLHF"},{"paperId":"e33a538e80d1877782df26e1493f5adc661ceec4","externalIds":{"DBLP":"journals/corr/abs-2309-06126","ArXiv":"2309.06126","ACL":"2023.wiesp-1.7","DOI":"10.48550/arXiv.2309.06126","CorpusId":261696577},"title":"AstroLLaMA: Towards Specialized Foundation Models in Astronomy"},{"paperId":"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","externalIds":{"DBLP":"conf/iclr/YueQZFH00C24","ArXiv":"2309.05653","DOI":"10.48550/arXiv.2309.05653","CorpusId":261696697},"title":"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"},{"paperId":"4c6fb350e7769cb730a15c62927b6e9b563d0157","externalIds":{"ArXiv":"2308.13565","DBLP":"journals/corr/abs-2308-13565","DOI":"10.48550/arXiv.2308.13565","CorpusId":274142505},"title":"DARWIN Series: Domain Specific Large Language Models for Natural Science"},{"paperId":"0b0debb710366cdff461938c80763eace1651af6","externalIds":{"DBLP":"journals/corr/abs-2308-12950","ArXiv":"2308.12950","DOI":"10.48550/arXiv.2308.12950","CorpusId":261100919},"title":"Code Llama: Open Foundation Models for Code"},{"paperId":"e3052ebca5eeae6a8a73e44517903d39746f5f3a","externalIds":{"ArXiv":"2308.12032","ACL":"2024.naacl-long.421","DBLP":"journals/corr/abs-2308-12032","DOI":"10.18653/v1/2024.naacl-long.421","CorpusId":261076515},"title":"From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","externalIds":{"DBLP":"journals/corr/abs-2308-09583","ArXiv":"2308.09583","DOI":"10.48550/arXiv.2308.09583","CorpusId":261030818},"title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"e3fd89a7f6b28973cfc68bfc51caebd8fb93f0bc","externalIds":{"DBLP":"journals/corr/abs-2308-09442","ArXiv":"2308.09442","DOI":"10.48550/arXiv.2308.09442","CorpusId":261030404},"title":"BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine"},{"paperId":"838cd69a0b6c9c244a6eebb0f4742c0625132de6","externalIds":{"DBLP":"journals/corr/abs-2308-08747","ArXiv":"2308.08747","DOI":"10.1109/TASLPRO.2025.3606231","CorpusId":261031244},"title":"An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-Tuning"},{"paperId":"be1a943609db984917964aa05372be88cb1da886","externalIds":{"ArXiv":"2308.05361","DBLP":"journals/corr/abs-2308-05361","DOI":"10.48550/arXiv.2308.05361","CorpusId":260775975},"title":"WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine"},{"paperId":"193955704f66923ac20a664bd184ed4663b2bdf9","externalIds":{"DBLP":"journals/corr/abs-2308-04014","ArXiv":"2308.04014","DOI":"10.48550/arXiv.2308.04014","CorpusId":260704601},"title":"Continual Pre-Training of Large Language Models: How to (re)warm your model?"},{"paperId":"de7e5fee8cf03bd485b1104d3e40e8ab45d76c0a","externalIds":{"ArXiv":"2307.14367","DBLP":"conf/aaai/AbdineCBV24","DOI":"10.1609/aaai.v38i10.28948","CorpusId":260203284},"title":"Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers"},{"paperId":"3f459219d75de63b5b7a26a8c6447ec1e79a985c","externalIds":{"DBLP":"journals/corr/abs-2307-13269","ArXiv":"2307.13269","DOI":"10.48550/arXiv.2307.13269","CorpusId":260155012},"title":"LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"3b6179c293df29e31d31cea46476f104ab6950f2","externalIds":{"DBLP":"conf/iclr/Peng00HHMYW24","ArXiv":"2306.14824","DOI":"10.48550/arXiv.2306.14824","CorpusId":259262263},"title":"Kosmos-2: Grounding Multimodal Large Language Models to the World"},{"paperId":"2e7f4c36ea33dbe412acb45b140e53eae70ef514","externalIds":{"DBLP":"conf/acl/ShaoG0023","ArXiv":"2306.12619","ACL":"2023.acl-short.109","DOI":"10.48550/arXiv.2306.12619","CorpusId":259224492},"title":"Class-Incremental Learning based on Label Generation"},{"paperId":"bb9a44c94a89dbe00f0061d05c70a45064ff6ea6","externalIds":{"DBLP":"journals/corr/abs-2306-09212","ArXiv":"2306.09212","DOI":"10.48550/arXiv.2306.09212","CorpusId":259164635},"title":"CMMLU: Measuring massive multitask language understanding in Chinese"},{"paperId":"2a68cfffde314b717ca3fc4bd3ffab597f1b6ea9","externalIds":{"ArXiv":"2306.09525","DBLP":"journals/corr/abs-2306-09525","DOI":"10.48550/arXiv.2306.09525","CorpusId":259187949},"title":"Explaining Legal Concepts with Augmented Large Language Models (GPT-4)"},{"paperId":"454c8fef2957aa2fb13eb2c7a454393a2ee83805","externalIds":{"DBLP":"journals/corr/abs-2306-08568","ArXiv":"2306.08568","CorpusId":259164815},"title":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct"},{"paperId":"116c19f5cdf2bf7884fd25ff2a7683ede6eaaa8a","externalIds":{"DBLP":"conf/kdd/XieLLXWN23","ArXiv":"2306.06707","DOI":"10.1145/3580305.3599891","CorpusId":259137655},"title":"QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search"},{"paperId":"32f541216112de78037d8e0f95ddc152eb6f05fa","externalIds":{"DBLP":"conf/wsdm/DengZHCSXF0WZLH24","ArXiv":"2306.05064","DOI":"10.1145/3616855.3635772","CorpusId":259108887},"title":"K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization"},{"paperId":"109929be7890ef982fb3b6be0d78609cfab1ea13","externalIds":{"DBLP":"journals/corr/abs-2306-05443","ArXiv":"2306.05443","DOI":"10.48550/arXiv.2306.05443","CorpusId":259129602},"title":"PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance"},{"paperId":"4b4ee637ef5107299212479c37a6594db5a72227","externalIds":{"DBLP":"conf/nips/LiuZHCTLWYGZL23","ArXiv":"2306.03030","DOI":"10.48550/arXiv.2306.03030","CorpusId":259075732},"title":"Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"119a3ed0898499fce0ce6af6958d566d82390ba5","externalIds":{"ArXiv":"2306.13089","DBLP":"conf/nips/ZhaoLMXFDKL23","DOI":"10.1101/2023.05.30.542904","CorpusId":259077070},"title":"GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning"},{"paperId":"7d29cc9cdeba6d2d338342d32abcedcc432e4aeb","externalIds":{"ArXiv":"2305.17529","DBLP":"journals/corr/abs-2305-17529","ACL":"2023.acl-long.906","DOI":"10.48550/arXiv.2305.17529","CorpusId":258959349},"title":"MeetingBank: A Benchmark Dataset for Meeting Summarization"},{"paperId":"df7d7e71eba619363e7dbe8b14f4baeb3100ca73","externalIds":{"DBLP":"journals/corr/abs-2305-16633","ArXiv":"2305.16633","DOI":"10.2139/ssrn.4458613","CorpusId":258941519},"title":"Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks"},{"paperId":"5889bd0211316ac04938fa0a05dc81a396965456","externalIds":{"ArXiv":"2305.16252","DBLP":"journals/corr/abs-2305-16252","DOI":"10.48550/arXiv.2305.16252","CorpusId":258887506},"title":"Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"e9b3e82b1c9eb4136df28e94f24cd823431be93b","externalIds":{"DBLP":"journals/corr/abs-2305-12281","ArXiv":"2305.12281","DOI":"10.48550/arXiv.2305.12281","CorpusId":258833488},"title":"Lifelong Language Pretraining with Distribution-Specialized Experts"},{"paperId":"6783b17fe4328f48403f57009a73f784de09f645","externalIds":{"DBLP":"journals/corr/abs-2305-12002","ArXiv":"2305.12002","DOI":"10.1145/3583780.3615285","CorpusId":258833440},"title":"XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"b6d6c33298b852cf63edac233deca70530d69a2a","externalIds":{"ArXiv":"2305.10403","DBLP":"journals/corr/abs-2305-10403","CorpusId":258740735},"title":"PaLM 2 Technical Report"},{"paperId":"67aff54dfbabf66193559b9467d60972fcf446c9","externalIds":{"ArXiv":"2305.09253","DBLP":"journals/corr/abs-2305-09253","DOI":"10.48550/arXiv.2305.09253","CorpusId":258715081},"title":"Online Continual Learning Without the Storage Constraint"},{"paperId":"b05cfda924a147dfc100dc0b3eea451c6db32868","externalIds":{"DBLP":"conf/acl/QinQHLWXLSZ23","ArXiv":"2305.08702","DOI":"10.48550/arXiv.2305.08702","CorpusId":258686422},"title":"Recyclable Tuning for Continual Pre-training"},{"paperId":"ccf45bf1e3d1b040cb28af274ba0e4fe97927616","externalIds":{"ArXiv":"2305.07972","DBLP":"journals/corr/abs-2305-07972","ACL":"2023.acl-long.368","DOI":"10.48550/arXiv.2305.07972","CorpusId":258685646},"title":"Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis"},{"paperId":"96af5ad10d707fd756e8036e4b4f7dc214089879","externalIds":{"DBLP":"journals/corr/abs-2305-07437","ArXiv":"2305.07437","DOI":"10.48550/arXiv.2305.07437","CorpusId":258676581},"title":"Continual Vision-Language Representation Learning with Off-Diagonal Information"},{"paperId":"d789725525d2fbb801bf49979064397674138d76","externalIds":{"ArXiv":"2305.05968","DBLP":"journals/corr/abs-2305-05968","DOI":"10.48550/arXiv.2305.05968","CorpusId":258587921},"title":"Investigating Forgetting in Pre-Trained Representations Through Continual Learning"},{"paperId":"fd38e1f3c5fa7e770c327180fd68bf8bbfc33a70","externalIds":{"ArXiv":"2305.04106","DBLP":"conf/sigsoft/WeyssowZK0S23","DOI":"10.1145/3611643.3616244","CorpusId":258556987},"title":"On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code"},{"paperId":"886e0962479ec6dac563666399ca4c96a468fcaa","externalIds":{"ArXiv":"2305.02309","DBLP":"journals/corr/abs-2305-02309","DOI":"10.48550/arXiv.2305.02309","CorpusId":258461229},"title":"CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"},{"paperId":"04ee9597be4d6d2457214334e495e591000b5542","externalIds":{"ArXiv":"2304.14454","CorpusId":258417843},"title":"PMC-LLaMA: Towards Building Open-source Language Models for Medicine"},{"paperId":"90e41626b8c78600da70c4350c67c3a10525cb37","externalIds":{"ArXiv":"2304.08247","CorpusId":258180068},"title":"MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","externalIds":{"DBLP":"conf/icml/BidermanSABOHKP23","ArXiv":"2304.01373","DOI":"10.48550/arXiv.2304.01373","CorpusId":257921893},"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"bce55193d9a887ad00774a9134df08cd521a85ae","externalIds":{"DBLP":"journals/corr/abs-2304-01097","ArXiv":"2304.01097","DOI":"10.48550/arXiv.2304.01097","CorpusId":257912795},"title":"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","externalIds":{"DBLP":"journals/corr/abs-2303-17564","ArXiv":"2303.17564","CorpusId":257833842},"title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"4a7f6c4e71e20311ade4e76e8d0945d499c31fcd","externalIds":{"PubMedCentral":"10364849","ArXiv":"2303.14070","DOI":"10.7759/cureus.40895","CorpusId":259252045,"PubMed":"37492832"},"title":"ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge"},{"paperId":"763d953e671e2b6c6d0df2f5bc5472fb6ce074de","externalIds":{"DOI":"10.1101/2023.03.16.23287316","CorpusId":257686179},"title":"The utility of ChatGPT for cancer treatment information"},{"paperId":"d1dc16a981a7fce5475a1342241baf596d0de9b3","externalIds":{"DBLP":"conf/cvpr/PrabhuHDTLGB23","ArXiv":"2303.11165","DOI":"10.1109/CVPR52729.2023.00360","CorpusId":257631846},"title":"Computationally Budgeted Continual Learning: What Does Matter?"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"631cee335dbae8f883f426b119686058c4b26951","externalIds":{"DBLP":"journals/corr/abs-2303-06628","ArXiv":"2303.06628","DOI":"10.1109/ICCV51070.2023.01752","CorpusId":257496481},"title":"Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models"},{"paperId":"201047e827ed9587158fc71256c576c8544e3dfc","externalIds":{"DBLP":"conf/iclr/TaoF023","ArXiv":"2303.01081","DOI":"10.48550/arXiv.2303.01081","CorpusId":257279790},"title":"Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"aafae4730b1add0b3e243e011db9ac87428f83cd","externalIds":{"ArXiv":"2302.09432","DBLP":"journals/corr/abs-2302-09432","DOI":"10.48550/arXiv.2302.09432","CorpusId":257038067},"title":"BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark"},{"paperId":"5ffca4f594ea2cf774779bda12fff38b64fe38ab","externalIds":{"DBLP":"conf/iclr/SarfrazAZ23","ArXiv":"2302.11344","DOI":"10.48550/arXiv.2302.11344","CorpusId":257079142},"title":"Error Sensitivity Modulation based Experience Replay: Mitigating Abrupt Representation Drift in Continual Learning"},{"paperId":"e3ec55e9e6720194a0ed5d4033d93a941c8a4f99","externalIds":{"DBLP":"conf/iclr/KeSLKK023","ArXiv":"2302.03241","CorpusId":258079422},"title":"Continual Pre-training of Language Models"},{"paperId":"74013b7cfa0fc524803350fca51341004565eb22","externalIds":{"DBLP":"conf/nips/XieS0L23","ArXiv":"2302.03169","DOI":"10.48550/arXiv.2302.03169","CorpusId":256627727},"title":"Data Selection for Language Models via Importance Resampling"},{"paperId":"9348656b761f7b76fb65cfe6fac55386b04a3a8a","externalIds":{"DBLP":"journals/corr/abs-2302-00487","ArXiv":"2302.00487","DOI":"10.1109/TPAMI.2024.3367329","CorpusId":256459333,"PubMed":"38407999"},"title":"A Comprehensive Survey of Continual Learning: Theory, Method and Application"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"a9be51698e7c2247853b7b6f1f70fc4d6d7ef605","externalIds":{"DBLP":"journals/corr/abs-2301-09785","ArXiv":"2301.09785","DOI":"10.48550/arXiv.2301.09785","CorpusId":256194369},"title":"Transformer-Patcher: One Mistake worth One Neuron"},{"paperId":"9c0a434b240299cec0029a1be93ab263d7ec9963","externalIds":{"DBLP":"journals/corr/abs-2301-04213","ArXiv":"2301.04213","DOI":"10.48550/arXiv.2301.04213","CorpusId":255595518},"title":"Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models"},{"paperId":"6052486bc9144dc1730c12bf35323af3792a1fd0","externalIds":{"ArXiv":"2212.13138","DBLP":"journals/corr/abs-2212-13138","PubMedCentral":"10396962","DOI":"10.1038/s41586-023-06291-2","CorpusId":255124952,"PubMed":"37438534"},"title":"Large language models encode clinical knowledge"},{"paperId":"e1b732e02cd6f41e4e1eb793ec4b356cee2587f1","externalIds":{"DBLP":"journals/corr/abs-2212-06742","ArXiv":"2212.06742","DOI":"10.48550/arXiv.2212.06742","CorpusId":254591305},"title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"84f1cf54197da8c2a1b9f6140ab2d986d1f8176f","externalIds":{"PubMedCentral":"9771807","DBLP":"journals/natmi/VenTT22","DOI":"10.1038/s42256-022-00568-3","CorpusId":254335115,"PubMed":"36567959"},"title":"Three types of incremental learning"},{"paperId":"0b40cdf63f651f89904e5e1c79ab7225b0a666ff","externalIds":{"ArXiv":"2211.12701","DBLP":"journals/corr/abs-2211-12701","DOI":"10.48550/arXiv.2211.12701","CorpusId":253801772},"title":"Continual Learning of Natural Language Processing Tasks: A Survey"},{"paperId":"0c12b22b4b12915546cdc07eaffd0492f0529346","externalIds":{"DBLP":"conf/bibm/YanXSYLR23","ArXiv":"2211.11363","DOI":"10.1109/BIBM58861.2023.10385733","CorpusId":253735287},"title":"AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model"},{"paperId":"560b1bc012588731b26748e33236570df777baa0","externalIds":{"ArXiv":"2211.11031","DBLP":"conf/nips/HartvigsenSPKG23","DOI":"10.48550/arXiv.2211.11031","CorpusId":253735429},"title":"Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","externalIds":{"DBLP":"journals/corr/abs-2211-09085","ArXiv":"2211.09085","CorpusId":253553203},"title":"Galactica: A Large Language Model for Science"},{"paperId":"ee8de585183763ff64cb3c81ecda2fc75fa81507","externalIds":{"ArXiv":"2211.05110","DBLP":"journals/corr/abs-2211-05110","DOI":"10.48550/arXiv.2211.05110","CorpusId":253420654},"title":"Large Language Models with Controllable Working Memory"},{"paperId":"4f91c3defb0482cf7b70c563dad56ab544807e79","externalIds":{"DBLP":"conf/nips/KimXKK022","ArXiv":"2211.02633","DOI":"10.48550/arXiv.2211.02633","CorpusId":253106310},"title":"A Theoretical Study on Solving Continual Learning"},{"paperId":"2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413","externalIds":{"DBLP":"conf/iclr/MengSABB23","ArXiv":"2210.07229","DOI":"10.48550/arXiv.2210.07229","CorpusId":252873467},"title":"Mass-Editing Memory in a Transformer"},{"paperId":"2589c1fc34886f86e970f1394e796f659f0b21c9","externalIds":{"ArXiv":"2210.07365","CorpusId":258480391},"title":"Is It Worth the (Environmental) Cost? Limited Evidence for Temporal Adaptation via Continuous Training"},{"paperId":"e053be7f36a0772b68eaaa14f15650c14071e4ab","externalIds":{"ACL":"2022.emnlp-main.695","DBLP":"conf/emnlp/KeLS0SL22","ArXiv":"2210.05549","DOI":"10.48550/arXiv.2210.05549","CorpusId":252815848},"title":"Continual Training of Language Models for Few-Shot Learning"},{"paperId":"7471cb40a33e9d971a922b5dff5ca9b4a73ca609","externalIds":{"DBLP":"journals/corr/abs-2210-03329","ArXiv":"2210.03329","DOI":"10.48550/arXiv.2210.03329","CorpusId":252762125},"title":"Calibrating Factual Knowledge in Pretrained Language Models"},{"paperId":"c1372b08e382030e905d1c8751a7794ee91e9d31","externalIds":{"DBLP":"journals/corr/abs-2210-03114","ArXiv":"2210.03114","DOI":"10.48550/arXiv.2210.03114","CorpusId":252734867},"title":"CLIP model is an Efficient Continual Learner"},{"paperId":"44279244407a64431810f982be6d0c7da4429dd7","externalIds":{"ArXiv":"2210.10341","DBLP":"journals/bib/LuoSXQZPL22","DOI":"10.1093/bib/bbac409","CorpusId":252542956,"PubMed":"36156661"},"title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"},{"paperId":"894d1ac91443eb7c28dc52116c93d5436a0acecf","externalIds":{"ArXiv":"2209.09476","DBLP":"conf/nips/000200YNJRIWD22","DOI":"10.48550/arXiv.2209.09476","CorpusId":252383174},"title":"SparCL: Sparse Continual Learning on the Edge"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"5f7f90e5a8a48ee5c4414983ac99e286fd0c2375","externalIds":{"DBLP":"journals/corr/abs-2207-13516","ArXiv":"2207.13516","DOI":"10.48550/arXiv.2207.13516","CorpusId":251105108},"title":"Online Continual Learning with Contrastive Vision Transformer"},{"paperId":"463d94fe036d1335c497491be14d47cbeda49601","externalIds":{"DBLP":"conf/eccv/WangZLZZ22","ArXiv":"2207.06543","DOI":"10.48550/arXiv.2207.06543","CorpusId":250526805},"title":"CoSCL: Cooperation of Small Continual Learners is Stronger than a Big One"},{"paperId":"d6954c43aa1ca197319c45d3988bc8fcec3de976","externalIds":{"ArXiv":"2206.15331","DBLP":"journals/jss/DakhelMNKDJ23","DOI":"10.48550/arXiv.2206.15331","CorpusId":250144223},"title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"1d650f1afd45c59ff907396fe8b678595dcb85ea","externalIds":{"DBLP":"conf/icml/MitchellLBMF22","ArXiv":"2206.06520","CorpusId":249642147},"title":"Memory-Based Model Editing at Scale"},{"paperId":"d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3","externalIds":{"ACL":"2022.emnlp-main.410","ArXiv":"2205.12393","DBLP":"conf/emnlp/ScialomCM22","DOI":"10.18653/v1/2022.emnlp-main.410","CorpusId":252815378},"title":"Fine-tuned Language Models are Continual Learners"},{"paperId":"0dac0e73dc0d6f0ebbbd45ea2e3bc60d437200e1","externalIds":{"DBLP":"journals/corr/abs-2205-10956","ArXiv":"2205.10956","DOI":"10.1145/3533767.3534219","CorpusId":248986947},"title":"CIRCLE: continual repair across programming languages"},{"paperId":"b3bc37a15aa74c523d656ad89b1896651f5eef72","externalIds":{"ArXiv":"2205.09357","DBLP":"journals/corr/abs-2205-09357","DOI":"10.48550/arXiv.2205.09357","CorpusId":248887419,"PubMed":"38986187"},"title":"Continual Pre-Training Mitigates Forgetting in Language and Vision"},{"paperId":"7cdaa08890895e1ad92afb5fad429690ad7b1dac","externalIds":{"DBLP":"conf/nips/LiuTMMHBR22","ArXiv":"2205.05638","DOI":"10.48550/arXiv.2205.05638","CorpusId":248693283},"title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"},{"paperId":"011c2b5a08c32e57f7c55539a1944bc733f83aa3","externalIds":{"DBLP":"journals/corr/abs-2205-02014","ArXiv":"2205.02014","ACL":"2022.acl-long.223","DOI":"10.48550/arXiv.2205.02014","CorpusId":248512744},"title":"On Continual Model Refinement in Out-of-Distribution Data Streams"},{"paperId":"a3ba7fdf789bcef381acd0d277a086428153bb9f","externalIds":{"ACL":"2022.emnlp-main.418","DBLP":"journals/corr/abs-2204-14211","ArXiv":"2204.14211","DOI":"10.48550/arXiv.2204.14211","CorpusId":248476156},"title":"TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models"},{"paperId":"06d7cb8c8816360feb33c3367073e0ef66d7d0b0","externalIds":{"ACL":"2022.emnlp-main.340","ArXiv":"2204.07705","DBLP":"conf/emnlp/WangMAKMNADASPK22","DOI":"10.18653/v1/2022.emnlp-main.340","CorpusId":253098274},"title":"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","externalIds":{"ArXiv":"2204.05660","DBLP":"conf/acl/MishraMVSCBK22","ACL":"2022.acl-long.246","DOI":"10.48550/arXiv.2204.05660","CorpusId":248118588},"title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"22274d28d90df56167d59f04d6f1106fe2f1c97a","externalIds":{"ArXiv":"2204.04799","DBLP":"journals/corr/abs-2204-04799","DOI":"10.48550/arXiv.2204.04799","CorpusId":248085201},"title":"DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","externalIds":{"DBLP":"journals/corr/abs-2203-15556","ArXiv":"2203.15556","CorpusId":247778764},"title":"Training Compute-Optimal Large Language Models"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"275b31fd4e12b24aa760cbdb2db2dd04f937258e","externalIds":{"ACL":"2022.acl-long.255","DBLP":"journals/corr/abs-2203-10652","ArXiv":"2203.10652","DOI":"10.48550/arXiv.2203.10652","CorpusId":247594532},"title":"Continual Sequence Generation with Adaptive Compositional Modules"},{"paperId":"0c8908707b4609bc53ea7a7c1d855088b7294dcf","externalIds":{"ArXiv":"2203.08512","DBLP":"journals/corr/abs-2203-08512","ACL":"2022.acl-long.218","DOI":"10.48550/arXiv.2203.08512","CorpusId":247476090},"title":"ConTinTin: Continual Learning from Task Instructions"},{"paperId":"4003193ef9fa5d408fcba8f9b1893b2be321283a","externalIds":{"DBLP":"conf/acl/ChalkidisP0TSS22","ArXiv":"2203.07228","ACL":"2022.acl-long.301","DOI":"10.48550/arXiv.2203.07228","CorpusId":247447641},"title":"FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing"},{"paperId":"bc7984bfcfae537dbe633eeeb8d69c42a994c724","externalIds":{"ArXiv":"2203.06311","ACL":"2022.findings-acl.220","DBLP":"conf/acl/QinZLL0SZ22","DOI":"10.48550/arXiv.2203.06311","CorpusId":247447415},"title":"ELLE: Efficient Lifelong Pre-training for Emerging Data"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"f4df78183261538e718066331898ee5cad7cad05","externalIds":{"DBLP":"journals/corr/abs-2202-12837","ArXiv":"2202.12837","ACL":"2022.emnlp-main.759","DOI":"10.18653/v1/2022.emnlp-main.759","CorpusId":247155069},"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","externalIds":{"DBLP":"conf/nips/MengBAB22","ArXiv":"2202.05262","CorpusId":255825985},"title":"Locating and Editing Factual Associations in GPT"},{"paperId":"39b5a3b516a060d8f7460e0fbedcce72f0190e20","externalIds":{"ACL":"2022.acl-demo.25","ArXiv":"2202.03829","DBLP":"journals/corr/abs-2202-03829","DOI":"10.18653/v1/2022.acl-demo.25","CorpusId":246652070},"title":"TimeLMs: Diachronic Language Models from Twitter"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"d6e389b27f7df2a55e51daca27293db8a9712b5a","externalIds":{"ArXiv":"2112.14146","DBLP":"journals/corr/abs-2112-14146","CorpusId":245537398},"title":"Towards continual task learning in artificial neural networks: current approaches and insights from neuroscience"},{"paperId":"b2d1fb4f78d24f03119f28a516ccabfc9591e71f","externalIds":{"DBLP":"journals/jmlr/MehtaPCS23","ArXiv":"2112.09153","CorpusId":245329773},"title":"An Empirical Investigation of the Role of Pre-training in Lifelong Learning"},{"paperId":"b526c3c450d9810ae8b037b4a87bf2a22ac48b38","externalIds":{"ArXiv":"2112.08654","DBLP":"journals/corr/abs-2112-08654","DOI":"10.1109/CVPR52688.2022.00024","CorpusId":245218925},"title":"Learning to Prompt for Continual Learning"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"62be538a25a6ca7ebf1b1ac9f338ad01fdc481fb","externalIds":{"DBLP":"journals/corr/abs-2112-02706","ArXiv":"2112.02706","CorpusId":244908578},"title":"Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning"},{"paperId":"4a247cbfca9dcf91e2da24e6d2d84601a9041a8f","externalIds":{"DBLP":"journals/corr/abs-2111-13654","ArXiv":"2111.13654","CorpusId":244709666},"title":"Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs"},{"paperId":"9286ac6e9b1aacd7d93496eb4615ae7678876d2a","externalIds":{"DBLP":"journals/corr/abs-2110-11309","ArXiv":"2110.11309","CorpusId":239050360},"title":"Fast Model Editing at Scale"},{"paperId":"74ee70e3ecbb2a7123a14de75bee7d3d8514f1cb","externalIds":{"DBLP":"journals/corr/abs-2110-11526","ArXiv":"2110.11526","CorpusId":239616391},"title":"Wide Neural Networks Forget Less Catastrophically"},{"paperId":"ed8931af08ce757a92a01ed43a0619522e10e8ff","externalIds":{"ACL":"2022.bigscience-1.1","ArXiv":"2110.08534","DBLP":"journals/corr/abs-2110-08534","CorpusId":239016173},"title":"Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora"},{"paperId":"fa133b4200729a57db96ae50aff8c4a5ff819f43","externalIds":{"DBLP":"journals/corr/abs-2110-07298","ArXiv":"2110.07298","CorpusId":238856821},"title":"LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5"},{"paperId":"c3855dd0cad419dba4ba8d3bb001c7599a7fce70","externalIds":{"DBLP":"conf/wsdm/RosinGR22","ArXiv":"2110.06366","DOI":"10.1145/3488560.3498529","CorpusId":238743808},"title":"Time Masking for Temporal Language Models"},{"paperId":"ce828f9986b196308a3e40b1de58af1e8e68d728","externalIds":{"DBLP":"journals/corr/abs-2110-03215","ArXiv":"2110.03215","CorpusId":238419458},"title":"Towards Continual Knowledge Learning of Language Models"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","externalIds":{"DBLP":"conf/emnlp/0034WJH21","ACL":"2021.emnlp-main.685","ArXiv":"2109.00859","DOI":"10.18653/v1/2021.emnlp-main.685","CorpusId":237386541},"title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"bf26ec8fac0bd3af67752f8b2282838120e65277","externalIds":{"DBLP":"journals/corr/abs-2108-09020","ArXiv":"2108.09020","DOI":"10.1109/ICCV48922.2021.00817","CorpusId":237259897},"title":"Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data"},{"paperId":"917c63f2186119166b3379f5d2816bb1a2f39b09","externalIds":{"ACL":"2022.naacl-main.407","DBLP":"journals/corr/abs-2108-05036","ArXiv":"2108.05036","DOI":"10.18653/v1/2022.naacl-main.407","CorpusId":236976189},"title":"DEMix Layers: Disentangling Domains for Modular Language Modeling"},{"paperId":"102ebe229df18c8733ea1b8def56cd79996e2178","externalIds":{"DBLP":"journals/fgcs/WuXSZM022","ArXiv":"2108.00941","DOI":"10.1016/j.future.2022.05.014","CorpusId":236772303},"title":"A Survey of Human-in-the-loop for Machine Learning"},{"paperId":"bea409e5e6312c2918bdf255b4f44a2ace313b9a","externalIds":{"ArXiv":"2107.12308","CorpusId":244400669},"title":"Revisiting Catastrophic Forgetting in Class Incremental Learning"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"ac8d33e4c0a45e227a47353f3f26fbb231482dc1","externalIds":{"ArXiv":"2106.15110","ACL":"2022.tacl-1.15","DBLP":"journals/tacl/DhingraCEGEC22","DOI":"10.1162/tacl_a_00459","CorpusId":235669861},"title":"Time-Aware Language Models as Temporal Knowledge Bases"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"0a41cc1e74e42c13204dec1fc27965d6e22020b8","externalIds":{"DBLP":"conf/kdd/HombaiahC0BN21","ArXiv":"2106.06297","DOI":"10.1145/3447548.3467162","CorpusId":235417543},"title":"Dynamic Language Models for Continuously Evolving Content"},{"paperId":"ff3d64942ee32c12816cd0d60d338d858edbc99f","externalIds":{"DBLP":"conf/iclr/RameshC22","ArXiv":"2106.03027","CorpusId":245007201},"title":"Model Zoo: A Growing Brain That Learns Continually"},{"paperId":"449735257c64564b9637ffa60ba55073e840f72e","externalIds":{"DBLP":"journals/corr/abs-2106-01499","ArXiv":"2106.01499","CorpusId":235313884},"title":"Personalizing Pre-trained Models"},{"paperId":"1e3e65e7773b7869d9bd7f5394b54199e48195e6","externalIds":{"ArXiv":"2105.03887","DBLP":"journals/aiopen/XiaoHLTS21","MAG":"3163842580","DOI":"10.1016/j.aiopen.2021.06.003","CorpusId":234342706},"title":"Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"240b0caabb415578bdea4da7d0a32bdff2e8163f","externalIds":{"DBLP":"journals/corr/abs-2104-08164","ArXiv":"2104.08164","ACL":"2021.emnlp-main.522","DOI":"10.18653/v1/2021.emnlp-main.522","CorpusId":233289412},"title":"Editing Factual Knowledge in Language Models"},{"paperId":"fcc67b627d800c610f26f620c2697cdfbdeb476b","externalIds":{"ArXiv":"2104.05025","DBLP":"journals/corr/abs-2203-03798","DOI":"10.48550/arXiv.2203.03798","CorpusId":247315339},"title":"New Insights on Reducing Abrupt Representation Change in Online Continual Learning"},{"paperId":"209fadc6f7fdbb92404ef69d2ee01df8e475d692","externalIds":{"DBLP":"conf/cvpr/BangKY0C21","ArXiv":"2103.17230","DOI":"10.1109/CVPR46437.2021.00812","CorpusId":232427874},"title":"Rainbow Memory: Continual Learning with a Memory of Diverse Samples"},{"paperId":"eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a","externalIds":{"ArXiv":"2103.08541","DBLP":"conf/naacl/SchusterFB21","MAG":"3170180819","ACL":"2021.naacl-main.52","DOI":"10.18653/V1/2021.NAACL-MAIN.52","CorpusId":232233599},"title":"Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"870ff1dde0c103c3d90be51880f984628e77a8d6","externalIds":{"DBLP":"conf/nips/LuGRHSBCDJTLZSZ21","ArXiv":"2102.04664","CorpusId":231855531},"title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"ba233d75aa403092bda0bffc026be7913673ad69","externalIds":{"DBLP":"conf/nips/LazaridouKGALTG21","ArXiv":"2102.01951","CorpusId":239886013},"title":"Mind the Gap: Assessing Temporal Generalization in Neural Language Models"},{"paperId":"7f092f1925bb094daa6d87992e224397e1daeab1","externalIds":{"ArXiv":"2101.10423","DBLP":"journals/corr/abs-2101-10423","DOI":"10.1016/j.neucom.2021.10.021","CorpusId":231709504},"title":"Online Continual Learning in Image Classification: An Empirical Survey"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","externalIds":{"DBLP":"journals/corr/abs-2101-00027","ArXiv":"2101.00027","CorpusId":230435736},"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"6eee69031d2e11aa03a5a8fcb219cff4562863be","externalIds":{"ACL":"2021.emnlp-main.436","DBLP":"conf/emnlp/Han0P21","ArXiv":"2012.15283","DOI":"10.18653/v1/2021.emnlp-main.436","CorpusId":237497382},"title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning"},{"paperId":"373bc164d7b552f8782988e7da6b0d00092a20b0","externalIds":{"ACL":"2020.coling-main.574","DBLP":"conf/coling/BiesialskaBC20","ArXiv":"2012.09823","MAG":"3112170794","DOI":"10.18653/v1/2020.coling-main.574","CorpusId":227231454},"title":"Continual Lifelong Learning in Natural Language Processing: A Survey"},{"paperId":"abaadb4c6affc4d874c4f59bfac60686e851cb5e","externalIds":{"DBLP":"journals/corr/abs-2011-07956","ArXiv":"2011.07956","MAG":"3101244625","CorpusId":226964491},"title":"Pre-training Text-to-Text Transformers for Concept-centric Common Sense"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"5baa3e00d66bc42db7e3908f0b70875cff9d0193","externalIds":{"MAG":"3098033339","ArXiv":"2008.11687","DBLP":"conf/nips/NeyshaburSZ20","CorpusId":221319407},"title":"What is being transferred in transfer learning?"},{"paperId":"65906e6027246ae9e4ecd18d6e019a24505c842e","externalIds":{"ArXiv":"2008.02275","MAG":"3047185145","DBLP":"journals/corr/abs-2008-02275","CorpusId":220968818},"title":"Aligning AI With Shared Human Values"},{"paperId":"3ad7dc8a109515c18ad8b0fe8ab9bb1bfc4599d4","externalIds":{"ArXiv":"2008.01411","DBLP":"journals/tnn/ZhaoWFWL22","MAG":"3047009786","DOI":"10.1109/TNNLS.2021.3072041","CorpusId":220961483,"PubMed":"33939615"},"title":"Memory-Efficient Class-Incremental Learning for Image Classification"},{"paperId":"a2f38d03fd363e920494ad65a5f0ad8bd18cd60b","externalIds":{"MAG":"3046375318","DBLP":"journals/corr/abs-2007-15779","ArXiv":"2007.15779","DOI":"10.1145/3458754","CorpusId":220919723},"title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"804a6d7c23335bbca6eec3b7d3c8366dcbe395a5","externalIds":{"DBLP":"journals/corr/abs-2008-02217","ArXiv":"2008.02217","MAG":"3047517563","CorpusId":220968978},"title":"Hopfield Networks is All You Need"},{"paperId":"7ea2a78a8d8a6327bd13aa4f2d9ace9231bd9662","externalIds":{"DBLP":"conf/cvpr/YuanTLWF20","MAG":"3034756453","DOI":"10.1109/CVPR42600.2020.00396","CorpusId":219962714},"title":"Revisiting Knowledge Distillation via Label Smoothing Regularization"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"f2f3c83db919a2429c4fcad2d0a0ed4e5294354a","externalIds":{"MAG":"3104215796","ArXiv":"2004.12651","ACL":"2020.emnlp-main.634","DBLP":"conf/emnlp/ChenHCCLY20","DOI":"10.18653/v1/2020.emnlp-main.634","CorpusId":216553067},"title":"Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","externalIds":{"DBLP":"conf/acl/GururanganMSLBD20","ArXiv":"2004.10964","MAG":"3017961061","ACL":"2020.acl-main.740","DOI":"10.18653/v1/2020.acl-main.740","CorpusId":216080466},"title":"Dont Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"e10b7cb072737b1fbdcba7948e573268b3573ae9","externalIds":{"DBLP":"conf/nips/BuzzegaBPAC20","ArXiv":"2004.07211","MAG":"3106522756","CorpusId":215768806},"title":"Dark Experience for General Continual Learning: a Strong, Simple Baseline"},{"paperId":"e165b379f983152874299e0f5a6e0c9596c9a3e8","externalIds":{"DBLP":"conf/iclr/SinitsinPPPB20","ArXiv":"2004.00345","MAG":"3014924543","CorpusId":213938729},"title":"Editable Neural Networks"},{"paperId":"89856db73f06315d6fa8444dc9e39b2f3cf07c52","externalIds":{"MAG":"3012735052","DBLP":"conf/eccv/EbrahimiMCDR20","ArXiv":"2003.09553","DOI":"10.1007/978-3-030-58621-8_23","CorpusId":214612169},"title":"Adversarial Continual Learning"},{"paperId":"4f03e69963b9649950ba29ae864a0de8c14f1f86","externalIds":{"DBLP":"conf/acl/WangTDWHJCJZ21","ArXiv":"2002.01808","MAG":"3005441132","ACL":"2021.findings-acl.121","DOI":"10.18653/v1/2021.findings-acl.121","CorpusId":211031933},"title":"K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"},{"paperId":"9cbc6deb1f957fe63d5c4b627a9cdefdafdba792","externalIds":{"DOI":"10.1201/9781351246743-20","CorpusId":242250193},"title":"Series"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6","externalIds":{"MAG":"3002330681","DBLP":"journals/corr/abs-2001-08435","ArXiv":"2001.08435","DOI":"10.5281/ZENODO.3608135","CorpusId":210868223},"title":"The Pushshift Reddit Dataset"},{"paperId":"8e58dc63817a2a26e5a2ddad38d8b1d19d1c3795","externalIds":{"ArXiv":"1912.03817","DBLP":"journals/corr/abs-1912-03817","DOI":"10.1109/SP40001.2021.00019","CorpusId":208909851},"title":"Machine Unlearning"},{"paperId":"04f4e55e14150b7c48b0287ba77c7443df76ed45","externalIds":{"DBLP":"conf/aaai/BiskZLGC20","MAG":"2998617917","ArXiv":"1911.11641","DOI":"10.1609/AAAI.V34I05.6239","CorpusId":208290939},"title":"PIQA: Reasoning about Physical Commonsense in Natural Language"},{"paperId":"41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76","externalIds":{"DBLP":"conf/emnlp/NiLM19","ACL":"D19-1018","MAG":"2971196067","DOI":"10.18653/v1/D19-1018","CorpusId":202621357},"title":"Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"},{"paperId":"c9b051b29feda7b62a4b683b1dfc37408724d8f5","externalIds":{"ArXiv":"1910.11473","DBLP":"conf/aaai/KhotCGJS20","MAG":"2996848635","DOI":"10.1609/AAAI.V34I05.6319","CorpusId":204915921},"title":"QASC: A Dataset for Question Answering via Sentence Composition"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"90e04f3ae23ca7df5f59b11453341e3db943b6f4","externalIds":{"DBLP":"journals/pami/LangeAMPJLST22","MAG":"3030364939","DOI":"10.1109/TPAMI.2021.3057446","CorpusId":218889912,"PubMed":"33544669"},"title":"A Continual Learning Survey: Defying Forgetting in Classification Tasks"},{"paperId":"81b4920ad488affaee27389ff9540b7fea90a4ce","externalIds":{"ACL":"D19-1332","MAG":"2971484364","ArXiv":"1909.03065","DBLP":"conf/emnlp/ZhouKNR19","DOI":"10.18653/v1/D19-1332","CorpusId":202541184},"title":"Going on a vacation takes longer than Going for a walk: A Study of Temporal Commonsense Understanding"},{"paperId":"8a1198282d3caff7bb8bbcf2ab48fae02dd8fdb2","externalIds":{"MAG":"2971524484","ArXiv":"1909.01520","DBLP":"journals/corr/abs-1909-01520","DOI":"10.1109/CVPRW50498.2020.00118","CorpusId":202539644},"title":"Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis"},{"paperId":"1097cf8cf5961589ff693b069002e7181e24e631","externalIds":{"DBLP":"conf/icdar/0001SSC19","MAG":"3004268082","DOI":"10.1109/ICDAR.2019.00156","CorpusId":209413409},"title":"OCR-VQA: Visual Question Answering by Reading Text in Images"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","externalIds":{"DBLP":"journals/corr/abs-1909-01066","MAG":"2996758945","ArXiv":"1909.01066","ACL":"D19-1250","DOI":"10.18653/v1/D19-1250","CorpusId":202539551},"title":"Language Models as Knowledge Bases?"},{"paperId":"66117f82def0c69a3b9cc77eb3e2694b0245ca86","externalIds":{"ArXiv":"1909.00277","DBLP":"journals/corr/abs-1909-00277","ACL":"D19-1243","MAG":"2971740165","DOI":"10.18653/v1/D19-1243","CorpusId":202540590},"title":"Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"},{"paperId":"7102bb3fe73bd057ff161d9db5214a267c1ef312","externalIds":{"MAG":"2970636124","ArXiv":"1908.10063","DBLP":"journals/corr/abs-1908-10063","CorpusId":201646244},"title":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"},{"paperId":"50dbda1cb9196f88914679fd34f4e44b2955434d","externalIds":{"ACL":"D19-5808","ArXiv":"1908.05852","DBLP":"conf/acl-mrqa/LinTCG19","MAG":"2987553933","DOI":"10.18653/v1/D19-5808","CorpusId":201058633},"title":"Reasoning Over Paragraph Effects in Situations"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","externalIds":{"ACL":"Q19-1026","MAG":"2912924812","DBLP":"journals/tacl/KwiatkowskiPRCP19","DOI":"10.1162/tacl_a_00276","CorpusId":86611921},"title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"d3cacb4806886eb2fe59c90d4b6f822c24ff1822","externalIds":{"ACL":"D19-1424","DBLP":"journals/corr/abs-1908-05620","MAG":"2971033911","ArXiv":"1908.05620","DOI":"10.18653/v1/D19-1424","CorpusId":199668646},"title":"Visualizing and Understanding the Effectiveness of BERT"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","externalIds":{"DBLP":"journals/corr/abs-1907-12412","ArXiv":"1907.12412","MAG":"2965210982","DOI":"10.1609/AAAI.V34I05.6428","CorpusId":198968327},"title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"45700f8163f13c1d512ac1853199d5df0733feb0","externalIds":{"MAG":"3019195211","ArXiv":"1907.03799","DBLP":"conf/cvpr/0001MP20","DOI":"10.1109/CVPRW50498.2020.00131","CorpusId":216036341},"title":"Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches"},{"paperId":"89ebbcf551a9ff67d99b3342e4553f913f45742d","externalIds":{"MAG":"2955321733","PubMedCentral":"6657653","DOI":"10.1016/j.cell.2019.06.012","CorpusId":195830225,"PubMed":"31280961"},"title":"Human Replay Spontaneously Reorganizes Experience"},{"paperId":"c16244f3090ec8bb1d74edf71991b87c9f0ca802","externalIds":{"DBLP":"conf/iclr/EbrahimiEDR20","MAG":"2948376147","ArXiv":"1906.02425","CorpusId":174802369},"title":"Uncertainty-guided Continual Learning with Bayesian Neural Networks"},{"paperId":"1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd","externalIds":{"DBLP":"conf/cvpr/WuCWYLGF19","MAG":"2954929116","ArXiv":"1905.13260","DOI":"10.1109/CVPR.2019.00046","CorpusId":173187918},"title":"Large Scale Incremental Learning"},{"paperId":"ad7129af0644dbcafa9aa2f111cb76526ea444a1","externalIds":{"MAG":"2971008823","DBLP":"conf/nips/ZellersHRBFRC19","ArXiv":"1905.12616","CorpusId":168169824},"title":"Defending Against Neural Fake News"},{"paperId":"af1f7739283bdbd2b7a94903041f6d6afd991907","externalIds":{"MAG":"2936135081","DBLP":"journals/corr/abs-1904-08920","ArXiv":"1904.08920","DOI":"10.1109/CVPR.2019.00851","CorpusId":85553602},"title":"Towards VQA Models That Can Read"},{"paperId":"a4bc4b98a917174ac2ab14bd5e66d64306079ab5","externalIds":{"DBLP":"journals/corr/abs-1904-02232","MAG":"2925618549","ArXiv":"1904.02232","ACL":"N19-1242","DOI":"10.18653/v1/N19-1242","CorpusId":102353837},"title":"BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","externalIds":{"MAG":"2919420119","DBLP":"conf/naacl/DuaWDSS019","ACL":"N19-1246","ArXiv":"1903.00161","DOI":"10.18653/v1/N19-1246","CorpusId":67855846},"title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"9c54962b0fd011d5fe3f5b5275cc6ba091a2c7ae","externalIds":{"MAG":"2947461406","CorpusId":173188188},"title":"On Tiny Episodic Memories in Continual Learning"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"4a954b3e72a61968ab235076bcc242aca3a05520","externalIds":{"MAG":"2952677972","DBLP":"journals/corr/abs-1812-00420","ArXiv":"1812.00420","CorpusId":54443381},"title":"Efficient Lifelong Learning with A-GEM"},{"paperId":"2b877889ac31b73d1ede70b00eb4c7118ef8eca2","externalIds":{"MAG":"2952093114","ArXiv":"1810.11910","DBLP":"journals/corr/abs-1810-11910","CorpusId":53100211},"title":"Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","externalIds":{"MAG":"2963323070","ACL":"P18-2124","ArXiv":"1806.03822","DBLP":"journals/corr/abs-1806-03822","DOI":"10.18653/v1/P18-2124","CorpusId":47018994},"title":"Know What You Dont Know: Unanswerable Questions for SQuAD"},{"paperId":"616b933539e1a56458c6cf0db390164fc2159541","externalIds":{"DOI":"10.2174/9781681086613118030004","CorpusId":261623231},"title":"A: Alignment"},{"paperId":"99ad0533f84c110da2d0713d5798e6e14080b159","externalIds":{"DBLP":"conf/naacl/KhashabiCRUR18","ACL":"N18-1023","MAG":"2804897457","DOI":"10.18653/v1/N18-1023","CorpusId":5112038},"title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"},{"paperId":"ae7619604821adce52c28daa2aed14f5a191d975","externalIds":{"MAG":"2804175194","DBLP":"journals/corr/abs-1805-06370","ArXiv":"1805.06370","CorpusId":21718339},"title":"Progress & Compress: A scalable framework for continual learning"},{"paperId":"11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25","externalIds":{"MAG":"2785611959","DBLP":"conf/lrec/ElSaharVRGHLS18","ACL":"L18-1544","CorpusId":4612975},"title":"T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"},{"paperId":"c5197d2dc844050edf5500ff59c3e8b1a98a74a2","externalIds":{"DBLP":"conf/nips/RitterBB18","MAG":"2803552920","ArXiv":"1805.07810","CorpusId":29169199},"title":"Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","externalIds":{"ArXiv":"1803.05457","DBLP":"journals/corr/abs-1803-05457","MAG":"2794325560","CorpusId":3922816},"title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"b1d24e8e08435b7c52335485a0d635abf9bc604c","externalIds":{"MAG":"2789566302","ACL":"N18-1074","ArXiv":"1803.05355","DBLP":"conf/naacl/ThorneVCM18","DOI":"10.18653/v1/N18-1074","CorpusId":4711425},"title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification"},{"paperId":"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c","externalIds":{"DBLP":"conf/cvpr/Gurari0SGLGLB18","MAG":"2788643321","ArXiv":"1802.08218","DOI":"10.1109/CVPR.2018.00380","CorpusId":3831582},"title":"VizWiz Grand Challenge: Answering Visual Questions from Blind People"},{"paperId":"5b2c83f41eacdf95e8b38300d2926ac37ea4709e","externalIds":{"MAG":"2950021130","DBLP":"conf/iclr/WuWGL18","ArXiv":"1804.01756","CorpusId":3524564},"title":"The Kanerva Machine: A Generative Distributed Memory"},{"paperId":"0be49527df4869a0132f5cbc8d4cfa3304ab5843","externalIds":{"MAG":"2786465559","DBLP":"journals/corr/abs-1802-10542","ArXiv":"1802.10542","CorpusId":3579956},"title":"Memory-based Parameter Adaptation"},{"paperId":"0b07dd216dee7696e5e670fcd755188c892d23c7","externalIds":{"MAG":"2783761400","PubMedCentral":"5847173","DOI":"10.1016/j.cub.2017.10.073","CorpusId":3868957,"PubMed":"29316421"},"title":"The Role of Hippocampal Replay in Memory and Planning"},{"paperId":"713b0d9005944f80af00addc81b162ca74ea4b14","externalIds":{"MAG":"2768412495","ArXiv":"1711.09601","DBLP":"journals/corr/abs-1711-09601","DOI":"10.1007/978-3-030-01219-9_9","CorpusId":4254748},"title":"Memory Aware Synapses: Learning what (not) to forget"},{"paperId":"872f3682a220b207658f2c695f37cbab8ee2330b","externalIds":{"MAG":"2946324023","CorpusId":198899615},"title":"An Exploratory Case"},{"paperId":"f5265e346382354887340c7b520d639162e2f598","externalIds":{"ACL":"W17-4508","DBLP":"conf/emnlp/VolskePSS17","MAG":"2760781482","DOI":"10.18653/v1/W17-4508","CorpusId":2204603},"title":"TL;DR: Mining Reddit to Learn Automatic Summarization"},{"paperId":"5b336170d3adaf5bbe5ef57235a5334ec3c6e782","externalIds":{"DBLP":"conf/conll/KhashabiKSR17","MAG":"2741412196","ACL":"K17-1010","DOI":"10.18653/v1/K17-1010","CorpusId":33698356},"title":"Learning What is Essential in Questions"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"fa025e5d117929361bcf798437957762eb5bb6d4","externalIds":{"DBLP":"conf/conll/LevySCZ17","MAG":"2962881743","ACL":"K17-1034","ArXiv":"1706.04115","DOI":"10.18653/v1/K17-1034","CorpusId":793385},"title":"Zero-Shot Relation Extraction via Reading Comprehension"},{"paperId":"118fae4b4d07453561f1eded88654f812c7c61ec","externalIds":{"DBLP":"conf/nips/Lopez-PazR17","MAG":"2734314755","ArXiv":"1706.08840","CorpusId":37308416},"title":"Gradient Episodic Memory for Continual Learning"},{"paperId":"636a79420d838eabe4af7fb25d6437de45ab64e8","externalIds":{"MAG":"2606964149","DBLP":"journals/corr/LaiXLYH17","ArXiv":"1704.04683","ACL":"D17-1082","DOI":"10.18653/v1/D17-1082","CorpusId":6826032},"title":"RACE: Large-scale ReAding Comprehension Dataset From Examinations"},{"paperId":"a99d857ecc78316a0d9a774972b775058d5644ca","externalIds":{"MAG":"2949268663","DBLP":"conf/icml/ZenkePG17","CorpusId":10409742,"PubMed":"31909397"},"title":"Continual Learning Through Synaptic Intelligence"},{"paperId":"510e26733aaff585d65701b9f1be7ca9d5afc586","externalIds":{"DBLP":"journals/corr/ShazeerMMDLHD17","MAG":"2952339051","ArXiv":"1701.06538","CorpusId":12462234},"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"2e55ba6c97ce5eb55abd959909403fe8da7e9fe9","externalIds":{"DBLP":"journals/corr/KirkpatrickPRVD16","MAG":"2560647685","ArXiv":"1612.00796","DOI":"10.1073/pnas.1611835114","CorpusId":4704285,"PubMed":"28292907"},"title":"Overcoming catastrophic forgetting in neural networks"},{"paperId":"1703631a938b397ba7e858161ce16448f6046d6f","externalIds":{"MAG":"2964189064","ArXiv":"1611.07725","DBLP":"conf/cvpr/RebuffiKSL17","DOI":"10.1109/CVPR.2017.587","CorpusId":206596260},"title":"iCaRL: Incremental Classifier and Representation Learning"},{"paperId":"66b8d34477cf1736f91fd22b27e37ce0b703c86e","externalIds":{"DBLP":"journals/corr/AljundiCT16","ArXiv":"1611.06194","MAG":"2950510923","DOI":"10.1109/CVPR.2017.753","CorpusId":914027},"title":"Expert Gate: Lifelong Learning with a Network of Experts"},{"paperId":"c3a159d6a85be68af4c985c281718bc5801ada19","externalIds":{"MAG":"2565989828","DBLP":"series/synthesis/2016Chen","DOI":"10.1007/978-3-031-01575-5","CorpusId":264145480},"title":"Lifelong Machine Learning"},{"paperId":"832fc9327695f7425d8759c6aaeec0fa2d7b0a90","externalIds":{"MAG":"2510759893","DBLP":"conf/acl/HewlettLJPFHKB16","ACL":"P16-1145","ArXiv":"1608.03542","DOI":"10.18653/v1/P16-1145","CorpusId":15870937},"title":"WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia"},{"paperId":"8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a","externalIds":{"DBLP":"journals/corr/LiH16e","MAG":"2949808626","ArXiv":"1606.09282","DOI":"10.1007/978-3-319-46493-0_37","CorpusId":4853851,"PubMed":"29990101"},"title":"Learning without Forgetting"},{"paperId":"5d83dd7bc824f76ec5d88cc05488331ae9359794","externalIds":{"MAG":"2434898965","DOI":"10.1126/science.aaf0941","CorpusId":794814,"PubMed":"27313047"},"title":"Organizing conceptual knowledge in humans with a gridlike code"},{"paperId":"53c9443e4e667170acc60ca1b31a0ec7151fe753","externalIds":{"ArXiv":"1606.04671","DBLP":"journals/corr/RusuRDSKKPH16","CorpusId":15350923},"title":"Progressive Neural Networks"},{"paperId":"6016495c5a5d4040773395350a49150ada14e1c1","externalIds":{"DBLP":"conf/lrec/BrummerDH16","ACL":"L16-1532","MAG":"2577240759","CorpusId":29658212},"title":"DBpedia Abstracts: A Large-Scale, Open, Multilingual NLP Training Corpus"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","externalIds":{"MAG":"2294501066","DBLP":"conf/www/HeM16","ArXiv":"1602.01585","DOI":"10.1145/2872427.2883037","CorpusId":1964279},"title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"e65142010431ffc089b272a1174214e00693e503","externalIds":{"MAG":"2963109634","ArXiv":"1511.02283","DBLP":"journals/corr/MaoHTCYM15","DOI":"10.1109/CVPR.2016.9","CorpusId":8745888},"title":"Generation and Comprehension of Unambiguous Object Descriptions"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","externalIds":{"DBLP":"journals/corr/ZhangZL15","MAG":"2963012544","ArXiv":"1509.01626","CorpusId":368182},"title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"1d5972b32a9b5a455a6eef389de5b7fca25771ad","externalIds":{"DBLP":"journals/corr/GaninUAGLLML15","MAG":"2998115938","ArXiv":"1505.07818","DOI":"10.1007/978-3-319-58347-1_10","CorpusId":2871880},"title":"Domain-Adversarial Training of Neural Networks"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","externalIds":{"ACL":"D14-1086","DBLP":"conf/emnlp/KazemzadehOMB14","MAG":"2251512949","DOI":"10.3115/v1/D14-1086","CorpusId":6308361},"title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc","externalIds":{"MAG":"2257408573","DBLP":"conf/wmt/BojarBFHKLMPPSS14","ACL":"W14-3302","DOI":"10.3115/v1/W14-3302","CorpusId":15535376},"title":"Findings of the 2014 Workshop on Statistical Machine Translation"},{"paperId":"5d833331b0e22ff359db05c62a8bca18c4f04b68","externalIds":{"ArXiv":"1312.3005","DBLP":"journals/corr/ChelbaMSGBK13","MAG":"2611669587","DOI":"10.21437/Interspeech.2014-564","CorpusId":14136307},"title":"One billion word benchmark for measuring progress in statistical language modeling"},{"paperId":"1c61f9ef06fe74505775a833ff849185757199e7","externalIds":{"MAG":"2113459411","ACL":"P11-1015","DBLP":"conf/acl/MaasDPHNP11","CorpusId":1428702},"title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde","externalIds":{"DBLP":"journals/ml/Ben-DavidBCKPV10","MAG":"2104094955","DOI":"10.1007/s10994-009-5152-4","CorpusId":8577357},"title":"A theory of learning from different domains"},{"paperId":"2dd77aea59408ef720b9accdd22478612d866352","externalIds":{"MAG":"1979019425","DOI":"10.1038/nature08577","CorpusId":4403208,"PubMed":"19946265"},"title":"Stably maintained dendritic spines are associated with lifelong memories"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"cfece5480969ecd19fc610608cdc1bbc853834c6","externalIds":{"MAG":"2068842497","DOI":"10.1016/S1053-8119(01)91925-1","CorpusId":1753586,"PubMed":"12507946"},"title":"Brain imaging of language plasticity in adopted adults: can a second language replace the first?"},{"paperId":"016e9cc85c658c6a69710b4c617609ad2a5d3a74","externalIds":{"DBLP":"conf/www/KwokEW01","MAG":"2055354597","DOI":"10.1145/371920.371973","CorpusId":5456456},"title":"Scaling question answering to the Web"},{"paperId":"2ebf18e7892e660a833152ddc6cf8f1d21a7b881","externalIds":{"MAG":"2047057213","DOI":"10.1037/0033-295X.102.3.419","CorpusId":2832081,"PubMed":"7624455"},"title":"Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory."},{"paperId":"3d6ae6e9b59a871fd9259836ac9b6b7628f697f2","externalIds":{"MAG":"2042959301","DOI":"10.1016/0165-0327(85)90062-x","CorpusId":57799138},"title":"Principles of Neural Science"},{"paperId":"73bcbf60b9ea369fdc396e532c2a9bd371fbeb4b","externalIds":{"DBLP":"conf/emnlp/WuVLH25","DOI":"10.18653/v1/2025.emnlp-tutorials.7","CorpusId":282902587},"title":"Continual Learning of Large Language Models"},{"paperId":"c2f1fbc5e829a8b667470bb4f61698f25210442b","externalIds":{"DBLP":"conf/iclr/0025L000WX24","CorpusId":271746234},"title":"CPPO: Continual Learning for Reinforcement Learning with Human Feedback"},{"paperId":"2f4bfc41704a090138afa4174c461896d5d9c98f","externalIds":{"DBLP":"conf/iclr/ChengHW24","DOI":"10.48550/arXiv.2309.09530","CorpusId":271745635},"title":"Adapting Large Language Models via Reading Comprehension"},{"paperId":"da3369c33ed94717166dba73dc0ebb5c9ddfe348","externalIds":{"DBLP":"journals/corr/abs-2404-08707","DOI":"10.48550/arXiv.2404.08707","CorpusId":285313618},"title":"Large Language Model Can Continue Evolving From Mistakes"},{"paperId":"135e3fe23727a7d7d2be27761554e387ac000d02","externalIds":{"DBLP":"conf/uai/GargDDSN23","CorpusId":260813955},"title":"In- or out-of-distribution detection via dual divergence estimation"},{"paperId":"6d37cc50131b1189a48906c46f4ee502158a9016","externalIds":{"ACL":"2023.swisstext-1.1","CorpusId":267035363},"title":"20 Minuten: A Multi-task News Summarisation Dataset for German"},{"paperId":"dcfc75c80b1c2038e452b5c4f2ae8ba2a146ea66","externalIds":{"DBLP":"conf/acl/ZhaoLC23","ACL":"2023.acl-long.747","DOI":"10.18653/v1/2023.acl-long.747","CorpusId":259370556},"title":"C-STANCE: A Large Dataset for Chinese Zero-Shot Stance Detection"},{"paperId":"85a5c0bb6660990c7b5e348c165c552c1d4a2c2a","externalIds":{"ACL":"2023.acl-long.703","DBLP":"conf/acl/MokDLTYY23","DOI":"10.18653/v1/2023.acl-long.703","CorpusId":259370786},"title":"Large-scale Lifelong Learning of In-context Instructions and How to Tackle It"},{"paperId":"b6a2f5f0c41ae933bebb6cd09bf5d065d7f675b0","externalIds":{"DBLP":"journals/corr/abs-2310-15694","DOI":"10.48550/arXiv.2310.15694","CorpusId":264439275},"title":"COPF: Continual Learning Human Preference through Optimal Policy Fitting"},{"paperId":"3e2e30dead442f92b49873215825f62bfcef2b2f","externalIds":{"DBLP":"conf/emnlp/SuLZZZ23","DOI":"10.18653/v1/2023.findings-emnlp.418","CorpusId":266177048},"title":"Efficient Continue Training of Temporal Language Model with Structural Information"},{"paperId":"4af8c3c6f1375ce8fa8c588edc108fe33d517699","externalIds":{"CorpusId":257834207},"title":"Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift"},{"paperId":"5142936bc579c92d2d6edaacb956a4031068849f","externalIds":{"DBLP":"journals/corr/abs-2311-09724","DOI":"10.48550/arXiv.2311.09724","CorpusId":283291177},"title":"Outcome-supervised Verifiers for Planning in Mathematical Reasoning"},{"paperId":"481072b40ab8a5f3a95001e37dcce39186e53eda","externalIds":{"DBLP":"conf/eccv/PourcelVF22","DOI":"10.1007/978-3-031-19806-9_42","CorpusId":253099780},"title":"Online Task-free Continual Learning with Dynamic Sparse Distributed Memory"},{"paperId":"8e125d392ea0d8240be654d90a28838711a5bd36","externalIds":{"DBLP":"conf/iclr/WuCLLQH22","CorpusId":247717213},"title":"Pretrained Language Model in Continual Learning: A Comparative Study"},{"paperId":"d2d3cee3342f7c0885a670c6bf53a509fdcd63b2","externalIds":{"DBLP":"conf/conll/YangLL22","ACL":"2022.conll-1.4","DOI":"10.18653/v1/2022.conll-1.4","CorpusId":256460954},"title":"Continual Learning for Natural Language Generations with Transformer Calibration"},{"paperId":"fe0825f9ddb1cccb545f4249da55b6b55e577bbd","externalIds":{"DBLP":"conf/iclr/SanhWRBSACSRDBX22","CorpusId":276421109},"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"291a00d8433fecd2dd10f7f13b62dae8ce500043","externalIds":{"ACL":"2021.eacl-main.95","DBLP":"conf/eacl/HeLCOLGP21","DOI":"10.18653/v1/2021.eacl-main.95","CorpusId":233189563},"title":"Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models"},{"paperId":"7b486a6eac4b46cf39e41c97b25ea22c5d27a883","externalIds":{"DBLP":"journals/corr/abs-2104-08773","CorpusId":233296635},"title":"Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"5c5751d45e298cea054f32b392c12c61027d2fe7","externalIds":{"MAG":"3015453090","DBLP":"conf/acl/LoWNKW20","ACL":"2020.acl-main.447","DOI":"10.18653/V1/2020.ACL-MAIN.447","CorpusId":215416146},"title":"S2ORC: The Semantic Scholar Open Research Corpus"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"92e121c6e114fe3cfb89370df03847c66a9b4e28","externalIds":{"CorpusId":199370376},"title":"An Adversarial Winograd Schema Challenge at Scale"},{"paperId":"3838387ea8dd1bb8c2306be5a63c1c120075c5a2","externalIds":{"ArXiv":"1908.05803","ACL":"D19-1606","DBLP":"conf/emnlp/DasigiLMSG19","MAG":"2968154628","DOI":"10.18653/v1/D19-1606","CorpusId":201058596},"title":"Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"9660ed967c2ce6e95c04eb506185ab18529ba90c","externalIds":{"MAG":"2623693039","CorpusId":67329015},"title":"Theoretical foundations of multi-task lifelong learning"},{"paperId":"d77d03fb7ae5572ad0d9e728ff05c73263852278","externalIds":{"CorpusId":60627476},"title":"& Computer"},{"paperId":"0116be8500276bef8f1ea7f191ebcaf57de84663","externalIds":{"DOI":"10.1201/9781420085938-c16","CorpusId":183016},"title":"Alignment"},{"paperId":"694b3c58712deefb59502847ba1b52b192c413e5","externalIds":{"ACL":"2005.mtsummit-papers.11","MAG":"29184969","DBLP":"conf/mtsummit/Koehn05","CorpusId":38407095},"title":"Europarl: A Parallel Corpus for Statistical Machine Translation"},{"paperId":"46fb0b392bc0dd3b54b3bfd67aed3f3d8c02be6f","externalIds":{"CorpusId":17056746},"title":"Notes of the ECML / MLnet Workshop on Empirical Learning of Natural Language Processing Tasks , April 26 , 1997 , Prague , Czech Republic Empirical Learning of Natural Language Processing Tasks"},{"paperId":"c213af6582c0d518a6e8e14217611c733eeb1ef1","externalIds":{"MAG":"140726337","DOI":"10.1016/S0079-7421(08)60536-8","CorpusId":61019113},"title":"Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"},{"paperId":"a213a4472f3926ccd2ca5d88ef0c468d6368eefc","externalIds":{"CorpusId":35621292,"PubMed":"5431839"},"title":"A foundation of language."},{"paperId":"58b7d478a8ced7fc8967ce21f4448b0f1a3f79b2","externalIds":{"CorpusId":271046247},"title":"on pre - training"}]}