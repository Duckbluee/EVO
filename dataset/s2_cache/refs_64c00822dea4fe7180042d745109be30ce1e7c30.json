{"references":[{"paperId":"9c3c5f74021d2ea62b1e85216faa68bffee2ba3c","externalIds":{"DBLP":"journals/eaai/ZengLCM24","DOI":"10.1016/j.engappai.2023.107335","CorpusId":264471196},"title":"A feature-based restoration dynamic interaction network for multimodal sentiment analysis"},{"paperId":"6c64ddd2190909de2c680dd18abc9b92e80c39f9","externalIds":{"ArXiv":"2312.17172","DBLP":"journals/corr/abs-2312-17172","DOI":"10.1109/CVPR52733.2024.02497","CorpusId":266573555},"title":"Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action"},{"paperId":"0c4f46e4dcae5527018e6432fb60cfe8c3354e97","externalIds":{"DBLP":"journals/corr/abs-2312-14125","ArXiv":"2312.14125","DOI":"10.48550/arXiv.2312.14125","CorpusId":266435847},"title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"c07d963604e6d1ee0eb7f948838afabadf40595a","externalIds":{"DBLP":"journals/corr/abs-2310-19812","ArXiv":"2310.19812","DOI":"10.48550/arXiv.2310.19812","CorpusId":264469587},"title":"Brain decoding: toward real-time reconstruction of visual perception"},{"paperId":"5d998aed5a1b5143d4e79806cdb614281989587b","externalIds":{"DBLP":"conf/wacv/XiaCOX24","ArXiv":"2310.02265","DOI":"10.1109/WACV57701.2024.00804","CorpusId":263608888},"title":"DREAM: Visual Decoding from REversing HumAn Visual SysteM"},{"paperId":"053f10cd2ba7fe9c508a9e63476a4f68ab1deb57","externalIds":{"ArXiv":"2309.14030","DBLP":"journals/corr/abs-2309-14030","DOI":"10.48550/arXiv.2309.14030","CorpusId":262466081},"title":"DeWave: Discrete EEG Waves Encoding for Brain Dynamics to Text Translation"},{"paperId":"c6229d32dee463d4518391b137c7455eff7c290e","externalIds":{"PubMedCentral":"10604156","DOI":"10.3390/bioengineering10101117","CorpusId":262593414,"PubMed":"37892847"},"title":"Dual-Guided Brain Diffusion Model: Natural Image Reconstruction from Human Visual Stimulus fMRI"},{"paperId":"b08f25092c219c5ec6fe0baa83e4073f30d278c0","externalIds":{"ArXiv":"2309.04509","DBLP":"journals/corr/abs-2309-04509","DOI":"10.1109/ICCV51070.2023.00719","CorpusId":261682368},"title":"The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion"},{"paperId":"4950668bccd35944f6f0d5b759813d6447cd3b9e","externalIds":{"DBLP":"journals/bspc/ZengXQHWK23","DOI":"10.1016/j.bspc.2023.105125","CorpusId":259779346},"title":"DM-RE2I: A framework based on diffusion model for the reconstruction from EEG to image"},{"paperId":"177885342c87c2365f2ffa6f662352a8347ea59e","externalIds":{"ArXiv":"2308.07428","DBLP":"journals/corr/abs-2308-07428","DOI":"10.48550/arXiv.2308.07428","CorpusId":260900333},"title":"UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity"},{"paperId":"6cba8e7bd71a531b7655bbdbfcaabccc3749bbda","externalIds":{"DBLP":"journals/pr/YeYZG24","DOI":"10.1016/j.patcog.2023.109915","CorpusId":261411295},"title":"Self-supervised cross-modal visual retrieval from brain activities"},{"paperId":"ccdfea446ff6223990f8b750e330162763cb4ec5","externalIds":{"PubMedCentral":"10427021","DOI":"10.1371/journal.pbio.3002176","CorpusId":260923013,"PubMed":"37582062"},"title":"Music can be reconstructed from human auditory cortex activity using nonlinear decoding models"},{"paperId":"ead2d3cdbf5fffeed104f4c6b48902a87a37fecf","externalIds":{"DBLP":"journals/corr/abs-2308-02510","ArXiv":"2308.02510","DOI":"10.48550/arXiv.2308.02510","CorpusId":260682870},"title":"Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals"},{"paperId":"ccba409e978c0aeb91aa5a6f9af16c122d53bda6","externalIds":{"PubMedCentral":"12770365","ArXiv":"2307.11078","DBLP":"journals/corr/abs-2307-11078","DOI":"10.1038/s41467-025-66731-7","CorpusId":259991343,"PubMed":"41318557"},"title":"Text-to-music generation models capture musical semantic representations in the human brain"},{"paperId":"ec95d893105707da8dce01ad28a1f5136aabfdd1","externalIds":{"DBLP":"conf/acl/XiZWL0023","ArXiv":"2307.05355","ACL":"2023.acl-long.741","DOI":"10.48550/arXiv.2307.05355","CorpusId":259370585},"title":"UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language"},{"paperId":"c2bd88a919994d7c6ac53d9a286104f02a396662","externalIds":{"DBLP":"journals/corr/abs-2306-16934","ArXiv":"2306.16934","DOI":"10.48550/arXiv.2306.16934","CorpusId":259286834},"title":"DreamDiffusion: Generating High-Quality Images from Brain EEG Signals"},{"paperId":"f53616ccf871a35fe6fa93648d31cbf37bc7b18d","externalIds":{"ArXiv":"2306.11629","DBLP":"journals/corr/abs-2306-11629","DOI":"10.48550/arXiv.2306.11629","CorpusId":259202442},"title":"Sound reconstruction from human brain activity via a generative model with brain-like auditory features"},{"paperId":"0eca6113caf845a982e1b89aedf2d5591748360e","externalIds":{"DBLP":"journals/corr/abs-2306-11536","ArXiv":"2306.11536","DOI":"10.48550/arXiv.2306.11536","CorpusId":259203479},"title":"Improving visual image reconstruction from human brain activity using latent diffusion models via multiple decoded inputs"},{"paperId":"4de8ee391a66fc6f075d339a3ab41ae8ca412a30","externalIds":{"DBLP":"conf/ijcnn/MengY23","DOI":"10.1109/IJCNN54540.2023.10191903","CorpusId":260386432},"title":"Semantics-guided hierarchical feature encoding generative adversarial network for natural image reconstruction from brain activities"},{"paperId":"4025bc5b68d638141d783555814f195af7c75aa6","externalIds":{"ArXiv":"2306.10082","DBLP":"journals/corr/abs-2306-10082","DOI":"10.48550/arXiv.2306.10082","CorpusId":259202535},"title":"DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding"},{"paperId":"5efcfc6476c5ddbd69fa1ec3ca598604ae376c1c","externalIds":{"DBLP":"conf/nips/ScottiBGSNCDVYW23","ArXiv":"2305.18274","CorpusId":258960607},"title":"Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors"},{"paperId":"e9fbd2894968ae31118188758b6fd4d95adb6f39","externalIds":{"DBLP":"journals/corr/abs-2305-17214","ArXiv":"2305.17214","DOI":"10.48550/arXiv.2305.17214","CorpusId":258960416},"title":"Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities"},{"paperId":"32d0ad59162b07bf469b6889930ef1d10e66f7f8","externalIds":{"ArXiv":"2305.12248","DBLP":"conf/nips/TangDVLH23","DOI":"10.48550/arXiv.2305.12248","CorpusId":258833330,"PubMed":"39015152"},"title":"Brain encoding models based on multimodal transformers can transfer across language and vision"},{"paperId":"12cbf907d40a5406ca855f51af54cc16d0b28cd6","externalIds":{"ArXiv":"2305.11675","DBLP":"journals/corr/abs-2305-11675","DOI":"10.48550/arXiv.2305.11675","CorpusId":258823377},"title":"Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity"},{"paperId":"a2098cd417464cca27f3cdd83e3af1d9236a5e3e","externalIds":{"ArXiv":"2305.11560","DBLP":"journals/corr/abs-2305-11560","DOI":"10.48550/arXiv.2305.11560","CorpusId":258823424},"title":"Brain Captioning: Decoding human brain activity into images and text"},{"paperId":"907023defd2d8c189192acd9d4c35628ac6ba868","externalIds":{"ArXiv":"2305.10135","DBLP":"conf/aaai/ZengLLGJTHL024","DOI":"10.48550/arXiv.2305.10135","CorpusId":258741345},"title":"Controllable Mind Visual Diffusion Model"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","externalIds":{"DBLP":"journals/corr/abs-2305-05665","ArXiv":"2305.05665","DOI":"10.1109/CVPR52729.2023.01457","CorpusId":258564264},"title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"10488612735fb6656fca5403dafa529baa371b44","externalIds":{"DBLP":"journals/titb/ShenZLZZQDZH23","DOI":"10.1109/JBHI.2023.3265805","CorpusId":258062260,"PubMed":"37037251"},"title":"Depression Recognition From EEG Signals Using an Adaptive Channel Fusion Method via Improved Focal Loss"},{"paperId":"09f2685cb1b32964dc6560a53bf966ef6d811754","externalIds":{"ArXiv":"2303.14730","CorpusId":259076476},"title":"Joint fMRI Decoding and Encoding with Latent Embedding Alignment"},{"paperId":"f81bda850d829460c33844dd01e775c23abdf9f3","externalIds":{"DBLP":"journals/corr/abs-2308-04249","ArXiv":"2308.04249","DOI":"10.1145/3581783.3613832","CorpusId":257757357},"title":"MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion"},{"paperId":"8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c","externalIds":{"ArXiv":"2303.12712","DBLP":"journals/corr/abs-2303-12712","CorpusId":257663729},"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"2824f18b3aeaae51b3fbe0d629c9b8a728da86d7","externalIds":{"DBLP":"conf/cvpr/TakagiN23","DOI":"10.1109/CVPR52729.2023.01389","CorpusId":253762952},"title":"High-resolution image reconstruction with latent diffusion models from human brain activity"},{"paperId":"1a450fcd40aeea64544fad08844de1119c33f03f","externalIds":{"PubMedCentral":"10511448","ArXiv":"2303.05334","DOI":"10.1038/s41598-023-42891-8","CorpusId":260439960,"PubMed":"37731047"},"title":"Natural scene reconstruction from fMRI signals using generative latent diffusion"},{"paperId":"f9464f298aa2b7519cb3265ea7ffd88481b08d3b","externalIds":{"PubMedCentral":"10038662","DOI":"10.7554/eLife.82580","CorpusId":251071904,"PubMed":"36847339"},"title":"THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior"},{"paperId":"019b5cb9f91114c3e0e4a3f6b85cc4b0ae706264","externalIds":{"DBLP":"journals/corr/abs-2302-12971","ArXiv":"2302.12971","CorpusId":257219216},"title":"BrainCLIP: Bridging Brain and Visual-Linguistic Representation via CLIP for Generic Natural Visual Stimulus Decoding from fMRI"},{"paperId":"399b893f484517d3f8afecba0c39a01ddb6cb20f","externalIds":{"DBLP":"journals/corr/abs-2302-10121","ArXiv":"2302.10121","DOI":"10.1109/ICASSP49357.2023.10096587","CorpusId":257038383},"title":"EEG2IMAGE: Image Reconstruction from EEG Brain Signals"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"fa0f3d8aa20e8987dbc7a516d5399cfa3dc97b1b","externalIds":{"ArXiv":"2301.12503","DBLP":"journals/corr/abs-2301-12503","DOI":"10.48550/arXiv.2301.12503","CorpusId":256390486},"title":"AudioLDM: Text-to-Audio Generation with Latent Diffusion Models"},{"paperId":"428854d9e75f94f0e61f37c6887c77800437d516","externalIds":{"ArXiv":"2301.11325","DBLP":"journals/corr/abs-2301-11325","DOI":"10.48550/arXiv.2301.11325","CorpusId":256274504},"title":"MusicLM: Generating Music From Text"},{"paperId":"6584892e7d0f0055ee9adabf03f36f2fa74319e5","externalIds":{"DBLP":"journals/corr/abs-2301-06267","ArXiv":"2301.06267","DOI":"10.1109/CVPR52729.2023.01852","CorpusId":255942320},"title":"Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models"},{"paperId":"e41f915d09f09ca685b8a0b3380d7d4b700e3d8e","externalIds":{"PubMedCentral":"9837107","DOI":"10.1038/s41598-022-27361-x","CorpusId":255774134,"PubMed":"36635340"},"title":"Neural decoding of music from the EEG"},{"paperId":"1371a2d24e67e9469e1607ad71c7b750f2db3a9a","externalIds":{"DBLP":"journals/nca/MishraSJB23","DOI":"10.1007/s00521-022-08178-1","CorpusId":255221648},"title":"NeuroGAN: image reconstruction from EEG signals via an attention-based GAN"},{"paperId":"20ef36ac2edf100af85e476e9f7d6da87dafb330","externalIds":{"ArXiv":"2212.06726","DBLP":"journals/corr/abs-2212-06726","DOI":"10.48550/arXiv.2212.06726","CorpusId":254591185},"title":"Semantic Brain Decoding: from fMRI to conceptually similar image reconstruction of visual stimuli"},{"paperId":"8e164661656007a5944c303e09bbc792d8700cb5","externalIds":{"DBLP":"journals/corr/abs-2212-02409","ArXiv":"2212.02409","DOI":"10.48550/arXiv.2212.02409","CorpusId":254246440},"title":"Decoding natural image stimuli from fMRI data with a surface-based convolutional network"},{"paperId":"97029b53d0252ea68472423dea33e5aa2316926d","externalIds":{"DBLP":"conf/iccv/XuWZWS23","ArXiv":"2211.08332","DOI":"10.1109/ICCV51070.2023.00713","CorpusId":253523371},"title":"Versatile Diffusion: Text, Images and Variations All in One Diffusion Model"},{"paperId":"c668b80885784a2c02b7837978ee95fefd108f1d","externalIds":{"DBLP":"journals/corr/abs-2211-06956","ArXiv":"2211.06956","DOI":"10.1109/CVPR52729.2023.02175","CorpusId":253510456},"title":"Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding"},{"paperId":"31ecd650d373dc5b92f0748937fd44154eb32ed7","externalIds":{"DOI":"10.1109/TNSRE.2022.3221962","CorpusId":253522143,"PubMed":"36374871"},"title":"Exploring the Intrinsic Features of EEG Signals via Empirical Mode Decomposition for Depression Recognition"},{"paperId":"cac69c272680c3607e584e0843c2774525cd3daf","externalIds":{"PubMedCentral":"9703977","DOI":"10.3389/fnins.2022.940972","CorpusId":253526485,"PubMed":"36452333"},"title":"Brain2Pix: Fully convolutional naturalistic video frame reconstruction from brain activity"},{"paperId":"fa7f0b902b4050bfca333b82a6834ce535bdfb1f","externalIds":{"DBLP":"journals/eswa/ZengLTCM23","DOI":"10.2139/ssrn.4083978","CorpusId":248205261},"title":"Heterogeneous graph convolution based on In-domain Self-supervision for Multimodal Sentiment Analysis"},{"paperId":"419eb47fea3931c4098232f44ccbc216275d3f56","externalIds":{"DBLP":"journals/pami/DuFLH23","ArXiv":"2210.06756","DOI":"10.1109/TPAMI.2023.3263181","CorpusId":252873127,"PubMed":"37030711"},"title":"Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features"},{"paperId":"7e2ca1df1e92e91c113daf056c0e00d1b22c5f25","externalIds":{"DBLP":"conf/nips/LinSS22","ArXiv":"2210.01769","DOI":"10.48550/arXiv.2210.01769","CorpusId":252693323},"title":"Mind Reader: Reconstructing complex images from brain activities"},{"paperId":"935e0765e93310ca9c64238953900fba56685ae7","externalIds":{"DOI":"10.1038/s41593-023-01304-9","CorpusId":252684880,"PubMed":"37127759"},"title":"Semantic reconstruction of continuous language from non-invasive brain recordings"},{"paperId":"df08b4b9a66dfa849dc8ac7c8fcf2900467f749e","externalIds":{"DBLP":"journals/corr/abs-2208-12415","ArXiv":"2208.12415","DOI":"10.48550/arXiv.2208.12415","CorpusId":251881345},"title":"MuLan: A Joint Embedding of Music Audio and Natural Language"},{"paperId":"e52479dc9f0f52f65222ff7ca86d9f59c2e6d12f","externalIds":{"DBLP":"journals/natmi/DefossezCRKK23","ArXiv":"2208.12266","DOI":"10.1038/s42256-023-00714-5","CorpusId":251881661},"title":"Decoding speech perception from non-invasive brain recordings"},{"paperId":"24c3205cfdc74db6fcf1cd0d20c00fbf966ea5ec","externalIds":{"ArXiv":"2206.03544","DBLP":"journals/corr/abs-2206-03544","DOI":"10.48550/arXiv.2206.03544","CorpusId":249461597},"title":"A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","externalIds":{"DBLP":"journals/corr/abs-2205-14100","ArXiv":"2205.14100","DOI":"10.48550/arXiv.2205.14100","CorpusId":249152323},"title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"1300966e1e0ef380645f1b7ce9f3e47737d88777","externalIds":{"DBLP":"journals/jvcir/MallikaUA22","DOI":"10.1016/j.jvcir.2022.103483","CorpusId":247399567},"title":"Neural Style Transfer for image within images and conditional GANs for destylization"},{"paperId":"f7b598b6e8cef20f14012ea999233ba324d6497d","externalIds":{"DBLP":"journals/corr/abs-2202-12692","ArXiv":"2202.12692","DOI":"10.1109/IJCNN55064.2022.9892673","CorpusId":247154884},"title":"Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs"},{"paperId":"7c597874535c1537d7ddff3b3723015b4dc79d30","externalIds":{"DBLP":"journals/corr/abs-2202-04200","ArXiv":"2202.04200","DOI":"10.1109/CVPR52688.2022.01103","CorpusId":246680316},"title":"MaskGIT: Masked Generative Image Transformer"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"597356d8b57fcc0e05aeeb0ad1a173456155fb5d","externalIds":{"DOI":"10.1093/cercor/bhab498","CorpusId":246286892,"PubMed":"35078227"},"title":"Reconstructing Rapid Natural Vision with fMRI-Conditional Video Generative Adversarial Network."},{"paperId":"35fad89170bf8c54f0ea89020c914733c42a8237","externalIds":{"DBLP":"journals/nca/KhareCAU22","DOI":"10.1007/s00521-021-06774-1","CorpusId":245809326},"title":"NeuroVision: perceived image regeneration using cProGAN"},{"paperId":"b5dd780af58334a4460031c8dd3be9b3b5388fe9","externalIds":{"DOI":"10.1109/TNSRE.2022.3140772","CorpusId":246363319,"PubMed":"34990366"},"title":"Enhanced System Robustness of Asynchronous BCI in Augmented Reality Using Steady-State Motion Visual Evoked Potential"},{"paperId":"790fcd618522a6045c733e3f985e0f0ca757a2ad","externalIds":{"DBLP":"journals/taffco/ShenZWDH22","MAG":"2967182021","DOI":"10.1109/TAFFC.2019.2934412","CorpusId":202288600},"title":"An Improved Empirical Mode Decomposition of Electroencephalogram Signals for Depression Detection"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"139e90776bde3e2657497d5bb759f2514da1b53f","externalIds":{"DOI":"10.1038/s41593-021-00962-x","CorpusId":245262002,"PubMed":"34916659"},"title":"A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence"},{"paperId":"6296aa7cab06eaf058f7291040b320b5a83c0091","externalIds":{"DBLP":"journals/corr/abs-2203-00667","ArXiv":"2203.00667","DOI":"10.1109/ICCCNT56998.2023.10306417","CorpusId":1033682},"title":"Generative Adversarial Networks"},{"paperId":"a407640e571457d8a519b0f253d48fa83be79076","externalIds":{"DBLP":"journals/cmpb/ZhangLLMWLWYZHC22","DOI":"10.1016/j.cmpb.2021.106586","CorpusId":245205987,"PubMed":"34963092"},"title":"A CNN-transformer hybrid approach for decoding visual neural activity into text"},{"paperId":"97bee918b08c244eb2e54d41e8ea6da00a3e5dbf","externalIds":{"DBLP":"conf/eccv/WuLJYFJD22","ArXiv":"2111.12417","DOI":"10.1007/978-3-031-19787-1_41","CorpusId":244527261},"title":"NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","externalIds":{"DBLP":"conf/cvpr/HeCXLDG22","ArXiv":"2111.06377","DOI":"10.1109/CVPR52688.2022.01553","CorpusId":243985980},"title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"05867d3b9f01ccadcd172714519aca22f213ae53","externalIds":{"DBLP":"journals/kbs/GuoLZWW23","ArXiv":"2110.06634","DOI":"10.1016/j.knosys.2023.110837","CorpusId":238744456},"title":"End-to-end translation of human neural activity to speech with a dual-dual generative adversarial network"},{"paperId":"fe7c66d256313498b447d1d67e76693e16a43cd1","externalIds":{"PubMedCentral":"8479122","DOI":"10.1038/s41597-021-01033-3","CorpusId":238218053,"PubMed":"34584100"},"title":"The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension"},{"paperId":"9bd1814af7c845e7bd46f77ac15d6bedf64e276f","externalIds":{"ArXiv":"2109.05070","DBLP":"conf/nips/CasanovaCVDR21","CorpusId":237491885},"title":"Instance-Conditioned GAN"},{"paperId":"426c9d639c11514df7ba6e96d2343695561ba0eb","externalIds":{"DBLP":"journals/corr/abs-2108-08827","ArXiv":"2108.08827","CorpusId":237213665},"title":"ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis"},{"paperId":"158196bdd8d7f9238420611b05c11a1151f8b9bf","externalIds":{"DBLP":"journals/nn/HuangYCWLWLLLZC21","DOI":"10.1016/j.neunet.2021.08.006","CorpusId":237410240,"PubMed":"34478941"},"title":"A neural decoding algorithm that generates language from visual activity evoked by natural images"},{"paperId":"ebe259796870ebccf26577044d0087884209b884","externalIds":{"DBLP":"conf/asru/ChungZHCQPW21","ArXiv":"2108.06209","DOI":"10.1109/ASRU51503.2021.9688253","CorpusId":237048255},"title":"w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training"},{"paperId":"59a0ef2d3bccebb544a2df4ad0453d49cc8e731f","externalIds":{"DBLP":"journals/taslp/ZeghidourLOST22","ArXiv":"2107.03312","DOI":"10.1109/taslp.2021.3129994","CorpusId":236149944},"title":"SoundStream: An End-to-End Neural Audio Codec"},{"paperId":"00bdfb9c03c7b9e6407d76ebfc6f894c0c8fb0f9","externalIds":{"ArXiv":"2106.05113","DBLP":"journals/corr/abs-2106-05113","CorpusId":235376762},"title":"More than meets the eye: Self-supervised depth reconstruction from brain activity"},{"paperId":"1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa","externalIds":{"DBLP":"conf/nips/DingYHZZYLZSYT21","ArXiv":"2105.13290","CorpusId":235212350},"title":"CogView: Mastering Text-to-Image Generation via Transformers"},{"paperId":"64ea8f180d0682e6c18d1eb688afdb2027c02794","externalIds":{"ArXiv":"2105.05233","DBLP":"journals/corr/abs-2105-05233","CorpusId":234357997},"title":"Diffusion Models Beat GANs on Image Synthesis"},{"paperId":"739ceacfafb1c4eaa17509351b647c773270b3ae","externalIds":{"DBLP":"journals/corr/abs-2104-02057","ArXiv":"2104.02057","MAG":"3145450063","DOI":"10.1109/ICCV48922.2021.00950","CorpusId":233024948},"title":"An Empirical Study of Training Self-Supervised Vision Transformers"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"82d23240c520e1470cded9d759fc665804f7f66f","externalIds":{"DBLP":"journals/corr/abs-2101-12083","ArXiv":"2101.12083","MAG":"3102059652","CorpusId":227275511},"title":"Reconstructing Perceptive Images from Brain Activity by Shape-Semantic GAN"},{"paperId":"1192660d960d44ae7416a3b7562e87c5f338d691","externalIds":{"DBLP":"journals/corr/abs-2101-08596","ArXiv":"2101.08596","CorpusId":231662084},"title":"LEAF: A Learnable Frontend for Audio Classification"},{"paperId":"fc002caccbeddcfb1850bae4124e34e3117b4e60","externalIds":{"DBLP":"journals/neuroimage/RenLXLYJG21","DOI":"10.1016/j.neuroimage.2020.117602","CorpusId":229935148,"PubMed":"33395572"},"title":"Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning"},{"paperId":"5de1e7b9aaa2a506286756b96f5d63f0e4f3e518","externalIds":{"DOI":"10.4000/hommesmigrations.12413","CorpusId":240846961},"title":"Rouge"},{"paperId":"81cc215e5bdbad3308b0b647d047d1c9fc3c8fc0","externalIds":{"DBLP":"journals/titb/WenLZCJ21","DOI":"10.1109/JBHI.2020.3047836","CorpusId":229723209,"PubMed":"33373308"},"title":"The Current Research of Combining Multi-Modal Brain-Computer Interfaces With Virtual Reality"},{"paperId":"ff87d553a73ab6604249c3f91347fbafb4874a9d","externalIds":{"DBLP":"journals/titb/ShenZHWGLDH21","DOI":"10.1109/JBHI.2020.3045718","CorpusId":229324415,"PubMed":"33338023"},"title":"An Optimal Channel Selection for EEG-Based Depression Detection via Kernel-Target Alignment"},{"paperId":"47f7ec3d0a5e6e83b6768ece35206a94dc81919c","externalIds":{"ArXiv":"2012.09841","MAG":"3111551570","DBLP":"journals/corr/abs-2012-09841","DOI":"10.1109/CVPR46437.2021.01268","CorpusId":229297973},"title":"Taming Transformers for High-Resolution Image Synthesis"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"3e577c9bdc82cb7fed337a74f90bbc4505fdfb69","externalIds":{"DBLP":"conf/iclr/Child21","ArXiv":"2011.10650","MAG":"3106570356","CorpusId":227127234},"title":"Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images"},{"paperId":"f37470d8c97aa84d4e2167d6fc0bef4460ee698a","externalIds":{"DBLP":"conf/icip/TakadaTOH20a","MAG":"3090157547","DOI":"10.1109/ICIP40778.2020.9191262","CorpusId":224908104},"title":"Generation of Viewed Image Captions From Human Brain Activity Via Unsupervised Text Latent Space"},{"paperId":"33b974b26a0665693e22c4d9f3dbf711b1aa8ab2","externalIds":{"DBLP":"journals/neuroimage/GazivBGHSGI22","MAG":"3083801059","PubMedCentral":"9133799","DOI":"10.1016/j.neuroimage.2022.119121","CorpusId":221589250,"PubMed":"35342004"},"title":"Self-supervised Natural Image Reconstruction and Large-scale Semantic Classification from Brain Activity"},{"paperId":"5a1e004817b6d459228851374777968d0ca7b4be","externalIds":{"MAG":"3040915549","PubMedCentral":"7502843","DOI":"10.1002/hbm.25136","CorpusId":220474419,"PubMed":"32648632"},"title":"Long short‐term memory‐based neural decoding of object categories evoked by natural images"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af","externalIds":{"DBLP":"conf/nips/CaronMMGBJ20","MAG":"3036224891","ArXiv":"2006.09882","CorpusId":219721240},"title":"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments"},{"paperId":"0321db266538e469af58a06da19c551963ae5542","externalIds":{"PubMedCentral":"7295758","MAG":"3034275316","DOI":"10.1038/s41597-020-0507-6","CorpusId":219638229,"PubMed":"32541806"},"title":"Neural and physiological data from participants listening to affective music"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"66831f683141c11ed7e20b0f2e8b40700740c164","externalIds":{"MAG":"3015371781","ArXiv":"2004.14368","DBLP":"journals/corr/abs-2004-14368","DOI":"10.1109/ICASSP40776.2020.9053174","CorpusId":216522760},"title":"Vggsound: A Large-Scale Audio-Visual Dataset"},{"paperId":"96c6bc5686e252c719d2adf208fa9def3401c0f2","externalIds":{"ArXiv":"2003.06105","MAG":"3045718925","DBLP":"journals/corr/abs-2003-06105","DOI":"10.1016/j.neuroscience.2020.07.040","CorpusId":212717764,"PubMed":"32736069"},"title":"BigGAN-based Bayesian Reconstruction of Natural Images from Human Brain Activity"},{"paperId":"82c11bded236d53e572a4c9a5b420075b5b6ad64","externalIds":{"DOI":"10.4135/9781412953948.n342","CorpusId":56610673},"title":"Pearson Correlation Coefficient"},{"paperId":"7416f85a4582d48d58e35ba218f73abe5fd76b10","externalIds":{"MAG":"3004371658","ArXiv":"2001.11761","DBLP":"journals/corr/abs-2001-11761","DOI":"10.1109/IJCNN48605.2020.9206960","CorpusId":211003765},"title":"Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN"},{"paperId":"14fdc18d9c164e5b0d6d946b3238c04e81921358","externalIds":{"MAG":"3035574324","DBLP":"conf/cvpr/KarrasLAHLA20","ArXiv":"1912.04958","DOI":"10.1109/cvpr42600.2020.00813","CorpusId":209202273},"title":"Analyzing and Improving the Image Quality of StyleGAN"},{"paperId":"a272c8975ea08dd3f57e480b7e5a214fa4e1667a","externalIds":{"DBLP":"reference/db/X09wi","DOI":"10.1007/s10670-020-00338-w","CorpusId":18793465},"title":"Generative Models"},{"paperId":"f29e00fed48c8e698ba8f64479999da298106f79","externalIds":{"MAG":"2985946514","DOI":"10.3390/app9224780","CorpusId":209314976},"title":"Image-To-Image Translation Using a Cross-Domain Auto-Encoder and Decoder"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","externalIds":{"MAG":"2982399380","ACL":"2020.acl-main.703","DBLP":"journals/corr/abs-1910-13461","ArXiv":"1910.13461","DOI":"10.18653/v1/2020.acl-main.703","CorpusId":204960716},"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"85381e0e89e98ea1ed8bd3dfa533d911263757df","externalIds":{"DBLP":"journals/corr/abs-1909-11646","MAG":"2996286887","ArXiv":"1909.11646","CorpusId":202749904},"title":"High Fidelity Speech Synthesis with Adversarial Networks"},{"paperId":"3f87942f09b445352156a4467587e987bbaa04ca","externalIds":{"DBLP":"journals/neuroimage/HanWSLZFL19","MAG":"2950953090","DOI":"10.1016/j.neuroimage.2019.05.039","CorpusId":260436433,"PubMed":"31103784"},"title":"Variational autoencoder: An unsupervised model for encoding and decoding fMRI activity in visual cortex"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","externalIds":{"DBLP":"journals/corr/abs-1908-10084","MAG":"2970641574","ArXiv":"1908.10084","ACL":"D19-1410","DOI":"10.18653/v1/D19-1410","CorpusId":201646309},"title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"50a6dd1d7e8dbddc42021dd996f52cb38436b8b5","externalIds":{"MAG":"2965806660","DBLP":"conf/ijcai/JiaoYYLZS19","DOI":"10.24963/ijcai.2019/192","CorpusId":199465800},"title":"Decoding EEG by Visual-guided Deep Neural Networks"},{"paperId":"965359b3008ab50dd04e171551220ec0e7f83aba","externalIds":{"MAG":"2971034910","ArXiv":"1907.05600","DBLP":"conf/nips/SongE19","CorpusId":196470871},"title":"Generative Modeling by Estimating Gradients of the Data Distribution"},{"paperId":"cde35c87aaabbc617d38f9cfaa2721a2e166d750","externalIds":{"DBLP":"journals/corr/abs-1907-02544","ArXiv":"1907.02544","MAG":"2970241862","CorpusId":195820291},"title":"Large Scale Adversarial Representation Learning"},{"paperId":"445f9cc4ef310b846739476be759672e9a9e72aa","externalIds":{"DBLP":"conf/nips/BeliyGHSGI19","MAG":"2971304514","ArXiv":"1907.02431","CorpusId":195798779},"title":"From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI"},{"paperId":"3dff4db4573000403ac4ab8b85dabc909f0d742b","externalIds":{"MAG":"2955323014","DOI":"10.1101/687681","CorpusId":198258649},"title":"A large single-participant fMRI dataset for probing brain responses to naturalistic stimuli in space and time"},{"paperId":"6be216d93421bf19c1659e7721241ae73d483baf","externalIds":{"MAG":"2947590261","DBLP":"journals/corr/abs-1906-00446","ArXiv":"1906.00446","CorpusId":173990382},"title":"Generating Diverse High-Fidelity Images with VQ-VAE-2"},{"paperId":"4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9","externalIds":{"DBLP":"conf/icml/TanL19","MAG":"2946948417","ArXiv":"1905.11946","CorpusId":167217261},"title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"3e271f66e696e68ada5ba1c41739c8d65426f71c","externalIds":{"MAG":"2940585064","PubMedCentral":"9714519","DOI":"10.1038/s41586-019-1119-1","CorpusId":91950725,"PubMed":"31019317"},"title":"Speech synthesis from neural decoding of spoken sentences"},{"paperId":"cf36635483035524773bed9e185a7bbadd05e30f","externalIds":{"MAG":"2913856666","PubMedCentral":"6793944","DOI":"10.1371/journal.pone.0223792","CorpusId":91263301,"PubMed":"31613926"},"title":"THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images"},{"paperId":"849a3f34b98b7a0a1ed7da0b029472b526c72169","externalIds":{"MAG":"2903567258","DOI":"10.1088/1741-2552/aaf594","CorpusId":54601092,"PubMed":"30523962"},"title":"Combination of high-frequency SSVEP-based BCI and computer vision for controlling a robotic arm"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"0232f39cf09a47982c24e311a7424f466f964b22","externalIds":{"ArXiv":"1810.10974","DBLP":"journals/pami/PalazzoSKGSS21","MAG":"2898488451","DOI":"10.1109/TPAMI.2020.2995909","CorpusId":53032353,"PubMed":"32750768"},"title":"Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features"},{"paperId":"4f17b0a6794c5cb241545ad316ad4d00a2d6c2b6","externalIds":{"MAG":"2897839226","DBLP":"conf/mm/TirupatturRSS18","DOI":"10.1145/3240508.3240641","CorpusId":52083141},"title":"ThoughtViz: Visualizing Human Thoughts Using Generative Adversarial Network"},{"paperId":"0cd7cd4d907a115d4832d4c60ec977dab75d2168","externalIds":{"DBLP":"journals/corr/abs-1810-03856","MAG":"2963510321","ArXiv":"1810.03856","PubMedCentral":"6529435","DOI":"10.1038/s42003-019-0438-y","CorpusId":52945847,"PubMed":"31925027"},"title":"Reconstructing faces from fMRI patterns using deep generative neural networks"},{"paperId":"e159036b37043b7ffe953cff152d0e8df3ce6e54","externalIds":{"MAG":"2943773313","ArXiv":"1809.01281","DBLP":"journals/corr/abs-1809-01281","PubMedCentral":"6502931","DOI":"10.1038/s41597-019-0052-3","CorpusId":146116346,"PubMed":"31061383"},"title":"BOLD5000, a public fMRI dataset while viewing 5000 visual images"},{"paperId":"c5b55f410365bb889c25a9f0354f2b86ec61c4f0","externalIds":{"MAG":"2963841322","ArXiv":"1808.06601","DBLP":"conf/nips/Wang0ZYTKC18","CorpusId":52049245},"title":"Video-to-Video Synthesis"},{"paperId":"dab6a458a6ba999607c08679464b6a930ff283b9","externalIds":{"MAG":"3104324110","DBLP":"journals/corr/abs-1805-06427","ArXiv":"1805.06427","DOI":"10.1088/1741-2552/aadea0","CorpusId":21716975,"PubMed":"30177583"},"title":"MOABB: trustworthy algorithm benchmarking for BCIs"},{"paperId":"b7a8473ba7e30d5e1835acbcb82544c064152894","externalIds":{"MAG":"2950997949","DBLP":"journals/corr/abs-1804-09535","ArXiv":"1804.09535","DOI":"10.1109/PCS.2018.8456308","CorpusId":13744000},"title":"Deep Convolutional AutoEncoder-based Lossy Image Compression"},{"paperId":"e7ec8a5444dab948339b89497d66c84ca8909fcf","externalIds":{"MAG":"2951287344","DBLP":"journals/ficn/ShenDMHK19","PubMedCentral":"6474395","DOI":"10.3389/fncom.2019.00021","CorpusId":91110332,"PubMed":"31031613"},"title":"End-to-End Deep Image Reconstruction From Human Brain Activity"},{"paperId":"2e251f561abbae06d3d2c3b2036b9cf83f0652ad","externalIds":{"MAG":"2757951733","DBLP":"journals/puc/KumarSRSD18","DOI":"10.1007/s00779-017-1083-4","CorpusId":19419340},"title":"Envisioned speech recognition using EEG sensors"},{"paperId":"6ef8407c9c8f9153f25d5339724c357191b47202","externalIds":{"ArXiv":"1802.02210","MAG":"2963019816","DBLP":"journals/corr/abs-1802-02210","DOI":"10.1109/SMC.2018.00107","CorpusId":3607215},"title":"Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli"},{"paperId":"29311a3c64af1f2d87e5f89ff62ad923fc50ee8a","externalIds":{"MAG":"2949290395","DBLP":"journals/ploscb/ShenHMK19","PubMedCentral":"6347330","DOI":"10.1101/240317","CorpusId":58667578,"PubMed":"30640910"},"title":"Deep image reconstruction from human brain activity"},{"paperId":"1dc9829d2a97be9e5a7da7eff513151acf4a46b8","externalIds":{"MAG":"2949869422","DBLP":"journals/neuroimage/SeeligerGAGG18","DOI":"10.1016/j.neuroimage.2018.07.043","CorpusId":51709940,"PubMed":"30031932"},"title":"Generative adversarial networks for reconstructing natural images from brain activity"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"744fe47157477235032f7bb3777800f9f2f45e52","externalIds":{"MAG":"2766527293","DBLP":"conf/iclr/KarrasALL18","ArXiv":"1710.10196","CorpusId":3568073},"title":"Progressive Growing of GANs for Improved Quality, Stability, and Variation"},{"paperId":"5e7136b89a3598d16e4d1b396f646abd4f6c50c2","externalIds":{"MAG":"2766524960","DBLP":"conf/mm/KavasidisPSGS17","DOI":"10.1145/3123266.3127907","CorpusId":7501660},"title":"Brain2Image: Converting Brain Signals into Images"},{"paperId":"c3c0fe8a70989166259a7b700701793192648cd3","externalIds":{"MAG":"2776446922","DBLP":"conf/iccv/PalazzoSKGS17","DOI":"10.1109/ICCV.2017.369","CorpusId":3151285},"title":"Generative Adversarial Networks Conditioned by Brain Signals"},{"paperId":"d59239df881e4839e4bdc3f9f61867c69bafcb0e","externalIds":{"MAG":"2753887715","PubMedCentral":"5577435","DOI":"10.1016/j.heliyon.2017.e00393","CorpusId":29928480,"PubMed":"28920094"},"title":"Convolutional auto-encoder for image denoising of ultra-low-dose CT"},{"paperId":"e76edb86f270c3a77ed9f5a1e1b305461f36f96f","externalIds":{"MAG":"2951910147","DBLP":"conf/cvpr/Tulyakov0YK18","ArXiv":"1707.04993","DOI":"10.1109/CVPR.2018.00165","CorpusId":4475365},"title":"MoCoGAN: Decomposing Motion and Content for Video Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"1dee041575f01b315387b14b863b5ce68cabc4fa","externalIds":{"MAG":"2601369343","DOI":"10.1016/j.tins.2017.02.004","CorpusId":3848509,"PubMed":"28314445"},"title":"Where Does EEG Come From and What Does It Mean?"},{"paperId":"5ba2218b708ca64ab556e39d5997202e012717d5","externalIds":{"MAG":"2593116425","DBLP":"conf/icassp/GemmekeEFJLMPR17","DOI":"10.1109/ICASSP.2017.7952261","CorpusId":21519176},"title":"Audio Set: An ontology and human-labeled dataset for audio events"},{"paperId":"062c41dad67bb68fefd9ff0c5c4d296e796004dc","externalIds":{"MAG":"2952493843","DBLP":"conf/iccv/SaitoMS17","ArXiv":"1611.06624","DOI":"10.1109/ICCV.2017.308","CorpusId":6945308},"title":"Temporal Generative Adversarial Nets with Singular Value Clipping"},{"paperId":"8acbe90d5b852dadea7810345451a99608ee54c7","externalIds":{"MAG":"2963073614","DBLP":"conf/cvpr/IsolaZZE17","ArXiv":"1611.07004","DOI":"10.1109/CVPR.2017.632","CorpusId":6200260},"title":"Image-to-Image Translation with Conditional Adversarial Networks"},{"paperId":"df0402517a7338ae28bc54acaac400de6b456a46","externalIds":{"MAG":"2519091744","ArXiv":"1609.03499","DBLP":"journals/corr/OordDZSVGKSK16","CorpusId":6254678},"title":"WaveNet: A Generative Model for Raw Audio"},{"paperId":"a70a909afc83e3caf7981682cdaed46e6669d81d","externalIds":{"ArXiv":"1609.00344","MAG":"2951332094","DBLP":"conf/cvpr/SpampinatoPKGSS17","DOI":"10.1109/CVPR.2017.479","CorpusId":5929952},"title":"Deep Learning Human Mind for Automated Visual Classification"},{"paperId":"c1b778f8a57c94a8933882aa28046bc38790b150","externalIds":{"MAG":"2507706845","ArXiv":"1608.03425","DOI":"10.1093/cercor/bhx268","CorpusId":3697403,"PubMed":"29059288"},"title":"Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision"},{"paperId":"8d783b56a3d17f95a9a0b2bf76f3ebf285e16ca8","externalIds":{"DBLP":"conf/acl/MatsuoKNNA16","ACL":"P16-3004","MAG":"2517609197","DOI":"10.18653/v1/P16-3004","CorpusId":8851964},"title":"Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity"},{"paperId":"88d98df703ed639fd832b6d65ff8821050762920","externalIds":{"MAG":"2343400697","DOI":"10.1146/annurev-neuro-070815-014045","CorpusId":38389630,"PubMed":"27145914"},"title":"Maps of the Auditory Cortex."},{"paperId":"0936352b78a52bc5d2b5e3f04233efc56664af51","externalIds":{"MAG":"2423557781","ArXiv":"1606.05328","DBLP":"journals/corr/OordKVEGK16","CorpusId":14989939},"title":"Conditional Image Generation with PixelCNN Decoders"},{"paperId":"eb7ee0bc355652654990bcf9f92f124688fde493","externalIds":{"MAG":"2963226019","DBLP":"conf/nips/ChenCDHSSA16","ArXiv":"1606.03657","CorpusId":5002792},"title":"InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"},{"paperId":"e1ce8d00729f9e61eeb315f3cbd7b5354706adbd","externalIds":{"ArXiv":"1605.09304","MAG":"2951774596","DBLP":"journals/corr/NguyenDYBC16","CorpusId":5284428},"title":"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks"},{"paperId":"41f1d50c85d3180476c4c7b3eea121278b0d8474","externalIds":{"MAG":"2953318193","ArXiv":"1601.06759","DBLP":"conf/icml/OordKK16","CorpusId":8142135},"title":"Pixel Recurrent Neural Networks"},{"paperId":"b06cc2f3b3cb1c0e0ad94cc512e9804112dc1d2f","externalIds":{"MAG":"2187545506","DOI":"10.12968/EYED.2016.17.9.30","CorpusId":73926590},"title":"Quality of assessment"},{"paperId":"e8b8a7778ace2a02f8db6fe321a54520c6b283ca","externalIds":{"DBLP":"conf/icml/LarsenSLW16","MAG":"2202109488","ArXiv":"1512.09300","CorpusId":8785311},"title":"Autoencoding beyond pixels using a learned similarity metric"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"3f25e17eb717e5894e0404ea634451332f85d287","externalIds":{"MAG":"2188365844","DBLP":"conf/nips/SohnLY15","CorpusId":13936837},"title":"Learning Structured Output Representation using Deep Conditional Generative Models"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","externalIds":{"MAG":"2183341477","DBLP":"conf/cvpr/SzegedyVISW16","ArXiv":"1512.00567","DOI":"10.1109/CVPR.2016.308","CorpusId":206593880},"title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"8388f1be26329fa45e5807e968a641ce170ea078","externalIds":{"MAG":"2949811265","ArXiv":"1511.06434","DBLP":"journals/corr/RadfordMC15","CorpusId":11758569},"title":"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"},{"paperId":"025dbb99945ca3c12365d3e635813045722c9e6c","externalIds":{"ArXiv":"1510.06479","PubMedCentral":"5458127","MAG":"2162058862","DBLP":"journals/corr/HorikawaK15","DOI":"10.1038/ncomms15037","CorpusId":2354567,"PubMed":"28530228"},"title":"Generic decoding of seen and imagined objects using hierarchical visual features"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"2dcef55a07f8607a819c21fe84131ea269cc2e3c","externalIds":{"MAG":"2129069237","DBLP":"journals/corr/Sohl-DicksteinW15","ArXiv":"1503.03585","CorpusId":14888175},"title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"},{"paperId":"750cc7813da3559dfd653cfbbf56ca3356b3162f","externalIds":{"MAG":"2124964692","DBLP":"conf/nips/XuRLJ14","CorpusId":7036324},"title":"Deep Convolutional Neural Network for Image Deconvolution"},{"paperId":"6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","externalIds":{"MAG":"1834627138","ArXiv":"1411.7766","DBLP":"journals/corr/LiuLWT14","DOI":"10.1109/ICCV.2015.425","CorpusId":459456},"title":"Deep Learning Face Attributes in the Wild"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","externalIds":{"MAG":"2949429431","ArXiv":"1409.1556","DBLP":"journals/corr/SimonyanZ14a","CorpusId":14124313},"title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","externalIds":{"MAG":"2950635152","DBLP":"conf/emnlp/ChoMGBBSB14","ACL":"D14-1179","ArXiv":"1406.1078","DOI":"10.3115/v1/D14-1179","CorpusId":5590763},"title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"2230aa1127b836c11b3b7acfe3dd2574adbceab5","externalIds":{"MAG":"2105400683","DOI":"10.1016/j.neuron.2013.10.017","CorpusId":16651046,"PubMed":"24314724"},"title":"EEG and MEG: Relevance to Neuroscience"},{"paperId":"19c6a93d714f5871b1a66cf2c71a71d611348c24","externalIds":{"MAG":"2072181746","DBLP":"journals/neuroimage/SchoenmakersBHG13","DOI":"10.1016/j.neuroimage.2013.07.043","CorpusId":1738661,"PubMed":"23886984"},"title":"Linear reconstruction of perceived images from human brain activity"},{"paperId":"7072ef929fa3d7c3177ca868309523c588a67bd1","externalIds":{"MAG":"2024729467","DBLP":"journals/neuroimage/EssenSBBYU13","DOI":"10.1016/j.neuroimage.2013.05.041","CorpusId":3207782,"PubMed":"23684880"},"title":"The WU-Minn Human Connectome Project: An overview"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"1c63c8bc5654a13575640b5c2a1b9725f02e028a","externalIds":{"MAG":"2126810579","DOI":"10.1016/j.cub.2011.08.031","CorpusId":10788863,"PubMed":"21945275"},"title":"Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies"},{"paperId":"001d66866a03a82550517c65094828b1cacba390","externalIds":{"MAG":"2127647230","DBLP":"journals/neuroimage/TremblayS11","DOI":"10.1016/j.neuroimage.2011.05.067","CorpusId":168627,"PubMed":"21664275"},"title":"On the context-dependent nature of the contribution of the ventral premotor cortex to speech perception"},{"paperId":"062cdcb52d52550b46c5d05f3083bfd7f494458b","externalIds":{"MAG":"2111143847","DBLP":"journals/neco/GervenLH10","DOI":"10.1162/NECO_a_00047","CorpusId":1707051,"PubMed":"20858128"},"title":"Neural Decoding with Hierarchical Generative Models"},{"paperId":"bd5b70e5d733bd5cd4aab4b1c1e88e38fa1a1895","externalIds":{"MAG":"1996954170","DOI":"10.1016/j.neuron.2010.08.017","CorpusId":182683,"PubMed":"20826306"},"title":"The Development of Human Functional Brain Networks"},{"paperId":"908091b4a8757c3b2f7d9cfa2c4f616ee12c5157","externalIds":{"MAG":"2017814585","DBLP":"conf/cvpr/XiaoHEOT10","DOI":"10.1109/CVPR.2010.5539970","CorpusId":1309931},"title":"SUN database: Large-scale scene recognition from abbey to zoo"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"f1c08d9d3856e0e0085bbb09f8748be44071616d","externalIds":{"MAG":"2097267381","DOI":"10.1016/j.neuron.2008.11.004","CorpusId":17327816,"PubMed":"19081384"},"title":"Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders"},{"paperId":"12ec492b6b2ad7c2cc47a5612e8d2a9b7fb35f91","externalIds":{"MAG":"2007226897","DOI":"10.1038/nature06713","CorpusId":54469209,"PubMed":"18322462"},"title":"Identifying natural images from human brain activity"},{"paperId":"c65a4fe64dc829801a47f3e8f806bbab0dcecf4e","externalIds":{"MAG":"1992007613","DOI":"10.1016/J.NEURON.2007.05.019","CorpusId":7662993,"PubMed":"17553419"},"title":"Brain states: top-down influences in sensory processing."},{"paperId":"eeff94ab4e21d3d3b84e73d765bbd1c38c0928bc","externalIds":{"DBLP":"journals/neuroimage/DegermanRPAJSA07","MAG":"2144934852","DOI":"10.1016/j.neuroimage.2006.11.019","CorpusId":17191508,"PubMed":"17204433"},"title":"Human brain activity associated with audiovisual perception and attention"},{"paperId":"d472c464d4e3f41d32bcf3981f573f74aa70037a","externalIds":{"MAG":"1965652053","DBLP":"journals/neuroimage/MeyerBJ06","DOI":"10.1016/j.neuroimage.2006.04.193","CorpusId":30054110,"PubMed":"16798014"},"title":"Electrical brain imaging reveals spatio-temporal dynamics of timbre perception in humans"},{"paperId":"48a2287e97b74a6f1986accf18795edcceb54511","externalIds":{"MAG":"2088809939","DOI":"10.1016/J.NEULET.2005.03.021","CorpusId":20442874,"PubMed":"15911143"},"title":"EEG-based neuroprosthesis control: a step towards clinical practice."},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d1c990fe6281865827364d6f8691027a4a5951b4","externalIds":{"MAG":"2153156486","DOI":"10.1146/ANNUREV.NEURO.27.070203.144220","CorpusId":6143322,"PubMed":"15217346"},"title":"The human visual cortex."},{"paperId":"a8171ca4a6fe976cd4e65bc8f0ffedb5c5cd5b48","externalIds":{"MAG":"2134888862","DOI":"10.1016/J.COGBRAINRES.2004.02.012","CorpusId":16868955,"PubMed":"15183394"},"title":"Brain areas underlying visual mental imagery and visual perception: an fMRI study."},{"paperId":"eae2e0fa72e898c289365c0af16daf57a7a6cf40","externalIds":{"MAG":"2133665775","DBLP":"journals/tip/WangBSS04","DOI":"10.1109/TIP.2003.819861","CorpusId":207761262,"PubMed":"15376593"},"title":"Image quality assessment: from error visibility to structural similarity"},{"paperId":"64225aff9e4fa03f1f6839c74c25a9444de4c7a7","externalIds":{"MAG":"2125038762","DOI":"10.1038/nn1077","CorpusId":7955526,"PubMed":"12819784"},"title":"Broca's area and the language instinct"},{"paperId":"99c4904813bc5e23d5bdcfc984adeba151a098da","externalIds":{"MAG":"1519623823","DOI":"10.1038/nrn1055","CorpusId":3203236,"PubMed":"12612634"},"title":"Cognitive neuroscience: Primary visual cortex and visual awareness"},{"paperId":"9437fa99472b0bc398903b60d445a414fdaa07b2","externalIds":{"DOI":"10.1136/bmj.326.7386.424/a","CorpusId":220116542},"title":"Reality"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"4db250554205e8ebe0425698297e29f6e588c2ce","externalIds":{"MAG":"2003739479","DOI":"10.1038/NRN730","CorpusId":7132655,"PubMed":"11836522"},"title":"What does fMRI tell us about neuronal activity?"},{"paperId":"6365c98fbfae5ec996b634831bff5ee3bf902a0d","externalIds":{"MAG":"2114104729","DOI":"10.1038/35084005","CorpusId":969175,"PubMed":"11449264"},"title":"Neurophysiological investigation of the basis of the fMRI signal"},{"paperId":"b950f939d1b436d81ec906755706485cb84e04e9","externalIds":{"MAG":"2019428019","DOI":"10.1006/NIMG.2000.0715","CorpusId":16472551,"PubMed":"11305897"},"title":"Human primary auditory cortex: cytoarchitectonic subdivisions and mapping into a spatial reference system."},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"24a15720ef1f6e931e0f95cb8bf4888107d0488b","externalIds":{"MAG":"2107860279","DOI":"10.1109/PACRIM.1993.407206","CorpusId":62230377},"title":"Mel-cepstral distance measure for objective speech quality assessment"},{"paperId":"04eca84abae88f7ea8afb2cc7dcd6c0185265f70","externalIds":{"MAG":"2084229232","DOI":"10.1016/0098-1354(92)80051-A","CorpusId":62207837},"title":"Autoassociative neural networks"},{"paperId":"302cb198e2d28cc8eeda4191f90b75418811c435","externalIds":{"DBLP":"journals/sigchi/MaskeryM92","MAG":"2058380797","DOI":"10.1145/142386.142389","CorpusId":15676557},"title":"Context"},{"paperId":"54aa0ce99461e7ff09a5255bdacea35fc2a76611","externalIds":{"DBLP":"conf/icml/MiliotouKHIB23","CorpusId":260927446},"title":"Generative Decoding of Visual Stimuli"},{"paperId":"1550ffa6cf4c6d82608eb1022d0cce1cd3ffe1b1","externalIds":{"DBLP":"conf/icml/0007Q023","CorpusId":260816435},"title":"Rethinking Visual Reconstruction: Experience-Based Content Completion Guided by Visual Cues"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"3d7b39ae5a68eb6bf693eeac28452eb2f42d4ebb","externalIds":{"DOI":"10.1007/978-3-319-24612-3_302606","CorpusId":241371228},"title":"Stimuli"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"b1786e74e233ac21f503f59d03f6af19a3699024","externalIds":{"CorpusId":209442203},"title":"A Better Autoencoder for Image: Convolutional Autoencoder"},{"paperId":"a007f29fe562eb8d9e4c77145f7e8650dfecea02","externalIds":{"MAG":"2735401819","DBLP":"conf/ijcnn/ChiuSWKL17","DOI":"10.1109/IJCNN.2017.7966228","CorpusId":12901276},"title":"A wireless steady state visually evoked potential-based BCI eating assistive system"},{"paperId":"e0822815433cc4d6b196ded6e398e60aa57e3fa4","externalIds":{"MAG":"2500979434","DOI":"10.1007/978-3-319-28603-7_30","CorpusId":78142250},"title":"BCI-Based Neuroprostheses and Physiotherapies for Stroke Motor Rehabilitation"},{"paperId":"48da11bda25c0372ebf3c5728494897e24b3c9a5","externalIds":{"MAG":"2111423163","DOI":"10.1146/ANNUREV.PSYCH.56.091103.070311","CorpusId":16634243,"PubMed":"15709929"},"title":"Models of brain function in neuroimaging."},{"paperId":"d4544713539c7dc664bf56db333ae217365ad4f0","externalIds":{"CorpusId":244900993},"title":"Data in Brief"}]}