{"abstract":"Unsupervised domain adaptation (UDA) aims to transfer knowledge from related but different labeled source domains to new unlabeled target domains. The existing UDA methods need to access the source domain data in the adaptation process. In some practical application scenarios, this assumption is no longer applicable due to the time-consuming data transmission and data privacy. In this paper, we are committed to solving the problem of source-free unsupervised domain adaptation (SFUDA). In this process, we only use the trained source domain model without accessing the source domain data. Many SFUDA methods use the pseudo labeling approach for selfsupervised learning. However, there is a lot of noise in the pseudo labels due to the domain shift, which will optimize the model in the wrong direction and affect the performance of the model in the target domain. In order to solve this problem, we propose a method for source-free unsupervised Domain Adaptation via denoising Mutual learning, dubbed as MutualDA. In self-training route, it is believed that the model can capture the distribution of target data in the training process, thus its output can be used as an effective supervision, but it is not advisable to use the output of the model to supervise itself, which will enlarge the error. Therefore, our method trains two symmetrical parallel models to supervise their learning with each otherâ€™s outputs, and the two models constantly transfer their learned knowledge to each other, which effectively reduces the impact of noise in pseudo labels and optimizes each model in a more correct direction. Experimental results show that compared with the existing UDA and SFUDA methods, our proposed method achieves the state-of-the-art performance on multiple datasets."}