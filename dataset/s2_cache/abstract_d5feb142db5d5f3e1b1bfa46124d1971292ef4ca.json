{"abstract":"Decoding information from bio-signals such as EEG, using machine learning has been a challenge due to the small data-sets and difﬁculty to obtain labels. We propose a reconstruction-based self-supervised learning model, the masked auto-encoder for EEG (MAEEG), for learning EEG representations by learning to reconstruct the masked EEG features using a transformer architecture. We found that MAEEG can learn representations that signiﬁcantly improve sleep stage classiﬁcation ( ∼ 5% accuracy increase) when only a small number of labels are given. We also found that input sample lengths and different ways of masking during reconstruction-based SSL pretraining have a huge effect on downstream model performance. Speciﬁcally, learning to reconstruct a larger proportion and more concentrated masked signal results in better performance on sleep classiﬁcation. Our ﬁndings provide insight into how reconstruction-based SSL could help representation learning for EEG. In study, we explore representation learning using reconstruction-based SSL on EEG data. We propose an SSL model, masked auto-encoder for learning EEG representations (MAEEG), which can learn EEG representations by reconstructing the raw signal from masked features. We found that MAEEG pretraining learns meaningful EEG representations, which yield better performance on sleep stage classiﬁcation. We further explore how masking may affect SSL and downstream task"}