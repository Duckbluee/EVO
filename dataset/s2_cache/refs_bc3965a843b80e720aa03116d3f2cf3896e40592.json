{"references":[{"paperId":"6bc92aa4bd2ed2fd5b558b33c20f389e2e56f7ae","externalIds":{"DBLP":"conf/cvpr/DelitzasTTSPE24","DOI":"10.1109/CVPR52733.2024.01377","CorpusId":269566806},"title":"SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes"},{"paperId":"a573ccae8e1da963883cdf22e91c2a1f0b1dc9fc","externalIds":{"DBLP":"journals/cgf/LeeSC24","ArXiv":"2403.13289","DOI":"10.1111/cgf.15061","CorpusId":268537142},"title":"Text‐to‐3D Shape Generation"},{"paperId":"57df118e953c65bc3e5d05cffd785bc349f7f897","externalIds":{"ArXiv":"2403.10997","DBLP":"conf/eccv/BhalgatLHZV24","DOI":"10.48550/arXiv.2403.10997","CorpusId":268512881},"title":"N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields"},{"paperId":"e788d5c6dc79f106296f8ad296bcbe512028d5f0","externalIds":{"DBLP":"journals/corr/abs-2403-09631","ArXiv":"2403.09631","DOI":"10.48550/arXiv.2403.09631","CorpusId":268385444},"title":"3D-VLA: A 3D Vision-Language-Action Generative World Model"},{"paperId":"ecd3091debcd2f393379508df70bceb94db0be3b","externalIds":{"ArXiv":"2403.01248","DBLP":"conf/icml/HuIJKYRSF24","DOI":"10.48550/arXiv.2403.01248","CorpusId":268230898},"title":"SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code"},{"paperId":"45f61c7c99e729c5666af436ad90c2c82c480375","externalIds":{"ArXiv":"2402.07207","DBLP":"journals/corr/abs-2402-07207","DOI":"10.48550/arXiv.2402.07207","CorpusId":267627441},"title":"GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting"},{"paperId":"a1f76db91c0debcf93ae9889736bce8470902113","externalIds":{"DBLP":"journals/corr/abs-2402-06196","ArXiv":"2402.06196","DOI":"10.48550/arXiv.2402.06196","CorpusId":267617032},"title":"Large Language Models: A Survey"},{"paperId":"b01e8f939c685dbf11cb38d3af7604ef1e77306f","externalIds":{"DBLP":"conf/eccv/JiaCYWNLLH24","ArXiv":"2401.09340","DOI":"10.48550/arXiv.2401.09340","CorpusId":267028244},"title":"SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding"},{"paperId":"fe07897aa2af4d25e1847577c6198e9bc72f2f5c","externalIds":{"DBLP":"journals/corr/abs-2401-08577","ArXiv":"2401.08577","DOI":"10.1109/CVPR52733.2024.02494","CorpusId":267028345},"title":"MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World"},{"paperId":"3f19484f941f209f45a51b1b69160e24e9b9dc99","externalIds":{"ArXiv":"2401.04092","DBLP":"conf/cvpr/WuYLZLGLW24","DOI":"10.1109/CVPR52733.2024.02098","CorpusId":266844463},"title":"GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation"},{"paperId":"d9263ba30a16067c44884ad559802505acf2f82d","externalIds":{"DBLP":"journals/corr/abs-2401-03201","ArXiv":"2401.03201","DOI":"10.1109/ICMEW63481.2024.10645462","CorpusId":266844828},"title":"3DMIT: 3D Multi-Modal Instruction Tuning for Scene Understanding"},{"paperId":"237964fa22828bc3e1124d868353770eb07ec245","externalIds":{"ArXiv":"2312.17247","DBLP":"journals/corr/abs-2312-17247","DOI":"10.1109/CVPR52733.2024.02645","CorpusId":266573192},"title":"Amodal Ground Truth and Completion in the Wild"},{"paperId":"42fb38cba6e078a1c5c2d98694dd55613ebca150","externalIds":{"DBLP":"conf/cvpr/Qin0ZWP24","ArXiv":"2312.16084","DOI":"10.1109/CVPR52733.2024.01895","CorpusId":266550750},"title":"LangSplat: 3D Language Gaussian Splatting"},{"paperId":"55d184e7bcf467f2824cd9603194e523b206f8e7","externalIds":{"ArXiv":"2312.16170","DBLP":"conf/cvpr/WangMZXLLCZCXLL24","DOI":"10.1109/CVPR52733.2024.01868","CorpusId":266551556},"title":"EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI"},{"paperId":"d23662d2275498f4fa7a937dabd60fbc7faf9c14","externalIds":{"ArXiv":"2312.16217","DBLP":"journals/corr/abs-2312-16217","DOI":"10.1109/CVPR52733.2024.01710","CorpusId":266573457},"title":"ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation"},{"paperId":"5edf706467dc76cd09319592d18db0ad4e1fb64d","externalIds":{"DBLP":"journals/corr/abs-2312-14074","ArXiv":"2312.14074","DOI":"10.48550/arXiv.2312.14074","CorpusId":266435483},"title":"LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding"},{"paperId":"59dab8002b49a9a516485d26ab43da1450f6273d","externalIds":{"DBLP":"journals/corr/abs-2312-10671","ArXiv":"2312.10671","DOI":"10.1109/CVPR52733.2024.00385","CorpusId":266348609},"title":"Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance"},{"paperId":"c2f1a101eb1c54cab321c47ce4f3b4b416f4786a","externalIds":{"DBLP":"journals/corr/abs-2312-10763","ArXiv":"2312.10763","DOI":"10.48550/arXiv.2312.10763","CorpusId":266359182},"title":"M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts"},{"paperId":"7f6cdece764b56262e910a801f8c133e6c27b7ab","externalIds":{"ArXiv":"2312.10807","CorpusId":266359732},"title":"Bridging Language and Action: A Survey of Language-Conditioned Robot Manipulation"},{"paperId":"95d791ad14db2c779daa67ca7fdc3a75214c42eb","externalIds":{"ArXiv":"2312.09738","DBLP":"journals/corr/abs-2312-09738","DOI":"10.48550/arXiv.2312.09738","CorpusId":266335471},"title":"3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V"},{"paperId":"1dbc2cdcae3e17c3d721d12a5a2d98ced727681a","externalIds":{"ArXiv":"2312.09067","DBLP":"conf/cvpr/YangSWVHH0HKLCY24","DOI":"10.1109/CVPR52733.2024.01536","CorpusId":266210109},"title":"Holodeck: Language Guided Generation of 3D Embodied AI Environments"},{"paperId":"a5ecfe7f961ab9a65aab033692dff71f5c70c2b6","externalIds":{"ArXiv":"2312.08168","DBLP":"conf/nips/HuangC0HXWLCZPZ24","DOI":"10.52202/079017-3620","CorpusId":266191146},"title":"Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers"},{"paperId":"a1fccc6bf1d25fa82b58d2b9ed41fbcee501e662","externalIds":{"ArXiv":"2312.07541","DBLP":"journals/corr/abs-2312-07541","DOI":"10.1145/3658193","CorpusId":266174016},"title":"SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration"},{"paperId":"8f62889dfa0efca9c83efc644e5ed4dcfb3540b0","externalIds":{"DBLP":"conf/cvpr/QiF0WW0LZ24","ArXiv":"2312.02980","DOI":"10.1109/CVPR52733.2024.02495","CorpusId":265659259},"title":"GPT4Point: A Unified Framework for Point-Language Understanding and Generation"},{"paperId":"574d3ea2e24c1857fb2bec0a4cec9a925800cf36","externalIds":{"DBLP":"journals/corr/abs-2312-03026","ArXiv":"2312.03026","DOI":"10.48550/arXiv.2312.03026","CorpusId":265689864},"title":"Uni3DL: Unified Model for 3D and Language Understanding"},{"paperId":"d8aca097ddc0062828948655d3a97e4204881582","externalIds":{"DBLP":"conf/cvpr/ZhengHZZ024","ArXiv":"2312.02010","DOI":"10.1109/CVPR52733.2024.01293","CorpusId":265608757},"title":"Towards Learning a Generalist Model for Embodied Navigation"},{"paperId":"fc53f8f3a84f1fc4993689d8f98cf6551d07a22d","externalIds":{"ArXiv":"2311.18651","DBLP":"journals/corr/abs-2311-18651","DOI":"10.1109/CVPR52733.2024.02496","CorpusId":265506642},"title":"LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning"},{"paperId":"11d30e827e473ece003d5bbc914d89d4a54313e1","externalIds":{"DBLP":"journals/corr/abs-2312-00093","ArXiv":"2312.00093","DOI":"10.1109/CVPR52733.2024.02012","CorpusId":265551966},"title":"GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs"},{"paperId":"a294b8632fed59e7079ef6187b0afa532c97ed7f","externalIds":{"ArXiv":"2312.00109","DBLP":"journals/corr/abs-2312-00109","DOI":"10.1109/CVPR52733.2024.01952","CorpusId":265551778},"title":"Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering"},{"paperId":"09157a8c0e7d7263ac035690118ddcbe295cee5c","externalIds":{"DBLP":"journals/tmm/YinCZJZLYC25","ArXiv":"2311.17618","DOI":"10.1109/TMM.2025.3535389","CorpusId":265498959},"title":"ShapeGPT: 3D Shape Generation With a Unified Multi-Modal Language Model"},{"paperId":"6c2b0947d537903a9d06ae83c5e5a4677060e3ff","externalIds":{"ArXiv":"2311.17737","DBLP":"conf/cvpr/LiD24a","DOI":"10.1109/CVPR52733.2024.01934","CorpusId":265498734},"title":"GenZI: Zero-Shot 3D Human-Scene Interaction Generation"},{"paperId":"302ecb79fe71af5058292d7c41dd4d67e0eb6363","externalIds":{"ArXiv":"2311.17907","DBLP":"journals/corr/abs-2311-17907","DOI":"10.48550/arXiv.2311.17907","CorpusId":265498994},"title":"CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting"},{"paperId":"13907a4fb2cec29589951738b96bc25ab5355834","externalIds":{"ArXiv":"2311.17261","DBLP":"conf/cvpr/ChenL0TN24","DOI":"10.1109/CVPR52733.2024.01992","CorpusId":265498870},"title":"SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors"},{"paperId":"01f949303303ec72cf74028295d548e49ebdc1f7","externalIds":{"ArXiv":"2311.16097","DBLP":"conf/cvpr/DillerD24","DOI":"10.1109/CVPR52733.2024.01880","CorpusId":265456241},"title":"CG-HOI: Contact-Guided 3D Human-Object Interaction Generation"},{"paperId":"e44a11954715bb36df0303e25783968e5f3bd610","externalIds":{"DBLP":"conf/cvpr/SiddiquiAATSRDN24","ArXiv":"2311.15475","DOI":"10.1109/CVPR52733.2024.01855","CorpusId":265457242},"title":"MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers"},{"paperId":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","externalIds":{"ArXiv":"2311.15383","DBLP":"conf/cvpr/YuanRFZC024","DOI":"10.1109/CVPR52733.2024.01949","CorpusId":265456115},"title":"Visual Programming for Zero-Shot Open-Vocabulary 3D Visual Grounding"},{"paperId":"9a898d3ebe03a6bd46176b721bc4bb839fe1cdcb","externalIds":{"ArXiv":"2311.13681","DBLP":"journals/corr/abs-2311-13681","DOI":"10.1109/CVPR52733.2024.02052","CorpusId":265445413},"title":"Compact 3D Gaussian Representation for Radiance Field"},{"paperId":"d3367dc9a7a1d7ae70a06eadc02b2430f4529f7c","externalIds":{"DBLP":"journals/corr/abs-2311-07226","ArXiv":"2311.07226","DOI":"10.48550/arXiv.2311.07226","CorpusId":265149884},"title":"Large Language Models for Robotics: A Survey"},{"paperId":"b4c998aea284dc1e74408b7e1ecd2f55bb21e7b0","externalIds":{"DBLP":"journals/corr/abs-2311-02873","ArXiv":"2311.02873","DOI":"10.48550/arXiv.2311.02873","CorpusId":262072783},"title":"OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data"},{"paperId":"88f4b8d1662473a6ad75e2e65aea41ea86b41765","externalIds":{"DBLP":"conf/cvpr/LiL0P24","ArXiv":"2310.17569","DOI":"10.1109/CVPR52733.2024.02602","CorpusId":264490858},"title":"SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching"},{"paperId":"588930cdd801f335b5e524d13f99aa94136a20a0","externalIds":{"DBLP":"journals/corr/abs-2310-12945","ArXiv":"2310.12945","DOI":"10.1109/3DV66043.2025.00119","CorpusId":264305584},"title":"3D-GPT: Procedural 3D Modeling with Large Language Models"},{"paperId":"4472418433cae23455987513a25ee1b7db25864d","externalIds":{"DBLP":"conf/iclr/BianBP024","ArXiv":"2310.07449","DOI":"10.48550/arXiv.2310.07449","CorpusId":263835173},"title":"PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction"},{"paperId":"2835b5dba741b4e48c2f578bbf7f8c6eed259832","externalIds":{"DBLP":"journals/corr/abs-2310-06773","ArXiv":"2310.06773","DOI":"10.48550/arXiv.2310.06773","CorpusId":263830324},"title":"Uni3D: Exploring Unified 3D Representation at Scale"},{"paperId":"894b2fe365642d350e0d688c33ba65124b1c2979","externalIds":{"DBLP":"journals/corr/abs-2310-05239","ArXiv":"2310.05239","DOI":"10.48550/arXiv.2310.05239","CorpusId":263831491},"title":"LAN-grasp: Using Large Language Models for Semantic Object Grasping"},{"paperId":"23cde1c9c8f4fe79e1ba5fc8f9c17bd1967f0a79","externalIds":{"DBLP":"conf/icra/YamazakiHVPTDNL24","ArXiv":"2310.03923","DOI":"10.1109/ICRA57147.2024.10610193","CorpusId":263829565},"title":"Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation"},{"paperId":"19933dd9e03058e686ef412262eef7696cce3e8f","externalIds":{"ArXiv":"2310.03026","DBLP":"journals/corr/abs-2310-03026","DOI":"10.48550/arXiv.2310.03026","CorpusId":263620279},"title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving"},{"paperId":"f01ff5acf9e086030c01beda6f433f99013ebbd4","externalIds":{"DBLP":"conf/icra/ChenSHKWBMS24","ArXiv":"2310.01957","DOI":"10.1109/ICRA57147.2024.10611018","CorpusId":263608168},"title":"Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"},{"paperId":"ccd6f8b6544f112de632e49bfbe592a0a654537d","externalIds":{"ArXiv":"2310.01412","DBLP":"journals/ral/XuZXZGWLZ24","DOI":"10.1109/LRA.2024.3440097","CorpusId":263605524},"title":"DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model"},{"paperId":"9db0247728950788a2b42097d81dc0e24eed6bb2","externalIds":{"DBLP":"conf/icra/YangC0MIFC24","ArXiv":"2309.12311","DOI":"10.1109/ICRA57147.2024.10610443","CorpusId":262084072},"title":"LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent"},{"paperId":"dca4ed6d4db18216796336d647f8d4bdf197f039","externalIds":{"DBLP":"journals/corr/abs-2309-07918","ArXiv":"2309.07918","DOI":"10.48550/arXiv.2309.07918","CorpusId":261822943},"title":"Unified Human-Scene Interaction via Prompted Chain-of-Contacts"},{"paperId":"20c0321dc08385d604a16a7ebec72b9c3ad6aabe","externalIds":{"ArXiv":"2309.07510","DBLP":"conf/nips/WuC0NZ023","DOI":"10.48550/arXiv.2309.07510","CorpusId":261822697},"title":"Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions"},{"paperId":"a34232bcb1ba2fa534fb6795c085f6d2ef9059d0","externalIds":{"DBLP":"conf/iccv/0019GC23","ArXiv":"2309.05251","DOI":"10.1109/ICCV51070.2023.01397","CorpusId":261681990},"title":"Multi3DRefer: Grounding Text Description to Multiple 3D Objects"},{"paperId":"710841e18a97f3e0f58fc9c3ab4595bcbb19d3ac","externalIds":{"DBLP":"journals/corr/abs-2309-04220","ArXiv":"2309.04220","DOI":"10.48550/arXiv.2309.04220","CorpusId":261660436},"title":"Score-PA: Score-based 3D Part Assembly"},{"paperId":"a6a7cda9eac019a91bcaccb6ef3b1c46630410f5","externalIds":{"DBLP":"conf/mm/HuangLH23","ArXiv":"2309.02224","DOI":"10.1145/3581783.3611902","CorpusId":261557394},"title":"Dense Object Grounding in 3D Scenes"},{"paperId":"09602838d1fdb1d56c053e93c00fd9603b9abf29","externalIds":{"DBLP":"journals/corr/abs-2309-00616","ArXiv":"2309.00616","DOI":"10.48550/arXiv.2309.00616","CorpusId":261494064},"title":"OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation"},{"paperId":"22ebfc211d184ed615729378a43fde175bf14478","externalIds":{"ArXiv":"2309.00615","DBLP":"journals/corr/abs-2309-00615","DOI":"10.48550/arXiv.2309.00615","CorpusId":261493787},"title":"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","externalIds":{"DBLP":"conf/eccv/XuWWCPL24","ArXiv":"2308.16911","DOI":"10.48550/arXiv.2308.16911","CorpusId":261397321},"title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"9aa01997226b5c4d705ae2e2f52c32681006654b","externalIds":{"DBLP":"conf/iclr/ShiWYMLY24","ArXiv":"2308.16512","DOI":"10.48550/arXiv.2308.16512","CorpusId":261395233},"title":"MVDream: Multi-view Diffusion for 3D Generation"},{"paperId":"82e29cc0a07e25998021c9f9af426cae11a62953","externalIds":{"DBLP":"conf/cvpr/TianACKG24","ArXiv":"2308.12469","DOI":"10.1109/CVPR52733.2024.00341","CorpusId":261101006},"title":"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","externalIds":{"ArXiv":"2308.10792","DBLP":"journals/corr/abs-2308-10792","DOI":"10.1145/3777411","CorpusId":261049152},"title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"64a80a33018a0fdc182b06111e32b2e08e186f6a","externalIds":{"ArXiv":"2308.04352","DBLP":"conf/iccv/ZhuMCD0023","DOI":"10.1109/ICCV51070.2023.00272","CorpusId":260704493},"title":"3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment"},{"paperId":"d44c82f534bb3f47d81d41084f870ef2e1efdbe9","externalIds":{"DBLP":"conf/mm/WangTJSZMZLZLJ23","ArXiv":"2308.02982","DOI":"10.1145/3581783.3611767","CorpusId":260680751},"title":"Beyond First Impressions: Integrating Joint Multi-modal Cues for Comprehensive 3D Representation"},{"paperId":"38939304bb760473141c2aca0305e44fbe04e6e8","externalIds":{"ArXiv":"2307.15818","DBLP":"conf/corl/ZitkovichYXXXXW23","DOI":"10.48550/arXiv.2307.15818","CorpusId":260293142},"title":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"},{"paperId":"2cc1d857e86d5152ba7fe6a8355c2a0150cc280a","externalIds":{"DBLP":"journals/tog/KerblKLD23","ArXiv":"2308.04079","DOI":"10.1145/3592433","CorpusId":259267917},"title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"11bca2cafe89e14dc733504f97e2489de697ceab","externalIds":{"ArXiv":"2307.07162","DBLP":"conf/wacv/FuLWDCSQ24","DOI":"10.1109/WACVW60836.2024.00102","CorpusId":259924488},"title":"Drive Like a Human: Rethinking Autonomous Driving with Large Language Models"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","externalIds":{"DBLP":"conf/corl/HuangWZL0023","ArXiv":"2307.05973","DOI":"10.48550/arXiv.2307.05973","CorpusId":259837330},"title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"3843469976bd895851bfa08c8208350745bf649f","externalIds":{"DBLP":"conf/corl/RanaHGA0S23","ArXiv":"2307.06135","DOI":"10.48550/arXiv.2307.06135","CorpusId":259837542},"title":"SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning"},{"paperId":"1a262a8eaf86f850b1b268dd40ce089ed88f339d","externalIds":{"ArXiv":"2306.13631","DBLP":"conf/nips/TakmazFSPTE23","DOI":"10.48550/arXiv.2306.13631","CorpusId":259243888},"title":"OpenMask3D: Open-Vocabulary 3D Instance Segmentation"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","externalIds":{"ArXiv":"2306.06687","DBLP":"conf/nips/YinWCSLLH0S0SO23","DOI":"10.48550/arXiv.2306.06687","CorpusId":259138958},"title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"acf1dfa02eefcab63124869f152fd36d2aad172a","externalIds":{"DBLP":"journals/corr/abs-2306-04933","ArXiv":"2306.04933","DOI":"10.48550/arXiv.2306.04933","CorpusId":259108694},"title":"InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding"},{"paperId":"42ea55edb46395469aee1b760829657e65ab6577","externalIds":{"DBLP":"conf/siggrapha/0002EOW23","ArXiv":"2306.03253","DOI":"10.1145/3610548.3618228","CorpusId":259089045},"title":"Zero-Shot 3D Shape Correspondence"},{"paperId":"8832a00e1ae2e74f9736046369e19e3d34ccae48","externalIds":{"DBLP":"conf/bmvc/DelitzasPHVABH23","ArXiv":"2306.02329","DOI":"10.48550/arXiv.2306.02329","CorpusId":259076122},"title":"Multi-CLIP: Contrastive Vision-Language Pre-training for Question Answering tasks in 3D Scenes"},{"paperId":"c5e9fd131cde68c218d0ea69cd617a67c7f35d42","externalIds":{"DBLP":"conf/nips/Wang00BL0023","ArXiv":"2305.16213","DOI":"10.48550/arXiv.2305.16213","CorpusId":258887357},"title":"ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation"},{"paperId":"ee156428803c5bd6e7372f6b27d74bcf88390db3","externalIds":{"ArXiv":"2305.14836","DBLP":"conf/aaai/QianCZJJ24","DOI":"10.48550/arXiv.2305.14836","CorpusId":258866014},"title":"NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario"},{"paperId":"266a8f8a8e1185ae543bf5551a4973877be506e7","externalIds":{"ArXiv":"2305.14093","DBLP":"conf/nips/LiuZZXYETXL23","CorpusId":258841162},"title":"Weakly Supervised 3D Open-vocabulary Segmentation"},{"paperId":"90971be23aa40fb78a7f2f5e1e5b2d9579a26964","externalIds":{"DBLP":"journals/corr/abs-2305-12427","ArXiv":"2305.12427","DOI":"10.48550/arXiv.2305.12427","CorpusId":258833066},"title":"VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representations"},{"paperId":"213a14d426acebf2f04709eea722a887a1f5f051","externalIds":{"DBLP":"journals/corr/abs-2305-11588","ArXiv":"2305.11588","DOI":"10.1109/TVCG.2024.3361502","CorpusId":258822803,"PubMed":"38315587"},"title":"Text2NeRF: Text-Driven 3D Scene Generation With Neural Radiance Fields"},{"paperId":"9a48f20ebf42e821b1e86409aa8b66aae3f79180","externalIds":{"DBLP":"journals/cvm/XuMY23","DOI":"10.1007/s41095-022-0321-5","CorpusId":258745841},"title":"A survey of deep learning-based 3D shape generation"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","externalIds":{"DBLP":"journals/corr/abs-2305-05665","ArXiv":"2305.05665","DOI":"10.1109/CVPR52729.2023.01457","CorpusId":258564264},"title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"af861b96ccdbae9b302f949c05ec2bfe8db13453","externalIds":{"DBLP":"conf/cvpr/YangDDW024","ArXiv":"2304.00962","DOI":"10.1109/CVPR52733.2024.01874","CorpusId":257913360},"title":"RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"836f0d803332853bb12a89495ea30f0e91c97bf6","externalIds":{"DBLP":"journals/corr/abs-2303-17606","ArXiv":"2303.17606","DOI":"10.1109/ICCV51070.2023.01322","CorpusId":257834153},"title":"AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control"},{"paperId":"0cbb518c364067200476a51e5ce7476a4f582770","externalIds":{"DBLP":"journals/corr/abs-2303-13873","ArXiv":"2303.13873","DOI":"10.1109/ICCV51070.2023.02033","CorpusId":257757213},"title":"Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation"},{"paperId":"d4a03a6e4920bf1ef6add32b58dfca63ae3643b9","externalIds":{"ArXiv":"2303.13186","DBLP":"journals/corr/abs-2303-13186","DOI":"10.48550/arXiv.2303.13186","CorpusId":257687268},"title":"ScanERU: Interactive 3D Visual Grounding based on Embodied Reference Understanding"},{"paperId":"fc64ff39e853791704f6b72eb824f4e86306fee1","externalIds":{"DBLP":"journals/corr/abs-2303-12218","ArXiv":"2303.12218","DOI":"10.1109/3DV62453.2024.00026","CorpusId":257663283},"title":"Compositional 3D Scene Generation using Locally Conditioned Diffusion"},{"paperId":"53974e68d7ea681349e9cd13babb5d511dc86460","externalIds":{"DBLP":"conf/iccv/ChenSLTN23","ArXiv":"2303.11396","DOI":"10.1109/ICCV51070.2023.01701","CorpusId":257636679},"title":"Text2Tex: Text-driven Texture Synthesis via Diffusion Models"},{"paperId":"efd20373f3d0a3d48c6ed6852aab5863f71733c2","externalIds":{"DBLP":"journals/corr/abs-2303-11327","ArXiv":"2303.11327","DOI":"10.1109/CVPR52729.2023.00888","CorpusId":257631929},"title":"3D Concept Learning and Reasoning from Multi-View Images"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0","externalIds":{"DBLP":"conf/eccv/LiuZRLZYJLYSZZ24","ArXiv":"2303.05499","DOI":"10.48550/arXiv.2303.05499","CorpusId":257427307},"title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"},{"paperId":"6b5b25212d92706b92d363681d8b0678e6e671d0","externalIds":{"DBLP":"conf/iccvw/0023DM23","ArXiv":"2303.04748","DOI":"10.1109/ICCVW60793.2023.00219","CorpusId":257404908},"title":"CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"06ee56a1e7030fa85b3e39897a2f4ef698b2faaf","externalIds":{"DBLP":"journals/tog/ReiserSVSMGBH23","ArXiv":"2302.12249","DOI":"10.1145/3592426","CorpusId":257102596},"title":"MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes"},{"paperId":"5e2bceb56f116e98baf7e418208057bc0e1c1861","externalIds":{"DBLP":"journals/corr/abs-2302-07241","ArXiv":"2302.07241","DOI":"10.48550/arXiv.2302.07241","CorpusId":256846496},"title":"ConceptFusion: Open-set Multimodal 3D Mapping"},{"paperId":"0c26bfc15a7caecce0ed4567dc2f2909b80e5bdd","externalIds":{"ArXiv":"2302.03668","DBLP":"journals/corr/abs-2302-03668","DOI":"10.48550/arXiv.2302.03668","CorpusId":256627601},"title":"Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery"},{"paperId":"5affc264027ec9fc95709c059613333b946cf271","externalIds":{"DBLP":"journals/corr/abs-2302-01721","ArXiv":"2302.01721","DOI":"10.1145/3588432.3591503","CorpusId":256597953},"title":"TEXTure: Text-Guided Texturing of 3D Shapes"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"d684fbe07585be651cc93d3c00ae3fe6df3ac877","externalIds":{"DBLP":"conf/icml/SingerSPAMKGVP023","ArXiv":"2301.11280","DOI":"10.48550/arXiv.2301.11280","CorpusId":256274791},"title":"Text-To-4D Dynamic Scene Generation"},{"paperId":"92d3f7cea95bba8cb905454324c3eeb84d2b6e58","externalIds":{"ArXiv":"2301.02508","DBLP":"journals/corr/abs-2301-02508","DOI":"10.1109/CVPR52729.2023.01070","CorpusId":255522451},"title":"End-to-End 3D Dense Captioning with Vote2Cap-DETR"},{"paperId":"9432bd77fa374d794bf2c8703b0e0e460380771f","externalIds":{"ArXiv":"2212.14704","DBLP":"conf/cvpr/XuW0CSQG23","DOI":"10.1109/CVPR52729.2023.02003","CorpusId":255340806},"title":"Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models"},{"paperId":"1b31dbf44e68b698120552366df03e6e35a1e428","externalIds":{"ArXiv":"2212.08051","DBLP":"conf/cvpr/DeitkeSSWMVSEKF23","DOI":"10.1109/CVPR52729.2023.01263","CorpusId":254685588},"title":"Objaverse: A Universe of Annotated 3D Objects"},{"paperId":"ecb00499e4d41861b269337b743d19e3f588ddcf","externalIds":{"ArXiv":"2212.05171","DBLP":"conf/cvpr/XueGXM0XXNS23","DOI":"10.1109/CVPR52729.2023.00120","CorpusId":254564501},"title":"ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding"},{"paperId":"781120c76e42cd13586b01311647815f5c0b0f52","externalIds":{"DBLP":"journals/corr/abs-2212-05231","ArXiv":"2212.05231","DOI":"10.1109/ICCV51070.2023.00305","CorpusId":254564276},"title":"NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction"},{"paperId":"8ee45aeb7c97e3346cc62f216f673b91277ac718","externalIds":{"DBLP":"conf/iccv/SongSWCW023","ArXiv":"2212.04088","DOI":"10.1109/ICCV51070.2023.00280","CorpusId":254408960},"title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"},{"paperId":"8abaaa14c7b2f089c081bf031cc4b144a60a6759","externalIds":{"ArXiv":"2212.00836","DBLP":"conf/iccv/ChenHCNC23","DOI":"10.1109/ICCV51070.2023.01660","CorpusId":254220973},"title":"UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding"},{"paperId":"64372cac2f962f183af418bc53570c133ff5ed23","externalIds":{"DBLP":"journals/corr/abs-2211-16312","ArXiv":"2211.16312","DOI":"10.1109/CVPR52729.2023.00677","CorpusId":254069374},"title":"PLA: Language-Driven Open-Vocabulary 3D Scene Understanding"},{"paperId":"774408d8848b129d93fb67548ec6571d99b31a2d","externalIds":{"DBLP":"conf/cvpr/PengGJTPF23","ArXiv":"2211.15654","DOI":"10.1109/CVPR52729.2023.00085","CorpusId":254044069},"title":"OpenScene: 3D Scene Understanding with Open Vocabularies"},{"paperId":"bdf4af8311637c681904e71cf50f96fd0026f578","externalIds":{"DBLP":"journals/corr/abs-2211-10440","ArXiv":"2211.10440","DOI":"10.1109/CVPR52729.2023.00037","CorpusId":253708074},"title":"Magic3D: High-Resolution Text-to-3D Content Creation"},{"paperId":"a2d2bbe4c542173662a444b33b76c66992697830","externalIds":{"DBLP":"conf/cvpr/BrooksHE23","ArXiv":"2211.09800","DOI":"10.1109/CVPR52729.2023.01764","CorpusId":253581213},"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"1b4af89592ed3d37beee3c2ff1af6c7c976af4b0","externalIds":{"DBLP":"conf/bmvc/ZhanXZ22","ArXiv":"2210.10046","DOI":"10.48550/arXiv.2210.10046","CorpusId":252967703},"title":"A Tri-Layer Plugin to Improve Occluded Detection"},{"paperId":"c1a4fb211cf995b1af1247a152fcc145594ec13b","externalIds":{"ArXiv":"2210.07474","DBLP":"journals/corr/abs-2210-07474","DOI":"10.48550/arXiv.2210.07474","CorpusId":252907411},"title":"SQA3D: Situated Question Answering in 3D Scenes"},{"paperId":"f1c8b5184d24d0f8ee17adb8d7c0eccc2b1734b9","externalIds":{"DBLP":"journals/corr/abs-2210-05633","ArXiv":"2210.05633","DOI":"10.1109/CVPR52729.2023.00477","CorpusId":252815804},"title":"Habitat-Matterport 3D Semantics Dataset"},{"paperId":"29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76","externalIds":{"DBLP":"journals/corr/abs-2210-04150","ArXiv":"2210.04150","DOI":"10.1109/CVPR52729.2023.00682","CorpusId":252780581},"title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","externalIds":{"DBLP":"journals/corr/abs-2210-03493","ArXiv":"2210.03493","CorpusId":252762275},"title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"4c94d04afa4309ec2f06bdd0fe3781f91461b362","externalIds":{"DBLP":"conf/iclr/PooleJBM23","ArXiv":"2209.14988","DOI":"10.48550/arXiv.2209.14988","CorpusId":252596091},"title":"DreamFusion: Text-to-3D using 2D Diffusion"},{"paperId":"d411589aa18055af09b4bdea67672f7ca74d08a3","externalIds":{"DBLP":"journals/corr/abs-2209-07896","ArXiv":"2209.07896","DOI":"10.1109/ICRA48891.2023.10161212","CorpusId":252355114},"title":"3D VSG: Long-term Semantic Scene Change Prediction through 3D Variable Scene Graphs"},{"paperId":"5f5b855f58da599c2730120e55d8320fac867279","externalIds":{"DBLP":"journals/corr/abs-2209-05629","ArXiv":"2209.05629","DOI":"10.48550/arXiv.2209.05629","CorpusId":252212141},"title":"Leveraging Large Language Models for Robot 3D Scene Understanding"},{"paperId":"36ee474d55d08465f9df76d290bf5a190d74db65","externalIds":{"DBLP":"journals/corr/abs-2208-02816","ArXiv":"2208.02816","DOI":"10.48550/arXiv.2208.02816","CorpusId":251320177},"title":"Expanding Language-Image Pretrained Models for General Video Recognition"},{"paperId":"04e541391e8dce14d099d00fb2c21dbbd8afe87f","externalIds":{"DBLP":"journals/corr/abs-2208-01626","ArXiv":"2208.01626","DOI":"10.48550/arXiv.2208.01626","CorpusId":251252882},"title":"Prompt-to-Prompt Image Editing with Cross Attention Control"},{"paperId":"af9f365ed86614c800f082bd8eb14be76072ad16","externalIds":{"DBLP":"journals/corr/abs-2207-12598","ArXiv":"2207.12598","DOI":"10.48550/arXiv.2207.12598","CorpusId":249145348},"title":"Classifier-Free Diffusion Guidance"},{"paperId":"d5c4550d285b57111e52c5956dbc40942d36b117","externalIds":{"DBLP":"conf/corl/HaS22","ArXiv":"2207.11514","DOI":"10.48550/arXiv.2207.11514","CorpusId":251040433},"title":"Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models"},{"paperId":"d3461268e1153b1abec8f999f6375378a33e0061","externalIds":{"DBLP":"conf/nips/Rasheed0K0K22","ArXiv":"2207.03482","DOI":"10.48550/arXiv.2207.03482","CorpusId":250334431},"title":"Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection"},{"paperId":"05f31f8d11629bec66e99b72862da48bbcb03ff7","externalIds":{"DBLP":"conf/nips/SchwarzSNL022","ArXiv":"2206.07695","DOI":"10.48550/arXiv.2206.07695","CorpusId":249674609},"title":"VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"6115a34ad9dd868fe886df815071023fd432e7a1","externalIds":{"ArXiv":"2205.15585","DBLP":"journals/corr/abs-2205-15585","DOI":"10.48550/arXiv.2205.15585","CorpusId":249209811},"title":"Decomposing NeRF for Editing via Feature Field Distillation"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","externalIds":{"DBLP":"journals/corr/abs-2205-11916","ArXiv":"2205.11916","CorpusId":249017743},"title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"9dae204dad41633188022002a04c8aa67c79a4e1","externalIds":{"DBLP":"journals/corr/abs-2205-06230","ArXiv":"2205.06230","DOI":"10.48550/arXiv.2205.06230","CorpusId":248721818},"title":"Simple Open-Vocabulary Object Detection with Vision Transformers"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"b3dd7c78ae3960474541f01d9a9ce7b5de65c110","externalIds":{"DBLP":"conf/eccv/RozenberszkiLD22","ArXiv":"2204.07761","DOI":"10.48550/arXiv.2204.07761","CorpusId":248227627},"title":"Language-Grounded Indoor 3D Semantic Segmentation in the Wild"},{"paperId":"6f2d01658b8ff1f68d07536775465eb8874f2693","externalIds":{"DBLP":"conf/eccv/ChngRSL22","ArXiv":"2204.05735","DOI":"10.1007/978-3-031-19827-4_16","CorpusId":248118559},"title":"Gaussian Activated Neural Radiance Fields for High Fidelity Reconstruction and Pose Estimation"},{"paperId":"5e76879aaea118b532fb24a50b721076d4c6ae93","externalIds":{"DBLP":"journals/corr/abs-2204-03610","ArXiv":"2204.03610","DOI":"10.1109/CVPR52688.2022.01857","CorpusId":248006101},"title":"Unified Contrastive Learning in Image-Text-Label Space"},{"paperId":"1d324246033499fe47c3464a1fcd969e7520758e","externalIds":{"DBLP":"conf/cvpr/CongZQRPHXYMM22","ArXiv":"2204.01026","DOI":"10.1109/CVPR52688.2022.01899","CorpusId":247939489},"title":"STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes"},{"paperId":"8941e477b2f39eb92712f04400412da60d349ec1","externalIds":{"DBLP":"conf/siggrapha/KhalidXBP22","ArXiv":"2203.13333","DOI":"10.1145/3550469.3555392","CorpusId":252089441},"title":"CLIP-Mesh: Generating textured meshes from text using pretrained image-text models"},{"paperId":"4f1d598f919aae55c3cbbc425ef1514a54e2b8cd","externalIds":{"DBLP":"conf/acl/GuSWTW22","ACL":"2022.acl-long.524","ArXiv":"2203.12667","DOI":"10.18653/v1/2022.acl-long.524","CorpusId":247627890},"title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions"},{"paperId":"0e8c3f15c210909a361ba3378d6fe2822ae4f93e","externalIds":{"DBLP":"conf/cvpr/MittalC0T22","ArXiv":"2203.09516","DOI":"10.1109/CVPR52688.2022.00040","CorpusId":247518895},"title":"AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation"},{"paperId":"cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e","externalIds":{"ArXiv":"2203.07281","ACL":"2023.eacl-main.277","DBLP":"conf/eacl/PrasadHZB23","DOI":"10.48550/arXiv.2203.07281","CorpusId":247447170},"title":"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models"},{"paperId":"9dc481ec44178e797466bbad968071917842156b","externalIds":{"DBLP":"journals/corr/abs-2203-03605","ArXiv":"2203.03605","DOI":"10.48550/arXiv.2203.03605","CorpusId":247292561},"title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"},{"paperId":"7ffbe47bd46e78d5e3ca3de5aed6d1e4fbf5757d","externalIds":{"ArXiv":"2203.03311","DBLP":"journals/corr/abs-2203-03311","DOI":"10.1109/TITS.2022.3195555","CorpusId":247292672},"title":"Comprehensive Review of Deep Learning-Based 3D Point Cloud Completion Processing and Analysis"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"f4df78183261538e718066331898ee5cad7cad05","externalIds":{"DBLP":"journals/corr/abs-2202-12837","ArXiv":"2202.12837","ACL":"2022.emnlp-main.759","DOI":"10.18653/v1/2022.emnlp-main.759","CorpusId":247155069},"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"60e69982ef2920596c6f31d6fd3ca5e9591f3db6","externalIds":{"ArXiv":"2201.05989","DBLP":"journals/tog/MullerESK22","DOI":"10.1145/3528223.3530127","CorpusId":246016186},"title":"Instant neural graphics primitives with a multiresolution hash encoding"},{"paperId":"e9581d9758062f76e029bd19a58c4ae976cfb414","externalIds":{"ArXiv":"2112.12750","DBLP":"journals/corr/abs-2112-12750","DOI":"10.1007/978-3-031-19809-0_30","CorpusId":245424883},"title":"SLIP: Self-supervision meets Language-Image Pre-training"},{"paperId":"c00846f91f071606cbad60be5125d56eaa1acd49","externalIds":{"ArXiv":"2112.11691","DBLP":"journals/tvcg/XuYDLGCL24","DOI":"10.1109/TVCG.2023.3340679","CorpusId":258832849,"PubMed":"38064324"},"title":"Comprehensive Visual Question Answering on Point Clouds through Compositional Scene Manipulation"},{"paperId":"8d737dc6a91a7bfde20aed7bb13d100476de5ae3","externalIds":{"DBLP":"conf/eccv/GhiasiGCL22","ArXiv":"2112.12143","DOI":"10.1007/978-3-031-20059-5_31","CorpusId":250895808},"title":"Scaling Open-Vocabulary Image Segmentation with Image-Level Labels"},{"paperId":"8f6c652a392995bd047a2f7b94474ab1e6e23ff0","externalIds":{"DBLP":"conf/cvpr/AzumaMKK22","ArXiv":"2112.10482","DOI":"10.1109/CVPR52688.2022.01854","CorpusId":245334889},"title":"ScanQA: 3D Question Answering for Spatial Scene Understanding"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"e77c484af99fc1eb3d3c36699ac81822e98cb74d","externalIds":{"DBLP":"conf/cvpr/LuddeckeE22","ArXiv":"2112.10003","DOI":"10.1109/CVPR52688.2022.00695","CorpusId":247794227},"title":"Image Segmentation Using Text and Image Prompts"},{"paperId":"837173ef1f260adc0d50b76675915776e1cc8ade","externalIds":{"ArXiv":"2112.09106","DBLP":"conf/cvpr/ZhongYZLCLZDYLG22","DOI":"10.1109/CVPR52688.2022.01629","CorpusId":245218534},"title":"RegionCLIP: Region-based Language-Image Pretraining"},{"paperId":"eef9b31e672cfa18d323ebd653d11f204ee80039","externalIds":{"DBLP":"journals/corr/abs-2112-07910","ArXiv":"2112.07910","DOI":"10.1109/CVPR52688.2022.01129","CorpusId":245144732},"title":"Decoupling Zero-Shot Semantic Segmentation"},{"paperId":"e91f73aaef155391b5b07e6612f5346dea888f64","externalIds":{"ArXiv":"2112.05131","DBLP":"conf/cvpr/Fridovich-KeilY22","DOI":"10.1109/CVPR52688.2022.00542","CorpusId":245006364},"title":"Plenoxels: Radiance Fields without Neural Networks"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","externalIds":{"DBLP":"conf/cvpr/SinghHGCGRK22","ArXiv":"2112.04482","DOI":"10.1109/CVPR52688.2022.01519","CorpusId":244954250},"title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"d15b27edf3630728cdb40f49946365d9011641cf","externalIds":{"ArXiv":"2112.03221","DBLP":"conf/cvpr/MichelBLBH22","DOI":"10.1109/CVPR52688.2022.01313","CorpusId":244908764},"title":"Text2Mesh: Text-Driven Neural Stylization for Meshes"},{"paperId":"03e1c3b5fdad9b21bbed3d13af7e8d6c73cbcfa6","externalIds":{"DBLP":"journals/corr/abs-2112-01455","ArXiv":"2112.01455","DOI":"10.1109/CVPR52688.2022.00094","CorpusId":244799255},"title":"Zero-Shot Text-Guided Object Generation with Dream Fields"},{"paperId":"52706e21fea7a583e9d86798a67148ea877fe664","externalIds":{"ArXiv":"2112.01551","DBLP":"conf/eccv/ChenWNC22","DOI":"10.1007/978-3-031-19824-3_29","CorpusId":251018164},"title":"D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding"},{"paperId":"8ee96d7e9347997423a51ef57faa6619fc4d9e37","externalIds":{"ArXiv":"2112.00484","DBLP":"journals/corr/abs-2112-00484","DOI":"10.1109/CVPR52688.2022.01835","CorpusId":244773144},"title":"Both Style and Fog Matter: Cumulative Domain Adaptation for Semantic Foggy Scene Understanding"},{"paperId":"dd2819016c6bf244c39b3e6707b60389bbdbcd21","externalIds":{"DBLP":"conf/cvpr/YuTR00L22","ArXiv":"2111.14819","DOI":"10.1109/CVPR52688.2022.01871","CorpusId":244714512},"title":"Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling"},{"paperId":"ec90ffa017a2cc6a51342509ce42b81b478aefb3","externalIds":{"ArXiv":"2111.12077","DBLP":"conf/cvpr/BarronMVSH22","DOI":"10.1109/CVPR52688.2022.00539","CorpusId":244488448},"title":"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields"},{"paperId":"521026c6097b0c0b44cb97b8a3952261527bea41","externalIds":{"ArXiv":"2111.11426","DBLP":"journals/corr/abs-2111-11426","DOI":"10.1111/cgf.14505","CorpusId":244478496},"title":"Neural Fields in Visual Computing and Beyond"},{"paperId":"4f7eb65f8d3c1eeb97e30f7ac68977ff16e1e942","externalIds":{"ArXiv":"2111.11215","DBLP":"conf/cvpr/0004SC22","DOI":"10.1109/CVPR52688.2022.00538","CorpusId":244477646},"title":"Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction"},{"paperId":"19c68c1e0cf0d47b96bd448e0ade1c4add0601d6","externalIds":{"ArXiv":"2111.08897","DBLP":"conf/nips/DehghanBCFFGKDJ21","CorpusId":237277377},"title":"ARKitScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data"},{"paperId":"46d846aa7df7b5c0cd4b129591b5754272701eae","externalIds":{"ArXiv":"2111.06719","ACL":"2022.naacl-main.290","DBLP":"conf/naacl/SuWQCLWWLLL0SZ22","DOI":"10.18653/v1/2022.naacl-main.290","CorpusId":248405974},"title":"On Transferability of Prompt Tuning for Natural Language Processing"},{"paperId":"8e970913466a81207230a09f6516bab944563cc0","externalIds":{"ArXiv":"2111.04276","DBLP":"journals/corr/abs-2111-04276","CorpusId":243848115},"title":"Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis"},{"paperId":"c09ebcb1ca6ad1eced57340f3e81e456416ed185","externalIds":{"DBLP":"conf/emnlp/LiKHdBN22","ArXiv":"2111.00607","ACL":"2022.emnlp-main.812","DOI":"10.18653/v1/2022.emnlp-main.812","CorpusId":253244266},"title":"A Systematic Investigation of Commonsense Knowledge in Large Language Models"},{"paperId":"c28b7dfe341f1e13a5a98efbce7946ef795cf9b8","externalIds":{"DBLP":"journals/corr/abs-2110-07904","ArXiv":"2110.07904","ACL":"2022.acl-long.346","DOI":"10.18653/v1/2022.acl-long.346","CorpusId":239009558},"title":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","externalIds":{"ArXiv":"2110.08207","DBLP":"journals/corr/abs-2110-08207","CorpusId":239009562},"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"767923635f2fd4467d848dba9655866e4f9b55c8","externalIds":{"ArXiv":"2110.05208","DBLP":"conf/iclr/LiLZCOSYY22","CorpusId":238582773},"title":"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm"},{"paperId":"738e3e0623054da29dc57fc6aee5e6711867c4e8","externalIds":{"DBLP":"journals/corr/abs-2110-02624","ArXiv":"2110.02624","DOI":"10.1109/CVPR52688.2022.01805","CorpusId":238408362},"title":"CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation"},{"paperId":"69ff4686b6517a0f9ae59503fedd8ed6e7be9983","externalIds":{"DBLP":"conf/iccv/ZhaoCS021","DOI":"10.1109/ICCV48922.2021.00292","CorpusId":244127479},"title":"3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds"},{"paperId":"69ee9b3a915951cc84b74599a3a2699a66d4004f","externalIds":{"DBLP":"conf/corl/ShridharMF21","ArXiv":"2109.12098","CorpusId":237396838},"title":"CLIPort: What and Where Pathways for Robotic Manipulation"},{"paperId":"f46bce5b7dd78736133c1af1824ddb83c0ec2e55","externalIds":{"DBLP":"conf/nips/RamakrishnanGWM21","ArXiv":"2109.08238","CorpusId":237563216},"title":"Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI"},{"paperId":"e553407be283d018e275f472d4d2fd709a6c9248","externalIds":{"DBLP":"conf/acl/GuHLH22","ArXiv":"2109.04332","ACL":"2022.acl-long.576","DOI":"10.18653/v1/2022.acl-long.576","CorpusId":237452236},"title":"PPT: Pre-trained Prompt Tuning for Few-shot Learning"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","externalIds":{"DBLP":"journals/corr/abs-2108-10904","ArXiv":"2108.10904","CorpusId":237291550},"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"23362ffad84a42a57b6633ebdf2da74ac4f4e8d7","externalIds":{"DBLP":"journals/corr/abs-2108-10743","ArXiv":"2108.10743","DOI":"10.1109/ICCV48922.2021.01240","CorpusId":237278052},"title":"DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization"},{"paperId":"0a4e7b3b98c1eea83bf253cb0bcb99e19e31659b","externalIds":{"ArXiv":"2108.02388","DBLP":"conf/mm/HeZLHHZ021","DOI":"10.1145/3474085.3475397","CorpusId":236924276},"title":"TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","externalIds":{"DBLP":"journals/csur/LiuYFJHN23","ArXiv":"2107.13586","DOI":"10.1145/3560815","CorpusId":236493269},"title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"260ad39a1dac4b451019e2bf17925f4df8e3b69a","externalIds":{"DBLP":"conf/nips/ChengSK21","ArXiv":"2107.06278","CorpusId":235829267},"title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation"},{"paperId":"cb56ea2d4de29481e25df6c318afc217eb7e4a7d","externalIds":{"DBLP":"journals/corr/abs-2107-03438","ArXiv":"2107.03438","CorpusId":235765540},"title":"LanguageRefer: Spatial-Language Model for 3D Visual Grounding"},{"paperId":"fda4530df9eec0e3f714dba3459ac50dab17d89c","externalIds":{"ArXiv":"2106.13043","DBLP":"journals/corr/abs-2106-13043","DOI":"10.1109/icassp43922.2022.9747631","CorpusId":235624127},"title":"Audioclip: Extending Clip to Image, Text and Audio"},{"paperId":"cf5647cb2613f5f697729eab567383006dcd4913","externalIds":{"ArXiv":"2106.10689","DBLP":"journals/corr/abs-2106-10689","CorpusId":235490453},"title":"NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction"},{"paperId":"ffba7fc5a4b350d50c406b8e52a57cc31ac971dd","externalIds":{"DBLP":"conf/cvpr/LassnerZ21","DOI":"10.1109/CVPR46437.2021.00149","CorpusId":235601887},"title":"Pulsar: Efficient Sphere-based Neural Rendering"},{"paperId":"5f1913828e30c3070f32c154d2d142ec17e91189","externalIds":{"DBLP":"conf/aaai/HuangLCL21","DOI":"10.1609/aaai.v35i2.16253","CorpusId":235306096},"title":"Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","externalIds":{"MAG":"3170403598","ArXiv":"2103.07191","DBLP":"conf/naacl/PatelBG21","ACL":"2021.naacl-main.168","DOI":"10.18653/V1/2021.NAACL-MAIN.168","CorpusId":232223322},"title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"fd2ab44a6f8a60507e8c86efc1225353466493c0","externalIds":{"DBLP":"conf/cvpr/ZhangC0ZPL21","ArXiv":"2103.06422","DOI":"10.1109/CVPR46437.2021.00872","CorpusId":232185507},"title":"Holistic 3D Scene Understanding from a Single Image with Implicit Representation"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","externalIds":{"DBLP":"conf/nips/HendrycksBKABTS21","ArXiv":"2103.03874","CorpusId":232134851},"title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"91408ef7aa1c5278e01c2deaf681f2ee7e9343ff","externalIds":{"DBLP":"journals/corr/abs-2103-01128","ArXiv":"2103.01128","DOI":"10.1109/ICCV48922.2021.00181","CorpusId":232092539},"title":"InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","externalIds":{"ArXiv":"2102.05918","DBLP":"conf/icml/JiaYXCPPLSLD21","CorpusId":231879586},"title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","externalIds":{"ArXiv":"2102.03334","DBLP":"journals/corr/abs-2102-03334","CorpusId":231839613},"title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","externalIds":{"DBLP":"conf/acl-deelio/LiuSZDCC22","ArXiv":"2101.06804","ACL":"2022.deelio-1.10","DOI":"10.18653/v1/2022.deelio-1.10","CorpusId":231632658},"title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"7a4ba78d377eea9650e5e399a0878e30bd22f648","externalIds":{"DBLP":"conf/cvpr/ChenGNC21","MAG":"3111353235","ArXiv":"2012.02206","DOI":"10.1109/CVPR46437.2021.00321","CorpusId":227305513},"title":"Scan2Cap: Context-aware Dense Captioning in RGB-D Scans"},{"paperId":"694bdf6e5906992dad2987a3cc8d1a176de691c9","externalIds":{"MAG":"3109476562","DBLP":"journals/corr/abs-2011-13961","ArXiv":"2011.13961","DOI":"10.1109/CVPR46437.2021.01018","CorpusId":227227965},"title":"D-NeRF: Neural Radiance Fields for Dynamic Scenes"},{"paperId":"13034a395d5c6728c9b11e777828d9998018cbf6","externalIds":{"DBLP":"conf/cvpr/LiNSW21","ArXiv":"2011.13084","MAG":"3108311884","DOI":"10.1109/CVPR46437.2021.00643","CorpusId":227208781},"title":"Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"414da5e0716eb4707e033b7967d7671dfe71ab71","externalIds":{"DBLP":"journals/corr/abs-2011-12950","ArXiv":"2011.12950","MAG":"3108309285","DOI":"10.1109/CVPR46437.2021.00930","CorpusId":227162620},"title":"Space-time Neural Irradiance Fields for Free-Viewpoint Video"},{"paperId":"bb8656979f38d95062ba55640c1be65535f57c6a","externalIds":{"MAG":"3109420014","ArXiv":"2011.12100","DBLP":"conf/cvpr/Niemeyer021","DOI":"10.1109/CVPR46437.2021.01129","CorpusId":227151657},"title":"GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields"},{"paperId":"5b0ea2c92ee16fa2f5a3dbc9315cd5c1e4ec1d88","externalIds":{"MAG":"3092948912","DBLP":"journals/corr/abs-2010-07492","ArXiv":"2010.07492","CorpusId":222380037},"title":"NeRF++: Analyzing and Improving Neural Radiance Fields"},{"paperId":"53794499a3830c3ebb365ecc57f0e8c8a20a682d","externalIds":{"MAG":"3095974555","DBLP":"conf/eccv/AchlioptasAXEG20","DOI":"10.1007/978-3-030-58452-8_25","CorpusId":221378802},"title":"ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes"},{"paperId":"691eddbfaebbc71f6a12d3c99d5c155042459434","externalIds":{"DBLP":"journals/corr/abs-2008-02268","ArXiv":"2008.02268","MAG":"3047146825","DOI":"10.1109/CVPR46437.2021.00713","CorpusId":220968781},"title":"NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections"},{"paperId":"17d7767a6ea87f4ab24d9cfaa5039160af9cad76","externalIds":{"ArXiv":"2007.11571","DBLP":"conf/nips/LiuGLCT20","MAG":"3092203888","CorpusId":220686483},"title":"Neural Sparse Voxel Fields"},{"paperId":"063f8b1ecf2394ca776ac61869734de9c1953808","externalIds":{"DBLP":"journals/corr/abs-2007-07779","MAG":"3042667808","ArXiv":"2007.07779","ACL":"2020.emnlp-demos.7","DOI":"10.18653/v1/2020.emnlp-demos.7","CorpusId":220525782},"title":"AdapterHub: A Framework for Adapting Transformers"},{"paperId":"62d337dbaead376ca042f23d62c0d4b65ec98546","externalIds":{"DBLP":"journals/corr/abs-2007-02442","ArXiv":"2007.02442","MAG":"3101267796","CorpusId":220364071},"title":"GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis"},{"paperId":"e23cb51f50f749320b9122fb5f75113b4d192c0a","externalIds":{"DBLP":"journals/corr/abs-2006-14799","ArXiv":"2006.14799","MAG":"3037013468","CorpusId":220128348},"title":"Evaluation of Text Generation: A Survey"},{"paperId":"56276404a473a640ac0778c196a6fbc03fb056f8","externalIds":{"DBLP":"journals/corr/abs-2006-12057","MAG":"3037265641","ArXiv":"2006.12057","CorpusId":219966277},"title":"Differentiable Rendering: A Survey"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"583d745ec6e1af705f695d899482996e4bc9d1b4","externalIds":{"DBLP":"journals/ral/QiuSSIK20","MAG":"3036594286","DOI":"10.1109/LRA.2020.3003290","CorpusId":220310726},"title":"3D-Aware Scene Change Captioning From Multiview Images"},{"paperId":"a0dc3135c40e150f0271002a96b7c9680b6cac40","externalIds":{"DBLP":"conf/nips/TancikSMFRSRBN20","ArXiv":"2006.10739","MAG":"3036843665","CorpusId":219791950},"title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains"},{"paperId":"1a0155937044e763034b1cb1f22d303d0e2c2eb6","externalIds":{"MAG":"3104700327","ArXiv":"2006.07793","DBLP":"journals/corr/abs-2006-07793","CorpusId":219686772},"title":"Generative 3D Part Assembly via Dynamic Graph Learning"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"c5a0f5021572a231885b096d16b7677f0468b9bb","externalIds":{"MAG":"3035154952","DBLP":"journals/corr/abs-2004-03967","ArXiv":"2004.03967","DOI":"10.1109/cvpr42600.2020.00402","CorpusId":215415808},"title":"Learning 3D Semantic Scene Graphs From 3D Indoor Reconstructions"},{"paperId":"9712cc6fc96f463842f9d41c565e3a8781bfba40","externalIds":{"MAG":"3014878043","DBLP":"conf/cvpr/JiangZS0FJ20","ArXiv":"2004.01658","DOI":"10.1109/cvpr42600.2020.00492","CorpusId":214795000},"title":"PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation"},{"paperId":"d846b06c98070ed1f9ffc4b5a10bd533640848ae","externalIds":{"MAG":"3034550906","ArXiv":"2003.06537","DBLP":"conf/cvpr/HanZXF20","DOI":"10.1109/cvpr42600.2020.00301","CorpusId":212725768},"title":"OccuSeg: Occupancy-Aware 3D Instance Segmentation"},{"paperId":"11da381e5ca3865dc6b2983e887ced4d25fc231f","externalIds":{"DBLP":"conf/cvpr/LiZFTT20","MAG":"3010719430","ArXiv":"2003.05855","DOI":"10.1109/cvpr42600.2020.00199","CorpusId":212675495},"title":"End-to-End Learning Local Multi-View Descriptors for 3D Point Clouds"},{"paperId":"84fae4c57e9f65459cf404866f83ff0b70fd7b75","externalIds":{"DBLP":"conf/icml/NashGEB20","ArXiv":"2002.10880","MAG":"3034727889","CorpusId":211296328},"title":"PolyGen: An Autoregressive Generative Model of 3D Meshes"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","externalIds":{"MAG":"3033156098","CorpusId":226096901},"title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"3465c06c872d8c48d628c5fc2d484087719351b6","externalIds":{"MAG":"3104953317","DBLP":"journals/corr/abs-1912-13318","ArXiv":"1912.13318","DOI":"10.1145/3394486.3403172","CorpusId":209515395},"title":"LayoutLM: Pre-training of Text and Layout for Document Image Understanding"},{"paperId":"6c161841cdb547f77930942e4ab46f4369751676","externalIds":{"DBLP":"journals/corr/abs-1912-08830","ArXiv":"1912.08830","MAG":"2995439012","DOI":"10.1007/978-3-030-58565-5_13","CorpusId":209414687},"title":"ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language"},{"paperId":"30e911f84f00423625f50ec70b9056ffc3d5b8ca","externalIds":{"DBLP":"journals/corr/abs-1912-08804","ArXiv":"1912.08804","MAG":"3035524985","DOI":"10.1109/CVPR42600.2020.00749","CorpusId":209405397},"title":"SynSin: End-to-End View Synthesis From a Single Image"},{"paperId":"f591c4869999f7d14095e176cb6ef72d2b904149","externalIds":{"DBLP":"conf/cvpr/0002JHZ20","ArXiv":"1912.07109","MAG":"3034964128","DOI":"10.1109/cvpr42600.2020.00133","CorpusId":209376681},"title":"SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization"},{"paperId":"3c3a7cee35a0ff46a99fc45f891f37eb9f1b1e9d","externalIds":{"ArXiv":"1912.00036","MAG":"2990884367","DBLP":"conf/cvpr/DaiDN20","DOI":"10.1109/cvpr42600.2020.00093","CorpusId":208527159},"title":"SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans"},{"paperId":"a75649771901a4881b44c0ceafa469fcc6e6f968","externalIds":{"MAG":"3044438666","ArXiv":"1911.12543","DBLP":"journals/tacl/JiangXAN20","DOI":"10.1162/tacl_a_00324","CorpusId":208513249},"title":"How Can We Know What Language Models Know?"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","externalIds":{"MAG":"2982399380","ACL":"2020.acl-main.703","DBLP":"journals/corr/abs-1910-13461","ArXiv":"1910.13461","DOI":"10.18653/v1/2020.acl-main.703","CorpusId":204960716},"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"f1e6bfd59428ecd91e8194b80503d3d24583a0ca","externalIds":{"DBLP":"conf/iccv/WaldANTN19","MAG":"2969056421","ArXiv":"1908.06109","DOI":"10.1109/ICCV.2019.00775","CorpusId":201070582},"title":"RIO: 3D Object Instance Re-Localization in Changing Indoor Environments"},{"paperId":"b2b0c31d036941cb557be4afb7101dc1b72f17cb","externalIds":{"MAG":"2968609420","DBLP":"conf/iccv/UyPHNY19","ArXiv":"1908.04616","DOI":"10.1109/ICCV.2019.00167","CorpusId":199552106},"title":"Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data"},{"paperId":"733d261df0949cdac6aedfdd284f99e0e452b4f5","externalIds":{"DBLP":"journals/corr/abs-1908-01210","ArXiv":"1908.01210","MAG":"2970086547","CorpusId":199442423},"title":"Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer"},{"paperId":"bcd411becb0f438720f84fd7d7a16ef6b94c8271","externalIds":{"MAG":"2991416037","ArXiv":"1907.05446","DBLP":"journals/corr/abs-1907-05446","CorpusId":208615299},"title":"General Evaluation for Instruction Conditioned Navigation using Dynamic Time Warping"},{"paperId":"15a8a5acbe771477f83366bcd60897894c3cdb9b","externalIds":{"MAG":"2951159596","DBLP":"journals/corr/abs-1906-04173","ArXiv":"1906.04173","DOI":"10.1145/3355089.3356513","CorpusId":184487385},"title":"Differentiable surface splatting for point-based geometry processing"},{"paperId":"b9d4a1ac5e41570082828b405b289aa6959252a4","externalIds":{"ArXiv":"1906.01618","DBLP":"journals/corr/abs-1906-01618","MAG":"2971278627","CorpusId":174798113},"title":"Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations"},{"paperId":"83d858045fcad929f0422b22a89759c1dea48dcb","externalIds":{"DBLP":"journals/corr/abs-1905-10711","ArXiv":"1905.10711","MAG":"2970899367","CorpusId":166228177},"title":"DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"8b751405526c28245eea5e925a6ede034c287bdb","externalIds":{"MAG":"2929298487","DBLP":"conf/iccv/Liu0LL19","ArXiv":"1904.01786","DOI":"10.1109/ICCV.2019.00780","CorpusId":102484000},"title":"Soft Rasterizer: A Differentiable Renderer for Image-Based 3D Reasoning"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","externalIds":{"MAG":"2963005248","DBLP":"conf/iclr/SaxtonGHK19","ArXiv":"1904.01557","CorpusId":85504763},"title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"dd81523b9accdf1c13cd37f76b22ab27d84b7a42","externalIds":{"MAG":"2951382975","DBLP":"journals/corr/abs-1901-05103","ArXiv":"1901.05103","DOI":"10.1109/CVPR.2019.00025","CorpusId":58007025},"title":"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"},{"paperId":"ee134bac4bdd3a4ab1a5045058d7f9314370cce9","externalIds":{"MAG":"2951030570","ArXiv":"1812.07003","DBLP":"journals/corr/abs-1812-07003","DOI":"10.1109/CVPR.2019.00455","CorpusId":56171922},"title":"3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans"},{"paperId":"2e689bdce24cf3644432505ce2783f03a1445ed2","externalIds":{"MAG":"2963926543","DBLP":"journals/corr/abs-1812-03828","ArXiv":"1812.03828","DOI":"10.1109/CVPR.2019.00459","CorpusId":54465161},"title":"Occupancy Networks: Learning 3D Reconstruction in Function Space"},{"paperId":"c3294425af6e2c059835ec7f0dca7290b48a8faf","externalIds":{"DBLP":"conf/cvpr/ChenZ19","ArXiv":"1812.02822","MAG":"2962849139","DOI":"10.1109/CVPR.2019.00609","CorpusId":54457478},"title":"Learning Implicit Fields for Generative Shape Modeling"},{"paperId":"00cf743a6944758a19d703e63e3197d0b13cd34b","externalIds":{"MAG":"2956121407","DBLP":"conf/cvpr/SongWZZGDSLY19","ArXiv":"1811.12222","DOI":"10.1109/CVPR.2019.00560","CorpusId":54083981},"title":"ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving"},{"paperId":"eb21d7e724b037e97ba657a5becb36fc2e7330d8","externalIds":{"MAG":"3008102851","DBLP":"conf/iccv/HenzlerM019","ArXiv":"1811.11606","DOI":"10.1109/ICCV.2019.01008","CorpusId":207976631},"title":"Escaping Plato’s Cave: 3D Shape From Adversarial Rendering"},{"paperId":"f3361bc40a60c8b215c47ad24a71339b42c60129","externalIds":{"DBLP":"journals/corr/abs-1811-10719","MAG":"2962946389","ArXiv":"1811.10719","DOI":"10.1109/CVPR.2019.01001","CorpusId":53728806},"title":"Learning View Priors for Single-View 3D Reconstruction"},{"paperId":"59b013e5f39f1f09aba67f59d7d39671dd449282","externalIds":{"ArXiv":"1810.09381","MAG":"2891186417","DBLP":"conf/nips/InsafutdinovD18","CorpusId":53046853},"title":"Unsupervised Learning of Shape and Pose with Differentiable Point Clouds"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","externalIds":{"MAG":"2885185669","DBLP":"journals/corr/abs-1808-06226","ArXiv":"1808.06226","ACL":"D18-2012","DOI":"10.18653/v1/D18-2012","CorpusId":52051958},"title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"6654ba1d3e61cdf5f4decc7464436046cf602ed1","externalIds":{"ArXiv":"1807.06757","DBLP":"journals/corr/abs-1807-06757","MAG":"2884565639","CorpusId":49865049},"title":"On Evaluation of Embodied Navigation Agents"},{"paperId":"a6b1359736c83e9572732e7581aa50b5cdef0e0f","externalIds":{"MAG":"2955935041","DBLP":"conf/cvpr/LiuQG19","ArXiv":"1806.01411","DOI":"10.1109/CVPR.2019.00062","CorpusId":67855731},"title":"FlowNet3D: Learning Scene Flow in 3D Point Clouds"},{"paperId":"643d11703569766bed0a994941ae5f7b3e101659","externalIds":{"MAG":"2952018420","DBLP":"conf/cvpr/GenovaCMSVF18","ArXiv":"1806.06098","DOI":"10.1109/CVPR.2018.00874","CorpusId":49300163},"title":"Unsupervised Training for 3D Morphable Model Regression"},{"paperId":"eaa28165804c8e35d37c906226d6a6f4615f9cb4","externalIds":{"DBLP":"journals/cgf/RoveriOPG18","MAG":"2805499196","DOI":"10.1111/cgf.13344","CorpusId":43932773},"title":"PointProNets: Consolidation of Point Clouds with Convolutional Neural Networks"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"fafe7c7aa0a19d40bbbf08b9e87d650438b01b67","externalIds":{"ArXiv":"1803.10409","DBLP":"conf/eccv/DaiN18","MAG":"2795014656","DOI":"10.1007/978-3-030-01249-6_28","CorpusId":4408791},"title":"3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation"},{"paperId":"ecc24760bfb423fc9bfd3b34162fea66dea1633c","externalIds":{"ArXiv":"1803.08495","DBLP":"conf/accv/ChenCSCFS18","MAG":"2950536097","DOI":"10.1007/978-3-030-20893-6_7","CorpusId":4707877},"title":"Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings"},{"paperId":"8691706ad0cf5e83969658b2e6bfffdc379440c9","externalIds":{"ArXiv":"1801.10198","MAG":"2950355077","DBLP":"conf/iclr/LiuSPGSKS18","CorpusId":3608234},"title":"Generating Wikipedia by Summarizing Long Sequences"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","externalIds":{"ACL":"P18-1031","MAG":"2952772027","DBLP":"conf/acl/RuderH18","DOI":"10.18653/v1/P18-1031","CorpusId":40100965},"title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"ae8370d1010aaa44c685d64fd40d730c3197d44f","externalIds":{"MAG":"2769312834","ArXiv":"1711.08588","DBLP":"journals/corr/abs-1711-08588","DOI":"10.1109/CVPR.2018.00272","CorpusId":3731246},"title":"SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation"},{"paperId":"c37c23b12e00168833eccff8025a830ce27c5abc","externalIds":{"MAG":"2952157315","DBLP":"conf/cvpr/AndersonWTB0S0G18","ArXiv":"1711.07280","DOI":"10.1109/CVPR.2018.00387","CorpusId":4673790},"title":"Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"},{"paperId":"80f5ee8578ee76e2c17824f211762ffec7e029d4","externalIds":{"DBLP":"journals/corr/abs-1711-06396","MAG":"2949954792","ArXiv":"1711.06396","DOI":"10.1109/CVPR.2018.00472","CorpusId":42427078},"title":"VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection"},{"paperId":"8337441971f941716a9e525a67f37088eb01fd13","externalIds":{"MAG":"2949525187","DBLP":"journals/corr/abs-1709-06158","ArXiv":"1709.06158","DOI":"10.1109/3DV.2017.00081","CorpusId":21435690},"title":"Matterport3D: Learning from RGB-D Data in Indoor Environments"},{"paperId":"1d83aa9ac9f26cd117d50d845743260f3a87250b","externalIds":{"MAG":"2951500648","DBLP":"journals/corr/LinKL17","ArXiv":"1706.07036","DOI":"10.1609/aaai.v32i1.12278","CorpusId":12973135},"title":"Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction"},{"paperId":"8674494bd7a076286b905912d26d47f7501c4046","externalIds":{"DBLP":"conf/nips/QiYSG17","MAG":"2950697424","ArXiv":"1706.02413","CorpusId":1745976},"title":"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"},{"paperId":"ae587a4a8842fbe01b9a043b66f762a89dca5074","externalIds":{"MAG":"2963739349","DBLP":"conf/cvpr/TulsianiZEM17","ArXiv":"1704.06254","DOI":"10.1109/CVPR.2017.30","CorpusId":73431591},"title":"Multi-view Supervision for Single-View Reconstruction via Differentiable Ray Consistency"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"13d9323a8716131911bfda048a40e2cde1a76a46","externalIds":{"MAG":"2949847915","DBLP":"conf/iclr/KimDHR17","ArXiv":"1702.00887","CorpusId":6961760},"title":"Structured Attention Networks"},{"paperId":"d997beefc0922d97202789d2ac307c55c2c52fba","externalIds":{"MAG":"2950642167","DBLP":"conf/cvpr/QiSMG17","ArXiv":"1612.00593","DOI":"10.1109/CVPR.2017.16","CorpusId":5115938},"title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"},{"paperId":"e4c29b7e33d2a273ac4d3391656762571a632d29","externalIds":{"ArXiv":"1612.00814","DBLP":"conf/nips/YanYYGL16","MAG":"2950701417","CorpusId":1608002},"title":"Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision"},{"paperId":"09c0c993500f5d15b3ccbfbec11bbb445bf51e57","externalIds":{"MAG":"2523049145","DBLP":"journals/corr/McCormacHDL16","ArXiv":"1609.05130","DOI":"10.1109/ICRA.2017.7989538","CorpusId":206852649},"title":"SemanticFusion: Dense 3D semantic mapping with convolutional neural networks"},{"paperId":"f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f","externalIds":{"DBLP":"journals/corr/AndersonFJG16","MAG":"2950201573","ArXiv":"1607.08822","DOI":"10.1007/978-3-319-46454-1_24","CorpusId":11933981},"title":"SPICE: Semantic Propositional Image Caption Evaluation"},{"paperId":"e7d0c37f4f3589a3b787f39e8307704da5ed8d6c","externalIds":{"DBLP":"conf/cvpr/SchonbergerF16","MAG":"2471962767","DOI":"10.1109/CVPR.2016.445","CorpusId":1728538},"title":"Structure-from-Motion Revisited"},{"paperId":"dc3f8c8513441915408ab0549e9ac5f2f2f31eec","externalIds":{"MAG":"2460657278","DBLP":"conf/cvpr/ArmeniSZJBFS16","DOI":"10.1109/CVPR.2016.170","CorpusId":9649070},"title":"3D Semantic Parsing of Large-Scale Indoor Spaces"},{"paperId":"9b686d76914befea66377ec79c1f9258d70ea7e3","externalIds":{"MAG":"2190691619","ArXiv":"1512.03012","DBLP":"journals/corr/ChangFGHHLSSSSX15","CorpusId":2554264},"title":"ShapeNet: An Information-Rich 3D Model Repository"},{"paperId":"7819ae2316a2b4794d8134813e097790dd60d353","externalIds":{"ArXiv":"1602.03725","DBLP":"conf/iccv/RhodinRRST15","MAG":"2950850061","DOI":"10.1109/ICCV.2015.94","CorpusId":11660023},"title":"A Versatile Scene Model with Differentiable Visibility Applied to Generative Pose Estimation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","externalIds":{"DBLP":"conf/acl/SennrichHB16a","ACL":"P16-1162","MAG":"1816313093","ArXiv":"1508.07909","DOI":"10.18653/v1/P16-1162","CorpusId":1114678},"title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"081651b38ff7533550a3adfc1c00da333a8fe86c","externalIds":{"DBLP":"conf/nips/YosinskiCBL14","ArXiv":"1411.1792","MAG":"2149933564","CorpusId":362467},"title":"How transferable are features in deep neural networks?"},{"paperId":"acb2697311c288ac139dbc5538cc2f3bdda3d59d","externalIds":{"DBLP":"conf/eccv/LoperB14","MAG":"183071939","DOI":"10.1007/978-3-319-10584-0_11","CorpusId":17868098},"title":"OpenDR: An Approximate Differentiable Renderer"},{"paperId":"c2adb138674a7f75ade139d0e56d7a8e83f0803e","externalIds":{"MAG":"2071906076","DBLP":"journals/tog/NiessnerZIS13","DOI":"10.1145/2508363.2508374","CorpusId":207207064},"title":"Real-time 3D reconstruction at scale using voxel hashing"},{"paperId":"7dc5dc2add7617d6a672916acc05477425b73ee9","externalIds":{"DBLP":"conf/icra/SenguptaGST13","MAG":"1971618559","DOI":"10.1109/ICRA.2013.6630632","CorpusId":6290883},"title":"Urban 3D semantic modelling using stereo vision"},{"paperId":"a975c6423e45e180d197cb861fca4d6770d47836","externalIds":{"MAG":"1826510911","CorpusId":109885072},"title":"Understanding Augmented Reality: Concepts and Applications"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","externalIds":{"DBLP":"conf/icassp/SchusterN12","MAG":"2121879602","DOI":"10.1109/ICASSP.2012.6289079","CorpusId":22320655},"title":"Japanese and Korean voice search"},{"paperId":"f96734630d102c5612420b3472d1c20eed0b96c1","externalIds":{"MAG":"1966061914","DBLP":"conf/iccv/PrisacariuR11","DOI":"10.1109/ICCV.2011.6126547","CorpusId":7628537},"title":"Shared shape spaces"},{"paperId":"e2975deb809f35bd47ac3de34e348675a0cfb3f6","externalIds":{"MAG":"1987648924","DBLP":"conf/ismar/NewcombeIHMKDKSHF11","DOI":"10.1109/ISMAR.2011.6092378","CorpusId":11830123},"title":"KinectFusion: Real-time dense surface mapping and tracking"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"c896b6c0e8e6be8ba28142e096d705241ce94b9a","externalIds":{"MAG":"2093834886","DBLP":"books/lib/OsherF03","DOI":"10.1016/s0898-1221(03)90179-9","CorpusId":27576942},"title":"Level set methods and dynamic implicit surfaces"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"dc9404cb021d7f859271de89aec145dd8b0fee87","externalIds":{"MAG":"2598319647","DBLP":"conf/iswc/FeinerMHW97","DOI":"10.1007/BF01682023","CorpusId":2108280},"title":"A touring machine: Prototyping 3D mobile augmented reality systems for exploring the urban environment"},{"paperId":"4fdc075d1cc14d64c11c3c860e86a082b761f4a0","externalIds":{"DBLP":"journals/presence/Azuma97","MAG":"2122122381","DOI":"10.1162/pres.1997.6.4.355","CorpusId":469744},"title":"A Survey of Augmented Reality"},{"paperId":"a2ad1cb2c3f2c08233b3f1b62512daf6232229d1","externalIds":{"DBLP":"conf/siggraph/CurlessL96","MAG":"2009422376","DOI":"10.1145/237170.237269","CorpusId":12358833},"title":"A volumetric method for building complex models from range images"},{"paperId":"64c97b97c98664a2c1a958a21a460ab628020c49","externalIds":{"DBLP":"journals/tmm/FengHZGYWM25","DOI":"10.1109/TMM.2023.3277736","CorpusId":258855397},"title":"Exploring Hierarchical Spatial Layout Cues for 3D Point Cloud Based Scene Graph Prediction"},{"paperId":"7637ed79d30d0139901175ae4abedd822c217ab4","externalIds":{"DBLP":"journals/corr/abs-2307-12981","DOI":"10.48550/arXiv.2307.12981","CorpusId":260356619},"title":"3D-LLM: Injecting the 3D World into Large Language Models"},{"paperId":"90428f3a8caa5082f825ebf3138514ddf273dae3","externalIds":{"CorpusId":253581838},"title":"Supplementary Materials for: NULL-text Inversion for Editing Real Images using Guided Diffusion Models"},{"paperId":"0efeef4c187059f9b676452482f2ba31c735b546","externalIds":{"DBLP":"conf/emnlp/KatoKCK23","DOI":"10.18653/v1/2023.findings-emnlp.56","CorpusId":266176149},"title":"ARKitSceneRefer: Text-based Localization of Small Objects in Diverse Real-World 3D Indoor Scenes"},{"paperId":"d929d1d5dcce662891fa26d1554d7e6a7422c7d9","externalIds":{"DBLP":"journals/corr/abs-2305-13876","DOI":"10.48550/arXiv.2305.13876","CorpusId":258841438},"title":"Cross3DVG: Baseline and Dataset for Cross-Dataset 3D Visual Grounding on Different RGB-D Scans"},{"paperId":"06d8562831c32844285a691c5250d04726df3c61","externalIds":{"DBLP":"journals/corr/abs-2307-12980","DOI":"10.48550/arXiv.2307.12980","CorpusId":260357841},"title":"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models"},{"paperId":"ecce44df1956db4ec486539c6543345344809958","externalIds":{"DBLP":"journals/corr/abs-2202-03052","CorpusId":265040027},"title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"0247f0c8671014e78640ec15924111a0de161e5d","externalIds":{"ACL":"2022.acl-long.174","DBLP":"conf/acl/WuS22","DOI":"10.18653/v1/2022.acl-long.174","CorpusId":248780043},"title":"Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis"},{"paperId":"ff169d09a933756e8798021dbf9e24a0bbfd9b38","externalIds":{"DBLP":"conf/iclr/Zhou0W0XYK22","CorpusId":251647228},"title":"Image BERT Pre-training with Online Tokenizer"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"6caf3307096a15832ace34a0d54cd28413503f8b","externalIds":{"DBLP":"journals/corr/abs-2102-07064","CorpusId":231924858},"title":"NeRF-: Neural Radiance Fields Without Known Camera Parameters"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"0c0a778e6fdf7e36b1750c533dcc916f86608607","externalIds":{"MAG":"2527310337","DBLP":"journals/tkde/XunJGZ17","DOI":"10.1109/TKDE.2016.2614508","CorpusId":13490401},"title":"A Survey on Context Learning"},{"paperId":"01efa5e5a7231fbc0358932ec8d1fcc1f021cf30","externalIds":{"DBLP":"books/daglib/p/CarmignianiF11","MAG":"145587688","DOI":"10.1007/978-1-4614-0064-6_1","CorpusId":17434740},"title":"Augmented Reality: An Overview"},{"paperId":"d61c6fedd153f69882cfcce4f0a1cb5dd03fad40","externalIds":{"DOI":"10.1038/226191c0","CorpusId":4158973},"title":"Problem Solvers"},{"paperId":"c3a2bb6a717a8c4a3aad6fc6648b3bd9fa23a226","externalIds":{"CorpusId":278744937},"title":"Transcribe3D: Grounding LLMs Using Transcribed Information for 3D Referential Reasoning with Self-Corrected Finetuning"},{"paperId":"5410086e3c7cb4cc48f73f75e843b4109befe092","externalIds":{"CorpusId":282134003},"title":"Representing Scenes as Neural Radiance Fields for View Synthesis"}]}