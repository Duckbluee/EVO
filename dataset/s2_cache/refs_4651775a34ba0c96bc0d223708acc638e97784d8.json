{"references":[{"paperId":"57a6b749ef0029fa3c6167f944ec956985dc3bfe","externalIds":{"DBLP":"journals/corr/abs-2505-24862","ArXiv":"2505.24862","DOI":"10.48550/arXiv.2505.24862","CorpusId":279071198},"title":"ViStoryBench: Comprehensive Benchmark Suite for Story Visualization"},{"paperId":"e72efe88b16972f115f9dc85f0f2b165a9f4e6a9","externalIds":{"DBLP":"journals/corr/abs-2504-13074","ArXiv":"2504.13074","DOI":"10.48550/arXiv.2504.13074","CorpusId":277856899},"title":"SkyReels-V2: Infinite-length Film Generative Model"},{"paperId":"1477a4d550c5ed10398bee8ba143bed9be2a9b70","externalIds":{"DBLP":"journals/corr/abs-2504-08685","ArXiv":"2504.08685","DOI":"10.48550/arXiv.2504.08685","CorpusId":277740920},"title":"Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model"},{"paperId":"06dba3977ce4bb4eee58b347bba340056f00a1ec","externalIds":{"DBLP":"journals/corr/abs-2504-02436","ArXiv":"2504.02436","DOI":"10.48550/arXiv.2504.02436","CorpusId":277509893},"title":"SkyReels-A2: Compose Anything in Video Diffusion Transformers"},{"paperId":"6d4def9e60c21ccf6e7620f619cd621b50363c5b","externalIds":{"ArXiv":"2503.21755","DBLP":"journals/corr/abs-2503-21755","DOI":"10.48550/arXiv.2503.21755","CorpusId":277350030},"title":"VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness"},{"paperId":"78664ad34a53b33afc0baa2d18742c07a9c70526","externalIds":{"ArXiv":"2503.11251","DBLP":"journals/corr/abs-2503-11251","DOI":"10.48550/arXiv.2503.11251","CorpusId":277043783},"title":"Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model"},{"paperId":"81a3c48930c06d8cdb6edcdeea2144412af00912","externalIds":{"ArXiv":"2503.07598","DBLP":"journals/corr/abs-2503-07598","DOI":"10.48550/arXiv.2503.07598","CorpusId":276928131},"title":"VACE: All-in-One Video Creation and Editing"},{"paperId":"71bc249efe4c4f9a11d84e4b05607ed09947bc67","externalIds":{"ArXiv":"2503.06053","DBLP":"journals/corr/abs-2503-06053","DOI":"10.48550/arXiv.2503.06053","CorpusId":276902632},"title":"DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation"},{"paperId":"5ffcf94ceb91c54d552125bc8f89bfc13b73a0f6","externalIds":{"ArXiv":"2502.10248","DBLP":"journals/corr/abs-2502-10248","DOI":"10.48550/arXiv.2502.10248","CorpusId":276395073},"title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model"},{"paperId":"4ee274e9c6d87c3e62314b112d13495654d0d5bb","externalIds":{"ArXiv":"2502.08639","DBLP":"conf/siggraph/WangLS0LX000G25","DOI":"10.1145/3721238.3730755","CorpusId":276287673},"title":"CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation"},{"paperId":"0dac8b51df2e94a7623e3aa471058b8f57e9a449","externalIds":{"DBLP":"journals/corr/abs-2502-07701","ArXiv":"2502.07701","DOI":"10.48550/arXiv.2502.07701","CorpusId":276258706},"title":"Magic 1-For-1: Generating One Minute Video Clips within One Minute"},{"paperId":"02db2bcb2b303a9069c345bbeb77b171900d1b61","externalIds":{"ArXiv":"2502.05240","DBLP":"journals/corr/abs-2502-05240","DOI":"10.48550/arXiv.2502.05240","CorpusId":276250459},"title":"Survey on AI-Generated Media Detection: From Non-MLLM to MLLM"},{"paperId":"d07d2ceb657595d142e01bc14351bb17c7e2b5d3","externalIds":{"ArXiv":"2501.08331","DBLP":"conf/cvpr/BurgertXXPCHMDL25","DOI":"10.1109/CVPR52734.2025.00011","CorpusId":275516429},"title":"Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise"},{"paperId":"b95baf93d1cfb406b50aa370ce5ba18788b5891d","externalIds":{"ArXiv":"2501.03575","DBLP":"journals/corr/abs-2501-03575","DOI":"10.48550/arXiv.2501.03575","CorpusId":275342594},"title":"Cosmos World Foundation Model Platform for Physical AI"},{"paperId":"1fa298e3f745099d39ca5bd088fcb62f87e8cc2f","externalIds":{"DBLP":"journals/corr/abs-2412-03603","ArXiv":"2412.03603","DOI":"10.48550/arXiv.2412.03603","CorpusId":274514554},"title":"HunyuanVideo: A Systematic Framework For Large Video Generative Models"},{"paperId":"156efc7517b0bb223ad1c132309ced2b371c42c7","externalIds":{"ArXiv":"2411.17440","DBLP":"conf/cvpr/YuanHHGSC0025","DOI":"10.1109/CVPR52734.2025.01211","CorpusId":274280619},"title":"Identity-Preserving Text-To-Video Generation by Frequency Decomposition"},{"paperId":"7943ec4a67151a559b25cd34369e661c9a7924c8","externalIds":{"ArXiv":"2410.21276","CorpusId":273662196},"title":"GPT-4o System Card"},{"paperId":"bc2ddba47a9d6b378abd8a8584849f3e4c410f24","externalIds":{"DBLP":"journals/corr/abs-2410-18072","ArXiv":"2410.18072","DOI":"10.48550/arXiv.2410.18072","CorpusId":273532769},"title":"WorldSimBench: Towards Video Generation Models as World Simulators"},{"paperId":"0296f050b655f29155f287a67b76c85838ce94f0","externalIds":{"ArXiv":"2410.05651","DBLP":"conf/iclr/YangKY25","DOI":"10.48550/arXiv.2410.05651","CorpusId":273228406},"title":"ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler"},{"paperId":"398090d5d5232fc9466fee8a9b904bf7a28bbded","externalIds":{"DBLP":"journals/corr/abs-2410-06158","ArXiv":"2410.06158","DOI":"10.48550/arXiv.2410.06158","CorpusId":273228660},"title":"GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation"},{"paperId":"d6837b60f100b40c5ee2284fbee3134a904c0603","externalIds":{"ArXiv":"2408.15239","DBLP":"journals/corr/abs-2408-15239","DOI":"10.48550/arXiv.2408.15239","CorpusId":271963112},"title":"Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation"},{"paperId":"75d7591078a74aa6bfcdaf5bde7dbe5146a45ecd","externalIds":{"DBLP":"journals/corr/abs-2408-14837","ArXiv":"2408.14837","DOI":"10.48550/arXiv.2408.14837","CorpusId":271962839},"title":"Diffusion Models Are Real-Time Game Engines"},{"paperId":"8af933a6e0d45e041a1ca35d461aad92022aa957","externalIds":{"ArXiv":"2408.02629","DBLP":"journals/corr/abs-2408-02629","DOI":"10.48550/arXiv.2408.02629","CorpusId":271710020},"title":"VidGen-1M: A Large-Scale Dataset for Text-to-video Generation"},{"paperId":"c3d746a8ae633431cdd64aa285c627f8ef847f42","externalIds":{"ArXiv":"2407.16124","DBLP":"journals/corr/abs-2407-16124","DOI":"10.48550/arXiv.2407.16124","CorpusId":271334698},"title":"Fréchet Video Motion Distance: A Metric for Evaluating Motion Consistency in Videos"},{"paperId":"f476823b1d58d6d53d79fdff0745518dfe5e6ada","externalIds":{"ArXiv":"2407.02315","DBLP":"journals/corr/abs-2407-02315","DOI":"10.48550/arXiv.2407.02315","CorpusId":270878366},"title":"VFIMamba: Video Frame Interpolation with State Space Models"},{"paperId":"40d63dc2b465c9081e4efc5a19514da151e97fe7","externalIds":{"ArXiv":"2407.01392","DBLP":"conf/nips/0003MDSTS24","DOI":"10.48550/arXiv.2407.01392","CorpusId":270869622},"title":"Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion"},{"paperId":"b15e6e2b1d81bc110f8fc98c3caf2e25e2512539","externalIds":{"ArXiv":"2406.06525","DBLP":"journals/corr/abs-2406-06525","DOI":"10.48550/arXiv.2406.06525","CorpusId":270371603},"title":"Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation"},{"paperId":"a3831bb69618682da68ea528101f868d0d905880","externalIds":{"ArXiv":"2405.19707","CorpusId":270123818},"title":"DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark"},{"paperId":"d25d4a07e6b5702c58e172e8918c90d3cf61ef33","externalIds":{"ArXiv":"2405.19745","DBLP":"journals/corr/abs-2405-19745","DOI":"10.1145/3641519.3657417","CorpusId":270123070},"title":"GaussianPrediction: Dynamic 3D Gaussian Prediction for Motion Extrapolation and Free View Synthesis"},{"paperId":"c157e81cd9b3b99a2102fb429d7ba20280385bcc","externalIds":{"DBLP":"journals/corr/abs-2405-17421","ArXiv":"2405.17421","DOI":"10.1109/CVPR52734.2025.00578","CorpusId":270062699},"title":"MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds"},{"paperId":"63a1ea11f2e16c292e2d596ea6a1c2fec77f9126","externalIds":{"ArXiv":"2405.10674","DBLP":"journals/corr/abs-2405-10674","DOI":"10.48550/arXiv.2405.10674","CorpusId":269899564},"title":"From Sora What We Can See: A Survey of Text-to-Video Generation"},{"paperId":"d7fbdba317e195c3b49dbcdb14b7b52a05bfb3f4","externalIds":{"DBLP":"journals/corr/abs-2405-03520","ArXiv":"2405.03520","DOI":"10.48550/arXiv.2405.03520","CorpusId":269604832},"title":"Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond"},{"paperId":"48cbad923c60ff4cfab54a52de491fa62268cf43","externalIds":{"DBLP":"journals/tmlr/MelnikLLYRR24","ArXiv":"2405.03150","DOI":"10.48550/arXiv.2405.03150","CorpusId":269606007},"title":"Video Diffusion Models: A Survey"},{"paperId":"e5c1e15e24708d169ef30c28d18a6d5cf3bd0836","externalIds":{"DBLP":"conf/mm/GeHZ0WTZ24","ArXiv":"2404.18202","DOI":"10.1145/3664647.3681488","CorpusId":269449620},"title":"WorldGPT: Empowering LLM as Multimodal World Model"},{"paperId":"7bf2aab978e7ce7762bac12bfb1c5a4de5a4db31","externalIds":{"ArXiv":"2404.05014","DBLP":"journals/pami/YuanHSXZLCYL25","DOI":"10.1109/TPAMI.2025.3558507","CorpusId":269005927,"PubMed":"40198299"},"title":"MagicTime: Time-Lapse Video Generation Models as Metamorphic Simulators"},{"paperId":"501e210b6fef5163d03ebff34e19b7445f3bee02","externalIds":{"DBLP":"journals/corr/abs-2404-04808","ArXiv":"2404.04808","DOI":"10.1109/CVPR52733.2024.01804","CorpusId":269005178},"title":"MemFlow: Optical Flow Estimation and Prediction with Memory"},{"paperId":"8fd3340ea220de17f75eea345b8f298942e3efcf","externalIds":{"ArXiv":"2403.16407","DBLP":"journals/corr/abs-2403-16407","DOI":"10.48550/arXiv.2403.16407","CorpusId":268681721},"title":"A Survey on Long Video Generation: Challenges, Methods, and Prospects"},{"paperId":"72d4fc041c520431e38b684f843721620ca40d8a","externalIds":{"DBLP":"conf/cvpr/ChenLQYZL024","ArXiv":"2403.17000","DOI":"10.1109/CVPR52733.2024.00882","CorpusId":268681447},"title":"Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution"},{"paperId":"bb47823ccdaca2c69f6a3fa77b6072b829ba001b","externalIds":{"ArXiv":"2403.15377","DBLP":"conf/eccv/WangLLYHCPZWSJLXZHQWW24","DOI":"10.1007/978-3-031-73013-9_23","CorpusId":271432386},"title":"InternVideo2: Scaling Foundation Models for Multimodal Video Understanding"},{"paperId":"90607c8d66c9e397d697214f326560231f7834ba","externalIds":{"ArXiv":"2403.11134","DBLP":"journals/cvm/WuYZYCYG24","DOI":"10.1007/s41095-024-0436-y","CorpusId":268513119},"title":"Recent advances in 3D Gaussian splatting"},{"paperId":"a70f267ce46eb8b46e9482e9bacb00e64b03c6ea","externalIds":{"DBLP":"journals/corr/abs-2403-09630","ArXiv":"2403.09630","DOI":"10.1109/CVPR52733.2024.01389","CorpusId":268385457},"title":"Generalized Predictive Model for Autonomous Driving"},{"paperId":"4c77262a193c39eae08331529508260dce909e81","externalIds":{"DBLP":"conf/mm/PengLZ0W024","ArXiv":"2403.07720","DOI":"10.1145/3664647.3681685","CorpusId":273641462},"title":"Multi-modal Auto-regressive Modeling via Visual Tokens"},{"paperId":"9db25ca30dcc8aa75c034ccaee0f6ee2ca934956","externalIds":{"DBLP":"journals/corr/abs-2403-06738","ArXiv":"2403.06738","DOI":"10.48550/arXiv.2403.06738","CorpusId":268357902},"title":"V3D: Video Diffusion Models are Effective 3D Generators"},{"paperId":"8eae862d9669e7001eeee17b49fba793df9672c4","externalIds":{"ArXiv":"2402.19479","DBLP":"conf/cvpr/ChenSMDCJF0RYT24","DOI":"10.1109/CVPR52733.2024.01265","CorpusId":268091168},"title":"Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers"},{"paperId":"a6f7485dfdf45320e82d84bcfdc51bcd52dff18b","externalIds":{"ArXiv":"2402.17177","DBLP":"journals/corr/abs-2402-17177","DOI":"10.48550/arXiv.2402.17177","CorpusId":268032569},"title":"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"},{"paperId":"43c44948874dd19dbb4df4b3127e6b248cb3c7e2","externalIds":{"ArXiv":"2401.09985","DBLP":"journals/corr/abs-2401-09985","DOI":"10.48550/arXiv.2401.09985","CorpusId":267035033},"title":"WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens"},{"paperId":"0c4f46e4dcae5527018e6432fb60cfe8c3354e97","externalIds":{"DBLP":"journals/corr/abs-2312-14125","ArXiv":"2312.14125","DOI":"10.48550/arXiv.2312.14125","CorpusId":266435847},"title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation"},{"paperId":"4443c9a43bff8dcd717e5c75115ec6497af2b953","externalIds":{"DBLP":"conf/iclr/WuJCCXLLLK24","ArXiv":"2312.13139","DOI":"10.48550/arXiv.2312.13139","CorpusId":266374724},"title":"Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation"},{"paperId":"8d649af6ae760347457d8bd0bebf7d1565f421e0","externalIds":{"DBLP":"conf/cvpr/JainNVB24","ArXiv":"2312.07509","DOI":"10.1109/CVPR52733.2024.00772","CorpusId":266174002},"title":"Peekaboo: Interactive Video Generation via Masked-Diffusion"},{"paperId":"905ba940236b00bebb2fd348d4d932e7887b0c0a","externalIds":{"DBLP":"journals/corr/abs-2312-06662","ArXiv":"2312.06662","DOI":"10.48550/arXiv.2312.06662","CorpusId":266163109},"title":"Photorealistic Video Generation with Diffusion Models"},{"paperId":"26d5fac4fd6f96c9a4212090f57a85e3a8591652","externalIds":{"ArXiv":"2312.02928","DBLP":"journals/corr/abs-2312-02928","DOI":"10.48550/arXiv.2312.02928","CorpusId":265658927},"title":"LivePhoto: Real Image Animation with Text-guided Motion Control"},{"paperId":"689c358c5f9b5b1693a8bcc7e6e0460012f5cf9e","externalIds":{"ArXiv":"2312.00785","DBLP":"journals/corr/abs-2312-00785","DOI":"10.1109/CVPR52733.2024.02157","CorpusId":265552038},"title":"Sequential Modeling Enables Scalable Learning for Large Vision Models"},{"paperId":"1b979967ab3f008c04604f1b4e36384a5aff21f9","externalIds":{"ArXiv":"2311.18829","DBLP":"conf/cvpr/WangBWFYYZDZWQY24","DOI":"10.1109/CVPR52733.2024.00804","CorpusId":265506813},"title":"MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation"},{"paperId":"b8df34f9df285a5dcc86fa94cafc67ff5a7abbca","externalIds":{"DBLP":"conf/cvpr/WengFWDWYZQBYLZ22","ArXiv":"2311.18834","DOI":"10.1109/CVPRW63382.2024.00735","CorpusId":265506663},"title":"ART•V: Auto-Regressive Text-to-Video Generation with Diffusion Models"},{"paperId":"4e9a8141da2a8c603722b07d096109207f8e0b66","externalIds":{"DBLP":"journals/corr/abs-2311-17982","ArXiv":"2311.17982","DOI":"10.1109/CVPR52733.2024.02060","CorpusId":265506207},"title":"VBench: Comprehensive Benchmark Suite for Video Generative Models"},{"paperId":"40626a059bcd8d3e7f364b410f831b9baf997b0c","externalIds":{"ArXiv":"2311.16933","DBLP":"conf/eccv/GuoYRALD24","DOI":"10.48550/arXiv.2311.16933","CorpusId":265466809},"title":"SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models"},{"paperId":"9d587b3f1d0608bf89fdb4ea6eed3312a73e938c","externalIds":{"DBLP":"conf/cvpr/XuZLYLZFS24","ArXiv":"2311.16498","DOI":"10.1109/CVPR52733.2024.00147","CorpusId":265466012},"title":"MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"8e37c6d2442ece49a80e63b65bcc4e9c0b49d580","externalIds":{"DBLP":"journals/corr/abs-2311-14294","ArXiv":"2311.14294","DOI":"10.48550/arXiv.2311.14294","CorpusId":265445082},"title":"Decouple Content and Motion for Conditional Image-to-Video Generation"},{"paperId":"287ef7416d5184f3f760016bac7b862d59316343","externalIds":{"ArXiv":"2311.11325","DBLP":"journals/corr/abs-2311-11325","DOI":"10.48550/arXiv.2311.11325","CorpusId":265295148},"title":"MoVideo: Motion-Aware Video Generation with Diffusion Models"},{"paperId":"85b10400864187230714506412c85610c786b5c3","externalIds":{"ArXiv":"2311.10709","DBLP":"conf/eccv/GirdharSBDARSYPM24","DOI":"10.48550/arXiv.2311.10709","CorpusId":265281059},"title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning"},{"paperId":"9b86ce1bde87b304141641b49299f4d0f1f7ba1d","externalIds":{"ArXiv":"2311.04145","DBLP":"journals/corr/abs-2311-04145","DOI":"10.48550/arXiv.2311.04145","CorpusId":265043460},"title":"I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models"},{"paperId":"b020cac5955b48c22ac59fa74bc49f6e3260a637","externalIds":{"ArXiv":"2310.20700","DBLP":"conf/iclr/Chen0ZZMY0L0024","DOI":"10.48550/arXiv.2310.20700","CorpusId":264820183},"title":"SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction"},{"paperId":"dd0e507097b3c93f2b2db67e69da5f093fb75f60","externalIds":{"ArXiv":"2310.16764","DBLP":"journals/corr/abs-2310-16764","DOI":"10.48550/arXiv.2310.16764","CorpusId":264451756},"title":"ConvNets Match Vision Transformers at Scale"},{"paperId":"083bab4a967c2221d9f4da9110fe37d8ca679078","externalIds":{"DBLP":"conf/eccv/XingXZCYLLWSW24","ArXiv":"2310.12190","DOI":"10.48550/arXiv.2310.12190","CorpusId":264306292},"title":"DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","externalIds":{"ArXiv":"2310.10647","DBLP":"journals/corr/abs-2310-10647","DOI":"10.1145/3696415","CorpusId":264172934},"title":"A Survey on Video Diffusion Models"},{"paperId":"985f0c89c5a607742ec43c1fdc2cbfe54541cbad","externalIds":{"ArXiv":"2310.05737","CorpusId":263830733},"title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation"},{"paperId":"2537ad7d72b089859119b8a614ec02358ec3ccb6","externalIds":{"DBLP":"conf/iccv/WangZCWS023","DOI":"10.1109/ICCV51070.2023.00110","CorpusId":267024015},"title":"SegGPT: Towards Segmenting Everything In Context"},{"paperId":"98478ac589e5b40a20630ff54bb4eec4ab4c5f6b","externalIds":{"ArXiv":"2309.17080","DBLP":"journals/corr/abs-2309-17080","DOI":"10.48550/arXiv.2309.17080","CorpusId":263310665},"title":"GAIA-1: A Generative World Model for Autonomous Driving"},{"paperId":"1ac0afeddeb8e4e600c6555d2aba765cbb8fee9c","externalIds":{"DBLP":"journals/corr/abs-2308-16876","ArXiv":"2308.16876","DOI":"10.48550/arXiv.2308.16876","CorpusId":261396217},"title":"SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation"},{"paperId":"6eb7aea517be123c089dca40c63dafabe18560b4","externalIds":{"DBLP":"journals/corr/abs-2308-16154","ArXiv":"2308.16154","DOI":"10.1109/ICCV51070.2023.00394","CorpusId":261339294},"title":"MMVP: Motion-Matrix-based Video Prediction"},{"paperId":"05cbac9a5101f47a6fabad72398616506572c9fa","externalIds":{"DBLP":"journals/corr/abs-2308-09592","ArXiv":"2308.09592","DOI":"10.1109/ICCV51070.2023.02106","CorpusId":261031087},"title":"StableVideo: Text-driven Consistency-aware Diffusion Video Editing"},{"paperId":"394aaa4b6343ec1a25fbf4693ac0caf479984a03","externalIds":{"ArXiv":"2308.08089","DBLP":"journals/corr/abs-2308-08089","DOI":"10.48550/arXiv.2308.08089","CorpusId":260925229},"title":"DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory"},{"paperId":"512fdb53eb341bc56769728274615c9bbe5bc23d","externalIds":{"ArXiv":"2307.08698","DBLP":"journals/corr/abs-2307-08698","DOI":"10.48550/arXiv.2307.08698","CorpusId":259936788},"title":"Flow Matching in Latent Space"},{"paperId":"89a8d24658d98370403d2775721dcc972bb06ed0","externalIds":{"DBLP":"journals/corr/abs-2306-11249","ArXiv":"2306.11249","DOI":"10.48550/arXiv.2306.11249","CorpusId":259203014},"title":"OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning"},{"paperId":"f02ea7a18f00859d9ea1b321e3385ae7d0170639","externalIds":{"DBLP":"journals/corr/abs-2306-02018","ArXiv":"2306.02018","DOI":"10.48550/arXiv.2306.02018","CorpusId":259075720},"title":"VideoComposer: Compositional Video Synthesis with Motion Controllability"},{"paperId":"76f07356f2e0d29b75ca075f0e22961aa74a6495","externalIds":{"DBLP":"conf/iclr/LuYFH00D24","ArXiv":"2305.13311","CorpusId":258832473},"title":"VDT: General-purpose Video Diffusion Transformers via Mask Modeling"},{"paperId":"63857fddb2a4bdf9d9cf88f6dd4eceaf09cadef1","externalIds":{"ArXiv":"2305.14343","DBLP":"journals/corr/abs-2305-14343","DOI":"10.48550/arXiv.2305.14343","CorpusId":258841355},"title":"Video Prediction Models as Rewards for Reinforcement Learning"},{"paperId":"43c93968ea54ea098879c548732272aa22ce04ab","externalIds":{"DBLP":"journals/ijcv/WangMCCDDQ25","ArXiv":"2305.03989","DOI":"10.1007/s11263-024-02231-3","CorpusId":258557986},"title":"LEO: Generative Latent Image Animator for Human Video Synthesis"},{"paperId":"f5a0c57f90c6abe31482e9f320ccac5ee789b135","externalIds":{"ArXiv":"2304.08818","DBLP":"journals/corr/abs-2304-08818","DOI":"10.1109/CVPR52729.2023.02161","CorpusId":258187553},"title":"Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"},{"paperId":"4fa122e7216a27745e5a6f1906669cba090cf2ee","externalIds":{"DBLP":"conf/cvpr/ParkK023","ArXiv":"2304.02225","DOI":"10.1109/CVPR52729.2023.00157","CorpusId":257952374},"title":"BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation"},{"paperId":"46a97c83626132db81602becab3379c1cc4edf44","externalIds":{"DBLP":"conf/iclr/GuWYS024","ArXiv":"2303.14897","DOI":"10.48550/arXiv.2303.14897","CorpusId":257766959},"title":"Seer: Language Instructed Video Prediction with Latent Diffusion Models"},{"paperId":"484d2194ce8459bfa9da906e556f63812c6ca999","externalIds":{"DBLP":"conf/cvpr/YuZJL0W23","ArXiv":"2303.14717","DOI":"10.1109/CVPR52729.2023.01422","CorpusId":257767123},"title":"CelebV-Text: A Large-Scale Facial Text-Video Dataset"},{"paperId":"b8b5015b153709176385873e34339f9e520d128f","externalIds":{"DBLP":"journals/corr/abs-2303-13744","ArXiv":"2303.13744","DOI":"10.1109/CVPR52729.2023.01769","CorpusId":257757116},"title":"Conditional Image-to-Video Generation with Latent Flow Diffusion Models"},{"paperId":"b1b890d107c8c2b60d846029ca4918a41219cfc3","externalIds":{"DBLP":"conf/cvpr/HuHHX023","ArXiv":"2303.09875","DOI":"10.1109/CVPR52729.2023.00593","CorpusId":257623041},"title":"A Dynamic Multi-Scale Voxel Flow Network for Video Prediction"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"5ac7ee33baa42ced66d6f906361429deeb9e6aba","externalIds":{"DBLP":"journals/corr/abs-2303-03684","ArXiv":"2303.03684","DOI":"10.1109/CVPR52729.2023.01796","CorpusId":257378578},"title":"MOSO: Decomposing MOtion, Scene and Object for Video Prediction"},{"paperId":"3437714fe565dd4366ad34431dbd4edecd418ab3","externalIds":{"ArXiv":"2302.11850","DBLP":"conf/icip/VillarCorralesWB23","DOI":"10.1109/ICIP49359.2023.10222810","CorpusId":257102906},"title":"Object-Centric Video Prediction Via Decoupling of Object Dynamics and Interactions"},{"paperId":"f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e","externalIds":{"DBLP":"journals/corr/abs-2301-04104","ArXiv":"2301.04104","DOI":"10.48550/arXiv.2301.04104","CorpusId":255569874},"title":"Mastering Diverse Domains through World Models"},{"paperId":"b3ae12b77950b4cd45f1176e77096d48a58354df","externalIds":{"DBLP":"conf/wacv/LiuCYWCT23","DOI":"10.1109/WACV56688.2023.00571","CorpusId":256650182},"title":"Meta-Auxiliary Learning for Future Depth Prediction in Videos"},{"paperId":"736973165f98105fec3729b7db414ae4d80fcbeb","externalIds":{"DBLP":"journals/corr/abs-2212-09748","ArXiv":"2212.09748","DOI":"10.1109/ICCV51070.2023.00387","CorpusId":254854389},"title":"Scalable Diffusion Models with Transformers"},{"paperId":"fe34137e5cc07235eae65ce53a54cd226b9f8b23","externalIds":{"DBLP":"journals/corr/abs-2212-05199","ArXiv":"2212.05199","DOI":"10.1109/CVPR52729.2023.01008","CorpusId":254563906},"title":"MAGVIT: Masked Generative Video Transformer"},{"paperId":"9ceaeff7117965832f4c05fd6355d021862d0a82","externalIds":{"DBLP":"conf/cvpr/WangWCS023","ArXiv":"2212.02499","DOI":"10.1109/CVPR52729.2023.00660","CorpusId":254246343},"title":"Images Speak in Images: A Generalist Painter for In-Context Visual Learning"},{"paperId":"15a591ec89cf2548f405af6aee887074096ae435","externalIds":{"ArXiv":"2212.06026","DBLP":"journals/corr/abs-2212-06026","DOI":"10.1016/j.imavis.2022.104612","CorpusId":254564674},"title":"Video Prediction by Efficient Transformers"},{"paperId":"583e87b70cfee68f6c160abaaaf7f53c07beb92b","externalIds":{"DBLP":"conf/iccv/DavtyanSF23","ArXiv":"2211.14575","DOI":"10.1109/ICCV51070.2023.02126","CorpusId":254044658},"title":"Efficient Video Prediction via Sparsely Conditioned Flow Matching"},{"paperId":"f2db26387e696d12028d41663ed5c6e439794206","externalIds":{"ArXiv":"2211.12824","DBLP":"conf/cvpr/FuYZFSWB23","DOI":"10.1109/CVPR52729.2023.01029","CorpusId":253801972},"title":"Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation"},{"paperId":"6310ac90d660b3359df2c375670799d08d5db454","externalIds":{"DBLP":"journals/tmlr/SenANJ22","ArXiv":"2210.16579","DOI":"10.48550/arXiv.2210.16579","CorpusId":253223146},"title":"INR-V: A Continuous Representation Space for Video-based Generative Tasks"},{"paperId":"cb5e13ad829a67f5fc25c113a7d39bebb940f3f8","externalIds":{"DBLP":"conf/iclr/WuDGKG23","ArXiv":"2210.05861","DOI":"10.48550/arXiv.2210.05861","CorpusId":252846354},"title":"SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models"},{"paperId":"af68f10ab5078bfc519caae377c90ee6d9c504e9","externalIds":{"ArXiv":"2210.02747","DBLP":"journals/corr/abs-2210-02747","CorpusId":252734897},"title":"Flow Matching for Generative Modeling"},{"paperId":"aa509ec67f311cd09d109356f7fa37a40072aabb","externalIds":{"DBLP":"journals/corr/abs-2210-02399","ArXiv":"2210.02399","DOI":"10.48550/arXiv.2210.02399","CorpusId":252715594},"title":"Phenaki: Variable Length Video Generation From Open Domain Textual Description"},{"paperId":"1e33716e8820b867d5a8aaebab44c2d3135ea4ac","externalIds":{"DBLP":"conf/iclr/SingerPH00ZHYAG23","ArXiv":"2209.14792","CorpusId":252595919},"title":"Make-A-Video: Text-to-Video Generation without Text-Video Data"},{"paperId":"244054a4254a2147e43a3dad9c124b9b7eb4a04a","externalIds":{"DBLP":"journals/corr/abs-2209-03003","ArXiv":"2209.03003","DOI":"10.48550/arXiv.2209.03003","CorpusId":252111177},"title":"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow"},{"paperId":"62f189757d1044fbc3106331acde23767d596564","externalIds":{"ArXiv":"2208.01582","DBLP":"journals/corr/abs-2208-01582","DOI":"10.1109/CVPR52729.2023.00532","CorpusId":251252913},"title":"ViP3D: End-to-End Visual Trajectory Prediction via 3D Agent Queries"},{"paperId":"6c6c996ba34fc02d00bc945b8d5c49807a3a552c","externalIds":{"DBLP":"conf/eccv/ZhuWZJTZLL22","ArXiv":"2207.12393","DOI":"10.48550/arXiv.2207.12393","CorpusId":251040093},"title":"CelebV-HQ: A Large-Scale Video Facial Attributes Dataset"},{"paperId":"a930064dbb82dcf5bd91171c1fd93fc22c339727","externalIds":{"DBLP":"journals/corr/abs-2207-09814","ArXiv":"2207.09814","DOI":"10.48550/arXiv.2207.09814","CorpusId":250698723},"title":"NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis"},{"paperId":"44f6612c238297304331d6fe6aa4b4f909f1c6f0","externalIds":{"DBLP":"conf/icra/NayakantiAZGRS23","ArXiv":"2207.05844","DOI":"10.1109/ICRA48891.2023.10160609","CorpusId":250493056},"title":"Wayformer: Motion Forecasting via Simple & Efficient Attention Networks"},{"paperId":"65fc1f1c567801fee3788974e753cdbf934f07e9","externalIds":{"DBLP":"conf/nips/BakerAZHTEHSC22","ArXiv":"2206.11795","DOI":"10.48550/arXiv.2206.11795","CorpusId":249953673},"title":"Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos"},{"paperId":"07d3837fff7bc872def43f34a62864e753c10a7f","externalIds":{"DBLP":"journals/corr/abs-2206-05099","ArXiv":"2206.05099","DOI":"10.1109/CVPR52688.2022.00317","CorpusId":249605809},"title":"SimVP: Simpler yet Better Video Prediction"},{"paperId":"393a244df22768a6140530b1e3ca76c273e1b5b5","externalIds":{"DBLP":"conf/cvpr/0012WC22","ArXiv":"2206.13454","DOI":"10.1109/CVPR52688.2022.01729","CorpusId":248409450},"title":"Optimizing Video Prediction via Video Frame Interpolation"},{"paperId":"707bd332d2c21dc5eb1f02a52d4a0506199aae76","externalIds":{"ArXiv":"2205.15868","DBLP":"journals/corr/abs-2205-15868","DOI":"10.48550/arXiv.2205.15868","CorpusId":249209614},"title":"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers"},{"paperId":"805748eec6be59ae0cd92a48400a902b3b7ed8e6","externalIds":{"DBLP":"conf/nips/HarveyNMWW22","ArXiv":"2205.11495","DOI":"10.48550/arXiv.2205.11495","CorpusId":248986725},"title":"Flexible Diffusion Modeling of Long Videos"},{"paperId":"3890d82362d07064687a4b5e9024fc4c92921998","externalIds":{"DBLP":"journals/corr/abs-2205-09853","ArXiv":"2205.09853","DOI":"10.48550/arXiv.2205.09853","CorpusId":248965384},"title":"MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","externalIds":{"DBLP":"conf/nips/HoSGC0F22","ArXiv":"2204.03458","DOI":"10.48550/arXiv.2204.03458","CorpusId":248006185},"title":"Video Diffusion Models"},{"paperId":"d772fccbc28a007913f78a53346ea1eb3a47504b","externalIds":{"ArXiv":"2204.02393","DBLP":"conf/eccv/ZhangPZ22","DOI":"10.1007/978-3-031-19809-0_7","CorpusId":250626771},"title":"Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining"},{"paperId":"7bffc157b3b3626a3912a3b0ef74ce5904630fce","externalIds":{"DBLP":"journals/corr/abs-2203-16194","ArXiv":"2203.16194","DOI":"10.48550/arXiv.2203.16194","CorpusId":247792986},"title":"FlowFormer: A Transformer Architecture for Optical Flow"},{"paperId":"b54e6762eeaa1f5c03ead7b8b1bda462bc055676","externalIds":{"DBLP":"conf/eccv/SunHRRFF22","ArXiv":"2203.10712","DOI":"10.1007/978-3-031-20047-2_10","CorpusId":252090011},"title":"Disentangling Architecture and Training for Optical Flow"},{"paperId":"6d1da150b14414d426af5a68f45b36c46223b81c","externalIds":{"ArXiv":"2203.09303","DBLP":"conf/bmvc/Villar-Corrales22","CorpusId":253224044},"title":"MSPred: Video Prediction at Multiple Spatio-Temporal Scales with Hierarchical Recurrent Networks"},{"paperId":"1641774b55a471a23eb31b722ee05c2e032fec7a","externalIds":{"PubMedCentral":"10606505","DBLP":"journals/corr/abs-2203-09481","ArXiv":"2203.09481","DOI":"10.3390/e25101469","CorpusId":247519223,"PubMed":"37895590"},"title":"Diffusion Probabilistic Modeling for Video Generation"},{"paperId":"74393ce60e64238d5ed0ff9952255d17b3be7092","externalIds":{"DBLP":"journals/tmlr/KumarHKC22","ArXiv":"2203.04946","CorpusId":252118897},"title":"Do better ImageNet classifiers assess perceptual similarity better?"},{"paperId":"ef4741b12cb8f01bcc68708c8cffcdc4237383f7","externalIds":{"ArXiv":"2202.10571","DBLP":"conf/iclr/YuTMKK0S22","CorpusId":247025714},"title":"Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks"},{"paperId":"d97e0adbade91d76b10e8790205a71877a9be42b","externalIds":{"DBLP":"journals/corr/abs-2112-14683","ArXiv":"2112.14683","DOI":"10.1109/CVPR52688.2022.00361","CorpusId":245537141},"title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"2805917375bf84ab06ad658af4ceaec85d4a5906","externalIds":{"ArXiv":"2112.10175","DBLP":"conf/ijcai/LiLQL23","DOI":"10.24963/ijcai.2023/121","CorpusId":245335065},"title":"On Efficient Transformer-Based Image Pre-training for Low-Level Vision"},{"paperId":"6296aa7cab06eaf058f7291040b320b5a83c0091","externalIds":{"DBLP":"journals/corr/abs-2203-00667","ArXiv":"2203.00667","DOI":"10.1109/ICCCNT56998.2023.10306417","CorpusId":1033682},"title":"Generative Adversarial Networks"},{"paperId":"9627e36c1896c7fa4a161dbf4045bc44a4e47d44","externalIds":{"DBLP":"conf/cvpr/MahapatraK22","ArXiv":"2112.03051","DOI":"10.1109/CVPR52688.2022.00365","CorpusId":244909354},"title":"Controllable Animation of Fluid Elements in Still Images"},{"paperId":"8b1602a911eb0f1753f22429952379a35c276a0f","externalIds":{"DBLP":"journals/corr/abs-2111-14973","ArXiv":"2111.14973","DOI":"10.1109/icra46639.2022.9812107","CorpusId":244730940},"title":"MultiPath++: Efficient Information Fusion and Trajectory Aggregation for Behavior Prediction"},{"paperId":"831e08f873e8d2ad25e5ef6d607d067a2bcbed88","externalIds":{"DBLP":"journals/corr/abs-2111-13817","ArXiv":"2111.13817","DOI":"10.1109/CVPR52688.2022.01696","CorpusId":244714646},"title":"Video Frame Interpolation Transformer"},{"paperId":"97bee918b08c244eb2e54d41e8ea6da00a3e5dbf","externalIds":{"DBLP":"conf/eccv/WuLJYFJD22","ArXiv":"2111.12417","DOI":"10.1007/978-3-031-19787-1_41","CorpusId":244527261},"title":"NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion"},{"paperId":"95805eb7ab0edcd05128cf0256feaea8e2497de9","externalIds":{"DBLP":"journals/corr/abs-2111-12594","ArXiv":"2111.12594","CorpusId":244527086},"title":"Conditional Object-Centric Learning from Video"},{"paperId":"6edf79d9064ac5f33a89333af67043f671a52115","externalIds":{"DBLP":"journals/pami/ChoiCBKL22","DOI":"10.1109/TPAMI.2021.3129819","CorpusId":244530051,"PubMed":"34813468"},"title":"Test-Time Adaptation for Video Frame Interpolation via Meta-Learning"},{"paperId":"e1a3e6856b6ac6af3600b5954392e5368603fd1b","externalIds":{"ArXiv":"2111.10337","DBLP":"conf/cvpr/XueHZS00FG22","DOI":"10.1109/CVPR52688.2022.00498","CorpusId":244462849},"title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","externalIds":{"DBLP":"conf/cvpr/HeCXLDG22","ArXiv":"2111.06377","DOI":"10.1109/CVPR52688.2022.01553","CorpusId":243985980},"title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"6bc2a1063c36ef873db05f75702fc7207bc5f229","externalIds":{"DBLP":"conf/mm/XueL0FLL21","ArXiv":"2109.02216","DOI":"10.1145/3474085.3475421","CorpusId":237420693},"title":"Learning Fine-Grained Motion Embedding for Landscape Animation"},{"paperId":"05c981d94e93f6b385bc0e793c3de7099a60befe","externalIds":{"DBLP":"journals/corr/abs-2108-02760","ArXiv":"2108.02760","DOI":"10.1109/ICCV48922.2021.01446","CorpusId":236924382},"title":"SLAMP: Stochastic Latent Appearance and Motion Prediction"},{"paperId":"247a9af5911e0a84162f133e5a674d317ccab333","externalIds":{"ArXiv":"2107.02790","DBLP":"conf/iccv/BlattmannMDO21","DOI":"10.1109/ICCV48922.2021.01444","CorpusId":235743021},"title":"iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","externalIds":{"ArXiv":"2106.04560","DBLP":"journals/corr/abs-2106-04560","DOI":"10.1109/CVPR52688.2022.01179","CorpusId":235367962},"title":"Scaling Vision Transformers"},{"paperId":"837d57006a94a308bbceff0481917fdd41094be9","externalIds":{"ArXiv":"2104.09762","DBLP":"conf/cvpr/Bei0S21","DOI":"10.1109/CVPR46437.2021.00096","CorpusId":233306997},"title":"Learning Semantic-Aware Dynamics for Video Prediction"},{"paperId":"df37e254a5e2d966680d9c4ef70b44d43a073eb9","externalIds":{"DBLP":"conf/cvpr/LeeKCKR21","ArXiv":"2104.00924","DOI":"10.1109/CVPR46437.2021.00307","CorpusId":233004334},"title":"Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"6ced62a77980c21eac494d49e4960a7b8dc36492","externalIds":{"DBLP":"conf/iccv/SimOK21","ArXiv":"2103.16206","DOI":"10.1109/ICCV48922.2021.01422","CorpusId":232417393},"title":"XVFI: eXtreme Video Frame Interpolation"},{"paperId":"b6382a7351c0c595f91472ac71d3b2d87b3c4844","externalIds":{"DBLP":"journals/corr/abs-2103-15691","ArXiv":"2103.15691","DOI":"10.1109/ICCV48922.2021.00676","CorpusId":232417054},"title":"ViViT: A Video Vision Transformer"},{"paperId":"2e08702b428d7948052a05ccf20ed0ecb261aacc","externalIds":{"DBLP":"journals/corr/abs-2103-04174","ArXiv":"2103.04174","DOI":"10.1109/CVPR46437.2021.00235","CorpusId":232146635},"title":"Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction"},{"paperId":"e8b540f92ba947d6814e5abe3afda4da4d9d78f3","externalIds":{"ArXiv":"2103.01950","DBLP":"journals/corr/abs-2103-01950","CorpusId":232092596},"title":"Predicting Video with VQVAE"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"fa08b41ccdfc5d8771adfbc34c176fa237d4646c","externalIds":{"DBLP":"conf/icml/BertasiusWT21","ArXiv":"2102.05095","CorpusId":231861462},"title":"Is Space-Time Attention All You Need for Video Understanding?"},{"paperId":"47f7ec3d0a5e6e83b6768ece35206a94dc81919c","externalIds":{"ArXiv":"2012.09841","MAG":"3111551570","DBLP":"journals/corr/abs-2012-09841","DOI":"10.1109/CVPR46437.2021.01268","CorpusId":229297973},"title":"Taming Transformers for High-Resolution Image Synthesis"},{"paperId":"26b8ed7c0527715e6117cf6dd02b82fb6e0e872e","externalIds":{"DBLP":"conf/wacv/KalluriPCT23","MAG":"3111283122","ArXiv":"2012.08512","DOI":"10.1109/WACV56688.2023.00211","CorpusId":229181044},"title":"FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation"},{"paperId":"6f6f73e69ee0d9d5d7d088bb882db1851d98175a","externalIds":{"DBLP":"conf/cvpr/Chen000DLMX0021","ArXiv":"2012.00364","MAG":"3109319753","DOI":"10.1109/CVPR46437.2021.01212","CorpusId":227239228},"title":"Pre-Trained Image Processing Transformer"},{"paperId":"eec8febf89785f00b894bb8d69f44118b86884e2","externalIds":{"DBLP":"conf/eccv/HuangZHSZ22","ArXiv":"2011.06294","DOI":"10.1007/978-3-031-19781-9_36","CorpusId":244499996},"title":"Real-Time Intermediate Flow Estimation for Video Frame Interpolation"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"b44bb1762640ed72091fd5f5fdc20719a6dc24af","externalIds":{"ArXiv":"2010.02193","DBLP":"journals/corr/abs-2010-02193","MAG":"3091507139","CorpusId":222133157},"title":"Mastering Atari with Discrete World Models"},{"paperId":"4c722aa398bdbd3a26a7c56b6442f2ede1332a14","externalIds":{"ArXiv":"2008.04776","CorpusId":245329900},"title":"DTVNet+: A High-Resolution Scenic Dataset for Dynamic Time-lapse Video Generation"},{"paperId":"9f28b8965e0e1dab3c38ea6c95a6a3ad60d83785","externalIds":{"ArXiv":"2007.00095","DBLP":"journals/corr/abs-2007-00095","MAG":"3039524844","CorpusId":220280348},"title":"Deep Learning for Vision-based Prediction: A Survey"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"ef25fe928b5a9f53296a938339dff5aeffcf6e7b","externalIds":{"DBLP":"conf/visapp/RakhimovVAZB21","ArXiv":"2006.10704","MAG":"3036033610","DOI":"10.5220/0010241801010112","CorpusId":219793091},"title":"Latent Video Transformer"},{"paperId":"db79a9a20acb3c15256655d52bb40e8831bb2345","externalIds":{"MAG":"3037838967","DBLP":"journals/pami/ChengC22","ArXiv":"2006.08070","DOI":"10.1109/TPAMI.2021.3100714","CorpusId":219686942,"PubMed":"34324423"},"title":"Multiple Video Frame Interpolation via Enhanced Deformable Separable Convolution"},{"paperId":"29858b40a15704398aecdca6bd2820f2fcc99891","externalIds":{"ArXiv":"2006.06676","DBLP":"conf/nips/KarrasAHLLA20","MAG":"3034720584","CorpusId":219636053},"title":"Training Generative Adversarial Networks with Limited Data"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"883659bbb413083bff8adfedc16fcbfb626f57f2","externalIds":{"ArXiv":"2005.01056","MAG":"3034328879","DBLP":"journals/corr/abs-2005-01056","DOI":"10.1109/cvprw50498.2020.00254","CorpusId":218487110},"title":"NTIRE 2020 Challenge on Perceptual Extreme Super-Resolution: Methods and Results"},{"paperId":"66831f683141c11ed7e20b0f2e8b40700740c164","externalIds":{"MAG":"3015371781","ArXiv":"2004.14368","DBLP":"journals/corr/abs-2004-14368","DOI":"10.1109/ICASSP40776.2020.9053174","CorpusId":216522760},"title":"Vggsound: A Large-Scale Audio-Visual Dataset"},{"paperId":"75fcd6f9e0dc9bbc55e24e85aad9b10ed2c9915d","externalIds":{"DBLP":"journals/corr/abs-2004-05214","ArXiv":"2004.05214","MAG":"3015879326","DOI":"10.1109/TPAMI.2020.3045007","CorpusId":215745618,"PubMed":"33320810"},"title":"A Review on Deep Learning Techniques for Video Prediction"},{"paperId":"bb4a9650ca3946c70a7e92007cc61dc0dfd75522","externalIds":{"DBLP":"conf/aaai/ChoiKHXL20","MAG":"2997150500","DOI":"10.1609/AAAI.V34I07.6693","CorpusId":213099044},"title":"Channel Attention Is All You Need for Video Frame Interpolation"},{"paperId":"8bfd55a4b1defa21c9efbbbcae2d04ca9e3b8b97","externalIds":{"MAG":"3035540643","DBLP":"conf/cvpr/WuGPC20","ArXiv":"2004.00542","DOI":"10.1109/cvpr42600.2020.00558","CorpusId":214743452},"title":"Future Video Synthesis With Object Motion Prediction"},{"paperId":"3230e2d6b4671cc03974af2219c6d3270e6fac70","externalIds":{"MAG":"3013965544","DBLP":"conf/ijcai/Teed021","ArXiv":"2003.12039","DOI":"10.1007/978-3-030-58536-5_24","CorpusId":214667893},"title":"RAFT: Recurrent All-Pairs Field Transforms for Optical Flow"},{"paperId":"f93501f123b38f80413dca335438c44eac61b7a3","externalIds":{"DBLP":"conf/eccv/HuCMGK20","MAG":"3012169209","ArXiv":"2003.06409","DOI":"10.1007/978-3-030-58517-4_45","CorpusId":212718089},"title":"Probabilistic Future Prediction for Video Scene Understanding"},{"paperId":"82da38e2ddd8aebbc13d9e4505bc86ad83c0d6da","externalIds":{"MAG":"3011611643","ArXiv":"2003.05534","DBLP":"conf/cvpr/Niklaus020","DOI":"10.1109/CVPR42600.2020.00548","CorpusId":212675709},"title":"Softmax Splatting for Video Frame Interpolation"},{"paperId":"603a0764092fbda01f3414071ea2813c49e1efa3","externalIds":{"ArXiv":"2003.01460","DBLP":"journals/corr/abs-2003-01460","MAG":"3034426027","DOI":"10.1109/cvpr42600.2020.01149","CorpusId":211818177},"title":"Disentangling Physical Dynamics From Unknown Factors for Unsupervised Video Prediction"},{"paperId":"95e96506dd17ef781f4a31dfb36b73bd53b9e499","externalIds":{"ArXiv":"2002.09219","DBLP":"journals/corr/abs-2002-09219","MAG":"3035268100","CorpusId":211252522},"title":"Stochastic Latent Residual Video Prediction"},{"paperId":"270b95262f2100c279c4c65ef2521473841ca8ad","externalIds":{"ArXiv":"1912.01603","DBLP":"conf/iclr/HafnerLB020","MAG":"2992977009","CorpusId":208547755},"title":"Dream to Control: Learning Behaviors by Latent Imagination"},{"paperId":"3e519d85cdcefdd1d2ad89829d6ad445695d8c58","externalIds":{"DBLP":"journals/corr/abs-1910-11215","ArXiv":"1910.11215","MAG":"2981626359","CorpusId":204851795},"title":"RoboNet: Large-Scale Multi-Robot Learning"},{"paperId":"cf65c7310c66489642a72823996eb08ac5a592d5","externalIds":{"DBLP":"journals/corr/abs-1908-08522","ArXiv":"1908.08522","MAG":"2991327061","DOI":"10.1109/ICCV.2019.01045","CorpusId":201316548},"title":"Compositional Video Prediction"},{"paperId":"84c0528cb2aa4bdacad989b5b43441161dd4ecda","externalIds":{"ArXiv":"1907.06987","DBLP":"journals/corr/abs-1907-06987","MAG":"2961193895","CorpusId":196831809},"title":"A Short Note on the Kinetics-700 Human Action Dataset"},{"paperId":"9311779489e597315488749ee6c386bfa3f3512e","externalIds":{"DBLP":"journals/corr/abs-1906-03327","ArXiv":"1906.03327","MAG":"2948859046","DOI":"10.1109/ICCV.2019.00272","CorpusId":182952863},"title":"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"},{"paperId":"e763fdc9ae56826ff799163ea035b29bffd8ea6f","externalIds":{"MAG":"2948412951","DBLP":"conf/iclr/WeissenbornTU20","ArXiv":"1906.02634","CorpusId":174802916},"title":"Scaling Autoregressive Video Models"},{"paperId":"f373bccc69bec811fa93b27d59a560b9e4ed0946","externalIds":{"DBLP":"conf/cvpr/KwonP19","MAG":"2966687987","DOI":"10.1109/CVPR.2019.00191","CorpusId":198118728},"title":"Predicting Future Frames Using Retrospective Cycle GAN"},{"paperId":"08a80e33fc646482b9fedc9f153238d960d670e5","externalIds":{"DBLP":"journals/corr/abs-1904-12165","MAG":"2991019415","ArXiv":"1904.12165","DOI":"10.1109/ICCV.2019.00770","CorpusId":139103210},"title":"Improved Conditional VRNNs for Video Prediction"},{"paperId":"85e3a8a6c46788bd01ded879bcca8f8eed0ad4f7","externalIds":{"ArXiv":"1904.01975","DBLP":"journals/corr/abs-1904-01975","MAG":"2933013660","CorpusId":102486513},"title":"D2-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"c165003060eeb01e05800a5ee4cd327f1e0bf5e3","externalIds":{"MAG":"2894961607","DBLP":"conf/eccv/RedaLSKBTTC18","ArXiv":"1811.00684","DOI":"10.1007/978-3-030-01234-2_44","CorpusId":52958347},"title":"SDC-Net: Video Prediction Using Spatially-Displaced Convolution"},{"paperId":"efa2cb68f619dea3770115884d6a5826f7117654","externalIds":{"MAG":"2894846593","DBLP":"conf/eccv/GuiWRM18","DOI":"10.1007/978-3-030-01237-3_27","CorpusId":52954281},"title":"Few-Shot Human Motion Prediction via Meta-learning"},{"paperId":"41cca0b0a27ba363ca56e7033569aeb1922b0ac9","externalIds":{"MAG":"2953072278","DBLP":"journals/corr/abs-1809-01999","ArXiv":"1809.01999","CorpusId":52171619},"title":"Recurrent World Models Facilitate Policy Evolution"},{"paperId":"c5b55f410365bb889c25a9f0354f2b86ec61c4f0","externalIds":{"MAG":"2963841322","ArXiv":"1808.06601","DBLP":"conf/nips/Wang0ZYTKC18","CorpusId":52049245},"title":"Video-to-Video Synthesis"},{"paperId":"5ec868ebe59918f94140bb2889b9027c55c09b65","externalIds":{"ArXiv":"1807.02635","MAG":"2952595815","DBLP":"journals/corr/abs-1807-02635","CorpusId":49651450},"title":"Video Prediction with Appearance and Motion Conditions"},{"paperId":"28a2ce59931804c8d4fc87abb7d47f6d4a2d7478","externalIds":{"DBLP":"conf/ijcai/XuWLW18","MAG":"2808011586","DOI":"10.24963/ijcai.2018/408","CorpusId":51604519},"title":"PredCNN: Predictive Learning with Cascade Convolutions"},{"paperId":"025a44bf059aef8b9ee2e6ca598bebefc59a4a61","externalIds":{"DBLP":"journals/corr/abs-1806-11230","MAG":"2810685774","ArXiv":"1806.11230","DOI":"10.1007/s11263-022-01594-9","CorpusId":49551723},"title":"Human Action Recognition and Prediction: A Survey"},{"paperId":"e65c2b0feddfe4c89e9955ca9b5ece6ef416628f","externalIds":{"DBLP":"conf/cvpr/YuCWXCLMD20","ArXiv":"1805.04687","MAG":"3016101116","DOI":"10.1109/cvpr42600.2020.00271","CorpusId":215415900},"title":"BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning"},{"paperId":"efb3e007d487b968b948b05ae9ce032de791d42f","externalIds":{"DBLP":"conf/eccv/GrothFPV18","MAG":"2799140957","ArXiv":"1804.08018","DOI":"10.1007/978-3-030-01246-5_43","CorpusId":5047070},"title":"ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking"},{"paperId":"9f67271a1edea3bf80c64c2d54e2a0a57612a567","externalIds":{"MAG":"2796303840","ArXiv":"1804.01523","DBLP":"journals/corr/abs-1804-01523","CorpusId":4591836},"title":"Stochastic Adversarial Video Prediction"},{"paperId":"ff332c21562c87cab5891d495b7d0956f2d9228b","externalIds":{"MAG":"2795843265","ArXiv":"1803.10122","DBLP":"journals/corr/abs-1803-10122","DOI":"10.5281/zenodo.1207631","CorpusId":4807711},"title":"World Models"},{"paperId":"de3b9eb697feed3d097e3f671afe395f48c1ab76","externalIds":{"MAG":"2952558689","ArXiv":"1802.07687","DBLP":"journals/corr/abs-1802-07687","CorpusId":3663219},"title":"Stochastic Video Generation with a Learned Prior"},{"paperId":"c468bbde6a22d961829e1970e6ad5795e05418d1","externalIds":{"ArXiv":"1801.03924","MAG":"2783879794","DBLP":"journals/corr/abs-1801-03924","DOI":"10.1109/CVPR.2018.00068","CorpusId":4766599},"title":"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"},{"paperId":"8a6acba7fb2aad1299fcf35701417e063d410ed4","externalIds":{"ArXiv":"1712.09867","MAG":"2777556841","DBLP":"conf/cvpr/LiuLLG18","DOI":"10.1109/CVPR.2018.00684","CorpusId":3865699},"title":"Future Frame Prediction for Anomaly Detection - A New Baseline"},{"paperId":"566d9637c465e4dfc1c1695ecc7155adbb54f45c","externalIds":{"DBLP":"conf/nips/WangLWGY17","MAG":"2768975186","CorpusId":4965597},"title":"PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs"},{"paperId":"baa1ae74fbf7ed6204f2f6364d51375ff81aabc1","externalIds":{"MAG":"2951724164","DBLP":"journals/corr/abs-1712-00080","ArXiv":"1712.00080","DOI":"10.1109/CVPR.2018.00938","CorpusId":10817557},"title":"Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation"},{"paperId":"c1045435c208a20f65b79baaa2d79783c2409c09","externalIds":{"MAG":"2962789486","DBLP":"journals/corr/abs-1711-09078","ArXiv":"1711.09078","DOI":"10.1007/s11263-018-01144-2","CorpusId":40412298},"title":"Video Enhancement with Task-Oriented Flow"},{"paperId":"a35fd89c4ea33d741b0ce0cf17bfb2e750aea51e","externalIds":{"MAG":"3102937540","DBLP":"conf/eccv/CaiBTT18","ArXiv":"1711.08682","DOI":"10.1007/978-3-030-01216-8_23","CorpusId":22236738},"title":"Deep Video Generation, Prediction and Completion of Human Action Sequences"},{"paperId":"775bf115923037ab2515b17ce31dd203a1f4575c","externalIds":{"MAG":"2768814045","ArXiv":"1711.06077","DBLP":"journals/corr/abs-1711-06077","DOI":"10.1109/CVPR.2018.00652","CorpusId":215763824},"title":"The Perception-Distortion Tradeoff"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"59d86da5c5936e7a236678bf5eaaa7753c226fb1","externalIds":{"DBLP":"conf/iclr/BabaeizadehFECL18","MAG":"2906780586","ArXiv":"1710.11252","CorpusId":9128667},"title":"Stochastic Variational Video Prediction"},{"paperId":"87a818723a2ada66a1193baf17b0383d9766781b","externalIds":{"MAG":"2950466956","ArXiv":"1709.07592","DBLP":"journals/corr/abs-1709-07592","DOI":"10.1109/CVPR.2018.00251","CorpusId":1504491},"title":"Learning to Generate Time-Lapse Videos Using Multi-stage Dynamic Generative Adversarial Networks"},{"paperId":"ed74b9390eda908060fa3501b8f20a836ec98d63","externalIds":{"MAG":"2964251418","DBLP":"conf/iccv/NiklausML17","ArXiv":"1708.01692","DOI":"10.1109/ICCV.2017.37","CorpusId":13697803},"title":"Video Frame Interpolation via Adaptive Separable Convolution"},{"paperId":"ee909ad489244016cf301bb7d7d8eeea423dbf35","externalIds":{"MAG":"2963017553","DBLP":"conf/iccv/HendricksWSSDR17","ArXiv":"1708.01641","DOI":"10.1109/ICCV.2017.618","CorpusId":1061352},"title":"Localizing Moments in Video with Natural Language"},{"paperId":"e76edb86f270c3a77ed9f5a1e1b305461f36f96f","externalIds":{"MAG":"2951910147","DBLP":"conf/cvpr/Tulyakov0YK18","ArXiv":"1707.04993","DOI":"10.1109/CVPR.2018.00165","CorpusId":4475365},"title":"MoCoGAN: Decomposing Motion and Content for Video Generation"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","externalIds":{"MAG":"2963981733","DBLP":"conf/nips/HeuselRUNH17","CorpusId":326772},"title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"b68811a9b5cafe4795a11c1048541750068b7ad0","externalIds":{"MAG":"2949901290","ArXiv":"1706.04261","DBLP":"conf/iccv/GoyalKMMWKHFYMH17","DOI":"10.1109/ICCV.2017.622","CorpusId":834612},"title":"The “Something Something” Video Database for Learning and Evaluating Visual Common Sense"},{"paperId":"051bc84485f0d9be02003e27e0ca67e9a06f2e4d","externalIds":{"DBLP":"conf/nips/ShiGL0YWW17","MAG":"2963866658","ArXiv":"1706.03458","CorpusId":25015381},"title":"Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model"},{"paperId":"1a8d3ad2b400bcebc9f17b309901ca5ef2e95315","externalIds":{"MAG":"2963045453","DBLP":"conf/nips/DentonB17","ArXiv":"1705.10915","CorpusId":13349803},"title":"Unsupervised Learning of Disentangled Representations from Video"},{"paperId":"b61a3f8b80bbd44f24544dc915f52fd30bbdf485","externalIds":{"ArXiv":"1705.07750","MAG":"2619082050","DBLP":"conf/cvpr/CarreiraZ17","DOI":"10.1109/CVPR.2017.502","CorpusId":206596127},"title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"},{"paperId":"3df8cc0384814c3fb05c44e494ced947a7d43f36","externalIds":{"MAG":"2963669520","ArXiv":"1705.00053","DBLP":"journals/corr/WalkerMGH17","DOI":"10.1109/ICCV.2017.361","CorpusId":3370894},"title":"The Pose Knows: Video Forecasting by Generating Pose Futures"},{"paperId":"f230cacc511b17b491bf3d90015bbbf85b9ef6af","externalIds":{"DBLP":"conf/icml/VillegasYZSLL17","MAG":"2952315244","ArXiv":"1704.05831","CorpusId":15117981},"title":"Learning to Generate Long-term Future via Hierarchical Prediction"},{"paperId":"566a2ede36a6493010ea42a7df49916739e00c9d","externalIds":{"ArXiv":"1704.04086","MAG":"2964337551","DBLP":"conf/iccv/HuangZLH17","DOI":"10.1109/ICCV.2017.267","CorpusId":13058841},"title":"Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis"},{"paperId":"49e8fec24cce8b73706bc5fcd2c3f681addb9982","externalIds":{"ArXiv":"1704.00675","MAG":"2916743882","DBLP":"journals/corr/Pont-TusetPCASG17","CorpusId":3619941},"title":"The 2017 DAVIS Challenge on Video Object Segmentation"},{"paperId":"516668a41d6106232a7cd56d20d3b3da343e5f36","externalIds":{"DBLP":"conf/iccv/LucNCVL17","MAG":"2601686579","ArXiv":"1703.07684","DOI":"10.1109/ICCV.2017.77","CorpusId":3073351},"title":"Predicting Deeper into the Future of Semantic Segmentation"},{"paperId":"5aaf0f7c8020e7212dc1ddaddf276e0516693b16","externalIds":{"DBLP":"conf/iccv/LiuYTLA17","MAG":"2950813077","ArXiv":"1702.02463","DOI":"10.1109/ICCV.2017.478","CorpusId":9207762},"title":"Video Frame Synthesis Using Deep Voxel Flow"},{"paperId":"8f92b4ea04758df2acfb49bd46a4cde923c3ddcb","externalIds":{"MAG":"2953317238","ArXiv":"1610.00696","DBLP":"conf/icra/FinnL17","DOI":"10.1109/ICRA.2017.7989324","CorpusId":2780699},"title":"Deep visual foresight for planning robot motion"},{"paperId":"b01871c114b122340209562972ff515b86b16ccf","externalIds":{"MAG":"2963629403","DBLP":"journals/corr/KalchbrennerOSD16","ArXiv":"1610.00527","CorpusId":14278057},"title":"Video Pixel Networks"},{"paperId":"c9a1e8e1ba2913ef0bdf1c5eaaa1ac0a79be3716","externalIds":{"MAG":"2524365899","DBLP":"journals/corr/Abu-El-HaijaKLN16","ArXiv":"1609.08675","CorpusId":11241677},"title":"YouTube-8M: A Large-Scale Video Classification Benchmark"},{"paperId":"397e9b56e46d3cc34af1525493e597facb104570","externalIds":{"MAG":"2963547393","DBLP":"journals/corr/XueWBF16","ArXiv":"1607.02586","CorpusId":12712095},"title":"Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks"},{"paperId":"571b0750085ae3d939525e62af510ee2cee9d5ea","externalIds":{"DBLP":"conf/nips/SalimansGZCRCC16","ArXiv":"1606.03498","MAG":"2949938177","CorpusId":1687220},"title":"Improved Techniques for Training GANs"},{"paperId":"b8e2e9f3ba008e28257195ec69a00e07f260131d","externalIds":{"DBLP":"conf/cvpr/XuMYR16","MAG":"2425121537","DOI":"10.1109/CVPR.2016.571","CorpusId":206594535},"title":"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"},{"paperId":"ad367b44f3434b9ba6b46b41ab083210f6827a9f","externalIds":{"MAG":"2964151830","DBLP":"journals/corr/LotterKC16","ArXiv":"1605.08104","CorpusId":71638},"title":"Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning"},{"paperId":"f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb","externalIds":{"MAG":"2953052133","DBLP":"conf/nips/FinnGL16","ArXiv":"1605.07157","CorpusId":2659157},"title":"Unsupervised Learning for Physical Interaction through Video Prediction"},{"paperId":"c8c494ee5488fe20e0aa01bddf3fc4632086d654","externalIds":{"DBLP":"journals/corr/CordtsORREBFRS16","MAG":"2953139137","ArXiv":"1604.01685","DOI":"10.1109/CVPR.2016.350","CorpusId":502946},"title":"The Cityscapes Dataset for Semantic Urban Scene Understanding"},{"paperId":"915c4bb289b3642489e904c65a47fa56efb60658","externalIds":{"MAG":"2331128040","DBLP":"journals/corr/JohnsonAL16","ArXiv":"1603.08155","DOI":"10.1007/978-3-319-46475-6_43","CorpusId":980236},"title":"Perceptual Losses for Real-Time Style Transfer and Super-Resolution"},{"paperId":"9179e740dad4ca4c183f7677b854e5b15f9a122f","externalIds":{"DBLP":"journals/corr/DosovitskiyB16","ArXiv":"1602.02644","MAG":"2963174698","CorpusId":8758543},"title":"Generating Images with Perceptual Similarity Metrics based on Deep Networks"},{"paperId":"17fa1c2a24ba8f731c8b21f1244463bc4b465681","externalIds":{"MAG":"2949900324","DBLP":"journals/corr/MathieuCL15","ArXiv":"1511.05440","CorpusId":205514},"title":"Deep multi-scale video prediction beyond mean square error"},{"paperId":"f9c990b1b5724e50e5632b94fdb7484ece8a6ce7","externalIds":{"ArXiv":"1506.04214","DBLP":"conf/nips/ShiCWYWW15","MAG":"2953118818","CorpusId":6352419},"title":"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d","externalIds":{"MAG":"2116435618","DBLP":"journals/corr/SrivastavaMS15","ArXiv":"1502.04681","CorpusId":11699847},"title":"Unsupervised Learning of Video Representations using LSTMs"},{"paperId":"6d4c9c923e9f145d1c01a2de2afc38ec23c44253","externalIds":{"MAG":"2308045930","DBLP":"conf/cvpr/KarpathyTSLSF14","DOI":"10.1109/CVPR.2014.223","CorpusId":206592218},"title":"Large-Scale Video Classification with Convolutional Neural Networks"},{"paperId":"f7636582c8abb648c4bb870b228f9ffbc6843c34","externalIds":{"MAG":"2137591992","DBLP":"conf/iccv/ZhangZD13","DOI":"10.1109/ICCV.2013.280","CorpusId":17297142},"title":"From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding"},{"paperId":"2cee43afc7aad6a8f7c4a6d566b60ebc79d57e7a","externalIds":{"MAG":"2034014085","DBLP":"conf/iccv/JhuangGZSB13","DOI":"10.1109/ICCV.2013.396","CorpusId":13000587},"title":"Towards Understanding Action Recognition"},{"paperId":"79b949d9b35c3f51dd20fb5c746cc81fc87147eb","externalIds":{"MAG":"2115579991","DBLP":"journals/ijrr/GeigerLSU13","DOI":"10.1177/0278364913491297","CorpusId":9455111},"title":"Vision meets robotics: The KITTI dataset"},{"paperId":"94078c96826e2a934b6ab4cbf84ccc188779640f","externalIds":{"MAG":"2004185490","DBLP":"conf/qomex/SongTZYX13","DOI":"10.1109/QoMEX.2013.6603201","CorpusId":21016627},"title":"The SJTU 4K video sequence dataset"},{"paperId":"da9e411fcf740569b6b356f330a1d0fc077c8d7c","externalIds":{"MAG":"24089286","ArXiv":"1212.0402","DBLP":"journals/corr/abs-1212-0402","CorpusId":7197134},"title":"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"34e0ba2daabfa4d3d22913ade8265aff50b5f917","externalIds":{"MAG":"2031454541","DBLP":"journals/pami/DollarWSP12","DOI":"10.1109/TPAMI.2011.155","CorpusId":206764948,"PubMed":"21808091"},"title":"Pedestrian Detection: An Evaluation of the State of the Art"},{"paperId":"8b3b8848a311c501e704c45c6d50430ab7068956","externalIds":{"MAG":"2126579184","DBLP":"conf/iccv/KuehneJGPS11","DOI":"10.1109/ICCV.2011.6126543","CorpusId":206769852},"title":"HMDB: A large video database for human motion recognition"},{"paperId":"b480f6a3750b4cebaf1db205692c8321d45926a2","externalIds":{"MAG":"2034328688","DBLP":"conf/icpr/SchuldtLC04","DOI":"10.1109/ICPR.2004.747","CorpusId":8777811},"title":"Recognizing human actions: a local SVM approach"},{"paperId":"eae2e0fa72e898c289365c0af16daf57a7a6cf40","externalIds":{"MAG":"2133665775","DBLP":"journals/tip/WangBSS04","DOI":"10.1109/TIP.2003.819861","CorpusId":207761262,"PubMed":"15376593"},"title":"Image quality assessment: from error visibility to structural similarity"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"319f22bd5abfd67ac15988aa5c7f705f018c3ccd","externalIds":{"MAG":"2154642048","DOI":"10.1016/B978-1-4832-1446-7.50035-2","CorpusId":62245742},"title":"Learning internal representations by error propagation"},{"paperId":"f329ae90bb7e6655b8ca3a19c4d06dbed7792d6f","externalIds":{"DBLP":"journals/corr/abs-2403-15377","DOI":"10.48550/arXiv.2403.15377","CorpusId":268667436},"title":"InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding"},{"paperId":"b57a0f46b4b5148b8bdd3cc2969d4ed43333bda0","externalIds":{"DBLP":"conf/eccv/LiangFZTGR24","DOI":"10.1007/978-3-031-72784-9_4","CorpusId":273232410},"title":"MoVideo: Motion-Aware Video Generation with Diffusion Model"},{"paperId":"281ca7826c2c9fe847b7777d9d3f865d27d7dbaf","externalIds":{"DBLP":"journals/corr/abs-2412-03758","DOI":"10.48550/arXiv.2412.03758","CorpusId":276618366},"title":"Advancing Auto-Regressive Continuation for Video Frames"},{"paperId":"418b5f9cef6af9674a40c691492b5ebe1744d75b","externalIds":{"DBLP":"conf/eccv/YoonKKJ24","DOI":"10.1007/978-3-031-72633-0_26","CorpusId":274435403},"title":"IAM-VFI : Interpolate Any Motion for Video Frame Interpolation with Motion Complexity Map"},{"paperId":"f9902e5b753dcbf7ed97ae6aa129cd402da61e18","externalIds":{"DBLP":"conf/l4dc/MassagueLGLSC23","CorpusId":259178242,"PubMed":"40893485"},"title":"Learning Object-Centric Dynamic Modes from Video and Emerging Properties"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","externalIds":{"ArXiv":"2103.14030","DBLP":"conf/iccv/LiuL00W0LG21","DOI":"10.1109/ICCV48922.2021.00986","CorpusId":232352874},"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"cd400ae50c0bed1fe47a7a07bbdcfb20ef5b5389","externalIds":{"MAG":"3015256491","DBLP":"journals/access/ZhouDE20","DOI":"10.1109/ACCESS.2020.2987281","CorpusId":216105210},"title":"Deep Learning in Next-Frame Prediction: A Benchmark Review"},{"paperId":"82a646e1cb33124f672beba451f5039e9e32fb6d","externalIds":{"MAG":"2908080566","DBLP":"conf/iclr/WangJYLLF19","CorpusId":86785011},"title":"Eidetic 3D LSTM: A Model for Video Prediction and Beyond"}]}