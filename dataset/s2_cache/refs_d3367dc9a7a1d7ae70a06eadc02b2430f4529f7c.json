{"references":[{"paperId":"117e4dbf933b8eda8629ec186a64a8cbfa85713c","externalIds":{"ArXiv":"2508.09061","DBLP":"journals/corr/abs-2508-09061","DOI":"10.48550/arXiv.2508.09061","CorpusId":280635093},"title":"VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception"},{"paperId":"a982333a27e449ba36860c4be64d0115065def08","externalIds":{"DBLP":"journals/corr/abs-2508-02095","ArXiv":"2508.02095","DOI":"10.48550/arXiv.2508.02095","CorpusId":279056794},"title":"VLM4D: Towards Spatiotemporal Awareness in Vision Language Models"},{"paperId":"39d9c3f1cd4bd5069713e50dc7301570575fc055","externalIds":{"DBLP":"journals/corr/abs-2507-06261","ArXiv":"2507.06261","CorpusId":280151524},"title":"Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities"},{"paperId":"725f8803dfe4551988fd4b51fdfa940340b4222e","externalIds":{"DBLP":"journals/corr/abs-2506-21539","ArXiv":"2506.21539","DOI":"10.48550/arXiv.2506.21539","CorpusId":280010695},"title":"WorldVLA: Towards Autoregressive Action World Model"},{"paperId":"d7f69aabc6d579935cf67f5ab27edf971d9daff6","externalIds":{"DBLP":"journals/corr/abs-2506-15096","ArXiv":"2506.15096","DOI":"10.48550/arXiv.2506.15096","CorpusId":279447777},"title":"DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory"},{"paperId":"989d24f3ab2c35b0698c42b800caab5246f6eb53","externalIds":{"DBLP":"journals/corr/abs-2506-13757","ArXiv":"2506.13757","DOI":"10.48550/arXiv.2506.13757","CorpusId":279410595},"title":"AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"},{"paperId":"8667e8a59b88586a7e2513334dae9ff84a15c731","externalIds":{"ArXiv":"2505.23450","DBLP":"journals/corr/abs-2505-23450","DOI":"10.48550/arXiv.2505.23450","CorpusId":278996166},"title":"Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents"},{"paperId":"a08764058571a3e1e01d5a6320c816bee0866851","externalIds":{"DBLP":"journals/corr/abs-2505-19381","ArXiv":"2505.19381","DOI":"10.48550/arXiv.2505.19381","CorpusId":278904380},"title":"DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving"},{"paperId":"c25b7f5f7c86eeeba5bd897fae216f5d0ac6c62d","externalIds":{"ArXiv":"2505.16278","DBLP":"journals/corr/abs-2505-16278","DOI":"10.48550/arXiv.2505.16278","CorpusId":278789416},"title":"DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving"},{"paperId":"a1d10dd3d0eb9d4f6c143976a13c4249dd59528e","externalIds":{"DBLP":"journals/corr/abs-2505-15659","ArXiv":"2505.15659","DOI":"10.48550/arXiv.2505.15659","CorpusId":278782778},"title":"FLARE: Robot Learning with Implicit World Modeling"},{"paperId":"dcb2087598da588f43d472b4a01daad5c68b194a","externalIds":{"DBLP":"journals/corr/abs-2505-11383","ArXiv":"2505.11383","DOI":"10.48550/arXiv.2505.11383","CorpusId":278714731},"title":"Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation"},{"paperId":"d2d84d56f730f81d276a02b48d5d44db5bde0b4a","externalIds":{"ArXiv":"2505.09388","DBLP":"journals/corr/abs-2505-09388","DOI":"10.48550/arXiv.2505.09388","CorpusId":278602855},"title":"Qwen3 Technical Report"},{"paperId":"0f30f667035352220102670c5b52f5133dfc579f","externalIds":{"DBLP":"journals/corr/abs-2505-05622","ArXiv":"2505.05622","DOI":"10.18653/v1/2025.acl-long.1511","CorpusId":278481201},"title":"CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory"},{"paperId":"d2089a5cca88a2efe8d33b9386bd460f786f5a60","externalIds":{"DOI":"10.1016/j.apmr.2025.03.011","CorpusId":278252750},"title":"Soft Wearable Robot that Provides Anti-Gravity Arm Support Improves Upper Limb Movement Quality in Individuals Post-Stroke"},{"paperId":"1e2a82ef5e0325a26ca0344c4f8c256c52fae7ec","externalIds":{"DBLP":"journals/corr/abs-2504-16054","ArXiv":"2504.16054","DOI":"10.48550/arXiv.2504.16054","CorpusId":277993634},"title":"π0.5: a Vision-Language-Action Model with Open-World Generalization"},{"paperId":"89c4124410976bc2473690eb5b54074d28668096","externalIds":{"DBLP":"journals/corr/abs-2504-08806","ArXiv":"2504.08806","DOI":"10.48550/arXiv.2504.08806","CorpusId":277780875},"title":"Endowing Embodied Agents with Spatial Reasoning Capabilities for Vision-and-Language Navigation"},{"paperId":"e90f31290cb2cba80abdaec108ebe764b2440a61","externalIds":{"DBLP":"journals/corr/abs-2503-22122","ArXiv":"2503.22122","DOI":"10.48550/arXiv.2503.22122","CorpusId":277435033},"title":"REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation"},{"paperId":"111581d79d42b3fd8dbb52ee1eed612d7a46a138","externalIds":{"ArXiv":"2503.19510","DBLP":"conf/rcar/LiWCWSM25","DOI":"10.1109/RCAR65431.2025.11139480","CorpusId":277313965},"title":"RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation"},{"paperId":"fb48d1326f632da1613a238439f9cf108a483f8b","externalIds":{"DBLP":"journals/corr/abs-2503-17309","ArXiv":"2503.17309","DOI":"10.48550/arXiv.2503.17309","CorpusId":277244500},"title":"LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language"},{"paperId":"731c50b0d6af4c1cb8d95f506541681ea487973b","externalIds":{"DBLP":"journals/corr/abs-2503-14734","ArXiv":"2503.14734","DOI":"10.48550/arXiv.2503.14734","CorpusId":277113335},"title":"GR00T N1: An Open Foundation Model for Generalist Humanoid Robots"},{"paperId":"1d72f5f6695bb270c3c12d06938228f53e24704f","externalIds":{"DBLP":"journals/corr/abs-2503-13966","ArXiv":"2503.13966","DOI":"10.1109/TMM.2025.3581809","CorpusId":277104725},"title":"FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks"},{"paperId":"01bde49e75eaa46baa2619fb239fdcde9dc47a31","externalIds":{"DBLP":"conf/cvpr/YinXZWZL25","ArXiv":"2503.10630","DOI":"10.1109/CVPR52734.2025.01775","CorpusId":276961636},"title":"UniGoal: Towards Universal Zero-shot Goal-oriented Navigation"},{"paperId":"ec2893ce63a5e3d22b63ca3c7ff7e62e4a11d91e","externalIds":{"ArXiv":"2503.09010","DBLP":"journals/corr/abs-2503-09010","DOI":"10.48550/arXiv.2503.09010","CorpusId":276938321},"title":"HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots"},{"paperId":"717a57ed620152adf823801802eeb421b9931b79","externalIds":{"ArXiv":"2503.08481","DBLP":"conf/cvpr/ZhouTZGD0W25","DOI":"10.1109/CVPR52734.2025.00651","CorpusId":276929115},"title":"PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability"},{"paperId":"72029ba479ffdd546f92e45314515742a8ebe5b8","externalIds":{"DBLP":"journals/corr/abs-2503-02247","ArXiv":"2503.02247","DOI":"10.1109/IROS60139.2025.11246684","CorpusId":276776282},"title":"WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation"},{"paperId":"d5a979c7c93b28c111387baae9efc652abcafb4e","externalIds":{"DBLP":"journals/corr/abs-2503-01616","ArXiv":"2503.01616","DOI":"10.1109/IROS60139.2025.11247714","CorpusId":276773952},"title":"RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation"},{"paperId":"1aa0ad840f5b5480cd3a804f0cec50cb7a549e04","externalIds":{"ArXiv":"2502.14254","DBLP":"journals/corr/abs-2502-14254","DOI":"10.48550/arXiv.2502.14254","CorpusId":276482497},"title":"Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation"},{"paperId":"10301766e5686bda76722ef2af1362213b934cc0","externalIds":{"ArXiv":"2501.15830","DBLP":"journals/corr/abs-2501-15830","DOI":"10.48550/arXiv.2501.15830","CorpusId":275921131},"title":"SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model"},{"paperId":"2eed1fad9bbf887d4395de40f20144c4fafefd7f","externalIds":{"PubMedCentral":"12443585","DBLP":"journals/nature/GuoYZSWZXZMBZY025","ArXiv":"2501.12948","DOI":"10.1038/s41586-025-09422-z","CorpusId":275789950,"PubMed":"40962978"},"title":"DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"},{"paperId":"3880f5bad862ba1b18f4f8ec060038b326b118ed","externalIds":{"ArXiv":"2501.09747","DBLP":"journals/corr/abs-2501-09747","DOI":"10.48550/arXiv.2501.09747","CorpusId":275570494},"title":"FAST: Efficient Action Tokenization for Vision-Language-Action Models"},{"paperId":"c37a42096449e9856a2ac025c419245afd2d2cf8","externalIds":{"DBLP":"conf/cvpr/PanZWZGD25","ArXiv":"2501.03841","DOI":"10.1109/CVPR52734.2025.01618","CorpusId":275342381},"title":"OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints"},{"paperId":"88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8","externalIds":{"ArXiv":"2412.15115","DBLP":"journals/corr/abs-2412-15115","DOI":"10.48550/arXiv.2412.15115","CorpusId":274859421},"title":"Qwen2.5 Technical Report"},{"paperId":"61c6cfb91ddb8246243d88a9cd988718e004f224","externalIds":{"DBLP":"journals/corr/abs-2412-09082","ArXiv":"2412.09082","DOI":"10.1109/CVPR52734.2025.01128","CorpusId":274656539},"title":"Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method"},{"paperId":"881370b510c86a86b20daae26074db050e7c017b","externalIds":{"DBLP":"journals/corr/abs-2412-04453","ArXiv":"2412.04453","DOI":"10.48550/arXiv.2412.04453","CorpusId":274514787},"title":"NaVILA: Legged Robot Vision-Language-Action Model for Navigation"},{"paperId":"6444cc06ba4556a2c81aeb463f9974515aac9347","externalIds":{"DBLP":"conf/cvpr/ZhengH025","ArXiv":"2412.00493","DOI":"10.1109/CVPR52734.2025.00841","CorpusId":274437503},"title":"Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding"},{"paperId":"0d8381d6cc2644805202a2b24af9ec740f42d797","externalIds":{"DBLP":"journals/corr/abs-2411-19626","ArXiv":"2411.19626","DOI":"10.1109/CVPR52734.2025.01615","CorpusId":274422942},"title":"GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding"},{"paperId":"54caf5927a6d656bb84d2a16e16fa2b0c5d6f522","externalIds":{"DBLP":"journals/corr/abs-2411-19650","ArXiv":"2411.19650","DOI":"10.48550/arXiv.2411.19650","CorpusId":274423241},"title":"CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation"},{"paperId":"b7366586ec4f3e3f31f3fac7df46fad758587566","externalIds":{"ArXiv":"2412.00171","DBLP":"journals/corr/abs-2412-00171","DOI":"10.48550/arXiv.2412.00171","CorpusId":274437637},"title":"RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World"},{"paperId":"ba8f1e36ca5f16a61d679f1ddf7b2cb3e298b354","externalIds":{"ArXiv":"2411.16537","DBLP":"journals/corr/abs-2411-16537","DOI":"10.1109/CVPR52734.2025.01470","CorpusId":274233932},"title":"RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics"},{"paperId":"7ec28caa2d1166d335065c6b46c04c2835983a88","externalIds":{"DBLP":"journals/corr/abs-2411-14869","ArXiv":"2411.14869","DOI":"10.1109/CVPR52734.2025.00842","CorpusId":274192366},"title":"BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence"},{"paperId":"7e7e59d2e247d99954081080ddd5aae93d10b9e0","externalIds":{"ArXiv":"2410.24164","DBLP":"journals/corr/abs-2410-24164","DOI":"10.48550/arXiv.2410.24164","CorpusId":273811174},"title":"π0: A Vision-Language-Action Flow Model for General Robot Control"},{"paperId":"faffd56c4024f552335a76639526d5d60e7a278b","externalIds":{"DBLP":"journals/corr/abs-2410-22662","ArXiv":"2410.22662","DOI":"10.48550/arXiv.2410.22662","CorpusId":273696063},"title":"EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents"},{"paperId":"8440692409f544c6eb3e4193fb391a1dc7e8be4d","externalIds":{"ArXiv":"2410.21845","DBLP":"journals/corr/abs-2410-21845","DOI":"10.48550/arXiv.2410.21845","CorpusId":273662453},"title":"Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning"},{"paperId":"4c590291dbbefa850d56864639a8fa4d2ada7b8b","externalIds":{"DBLP":"journals/corr/abs-2410-13979","ArXiv":"2410.13979","DOI":"10.1109/IROS60139.2025.11245856","CorpusId":273482439},"title":"RecoveryChaining: Learning Local Recovery Policies for Robust Manipulation"},{"paperId":"acd5445dfc6ff49622e5382fdb383472220d828f","externalIds":{"DBLP":"journals/corr/abs-2410-03290","ArXiv":"2410.03290","DOI":"10.48550/arXiv.2410.03290","CorpusId":273163176},"title":"Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models"},{"paperId":"a549f6fcec90bb5522d0a5a49bcb4f937a735d17","externalIds":{"DBLP":"journals/corr/abs-2410-03035","ArXiv":"2410.03035","DOI":"10.1109/ICRA55743.2025.11128238","CorpusId":273162481},"title":"SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments"},{"paperId":"11ff70627b58779220545bf4b15d4b70960839e8","externalIds":{"ArXiv":"2410.02742","DBLP":"journals/corr/abs-2410-02742","DOI":"10.48550/arXiv.2410.02742","CorpusId":273098065},"title":"Grounding Large Language Models In Embodied Environment With Imperfect World Models"},{"paperId":"076f35d58d26ac74c71e6e849dfffc0707aa8a6f","externalIds":{"DBLP":"conf/iclr/DuanPKWTYKFMG25","ArXiv":"2410.00371","DOI":"10.48550/arXiv.2410.00371","CorpusId":273022765},"title":"AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation"},{"paperId":"dd35664df802511a9246bd5bdec808a53c9164b0","externalIds":{"DBLP":"journals/corr/abs-2409-15146","ArXiv":"2409.15146","DOI":"10.1109/ICRA55743.2025.11127808","CorpusId":272827992},"title":"COHERENT: Collaboration of Heterogeneous Multi-Robot System with Large Language Models"},{"paperId":"dc62bc6536e9e3ad80242f10f44c046e4c7bd3d1","externalIds":{"ArXiv":"2409.12514","DBLP":"journals/corr/abs-2409-12514","DOI":"10.1109/LRA.2025.3544909","CorpusId":272753287},"title":"TinyVLA: Toward Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation"},{"paperId":"b75f344d962b3b25b64c0c96a86244c4c3c6e490","externalIds":{"DBLP":"journals/corr/abs-2407-10943","ArXiv":"2407.10943","DOI":"10.48550/arXiv.2407.10943","CorpusId":271212467},"title":"GRUtopia: Dream General Robots in a City at Scale"},{"paperId":"56b545002ebb897de167a2de1603f06ab8f6bb0a","externalIds":{"DBLP":"journals/corr/abs-2407-08693","ArXiv":"2407.08693","DOI":"10.48550/arXiv.2407.08693","CorpusId":271097636},"title":"Robotic Control via Embodied Chain-of-Thought Reasoning"},{"paperId":"440d7944dc701dbe4030d3fbad49036700fba788","externalIds":{"ArXiv":"2407.07726","DBLP":"journals/corr/abs-2407-07726","DOI":"10.48550/arXiv.2407.07726","CorpusId":271088378},"title":"PaliGemma: A versatile 3B VLM for transfer"},{"paperId":"1872b0a2ad3d44ca325ac80ccea5788c9b4a6574","externalIds":{"ArXiv":"2406.13807","DBLP":"journals/corr/abs-2406-13807","DOI":"10.48550/arXiv.2406.13807","CorpusId":270619444},"title":"AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding"},{"paperId":"c6045936879d7e31d36ca715a175d6547801d133","externalIds":{"DBLP":"conf/rss/LiuLYGL24","ArXiv":"2406.12224","DOI":"10.48550/arXiv.2406.12224","CorpusId":270562739},"title":"Leveraging Large Language Model for Heterogeneous Ad Hoc Teamwork Collaboration"},{"paperId":"2797cbda8c845504119b62ee25deb1500ec2dfaf","externalIds":{"ArXiv":"2406.11931","DBLP":"journals/corr/abs-2406-11931","DOI":"10.48550/arXiv.2406.11931","CorpusId":270562723},"title":"DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence"},{"paperId":"8f9ceb5ffad8e7a066dfc9d9aaa5153b714740ee","externalIds":{"DBLP":"journals/corr/abs-2406-09246","ArXiv":"2406.09246","DOI":"10.48550/arXiv.2406.09246","CorpusId":270440391},"title":"OpenVLA: An Open-Source Vision-Language-Action Model"},{"paperId":"a6192930521298c81d3452cf0fe0a49829efccef","externalIds":{"DBLP":"journals/nn/WangMCLCF24","DOI":"10.1016/j.neunet.2024.106472","CorpusId":270604945,"PubMed":"38936112"},"title":"GeneWorker: An end-to-end robotic reinforcement learning approach with collaborative generator and worker networks"},{"paperId":"e1b62c7ee4e22ab63e3b0c9968563e6675833e36","externalIds":{"ArXiv":"2405.14314","DBLP":"journals/corr/abs-2405-14314","DOI":"10.48550/arXiv.2405.14314","CorpusId":269982300},"title":"Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration"},{"paperId":"1d2753d74025e7a71594506623be81f18b073adb","externalIds":{"DBLP":"journals/corr/abs-2405-12213","ArXiv":"2405.12213","DOI":"10.48550/arXiv.2405.12213","CorpusId":266379116},"title":"Octo: An Open-Source Generalist Robot Policy"},{"paperId":"12693672913f00a32b31fe68a3d0d3d4c40cc352","externalIds":{"ArXiv":"2402.15852","DBLP":"journals/corr/abs-2402-15852","DOI":"10.48550/arXiv.2402.15852","CorpusId":267938569},"title":"NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation"},{"paperId":"5f69ca8747fecc743fa6d37027256fda9eac0d95","externalIds":{"DBLP":"conf/aaai/LiPSB24","ArXiv":"2402.03561","DOI":"10.48550/arXiv.2402.03561","CorpusId":267499651},"title":"VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation"},{"paperId":"35b142ea69598e6241f0011312128031df55895c","externalIds":{"ArXiv":"2402.03300","DBLP":"journals/corr/abs-2402-03300","DOI":"10.48550/arXiv.2402.03300","CorpusId":267412607},"title":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"},{"paperId":"46e32d4b037968a10a6a5f531731e4b514ae6cd2","externalIds":{"DBLP":"conf/cvpr/0001CBZTL22","ArXiv":"2401.06341","DOI":"10.1109/CVPRW63382.2024.00754","CorpusId":266977321},"title":"AffordanceLLM: Grounding Affordance from Vision Language Models"},{"paperId":"fc3819a50705fc3cf90ab92f2a206b858fef3b19","externalIds":{"ArXiv":"2401.02117","DBLP":"journals/corr/abs-2401-02117","DOI":"10.48550/arXiv.2401.02117","CorpusId":266755740},"title":"Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation"},{"paperId":"c0b3a44ff6b482a073138eb243da9b762eb1cd1f","externalIds":{"ArXiv":"2312.14457","DBLP":"conf/eccv/DingZZSZHYW24","DOI":"10.48550/arXiv.2312.14457","CorpusId":266520894},"title":"QUAR-VLA: Vision-Language-Action Model for Quadruped Robots"},{"paperId":"a727d8e834f779cebdc4e5b6d76659f864b3f5be","externalIds":{"DBLP":"conf/icpads/ZengGWY23","DOI":"10.1109/ICPADS60453.2023.00126","CorpusId":268725737},"title":"Distributed Training of Large Language Models"},{"paperId":"bc8d248fb86a3b6a285e8b9a6fe2c09e7f0b19c9","externalIds":{"ArXiv":"2311.13884","DBLP":"journals/corr/abs-2311-13884","DOI":"10.48550/arXiv.2311.13884","CorpusId":265445440},"title":"Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","externalIds":{"DBLP":"journals/corr/abs-2311-13165","ArXiv":"2311.13165","DOI":"10.1109/BigData59044.2023.10386743","CorpusId":265351653},"title":"Multimodal Large Language Models: A Survey"},{"paperId":"13d12b26db345f62e8e512db181b96a7f8763b47","externalIds":{"DBLP":"conf/icml/HuangYMLLW0ZJ024","ArXiv":"2311.12871","DOI":"10.48550/arXiv.2311.12871","CorpusId":265351495},"title":"An Embodied Generalist Agent in 3D World"},{"paperId":"fde266447d68b2728752a7835e8c97e1af316ac4","externalIds":{"DBLP":"journals/pami/WangCLJHZLHZYML25","ArXiv":"2311.05997","DOI":"10.1109/TPAMI.2024.3511593","CorpusId":265129059},"title":"JARVIS-1: Open-World Multi-Task Agents With Memory-Augmented Multimodal Language Models"},{"paperId":"857efca0d4cbece97da4b904de1d506c2af25f9c","externalIds":{"DBLP":"conf/iclr/LiLZYXWCJ0LLK24","ArXiv":"2311.01378","DOI":"10.48550/arXiv.2311.01378","CorpusId":264935429},"title":"Vision-Language Foundation Models as Effective Robot Imitators"},{"paperId":"2b787cd20fbd226746220f0e6e7cf9c4d4e4079b","externalIds":{"ArXiv":"2310.11604","DBLP":"journals/corr/abs-2310-11604","DOI":"10.1109/LRA.2024.3410155","CorpusId":264289016},"title":"Language Models as Zero-Shot Trajectory Generators"},{"paperId":"ef7d31137ef06c5be8c2824ecc5af6ce3358cc8f","externalIds":{"DBLP":"journals/corr/abs-2310-08864","ArXiv":"2310.08864","DOI":"10.1109/ICRA57147.2024.10611477","CorpusId":263626099},"title":"Open X-Embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration0"},{"paperId":"8c9e95f32982ca21a7e4ef9c986aaa934cb11293","externalIds":{"DBLP":"conf/nips/SontakkeZAPBSFI23","ArXiv":"2310.07899","DOI":"10.48550/arXiv.2310.07899","CorpusId":263909538},"title":"RoboCLIP: One Demonstration is Enough to Learn Robot Policies"},{"paperId":"c3d14e7a319ab764297a60112ce74af201762a73","externalIds":{"DBLP":"journals/corr/abs-2310-06114","ArXiv":"2310.06114","DOI":"10.48550/arXiv.2310.06114","CorpusId":263830899},"title":"Learning Interactive Real-World Simulators"},{"paperId":"1ad735714ad2e4ee5b94ce26c976e5ee5c7cde3b","externalIds":{"ArXiv":"2309.15943","DBLP":"journals/corr/abs-2309-15943","DOI":"10.1109/ICRA57147.2024.10610676","CorpusId":263143275},"title":"Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?"},{"paperId":"b1e36c125d0b0ef48bc55ae7b8a92c8afc40e300","externalIds":{"DBLP":"conf/icra/CaiHCLGS024","ArXiv":"2309.10309","DOI":"10.1109/ICRA57147.2024.10610499","CorpusId":262053943},"title":"Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill"},{"paperId":"bf89341e16fc0a379ab5e4c6370cf7ea4e9afd03","externalIds":{"DBLP":"journals/corr/abs-2309-10150","ArXiv":"2309.10150","DOI":"10.48550/arXiv.2309.10150","CorpusId":262054345},"title":"Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions"},{"paperId":"0c72450890a54b68d63baa99376131fda8f06cf9","externalIds":{"ArXiv":"2309.07864","DBLP":"journals/corr/abs-2309-07864","DOI":"10.48550/arXiv.2309.07864","CorpusId":261817592},"title":"The Rise and Potential of Large Language Model Based Agents: A Survey"},{"paperId":"148e95859248878a0695a31ef6165614a01df631","externalIds":{"ArXiv":"2309.01918","DBLP":"conf/icra/BharadhwajVSGTK24","DOI":"10.1109/ICRA57147.2024.10611293","CorpusId":261518421},"title":"RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking"},{"paperId":"e4bb1b1f97711a7634bf4bff72c56891be2222e6","externalIds":{"DBLP":"journals/corr/abs-2309-02427","ArXiv":"2309.02427","DOI":"10.48550/arXiv.2309.02427","CorpusId":261556862},"title":"Cognitive Architectures for Language Agents"},{"paperId":"316f980cfd2e217234386166a46eb080bf027cdd","externalIds":{"ArXiv":"2309.02561","DBLP":"conf/icra/GaoSXX0IMS24","DOI":"10.1109/ICRA57147.2024.10610090","CorpusId":261556939},"title":"Physically Grounded Vision-Language Models for Robotic Manipulation"},{"paperId":"b4115b608b934627bcdfb0750f0e5305ea7351f9","externalIds":{"ArXiv":"2308.01552","DBLP":"journals/corr/abs-2308-01552","DOI":"10.48550/arXiv.2308.01552","CorpusId":260438734},"title":"InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent"},{"paperId":"38939304bb760473141c2aca0305e44fbe04e6e8","externalIds":{"ArXiv":"2307.15818","DBLP":"conf/corl/ZitkovichYXXXXW23","DOI":"10.48550/arXiv.2307.15818","CorpusId":260293142},"title":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"},{"paperId":"6f1aab7b3ca0a487e6a57571b4ef9993e6325435","externalIds":{"DBLP":"conf/iccv/LinCHLTG23","ArXiv":"2307.11984","DOI":"10.1109/ICCV51070.2023.00764","CorpusId":260125181},"title":"Learning Vision-and-Language Navigation from YouTube Videos"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","externalIds":{"DBLP":"conf/corl/HuangWZL0023","ArXiv":"2307.05973","DOI":"10.48550/arXiv.2307.05973","CorpusId":259837330},"title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"d62c4d00b277e948956b6610ce2644e88fe1577b","externalIds":{"DBLP":"journals/cacm/Cerf23c","ArXiv":"2307.05782","DOI":"10.1007/978-981-96-6259-3","CorpusId":259837466,"PubMed":"38320147"},"title":"Large Language Models"},{"paperId":"c5d18dbb92d0cd5393baa1e69de33d6922ac3e57","externalIds":{"DBLP":"conf/icra/MandiJS24","ArXiv":"2307.04738","DOI":"10.1109/ICRA57147.2024.10610855","CorpusId":259501567},"title":"RoCo: Dialectic Multi-Robot Collaboration with Large Language Models"},{"paperId":"03251361c1d67c6b5badffc7059fdd7fbfea1fed","externalIds":{"DBLP":"journals/corr/abs-2306-17840","ArXiv":"2306.17840","DOI":"10.1109/ICRA57147.2024.10610634","CorpusId":259309028},"title":"Statler: State-Maintaining Language Models for Embodied Reasoning"},{"paperId":"d9823ffa34f865fb1d0adef95d64a0c352ae125f","externalIds":{"DBLP":"journals/corr/abs-2306-15724","ArXiv":"2306.15724","DOI":"10.48550/arXiv.2306.15724","CorpusId":259274760},"title":"REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"98cfd7b1b29453c4e82536f5afdc6ddc58bbb1b3","externalIds":{"DBLP":"journals/corr/abs-2306-03310","ArXiv":"2306.03310","DOI":"10.48550/arXiv.2306.03310","CorpusId":259089508},"title":"LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning"},{"paperId":"3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a","externalIds":{"ArXiv":"2305.18565","DBLP":"journals/corr/abs-2305-18565","DOI":"10.48550/arXiv.2305.18565","CorpusId":258967670},"title":"PaLI-X: On Scaling up a Multilingual Vision and Language Model"},{"paperId":"8199c9d55dd998f69f703e0ad250ca0697e3ad27","externalIds":{"DBLP":"conf/aaai/ZhouHW24","ArXiv":"2305.16986","DOI":"10.48550/arXiv.2305.16986","CorpusId":258947250},"title":"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models"},{"paperId":"c695c4e68561347564ea0daa50dc339dff73d8c5","externalIds":{"DBLP":"journals/corr/abs-2305-17144","ArXiv":"2305.17144","DOI":"10.48550/arXiv.2305.17144","CorpusId":258959262},"title":"Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory"},{"paperId":"5dbffedcabe3fa43060ebbe2b1789500edfd871f","externalIds":{"DBLP":"conf/emnlp/HaoGMHWWH23","ArXiv":"2305.14992","DOI":"10.48550/arXiv.2305.14992","CorpusId":258865812},"title":"Reasoning with Language Model is Planning with World Model"},{"paperId":"4780d0a027c5c5a8e01d7cf697f6296880ffc945","externalIds":{"ArXiv":"2305.14325","DBLP":"journals/corr/abs-2305-14325","DOI":"10.48550/arXiv.2305.14325","CorpusId":258841118},"title":"Improving Factuality and Reasoning in Language Models through Multiagent Debate"},{"paperId":"90027ca7802645671a69b00b65e1fa94e6b63544","externalIds":{"ArXiv":"2305.18323","DBLP":"journals/corr/abs-2305-18323","DOI":"10.48550/arXiv.2305.18323","CorpusId":258967566},"title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"},{"paperId":"9a9b1e2968302eb882870537d4af6e2c722dfd1a","externalIds":{"DBLP":"journals/corr/abs-2305-14497","ArXiv":"2305.14497","DOI":"10.48550/arXiv.2305.14497","CorpusId":258865576},"title":"Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"91eb20f923ea3b0246868902aef4e9bea572b800","externalIds":{"DBLP":"journals/corr/abs-2304-13705","ArXiv":"2304.13705","DOI":"10.48550/arXiv.2304.13705","CorpusId":258331658},"title":"Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","externalIds":{"DBLP":"journals/corr/abs-2304-08354","ArXiv":"2304.08354","DOI":"10.1145/3704435","CorpusId":258179336},"title":"Tool Learning with Foundation Models"},{"paperId":"253b41369d003952874c6a47a6038277b165cfa0","externalIds":{"DBLP":"journals/corr/abs-2304-08488","ArXiv":"2304.08488","DOI":"10.1109/CVPR52729.2023.01324","CorpusId":258180471},"title":"Affordances from Human Videos as a Versatile Representation for Robotics"},{"paperId":"148501704f8c218347ace30a45a5aa7bfa9c20b2","externalIds":{"DBLP":"journals/cvm/YangGXLPWTG25","ArXiv":"2304.06906","DOI":"10.48550/arXiv.2304.06906","CorpusId":258170015},"title":"Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"5278a8eb2ba2429d4029745caf4e661080073c81","externalIds":{"DBLP":"conf/uist/ParkOCMLB23","ArXiv":"2304.03442","DOI":"10.1145/3586183.3606763","CorpusId":258040990},"title":"Generative Agents: Interactive Simulacra of Human Behavior"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"7bf72a3b5fbac8bc0f461780810fbc781c28ef53","externalIds":{"DBLP":"conf/nips/LiHIKG23","ArXiv":"2303.17760","CorpusId":268042527},"title":"CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society"},{"paperId":"35aba190f28b5c39df333c06ca21f46bd4845eba","externalIds":{"DBLP":"journals/corr/abs-2303-15343","ArXiv":"2303.15343","DOI":"10.1109/ICCV51070.2023.01100","CorpusId":257767223},"title":"Sigmoid Loss for Language Image Pre-Training"},{"paperId":"c8f2aced926707fba8a0535a6df5b5823d394bac","externalIds":{"ArXiv":"2304.06632","DBLP":"journals/corr/abs-2304-06632","DOI":"10.48550/arXiv.2304.06632","CorpusId":258107993},"title":"AI-Generated Content (AIGC): A Survey"},{"paperId":"adfc3db2b4dd83fc81b613b76ca0617d9f7cf871","externalIds":{"ArXiv":"2304.06032","DBLP":"journals/corr/abs-2304-06032","DOI":"10.1145/3543873.3587583","CorpusId":258108184},"title":"Web 3.0: The Future of Internet"},{"paperId":"df79c20b2308ca945b70dcf5a8cdb96e42e0d996","externalIds":{"DOI":"10.1038/d41586-023-00816-5","CorpusId":257580633,"PubMed":"36928404"},"title":"GPT-4 is here: what scientists think"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"5d8a0a5c4a5ecdce0ff293d479803bc976ea2b0b","externalIds":{"ArXiv":"2303.00855","DBLP":"conf/nips/HuangXSDZLFMLHI23","CorpusId":266173978},"title":"Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"a68db57f08f6d72c0d3b22d451d2606dca880f94","externalIds":{"DBLP":"journals/corr/abs-2302-12422","ArXiv":"2302.12422","DOI":"10.48550/arXiv.2302.12422","CorpusId":257205825},"title":"MimicPlay: Long-Horizon Imitation Learning by Watching Human Play"},{"paperId":"3e9b41431543c7380088582042d50c70467f3999","externalIds":{"DBLP":"journals/intpolrev/HelbergerD23","DOI":"10.14763/2023.1.1682","CorpusId":257271887},"title":"ChatGPT and the AI Act"},{"paperId":"ccb1ccc4deacc4fb18000f0e1ce24329548963ae","externalIds":{"ArXiv":"2302.01560","DBLP":"journals/corr/abs-2302-01560","DOI":"10.48550/arXiv.2302.01560","CorpusId":256598146},"title":"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents"},{"paperId":"fc918d6f8e2523696c34fa1be5aabdb42e9648d2","externalIds":{"DBLP":"conf/icml/NottinghamAS0H023","ArXiv":"2301.12050","DOI":"10.48550/arXiv.2301.12050","CorpusId":256389514},"title":"Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","externalIds":{"ArXiv":"2212.10403","DBLP":"conf/acl/0009C23","DOI":"10.48550/arXiv.2212.10403","CorpusId":254877753},"title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"3cbffab9d7981da6662d474aaa056dcbd3c1701e","externalIds":{"ArXiv":"2212.09196","DBLP":"journals/corr/abs-2212-09196","DOI":"10.1038/s41562-023-01659-w","CorpusId":254854575,"PubMed":"37524930"},"title":"Emergent analogical reasoning in large language models"},{"paperId":"fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d","externalIds":{"ArXiv":"2212.06817","DBLP":"conf/rss/BrohanBCCDFGHHH23","DOI":"10.48550/arXiv.2212.06817","CorpusId":254591260},"title":"RT-1: Robotics Transformer for Real-World Control at Scale"},{"paperId":"8ee45aeb7c97e3346cc62f216f673b91277ac718","externalIds":{"DBLP":"conf/iccv/SongSWCW023","ArXiv":"2212.04088","DOI":"10.1109/ICCV51070.2023.00280","CorpusId":254408960},"title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"},{"paperId":"bd6d85743f399100b052620c1c4550cbfea7da6f","externalIds":{"DBLP":"conf/iros/LawsonQ23","ArXiv":"2211.06407","DOI":"10.1109/IROS55552.2023.10341628","CorpusId":253498865},"title":"Control Transformer: Robot Navigation in Unknown Environments Through PRM-Guided Return-Conditioned Sequence Modeling"},{"paperId":"7853fd5f862126d257bf90d91a8533dc4795f769","externalIds":{"ArXiv":"2210.16282","DBLP":"journals/corr/abs-2210-16282","DOI":"10.48550/arXiv.2210.16282","CorpusId":253223891},"title":"Big Data Meets Metaverse: A Survey"},{"paperId":"b287a2765e5bceb732de39dafdf70594dc9cd664","externalIds":{"DBLP":"journals/ftcgv/GanLLWLG22","ArXiv":"2210.09263","DOI":"10.48550/arXiv.2210.09263","CorpusId":252918286},"title":"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"},{"paperId":"05c4bfd0b16fd0ab0e72e0866d0a5bfec5ad7ded","externalIds":{"DBLP":"conf/rss/KumarSENYFL23","ArXiv":"2210.05178","DOI":"10.15607/RSS.2023.XIX.019","CorpusId":252816019},"title":"Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials"},{"paperId":"3ac400f1ca96a7ccb5a1b7790684abcb00464871","externalIds":{"DBLP":"journals/corr/abs-2210-03370","ArXiv":"2210.03370","DOI":"10.1109/ICRA48891.2023.10161227","CorpusId":252762097},"title":"GNM: A General Navigation Model to Drive Any Robot"},{"paperId":"af68f10ab5078bfc519caae377c90ee6d9c504e9","externalIds":{"ArXiv":"2210.02747","DBLP":"journals/corr/abs-2210-02747","CorpusId":252734897},"title":"Flow Matching for Generative Modeling"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","externalIds":{"DBLP":"journals/corr/abs-2209-11302","ArXiv":"2209.11302","DOI":"10.1109/ICRA48891.2023.10161317","CorpusId":252519594},"title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","externalIds":{"ArXiv":"2209.06794","DBLP":"journals/corr/abs-2209-06794","DOI":"10.48550/arXiv.2209.06794","CorpusId":252222320},"title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"97e6b89f8f256289b01b9f31799d957db81f2d4e","externalIds":{"DBLP":"conf/icra/BuckerFHKMVB23","ArXiv":"2208.02918","DOI":"10.1109/ICRA48891.2023.10161068","CorpusId":251371492},"title":"LATTE: LAnguage Trajectory TransformEr"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","externalIds":{"ArXiv":"2207.04429","DBLP":"conf/corl/ShahOIL22","DOI":"10.48550/arXiv.2207.04429","CorpusId":250426345},"title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"d28b9f65c849eba9ba2b27f7e91906f46fbe7fa1","externalIds":{"DBLP":"journals/corr/abs-2207-09450","ArXiv":"2207.09450","DOI":"10.15607/rss.2022.xviii.026","CorpusId":248941578},"title":"Human-to-Robot Imitation in the Wild"},{"paperId":"01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8","externalIds":{"ArXiv":"2206.11251","DBLP":"journals/corr/abs-2206-11251","DOI":"10.48550/arXiv.2206.11251","CorpusId":249926747},"title":"Behavior Transformers: Cloning k modes with one stone"},{"paperId":"0dd478a6e6ffe0fddc8f3d9705db3cb6d70801ce","externalIds":{"ArXiv":"2206.07423","DBLP":"journals/corr/abs-2206-07423","DOI":"10.1109/ICRA48891.2023.10161289","CorpusId":249674321},"title":"Zero-Shot Object Goal Visual Navigation"},{"paperId":"e9c371f05cb1144211ba22f2bb48aba72f49a811","externalIds":{"DBLP":"journals/corr/abs-2206-06922","ArXiv":"2206.06922","DOI":"10.48550/arXiv.2206.06922","CorpusId":249642130},"title":"Object Scene Representation Transformer"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","externalIds":{"DBLP":"journals/corr/abs-2205-11916","ArXiv":"2205.11916","CorpusId":249017743},"title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","externalIds":{"ArXiv":"2205.10625","DBLP":"conf/iclr/ZhouSHWS0SCBLC23","DOI":"10.48550/arXiv.2205.10625","CorpusId":248986239},"title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"5922f437512158970c417f4413bface021df5f78","externalIds":{"DBLP":"journals/corr/abs-2205-06175","ArXiv":"2205.06175","DOI":"10.48550/arXiv.2205.06175","CorpusId":248722148},"title":"A Generalist Agent"},{"paperId":"51479457d258642ef04b358eaf66b17fb2e1d90a","externalIds":{"DOI":"10.1149/10701.12389ecst","CorpusId":248460807},"title":"Social Robots in Education for Long-Term Human-Robot Interaction : Socially Supportive Behaviour of Robotic Tutor for Creating Robo-Tangible Learning Environment in a Guided Discovery Learning Interaction"},{"paperId":"3fa5cfd3745b1b4df814283eac955a560e746bfd","externalIds":{"ArXiv":"2204.05618","DBLP":"journals/corr/abs-2204-05618","DOI":"10.48550/arXiv.2204.05618","CorpusId":248118795},"title":"When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","externalIds":{"DBLP":"conf/corl/IchterBCFHHHIIJ22","ArXiv":"2204.01691","CorpusId":247939706},"title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"4f1d598f919aae55c3cbbc425ef1514a54e2b8cd","externalIds":{"DBLP":"conf/acl/GuSWTW22","ACL":"2022.acl-long.524","ArXiv":"2203.12667","DOI":"10.18653/v1/2022.acl-long.524","CorpusId":247627890},"title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","externalIds":{"DBLP":"conf/iclr/0002WSLCNCZ23","ArXiv":"2203.11171","CorpusId":247595263},"title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"8d3fddfe59a21b8245b375d33fe91e215237ddb1","externalIds":{"DBLP":"journals/corr/abs-2203-00352","ArXiv":"2203.00352","DOI":"10.48550/arXiv.2203.00352","CorpusId":244796870},"title":"Affordance Learning from Play for Sample-Efficient Policy Learning"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","externalIds":{"DBLP":"journals/corr/abs-2202-13169","ArXiv":"2202.13169","DOI":"10.1145/3520312.3534862","CorpusId":247158549},"title":"A systematic evaluation of large language models of code"},{"paperId":"95a3456abf114634cffb11621313b045a14fdaf6","externalIds":{"DBLP":"journals/corr/abs-2202-11271","ArXiv":"2202.11271","DOI":"10.15607/RSS.2022.XVIII.019","CorpusId":247058659},"title":"ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints"},{"paperId":"06b10851b7a53316b3b6588017c9f3b9aae8c7cb","externalIds":{"DBLP":"journals/make/Hutsebaut-Buysse22","DOI":"10.3390/make4010009","CorpusId":246984577},"title":"Hierarchical Reinforcement Learning: A Survey and Open Research Challenges"},{"paperId":"1d803f07e4591bd67c358eef715bcd443e821894","externalIds":{"ArXiv":"2202.02005","DBLP":"journals/corr/abs-2202-02005","CorpusId":237257594},"title":"BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","externalIds":{"DBLP":"journals/corr/abs-2201-07207","ArXiv":"2201.07207","CorpusId":246035276},"title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"2db0fbdb68919bbaccdd67a610a4e30c41fc41d6","externalIds":{"ArXiv":"2112.02845","DBLP":"journals/corr/abs-2112-02845","CorpusId":245335360},"title":"Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence Model Tackles All SMAC Tasks"},{"paperId":"3ea60cbce6c9065661d207fccf021c5d58a83f01","externalIds":{"DBLP":"conf/cvpr/0006GWY0LW22","ArXiv":"2111.12233","DOI":"10.1109/CVPR52688.2022.01745","CorpusId":244527510},"title":"Scaling Up Vision-Language Pretraining for Image Captioning"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"fd399c7068512858b27535f75c8c31d2442dbaac","externalIds":{"DBLP":"conf/icra/ZhaoLLA22","ArXiv":"2110.13423","DOI":"10.1109/icra46639.2022.9812450","CorpusId":239885606},"title":"Towards More Generalizable One-shot Visual Imitation Learning"},{"paperId":"348a855fe01f3f4273bf0ecf851ca688686dbfcc","externalIds":{"ArXiv":"2110.06169","DBLP":"journals/corr/abs-2110-06169","CorpusId":238634325},"title":"Offline Reinforcement Learning with Implicit Q-Learning"},{"paperId":"de688c6e73ccf6ed33ff1cc7919d24456a1f74e2","externalIds":{"DBLP":"journals/corr/abs-2109-10282","ArXiv":"2109.10282","DOI":"10.1609/aaai.v37i11.26538","CorpusId":237581568},"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"},{"paperId":"3032844d6ac6882ccb03e7a2c22a0026b210ac05","externalIds":{"DBLP":"journals/corr/abs-2108-03298","ArXiv":"2108.03298","CorpusId":236956615},"title":"What Matters in Learning from Offline Human Demonstrations for Robot Manipulation"},{"paperId":"ef24c31e07443b5d08dfbc822bff06acb7bc7cbd","externalIds":{"DBLP":"journals/corr/abs-2107-06277","ArXiv":"2107.06277","CorpusId":235829153},"title":"Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability"},{"paperId":"248b15c7f9bfd37d25db115efe75ed556120eca6","externalIds":{"DBLP":"journals/pieee/DorigoTT21","DOI":"10.1109/JPROC.2021.3072740","CorpusId":235600121},"title":"Swarm Robotics: Past, Present, and Future"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","externalIds":{"ArXiv":"2106.13884","DBLP":"journals/corr/abs-2106-13884","CorpusId":235658331},"title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"27de2766018c8c596bb1a869f091f7bbdbb02381","externalIds":{"ArXiv":"2106.10496","CorpusId":235490389},"title":"The Tangent Exponential Model"},{"paperId":"90357a6dc817e2f7cec477a51156675fbf545cf1","externalIds":{"ArXiv":"2106.02636","DBLP":"conf/nips/ZellersLHYPCFC21","CorpusId":235352775},"title":"MERLOT: Multimodal Neural Script Knowledge Models"},{"paperId":"f864d4d2267abba15eb43db54f58286aef78292b","externalIds":{"DBLP":"conf/nips/JannerLL21","ArXiv":"2106.02039","CorpusId":235313679},"title":"Offline Reinforcement Learning as One Big Sequence Modeling Problem"},{"paperId":"c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500","externalIds":{"DBLP":"journals/corr/abs-2106-01345","MAG":"3169291081","ArXiv":"2106.01345","CorpusId":235294299},"title":"Decision Transformer: Reinforcement Learning via Sequence Modeling"},{"paperId":"3e85d208b1b927fdb69ecf8336c70995818aaebd","externalIds":{"ArXiv":"2104.08212","DBLP":"journals/corr/abs-2104-08212","CorpusId":233289968},"title":"MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale"},{"paperId":"16486b905177860b5b68b7f4da9e499562c674b0","externalIds":{"ArXiv":"2104.05859","DBLP":"journals/corr/abs-2104-05859","CorpusId":233219929},"title":"RECON: Rapid Exploration for Open-World Navigation with Latent Goal Models"},{"paperId":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","externalIds":{"ArXiv":"2104.05837","DBLP":"conf/emnlp/SafaviK21","ACL":"2021.emnlp-main.81","DOI":"10.18653/v1/2021.emnlp-main.81","CorpusId":233219920},"title":"Relational World Knowledge Representation in Contextual Language Models: A Review"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"b7f7d441a56a012fd3e5b1f1da37ac77708645a8","externalIds":{"DBLP":"journals/tsmc/KarolyGKR21","DOI":"10.1109/TSMC.2020.3018325","CorpusId":231617949},"title":"Deep Learning in Robotics: Survey on Model Structures and Training Strategies"},{"paperId":"0fd47bf484a05001ce787747cf5a879b9202ebfa","externalIds":{"DBLP":"journals/corr/abs-2012-09812","MAG":"3112720093","ArXiv":"2012.09812","DOI":"10.1109/ICRA48506.2021.9561936","CorpusId":229298058},"title":"ViNG: Learning Open-World Navigation with Visual Goals"},{"paperId":"c8d22fd8e8e094d624a484d770a60873baaba5f4","externalIds":{"DBLP":"journals/nle/Dale21","MAG":"3112103703","DOI":"10.1017/S1351324920000601","CorpusId":229170661},"title":"GPT-3: What’s it good for?"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0","externalIds":{"DBLP":"journals/corr/abs-2010-05731","ACL":"2020.emnlp-main.586","MAG":"3099178230","ArXiv":"2010.05731","DOI":"10.18653/v1/2020.emnlp-main.586","CorpusId":222290596},"title":"Probing Pretrained Language Models for Lexical Semantics"},{"paperId":"f8492a321d66c381637b693a24af994af41b3cdf","externalIds":{"DBLP":"journals/corr/abs-2009-07888","MAG":"3085267010","ArXiv":"2009.07888","DOI":"10.1109/TPAMI.2023.3292075","CorpusId":221761694,"PubMed":"37402188"},"title":"Transfer Learning in Deep Reinforcement Learning: A Survey"},{"paperId":"77366bef01df1ab277149b330336a0ef9c5041c4","externalIds":{"DOI":"10.1201/b11359-7","CorpusId":109547175},"title":"Transformer"},{"paperId":"28db20a81eec74a50204686c3cf796c42a020d2e","externalIds":{"ArXiv":"2006.04779","DBLP":"journals/corr/abs-2006-04779","MAG":"3102848167","CorpusId":219530894},"title":"Conservative Q-Learning for Offline Reinforcement Learning"},{"paperId":"1b0c78644069b231b9f2421086f21a17644b9726","externalIds":{"DBLP":"journals/ijrr/ArkinPRWRHP20","MAG":"3033090297","DOI":"10.1177/0278364920917755","CorpusId":219719691},"title":"Multimodal estimation and communication of latent semantic knowledge for robust execution of robot instructions"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"1301e9d11b728268ed1ff3f1a9adc155308d5250","externalIds":{"ArXiv":"2005.07648","DBLP":"conf/rss/LynchS21","DOI":"10.15607/RSS.2021.XVII.047","CorpusId":235657751},"title":"Language Conditioned Imitation Learning Over Unstructured Data"},{"paperId":"bc23dfb70e109dbe666b979f995e2779b96e1073","externalIds":{"MAG":"3003205975","DBLP":"journals/arcras/TellexGKM20","DOI":"10.1146/annurev-control-101119-071628","CorpusId":213739887},"title":"Robots That Use Language"},{"paperId":"e90e7b1f594ee21f5646a4786a7c6246af474261","externalIds":{"DBLP":"journals/ral/KahnAL21","ArXiv":"2002.05700","MAG":"3006388420","DOI":"10.1109/LRA.2021.3057023","CorpusId":211096687},"title":"BADGR: An Autonomous Self-Supervised Learning-Based Navigation System"},{"paperId":"80376bdec5f534be78ba82821f540590ebce5559","externalIds":{"DBLP":"journals/corr/abs-2002-08910","MAG":"3102659883","ACL":"2020.emnlp-main.437","ArXiv":"2002.08910","DOI":"10.18653/v1/2020.emnlp-main.437","CorpusId":211205183},"title":"How Much Knowledge Can You Pack into the Parameters of a Language Model?"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"4270ea0c40857b0fdfe30feab34e71d416929f4b","externalIds":{"MAG":"2998859407","DOI":"10.3390/app10020497","CorpusId":214429751},"title":"Semantic Information for Robot Navigation: A Survey"},{"paperId":"503bbfbc6c9654303ce993cf1dae31034dda308c","externalIds":{"DBLP":"conf/aaai/SongZH020","MAG":"2983160116","ArXiv":"1911.05889","DOI":"10.1609/AAAI.V34I05.6417","CorpusId":208006638},"title":"Generating Persona Consistent Dialogues by Exploiting Natural Language Inference"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"59a916cdc943f0282908e6f3fa0360f4c5fb78d0","externalIds":{"DBLP":"conf/icml/ParisottoSRPGJJ20","ArXiv":"1910.06764","MAG":"2980433389","CorpusId":204578308},"title":"Stabilizing Transformers for Reinforcement Learning"},{"paperId":"6648b4db5f12c30941ea78c695e77aded19672bb","externalIds":{"MAG":"2997591391","ArXiv":"1909.11059","DBLP":"journals/corr/abs-1909-11059","DOI":"10.1609/AAAI.V34I07.7005","CorpusId":202734445},"title":"Unified Vision-Language Pre-Training for Image Captioning and VQA"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0889fcee1acd985af76a3907d5d0029bf260be9","externalIds":{"DBLP":"conf/nips/EysenbachSL19","MAG":"2950885698","ArXiv":"1906.05253","CorpusId":186206882},"title":"Search on the Replay Buffer: Bridging Planning and Reinforcement Learning"},{"paperId":"7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50","externalIds":{"DBLP":"conf/ijcai/LuketinaNFFAGWR19","MAG":"2948380112","ArXiv":"1906.03926","DOI":"10.24963/ijcai.2019/880","CorpusId":182952502},"title":"A Survey of Reinforcement Learning Informed by Natural Language"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","externalIds":{"DBLP":"conf/cvpr/MarinoRFM19","ArXiv":"1906.00067","MAG":"2947312908","DOI":"10.1109/CVPR.2019.00331","CorpusId":173991173},"title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9","externalIds":{"DBLP":"conf/icml/TanL19","MAG":"2946948417","ArXiv":"1905.11946","CorpusId":167217261},"title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"paperId":"b4a35e548de27b6924e5f2ee41d37238a5c4a1d5","externalIds":{"MAG":"2929928372","ArXiv":"1904.01201","DBLP":"journals/corr/abs-1904-01201","DOI":"10.1109/ICCV.2019.00943","CorpusId":91184540},"title":"Habitat: A Platform for Embodied AI Research"},{"paperId":"4625628163a2ee0e6cd320cd7a14b4ccded2a631","externalIds":{"MAG":"2923504512","DBLP":"journals/corr/abs-1903-08254","ArXiv":"1903.08254","CorpusId":84187276},"title":"Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"},{"paperId":"5d9f17ff3816b194d33570af1a2b751da57c3d8d","externalIds":{"MAG":"2955290086","ArXiv":"1903.02749","DBLP":"journals/ral/HiroseXMSS19","DOI":"10.1109/LRA.2019.2925731","CorpusId":71148543},"title":"Deep Visual MPC-Policy Learning for Navigation"},{"paperId":"d858948bedba72fa394e9684f45c3051c43fdf48","externalIds":{"DBLP":"journals/corr/abs-1902-09458","ArXiv":"1902.09458","MAG":"2915157612","DOI":"10.1109/tro.2020.2975428","CorpusId":67855822},"title":"Long-Range Indoor Navigation With PRM-RL"},{"paperId":"1f46c7f63d8c5000a583024a7b1ec134d0623a93","externalIds":{"DBLP":"journals/ker/HrabiaLA18","MAG":"2894761520","DOI":"10.1017/S0269888918000176","CorpusId":52962932},"title":"Towards adaptive multi-robot systems: self-organization and self-adaptation"},{"paperId":"e89a4fe6e8286eccedd702216153f0f248adb151","externalIds":{"DBLP":"conf/cvpr/XiaZHSMS18","ArXiv":"1808.10654","MAG":"2799034341","DOI":"10.1109/CVPR.2018.00945","CorpusId":49358881},"title":"Gibson Env: Real-World Perception for Embodied Agents"},{"paperId":"39b7007e6f3dd0744833f292f07ed77973503bfd","externalIds":{"MAG":"2950614095","DBLP":"journals/corr/abs-1805-08296","ArXiv":"1805.08296","CorpusId":43920938},"title":"Data-Efficient Hierarchical Reinforcement Learning"},{"paperId":"86430b50921c7aecb212b4cd0a43a73b091cf95f","externalIds":{"DBLP":"journals/corr/abs-1804-00168","MAG":"2951796010","ArXiv":"1804.00168","CorpusId":4931762},"title":"Learning to Navigate in Cities Without a Map"},{"paperId":"68c108795deef06fa929d1f6e96b75dbf7ce8531","externalIds":{"DBLP":"conf/nips/GuptaMLAL18","MAG":"2963176272","ArXiv":"1802.07245","CorpusId":3418899},"title":"Meta-Reinforcement Learning of Structured Exploration Strategies"},{"paperId":"89c8aad71433f7638d2e2c009e1ea20e039f832d","externalIds":{"DBLP":"journals/corr/abs-1712-05474","MAG":"2776202271","ArXiv":"1712.05474","CorpusId":28328610},"title":"AI2-THOR: An Interactive 3D Environment for Visual AI"},{"paperId":"c37c23b12e00168833eccff8025a830ce27c5abc","externalIds":{"MAG":"2952157315","DBLP":"conf/cvpr/AndersonWTB0S0G18","ArXiv":"1711.07280","DOI":"10.1109/CVPR.2018.00387","CorpusId":4673790},"title":"Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"},{"paperId":"b864f89eaa91120e04e8c62eb0b36568ab4244a8","externalIds":{"DBLP":"journals/corr/abs-1710-04615","MAG":"2761060327","ArXiv":"1710.04615","DOI":"10.1109/ICRA.2018.8461249","CorpusId":3720790},"title":"Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation"},{"paperId":"c9141c5a1cb0a0f457fb7b34d21e154f2c336baf","externalIds":{"DBLP":"journals/corr/abs-1709-10489","MAG":"2759463506","ArXiv":"1709.10489","DOI":"10.1109/ICRA.2018.8460655","CorpusId":38756145},"title":"Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation"},{"paperId":"7cfa5c97164129ce3630511f639040d28db1d4b7","externalIds":{"DBLP":"journals/corr/abs-1709-07871","MAG":"2951555602","ArXiv":"1709.07871","DOI":"10.1609/aaai.v32i1.11671","CorpusId":19119291},"title":"FiLM: Visual Reasoning with a General Conditioning Layer"},{"paperId":"4f03dcc7cc3aadae16655c87c6c882407617c725","externalIds":{"DBLP":"journals/corr/abs-1708-02072","MAG":"2963560049","ArXiv":"1708.02072","DOI":"10.1609/aaai.v32i1.11651","CorpusId":22910766},"title":"Measuring Catastrophic Forgetting in Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"049c6e5736313374c6e594c34b9be89a3a09dced","externalIds":{"MAG":"2949267040","ArXiv":"1703.01161","DBLP":"conf/icml/VezhnevetsOSHJS17","CorpusId":6656096},"title":"FeUdal Networks for Hierarchical Reinforcement Learning"},{"paperId":"8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a","externalIds":{"MAG":"2526379199","ArXiv":"1609.07088","DBLP":"journals/corr/DevinGDAL16","DOI":"10.1109/ICRA.2017.7989250","CorpusId":18015872},"title":"Learning modular neural network policies for multi-task and multi-robot transfer"},{"paperId":"7af7f2f539cd3479faae4c66bbef49b0f66202fa","externalIds":{"DBLP":"journals/corr/ZhuMKLGFF16","MAG":"2950465560","ArXiv":"1609.05143","DOI":"10.1109/ICRA.2017.7989381","CorpusId":2305273},"title":"Target-driven visual navigation in indoor scenes using deep reinforcement learning"},{"paperId":"9d3d3c541a05316b75ff4471c0d180a42a303b5a","externalIds":{"MAG":"2294092760","DBLP":"conf/iser/DuvalletWHHOTRS14","DOI":"10.1007/978-3-319-23778-7_25","CorpusId":16096595},"title":"Inferring Maps and Behaviors from Natural Language Instructions"},{"paperId":"80f15048f9774191c3ae2ab8950b6d49f2d05295","externalIds":{"MAG":"1933065844","ArXiv":"1506.04089","DBLP":"journals/corr/MeiBW15","DOI":"10.1609/aaai.v30i1.10364","CorpusId":979457},"title":"Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences"},{"paperId":"235585741d5d2eddb85949226ddf4f581fd79dc1","externalIds":{"MAG":"2113953866","DBLP":"conf/atal/BrysHTN15","DOI":"10.65109/junf6077","CorpusId":15700284},"title":"Policy Transfer using Reward Shaping"},{"paperId":"f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","externalIds":{"MAG":"1591801644","ArXiv":"1409.2329","DBLP":"journals/corr/ZarembaSV14","CorpusId":17719760},"title":"Recurrent Neural Network Regularization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"68514af8a4ab5c8a10a6fb9e1f61948a94d90281","externalIds":{"MAG":"2768422825","DBLP":"series/tanlp/HassanD14","DOI":"10.1007/978-3-642-45358-8_6","CorpusId":7721910},"title":"Statistical Machine Translation"},{"paperId":"030e1918c63fb7c27946a631c9e8a6abe14c99de","externalIds":{"MAG":"2099752828","DBLP":"conf/aips/CrosbyRP13","DOI":"10.1609/icaps.v23i1.13564","CorpusId":9449916},"title":"Automated Agent Decomposition for Classical Planning"},{"paperId":"a1eb18784261a19730b05cc1b637049b0d7d417a","externalIds":{"MAG":"2126558794","DOI":"10.1016/j.apergo.2012.10.010","CorpusId":25430130,"PubMed":"23157974"},"title":"The effects of overall robot shape on the emotions invoked in users and the perceived personalities of robot."},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","externalIds":{"MAG":"2950577311","DBLP":"journals/corr/abs-1301-3781","ArXiv":"1301.3781","CorpusId":5959482},"title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"c51b1e958e138c48c18ec882c4ed1dd97e89284d","externalIds":{"MAG":"1479925024","DBLP":"books/mk/Ginsberg93","DOI":"10.1016/c2009-0-27845-7","CorpusId":28363152},"title":"Essentials of Artificial Intelligence"},{"paperId":"4b17662f21c6313907a022af20f88616d11620eb","externalIds":{"DBLP":"conf/aaai/ChenM11","MAG":"2118781169","DOI":"10.1609/aaai.v25i1.7974","CorpusId":215717032},"title":"Learning to Interpret Natural Language Navigation Instructions from Observations"},{"paperId":"a389456fdc46e458d0a813a608b71b44b2ff8e62","externalIds":{"DBLP":"conf/aaai/TellexKDWBTR11","MAG":"2236233024","DOI":"10.1609/aaai.v25i1.7979","CorpusId":220828823},"title":"Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation"},{"paperId":"2f3f866dd8e4a187c033e55fc8d31b6be77afd2d","externalIds":{"MAG":"55989445","DBLP":"conf/ijcai/ShimizuH09","CorpusId":59744142},"title":"Learning to Follow Navigational Route Instructions"},{"paperId":"33d0c4dd77288625923192e7c533def3c8b14edb","externalIds":{"MAG":"2571293732","CorpusId":56904749},"title":"The theory of affordances"},{"paperId":"8fac1f407ef8caf925e654a984303629ffc7ea4c","externalIds":{"MAG":"2026085771","DOI":"10.1016/j.jphysparis.2006.03.014","CorpusId":2231038,"PubMed":"16750617"},"title":"Planning and problem solving: From neuropsychology to functional neuroimaging"},{"paperId":"6498ce44950a8090e574b250bb946f23307d66f1","externalIds":{"DBLP":"journals/aicom/SebastiaOM06","MAG":"1559756103","DOI":"10.3233/EAI-2006-361","CorpusId":33236132},"title":"Decomposition of planning problems"},{"paperId":"583afba01719f881bf7faa17bd830e8b12572314","externalIds":{"MAG":"1509652479","DOI":"10.1016/b978-0-12-678957-7.x5000-2","CorpusId":29397059},"title":"Mechanisms of Memory"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","externalIds":{"MAG":"2140679639","DBLP":"conf/nips/BengioDV00","CorpusId":221275765},"title":"A Neural Probabilistic Language Model"},{"paperId":"c34b3f87c759e5da1b993fe1a5e82d77915b8c1d","externalIds":{"MAG":"2029130131","DBLP":"journals/air/Ribeiro02","DOI":"10.1023/A:1015008417172","CorpusId":3276732},"title":"Reinforcement Learning Agents"},{"paperId":"512df75b028a807042a61ca1842b730aa62255d1","externalIds":{"MAG":"2149524486","DBLP":"journals/corr/cs-AI-0004001","ArXiv":"cs/0004001","CorpusId":7158833},"title":"A Theory of Universal Artificial Intelligence based on Algorithmic Complexity"},{"paperId":"7e81dac6260c4768af3a28ac21c78c5a38a5f7d0","externalIds":{"MAG":"2128990851","DBLP":"journals/trob/KavrakiSLO96","DOI":"10.1109/70.508439","CorpusId":18974595},"title":"Probabilistic roadmaps for path planning in high-dimensional configuration spaces"},{"paperId":"9548ac30c113562a51e603dbbc8e9fa651cfd3ab","externalIds":{"DBLP":"conf/icassp/KneserN95","MAG":"1934041838","DOI":"10.1109/ICASSP.1995.479394","CorpusId":9685476},"title":"Improved backing-off for M-gram language modeling"},{"paperId":"5518142f3b3d73ac8f71bb853c2f1cba14a28457","externalIds":{"MAG":"2255817990","DBLP":"journals/logcom/Goodwin95","DOI":"10.1093/logcom/5.6.763","CorpusId":15174191},"title":"Formalizing Properties of Agents"},{"paperId":"3de5d40b60742e3dfa86b19e7f660962298492af","externalIds":{"MAG":"2121227244","ACL":"J92-4003","DBLP":"journals/coling/BrownPdLM92","CorpusId":10986188},"title":"Class-Based n-gram Models of Natural Language"},{"paperId":"4019e9985f15efe3f21132e41e2aa9fc1e2cf15c","externalIds":{"MAG":"2104095126","DOI":"10.1017/S0140525X00071351","CorpusId":145761266},"title":"Planning and the brain"},{"paperId":"74f05cd2e847a0472a28c475c78d1654422903a5","externalIds":{"MAG":"3022778360","DBLP":"journals/ai/Brooks91","DOI":"10.1016/0004-3702(91)90053-M","CorpusId":207507849},"title":"Intelligence without Representation"},{"paperId":"fb39b30187babe9bee4afb30b034b5f93b74859a","externalIds":{"MAG":"2097856935","DBLP":"journals/trob/Brooks86","DOI":"10.1109/JRA.1986.1087032","CorpusId":263892775},"title":"A robust layered control system for a mobile robot"},{"paperId":"af465996da89a302fae95c2fe22e54d2b79e4ac3","externalIds":{"DBLP":"books/ox/90/NewellS90","MAG":"1968079292","DOI":"10.1145/360018.360022","CorpusId":5581562},"title":"Computer science as empirical inquiry: symbols and search"},{"paperId":"8f71a9729ae782fa0d3b077d8cba97fcc636e70e","externalIds":{"MAG":"1997480541","DOI":"10.1080/14640746808400161","CorpusId":1212273,"PubMed":"5683766"},"title":"Reasoning about a Rule"},{"paperId":"ede45bcd58cd9fe7b2a120a075e48d2dfe3e941c","externalIds":{"DBLP":"conf/acl/PengWZLM24","DOI":"10.18653/v1/2024.findings-acl.283","CorpusId":271904258},"title":"Chain-of-Question: A Progressive Question Decomposition Approach for Complex Knowledge Base Question Answering"},{"paperId":"212f84a6f7d69114e3ae4171c2160278a9061d89","externalIds":{"DBLP":"journals/trob/LiuNYSC24","DOI":"10.1109/TRO.2024.3422051","CorpusId":270946803},"title":"Unlocking Human-Like Facial Expressions in Humanoid Robots: A Novel Approach for Action Unit Driven Facial Expression Disentangled Synthesis"},{"paperId":"4747e72c5bc706c50e76953188f0144df18992d0","externalIds":{"DBLP":"journals/corr/abs-2307-07924","DOI":"10.48550/arXiv.2307.07924","CorpusId":259936967},"title":"Communicative Agents for Software Development"},{"paperId":"2f0df7d27b28c80d5b389f30d8d243d62b76bc48","externalIds":{"DOI":"10.1007/978-3-319-32010-6_300155","CorpusId":215989524},"title":"Mixture-of-Experts"},{"paperId":"49f8a31e13998ae431dde8092973e6bd0f8385be","externalIds":{"DBLP":"conf/iclr/Jang0K22","CorpusId":251648921},"title":"GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems"},{"paperId":"65a8e6321f3a20b9bd5dd7b8d05e47c75eeb7580","externalIds":{"DBLP":"conf/icml/BagariaS021","CorpusId":221094080},"title":"Skill Discovery for Exploration and Planning using Deep Skill Graphs"},{"paperId":"eaa09c607780373cc809bce89b6b28b17e301f27","externalIds":{"DBLP":"conf/nips/RyooPADA21","CorpusId":245122526},"title":"TokenLearner: Adaptive Space-Time Tokenization for Videos"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"0c0a778e6fdf7e36b1750c533dcc916f86608607","externalIds":{"MAG":"2527310337","DBLP":"journals/tkde/XunJGZ17","DOI":"10.1109/TKDE.2016.2614508","CorpusId":13490401},"title":"A Survey on Context Learning"},{"paperId":"4f8d648c52edf74e41b0996128aa536e13cc7e82","externalIds":{"DBLP":"journals/ijsc/HaoZM16","DOI":"10.1142/S1793351X16500045","CorpusId":1779661},"title":"Deep Learning"},{"paperId":"3954a3f80cf1b1f76430d80e924d85f2f1ba6799","externalIds":{"MAG":"46490633","DBLP":"conf/iser/MatuszekHZF12","DOI":"10.1007/978-3-319-00065-7_28","CorpusId":1658890},"title":"Learning to Parse Natural Language Commands to a Robot Control System"},{"paperId":"c69eea42fc1020fda6cab55021c73dd2a51ec736","externalIds":{"CorpusId":2689497},"title":"Dynamics ∗"},{"paperId":"6df43f70f383007a946448122b75918e3a9d6682","externalIds":{"MAG":"1594201624","DBLP":"conf/ijcai/Kaelbling93","CorpusId":5538688},"title":"Learning to Achieve Goals"},{"paperId":"dd0e29a50c1790a59605d51fb2724d4e8c0d1922","externalIds":{"MAG":"1937795883","CorpusId":143408225},"title":"Psychology of Reasoning: Structure and Content"},{"paperId":"be4f6f13ac0913716f9a8dcf1d4c26a1f8057354","externalIds":{"CorpusId":279618395},"title":"Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge"},{"paperId":"4e2bb05fd13a3681a0712cc037882e661a7fb8f4","externalIds":{"DOI":"10.1145/2592798.2592801","CorpusId":243434635},"title":"Tesla"}]}