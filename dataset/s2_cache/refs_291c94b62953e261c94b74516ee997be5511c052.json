{"references":[{"paperId":"e2e1bb39ab385705401b8abdd0fd16135bf8caf2","externalIds":{"DBLP":"journals/corr/abs-2406-16989","ArXiv":"2406.16989","DOI":"10.48550/arXiv.2406.16989","CorpusId":270710718},"title":"Retrieval-Augmented Mixture of LoRA Experts for Uploadable Machine Learning"},{"paperId":"e2403e398314d49c5f56e05105a420a6f93e3cb2","externalIds":{"DBLP":"conf/emnlp/WuW0W24","ACL":"2024.emnlp-main.450","ArXiv":"2406.11909","DOI":"10.48550/arXiv.2406.11909","CorpusId":270562608},"title":"Mixture-of-Subspaces in Low-Rank Adaptation"},{"paperId":"779c7800b7203f179a53182b6e10964588a5a72a","externalIds":{"DBLP":"journals/corr/abs-2406-09044","ArXiv":"2406.09044","DOI":"10.48550/arXiv.2406.09044","CorpusId":270440848},"title":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning"},{"paperId":"27c62941c920a0342e337974aed6a9ba80d4b965","externalIds":{"ArXiv":"2406.08447","DBLP":"conf/nips/HayouG024","DOI":"10.48550/arXiv.2406.08447","CorpusId":270391476},"title":"The Impact of Initialization on LoRA Finetuning Dynamics"},{"paperId":"7777c897ea1abd046dd30a5d13504ef0afa02fca","externalIds":{"ArXiv":"2406.05678","DBLP":"journals/corr/abs-2406-05678","DOI":"10.48550/arXiv.2406.05678","CorpusId":270370979},"title":"SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models"},{"paperId":"39c473ced3121883ec747e92175d29e44a1237c9","externalIds":{"DBLP":"journals/fcsc/LiLBCDJ24","DOI":"10.1007/s11704-024-31018-5","CorpusId":270329204},"title":"RA-CFGPT: Chinese financial assistant with retrieval-augmented large language model"},{"paperId":"83844c987e3351d39fb866ae5bad9a6a499e5198","externalIds":{"ArXiv":"2406.02214","DBLP":"journals/corr/abs-2406-02214","DOI":"10.48550/arXiv.2406.02214","CorpusId":270226414},"title":"SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining"},{"paperId":"7eb2d3eacf80a884aea82c929dcb21ee466af0bc","externalIds":{"DBLP":"conf/ecai/BalazyBAT25","ArXiv":"2405.17604","DOI":"10.48550/arXiv.2405.17604","CorpusId":270068411},"title":"LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters"},{"paperId":"b1cf424ce408615cac8c809a2d24519c513a26e2","externalIds":{"DBLP":"journals/corr/abs-2405-17357","ArXiv":"2405.17357","DOI":"10.48550/arXiv.2405.17357","CorpusId":270062642},"title":"DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution"},{"paperId":"4846ca4c1cf64e5adca2cc08767bc3514deb0cc7","externalIds":{"DBLP":"journals/corr/abs-2405-15179","ArXiv":"2405.15179","DOI":"10.48550/arXiv.2405.15179","CorpusId":270045525},"title":"VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks"},{"paperId":"79a0648bb7e22ae183932707718b73b67e10b735","externalIds":{"ArXiv":"2405.12130","DBLP":"journals/corr/abs-2405-12130","DOI":"10.48550/arXiv.2405.12130","CorpusId":269921932},"title":"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning"},{"paperId":"be12e6806e9190c3954cb34af7a6923b65cfedba","externalIds":{"ArXiv":"2405.09673","DBLP":"journals/corr/abs-2405-09673","DOI":"10.48550/arXiv.2405.09673","CorpusId":269791237},"title":"LoRA Learns Less and Forgets Less"},{"paperId":"50bbb265a4f5b7c3f2de2cdbdd57aca12ecb9b97","externalIds":{"DBLP":"conf/icml/GaoWCLWC024","ArXiv":"2405.03003","DOI":"10.48550/arXiv.2405.03003","CorpusId":269605083},"title":"Parameter-Efficient Fine-Tuning with Discrete Fourier Transform"},{"paperId":"7804a4aadb2da9e462e2eeabdbafcd88de801d13","externalIds":{"ArXiv":"2404.18848","DBLP":"journals/corr/abs-2404-18848","DOI":"10.48550/arXiv.2404.18848","CorpusId":269449686},"title":"FeDeRA: Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition"},{"paperId":"267ffcaa65482aa6d993b7572537379e28568cfd","externalIds":{"DBLP":"journals/corr/abs-2404-09610","ArXiv":"2404.09610","DOI":"10.48550/arXiv.2404.09610","CorpusId":269149525},"title":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control"},{"paperId":"779fa93616f21352356974c7326ed0884ca4d49f","externalIds":{"DBLP":"journals/corr/abs-2404-05086","ArXiv":"2404.05086","DOI":"10.48550/arXiv.2404.05086","CorpusId":269005309},"title":"A Note on LoRA"},{"paperId":"ee4014497ccf2f65d6e05d3956b0e6b0c7369bae","externalIds":{"ArXiv":"2404.02948","DBLP":"conf/nips/MengWZ24","DOI":"10.48550/arXiv.2404.02948","CorpusId":268889493},"title":"PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"},{"paperId":"c6dd8a90d29345830b8958868f1295db174321c0","externalIds":{"DBLP":"journals/corr/abs-2404-00228","ArXiv":"2404.00228","DOI":"10.1109/CVPR52733.2024.02231","CorpusId":268819832},"title":"InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning"},{"paperId":"8787164d9961ae8c22a1e33e2ed89554f4213432","externalIds":{"DBLP":"journals/corr/abs-2403-20320","ArXiv":"2403.20320","DOI":"10.1109/CVPR52733.2024.01533","CorpusId":268793804},"title":"MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning"},{"paperId":"2f6a31f5457c7d0a16bbaf990adb40b1903f40b4","externalIds":{"ArXiv":"2403.18461","CorpusId":268724119},"title":"An Efficient and Harmonized Framework for Balanced Cross-Domain Feature Integration"},{"paperId":"c739eb7f0302e85e935d1e2fdb903fe01b812804","externalIds":{"DBLP":"journals/corr/abs-2403-17919","ArXiv":"2403.17919","DOI":"10.48550/arXiv.2403.17919","CorpusId":268691349},"title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning"},{"paperId":"6191e48a413790a5e7defffdea4b0faac659ca35","externalIds":{"DBLP":"journals/corr/abs-2403-17749","ArXiv":"2403.17749","DOI":"10.1109/CVPR52733.2024.02638","CorpusId":268691776},"title":"Multi-Task Dense Prediction via Mixture of Low-Rank Experts"},{"paperId":"e4d913a4a1e5286b93e4dca0e032c58c3794e873","externalIds":{"ACL":"2024.naacl-long.35","DBLP":"journals/corr/abs-2403-16187","ArXiv":"2403.16187","DOI":"10.48550/arXiv.2403.16187","CorpusId":268681000},"title":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models"},{"paperId":"e04d2413139ac947ce9ebe44510424964206f6ad","externalIds":{"DBLP":"journals/corr/abs-2403-14572","ArXiv":"2403.14572","DOI":"10.48550/arXiv.2403.14572","CorpusId":268553753},"title":"Implicit Style-Content Separation using B-LoRA"},{"paperId":"543eb210bb6e317d535fe4b2e5996c361caab5e1","externalIds":{"ArXiv":"2403.14888","DBLP":"conf/acl/XueZD024","DOI":"10.48550/arXiv.2403.14888","CorpusId":268667584},"title":"AutoRE: Document-Level Relation Extraction with Large Language Models"},{"paperId":"916b4926cda574dc3f9486bb9994b6f2788dd800","externalIds":{"DBLP":"journals/corr/abs-2403-14608","ArXiv":"2403.14608","DOI":"10.48550/arXiv.2403.14608","CorpusId":268553763},"title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey"},{"paperId":"2315fd198e353d697a48d078ddfdc2abea891dd8","externalIds":{"DBLP":"conf/acl/LiuKLWJB24","ArXiv":"2403.13269","DOI":"10.48550/arXiv.2403.13269","CorpusId":268536918},"title":"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models"},{"paperId":"fb84816401a7254681c17901875b2bd3112397e4","externalIds":{"DBLP":"journals/corr/abs-2403-13037","ArXiv":"2403.13037","DOI":"10.48550/arXiv.2403.13037","CorpusId":268537094},"title":"BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models"},{"paperId":"49e0456e4a21b343ff0f4b6a2dc762a8daad4afe","externalIds":{"DBLP":"journals/corr/abs-2403-12313","ArXiv":"2403.12313","DOI":"10.48550/arXiv.2403.12313","CorpusId":268531521},"title":"Improving LoRA in Privacy-preserving Federated Learning"},{"paperId":"286f1d0ab26b7882f2b97bb878c58e5115f5d730","externalIds":{"ArXiv":"2403.11530","DBLP":"journals/corr/abs-2403-11530","DOI":"10.1109/CVPR52733.2024.02705","CorpusId":268532367},"title":"Continual Forgetting for Pre-Trained Vision Models"},{"paperId":"15b0a6ccb198b2936e36266be992da78a29953fd","externalIds":{"ArXiv":"2403.12285","DBLP":"journals/corr/abs-2403-12285","DOI":"10.48550/arXiv.2403.12285","CorpusId":268531242},"title":"FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications"},{"paperId":"786b09f188b68a65b8db00acf3bb6e319093d855","externalIds":{"ArXiv":"2403.11158","DBLP":"journals/corr/abs-2403-11158","DOI":"10.48550/arXiv.2403.11158","CorpusId":268513594},"title":"An Empirical Study on JIT Defect Prediction Based on BERT-style Model"},{"paperId":"db8d88f2398f27285a4d6f036c3b6b8d6780eafc","externalIds":{"DBLP":"journals/corr/abs-2403-10983","ArXiv":"2403.10983","DOI":"10.48550/arXiv.2403.10983","CorpusId":268512816},"title":"OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models"},{"paperId":"78da960c61d41c9c0fc2711fecacc5f5f29660ff","externalIds":{"ACL":"2024.naacl-long.282","DBLP":"journals/corr/abs-2403-09113","ArXiv":"2403.09113","DOI":"10.48550/arXiv.2403.09113","CorpusId":268384990},"title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning"},{"paperId":"8e1643d8ce154c89842065940958dff6d529d5f9","externalIds":{"ArXiv":"2403.07500","DBLP":"journals/corr/abs-2403-07500","DOI":"10.48550/arXiv.2403.07500","CorpusId":268363514},"title":"Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation"},{"paperId":"90286d3c58c84dd6dcb09d9865dc748434d94d5a","externalIds":{"DBLP":"journals/corr/abs-2403-06208","ArXiv":"2403.06208","DOI":"10.1609/aaai.v38i17.29931","CorpusId":268357029},"title":"Personalized LoRA for Human-Centered Text Understanding"},{"paperId":"eb2c30897dd1cf01f5a1d31ae402eb641a9123a1","externalIds":{"ArXiv":"2403.05231","DBLP":"journals/corr/abs-2403-05231","DOI":"10.48550/arXiv.2403.05231","CorpusId":268297150},"title":"Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance"},{"paperId":"c1fa6255cc9fc3128f74befc7855e255bc7a2c6e","externalIds":{"ArXiv":"2403.03507","DBLP":"journals/corr/abs-2403-03507","DOI":"10.48550/arXiv.2403.03507","CorpusId":268253596},"title":"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"},{"paperId":"a3b4b2a19043409a02bde45ae4a8a9948480874d","externalIds":{"DBLP":"journals/corr/abs-2403-03432","ACL":"2024.lrec-main.994","ArXiv":"2403.03432","DOI":"10.48550/arXiv.2403.03432","CorpusId":268253292},"title":"Mixture-of-LoRAs: An Efficient Multitask Tuning Method for Large Language Models"},{"paperId":"df5b72f541fdd0328563ec93896d217c3b3660e3","externalIds":{"DBLP":"journals/corr/abs-2403-02084","ArXiv":"2403.02084","DOI":"10.48550/arXiv.2403.02084","CorpusId":268247663},"title":"ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models"},{"paperId":"f34c75e2941fb43fec6dcd70843cae2c7532d350","externalIds":{"ArXiv":"2403.02217","DBLP":"journals/corr/abs-2403-02217","DOI":"10.48550/arXiv.2403.02217","CorpusId":268248571},"title":"DragTex: Generative Point-Based Texture Editing on 3D Mesh"},{"paperId":"59a48bf399484a2e5b28c1f1168639acc8cf3412","externalIds":{"DBLP":"journals/taslp/JinLT24","ArXiv":"2403.01754","DOI":"10.1109/TASLP.2024.3477330","CorpusId":268248463},"title":"Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models"},{"paperId":"0f5d1862d01917152cdefbff8b3f6cc9ea922c0c","externalIds":{"ArXiv":"2403.02221","DBLP":"journals/corr/abs-2403-02221","DOI":"10.48550/arXiv.2403.02221","CorpusId":268248693},"title":"TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models"},{"paperId":"8089b431b2e09c27967428fb542c0935fb95ec30","externalIds":{"DBLP":"journals/corr/abs-2403-01165","ArXiv":"2403.01165","DOI":"10.48550/arXiv.2403.01165","CorpusId":268230901},"title":"STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models"},{"paperId":"8c785ebee1f34464dbc85ab4113bccafd7a74b0a","externalIds":{"DBLP":"conf/acl/Quan24","ArXiv":"2403.01197","DOI":"10.48550/arXiv.2403.01197","CorpusId":268230489},"title":"DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling"},{"paperId":"1cb37e155e3414ff7774c6c91e72699c35d971c7","externalIds":{"DBLP":"journals/corr/abs-2402-18865","ArXiv":"2402.18865","DOI":"10.48550/arXiv.2402.18865","CorpusId":268063771},"title":"Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning"},{"paperId":"392305c7cf8922af0a919d05a26852e5d4150e9c","externalIds":{"DBLP":"conf/acl/ShiHSLZHWD0Z24","ArXiv":"2402.18039","DOI":"10.48550/arXiv.2402.18039","CorpusId":268041488},"title":"ResLoRA: Identity Residual Mapping in Low-Rank Adaption"},{"paperId":"ab45754a66f17f3eb4c86ae6305f01d09726447d","externalIds":{"DBLP":"journals/corr/abs-2403-08822","ArXiv":"2403.08822","DOI":"10.1117/12.3032013","CorpusId":268385523},"title":"LoRA-SP: streamlined partial parameter adaptation for resource efficient fine-tuning of large language models"},{"paperId":"ffe34206504bfc55524312504b9ab7c538b83a5b","externalIds":{"DBLP":"conf/nips/WooPKJKJL24","ArXiv":"2402.17812","DOI":"10.48550/arXiv.2402.17812","CorpusId":268041590},"title":"DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation"},{"paperId":"8c307cbf2d081a3a84e30266eb340fc99576526c","externalIds":{"ArXiv":"2402.17263","DBLP":"journals/corr/abs-2402-17263","DOI":"10.48550/arXiv.2402.17263","CorpusId":268032715},"title":"Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning"},{"paperId":"ec079312f82d98196ec6053f037462956935a73f","externalIds":{"ArXiv":"2402.16828","DBLP":"journals/corr/abs-2402-16828","DOI":"10.48550/arXiv.2402.16828","CorpusId":268031949},"title":"Training Neural Networks from Scratch with Parallel Low-Rank Adapters"},{"paperId":"21b8e360ac5d7101ebd5ca2ebdd859f3d3f39b7b","externalIds":{"DBLP":"journals/corr/abs-2402-16842","ArXiv":"2402.16842","DOI":"10.48550/arXiv.2402.16842","CorpusId":268033026},"title":"Asymmetry in Low-Rank Adapters of Foundation Models"},{"paperId":"1cba4c9ad9b7f645aeb97e6a4ac369fd0cde6490","externalIds":{"ArXiv":"2402.16141","DBLP":"journals/corr/abs-2402-16141","DOI":"10.48550/arXiv.2402.16141","CorpusId":267938515},"title":"PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization"},{"paperId":"034b4d52040db023e912768b7ef7eafb88e72a3e","externalIds":{"DBLP":"journals/corr/abs-2403-00812","ArXiv":"2403.00812","DOI":"10.48550/arXiv.2403.00812","CorpusId":268230872},"title":"LoRA Meets Dropout under a Unified Framework"},{"paperId":"4c111e261bf0f218341e2295cbdd6aae5a5329a7","externalIds":{"DBLP":"journals/corr/abs-2402-15896","ArXiv":"2402.15896","DOI":"10.48550/arXiv.2402.15896","CorpusId":267938649},"title":"Multimodal Instruction Tuning with Conditional Mixture of LoRA"},{"paperId":"5efc1f93724a4fc85751e11fe2c6f184e5b40d47","externalIds":{"ArXiv":"2402.15415","DBLP":"journals/corr/abs-2402-15415","DOI":"10.48550/arXiv.2402.15415","CorpusId":267897708},"title":"The Impact of LoRA on the Emergence of Clusters in Transformers"},{"paperId":"187de1107f760e10c52d29e9cd720fa63cb01ea8","externalIds":{"DBLP":"conf/emnlp/YeoH0R24","ArXiv":"2402.15151","DOI":"10.48550/arXiv.2402.15151","CorpusId":267897699},"title":"Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing"},{"paperId":"cecf84ccb00e9de02a065605a7130f27e5595657","externalIds":{"DBLP":"journals/corr/abs-2402-15414","ArXiv":"2402.15414","DOI":"10.48550/arXiv.2402.15414","CorpusId":267897786},"title":"Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?"},{"paperId":"2c40e142a4f27d14591b30f7afeb994d42287566","externalIds":{"ArXiv":"2402.15061","DBLP":"journals/corr/abs-2402-15061","DOI":"10.48550/arXiv.2402.15061","CorpusId":267897581},"title":"Fine-tuning Large Language Models for Domain-specific Machine Translation"},{"paperId":"b0c42343a85ae9d4db7071f1261d1810ad43c934","externalIds":{"DBLP":"conf/eccv/RenZYSLLKS24","ArXiv":"2402.14780","DOI":"10.48550/arXiv.2402.14780","CorpusId":267782887},"title":"Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models"},{"paperId":"a80d962fe8d5dc3ed19583419e2de46aef4fe8ba","externalIds":{"ArXiv":"2402.13210","DBLP":"journals/corr/abs-2402-13210","DOI":"10.48550/arXiv.2402.13210","CorpusId":267760094},"title":"Bayesian Reward Models for LLM Alignment"},{"paperId":"241eefc1bb11e693e0fef6977a65a0a822fb8f5e","externalIds":{"DBLP":"conf/icml/HayouG024","ArXiv":"2402.12354","DOI":"10.48550/arXiv.2402.12354","CorpusId":267750102},"title":"LoRA+: Efficient Low Rank Adaptation of Large Models"},{"paperId":"20214d0a8147dbd26c0481b39fe59682155033cd","externalIds":{"DBLP":"conf/icml/JangLR24","ArXiv":"2402.11867","DOI":"10.48550/arXiv.2402.11867","CorpusId":267750966},"title":"LoRA Training in the NTK Regime has No Spurious Local Minima"},{"paperId":"af601410a678f1a6bc31d1bfe3e2a8f32d243b8c","externalIds":{"ArXiv":"2402.11896","DBLP":"journals/corr/abs-2402-11896","DOI":"10.48550/arXiv.2402.11896","CorpusId":267750140},"title":"SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning"},{"paperId":"bbf10770831abb944601a17620589e3d781f99d2","externalIds":{"ArXiv":"2402.11505","DBLP":"conf/nips/BaiCQYL24","DOI":"10.52202/079017-0461","CorpusId":267750117},"title":"Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources"},{"paperId":"86f6fc1f96d189a314f10680eed4e7e9807c4f5e","externalIds":{"DBLP":"journals/corr/abs-2402-11248","ArXiv":"2402.11248","DOI":"10.48550/arXiv.2402.11248","CorpusId":267750306},"title":"CoLLaVO: Crayon Large Language and Vision mOdel"},{"paperId":"59b4b5b1e2198f264536d83e33d96b0a45ed3bac","externalIds":{"ArXiv":"2402.11260","DBLP":"journals/corr/abs-2402-11260","DOI":"10.48550/arXiv.2402.11260","CorpusId":267751418},"title":"MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning"},{"paperId":"ca4f0d2c85cfe46b97ec42b38decda107780769d","externalIds":{"ArXiv":"2402.10462","DBLP":"conf/emnlp/RajabzadehVZTK024","ACL":"2024.emnlp-industry.53","DOI":"10.48550/arXiv.2402.10462","CorpusId":267740414},"title":"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning"},{"paperId":"ec345204fb7a465f6263bd54270ecd6f459e48ca","externalIds":{"DBLP":"conf/acl/ZhaoG0ZYK024","ArXiv":"2402.09997","DOI":"10.48550/arXiv.2402.09997","CorpusId":267682151},"title":"LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild"},{"paperId":"da053e2a4ba1b244940c8f2cad5dcdf0d730f85f","externalIds":{"ArXiv":"2402.09353","DBLP":"conf/icml/LiuWY0WCC24","DOI":"10.48550/arXiv.2402.09353","CorpusId":267657886},"title":"DoRA: Weight-Decomposed Low-Rank Adaptation"},{"paperId":"94e531c3b139cdc50ab4b7be21a29dba8f87e3c0","externalIds":{"ArXiv":"2402.08562","DBLP":"journals/corr/abs-2402-08562","DOI":"10.48550/arXiv.2402.08562","CorpusId":267636601},"title":"Higher Layers Need More LoRA Experts"},{"paperId":"6cd1a41a8cc8feadff889d5f9de4c2cf0f6e3bf3","externalIds":{"ArXiv":"2402.07721","DBLP":"journals/corr/abs-2402-07721","DOI":"10.48550/arXiv.2402.07721","CorpusId":267627739},"title":"LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation"},{"paperId":"ad39d0f4b307be03f11fa553dd179ad0fa9e40e5","externalIds":{"DBLP":"journals/corr/abs-2402-07148","ArXiv":"2402.07148","DOI":"10.48550/arXiv.2402.07148","CorpusId":267627013},"title":"X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design"},{"paperId":"eec5c2a8c12d10b128c009cc6aea437df7a22292","externalIds":{"DBLP":"journals/corr/abs-2402-06247","ArXiv":"2402.06247","DOI":"10.1109/SANER60148.2024.00055","CorpusId":267617132},"title":"Delving into Parameter-Efficient Fine-Tuning in Code Change Learning: An Empirical Study"},{"paperId":"4d5cbfbba6e336075cf11c7229b38b098d9243d1","externalIds":{"ACL":"2024.emnlp-main.1168","DBLP":"conf/emnlp/LiaoHKM24","ArXiv":"2402.05147","DOI":"10.48550/arXiv.2402.05147","CorpusId":267547974},"title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model"},{"paperId":"840f759d110aea21ebbfe0523c435e11a2759aba","externalIds":{"DBLP":"conf/icml/HaoCM24","ArXiv":"2402.03293","DOI":"10.48550/arXiv.2402.03293","CorpusId":267412117},"title":"Flora: Low-Rank Adapters Are Secretly Gradient Compressors"},{"paperId":"42dfd70854193a89e2188af09e109ee1267d4002","externalIds":{"ArXiv":"2402.02347","DBLP":"conf/icml/ZhangP24","DOI":"10.48550/arXiv.2402.02347","CorpusId":267412914},"title":"Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models"},{"paperId":"ad02ebac21ac53278735984e8c115e40a29c3aa0","externalIds":{"ArXiv":"2402.01376","DBLP":"journals/corr/abs-2402-01376","DOI":"10.48550/arXiv.2402.01376","CorpusId":267406543},"title":"LoTR: Low Tensor Rank Weight Adaptation"},{"paperId":"d06a6de8fba28633d2d2bf06f2456c03d34bddfe","externalIds":{"ArXiv":"2401.17602","CorpusId":272078096},"title":"Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning"},{"paperId":"65fb348291de709a379a3f0d00b48726a1a674d2","externalIds":{"DBLP":"journals/corr/abs-2401-16635","ArXiv":"2401.16635","DOI":"10.48550/arXiv.2401.16635","CorpusId":267320613},"title":"Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble"},{"paperId":"bce43cb9af37a0c8d90f8cadaebd6bb002685edd","externalIds":{"ArXiv":"2401.16420","DBLP":"journals/corr/abs-2401-16420","DOI":"10.48550/arXiv.2401.16420","CorpusId":267311889},"title":"InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model"},{"paperId":"af9676f9beaeca214bfbf7f2897828820d48abdc","externalIds":{"DBLP":"journals/corr/abs-2401-16160","ArXiv":"2401.16160","DOI":"10.48550/arXiv.2401.16160","CorpusId":267312176},"title":"LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs"},{"paperId":"b312cc0d09c482f285f584951f5ac65278567ab2","externalIds":{"ArXiv":"2401.15266","DBLP":"journals/aei/YeLFN24","DOI":"10.48550/arXiv.2401.15266","CorpusId":267312103},"title":"SAM-based instance segmentation models for the automation of structural damage detection"},{"paperId":"e010c068d4d1136c45f4a06857c45a24b3f07d3f","externalIds":{"DBLP":"journals/corr/abs-2401-13220","ArXiv":"2401.13220","DOI":"10.1109/TNNLS.2025.3611322","CorpusId":267200053,"PubMed":"41144418"},"title":"Segment Any Cell: A SAM-Based Auto-Prompting Fine-Tuning Framework for Nuclei Segmentation"},{"paperId":"a40d25ee75ead977286ef9a6cb716a4cf77bc551","externalIds":{"ArXiv":"2401.12379","DBLP":"journals/corr/abs-2401-12379","DOI":"10.48550/arXiv.2401.12379","CorpusId":267095001},"title":"Analyzing the Effectiveness of Large Language Models on Text-to-SQL Synthesis"},{"paperId":"8f5f69741430500f421358a87f46ebadbc7bfe9e","externalIds":{"DBLP":"journals/corr/abs-2401-11240","ArXiv":"2401.11240","DOI":"10.48550/arXiv.2401.11240","CorpusId":267068417},"title":"CaraServe: CPU-Assisted and Rank-Aware LoRA Serving for Generative LLM Inference"},{"paperId":"81f5c258beca404a70880f89d1f803f4c8063e73","externalIds":{"DBLP":"journals/dase/ZhouSL24","DOI":"10.1007/s41019-023-00235-6","CorpusId":265296903},"title":"DB-GPT: Large Language Model Meets Database"},{"paperId":"1abc3d129e59130514e1e06530cc45e790b8c55c","externalIds":{"DBLP":"journals/corr/abs-2401-10341","ArXiv":"2401.10341","DOI":"10.48550/arXiv.2401.10341","CorpusId":261010411},"title":"ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks"},{"paperId":"4d7da69c61db94386424675d8fa01fb0a279f3b5","externalIds":{"ArXiv":"2401.06432","ACL":"2024.emnlp-main.717","DBLP":"conf/emnlp/ChoL0FJ24","DOI":"10.18653/v1/2024.emnlp-main.717","CorpusId":266977347},"title":"Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models"},{"paperId":"129e69afd09672620bb5000531b13c96510b5fec","externalIds":{"DBLP":"journals/corr/abs-2401-06374","ArXiv":"2401.06374","DOI":"10.48550/arXiv.2401.06374","CorpusId":266977409},"title":"SamLP: A Customized Segment Anything Model for License Plate Detection"},{"paperId":"5122bd001ec67d80543abe284bf7e0bf31da45d5","externalIds":{"DBLP":"journals/corr/abs-2401-04151","ArXiv":"2401.04151","DOI":"10.48550/arXiv.2401.04151","CorpusId":266899736},"title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning"},{"paperId":"cdd0e94e51a02bac22ca5e94fa95daa18f36e226","externalIds":{"ArXiv":"2401.00243","DBLP":"journals/corr/abs-2401-00243","DOI":"10.48550/arXiv.2401.00243","CorpusId":266693165},"title":"Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"},{"paperId":"c3bacc93d8c0c6ed31763d0c0fc7951f0ab82ed8","externalIds":{"DBLP":"journals/corr/abs-2312-17493","ArXiv":"2312.17493","DOI":"10.1145/3682068","CorpusId":266690822},"title":"Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning"},{"paperId":"5423067581ac9d51d19253c9085b871ef0373dfe","externalIds":{"DBLP":"journals/corr/abs-2312-15926","ArXiv":"2312.15926","DOI":"10.48550/arXiv.2312.15926","CorpusId":266551669},"title":"FedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models"},{"paperId":"bac400fdf382ac5de44accb3fb9e9f6863b5229e","externalIds":{"DBLP":"journals/tse/SilvaFM25","ArXiv":"2312.15698","DOI":"10.1109/TSE.2025.3581062","CorpusId":266551826},"title":"RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair"},{"paperId":"2d4a853affeb0b164fc1134df612aea658f36459","externalIds":{"DBLP":"journals/corr/abs-2312-12379","ArXiv":"2312.12379","DOI":"10.48550/arXiv.2312.12379","CorpusId":266362594},"title":"Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning"},{"paperId":"54c7bdf63719bb366987bb6e9e857335a479f4d9","externalIds":{"ArXiv":"2312.10794","DBLP":"journals/corr/abs-2312-10794","DOI":"10.48550/arXiv.2312.10794","CorpusId":266359450},"title":"A mathematical perspective on Transformers"},{"paperId":"9d793e542b757f234431d209e711c6ef88aa29de","externalIds":{"ArXiv":"2312.09979","CorpusId":266335873},"title":"LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin"},{"paperId":"050c7234ab51ec62ae54189cd1ad63b0236ef7be","externalIds":{"ArXiv":"2312.06899","DBLP":"journals/corr/abs-2312-06899","DOI":"10.48550/arXiv.2312.06899","CorpusId":266174608},"title":"LoRA-Enhanced Distillation on Guided Diffusion Models"},{"paperId":"b8e42491298ec32f62ff68e36fd9ce9c153b4af3","externalIds":{"ArXiv":"2312.06974","DBLP":"journals/corr/abs-2312-06974","DOI":"10.48550/arXiv.2312.06974","CorpusId":266174353},"title":"SM70: A Large Language Model for Medical Devices"},{"paperId":"c15380dcda5a010827e3b014dcebe95b1218c680","externalIds":{"DBLP":"journals/corr/abs-2312-06439","ArXiv":"2312.06439","DOI":"10.1109/CVPR52733.2024.00513","CorpusId":266162414},"title":"DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior"},{"paperId":"61d792bde3b4c1562fa35a639e92385b46dfdaa8","externalIds":{"DBLP":"conf/iclr/WenC24","ArXiv":"2312.05677","DOI":"10.48550/arXiv.2312.05677","CorpusId":266162346},"title":"Batched Low-Rank Adaptation of Foundation Models"},{"paperId":"1c6a5d033743f345447e45e1eb6d6c7cadee9f78","externalIds":{"ArXiv":"2312.05621","DBLP":"conf/emnlp/QiTSQXQ23","DOI":"10.18653/v1/2023.emnlp-industry.45","CorpusId":265751937},"title":"PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching"},{"paperId":"3333fa6dc9d39cad3d5cd87da9ae39e5a6aefe27","externalIds":{"ArXiv":"2312.04410","DBLP":"conf/cvpr/GuoXPNWVSHS24","DOI":"10.1109/CVPR52733.2024.00721","CorpusId":266054322},"title":"Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models"},{"paperId":"0c6111ba052c0c26985f427add713b8c740a4455","externalIds":{"ArXiv":"2312.03993","CorpusId":266044054},"title":"Style Transfer to Calvin and Hobbes comics using Stable Diffusion"},{"paperId":"a015eb5b187f1fb509f188a868ee549ecda98c26","externalIds":{"DBLP":"journals/corr/abs-2312-02216","ArXiv":"2312.02216","DOI":"10.48550/arXiv.2312.02216","CorpusId":265659457},"title":"DragVideo: Interactive Drag-style Video Editing"},{"paperId":"3aa03f01891f0fa5d749e1edce20438d7bd69ea6","externalIds":{"ArXiv":"2311.18763","DBLP":"conf/cvpr/SmithHKSJ22","DOI":"10.1109/CVPRW63382.2024.00181","CorpusId":265506232},"title":"Continual Diffusion with STAMINA: STack-And-Mask INcremental Adapters"},{"paperId":"e5389acae799cc712366cd58214d8d932b731fb1","externalIds":{"DBLP":"journals/corr/abs-2312-00085","ArXiv":"2312.00085","DOI":"10.48550/arXiv.2312.00085","CorpusId":265552030},"title":"X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation"},{"paperId":"d16f72b7be526dee5eb49e5afffeea2bddba5e66","externalIds":{"ArXiv":"2311.17946","DBLP":"journals/corr/abs-2311-17946","DOI":"10.48550/arXiv.2311.17946","CorpusId":265506678},"title":"DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback"},{"paperId":"5bc6cdf647d9d132584e73a844b8a3988cad3b83","externalIds":{"DBLP":"conf/cvpr/YooKKS24","ArXiv":"2311.16739","DOI":"10.1109/CVPR52733.2024.00413","CorpusId":265466179},"title":"As-Plausible-As-Possible: Plausibility-Aware Mesh Deformation Using 2D Diffusion Priors"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"b10d8f2913a18f90561e616881a42ae1f36d74c9","externalIds":{"ArXiv":"2311.14030","DBLP":"journals/corr/abs-2311-14030","DOI":"10.48550/arXiv.2311.14030","CorpusId":265445049},"title":"PrivateLoRA For Efficient Privacy Preserving LLM"},{"paperId":"dff9b29918369f2ce7c06d13258ffad5c644788a","externalIds":{"DBLP":"journals/tmlr/YadavCRB25","ArXiv":"2311.13171","DOI":"10.48550/arXiv.2311.13171","CorpusId":265351803},"title":"ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization"},{"paperId":"9711a5846444270bb05da8f58ebb355dd8d8f04e","externalIds":{"ArXiv":"2311.13617","DBLP":"journals/corr/abs-2311-13617","DOI":"10.48550/arXiv.2311.13617","CorpusId":265444945},"title":"Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to 3D Prior with Progressive Learning"},{"paperId":"70ded1d6e83a1cbeecec256a070c4b9ebfc6085f","externalIds":{"ArXiv":"2311.11696","DBLP":"journals/corr/abs-2311-11696","DOI":"10.48550/arXiv.2311.11696","CorpusId":265294736},"title":"Sparse Low-rank Adaptation of Pre-trained Language Models"},{"paperId":"244d4955ceb0f6e2b881e6c3e9638c26f31c23cf","externalIds":{"ArXiv":"2311.11501","DBLP":"journals/corr/abs-2311-11501","DOI":"10.48550/arXiv.2311.11501","CorpusId":265294849},"title":"MultiLoRA: Democratizing LoRA for Better Multi-Task Learning"},{"paperId":"d5da19399500a4b8728409237aa2608e47a4654f","externalIds":{"DBLP":"conf/aiccc/Belofsky23","ArXiv":"2311.10847","DOI":"10.1145/3639592.3639615","CorpusId":265294472},"title":"Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization"},{"paperId":"4c387de9f5d32b9990cd1a006fef17101ce98d4c","externalIds":{"ACL":"2024.naacl-long.481","DBLP":"conf/naacl/RenduchintalaKK24","ArXiv":"2311.09578","DOI":"10.18653/v1/2024.naacl-long.481","CorpusId":265221277},"title":"Tied-LoRA: Enhancing parameter efficiency of LoRA with Weight Tying"},{"paperId":"077e8f6d633c2ee7a7ba82579ac3d1fb98740785","externalIds":{"ArXiv":"2311.09773","DBLP":"journals/corr/abs-2311-09773","DOI":"10.48550/arXiv.2311.09773","CorpusId":265220755},"title":"To be or not to be? an exploration of continuously controllable prompt engineering"},{"paperId":"088617a8862cfa372a62070916e88a5f10e7690b","externalIds":{"ArXiv":"2311.09216","DBLP":"conf/eamt/MujadiaUBPSKS24","ACL":"2024.eamt-1.19","DOI":"10.48550/arXiv.2311.09216","CorpusId":265212810},"title":"Assessing Translation Capabilities of Large Language Models involving English and Indian Languages"},{"paperId":"525cd5d5c7d823d0b2ad4eaf086622940d45ed6d","externalIds":{"ArXiv":"2311.09179","DBLP":"journals/corr/abs-2311-09179","DOI":"10.48550/arXiv.2311.09179","CorpusId":265213347},"title":"SiRA: Sparse Mixture of Low Rank Adaptation"},{"paperId":"9d10dc85d2b4e9d468e6170ce389a12b7e5e9234","externalIds":{"DBLP":"journals/corr/abs-2311-05556","ArXiv":"2311.05556","DOI":"10.48550/arXiv.2311.05556","CorpusId":265067414},"title":"LCM-LoRA: A Universal Stable-Diffusion Acceleration Module"},{"paperId":"faa4c46e1cbd99e486c7dc2881e024b79967961b","externalIds":{"DBLP":"journals/corr/abs-2311-03285","ArXiv":"2311.03285","DOI":"10.48550/arXiv.2311.03285","CorpusId":265033787},"title":"S-LoRA: Serving Thousands of Concurrent LoRA Adapters"},{"paperId":"70bb5d11373625bd27cec8f0bce4e105d47850cd","externalIds":{"ArXiv":"2311.02428","DBLP":"journals/corr/abs-2311-02428","DOI":"10.48550/arXiv.2311.02428","CorpusId":265033920},"title":"Task Arithmetic with LoRA for Continual Learning"},{"paperId":"98b21ccaa73d8d822b2cd3e982503a6c7d541e97","externalIds":{"DBLP":"journals/corr/abs-2311-00339","ArXiv":"2311.00339","DOI":"10.48550/arXiv.2311.00339","CorpusId":264833207},"title":"Space Narrative: Generating Images and 3D Scenes of Chinese Garden from Text using Deep Learning"},{"paperId":"defe18a0a862b2024f1ea1671edd8b086a053c98","externalIds":{"DBLP":"journals/corr/abs-2310-19975","ArXiv":"2310.19975","DOI":"10.48550/arXiv.2310.19975","CorpusId":264744285,"PubMed":"38833265"},"title":"BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing"},{"paperId":"3e5e06bc5624b2408fec03c301cfe6c3a48747fa","externalIds":{"ArXiv":"2310.18840","DBLP":"journals/corr/abs-2310-18840","DOI":"10.1109/WACV57701.2024.00486","CorpusId":264590753},"title":"Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models"},{"paperId":"c2314751d367b34239a537fe27e2bd51a8b84528","externalIds":{"DBLP":"conf/mlsys/Chen0WZCK24","ArXiv":"2310.18547","DOI":"10.48550/arXiv.2310.18547","CorpusId":264590197},"title":"Punica: Multi-Tenant LoRA Serving"},{"paperId":"9153dc5312394e604cedc43af2a17a7ad4bb6743","externalIds":{"ArXiv":"2310.17513","DBLP":"journals/corr/abs-2310-17513","DOI":"10.48550/arXiv.2310.17513","CorpusId":264490889},"title":"The Expressive Power of Low-Rank Adaptation"},{"paperId":"ca53c1d1ba1a1386f860fa13d7729160571e1643","externalIds":{"DBLP":"journals/corr/abs-2310-18356","ArXiv":"2310.18356","DOI":"10.48550/arXiv.2310.18356","CorpusId":264590698},"title":"LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery"},{"paperId":"428282da6bf862a781faef25f88394ccdb63610e","externalIds":{"DBLP":"journals/corr/abs-2310-15539","ArXiv":"2310.15539","DOI":"10.48550/arXiv.2310.15539","CorpusId":264438914},"title":"SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation"},{"paperId":"2bc796d5aff8e4323ebb7b2d62da34384f765e32","externalIds":{"DBLP":"conf/sigir/Liu00ZX0024","ArXiv":"2310.18339","DOI":"10.1145/3626772.3657722","CorpusId":264590549},"title":"When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications"},{"paperId":"9af2b40a6d9edeb3d53d7e612018bdbff993ffd2","externalIds":{"ArXiv":"2310.13448","DBLP":"conf/emnlp/AlvesGAPRSCM23","DOI":"10.48550/arXiv.2310.13448","CorpusId":264405904},"title":"Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning"},{"paperId":"5f687f930e9a3e15082ceb616cd267cf8a92b5f8","externalIds":{"ArXiv":"2310.13283","CorpusId":267627316},"title":"pFedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning"},{"paperId":"25738c43c0c4788d803981eaf5d397691aba0958","externalIds":{"ArXiv":"2310.12798","DBLP":"journals/corr/abs-2310-12798","DOI":"10.48550/arXiv.2310.12798","CorpusId":264306303},"title":"MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter"},{"paperId":"f405233e555547c63b96da1ae242817b023c4b37","externalIds":{"DBLP":"journals/nn/ZhangWWTLWQ25","ArXiv":"2310.11374","DOI":"10.1016/j.neunet.2025.107901","CorpusId":264172431,"PubMed":"40752409"},"title":"DialogueLLM: Context and emotion knowledge-tuned large language models for emotion recognition in conversations"},{"paperId":"f637047c326329dfc4fdb945cd1264214d47c30a","externalIds":{"DOI":"10.3390/math11204317","CorpusId":264336659},"title":"Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning"},{"paperId":"1533c2cbfa402f7f6caf0d126be7864795afbe9d","externalIds":{"DBLP":"conf/icassp/ChenHAHPLGBG24","ArXiv":"2310.09424","DOI":"10.1109/ICASSP48485.2024.10447553","CorpusId":264146423},"title":"SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation"},{"paperId":"af8123ecdff838f63e4eba0b36b8babe4c5cee65","externalIds":{"ArXiv":"2310.08659","DBLP":"conf/iclr/Li00KHCZ24","DOI":"10.48550/arXiv.2310.08659","CorpusId":264128197},"title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"},{"paperId":"baeb8b5869eb94a2e43ea8cd0df3b8077ec8e91c","externalIds":{"DBLP":"journals/bspc/ChenWNLS25","ArXiv":"2310.07183","DOI":"10.48550/arXiv.2310.07183","CorpusId":263835267},"title":"SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation"},{"paperId":"a09cbe54f842b9b243de4307349cb2800f3044d3","externalIds":{"ArXiv":"2310.04742","DBLP":"conf/iclr/Tang00Z000T24","DOI":"10.48550/arXiv.2310.04742","CorpusId":263831551},"title":"Parameter Efficient Multi-task Model Fusion with Partial Linearization"},{"paperId":"17ca659a9d0fde83b0e7e21f66593d645b7dcc82","externalIds":{"ArXiv":"2310.01208","DBLP":"journals/corr/abs-2310-01208","DOI":"10.48550/arXiv.2310.01208","CorpusId":263605499},"title":"Label Supervised LLaMA Finetuning"},{"paperId":"cb62be8c85f1dd0e7c4ea24ed4feb5b90229ee25","externalIds":{"ArXiv":"2310.00035","DBLP":"journals/corr/abs-2310-00035","DOI":"10.48550/arXiv.2310.00035","CorpusId":263334363},"title":"LoRA ensembles for large language model fine-tuning"},{"paperId":"945db0077b6d19b720f5998b3f61300013c4f885","externalIds":{"ArXiv":"2309.14717","DBLP":"conf/iclr/XuXG0CZC0024","DOI":"10.48550/arXiv.2309.14717","CorpusId":262825568},"title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"},{"paperId":"b6346f9fa093b8e85df712485a2b851b9f680dac","externalIds":{"DBLP":"journals/corr/abs-2309-12307","ArXiv":"2309.12307","DOI":"10.48550/arXiv.2309.12307","CorpusId":262084134},"title":"LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"},{"paperId":"e54ab1b3cf2fe3f695e0bfc0bde55c4aacd68249","externalIds":{"DBLP":"journals/corr/abs-2309-09902","ACL":"2024.jlcl-1.1","ArXiv":"2309.09902","DOI":"10.21248/jlcl.37.2024.244","CorpusId":262044336},"title":"Speaker Attribution in German Parliamentary Debates with QLoRA-adapted Large Language Models"},{"paperId":"e56dc21699e6283fce072ffc908cb9f66321760d","externalIds":{"DBLP":"journals/corr/abs-2309-09055","ArXiv":"2309.09055","DOI":"10.48550/arXiv.2309.09055","CorpusId":261884455},"title":"Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF"},{"paperId":"587d0627031c165985c69036f62d5d21fc38e3f7","externalIds":{"ArXiv":"2309.02411","DBLP":"journals/corr/abs-2309-02411","DOI":"10.48550/arXiv.2309.02411","CorpusId":261556652},"title":"Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices"},{"paperId":"c12db2e67d1fb289266faa5507ff112c9a062465","externalIds":{"DBLP":"journals/corr/abs-2309-00754","ArXiv":"2309.00754","DOI":"10.48550/arXiv.2309.00754","CorpusId":261530070},"title":"Efficient RLHF: Reducing the Memory Usage of PPO"},{"paperId":"7a4fd15c536e2c77a13daaa6b85cb4489d022c9f","externalIds":{"ArXiv":"2308.14256","CorpusId":266210188},"title":"FaceChain: A Playground for Human-centric Artificial Intelligence Generated Content"},{"paperId":"c2764eeb1c7d1c7eb6287505e54818c032ca3d7e","externalIds":{"DBLP":"journals/corr/abs-2308-14133","ArXiv":"2308.14133","DOI":"10.48550/arXiv.2308.14133","CorpusId":261242605},"title":"Cheap Lunch for Medical Image Segmentation by Fine-tuning SAM on Few Exemplars"},{"paperId":"1882849855895456fe842203f245ffaf66b72eff","externalIds":{"DBLP":"journals/corr/abs-2308-13111","ArXiv":"2308.13111","DOI":"10.48550/arXiv.2308.13111","CorpusId":261214713},"title":"Bayesian low-rank adaptation for large language models"},{"paperId":"40514026a8d770b0393298a707c8be742f89367e","externalIds":{"DBLP":"journals/corr/abs-2308-13032","ArXiv":"2308.13032","DOI":"10.48550/arXiv.2308.13032","CorpusId":261214796},"title":"Financial News Analytics Using Fine-Tuned Llama 2 GPT Model"},{"paperId":"eaf64cca235f3e6ddc3633c5378bc795c61c25f6","externalIds":{"DBLP":"journals/corr/abs-2308-12043","ArXiv":"2308.12043","DOI":"10.48550/arXiv.2308.12043","CorpusId":261076438},"title":"IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"},{"paperId":"5e779145a1f40edce3dc01e1be06530776bcf4db","externalIds":{"DBLP":"conf/miccai/WangIXZR23","ArXiv":"2308.07156","DOI":"10.48550/arXiv.2308.07156","CorpusId":260887698},"title":"SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation"},{"paperId":"5ce0fa3f0882910a5dfa2697bb04c8785d914725","externalIds":{"ArXiv":"2308.06522","DBLP":"journals/corr/abs-2308-06522","DOI":"10.48550/arXiv.2308.06522","CorpusId":260887495},"title":"SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models"},{"paperId":"8ce219059d777c2333ee21cb2af2aad71275c98f","externalIds":{"ArXiv":"2308.03303","DBLP":"journals/corr/abs-2308-03303","DOI":"10.48550/arXiv.2308.03303","CorpusId":260683267},"title":"LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"},{"paperId":"3f459219d75de63b5b7a26a8c6447ec1e79a985c","externalIds":{"DBLP":"journals/corr/abs-2307-13269","ArXiv":"2307.13269","DOI":"10.48550/arXiv.2307.13269","CorpusId":260155012},"title":"LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition"},{"paperId":"ae23def9454956869987b4bab2c6de7c060fd217","externalIds":{"DBLP":"conf/iccvw/Khandelwal23a","ArXiv":"2308.00135","DOI":"10.1109/ICCVW60793.2023.00324","CorpusId":260351466},"title":"InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing"},{"paperId":"e2628754de142e2a9733f5889124fef540b58005","externalIds":{"ArXiv":"2307.10616","DBLP":"journals/csur/YeFDYT24","DOI":"10.1145/3625558","CorpusId":259991420},"title":"Heterogeneous Federated Learning: State-of-the-art and Research Challenges"},{"paperId":"972b2c4dcb9712d5cd1c5d9f06a5ac0c5e084350","externalIds":{"DBLP":"journals/corr/abs-2307-10512","ArXiv":"2307.10512","DOI":"10.48550/arXiv.2307.10512","CorpusId":259991244},"title":"IvyGPT: InteractiVe Chinese pathwaY language model in medical domain"},{"paperId":"6121fb3e393597e02481a516f0035f06ec9a5836","externalIds":{"DBLP":"journals/corr/abs-2307-10485","ArXiv":"2307.10485","DOI":"10.48550/arXiv.2307.10485","CorpusId":259991068},"title":"FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"0d2adcddccd72de47c263b6e4e0aab3dd0582a52","externalIds":{"DBLP":"conf/iclr/LialinMSR24","ArXiv":"2307.05695","CorpusId":259836974},"title":"ReLoRA: High-Rank Training Through Low-Rank Updates"},{"paperId":"12bbff41483f1a4b31882a0a0a2fe310468a9fd3","externalIds":{"DBLP":"conf/clef/SuriMSS23","ArXiv":"2307.05162","DOI":"10.48550/arXiv.2307.05162","CorpusId":259766045},"title":"SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization"},{"paperId":"c1caa303549764d220ff17dc1785985dd1ba6047","externalIds":{"DBLP":"journals/corr/abs-2307-04725","ArXiv":"2307.04725","DOI":"10.48550/arXiv.2307.04725","CorpusId":259501509},"title":"AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning"},{"paperId":"6651eb8205e3d90c420fbdf8a2740c74e590e545","externalIds":{"DBLP":"conf/acl-clinicalnlp/GemaMDHA24","ACL":"2024.clinicalnlp-1.9","ArXiv":"2307.03042","DOI":"10.48550/arXiv.2307.03042","CorpusId":259361061},"title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain"},{"paperId":"7f1a473834eea608980e4e04cce21be18d65b9b6","externalIds":{"ArXiv":"2306.14870","DBLP":"conf/nips/ZhangCLH23","DOI":"10.48550/arXiv.2306.14870","CorpusId":259262373},"title":"Composing Parameter-Efficient Modules with Arithmetic Operations"},{"paperId":"1e09b83fe064826a9a1ac61a7bdc00f26be41aee","externalIds":{"DBLP":"journals/corr/abs-2306-07954","ArXiv":"2306.07954","DOI":"10.1145/3610548.3618160","CorpusId":259144797},"title":"Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation"},{"paperId":"8efe0fda5b5249dfcc860af5df3ace90392a1f7c","externalIds":{"ArXiv":"2305.19124","DBLP":"journals/corr/abs-2305-19124","DOI":"10.48550/arXiv.2305.19124","CorpusId":258967805},"title":"Calliffusion: Chinese Calligraphy Generation and Style Transfer with Diffusion Modeling"},{"paperId":"5728ecb3a11c1586c4ae53e11ab395a0263eb5f4","externalIds":{"ArXiv":"2305.18292","DBLP":"conf/nips/GuWWSCFXZCWGSS23","DOI":"10.48550/arXiv.2305.18292","CorpusId":258960192},"title":"Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models"},{"paperId":"b463f5dd8ed6871ddeac03754c1c2d99547b08fe","externalIds":{"ArXiv":"2305.18403","DBLP":"conf/acl/Zhang0SYOYZ24","DOI":"10.18653/v1/2024.findings-acl.178","CorpusId":258967906},"title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"},{"paperId":"4e16bfc8ded08fbec67666869f39c043a6770946","externalIds":{"DBLP":"conf/acl/LiaoMM23","ACL":"2023.acl-long.233","ArXiv":"2305.16742","DOI":"10.48550/arXiv.2305.16742","CorpusId":258947572},"title":"Parameter-Efficient Fine-Tuning without Introducing New Latency"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"7787efaf502421eac9b6b0fd946a82e1ecf4c8c9","externalIds":{"DBLP":"journals/corr/abs-2305-11067","ArXiv":"2305.11067","DOI":"10.48550/arXiv.2305.11067","CorpusId":258762379},"title":"Generating coherent comic with rich story using ChatGPT and Stable Diffusion"},{"paperId":"6178c99328984452551b0356b64d048d341b9a9e","externalIds":{"DBLP":"journals/corr/abs-2305-08285","ArXiv":"2305.08285","DOI":"10.48550/arXiv.2305.08285","CorpusId":258762898},"title":"Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence Modeling"},{"paperId":"e5526fcee23b6dc7b7bf0d83607d88d12cf6baf2","externalIds":{"DBLP":"conf/nips/GeshkovskiLPR23","ArXiv":"2305.05465","DOI":"10.48550/arXiv.2305.05465","CorpusId":258564860},"title":"The emergence of clusters in self-attention dynamics"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","externalIds":{"DBLP":"journals/corr/abs-2304-14178","ArXiv":"2304.14178","DOI":"10.48550/arXiv.2304.14178","CorpusId":258352455},"title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"705315602e9e78b155220169d7704475efeb4a11","externalIds":{"DBLP":"journals/corr/abs-2304-13785","ArXiv":"2304.13785","DOI":"10.48550/arXiv.2304.13785","CorpusId":258352583},"title":"Customized Segment Anything Model for Medical Image Segmentation"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"7dd34b31d9b2528532efc6b0e0495090a0226046","externalIds":{"DBLP":"conf/mipr/KongLW23","ArXiv":"2303.00917","DOI":"10.1109/MIPR59079.2023.00033","CorpusId":257280118},"title":"Enhancing General Face Forgery Detection via Vision Transformer with Low-Rank Adaptation"},{"paperId":"76b19363b10d7ea783e4a6494eae40d73c8e9628","externalIds":{"DBLP":"journals/natmi/DingQYWYSHCCCYZWLZCLTLS23","DOI":"10.1038/s42256-023-00626-4","CorpusId":257316425},"title":"Parameter-efficient fine-tuning of large-scale pre-trained language models"},{"paperId":"5aa7bdcae38076b80229c0a024f5b656ac6607af","externalIds":{"DBLP":"journals/corr/abs-2212-10650","ArXiv":"2212.10650","DOI":"10.48550/arXiv.2212.10650","CorpusId":254926823},"title":"KronA: Parameter Efficient Tuning with Kronecker Adapter"},{"paperId":"2dc07d25a22467c01b6d5088ea148e58eb18cc2a","externalIds":{"DBLP":"journals/corr/abs-2212-05901","ArXiv":"2212.05901","DOI":"10.48550/arXiv.2212.05901","CorpusId":254564456},"title":"Parameter-Efficient Finetuning of Transformers for Source Code"},{"paperId":"6b3e939d93c82c269f552e7e2050524c3ad9b73b","externalIds":{"DBLP":"journals/corr/abs-2211-09790","ArXiv":"2211.09790","DOI":"10.1109/CVPR52729.2023.01440","CorpusId":253581517},"title":"ConStruct-VL: Data-Free Continual Structured VL Concepts Learning*"},{"paperId":"85e959eef45114974c8f8643e88af23936fff3d1","externalIds":{"DBLP":"journals/corr/abs-2210-07558","ACL":"2023.eacl-main.239","ArXiv":"2210.07558","DOI":"10.48550/arXiv.2210.07558","CorpusId":252907428},"title":"DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation"},{"paperId":"9a1d94a930168918a1a1e1939b089d16d58d7865","externalIds":{"DBLP":"journals/corr/abs-2210-05643","ArXiv":"2210.05643","DOI":"10.48550/arXiv.2210.05643","CorpusId":252815771},"title":"A Kernel-Based View of Language Model Fine-Tuning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"af593c53a9221bd12211f78d4f1ebd6b59cc4e7c","externalIds":{"DBLP":"conf/aaai/HeLZYW23","ArXiv":"2203.16329","DOI":"10.1609/aaai.v37i1.25160","CorpusId":254247258},"title":"Parameter-Efficient Model Adaptation for Vision Transformers"},{"paperId":"2002afb402fb55bd42108e3cabffe2996bdf5e37","externalIds":{"DBLP":"journals/corr/abs-2202-09817","ArXiv":"2202.09817","DOI":"10.1007/s11704-023-3131-8","CorpusId":247011913},"title":"\n \n \n \n $$\\cal{Y}$$\n \n \n Y\n \n \n -Tuning: an efficient tuning paradigm for large-scale pre-trained models via label representation learning"},{"paperId":"fe9d978f7718474e9613bac114c398614f09be71","externalIds":{"DBLP":"conf/aistats/SanderABP22","ArXiv":"2110.11773","CorpusId":239616044},"title":"Sinkformers: Transformers with Doubly Stochastic Attention"},{"paperId":"43a87867fe6bf4eb920f97fc753be4b727308923","externalIds":{"DBLP":"journals/corr/abs-2110-04366","ArXiv":"2110.04366","CorpusId":238583580},"title":"Towards a Unified View of Parameter-Efficient Transfer Learning"},{"paperId":"8415db10a4e2354b7d57b75848f1485ce5d0e7ce","externalIds":{"DBLP":"conf/nips/DaxbergerKIEBH21","ArXiv":"2106.14806","CorpusId":235658031},"title":"Laplace Redux - Effortless Bayesian Deep Learning"},{"paperId":"339b2b711fb5b228d097b03ebc3e62a521779235","externalIds":{"DBLP":"conf/acl/ZakenGR22","ACL":"2022.acl-short.1","ArXiv":"2106.10199","DOI":"10.18653/v1/2022.acl-short.1","CorpusId":231672601},"title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"656ed155c2d345c19d9bff4b50f2ae00db8407cc","externalIds":{"ArXiv":"2106.04647","DBLP":"conf/nips/MahabadiHR21","CorpusId":235356070},"title":"Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"e54ffc76d805c48660bb0fd20019ca82ac94ba0d","externalIds":{"ArXiv":"2012.13255","DBLP":"conf/acl/AghajanyanGZ20","ACL":"2021.acl-long.568","DOI":"10.18653/v1/2021.acl-long.568","CorpusId":229371560},"title":"Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"26051e0070a59444eefe5bed36627703235405b7","externalIds":{"DOI":"10.1007/1-4020-0613-6_16833","CorpusId":240640357},"title":"Segment"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"7a84a692327534fd227fa1e07fcb3816b633c591","externalIds":{"MAG":"2809090039","ArXiv":"1806.07572","DBLP":"conf/nips/JacotHG18","CorpusId":49321232},"title":"Neural Tangent Kernel: Convergence and Generalization in Neural Networks"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"d5cec2e4f53a0e749b9ea22697578917fdff00aa","externalIds":{"MAG":"2101677491","DBLP":"conf/icec/HansenO96","DOI":"10.1109/ICEC.1996.542381","CorpusId":10836085},"title":"Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation"},{"paperId":"37ae0ead03605de4c72f6d77af19b9001c0937f9","externalIds":{"MAG":"2136885855","DOI":"10.1002/NAV.3800030109","CorpusId":122654717},"title":"An algorithm for quadratic programming"},{"paperId":"ae0a8ca6eb067080e53b9618bb2653e8f02fe620","externalIds":{"DBLP":"journals/corr/abs-2402-04902","DOI":"10.48550/arXiv.2402.04902","CorpusId":267523125},"title":"L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ"},{"paperId":"a0748478cd2752b733b4183dbd0dcd1031c38b6e","externalIds":{"DBLP":"journals/corr/abs-2403-10704","DOI":"10.48550/arXiv.2403.10704","CorpusId":268513245},"title":"PERL: Parameter Efficient Reinforcement Learning from Human Feedback"},{"paperId":"a5ee7c43194332ccab7f22a51605660abd99bf41","externalIds":{"DBLP":"journals/corr/abs-2401-17602","DOI":"10.48550/arXiv.2401.17602","CorpusId":267523753},"title":"Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning"},{"paperId":"72f8c25d0f72322c59efd7681155f221c7440c33","externalIds":{"DBLP":"journals/corr/abs-2408-04556","DOI":"10.48550/arXiv.2408.04556","CorpusId":276450724},"title":"Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models"},{"paperId":"5ef82a8c8aa50f99285f2143b57ca4e82da1af80","externalIds":{"DBLP":"journals/corr/abs-2303-10512","DOI":"10.48550/arXiv.2303.10512","CorpusId":257631760},"title":"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"},{"paperId":"1fb8f2d080e965c833c777f06fccf09dc9856b91","externalIds":{"DBLP":"journals/corr/abs-2310-18339","DOI":"10.48550/arXiv.2310.18339","CorpusId":271065459},"title":"MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications"},{"paperId":"cf874e0c25f34cde830718724c3bd57f4c2badce","externalIds":{"DBLP":"journals/corr/abs-2311-02684","DOI":"10.48550/arXiv.2311.02684","CorpusId":265033625},"title":"Octavius: Mitigating Task Interference in MLLMs via MoE"},{"paperId":"61b0f5cfd4f951632435707948201474e16e835b","externalIds":{"DBLP":"journals/corr/abs-2305-12031","DOI":"10.48550/arXiv.2305.12031","CorpusId":258832351},"title":"Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding"},{"paperId":"9db1ae5f17624020598a52abe961d894dc365c7a","externalIds":{"DBLP":"journals/corr/abs-2309-01770","DOI":"10.48550/arXiv.2309.01770","CorpusId":261531689},"title":"StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation"},{"paperId":"69863ca2b6bd49f0f76cab8503db596397bf4dd9","externalIds":{"DBLP":"journals/corr/abs-2310-01886","DOI":"10.48550/arXiv.2310.01886","CorpusId":271860528},"title":"Effective and Parameter-Efficient Reusing Fine-Tuned Models"},{"paperId":"4e6d82ffde7a9dcb4300c64340abbeaf50501793","externalIds":{"DBLP":"journals/corr/abs-2312-02515","DOI":"10.48550/arXiv.2312.02515","CorpusId":275516672},"title":"ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU"},{"paperId":"05597671940c12260efd05a0f3b36285eb5fd7b3","externalIds":{"DBLP":"journals/corr/abs-2312-03993","DOI":"10.48550/arXiv.2312.03993","CorpusId":281103901},"title":"Style Transfer to Calvin and Hobbes comics using Stable Diffusion"}]}