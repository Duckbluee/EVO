{"paperId":"5760218e4635cc2841dc7fba1752427a023c2193","externalIds":{"DBLP":"journals/fcsc/XuHZQXYG25","ArXiv":"2404.00929","DOI":"10.1007/s11704-024-40579-4","CorpusId":268819377},"title":"A survey on multilingual large language models: corpora, alignment, and bias","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2257136845","name":"Yuemei Xu"},{"authorId":"2258334185","name":"Ling Hu"},{"authorId":"2294513520","name":"Jiayi Zhao"},{"authorId":"2294361104","name":"Zihan Qiu"},{"authorId":"2294363807","name":"Yuqi Ye"},{"authorId":"2294933103","name":"Hanwen Gu"}],"abstract":"Based on the foundation of Large Language Models (LLMs), Multilingual LLMs (MLLMs) have been developed to address the challenges faced in multilingual natural language processing, hoping to achieve knowledge transfer from high-resource languages to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolutions, key techniques, and multilingual capacities. Secondly, we explore the multilingual training corpora of MLLMs and the multilingual datasets oriented for downstream tasks that are crucial to enhance the cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art studies of multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs, including its categories, evaluation metrics, and debiasing techniques. Finally, we discuss existing challenges and point out promising research directions of MLLMs."}