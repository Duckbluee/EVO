{"abstract":"Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of <italic>rigid</italic> scenes. A key challenge in making such methods applicable to <italic>articulated</italic> objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of <inline-formula><tex-math notation=\"LaTeX\">$150\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>150</mml:mn><mml:mo>Ã—</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"chen-ieq1-3271569.gif\"/></alternatives></inline-formula>. These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans."}