{"references":[{"paperId":"2d8c62fffb181768a7fc89db09f5f8f068a1ed98","externalIds":{"ArXiv":"2404.13903","DBLP":"conf/eccv/XuSFLGZW24","DOI":"10.48550/arXiv.2404.13903","CorpusId":269293852},"title":"Accelerating Image Generation with Sub-path Linear Approximation Model"},{"paperId":"2900738d1d9a51726b6baae203d100e379f74418","externalIds":{"ArXiv":"2403.12962","DBLP":"journals/corr/abs-2403-12962","DOI":"10.1109/CVPR52733.2024.00831","CorpusId":268531779},"title":"Fresco: Spatial-Temporal Correspondence for Zero-Shot Video Translation"},{"paperId":"6e0543f8371e524c23f86e4d2277f94955bba80f","externalIds":{"ArXiv":"2403.06098","DBLP":"conf/nips/WangY24","DOI":"10.48550/arXiv.2403.06098","CorpusId":269293420},"title":"VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models"},{"paperId":"8eae862d9669e7001eeee17b49fba793df9672c4","externalIds":{"ArXiv":"2402.19479","DBLP":"conf/cvpr/ChenSMDCJF0RYT24","DOI":"10.1109/CVPR52733.2024.01265","CorpusId":268091168},"title":"Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers"},{"paperId":"9761bcf49892601a3bec07d616c13c7f8bb7ac6c","externalIds":{"DBLP":"journals/corr/abs-2402-17525","ArXiv":"2402.17525","DOI":"10.1109/TPAMI.2025.3541625","CorpusId":268033671,"PubMed":"40031849"},"title":"Diffusion Model-Based Image Editing: A Survey"},{"paperId":"66a05b7405aa3591a8fb74e5958c8d6dc994606e","externalIds":{"DBLP":"journals/corr/abs-2402-13185","ArXiv":"2402.13185","DOI":"10.1145/3746027.3755462","CorpusId":267760278},"title":"UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing"},{"paperId":"d599dc40c9cb8d6d76554ee7d21d20c22cc7cdb5","externalIds":{"ArXiv":"2402.04324","DBLP":"journals/tmlr/RenYZWDHC24","DOI":"10.48550/arXiv.2402.04324","CorpusId":267523282},"title":"ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation"},{"paperId":"8af695e2ca1ee13524caf9ceffdaf5395131241e","externalIds":{"DBLP":"journals/corr/abs-2402-01166","ArXiv":"2402.01166","DOI":"10.48550/arXiv.2402.01166","CorpusId":267406408},"title":"A Comprehensive Survey on 3D Content Generation"},{"paperId":"9cd6c6d85de6180dd92ba43e685663067cf3ab7f","externalIds":{"ArXiv":"2401.14159","DBLP":"journals/corr/abs-2401-14159","DOI":"10.48550/arXiv.2401.14159","CorpusId":267212047},"title":"Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks"},{"paperId":"492bc8339d8aac442c4ec13f8c1d59e980a3af2f","externalIds":{"DBLP":"conf/cvpr/ChenZCXWWS24","ArXiv":"2401.09047","DOI":"10.1109/CVPR52733.2024.00698","CorpusId":267028095},"title":"VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models"},{"paperId":"0f9b66c9208b11369e9d94d85b7dc23bcc5115e9","externalIds":{"DBLP":"journals/corr/abs-2401-07519","ArXiv":"2401.07519","DOI":"10.48550/arXiv.2401.07519","CorpusId":266999462},"title":"InstantID: Zero-shot Identity-Preserving Generation in Seconds"},{"paperId":"e0eac8c64be3313e581c28a495bec192e7e67284","externalIds":{"ArXiv":"2401.03048","DBLP":"journals/tmlr/Ma0CJ0LC025","CorpusId":266844878},"title":"Latte: Latent Diffusion Transformer for Video Generation"},{"paperId":"43bafa19f94a42caea89b32a86489aa850317617","externalIds":{"DBLP":"conf/cvpr/LiangWWYLZMHZVM24","ArXiv":"2312.17681","DOI":"10.1109/CVPR52733.2024.00784","CorpusId":266690780},"title":"FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis"},{"paperId":"e0d62e25811018636c22d2cc76650b9d31968890","externalIds":{"DBLP":"conf/cvpr/WuCWJKXLYV24","ArXiv":"2312.13834","DOI":"10.1109/CVPR52733.2024.00789","CorpusId":266435967},"title":"Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis"},{"paperId":"3f7cca8661cfd609f18e7d0da0db3d4ab2b2e43a","externalIds":{"DBLP":"conf/cvpr/LiMYY24","ArXiv":"2312.10656","DOI":"10.1109/CVPR52733.2024.00715","CorpusId":266348383},"title":"VidToMe: Video Token Merging for Zero-Shot Video Editing"},{"paperId":"ffd746c472990fdc29c95a95268954b271a91ca3","externalIds":{"ArXiv":"2312.04524","DBLP":"journals/corr/abs-2312-04524","DOI":"10.1109/CVPR52733.2024.00622","CorpusId":266053899},"title":"RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models"},{"paperId":"28315d5dbafab7c5603d1365aea06a5c94506532","externalIds":{"DBLP":"journals/corr/abs-2312-02503","ArXiv":"2312.02503","DOI":"10.48550/arXiv.2312.02503","CorpusId":265659524},"title":"SAVE: Protagonist Diversification with Structure Agnostic Video Editing"},{"paperId":"6afcba8bee68230123c41093e43e6a2bedd62c5f","externalIds":{"ArXiv":"2312.03772","DBLP":"journals/corr/abs-2312-03772","DOI":"10.48550/arXiv.2312.03772","CorpusId":266051993},"title":"DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing"},{"paperId":"58dfc9cfc39787322a74cc93c22c9d3028aa9ad7","externalIds":{"DBLP":"journals/corr/abs-2312-02936","ArXiv":"2312.02936","DOI":"10.48550/arXiv.2312.02936","CorpusId":265659106},"title":"Drag-A-Video: Non-rigid Video Editing with Point-based Interaction"},{"paperId":"3754df53a5135064ba312251ad47b478c4afb2ef","externalIds":{"DBLP":"conf/cvpr/GuZWYL0WZST24","ArXiv":"2312.02087","DOI":"10.1109/CVPR52733.2024.00728","CorpusId":265609343},"title":"VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence"},{"paperId":"a015eb5b187f1fb509f188a868ee549ecda98c26","externalIds":{"DBLP":"journals/corr/abs-2312-02216","ArXiv":"2312.02216","DOI":"10.48550/arXiv.2312.02216","CorpusId":265659457},"title":"DragVideo: Interactive Drag-style Video Editing"},{"paperId":"c562f5b6cbcffb2a42df39b5a7ca9a7cd17c5e5f","externalIds":{"ArXiv":"2312.00845","DBLP":"conf/cvpr/JeongPY24","DOI":"10.1109/CVPR52733.2024.00880","CorpusId":265608824},"title":"VMC: Video Motion Customization Using Temporal Attention Adaption for Text-to-Video Diffusion Models"},{"paperId":"875358079869189aa185335c621f7e9346b2f5a5","externalIds":{"ArXiv":"2311.18830","DBLP":"journals/corr/abs-2311-18830","DOI":"10.1109/CVPR52733.2024.00753","CorpusId":265506039},"title":"MotionEditor: Editing Video Motion via Content-Aware Diffusion"},{"paperId":"516a4c05883d4dc130bb7a638ad8e427b6756aac","externalIds":{"DBLP":"journals/corr/abs-2311-18828","ArXiv":"2311.18828","DOI":"10.1109/CVPR52733.2024.00632","CorpusId":265506768},"title":"One-Step Diffusion with Distribution Matching Distillation"},{"paperId":"fc9994f5d0b966b16cbe61e6393b9f8be65bc416","externalIds":{"ArXiv":"2311.18827","DBLP":"journals/corr/abs-2311-18827","DOI":"10.48550/arXiv.2311.18827","CorpusId":265506378},"title":"Motion-Conditioned Image Animation for Video Editing"},{"paperId":"4e9a8141da2a8c603722b07d096109207f8e0b66","externalIds":{"DBLP":"journals/corr/abs-2311-17982","ArXiv":"2311.17982","DOI":"10.1109/CVPR52733.2024.02060","CorpusId":265506207},"title":"VBench: Comprehensive Benchmark Suite for Video Generative Models"},{"paperId":"654c66239fd00174c22d9fb0dc7e6060022632f3","externalIds":{"DBLP":"conf/cvpr/BrackFKTSKP24","ArXiv":"2311.16711","DOI":"10.1109/CVPR52733.2024.00846","CorpusId":265466786},"title":"LEDITS++: Limitless Image Editing Using Text-to-Image Models"},{"paperId":"c8dc4af5c61f95cc79b7f83e8339efa62af8f811","externalIds":{"ArXiv":"2311.17117","DBLP":"conf/cvpr/Hu24","DOI":"10.1109/CVPR52733.2024.00779","CorpusId":265499043},"title":"Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"},{"paperId":"efb91bed09e03b8a0e9cae23052b1ae537906254","externalIds":{"DBLP":"conf/emnlp/YuZHSC24","ACL":"2024.emnlp-main.1137","ArXiv":"2311.17041","DOI":"10.18653/v1/2024.emnlp-main.1137","CorpusId":265466889},"title":"Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties"},{"paperId":"d730d42bb655b3b44727d71c147f9758612043a8","externalIds":{"DBLP":"journals/corr/abs-2311-17042","ArXiv":"2311.17042","DOI":"10.48550/arXiv.2311.17042","CorpusId":265466173},"title":"Adversarial Diffusion Distillation"},{"paperId":"9d587b3f1d0608bf89fdb4ea6eed3312a73e938c","externalIds":{"DBLP":"conf/cvpr/XuZLYLZFS24","ArXiv":"2311.16498","DOI":"10.1109/CVPR52733.2024.00147","CorpusId":265466012},"title":"MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"30bad170cc8a0834f0d54f1e89a863a6361f15bd","externalIds":{"DBLP":"conf/icml/ChangSGXFSYZY024","ArXiv":"2311.12052","CorpusId":265309002},"title":"MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion"},{"paperId":"85b10400864187230714506412c85610c786b5c3","externalIds":{"ArXiv":"2311.10709","DBLP":"conf/eccv/GirdharSBDARSYPM24","DOI":"10.48550/arXiv.2311.10709","CorpusId":265281059},"title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning"},{"paperId":"9b86ce1bde87b304141641b49299f4d0f1f7ba1d","externalIds":{"ArXiv":"2311.04145","DBLP":"journals/corr/abs-2311-04145","DOI":"10.48550/arXiv.2311.04145","CorpusId":265043460},"title":"I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models"},{"paperId":"e8bbffb8413cb1f88e99a7ecbabd21a6eac82271","externalIds":{"DBLP":"conf/iclr/ChengX024","ArXiv":"2311.00213","DOI":"10.48550/arXiv.2311.00213","CorpusId":264833165},"title":"Consistent Video-to-Video Transfer Using Synthetic Dataset"},{"paperId":"b020cac5955b48c22ac59fa74bc49f6e3260a637","externalIds":{"ArXiv":"2310.20700","DBLP":"conf/iclr/Chen0ZZMY0L0024","DOI":"10.48550/arXiv.2310.20700","CorpusId":264820183},"title":"SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction"},{"paperId":"1891c3756f870d902a0b793a1dcd5cc34c778612","externalIds":{"DBLP":"journals/corr/abs-2310-19512","ArXiv":"2310.19512","DOI":"10.48550/arXiv.2310.19512","CorpusId":264803867},"title":"VideoCrafter1: Open Diffusion Models for High-Quality Video Generation"},{"paperId":"f8e63df903d95f08839016db3d59d07af7f1275f","externalIds":{"DBLP":"journals/corr/abs-2310-16003","ArXiv":"2310.16003","DOI":"10.48550/arXiv.2310.16003","CorpusId":264439289},"title":"CVPR 2023 Text Guided Video Editing Competition"},{"paperId":"66d927fdb6c2774131960c75275546fd5ee3dd72","externalIds":{"ArXiv":"2310.11440","DBLP":"journals/corr/abs-2310-11440","DOI":"10.1109/CVPR52733.2024.02090","CorpusId":264172222},"title":"EvalCrafter: Benchmarking and Evaluating Large Video Generation Models"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","externalIds":{"ArXiv":"2310.10647","DBLP":"journals/corr/abs-2310-10647","DOI":"10.1145/3696415","CorpusId":264172934},"title":"A Survey on Video Diffusion Models"},{"paperId":"c7590e69de511e476073ebb27958dcc9327c37e7","externalIds":{"ArXiv":"2310.10769","DBLP":"journals/corr/abs-2310-10769","DOI":"10.48550/arXiv.2310.10769","CorpusId":264172280},"title":"LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation"},{"paperId":"6541f0f74cf6f6d0d8b4a8d9efb64d5e0729bc13","externalIds":{"DBLP":"conf/eccv/ZhaoGWZLWKS24","ArXiv":"2310.08465","DOI":"10.48550/arXiv.2310.08465","CorpusId":263909602},"title":"MotionDirector: Motion Customization of Text-to-Video Diffusion Models"},{"paperId":"6487ec82f6d8082a5b402a5416ea03009acb1679","externalIds":{"DBLP":"journals/cgf/PoYGABBCDHKLLMNOTWW24","ArXiv":"2310.07204","DOI":"10.1111/cgf.15063","CorpusId":263835355},"title":"State of the Art on Diffusion Models for Visual Computing"},{"paperId":"005f3db2174474853cd681d6b8547ee887d36500","externalIds":{"DBLP":"journals/corr/abs-2310-05922","ArXiv":"2310.05922","DOI":"10.48550/arXiv.2310.05922","CorpusId":263829513},"title":"FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing"},{"paperId":"8b7cce220c3b19f9b2d4a6c531907ed3b592b55e","externalIds":{"DBLP":"journals/corr/abs-2310-04378","ArXiv":"2310.04378","CorpusId":263831037},"title":"Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference"},{"paperId":"0439bbee19b4fba596d1e2498b3f45ca265432ba","externalIds":{"ArXiv":"2310.01107","DBLP":"journals/corr/abs-2310-01107","DOI":"10.48550/arXiv.2310.01107","CorpusId":263605399},"title":"Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models"},{"paperId":"53d193a9fb82ce0c2b5d368071b405f63dd98c90","externalIds":{"DBLP":"conf/cvpr/FengWWYBL0G24","ArXiv":"2309.16496","DOI":"10.1109/CVPR52733.2024.00641","CorpusId":263139349},"title":"CCEdit: Creative and Controllable Video Editing via Diffusion Models"},{"paperId":"a5b7fc1bff0910ff31975ec0a15ed30c41f0a968","externalIds":{"DBLP":"journals/ijcv/ZhangWLZRGGS25","ArXiv":"2309.15818","DOI":"10.1007/s11263-024-02271-9","CorpusId":263151295},"title":"Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation"},{"paperId":"ed4603ea341acc26cab24f41aa40524fb7779917","externalIds":{"DBLP":"journals/ijcv/WangCMZHWYHYYGWSJCLDLQL25","ArXiv":"2309.15103","DOI":"10.1007/s11263-024-02295-1","CorpusId":262823915},"title":"LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models"},{"paperId":"ef5d682a3efed36cdd3809d51a1a984f84c4b478","externalIds":{"DBLP":"conf/cvpr/Li0SH24","ArXiv":"2309.07906","DOI":"10.1109/CVPR52733.2024.02279","CorpusId":261823270},"title":"Generative Image Dynamics"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"b08f25092c219c5ec6fe0baa83e4073f30d278c0","externalIds":{"ArXiv":"2309.04509","DBLP":"journals/corr/abs-2309-04509","DOI":"10.1109/ICCV51070.2023.00719","CorpusId":261682368},"title":"The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion"},{"paperId":"8819777e104f8c4197c262e11a01b070b50007aa","externalIds":{"ArXiv":"2308.14749","DBLP":"journals/corr/abs-2308-14749","DOI":"10.48550/arXiv.2308.14749","CorpusId":261276629},"title":"MagicEdit: High-Fidelity and Temporally Coherent Video Editing"},{"paperId":"49faa5c9bf6459a256f68872fb3b51df6b0a2dd8","externalIds":{"DBLP":"journals/corr/abs-2308-13142","ArXiv":"2308.13142","DOI":"10.48550/arXiv.2308.13142","CorpusId":261214460},"title":"A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions"},{"paperId":"c675d23b08f42760133cabff46753bc46a9bceec","externalIds":{"ArXiv":"2308.09710","DBLP":"journals/corr/abs-2308-09710","DOI":"10.1109/CVPR52733.2024.00748","CorpusId":261030352},"title":"SimDA: Simple Diffusion Adapter for Efficient Video Generation"},{"paperId":"05cbac9a5101f47a6fabad72398616506572c9fa","externalIds":{"DBLP":"journals/corr/abs-2308-09592","ArXiv":"2308.09592","DOI":"10.1109/ICCV51070.2023.02106","CorpusId":261031087},"title":"StableVideo: Text-driven Consistency-aware Diffusion Video Editing"},{"paperId":"c2d65fc3a7fde3f7662c6ef9448e5737d7e5551f","externalIds":{"ArXiv":"2308.07926","DBLP":"journals/corr/abs-2308-07926","DOI":"10.1109/CVPR52733.2024.00773","CorpusId":260899973},"title":"CoDeF: Content Deformation Fields for Temporally Consistent Video Processing"},{"paperId":"84f0a99d0f0015a6145c94468870d43ab1d166fd","externalIds":{"DBLP":"journals/corr/abs-2308-06571","ArXiv":"2308.06571","DOI":"10.48550/arXiv.2308.06571","CorpusId":260887737},"title":"ModelScope Text-to-Video Technical Report"},{"paperId":"d4c33129904a62965443c9cc7906d96814671ef4","externalIds":{"DBLP":"journals/corr/abs-2307-14073","ArXiv":"2307.14073","DOI":"10.48550/arXiv.2307.14073","CorpusId":260164708},"title":"VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet"},{"paperId":"4761f173965195798cd3046ef4af608a83504e4d","externalIds":{"DBLP":"conf/iclr/GeyerBBD24","ArXiv":"2307.10373","DOI":"10.48550/arXiv.2307.10373","CorpusId":259991741},"title":"TokenFlow: Consistent Diffusion Features for Consistent Video Editing"},{"paperId":"9b4e61dda9db6317afae1bd4a12356d00769d9f3","externalIds":{"DBLP":"conf/cvpr/ChenHLSZZ24","ArXiv":"2307.09481","DOI":"10.1109/CVPR52733.2024.00630","CorpusId":259951373},"title":"AnyDoor: Zero-shot Object-level Image Customization"},{"paperId":"369b449415d50387fba048bbd4d26ee890df84b5","externalIds":{"ArXiv":"2307.06942","DBLP":"conf/iclr/WangH00YML0C00024","DOI":"10.48550/arXiv.2307.06942","CorpusId":259847783},"title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"},{"paperId":"c1caa303549764d220ff17dc1785985dd1ba6047","externalIds":{"DBLP":"journals/corr/abs-2307-04725","ArXiv":"2307.04725","DOI":"10.48550/arXiv.2307.04725","CorpusId":259501509},"title":"AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning"},{"paperId":"4f7e0cae672da841e7bbcbab4ed40de93f7a0297","externalIds":{"DBLP":"conf/cvpr/LingCZC0Z24","ArXiv":"2307.04684","DOI":"10.1109/CVPR52733.2024.00655","CorpusId":259501791},"title":"FreeDrag: Feature Dragging for Reliable Point-Based Image Editing"},{"paperId":"2cfaa5b3571d3b75f040f6d639359a3c673f5561","externalIds":{"DBLP":"journals/corr/abs-2307-02421","ArXiv":"2307.02421","DOI":"10.48550/arXiv.2307.02421","CorpusId":259342813},"title":"DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models"},{"paperId":"1a61b1f46ef6afb58ee2ecb689c757f8ad2f84d0","externalIds":{"DBLP":"conf/cvpr/WangLL0LYZ0W24","ArXiv":"2307.00040","DOI":"10.1109/CVPR52733.2024.00891","CorpusId":259316865},"title":"Disco: Disentangled Control for Realistic Human Dance Generation"},{"paperId":"03a281a176413ed4d140293edc5bf04a3ad7a1f1","externalIds":{"DBLP":"conf/cvpr/ShiXLPYZTB24","ArXiv":"2306.14435","DOI":"10.1109/CVPR52733.2024.00844","CorpusId":259252555},"title":"DragDiffusion: Harnessing Diffusion Models for Interactive Point-Based Image Editing"},{"paperId":"33e7493ebe199b44620957e91f65f5b2de34df5e","externalIds":{"DBLP":"journals/tmlr/CouaironRHT24","ArXiv":"2306.08707","DOI":"10.48550/arXiv.2306.08707","CorpusId":259164907},"title":"VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing"},{"paperId":"1e09b83fe064826a9a1ac61a7bdc00f26be41aee","externalIds":{"DBLP":"journals/corr/abs-2306-07954","ArXiv":"2306.07954","DOI":"10.1145/3610548.3618160","CorpusId":259144797},"title":"Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation"},{"paperId":"05aea4a4646cb971bb253ced42e8935034a60b57","externalIds":{"ArXiv":"2306.07257","DBLP":"conf/mm/Zhu0H0TCGSF23","DOI":"10.1145/3581783.3612707","CorpusId":259138745},"title":"MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images"},{"paperId":"05d6db8f4727c0cb4fd7bb63cd98de25b2888016","externalIds":{"DBLP":"journals/corr/abs-2306-04542","ArXiv":"2306.04542","DOI":"10.48550/arXiv.2306.04542","CorpusId":259095911},"title":"On the Design Fundamentals of Diffusion Models: A Survey"},{"paperId":"f421b314aaff48e463507034691cfdd3f93cd4c2","externalIds":{"ArXiv":"2306.03881","DBLP":"journals/corr/abs-2306-03881","DOI":"10.48550/arXiv.2306.03881","CorpusId":259089017},"title":"Emergent Correspondence from Image Diffusion"},{"paperId":"f02ea7a18f00859d9ea1b321e3385ae7d0170639","externalIds":{"DBLP":"journals/corr/abs-2306-02018","ArXiv":"2306.02018","DOI":"10.48550/arXiv.2306.02018","CorpusId":259075720},"title":"VideoComposer: Compositional Video Synthesis with Motion Controllability"},{"paperId":"a3e554bb1e62e4e4e14689d5416f27bfe7bc6e0c","externalIds":{"ArXiv":"2306.02000","CorpusId":259075837},"title":"Context-PIPs: Persistent Independent Particles Demands Spatial Context Features"},{"paperId":"52b10ae66d025e99fbb602935e155f97f4f0696f","externalIds":{"DBLP":"journals/corr/abs-2306-00943","ArXiv":"2306.00943","DOI":"10.1109/TVCG.2024.3365804","CorpusId":258999372,"PubMed":"38354074"},"title":"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance"},{"paperId":"c82ec9bffad8f09621bf5de07ebdbcb4a6a6ea48","externalIds":{"ArXiv":"2305.19193","DBLP":"journals/corr/abs-2305-19193","DOI":"10.48550/arXiv.2305.19193","CorpusId":258967411},"title":"Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models"},{"paperId":"477ae5324c206c35d4bde4fe3fad21c74349a723","externalIds":{"DBLP":"conf/nips/GuWZFXLZ00JW23","ArXiv":"2305.18286","DOI":"10.48550/arXiv.2305.18286","CorpusId":258960099},"title":"Photoswap: Personalized Subject Swapping in Images"},{"paperId":"5712960ca1d637ba6e57de43fad3daac04bff4e2","externalIds":{"ArXiv":"2305.17431","DBLP":"journals/corr/abs-2305-17431","DOI":"10.48550/arXiv.2305.17431","CorpusId":258960525},"title":"Towards Consistent Video Editing with Text-to-Image Diffusion Models"},{"paperId":"529191401a8a5f0a8bdb2a1c01301d76af585a3a","externalIds":{"DBLP":"journals/corr/abs-2305-13077","ArXiv":"2305.13077","DOI":"10.48550/arXiv.2305.13077","CorpusId":258832670},"title":"ControlVideo: Training-free Controllable Text-to-Video Generation"},{"paperId":"205d2ed0906440f07a0275d7d6a63bced60951fc","externalIds":{"DBLP":"conf/icmcs/Qin0TCZ24","ArXiv":"2305.12328","DOI":"10.1109/ICME57554.2024.10687529","CorpusId":258832497},"title":"InstructVid2Vid: Controllable Video Editing with Natural Language Instructions"},{"paperId":"12cbf907d40a5406ca855f51af54cc16d0b28cd6","externalIds":{"ArXiv":"2305.11675","DBLP":"journals/corr/abs-2305-11675","DOI":"10.48550/arXiv.2305.11675","CorpusId":258823377},"title":"Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity"},{"paperId":"9f411fda2ad5b141a3115f707bcf5ee865b3fb94","externalIds":{"DBLP":"conf/nips/TangYZ0B23","ArXiv":"2305.11846","DOI":"10.48550/arXiv.2305.11846","CorpusId":258822817},"title":"Any-to-Any Generation via Composable Diffusion"},{"paperId":"05b15934d837dc84afa96824742d3dcc7ec88e09","externalIds":{"DBLP":"journals/corr/abs-2305-10973","ArXiv":"2305.10973","DOI":"10.1145/3588432.3591500","CorpusId":258762550},"title":"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold"},{"paperId":"02bc11de9d2f75bad48166098aa6b30fffee4d70","externalIds":{"ArXiv":"2305.10474","DBLP":"journals/corr/abs-2305-10474","DOI":"10.1109/ICCV51070.2023.02096","CorpusId":258762178},"title":"Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models"},{"paperId":"5f51eda9f7abddca027941d50fb0b6bf6f508eff","externalIds":{"ArXiv":"2305.08850","DBLP":"journals/corr/abs-2305-08850","DOI":"10.48550/arXiv.2305.08850","CorpusId":258686590},"title":"Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts"},{"paperId":"bbdc4118df106d4ba7af9d7d94d7f0a1144c11e2","externalIds":{"ArXiv":"2305.06558","DBLP":"journals/corr/abs-2305-06558","DOI":"10.48550/arXiv.2305.06558","CorpusId":258615204},"title":"Segment and Track Anything"},{"paperId":"15efd2755d422c2bd801fdd2bfdc6dca9adf337c","externalIds":{"DBLP":"journals/corr/abs-2305-04001","ArXiv":"2305.04001","DOI":"10.48550/arXiv.2305.04001","CorpusId":258557566},"title":"AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion"},{"paperId":"cf694df964caa156ec306b45d3a3127533cb458f","externalIds":{"ArXiv":"2305.01569","DBLP":"journals/corr/abs-2305-01569","DOI":"10.48550/arXiv.2305.01569","CorpusId":258437096},"title":"Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation"},{"paperId":"d64755f140ad742495518714ebd457b4d95ce341","externalIds":{"ArXiv":"2304.14404","DBLP":"journals/corr/abs-2304-14404","DOI":"10.48550/arXiv.2304.14404","CorpusId":258352582},"title":"Motion-Conditioned Diffusion Model for Controllable Video Synthesis"},{"paperId":"c0cf0971c153a84bbf0729d289b3b960969bb5dd","externalIds":{"ArXiv":"2304.11968","DBLP":"journals/corr/abs-2304-11968","DOI":"10.48550/arXiv.2304.11968","CorpusId":258298171},"title":"Track Anything: Segment Anything Meets Videos"},{"paperId":"f5a0c57f90c6abe31482e9f320ccac5ee789b135","externalIds":{"ArXiv":"2304.08818","DBLP":"journals/corr/abs-2304-08818","DOI":"10.1109/CVPR52729.2023.02161","CorpusId":258187553},"title":"Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"},{"paperId":"85963807c11abe38e9a2797d9860e012238607ef","externalIds":{"DBLP":"conf/iccv/CaoWQSQZ23","ArXiv":"2304.08465","DOI":"10.1109/ICCV51070.2023.02062","CorpusId":258179432},"title":"MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing"},{"paperId":"8ad199f11f386319ebd2706c372562677c98fae3","externalIds":{"DBLP":"journals/corr/abs-2304-08477","ArXiv":"2304.08477","DOI":"10.48550/arXiv.2304.08477","CorpusId":258180320},"title":"Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation"},{"paperId":"31a1b96392c65a70ab944e276170c81fdffcc73b","externalIds":{"DBLP":"journals/corr/abs-2304-08551","ArXiv":"2304.08551","DOI":"10.48550/arXiv.2304.08551","CorpusId":258187320},"title":"Generative Disco: Text-to-Video Generation for Music Visualization"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"34e95464be6cc3041041f145758493401b8a75e8","externalIds":{"DBLP":"journals/corr/abs-2304-06025","ArXiv":"2304.06025","DOI":"10.1109/ICCV51070.2023.02073","CorpusId":258078892},"title":"DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"},{"paperId":"9733025aea2ba71792be10c18d635e8fc1455e31","externalIds":{"DBLP":"journals/corr/abs-2304-03411","ArXiv":"2304.03411","DOI":"10.1109/CVPR52733.2024.00816","CorpusId":258041269},"title":"InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning"},{"paperId":"ee73edebd42626d9c2d91e35fd2ed3cdb0fb26d0","externalIds":{"DBLP":"conf/aaai/MaHCWC0C24","ArXiv":"2304.01186","DOI":"10.48550/arXiv.2304.01186","CorpusId":257912672},"title":"Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos"},{"paperId":"83b8e18488d8f31dd017ec0b26531cef4b635b36","externalIds":{"DBLP":"conf/nips/ChenHLRJCC23","ArXiv":"2304.00186","DOI":"10.48550/arXiv.2304.00186","CorpusId":257913352},"title":"Subject-driven Text-to-Image Generation via Apprenticeship Learning"},{"paperId":"6518091d7f2af10629b836df8c7a53ce4104c4fb","externalIds":{"DBLP":"conf/cvpr/BolyaH23","ArXiv":"2303.17604","DOI":"10.1109/CVPRW59228.2023.00484","CorpusId":257833518},"title":"Token Merging for Fast Stable Diffusion"},{"paperId":"782838a8699e10b80a0a359f2f7c448aef2ee429","externalIds":{"ArXiv":"2303.17599","DBLP":"journals/corr/abs-2303-17599","DOI":"10.48550/arXiv.2303.17599","CorpusId":257833877},"title":"Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models"},{"paperId":"46a97c83626132db81602becab3379c1cc4edf44","externalIds":{"DBLP":"conf/iclr/GuWYS024","ArXiv":"2303.14897","DOI":"10.48550/arXiv.2303.14897","CorpusId":257766959},"title":"Seer: Language Instructed Video Prediction with Latent Diffusion Models"},{"paperId":"b8b5015b153709176385873e34339f9e520d128f","externalIds":{"DBLP":"journals/corr/abs-2303-13744","ArXiv":"2303.13744","DOI":"10.1109/CVPR52729.2023.01769","CorpusId":257757116},"title":"Conditional Image-to-Video Generation with Latent Flow Diffusion Models"},{"paperId":"923a03032014a12c4e8b26511c0394e1b915fe74","externalIds":{"DBLP":"conf/iccv/KhachatryanMTHW23","ArXiv":"2303.13439","DOI":"10.1109/ICCV51070.2023.01462","CorpusId":257687280},"title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"},{"paperId":"32a3c2fbd3e733bd0eea938517fec2ff8dc7c701","externalIds":{"DBLP":"conf/iccv/CeylanHM23","ArXiv":"2303.12688","DOI":"10.1109/ICCV51070.2023.02121","CorpusId":257663916},"title":"Pix2Video: Video Editing using Image Diffusion"},{"paperId":"14ccb8bcceb6de10eda6ad08bec242a4f2946497","externalIds":{"DBLP":"conf/iccv/QiCZLWSC23","ArXiv":"2303.09535","DOI":"10.1109/ICCV51070.2023.01460","CorpusId":257557738},"title":"FateZero: Fusing Attentions for Zero-shot Text-based Video Editing"},{"paperId":"e441f22769447578177b956e8f064dc426f91752","externalIds":{"DBLP":"journals/corr/abs-2303-07945","ArXiv":"2303.07945","DOI":"10.48550/arXiv.2303.07945","CorpusId":257504818},"title":"Edit-A-Video: Single Video Editing with Object-Aware Consistency"},{"paperId":"35ccd924de9e8483bdcf144cbf2edf09be157b7e","externalIds":{"ArXiv":"2303.07909","DBLP":"journals/corr/abs-2303-07909","DOI":"10.48550/arXiv.2303.07909","CorpusId":257505012},"title":"Text-to-image Diffusion Models in Generative AI: A Survey"},{"paperId":"6283502d6900a0b403e2454b1cb1cf16ddefd5a7","externalIds":{"DBLP":"conf/cvpr/LiuZ00J24","ArXiv":"2303.04761","DOI":"10.1109/CVPR52733.2024.00821","CorpusId":257405406},"title":"Video-P2P: Video Editing with Cross-Attention Control"},{"paperId":"ac974291d7e3a152067382675524f3e3c2ded11b","externalIds":{"DBLP":"conf/icml/SongD0S23","ArXiv":"2303.01469","DOI":"10.1007/978-1-4842-1329-2_9","CorpusId":257280191},"title":"Consistency Models"},{"paperId":"e15900cf7c93d4b6e45a12fe3534840c910467e1","externalIds":{"DBLP":"journals/corr/abs-2302-13848","ArXiv":"2302.13848","DOI":"10.1109/ICCV51070.2023.01461","CorpusId":257219968},"title":"ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation"},{"paperId":"58842cdca3ea68f7b9e638b288fc247a6f26dafc","externalIds":{"DBLP":"conf/aaai/MouWXW0QS24","ArXiv":"2302.08453","DOI":"10.48550/arXiv.2302.08453","CorpusId":256900833},"title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"ecd0b23e4828fca585a05eff56563852d35858d9","externalIds":{"DOI":"10.1007/s00113-023-01296-y","CorpusId":256787765,"PubMed":"36763148"},"title":"ChatGPT"},{"paperId":"07be0ec1f45e21a1032616535d0290ee6bfe0f6b","externalIds":{"DBLP":"conf/iccv/EsserCAGG23","ArXiv":"2302.03011","DOI":"10.1109/ICCV51070.2023.00675","CorpusId":256615582},"title":"Structure and Content-Guided Video Synthesis with Diffusion Models"},{"paperId":"9758ddd6ffbaac75aa0447a9664e6989811a05e2","externalIds":{"DBLP":"journals/corr/abs-2302-01329","ArXiv":"2302.01329","DOI":"10.48550/arXiv.2302.01329","CorpusId":256503757},"title":"Dreamix: Video Diffusion Models are General Video Editors"},{"paperId":"304cbe454a0239401f3d88fde55045f99fe90549","externalIds":{"DBLP":"conf/cvpr/LeeJCQ023","ArXiv":"2301.13173","DOI":"10.1109/CVPR52729.2023.01376","CorpusId":256390381},"title":"Shape-Aware Text-Driven Layered Video Editing"},{"paperId":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","externalIds":{"DBLP":"conf/iccv/WuGWLGSHSQS23","ArXiv":"2212.11565","DOI":"10.1109/ICCV51070.2023.00701","CorpusId":254974187},"title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"},{"paperId":"736973165f98105fec3729b7db414ae4d80fcbeb","externalIds":{"DBLP":"journals/corr/abs-2212-09748","ArXiv":"2212.09748","DOI":"10.1109/ICCV51070.2023.00387","CorpusId":254854389},"title":"Scalable Diffusion Models with Transformers"},{"paperId":"144eca44e250cc462f6fc3a172abb865978f66f5","externalIds":{"DBLP":"conf/cvpr/KumariZ0SZ23","ArXiv":"2212.04488","DOI":"10.1109/CVPR52729.2023.00192","CorpusId":254408780},"title":"Multi-Concept Customization of Text-to-Image Diffusion"},{"paperId":"b000d6865db824af1563708fb7a545ddd65c6b3a","externalIds":{"DBLP":"conf/cvpr/TumanyanGBD23","ArXiv":"2211.12572","DOI":"10.1109/CVPR52729.2023.00191","CorpusId":253801961},"title":"Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation"},{"paperId":"a2d2bbe4c542173662a444b33b76c66992697830","externalIds":{"DBLP":"conf/cvpr/BrooksHE23","ArXiv":"2211.09800","DOI":"10.1109/CVPR52729.2023.01764","CorpusId":253581213},"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions"},{"paperId":"b8ac29f2da80cf3f7f367cf6ebabe3b18114a2e6","externalIds":{"ArXiv":"2211.04894","DBLP":"conf/iccv/Wu0LCHWSYL23","DOI":"10.1109/ICCV51070.2023.01843","CorpusId":257378482},"title":"Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives"},{"paperId":"e24f4b28167b05fbf7d29000490fc0a4e4c109c7","externalIds":{"ArXiv":"2211.01324","DBLP":"journals/corr/abs-2211-01324","DOI":"10.48550/arXiv.2211.01324","CorpusId":253254800},"title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers"},{"paperId":"8ff9667c8c948df1e84606dc086d9a4fba2256f7","externalIds":{"DBLP":"journals/corr/abs-2210-16031","ArXiv":"2210.16031","DOI":"10.48550/arXiv.2210.16031","CorpusId":253223865},"title":"UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance"},{"paperId":"ee9c6f9f9702553f404856287e1388a2916d5383","externalIds":{"DBLP":"journals/corr/abs-2210-15257","ArXiv":"2210.15257","DOI":"10.1109/CVPR52729.2023.00977","CorpusId":253157690},"title":"ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts"},{"paperId":"1dff6b1b35e2d45d4db57c8b4e4395486c3e365f","externalIds":{"ArXiv":"2210.09461","DBLP":"conf/iclr/BolyaFDZFH23","DOI":"10.48550/arXiv.2210.09461","CorpusId":252968113},"title":"Token Merging: Your ViT But Faster"},{"paperId":"23e261a20a315059b4de5492ed071c97a20c12e7","externalIds":{"ArXiv":"2210.09276","DBLP":"journals/corr/abs-2210-09276","DOI":"10.1109/CVPR52729.2023.00582","CorpusId":252918469},"title":"Imagic: Text-Based Real Image Editing with Diffusion Models"},{"paperId":"f170754f8ab3187514292c12b1cbb431c0a8a634","externalIds":{"ArXiv":"2210.09292","DBLP":"journals/corr/abs-2210-09292","DOI":"10.48550/arXiv.2210.09292","CorpusId":252918532},"title":"Efficient Diffusion Models for Vision: A Survey"},{"paperId":"625d57bd52c60cd79aa4add6c4420dc2ad3b808a","externalIds":{"DBLP":"journals/corr/abs-2210-03142","ArXiv":"2210.03142","DOI":"10.1109/CVPR52729.2023.01374","CorpusId":252762155},"title":"On Distillation of Guided Diffusion Models"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"1e33716e8820b867d5a8aaebab44c2d3135ea4ac","externalIds":{"DBLP":"conf/iclr/SingerPH00ZHYAG23","ArXiv":"2209.14792","CorpusId":252595919},"title":"Make-A-Video: Text-to-Video Generation without Text-Video Data"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","externalIds":{"DBLP":"journals/corr/abs-2209-00796","ArXiv":"2209.00796","DOI":"10.1145/3626235","CorpusId":252070859},"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744","externalIds":{"ArXiv":"2208.12242","DBLP":"conf/cvpr/RuizLJPRA23","DOI":"10.1109/CVPR52729.2023.02155","CorpusId":251800180},"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"paperId":"04e541391e8dce14d099d00fb2c21dbbd8afe87f","externalIds":{"DBLP":"journals/corr/abs-2208-01626","ArXiv":"2208.01626","DOI":"10.48550/arXiv.2208.01626","CorpusId":251252882},"title":"Prompt-to-Prompt Image Editing with Cross Attention Control"},{"paperId":"5406129d9d7d00dc310671c43597101b0ee93629","externalIds":{"ArXiv":"2208.01618","DBLP":"journals/corr/abs-2208-01618","DOI":"10.48550/arXiv.2208.01618","CorpusId":251253049},"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"},{"paperId":"af9f365ed86614c800f082bd8eb14be76072ad16","externalIds":{"DBLP":"journals/corr/abs-2207-12598","ArXiv":"2207.12598","DOI":"10.48550/arXiv.2207.12598","CorpusId":249145348},"title":"Classifier-Free Diffusion Guidance"},{"paperId":"01724c36660359545e1368fc80c99f4bde44a190","externalIds":{"DBLP":"conf/eccv/ChengS22","ArXiv":"2207.07115","DOI":"10.48550/arXiv.2207.07115","CorpusId":250526250},"title":"XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model"},{"paperId":"12a8a6c8b9006f2608e3d4914c78ee85cc261395","externalIds":{"DBLP":"journals/ieeemm/VadakitalDLTLR22","DOI":"10.1109/MMUL.2022.3175654","CorpusId":252410854},"title":"The MPEG Immersive Video Standardâ€”Current Status and Future Outlook"},{"paperId":"ef669bb2d0a3e957a91c1dde85ce01c6984ad7d6","externalIds":{"DBLP":"journals/tog/AvrahamiFL23","ArXiv":"2206.02779","DOI":"10.1145/3592450","CorpusId":249394540},"title":"Blended Latent Diffusion"},{"paperId":"2f4c451922e227cbbd4f090b74298445bbd900d0","externalIds":{"DBLP":"journals/corr/abs-2206-00364","ArXiv":"2206.00364","DOI":"10.48550/arXiv.2206.00364","CorpusId":249240415},"title":"Elucidating the Design Space of Diffusion-Based Generative Models"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"805748eec6be59ae0cd92a48400a902b3b7ed8e6","externalIds":{"DBLP":"conf/nips/HarveyNMWW22","ArXiv":"2205.11495","DOI":"10.48550/arXiv.2205.11495","CorpusId":248986725},"title":"Flexible Diffusion Modeling of Long Videos"},{"paperId":"3890d82362d07064687a4b5e9024fc4c92921998","externalIds":{"DBLP":"journals/corr/abs-2205-09853","ArXiv":"2205.09853","DOI":"10.48550/arXiv.2205.09853","CorpusId":248965384},"title":"MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","externalIds":{"DBLP":"journals/corr/abs-2205-01917","ArXiv":"2205.01917","DOI":"10.48550/arXiv.2205.01917","CorpusId":248512473},"title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"e4d66b15fce00531b96af6330238301ebbb76291","externalIds":{"ArXiv":"2204.11823","DBLP":"journals/corr/abs-2204-11823","DOI":"10.48550/arXiv.2204.11823","CorpusId":248377018},"title":"StyleGAN-Human: A Data-Centric Odyssey of Human Generation"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"6cd66bafd46f027c43519c880c0e47e30d72b1c3","externalIds":{"DBLP":"conf/eccv/HarleyFF22","ArXiv":"2204.04153","DOI":"10.1007/978-3-031-20047-2_4","CorpusId":248069518},"title":"Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","externalIds":{"DBLP":"conf/nips/HoSGC0F22","ArXiv":"2204.03458","DOI":"10.48550/arXiv.2204.03458","CorpusId":248006185},"title":"Video Diffusion Models"},{"paperId":"7bb2ebcd7cb3a142aa4779f414c69b1702667588","externalIds":{"DBLP":"journals/corr/abs-2203-04279","ArXiv":"2203.04279","DOI":"10.1109/CVPR52688.2022.00851","CorpusId":247315516},"title":"Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences"},{"paperId":"7e839c2667479d91e21e84583c27257dc7dc1a36","externalIds":{"ArXiv":"2202.05830","DBLP":"journals/corr/abs-2202-05830","CorpusId":246823323},"title":"Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"d97e0adbade91d76b10e8790205a71877a9be42b","externalIds":{"DBLP":"journals/corr/abs-2112-14683","ArXiv":"2112.14683","DOI":"10.1109/CVPR52688.2022.00361","CorpusId":245537141},"title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"7002ae048e4b8c9133a55428441e8066070995cb","externalIds":{"ArXiv":"2112.10741","DBLP":"journals/corr/abs-2112-10741","CorpusId":245335086},"title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","externalIds":{"DBLP":"journals/corr/abs-2112-03857","ArXiv":"2112.03857","DOI":"10.1109/CVPR52688.2022.01069","CorpusId":244920947},"title":"Grounded Language-Image Pre-training"},{"paperId":"658a017302d29e4acf4ca789cb5d9f27983717ff","externalIds":{"DBLP":"conf/cvpr/ChengMSKG22","ArXiv":"2112.01527","DOI":"10.1109/CVPR52688.2022.00135","CorpusId":244799297},"title":"Masked-attention Mask Transformer for Universal Image Segmentation"},{"paperId":"414e554d281d529401c873cb9c97186365ec5dd8","externalIds":{"ArXiv":"2111.14822","DBLP":"journals/corr/abs-2111-14822","DOI":"10.1109/CVPR52688.2022.01043","CorpusId":244714856},"title":"Vector Quantized Diffusion Model for Text-to-Image Synthesis"},{"paperId":"88e8801e4daf404d3d40f1648ef29faeb8e6d58a","externalIds":{"ArXiv":"2111.14818","DBLP":"conf/cvpr/AvrahamiLF22","DOI":"10.1109/CVPR52688.2022.01767","CorpusId":244714366},"title":"Blended Diffusion for Text-driven Editing of Natural Images"},{"paperId":"e4733ff919aefc7048774876b05e73bae56fcb49","externalIds":{"ArXiv":"2111.13680","DBLP":"journals/corr/abs-2111-13680","DOI":"10.1109/CVPR52688.2022.00795","CorpusId":244709323},"title":"GMFlow: Learning Optical Flow via Global Matching"},{"paperId":"e1a3e6856b6ac6af3600b5954392e5368603fd1b","externalIds":{"ArXiv":"2111.10337","DBLP":"conf/cvpr/XueHZS00FG22","DOI":"10.1109/CVPR52688.2022.00498","CorpusId":244462849},"title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions"},{"paperId":"37c9c4e7648f639c0b36f150fc6c6c90b3682f4a","externalIds":{"DBLP":"journals/corr/abs-2111-05826","ArXiv":"2111.05826","DOI":"10.1145/3528233.3530757","CorpusId":243938678},"title":"Palette: Image-to-Image Diffusion Models"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"8f8dedb511c0324d1cb7f9750560109ca9290b5f","externalIds":{"ArXiv":"2110.02711","DBLP":"conf/cvpr/KimKY22a","DOI":"10.1109/CVPR52688.2022.00246","CorpusId":244909410},"title":"DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation"},{"paperId":"7806ad7885d732040cb1fbf23857bba5b6779edd","externalIds":{"DBLP":"journals/tog/KastenOWD21","ArXiv":"2109.11418","DOI":"10.1145/3478513.3480546","CorpusId":237605410},"title":"Layered neural atlases for consistent video editing"},{"paperId":"6560df566443a3634b7674942aebe0f0e0af3b00","externalIds":{"DBLP":"conf/iccv/0002LYH00P021","ArXiv":"2108.07009","DOI":"10.1109/ICCV48922.2021.00507","CorpusId":237091320},"title":"Pixel Difference Networks for Efficient Edge Detection"},{"paperId":"f671a09e3e5922e6d38cb77dda8d76d5ceac2a27","externalIds":{"DBLP":"conf/iclr/MengHSSWZE22","ArXiv":"2108.01073","CorpusId":245704504},"title":"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"95f5bafba97beb9b4f8c1fe607f04ec28efab7f9","externalIds":{"DBLP":"journals/corr/abs-2106-03802","ArXiv":"2106.03802","CorpusId":235363972},"title":"Learning to Efficiently Sample from Diffusion Probabilistic Models"},{"paperId":"d0829986bd7d8904ee855117c974140de3be742c","externalIds":{"DBLP":"journals/corr/abs-2103-03319","DOI":"10.1109/CVPR46437.2021.01256","CorpusId":232135122},"title":"Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos"},{"paperId":"0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4","externalIds":{"DBLP":"journals/jmlr/HoSCFNS22","ArXiv":"2106.15282","CorpusId":235619773},"title":"Cascaded Diffusion Models for High Fidelity Image Generation"},{"paperId":"64ea8f180d0682e6c18d1eb688afdb2027c02794","externalIds":{"ArXiv":"2105.05233","DBLP":"journals/corr/abs-2105-05233","CorpusId":234357997},"title":"Diffusion Models Beat GANs on Image Synthesis"},{"paperId":"c64025f83864ec9c40e2970a24314b6b84d4c753","externalIds":{"DBLP":"journals/corr/abs-2104-14806","ArXiv":"2104.14806","CorpusId":233476314},"title":"GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"de18baa4964804cf471d85a5a090498242d2e79f","externalIds":{"ArXiv":"2102.09672","DBLP":"conf/icml/NicholD21","CorpusId":231979499},"title":"Improved Denoising Diffusion Probabilistic Models"},{"paperId":"47f7ec3d0a5e6e83b6768ece35206a94dc81919c","externalIds":{"ArXiv":"2012.09841","MAG":"3111551570","DBLP":"journals/corr/abs-2012-09841","DOI":"10.1109/CVPR46437.2021.01268","CorpusId":229297973},"title":"Taming Transformers for High-Resolution Image Synthesis"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"014576b866078524286802b1d0e18628520aa886","externalIds":{"ArXiv":"2010.02502","DBLP":"journals/corr/abs-2010-02502","MAG":"3092442149","CorpusId":222140788},"title":"Denoising Diffusion Implicit Models"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"3230e2d6b4671cc03974af2219c6d3270e6fac70","externalIds":{"MAG":"3013965544","DBLP":"conf/ijcai/Teed021","ArXiv":"2003.12039","DOI":"10.1007/978-3-030-58536-5_24","CorpusId":214667893},"title":"RAFT: Recurrent All-Pairs Field Transforms for Optical Flow"},{"paperId":"14fdc18d9c164e5b0d6d946b3238c04e81921358","externalIds":{"MAG":"3035574324","DBLP":"conf/cvpr/KarrasLAHLA20","ArXiv":"1912.04958","DOI":"10.1109/cvpr42600.2020.00813","CorpusId":209202273},"title":"Analyzing and Improving the Image Quality of StyleGAN"},{"paperId":"965359b3008ab50dd04e171551220ec0e7f83aba","externalIds":{"MAG":"2971034910","ArXiv":"1907.05600","DBLP":"conf/nips/SongE19","CorpusId":196470871},"title":"Generative Modeling by Estimating Gradients of the Data Distribution"},{"paperId":"7bd83b055702bc178aa26def5b6df463f8eab7b9","externalIds":{"MAG":"2955639361","DBLP":"journals/corr/abs-1907-01341","ArXiv":"1907.01341","DOI":"10.1109/TPAMI.2020.3019967","CorpusId":195776274,"PubMed":"32853149"},"title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer"},{"paperId":"0167e98f6d2e4c44b505c0f74f91425f62dfc62c","externalIds":{"DBLP":"journals/pami/WangLFSLY22","MAG":"2938260698","ArXiv":"1904.09146","DOI":"10.1109/TPAMI.2021.3051099","CorpusId":126180710,"PubMed":"33434124"},"title":"Salient Object Detection in the Deep Learning Era: An In-Depth Survey"},{"paperId":"2e49d98756868c52dcb37e32a10533c361fbab89","externalIds":{"ArXiv":"1901.07973","MAG":"2912064184","DBLP":"conf/cvpr/GeZWTL19","DOI":"10.1109/CVPR.2019.00548","CorpusId":59158744},"title":"DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images"},{"paperId":"0c5f6d07b2a355312ba50132bab30832d1a4d883","externalIds":{"MAG":"3031246127","ArXiv":"1811.09245","DBLP":"journals/ijcv/SaitoSKK20","DOI":"10.1007/s11263-020-01333-y","CorpusId":218978582},"title":"Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN"},{"paperId":"4bbfd46721c145852e443ae4aad35148b814bf91","externalIds":{"ArXiv":"1811.08383","DBLP":"conf/iccv/LinGH19","MAG":"2990152177","DOI":"10.1109/ICCV.2019.00718","CorpusId":85542740},"title":"TSM: Temporal Shift Module for Efficient Video Understanding"},{"paperId":"098b68fe34dba7fc4e206035ae2d149944bbca8f","externalIds":{"MAG":"2886787375","DBLP":"journals/corr/abs-1808-00449","ArXiv":"1808.00449","DOI":"10.1007/978-3-030-01267-0_11","CorpusId":51892815},"title":"Learning Blind Video Temporal Consistency"},{"paperId":"d08b35243edc5be07387a9ed218070b31e502901","externalIds":{"DBLP":"journals/corr/abs-1803-08494","ArXiv":"1803.08494","MAG":"2795783309","DOI":"10.1007/s11263-019-01198-w","CorpusId":4076251},"title":"Group Normalization"},{"paperId":"84de7d27e2f6160f634a483e8548c499a2cda7fa","externalIds":{"MAG":"2785678896","DBLP":"journals/corr/abs-1802-05957","ArXiv":"1802.05957","CorpusId":3366315},"title":"Spectral Normalization for Generative Adversarial Networks"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"7cfa5c97164129ce3630511f639040d28db1d4b7","externalIds":{"DBLP":"journals/corr/abs-1709-07871","MAG":"2951555602","ArXiv":"1709.07871","DOI":"10.1609/aaai.v32i1.11671","CorpusId":19119291},"title":"FiLM: Visual Reasoning with a General Conditioning Layer"},{"paperId":"e76edb86f270c3a77ed9f5a1e1b305461f36f96f","externalIds":{"MAG":"2951910147","DBLP":"conf/cvpr/Tulyakov0YK18","ArXiv":"1707.04993","DOI":"10.1109/CVPR.2018.00165","CorpusId":4475365},"title":"MoCoGAN: Decomposing Motion and Content for Video Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"be0ef77fb0345c5851bb5d297f3ed84ae3c581ee","externalIds":{"MAG":"2940630528","ArXiv":"1703.06868","DBLP":"conf/iccv/HuangB17","DOI":"10.1109/ICCV.2017.167","CorpusId":6576859},"title":"Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization"},{"paperId":"062c41dad67bb68fefd9ff0c5c4d296e796004dc","externalIds":{"MAG":"2952493843","DBLP":"conf/iccv/SaitoMS17","ArXiv":"1611.06624","DOI":"10.1109/ICCV.2017.308","CorpusId":6945308},"title":"Temporal Generative Adversarial Nets with Singular Value Clipping"},{"paperId":"ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1","externalIds":{"MAG":"2520707650","ArXiv":"1609.02612","DBLP":"conf/nips/VondrickPT16","DOI":"10.13016/M26GIH-TNYZ","CorpusId":9933254},"title":"Generating Videos with Scene Dynamics"},{"paperId":"63de0ad39d807f0c256f851428f211e8d5fcd3bb","externalIds":{"DBLP":"journals/corr/UlyanovVL16","ArXiv":"1607.08022","MAG":"2502312327","CorpusId":16516553},"title":"Instance Normalization: The Missing Ingredient for Fast Stylization"},{"paperId":"de5e7320729f5d3cbb6709eb6329ec41ace8c95d","externalIds":{"ArXiv":"1606.08415","MAG":"2899663614","CorpusId":125617073},"title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"e944b414e9f601a6008076bd43b91d382090adbc","externalIds":{"DBLP":"conf/cvpr/GaidonWCV16","MAG":"2949907962","ArXiv":"1605.06457","DOI":"10.1109/CVPR.2016.470","CorpusId":1203247},"title":"VirtualWorlds as Proxy for Multi-object Tracking Analysis"},{"paperId":"1ced31e02234bc3d1092ffb2c7442ffbd51cb309","externalIds":{"DBLP":"journals/corr/MayerIHFCDB15","MAG":"2259424905","ArXiv":"1512.02134","DOI":"10.1109/CVPR.2016.438","CorpusId":206594275},"title":"A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation"},{"paperId":"f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6","externalIds":{"DBLP":"journals/corr/GatysEB15a","ArXiv":"1508.06576","MAG":"1924619199","DOI":"10.1167/16.12.326","CorpusId":13914930},"title":"A Neural Algorithm of Artistic Style"},{"paperId":"0f899b92b7fb03b609fee887e4b6f3b633eaf30d","externalIds":{"MAG":"299440670","ArXiv":"1505.05770","DBLP":"journals/corr/RezendeM15","CorpusId":12554042},"title":"Variational Inference with Normalizing Flows"},{"paperId":"c2fb5b39428818d7ec8cc78e152e19c21b7db568","externalIds":{"DBLP":"conf/iccv/DosovitskiyFIHH15","MAG":"2951309005","ArXiv":"1504.06852","DOI":"10.1109/ICCV.2015.316","CorpusId":12552176},"title":"FlowNet: Learning Optical Flow with Convolutional Networks"},{"paperId":"8da55e685a7bef9c897788ab519a8710c695c419","externalIds":{"DBLP":"journals/corr/XieT15","ArXiv":"1504.06375","MAG":"845365781","DOI":"10.1007/s11263-017-1004-z","CorpusId":6423078},"title":"Holistically-Nested Edge Detection"},{"paperId":"2dcef55a07f8607a819c21fe84131ea269cc2e3c","externalIds":{"MAG":"2129069237","DBLP":"journals/corr/Sohl-DicksteinW15","ArXiv":"1503.03585","CorpusId":14888175},"title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"5f5dc5b9a2ba710937e2c413b37b053cd673df02","externalIds":{"DBLP":"journals/corr/KingmaW13","MAG":"2951004968","ArXiv":"1312.6114","CorpusId":216078090},"title":"Auto-Encoding Variational Bayes"},{"paperId":"7d53f0c87c8ab0de6f3e74515e3ffaf3fab40c62","externalIds":{"MAG":"1513100184","DBLP":"conf/eccv/ButlerWSB12","DOI":"10.1007/978-3-642-33783-3_44","CorpusId":4637111},"title":"A Naturalistic Open Source Movie for Optical Flow Evaluation"},{"paperId":"f43fe15d04f220b6d537d35a8412903acb4700f6","externalIds":{"MAG":"2144476963","DBLP":"journals/ijcv/SandT08","DOI":"10.1007/s11263-008-0136-6","CorpusId":7697706},"title":"Particle Video: Long-Range Motion Estimation Using Point Trajectories"},{"paperId":"9966e890f2eedb4577e11b9d5a66380a4d9341fe","externalIds":{"DBLP":"journals/jmlr/Hyvarinen05","MAG":"1505878979","CorpusId":1152227},"title":"Estimation of Non-Normalized Statistical Models by Score Matching"},{"paperId":"a3229dc33ecb80c59a75b906c46b586dd059b781","externalIds":{"MAG":"2620619910","DBLP":"journals/ai/HornS81","DOI":"10.1117/12.965761","CorpusId":1371968},"title":"Determining Optical Flow"},{"paperId":"7af366c2d5b1a46a872adda781235845bd671f03","externalIds":{"DBLP":"journals/corr/abs-2403-14468","DOI":"10.48550/arXiv.2403.14468","CorpusId":279586284},"title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks"},{"paperId":"48a7c60b023ebd2cf0587df1cc09f1309fe51d28","externalIds":{"DBLP":"journals/corr/abs-2305-13840","DOI":"10.48550/arXiv.2305.13840","CorpusId":258841645},"title":"Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models"},{"paperId":"90428f3a8caa5082f825ebf3138514ddf273dae3","externalIds":{"CorpusId":253581838},"title":"Supplementary Materials for: NULL-text Inversion for Editing Real Images using Guided Diffusion Models"},{"paperId":"2d66cfa143ec6a600020762ec0c22dcc5f5a4be2","externalIds":{"DBLP":"journals/corr/abs-2312-04557","DOI":"10.48550/arXiv.2312.04557","CorpusId":271403848},"title":"GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation"},{"paperId":"71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f","externalIds":{"DBLP":"journals/corr/abs-2209-02646","DOI":"10.48550/arXiv.2209.02646","CorpusId":252090040},"title":"A Survey on Generative Diffusion Model"}]}