{"references":[{"paperId":"8c5acaafe43e710d55b08c63d567550ad26ec437","externalIds":{"DBLP":"journals/corr/abs-2403-04696","ArXiv":"2403.04696","DOI":"10.48550/arXiv.2403.04696","CorpusId":268264232},"title":"Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification"},{"paperId":"3414be052766667fe375221804e48f4a9c815ba5","externalIds":{"ArXiv":"2403.03627","DBLP":"journals/corr/abs-2403-03627","DOI":"10.48550/arXiv.2403.03627","CorpusId":268253550},"title":"Multimodal Large Language Models to Support Real-World Fact-Checking"},{"paperId":"5a7ae6c94bc59d7ee3fadb0f27788e2ac828ef88","externalIds":{"ArXiv":"2402.02420","DBLP":"conf/emnlp/WangWMLGDN24","ACL":"2024.emnlp-main.1088","DOI":"10.18653/v1/2024.emnlp-main.1088","CorpusId":267412450},"title":"Factuality of Large Language Models: A Survey"},{"paperId":"028d75496e51943f52c7b2177344a3c089c18058","externalIds":{"DBLP":"journals/corr/abs-2401-06855","ArXiv":"2401.06855","DOI":"10.48550/arXiv.2401.06855","CorpusId":266999558},"title":"Fine-grained Hallucination Detection and Editing for Language Models"},{"paperId":"6920de816acd201aadc0de51cf0fa62fa92bb0cc","externalIds":{"ArXiv":"2311.13240","DBLP":"journals/corr/abs-2311-13240","DOI":"10.48550/arXiv.2311.13240","CorpusId":265351565},"title":"On the Calibration of Large Language Models and Alignment"},{"paperId":"67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3","externalIds":{"ArXiv":"2311.08718","DBLP":"conf/icml/HouLQAC024","DOI":"10.48550/arXiv.2311.08718","CorpusId":265213190},"title":"Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling"},{"paperId":"df9981b9bbfc619652e84cd0eefa595274da2fef","externalIds":{"ArXiv":"2311.08877","DBLP":"journals/corr/abs-2311-08877","DOI":"10.48550/arXiv.2311.08877","CorpusId":265213392},"title":"Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation"},{"paperId":"3a89e289e2dd29f5e52a2bf354a637762b661257","externalIds":{"DBLP":"journals/corr/abs-2311-08401","ArXiv":"2311.08401","DOI":"10.48550/arXiv.2311.08401","CorpusId":265158181},"title":"Fine-tuning Language Models for Factuality"},{"paperId":"444f3b7293b85b7d37600372941a289f9163abd1","externalIds":{"ArXiv":"2311.07383","DBLP":"journals/corr/abs-2311-07383","DOI":"10.48550/arXiv.2311.07383","CorpusId":265149591},"title":"LM-Polygraph: Uncertainty Estimation for Language Models"},{"paperId":"6af460d34bfc8e955e43fbe15cedcf329b48bc19","externalIds":{"DBLP":"conf/emnlp/ZhangLDMS23","ArXiv":"2311.01740","DOI":"10.48550/arXiv.2311.01740","CorpusId":265019095},"title":"SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency"},{"paperId":"c1284ee1ddf29955a1a02bdc45abdaac63745017","externalIds":{"ArXiv":"2310.17918","DBLP":"journals/corr/abs-2310-17918","ACL":"2024.naacl-long.390","DOI":"10.48550/arXiv.2310.17918","CorpusId":264555682},"title":"Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method"},{"paperId":"03fab98a9be74a253688840dba9144737a8ca92d","externalIds":{"DBLP":"journals/corr/abs-2310-07521","ArXiv":"2310.07521","DOI":"10.48550/arXiv.2310.07521","CorpusId":263835211},"title":"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity"},{"paperId":"570e4fec8c8f1c96b76accbb07d40e0528aafb4a","externalIds":{"DBLP":"journals/corr/abs-2309-03882","ArXiv":"2309.03882","DOI":"10.48550/arXiv.2309.03882","CorpusId":261582594},"title":"Large Language Models Are Not Robust Multiple Choice Selectors"},{"paperId":"c01c7c1f903dfaa78812fb20a6cb2db25e4712e3","externalIds":{"ArXiv":"2308.16175","DBLP":"conf/acl/Chen024","DOI":"10.18653/v1/2024.acl-long.283","CorpusId":263611057},"title":"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness"},{"paperId":"288063323dddad9bea7eb1230a2048546435687e","externalIds":{"ArXiv":"2308.13387","DBLP":"journals/corr/abs-2308-13387","DOI":"10.48550/arXiv.2308.13387","CorpusId":261214837},"title":"Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs"},{"paperId":"7142e920b6b9355d9cbacc9450818f912eca138e","externalIds":{"ArXiv":"2308.05374","DBLP":"journals/corr/abs-2308-05374","DOI":"10.48550/arXiv.2308.05374","CorpusId":260775522},"title":"Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment"},{"paperId":"f27a23716798526746d87cc34aa56484734eb140","externalIds":{"DBLP":"journals/corr/abs-2307-15703","ArXiv":"2307.15703","DOI":"10.48550/arXiv.2307.15703","CorpusId":260316110},"title":"Uncertainty in Natural Language Generation: From Theory to Applications"},{"paperId":"1827dd28ef866eaeb929ddf4bcfa492880aba4c7","externalIds":{"DBLP":"journals/corr/abs-2307-03987","ArXiv":"2307.03987","DOI":"10.48550/arXiv.2307.03987","CorpusId":263699899},"title":"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation"},{"paperId":"c7091540c1fa77f1c6b27482f349330f8e559d6f","externalIds":{"DBLP":"journals/corr/abs-2307-00175","ArXiv":"2307.00175","DOI":"10.1007/s11098-023-02094-3","CorpusId":259316868},"title":"Still no lie detector for language models: probing empirical and conceptual roadblocks"},{"paperId":"8f7297454d7f44365b9bcda5ebb9439a43daf5e6","externalIds":{"DBLP":"journals/corr/abs-2306-13063","ArXiv":"2306.13063","DOI":"10.48550/arXiv.2306.13063","CorpusId":259224389},"title":"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs"},{"paperId":"db1aa71314016e12e115fbe449a688f523e52e77","externalIds":{"ArXiv":"2306.05317","ACL":"2023.bionlp-1.51","DBLP":"journals/corr/abs-2306-05317","DOI":"10.48550/arXiv.2306.05317","CorpusId":259108630},"title":"CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models"},{"paperId":"405f8f5f1c6df1b3343c812832479aad5180b65f","externalIds":{"ArXiv":"2306.03341","DBLP":"journals/corr/abs-2306-03341","CorpusId":259088877},"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"},{"paperId":"7a1e71cb1310c4a873e7a4e54d1a6dab0553adce","externalIds":{"ArXiv":"2306.01116","DBLP":"journals/corr/abs-2306-01116","DOI":"10.48550/arXiv.2306.01116","CorpusId":259063761},"title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"},{"paperId":"eb971944bccf9793ac463c3e2f4d4251d4e8e071","externalIds":{"DBLP":"journals/corr/abs-2305-18153","ArXiv":"2305.18153","DOI":"10.48550/arXiv.2305.18153","CorpusId":258959258},"title":"Do Large Language Models Know What They Don't Know?"},{"paperId":"28a7ced384549eaae74ea9ad3ee21189a0625afe","externalIds":{"ArXiv":"2305.19148","DBLP":"journals/corr/abs-2305-19148","ACL":"2023.acl-long.783","DOI":"10.48550/arXiv.2305.19148","CorpusId":258967265},"title":"Mitigating Label Biases for In-context Learning"},{"paperId":"ab4ce5dda7ad4d9032995c9c049a89d65723c6aa","externalIds":{"ArXiv":"2305.14975","DBLP":"conf/emnlp/TianMZSRYFM23","DOI":"10.48550/arXiv.2305.14975","CorpusId":258865733},"title":"Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback"},{"paperId":"76750c59ec126cc4bfdfef30648598bd5b94220b","externalIds":{"ArXiv":"2305.13788","DBLP":"conf/emnlp/LeeAT23","DOI":"10.18653/v1/2023.emnlp-main.278","CorpusId":258841424},"title":"Can Large Language Models Capture Dissenting Human Voices?"},{"paperId":"7bd4ca8706a79983d31ab74e6c79bfdfd949602e","externalIds":{"ArXiv":"2305.14160","DBLP":"journals/corr/abs-2305-14160","DOI":"10.48550/arXiv.2305.14160","CorpusId":258841117},"title":"Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning"},{"paperId":"1e66b2d2a56842f07e5d6bf4ac0dbe5da829d052","externalIds":{"ArXiv":"2306.05540","DBLP":"journals/corr/abs-2306-05540","DOI":"10.48550/arXiv.2306.05540","CorpusId":259129463},"title":"DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text"},{"paperId":"b6d6c33298b852cf63edac233deca70530d69a2a","externalIds":{"ArXiv":"2305.10403","DBLP":"journals/corr/abs-2305-10403","CorpusId":258740735},"title":"PaLM 2 Technical Report"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"f406aceba4f29cc7cfbe7edb2f52f01374486589","externalIds":{"ArXiv":"2304.13734","DBLP":"journals/corr/abs-2304-13734","DOI":"10.18653/v1/2023.findings-emnlp.68","CorpusId":258352729},"title":"The Internal State of an LLM Knows When its Lying"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"296cad46f56329843ded4a8b7d7633c9d436f113","externalIds":{"ArXiv":"2304.08653","DBLP":"conf/emnlp/ZablotskaiaPMN023","DOI":"10.48550/arXiv.2304.08653","CorpusId":258187125},"title":"On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study"},{"paperId":"d7f3efd1fc1b58db4b5d90c75b5e215421fda4a9","externalIds":{"DBLP":"conf/wsdm/WangXKZYZ23","DOI":"10.1145/3539597.3570382","CorpusId":257079740},"title":"Reducing Negative Effects of the Biases of Language Models in Zero-Shot Setting"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"217e436fd23fe4184828e02a2b143835d6fd3b28","externalIds":{"DBLP":"conf/emnlp/ZhouJH23","ArXiv":"2302.13439","DOI":"10.18653/v1/2023.emnlp-main.335","CorpusId":265150666},"title":"Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models"},{"paperId":"ba121a6e2583c5f9b137f04324c25239c63d3473","externalIds":{"DBLP":"journals/corr/abs-2302-06690","ArXiv":"2302.06690","ACL":"2023.findings-eacl.40","DOI":"10.48550/arXiv.2302.06690","CorpusId":256846523},"title":"Bag of Tricks for In-Distribution Calibration of Pretrained Transformers"},{"paperId":"89c3bd70ad33c4f8832f00ab98872b77861ee0ec","externalIds":{"DBLP":"journals/corr/abs-2212-03827","ArXiv":"2212.03827","DOI":"10.48550/arXiv.2212.03827","CorpusId":254366253},"title":"Discovering Latent Knowledge in Language Models Without Supervision"},{"paperId":"6d951d939d3f27054215f2606a0cf89ed21550e9","externalIds":{"ArXiv":"2212.02216","DBLP":"journals/corr/abs-2212-02216","DOI":"10.48550/arXiv.2212.02216","CorpusId":254246441},"title":"Improving Few-Shot Performance of Language Models via Nearest Neighbor Calibration"},{"paperId":"851e71163d86368deb6b6d6ed5e5d2187c782ff4","externalIds":{"DBLP":"journals/corr/abs-2210-16133","ArXiv":"2210.16133","ACL":"2022.emnlp-main.124","DOI":"10.48550/arXiv.2210.16133","CorpusId":253224378},"title":"Stop Measuring Calibration When Humans Disagree"},{"paperId":"d785f543f7a344fcec7ccbe55089a7f782e32bd7","externalIds":{"ACL":"2022.emnlp-main.399","DBLP":"journals/corr/abs-2210-13210","ArXiv":"2210.13210","DOI":"10.48550/arXiv.2210.13210","CorpusId":253097869},"title":"Mutual Information Alleviates Hallucinations in Abstractive Summarization"},{"paperId":"551b05734eb2181c4ca009a411144e8447ed1606","externalIds":{"DBLP":"journals/corr/abs-2210-04714","ArXiv":"2210.04714","DOI":"10.48550/arXiv.2210.04714","CorpusId":247613322},"title":"Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis"},{"paperId":"94b6f6822f364cf7b1a3a9984667c009e2ec6a65","externalIds":{"DBLP":"conf/iclr/0006LZKSLL23","ArXiv":"2209.15558","DOI":"10.48550/arXiv.2209.15558","CorpusId":252668614},"title":"Out-of-Distribution Detection and Selective Generation for Conditional Language Models"},{"paperId":"891edceb78a274b0c2494d8176bc4d6f6e3f9cbc","externalIds":{"DBLP":"journals/corr/abs-2210-00045","ArXiv":"2210.00045","DOI":"10.48550/arXiv.2210.00045","CorpusId":252683988},"title":"Calibrating Sequence likelihood Improves Conditional Language Generation"},{"paperId":"196ee3bf7724cd6b01972a7eeb4b4416a781b0ee","externalIds":{"DBLP":"journals/corr/abs-2209-06995","ArXiv":"2209.06995","DOI":"10.48550/arXiv.2209.06995","CorpusId":261031715},"title":"Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach"},{"paperId":"86d0d3855f94105e25d81cab9f3d269c6062a9c4","externalIds":{"DBLP":"conf/iclr/SuKWSWX0OZS023","ArXiv":"2209.01975","DOI":"10.48550/arXiv.2209.01975","CorpusId":252089424},"title":"Selective Annotation Makes Language Models Better Few-Shot Learners"},{"paperId":"85e27aec760b3d16ab779e335fc195bc578d0840","externalIds":{"ArXiv":"2207.05161","DBLP":"conf/nips/SunBCSS23","CorpusId":250450892},"title":"What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization"},{"paperId":"142ebbf4760145f591166bde2564ac70c001e927","externalIds":{"ArXiv":"2207.05221","DBLP":"journals/corr/abs-2207-05221","DOI":"10.48550/arXiv.2207.05221","CorpusId":250451161},"title":"Language Models (Mostly) Know What They Know"},{"paperId":"374dd173491a59a10bbb2b3519ebcfe3649f529d","externalIds":{"DBLP":"journals/corr/abs-2205-14334","ArXiv":"2205.14334","DOI":"10.48550/arXiv.2205.14334","CorpusId":249191391},"title":"Teaching Models to Express Their Uncertainty in Words"},{"paperId":"615671736f303f05a9f2a026a9e677788eef96b3","externalIds":{"DBLP":"conf/emnlp/Si0MB22","ArXiv":"2205.12507","DOI":"10.18653/v1/2022.findings-emnlp.204","CorpusId":253098276},"title":"Re-Examining Calibration: The Case of Question Answering"},{"paperId":"6a483cd1cbecd66150c9bbcd01606723950281bc","externalIds":{"ArXiv":"2205.10183","DBLP":"conf/iclr/HanH0SW23","DOI":"10.48550/arXiv.2205.10183","CorpusId":248964978},"title":"Prototypical Calibration for Few-shot Learning of Language Models"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"05f6628948f79d0cce8664cc8146fd459d53e9d5","externalIds":{"DBLP":"conf/acl/ParkC22","ArXiv":"2203.07559","ACL":"2022.acl-long.368","DOI":"10.48550/arXiv.2203.07559","CorpusId":247450599},"title":"On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency"},{"paperId":"56d5d47d95f6cec904235901728c0b7289bbeb00","externalIds":{"ArXiv":"2203.01677","DBLP":"journals/corr/abs-2203-01677","DOI":"10.48550/arXiv.2203.01677","CorpusId":247223086},"title":"Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation"},{"paperId":"f4df78183261538e718066331898ee5cad7cad05","externalIds":{"DBLP":"journals/corr/abs-2202-12837","ArXiv":"2202.12837","ACL":"2022.emnlp-main.759","DOI":"10.18653/v1/2022.emnlp-main.759","CorpusId":247155069},"title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"972706306f85b1bfb40c7d35c796ad5174eb0c9c","externalIds":{"DBLP":"journals/corr/abs-2111-09543","ArXiv":"2111.09543","CorpusId":244346093},"title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"},{"paperId":"cf3cfb90a6d8c431dc8a7f115b011d5ffbb439ee","externalIds":{"DBLP":"journals/corr/abs-2111-07997","ACL":"2022.naacl-main.431","ArXiv":"2111.07997","DOI":"10.18653/v1/2022.naacl-main.431","CorpusId":244117167},"title":"Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection"},{"paperId":"fdae6cb9beff9749009798b95192a4549f8761a2","externalIds":{"DBLP":"journals/corr/abs-2106-04972","ArXiv":"2106.04972","CorpusId":235376772},"title":"Understanding Softmax Confidence and Uncertainty"},{"paperId":"fb51dc284e42927d018858fcc6618d16cbdfc042","externalIds":{"ArXiv":"2106.03020","DBLP":"journals/corr/abs-2106-03020","MAG":"3166996085","ACL":"2021.acl-short.109","DOI":"10.18653/v1/2021.acl-short.109","CorpusId":235358498},"title":"Embracing Ambiguity: Shifting the Training Target of NLI Models"},{"paperId":"2c33f4aa30a4d8f708db79eef5c9e2a0ce84695b","externalIds":{"DBLP":"journals/corr/abs-2106-01494","ArXiv":"2106.01494","ACL":"2021.findings-acl.172","DOI":"10.18653/v1/2021.findings-acl.172","CorpusId":235313893},"title":"Knowing More About Questions Can Help: Improving Calibration in Question Answering"},{"paperId":"0921322cf6ea34d1852f13cb67eeac9d1f863518","externalIds":{"DBLP":"conf/iclr/MalininG21","MAG":"3123791752","DOI":"10.17863/CAM.63497","CorpusId":231895728},"title":"Uncertainty Estimation in Autoregressive Structured Prediction"},{"paperId":"9d81bc8bebf1beb936427c224afb219b54a64f1e","externalIds":{"DBLP":"conf/emnlp/HoltzmanWSCZ21","ACL":"2021.emnlp-main.564","ArXiv":"2104.08315","DOI":"10.18653/v1/2021.emnlp-main.564","CorpusId":233296182},"title":"Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right"},{"paperId":"2476832edb11cb3de46587f55e270d6df328b32d","externalIds":{"ACL":"2021.eacl-main.236","DBLP":"conf/eacl/XiaoW21","ArXiv":"2103.15025","DOI":"10.18653/v1/2021.eacl-main.236","CorpusId":232404053},"title":"On Hallucination and Predictive Uncertainty in Conditional Language Generation"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"d7ac65d335b5d847f4f5826313a8732bc7abc7a8","externalIds":{"MAG":"3132736064","ArXiv":"2102.09690","DBLP":"journals/corr/abs-2102-09690","CorpusId":231979430},"title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"d77c78c9439422ed88e754f776a642d43a8acb66","externalIds":{"ArXiv":"2012.14983","DBLP":"journals/tacl/MielkeSDB22","DOI":"10.1162/tacl_a_00494","CorpusId":250073258},"title":"Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration"},{"paperId":"33422275fbb9958f55419620697faf531482699b","externalIds":{"DBLP":"journals/tacl/JiangADN21","ArXiv":"2012.00955","DOI":"10.1162/tacl_a_00407","CorpusId":235078802},"title":"How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering"},{"paperId":"4ea5678069a6c4213f53972872a211d780f9f42b","externalIds":{"DBLP":"journals/corr/abs-2010-06610","ArXiv":"2010.06610","MAG":"3093141676","CorpusId":222341528},"title":"Training independent subnetworks for robust prediction"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"66ec09722934308519b7e9064691021588093d9e","externalIds":{"MAG":"3038776261","ArXiv":"2007.01458","DBLP":"conf/icml/MoonKSH20","CorpusId":220347323},"title":"Confidence-Aware Learning for Deep Neural Networks"},{"paperId":"cb693ce346f44e6b89ee814c6bb9f0e5cc2fa9d2","externalIds":{"DBLP":"journals/corr/abs-2006-09462","MAG":"3035847612","ArXiv":"2006.09462","ACL":"2020.acl-main.503","DOI":"10.18653/v1/2020.acl-main.503","CorpusId":219721462},"title":"Selective Question Answering under Domain Shift"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"3ca35c7df229549997491787d93c01de29af206d","externalIds":{"MAG":"3035072529","ArXiv":"2005.00963","ACL":"2020.acl-main.278","DBLP":"journals/corr/abs-2005-00963","DOI":"10.18653/v1/2020.acl-main.278","CorpusId":218487046},"title":"On the Inference Calibration of Neural Machine Translation"},{"paperId":"9b539d413393047b28bb7be9b195f142aaf7a80e","externalIds":{"ACL":"2021.eacl-main.24","MAG":"3023786569","DBLP":"conf/eacl/RollerDGJWLXOSB21","ArXiv":"2004.13637","DOI":"10.18653/v1/2021.eacl-main.24","CorpusId":216562425},"title":"Recipes for Building an Open-Domain Chatbot"},{"paperId":"52f47e781852a77abedada48cfa971b24c919dde","externalIds":{"ACL":"2020.emnlp-main.21","DBLP":"journals/corr/abs-2003-07892","ArXiv":"2003.07892","MAG":"3104939451","DOI":"10.18653/v1/2020.emnlp-main.21","CorpusId":212747810},"title":"Calibration of Pre-trained Transformers"},{"paperId":"07d440f44f5f955afef3c32f2610c7a716c36f97","externalIds":{"ArXiv":"2002.09437","DBLP":"journals/corr/abs-2002-09437","MAG":"3007260230","CorpusId":211252346},"title":"Calibrating Deep Neural Networks using Focal Loss"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","externalIds":{"MAG":"2996264288","ArXiv":"1912.08777","DBLP":"conf/icml/ZhangZSL20","CorpusId":209405420},"title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","externalIds":{"MAG":"2982399380","ACL":"2020.acl-main.703","DBLP":"journals/corr/abs-1910-13461","ArXiv":"1910.13461","DOI":"10.18653/v1/2020.acl-main.703","CorpusId":204960716},"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"5a05dd1e5e2859ecb845ee75e4ede22805fb22a6","externalIds":{"MAG":"2979585146","DBLP":"conf/nips/CorbiereTBCP19","ArXiv":"1910.04851","CorpusId":204401816},"title":"Addressing Failure Prediction by Learning Model Confidence"},{"paperId":"d4691aef27ae3c768b90c34ca5d8521d202eb47c","externalIds":{"MAG":"2971118045","DBLP":"conf/nips/KullPKFSF19","ArXiv":"1910.12656","CorpusId":202773833},"title":"Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","externalIds":{"DBLP":"journals/corr/abs-1909-01066","MAG":"2996758945","ArXiv":"1909.01066","ACL":"D19-1250","DOI":"10.18653/v1/D19-1250","CorpusId":202539551},"title":"Language Models as Knowledge Bases?"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","externalIds":{"DBLP":"journals/corr/abs-1908-10084","MAG":"2970641574","ArXiv":"1908.10084","ACL":"D19-1410","DOI":"10.18653/v1/D19-1410","CorpusId":201646309},"title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"3b656bba99dbcf6c6d8fd764d53ea64cd38c7050","externalIds":{"MAG":"2933254221","ArXiv":"1904.01685","DBLP":"conf/cvpr/NixonDZJT19","CorpusId":102486060},"title":"Measuring Calibration in Deep Learning"},{"paperId":"96c90ab340b2823a0040cd64b25d1d32af807994","externalIds":{"MAG":"2918914336","ArXiv":"1903.00802","DBLP":"journals/corr/abs-1903-00802","CorpusId":67855916},"title":"Calibration of Encoder Decoder Models for Neural Machine Translation"},{"paperId":"162cad5df347bdac469331df540440b320b5aa21","externalIds":{"ArXiv":"1901.11196","MAG":"2911588830","DBLP":"conf/emnlp/WeiZ19","ACL":"D19-1670","DOI":"10.18653/v1/D19-1670","CorpusId":59523656},"title":"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"},{"paperId":"fc097d528fd62fe76d73fafbf0c57473b58d1e84","externalIds":{"DBLP":"conf/wmt/MurrayC18","ACL":"W18-6322","ArXiv":"1808.10006","MAG":"2951062070","DOI":"10.18653/v1/W18-6322","CorpusId":52132833},"title":"Correcting Length Bias in Neural Machine Translation"},{"paperId":"d03ca175e2b2745126e792fdc31dfadae4c63afa","externalIds":{"ArXiv":"1807.03888","DBLP":"journals/corr/abs-1807-03888","MAG":"2867167548","CorpusId":49667948},"title":"A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks"},{"paperId":"1b59eea8ec4684381a885b59acd09c9151a49487","externalIds":{"MAG":"2921861056","DBLP":"conf/icml/VermaLBNMLB19","CorpusId":59604501},"title":"Manifold Mixup: Better Representations by Interpolating Hidden States"},{"paperId":"431ba9fae8fccad1665979d455c6307786e47318","externalIds":{"ArXiv":"1802.04865","DBLP":"journals/corr/abs-1802-04865","MAG":"2786712888","CorpusId":3271220},"title":"Learning Confidence for Out-of-Distribution Detection in Neural Networks"},{"paperId":"4feef0fd284feb1233399b400eb897f59ec92755","externalIds":{"MAG":"2765407302","DBLP":"journals/corr/abs-1710-09412","ArXiv":"1710.09412","CorpusId":3162051},"title":"mixup: Beyond Empirical Risk Minimization"},{"paperId":"79cfb51a51fc093f66aac8e858afe2e14d4a1f20","externalIds":{"MAG":"2950100464","DBLP":"journals/corr/abs-1708-02002","DOI":"10.1109/ICCV.2017.324","CorpusId":47252984},"title":"Focal Loss for Dense Object Detection"},{"paperId":"d65ce2b8300541414bfe51d03906fca72e93523c","externalIds":{"MAG":"2950953798","ArXiv":"1706.04599","DBLP":"journals/corr/GuoPSW17","CorpusId":28671436},"title":"On Calibration of Modern Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"ff7bcaa4556cb13fc7bf03e477172493546172cd","externalIds":{"DBLP":"journals/corr/KendallG17","MAG":"2600383743","ArXiv":"1703.04977","CorpusId":71134},"title":"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"},{"paperId":"6ce1922802169f757bbafc6e087cc274a867c763","externalIds":{"ArXiv":"1701.06548","DBLP":"conf/iclr/PereyraTCKH17","MAG":"2581377246","CorpusId":9545399},"title":"Regularizing Neural Networks by Penalizing Confident Output Distributions"},{"paperId":"802168a81571dde28f5ddb94d84677bc007afa7b","externalIds":{"DBLP":"conf/nips/Lakshminarayanan17","ArXiv":"1612.01474","MAG":"2963238274","CorpusId":6294674},"title":"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","externalIds":{"MAG":"2183341477","DBLP":"conf/cvpr/SzegedyVISW16","ArXiv":"1512.00567","DOI":"10.1109/CVPR.2016.308","CorpusId":206593880},"title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6","externalIds":{"MAG":"2964059111","DBLP":"conf/icml/GalG16","ArXiv":"1506.02142","CorpusId":160705},"title":"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016","externalIds":{"MAG":"2138779671","DBLP":"journals/ftml/KuleszaT12","ArXiv":"1207.6083","DOI":"10.1561/2200000044","CorpusId":51975610},"title":"Determinantal Point Processes for Machine Learning"},{"paperId":"c3a24b0b38922c4f3a825edb97cc470a4ca7af75","externalIds":{"DOI":"10.1016/S0140-6736(00)04549-9","CorpusId":208793436},"title":"Vision"},{"paperId":"48ddd9101a90fe65e3061de69626741b843ff5e4","externalIds":{"MAG":"2771514092","DBLP":"journals/pr/Bradley97","DOI":"10.1016/S0031-3203(96)00142-2","CorpusId":13806304},"title":"The use of the area under the ROC curve in the evaluation of machine learning algorithms"},{"paperId":"f9f23c63e2822687096b86edf4ae9435cb579b8c","externalIds":{"DBLP":"conf/eacl/WangLHNB24","ACL":"2024.findings-eacl.61","DOI":"10.18653/v1/2024.findings-eacl.61","CorpusId":268417319},"title":"Do-Not-Answer: Evaluating Safeguards in LLMs"},{"paperId":"b195b707dc54fab51ae22a0cd7511e151b6533f4","externalIds":{"DBLP":"conf/eacl/YoshikawaO23","ACL":"2023.findings-eacl.150","DOI":"10.18653/v1/2023.findings-eacl.150","CorpusId":258378336},"title":"Selective-LAMA: Selective Prediction for Confidence-Aware Evaluation of Language Models"},{"paperId":"1fa4469e5bc5d096572902fe14b0d66078a24c47","externalIds":{"DBLP":"journals/corr/abs-2302-13439","DOI":"10.48550/arXiv.2302.13439","CorpusId":257220189},"title":"Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models"},{"paperId":"2a4b854eb06b8c5a74eaf7494ad87322ba0f6c69","externalIds":{"DBLP":"journals/corr/abs-2308-16175","DOI":"10.48550/arXiv.2308.16175","CorpusId":261339369},"title":"Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment"},{"paperId":"1be6507fef4bf89c24b44dd9bca35d9195a663d8","externalIds":{"DBLP":"conf/acl/VazhentsevTVPVP23","DOI":"10.18653/v1/2023.findings-acl.93","CorpusId":259858831},"title":"Efficient Out-of-Domain Detection for Sequence to Sequence Models"},{"paperId":"f841a80e5de0bc5e0b4ffb4eff91d443672e33a9","externalIds":{"DBLP":"conf/acl/VazhentsevKTPPB23","ACL":"2023.acl-long.652","DOI":"10.18653/v1/2023.acl-long.652","CorpusId":259370752},"title":"Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks"},{"paperId":"43b15205c98b5e0693f128ebdd4c57c4ba854049","externalIds":{"ACL":"2023.acl-long.141","DBLP":"conf/acl/YuZXZSZ23","DOI":"10.18653/v1/2023.acl-long.141","CorpusId":252280753},"title":"Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach"},{"paperId":"05aa63683f7d027c18f560d4472ac87c1ea754fe","externalIds":{"DBLP":"conf/nips/PenedoMHCACPAL23","CorpusId":268096300},"title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only"},{"paperId":"754e1b2a9df067a03dbc6605b0cd07f3112e361a","externalIds":{"DBLP":"journals/corr/abs-2311-09731","DOI":"10.48550/arXiv.2311.09731","CorpusId":271866244},"title":"Prudent Silence or Foolish Babble? Examining Large Language Models' Responses to the Unknown"},{"paperId":"8ae920111435a7db8da360c654c771c53f57c69a","externalIds":{"ACL":"2022.acl-long.566","DBLP":"conf/acl/VazhentsevKSTTF22","DOI":"10.18653/v1/2022.acl-long.566","CorpusId":248780161},"title":"Uncertainty Estimation of Transformer Predictions for Misclassification Detection"},{"paperId":"a5584d2d9b0de9e1692241d46d0c70942919cd60","externalIds":{"DBLP":"conf/acl/Kumar22","ACL":"2022.acl-long.49","DOI":"10.18653/v1/2022.acl-long.49","CorpusId":248780520},"title":"Answer-level Calibration for Free-form Multiple Choice Question Answering"},{"paperId":"e1b9e395fb92704285aa158540a1cd8b9190a650","externalIds":{"DBLP":"journals/corr/abs-2207-05161","DOI":"10.48550/arXiv.2207.05161","CorpusId":274639150},"title":"DAUX: a Density-based Approach for Uncertainty eXplanations"},{"paperId":"544cdd96dff6f4744ff3186b1f6f8e6b5d997fa6","externalIds":{"ACL":"2021.eacl-main.157","DBLP":"conf/eacl/ShelmanovTPFPP21","DOI":"10.18653/v1/2021.eacl-main.157","CorpusId":231993280},"title":"How Certain is Your Transformer?"},{"paperId":"d769e1e5bc3511d70157fa3ea82591f5491421fe","externalIds":{"DOI":"10.1007/978-3-030-57077-4_9","CorpusId":242728340},"title":"Chatbot"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"70d6dfdc40c4681ba5d51d60116db0311b5126ce","externalIds":{"DBLP":"reference/db/Hiemstra18","DOI":"10.1007/978-0-387-39940-9_923","CorpusId":10924669},"title":"Language Models"}]}