{"references":[{"paperId":"8613bc7c699c58814c528b04fe12fb553ef906ef","externalIds":{"DBLP":"journals/pami/WangWGW25","DOI":"10.1109/TPAMI.2024.3518762","CorpusId":276118180,"PubMed":"40030702"},"title":"PDPP: Projected Diffusion for Procedure Planning in Instructional Videos"},{"paperId":"db7541787a071768603f3254b2411fcfa983be26","externalIds":{"DBLP":"conf/nips/0001WZCY24","ArXiv":"2412.19806","DOI":"10.48550/arXiv.2412.19806","CorpusId":270556280},"title":"Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing"},{"paperId":"ff6fbf97165e7912e2f4b993a2a6774101a6ad77","externalIds":{"DBLP":"conf/nips/WengYXWJ24","ArXiv":"2408.15241","DOI":"10.48550/arXiv.2408.15241","CorpusId":271962817},"title":"GenRec: Unifying Video Generation and Recognition with Diffusion Models"},{"paperId":"8613c1081a6ab34e2f980e35c06a1af461d7314e","externalIds":{"DBLP":"journals/corr/abs-2406-09399","ArXiv":"2406.09399","DOI":"10.48550/arXiv.2406.09399","CorpusId":270440676},"title":"OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation"},{"paperId":"d657de11802239a58a77f486a0c6861e89d4da50","externalIds":{"DBLP":"journals/corr/abs-2406-06465","ArXiv":"2406.06465","DOI":"10.48550/arXiv.2406.06465","CorpusId":270371710},"title":"AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction"},{"paperId":"4cdc184f728aa27d6ab9b1791ade28d9e761ceab","externalIds":{"ArXiv":"2405.15881","DBLP":"journals/corr/abs-2405-15881","DOI":"10.48550/arXiv.2405.15881","CorpusId":270063523},"title":"Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation"},{"paperId":"95fa89d968216b7c31e5993bd9bc08dfd9a81c20","externalIds":{"DBLP":"journals/corr/abs-2405-03025","ArXiv":"2405.03025","DOI":"10.48550/arXiv.2405.03025","CorpusId":269604707},"title":"Matten: Video Generation with Mamba-Attention"},{"paperId":"74d2320484531c4425f9941ec55e13e4919427fe","externalIds":{"ArXiv":"2404.18620","DBLP":"journals/corr/abs-2404-18620","DOI":"10.48550/arXiv.2404.18620","CorpusId":269449355},"title":"FlexiFilm: Long Video Generation with Flexible Conditions"},{"paperId":"5cbe42a201e2d1a90ac83dadc78afec6a3d20fec","externalIds":{"DBLP":"journals/corr/abs-2404-16306","ArXiv":"2404.16306","DOI":"10.1109/CVPR52733.2024.00861","CorpusId":269362272},"title":"TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models"},{"paperId":"a9409e4e599501fc753173d8889010413d5ab855","externalIds":{"ArXiv":"2404.12391","DBLP":"journals/corr/abs-2404-12391","DOI":"10.1109/CVPR52733.2024.00695","CorpusId":269214646},"title":"On the Content Bias in Fr√©chet Video Distance"},{"paperId":"abfcabc96921e535d7790fdf11fbe87d8e63fd27","externalIds":{"DBLP":"journals/corr/abs-2404-00234","ArXiv":"2404.00234","DOI":"10.1109/CVPR52733.2024.00834","CorpusId":268819254},"title":"Grid Diffusion Models for Text-to-Video Generation"},{"paperId":"ae7f4a6af77bc1abae1a5935827d96e2de75d969","externalIds":{"ArXiv":"2403.14148","DBLP":"conf/iclr/YuNHLSA24","DOI":"10.48550/arXiv.2403.14148","CorpusId":268554134},"title":"Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition"},{"paperId":"8eae862d9669e7001eeee17b49fba793df9672c4","externalIds":{"ArXiv":"2402.19479","DBLP":"conf/cvpr/ChenSMDCJF0RYT24","DOI":"10.1109/CVPR52733.2024.01265","CorpusId":268091168},"title":"Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers"},{"paperId":"aac7c6a33dc0f71bd4853e41e205ad908d3b3d09","externalIds":{"DBLP":"journals/corr/abs-2402-17485","ArXiv":"2402.17485","DOI":"10.48550/arXiv.2402.17485","CorpusId":268032834},"title":"EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions"},{"paperId":"97cb2eb0d0517e34bf4202f0593600bb6fa043cd","externalIds":{"DBLP":"journals/corr/abs-2402-14797","ArXiv":"2402.14797","DOI":"10.1109/CVPR52733.2024.00672","CorpusId":267782739},"title":"Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis"},{"paperId":"7f6f1356889c5de1509ce59cb53eacb019048456","externalIds":{"ArXiv":"2403.09669","DBLP":"journals/corr/abs-2403-09669","DOI":"10.48550/arXiv.2403.09669","CorpusId":268510605},"title":"STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models"},{"paperId":"ba5d21ca1a8b2f27d244a7d79ac95ac8d5e87e0b","externalIds":{"ArXiv":"2401.16224","DBLP":"conf/ijcai/Duan0CQ024","DOI":"10.48550/arXiv.2401.16224","CorpusId":267311920},"title":"Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models"},{"paperId":"c5a3b7f3d0203f001621e48a52e24c029963a2a3","externalIds":{"DBLP":"journals/corr/abs-2401-07781","ArXiv":"2401.07781","DOI":"10.48550/arXiv.2401.07781","CorpusId":266998720},"title":"Towards A Better Metric for Text-to-Video Generation"},{"paperId":"e0eac8c64be3313e581c28a495bec192e7e67284","externalIds":{"ArXiv":"2401.03048","DBLP":"journals/tmlr/Ma0CJ0LC025","CorpusId":266844878},"title":"Latte: Latent Diffusion Transformer for Video Generation"},{"paperId":"322ca4c62d68ca8814534042096bbec40f743679","externalIds":{"DBLP":"journals/corr/abs-2401-01651","ArXiv":"2401.01651","DOI":"10.48550/arXiv.2401.01651","CorpusId":266741637},"title":"AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI"},{"paperId":"43bafa19f94a42caea89b32a86489aa850317617","externalIds":{"DBLP":"conf/cvpr/LiangWWYLZMHZVM24","ArXiv":"2312.17681","DOI":"10.1109/CVPR52733.2024.00784","CorpusId":266690780},"title":"FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis"},{"paperId":"e0d62e25811018636c22d2cc76650b9d31968890","externalIds":{"DBLP":"conf/cvpr/WuCWJKXLYV24","ArXiv":"2312.13834","DOI":"10.1109/CVPR52733.2024.00789","CorpusId":266435967},"title":"Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis"},{"paperId":"e97b37c3d4c73912e19c5d2fc664ee2fb7a55c46","externalIds":{"DBLP":"journals/corr/abs-2312-09109","ArXiv":"2312.09109","DOI":"10.48550/arXiv.2312.09109","CorpusId":266209871},"title":"VideoLCM: Video Latent Consistency Model"},{"paperId":"905ba940236b00bebb2fd348d4d932e7887b0c0a","externalIds":{"DBLP":"journals/corr/abs-2312-06662","ArXiv":"2312.06662","DOI":"10.48550/arXiv.2312.06662","CorpusId":266163109},"title":"Photorealistic Video Generation with Diffusion Models"},{"paperId":"aa2d103e58acb17908799954fb50058125ef17ca","externalIds":{"DBLP":"journals/corr/abs-2312-04433","ArXiv":"2312.04433","DOI":"10.1109/CVPR52733.2024.00625","CorpusId":266053833},"title":"Dream Video: Composing Your Dream Videos with Customized Subject and Motion"},{"paperId":"457828497f66d3c4abb1f0ee0d0dc58ab74d01cd","externalIds":{"DBLP":"conf/cvpr/ChenXRC0XS00P24","ArXiv":"2312.04557","DOI":"10.1109/CVPR52733.2024.00616","CorpusId":266053134},"title":"GenTron: Diffusion Transformers for Image and Video Generation"},{"paperId":"44999d9bae3976d38e91b742c4541a21c7000260","externalIds":{"DBLP":"conf/siggraph/WangYWLCXLS24","ArXiv":"2312.03641","DOI":"10.1145/3641519.3657518","CorpusId":265696111},"title":"MotionCtrl: A Unified and Flexible Motion Controller for Video Generation"},{"paperId":"d37533f318857a480b52c258ae4581d84f5acffa","externalIds":{"ArXiv":"2312.03816","DBLP":"journals/corr/abs-2312-03816","DOI":"10.1109/CVPR52733.2024.00684","CorpusId":266055411},"title":"AVID: Any-Length Video Inpainting with Diffusion Model"},{"paperId":"875358079869189aa185335c621f7e9346b2f5a5","externalIds":{"ArXiv":"2311.18830","DBLP":"journals/corr/abs-2311-18830","DOI":"10.1109/CVPR52733.2024.00753","CorpusId":265506039},"title":"MotionEditor: Editing Video Motion via Content-Aware Diffusion"},{"paperId":"1b979967ab3f008c04604f1b4e36384a5aff21f9","externalIds":{"ArXiv":"2311.18829","DBLP":"conf/cvpr/WangBWFYYZDZWQY24","DOI":"10.1109/CVPR52733.2024.00804","CorpusId":265506813},"title":"MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation"},{"paperId":"e24241a8188d7c41d841cd6413da936adc247bb4","externalIds":{"ArXiv":"2311.18837","DBLP":"journals/corr/abs-2311-18837","DOI":"10.48550/arXiv.2311.18837","CorpusId":265506407},"title":"VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models"},{"paperId":"4e9a8141da2a8c603722b07d096109207f8e0b66","externalIds":{"DBLP":"journals/corr/abs-2311-17982","ArXiv":"2311.17982","DOI":"10.1109/CVPR52733.2024.02060","CorpusId":265506207},"title":"VBench: Comprehensive Benchmark Suite for Video Generative Models"},{"paperId":"c8dc4af5c61f95cc79b7f83e8339efa62af8f811","externalIds":{"ArXiv":"2311.17117","DBLP":"conf/cvpr/Hu24","DOI":"10.1109/CVPR52733.2024.00779","CorpusId":265499043},"title":"Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"},{"paperId":"9d587b3f1d0608bf89fdb4ea6eed3312a73e938c","externalIds":{"DBLP":"conf/cvpr/XuZLYLZFS24","ArXiv":"2311.16498","DOI":"10.1109/CVPR52733.2024.00147","CorpusId":265466012},"title":"MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"908c1a6386f57fcfd2aa1186b4c4275bfd7d5f6c","externalIds":{"DBLP":"journals/corr/abs-2311-14768","ArXiv":"2311.14768","DOI":"10.48550/arXiv.2311.14768","CorpusId":265456553},"title":"AdaDiff: Adaptive Step Selection for Fast Diffusion"},{"paperId":"8467c119251ea48204586d1c3c67aa8fb781f7c4","externalIds":{"ArXiv":"2311.10982","DBLP":"journals/corr/abs-2311-10982","DOI":"10.1109/CVPR52733.2024.00845","CorpusId":265294415},"title":"Make Pixels Dance: High-Dynamic Video Generation"},{"paperId":"85b10400864187230714506412c85610c786b5c3","externalIds":{"ArXiv":"2311.10709","DBLP":"conf/eccv/GirdharSBDARSYPM24","DOI":"10.48550/arXiv.2311.10709","CorpusId":265281059},"title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning"},{"paperId":"426b58e945044f89137f19a76e162c20065ff362","externalIds":{"DBLP":"journals/corr/abs-2311-01813","ArXiv":"2311.01813","DOI":"10.48550/arXiv.2311.01813","CorpusId":265019385},"title":"FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation"},{"paperId":"083bab4a967c2221d9f4da9110fe37d8ca679078","externalIds":{"DBLP":"conf/eccv/XingXZCYLLWSW24","ArXiv":"2310.12190","DOI":"10.48550/arXiv.2310.12190","CorpusId":264306292},"title":"DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors"},{"paperId":"66d927fdb6c2774131960c75275546fd5ee3dd72","externalIds":{"ArXiv":"2310.11440","DBLP":"journals/corr/abs-2310-11440","DOI":"10.1109/CVPR52733.2024.02090","CorpusId":264172222},"title":"EvalCrafter: Benchmarking and Evaluating Large Video Generation Models"},{"paperId":"005f3db2174474853cd681d6b8547ee887d36500","externalIds":{"DBLP":"journals/corr/abs-2310-05922","ArXiv":"2310.05922","DOI":"10.48550/arXiv.2310.05922","CorpusId":263829513},"title":"FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing"},{"paperId":"0439bbee19b4fba596d1e2498b3f45ca265432ba","externalIds":{"ArXiv":"2310.01107","DBLP":"journals/corr/abs-2310-01107","DOI":"10.48550/arXiv.2310.01107","CorpusId":263605399},"title":"Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models"},{"paperId":"87bf66eb6d22df17f70170a0e575b4f12c4813ef","externalIds":{"ArXiv":"2309.17444","DBLP":"conf/iclr/LianSYDL24","DOI":"10.48550/arXiv.2309.17444","CorpusId":263310643},"title":"LLM-grounded Video Diffusion Models"},{"paperId":"53d193a9fb82ce0c2b5d368071b405f63dd98c90","externalIds":{"DBLP":"conf/cvpr/FengWWYBL0G24","ArXiv":"2309.16496","DOI":"10.1109/CVPR52733.2024.00641","CorpusId":263139349},"title":"CCEdit: Creative and Controllable Video Editing via Diffusion Models"},{"paperId":"a5b7fc1bff0910ff31975ec0a15ed30c41f0a968","externalIds":{"DBLP":"journals/ijcv/ZhangWLZRGGS25","ArXiv":"2309.15818","DOI":"10.1007/s11263-024-02271-9","CorpusId":263151295},"title":"Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation"},{"paperId":"ed4603ea341acc26cab24f41aa40524fb7779917","externalIds":{"DBLP":"journals/ijcv/WangCMZHWYHYYGWSJCLDLQL25","ArXiv":"2309.15103","DOI":"10.1007/s11263-024-02295-1","CorpusId":262823915},"title":"LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models"},{"paperId":"16753e0317730e8c1b297338300a8c6163dd06f2","externalIds":{"ArXiv":"2309.15091","DBLP":"journals/corr/abs-2309-15091","DOI":"10.48550/arXiv.2309.15091","CorpusId":262825203},"title":"VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning"},{"paperId":"120aca3e415b6641a0b0cd20695ab85ed7789612","externalIds":{"DBLP":"journals/corr/abs-2309-14494","ArXiv":"2309.14494","DOI":"10.48550/arXiv.2309.14494","CorpusId":262824917},"title":"Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator"},{"paperId":"ef5d682a3efed36cdd3809d51a1a984f84c4b478","externalIds":{"DBLP":"conf/cvpr/Li0SH24","ArXiv":"2309.07906","DOI":"10.1109/CVPR52733.2024.02279","CorpusId":261823270},"title":"Generative Image Dynamics"},{"paperId":"8c68e6ed656b1a6712dcbafc972bdb18678b9d16","externalIds":{"DBLP":"journals/corr/abs-2309-08009","ArXiv":"2309.08009","DOI":"10.48550/arXiv.2309.08009","CorpusId":262012984},"title":"Measuring the Quality of Text-to-Video Model Outputs: Metrics and Dataset"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"b08f25092c219c5ec6fe0baa83e4073f30d278c0","externalIds":{"ArXiv":"2309.04509","DBLP":"journals/corr/abs-2309-04509","DOI":"10.1109/ICCV51070.2023.00719","CorpusId":261682368},"title":"The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion"},{"paperId":"aadd141d5853dabdc631b73d2bc8700e8ac7b8c0","externalIds":{"ArXiv":"2309.03549","DBLP":"journals/corr/abs-2309-03549","DOI":"10.48550/arXiv.2309.03549","CorpusId":261582324},"title":"Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation"},{"paperId":"18c612220419eafa98083e6891cbb6239b8af42a","externalIds":{"ArXiv":"2309.00908","DBLP":"journals/corr/abs-2309-00908","DOI":"10.48550/arXiv.2309.00908","CorpusId":261531037},"title":"MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation"},{"paperId":"156fc6ed50e9873814dd8554526eedf9841a1be0","externalIds":{"ArXiv":"2309.00398","DBLP":"journals/corr/abs-2309-00398","DOI":"10.48550/arXiv.2309.00398","CorpusId":261494083},"title":"VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation"},{"paperId":"8414b2eacff7ba614be73b2436a4c705a6f2426d","externalIds":{"ArXiv":"2308.16909","DBLP":"journals/corr/abs-2308-16909","DOI":"10.1109/ICCV51070.2023.02089","CorpusId":261395372},"title":"StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation"},{"paperId":"8819777e104f8c4197c262e11a01b070b50007aa","externalIds":{"ArXiv":"2308.14749","DBLP":"journals/corr/abs-2308-14749","DOI":"10.48550/arXiv.2308.14749","CorpusId":261276629},"title":"MagicEdit: High-Fidelity and Temporally Coherent Video Editing"},{"paperId":"7ae26eb895b2acd4949a33d0879cae15f1b0d6e3","externalIds":{"DBLP":"journals/corr/abs-2308-10648","ArXiv":"2308.10648","DOI":"10.48550/arXiv.2308.10648","CorpusId":261049739},"title":"EVE: Efficient zero-shot text-based Video Editing with Depth Map Guidance and Temporal Consistency Constraints"},{"paperId":"2ef3c9b0830f0a599d900d267c840667295a0811","externalIds":{"DBLP":"journals/corr/abs-2308-10079","ArXiv":"2308.10079","DOI":"10.48550/arXiv.2308.10079","CorpusId":261049744},"title":"MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance"},{"paperId":"c675d23b08f42760133cabff46753bc46a9bceec","externalIds":{"ArXiv":"2308.09710","DBLP":"journals/corr/abs-2308-09710","DOI":"10.1109/CVPR52733.2024.00748","CorpusId":261030352},"title":"SimDA: Simple Diffusion Adapter for Efficient Video Generation"},{"paperId":"05cbac9a5101f47a6fabad72398616506572c9fa","externalIds":{"DBLP":"journals/corr/abs-2308-09592","ArXiv":"2308.09592","DOI":"10.1109/ICCV51070.2023.02106","CorpusId":261031087},"title":"StableVideo: Text-driven Consistency-aware Diffusion Video Editing"},{"paperId":"0e562251ce4dd1a3301a932dc4bb2deabeb8fe78","externalIds":{"DBLP":"journals/tomccap/WangLZLDCC24","ArXiv":"2308.09091","DOI":"10.1145/3691344","CorpusId":261030771},"title":"Edit Temporal-Consistent Videos with Image Diffusion Model"},{"paperId":"4c5823f5e11faef0104fb7193a4ee84bea5dc22b","externalIds":{"ArXiv":"2308.08316","DBLP":"journals/corr/abs-2308-08316","DOI":"10.48550/arXiv.2308.08316","CorpusId":260926302},"title":"Dual-Stream Diffusion Net for Text-to-Video Generation"},{"paperId":"394aaa4b6343ec1a25fbf4693ac0caf479984a03","externalIds":{"ArXiv":"2308.08089","DBLP":"journals/corr/abs-2308-08089","DOI":"10.48550/arXiv.2308.08089","CorpusId":260925229},"title":"DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory"},{"paperId":"10603f24428220cb3b78b0c23fb7e24cbee71f95","externalIds":{"ArXiv":"2308.07749","DBLP":"journals/corr/abs-2308-07749","DOI":"10.48550/arXiv.2308.07749","CorpusId":260900206},"title":"Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model"},{"paperId":"84f0a99d0f0015a6145c94468870d43ab1d166fd","externalIds":{"DBLP":"journals/corr/abs-2308-06571","ArXiv":"2308.06571","DOI":"10.48550/arXiv.2308.06571","CorpusId":260887737},"title":"ModelScope Text-to-Video Technical Report"},{"paperId":"96c430ffce99cdedb6a1b4537113f803e0c3b860","externalIds":{"ArXiv":"2308.03463","DBLP":"conf/pkdd/DuanYWCWQH24","DOI":"10.48550/arXiv.2308.03463","CorpusId":260680830},"title":"DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis"},{"paperId":"68a70932b513a75a1887e2d3311c5502ac0b3803","externalIds":{"ArXiv":"2307.16687","DBLP":"journals/corr/abs-2307-16687","DOI":"10.1109/ICCV51070.2023.01365","CorpusId":260333942},"title":"DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation"},{"paperId":"d4c33129904a62965443c9cc7906d96814671ef4","externalIds":{"DBLP":"journals/corr/abs-2307-14073","ArXiv":"2307.14073","DOI":"10.48550/arXiv.2307.14073","CorpusId":260164708},"title":"VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet"},{"paperId":"ae23def9454956869987b4bab2c6de7c060fd217","externalIds":{"DBLP":"conf/iccvw/Khandelwal23a","ArXiv":"2308.00135","DOI":"10.1109/ICCVW60793.2023.00324","CorpusId":260351466},"title":"InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing"},{"paperId":"4761f173965195798cd3046ef4af608a83504e4d","externalIds":{"DBLP":"conf/iclr/GeyerBBD24","ArXiv":"2307.10373","DOI":"10.48550/arXiv.2307.10373","CorpusId":259991741},"title":"TokenFlow: Consistent Diffusion Features for Consistent Video Editing"},{"paperId":"b2ad898fd78989e9b76c4dd4dda4e6a38022a253","externalIds":{"DBLP":"conf/iccv/FlaboreaCMDPG23","ArXiv":"2307.07205","DOI":"10.1109/ICCV51070.2023.00947","CorpusId":259924822},"title":"Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection"},{"paperId":"369b449415d50387fba048bbd4d26ee890df84b5","externalIds":{"ArXiv":"2307.06942","DBLP":"conf/iclr/WangH00YML0C00024","DOI":"10.48550/arXiv.2307.06942","CorpusId":259847783},"title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"},{"paperId":"77040969110fab39a55699cb06f9edf68789445a","externalIds":{"ArXiv":"2307.06940","DBLP":"journals/corr/abs-2307-06940","DOI":"10.48550/arXiv.2307.06940","CorpusId":259847699},"title":"Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation"},{"paperId":"c1caa303549764d220ff17dc1785985dd1ba6047","externalIds":{"DBLP":"journals/corr/abs-2307-04725","ArXiv":"2307.04725","DOI":"10.48550/arXiv.2307.04725","CorpusId":259501509},"title":"AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning"},{"paperId":"276d80c7bb09d32e364e633ed5b49ac26c191842","externalIds":{"DBLP":"conf/nips/LiXXZZZZ023","ArXiv":"2307.02869","DOI":"10.48550/arXiv.2307.02869","CorpusId":259360603},"title":"MomentDiff: Generative Video Moment Retrieval from Random to Real"},{"paperId":"81583156b7786f50f3a2ede06ccd308c6c80f2e1","externalIds":{"DBLP":"journals/corr/abs-2307-04787","ArXiv":"2307.04787","DOI":"10.48550/arXiv.2307.04787","CorpusId":259766481},"title":"Collaborative Score Distillation for Consistent Visual Synthesis"},{"paperId":"dce3db6977b27a37a4baf073d0734f6e616a2a1a","externalIds":{"DBLP":"journals/corr/abs-2307-01533","ArXiv":"2307.01533","DOI":"10.48550/arXiv.2307.01533","CorpusId":259342152},"title":"Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations"},{"paperId":"da771b92882d87f97dead9c5705d67ae0c717916","externalIds":{"ArXiv":"2306.11173","DBLP":"journals/corr/abs-2306-11173","DOI":"10.48550/arXiv.2306.11173","CorpusId":259203373},"title":"GD-VDM: Generated Depth for better Diffusion-based Video Generation"},{"paperId":"d0d0bb67ff0f7a1373da65c89f7b9e58fd408e90","externalIds":{"DBLP":"conf/ijcnn/LiuWLHL23","DOI":"10.1109/IJCNN54540.2023.10191565","CorpusId":260387646},"title":"ED-T2V: An Efficient Training Framework for Diffusion-based Text-to-Video Generation"},{"paperId":"33e7493ebe199b44620957e91f65f5b2de34df5e","externalIds":{"DBLP":"journals/tmlr/CouaironRHT24","ArXiv":"2306.08707","DOI":"10.48550/arXiv.2306.08707","CorpusId":259164907},"title":"VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing"},{"paperId":"1e09b83fe064826a9a1ac61a7bdc00f26be41aee","externalIds":{"DBLP":"journals/corr/abs-2306-07954","ArXiv":"2306.07954","DOI":"10.1145/3610548.3618160","CorpusId":259144797},"title":"Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation"},{"paperId":"05aea4a4646cb971bb253ced42e8935034a60b57","externalIds":{"ArXiv":"2306.07257","DBLP":"conf/mm/Zhu0H0TCGSF23","DOI":"10.1145/3581783.3612707","CorpusId":259138745},"title":"MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images"},{"paperId":"74e3b4dd3f12d0eae1de836be288fe19c3f5e2db","externalIds":{"ArXiv":"2306.03437","DBLP":"journals/corr/abs-2306-03437","DOI":"10.48550/arXiv.2306.03437","CorpusId":259088908},"title":"DFormer: Diffusion-guided Transformer for Universal Image Segmentation"},{"paperId":"b9d7c876d17b86f07fe8296b5c9dc5c140949126","externalIds":{"ArXiv":"2306.02562","DBLP":"conf/ijcai/YangZ0JH23","DOI":"10.48550/arXiv.2306.02562","CorpusId":259075596},"title":"Video Diffusion Models with Local-Global Context Guidance"},{"paperId":"5f563855ac78176203c70ac7fc5772713fa5ca26","externalIds":{"ArXiv":"2306.02903","DBLP":"journals/corr/abs-2306-02903","DOI":"10.48550/arXiv.2306.02903","CorpusId":259075281},"title":"Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions"},{"paperId":"f6c494234a818b2aec286b257ee6117f2894bcf7","externalIds":{"DBLP":"conf/icassp/ElizaldeDIW23","DOI":"10.1109/ICASSP49357.2023.10095889","CorpusId":249605738},"title":"CLAP Learning Audio Concepts from Natural Language Supervision"},{"paperId":"f02ea7a18f00859d9ea1b321e3385ae7d0170639","externalIds":{"DBLP":"journals/corr/abs-2306-02018","ArXiv":"2306.02018","DOI":"10.48550/arXiv.2306.02018","CorpusId":259075720},"title":"VideoComposer: Compositional Video Synthesis with Motion Controllability"},{"paperId":"8de8177147fc2dda4b430ca6d6cb2fdc179756aa","externalIds":{"ArXiv":"2306.01732","DBLP":"journals/corr/abs-2306-01732","DOI":"10.48550/arXiv.2306.01732","CorpusId":259063794},"title":"Video Colorization with Pre-trained Text-to-Image Diffusion Models"},{"paperId":"7820f9e98c9d064a0402685be2cf875a916edd27","externalIds":{"DBLP":"journals/corr/abs-2306-01872","ArXiv":"2306.01872","DOI":"10.48550/arXiv.2306.01872","CorpusId":259075709},"title":"Probabilistic Adaptation of Text-to-Video Models"},{"paperId":"52b10ae66d025e99fbb602935e155f97f4f0696f","externalIds":{"DBLP":"journals/corr/abs-2306-00943","ArXiv":"2306.00943","DOI":"10.1109/TVCG.2024.3365804","CorpusId":258999372,"PubMed":"38354074"},"title":"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance"},{"paperId":"25a69bdd7f87ecc287b4e9a64eb4d6d562371add","externalIds":{"ArXiv":"2308.14726","DBLP":"journals/corr/abs-2308-14726","DOI":"10.1109/CVPR52729.2023.01703","CorpusId":261080918},"title":"PanoSwin: a Pano-style Swin Transformer for Panorama Understanding"},{"paperId":"d65a1a5b425385e99697865d8ec0af0678caae41","externalIds":{"DBLP":"journals/corr/abs-2305-18670","ArXiv":"2305.18670","DOI":"10.48550/arXiv.2305.18670","CorpusId":258968060},"title":"SAVE: Spectral-Shift-Aware Adaptation of Image Diffusion Models for Text-guided Video Editing"},{"paperId":"481e00d33baba3a2c8023e0185630bb17240bca5","externalIds":{"ArXiv":"2305.18264","DBLP":"journals/corr/abs-2305-18264","DOI":"10.48550/arXiv.2305.18264","CorpusId":258960293},"title":"Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising"},{"paperId":"5712960ca1d637ba6e57de43fad3daac04bff4e2","externalIds":{"ArXiv":"2305.17431","DBLP":"journals/corr/abs-2305-17431","DOI":"10.48550/arXiv.2305.17431","CorpusId":258960525},"title":"Towards Consistent Video Editing with Text-to-Image Diffusion Models"},{"paperId":"26dc7a63c0660d6861de980f26a53c7fc2ee0d5f","externalIds":{"ArXiv":"2305.16301","DBLP":"journals/corr/abs-2305-16301","DOI":"10.48550/arXiv.2305.16301","CorpusId":258888066},"title":"Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos"},{"paperId":"abab79d1135684d039cfbebd0097e48ef4c1940c","externalIds":{"DBLP":"conf/cvpr/ZhouS23","ArXiv":"2305.14598","DOI":"10.1109/CVPRW59228.2023.00090","CorpusId":258865185},"title":"Vision + Language Applications: A Survey"},{"paperId":"b1750d2a6e3480e690999916a86c8b3876577b39","externalIds":{"ArXiv":"2305.14330","DBLP":"journals/corr/abs-2305-14330","DOI":"10.48550/arXiv.2305.14330","CorpusId":258841252},"title":"Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation"},{"paperId":"529191401a8a5f0a8bdb2a1c01301d76af585a3a","externalIds":{"DBLP":"journals/corr/abs-2305-13077","ArXiv":"2305.13077","DOI":"10.48550/arXiv.2305.13077","CorpusId":258832670},"title":"ControlVideo: Training-free Controllable Text-to-Video Generation"},{"paperId":"23005cd763b0ad723390f07c4a82693d58657919","externalIds":{"DBLP":"journals/corr/abs-2305-12689","ArXiv":"2305.12689","DOI":"10.48550/arXiv.2305.12689","CorpusId":258832477},"title":"FIT: Far-reaching Interleaved Transformers"},{"paperId":"205d2ed0906440f07a0275d7d6a63bced60951fc","externalIds":{"DBLP":"conf/icmcs/Qin0TCZ24","ArXiv":"2305.12328","DOI":"10.1109/ICME57554.2024.10687529","CorpusId":258832497},"title":"InstructVid2Vid: Controllable Video Editing with Natural Language Instructions"},{"paperId":"9f411fda2ad5b141a3115f707bcf5ee865b3fb94","externalIds":{"DBLP":"conf/nips/TangYZ0B23","ArXiv":"2305.11846","DOI":"10.48550/arXiv.2305.11846","CorpusId":258822817},"title":"Any-to-Any Generation via Composable Diffusion"},{"paperId":"12cbf907d40a5406ca855f51af54cc16d0b28cd6","externalIds":{"ArXiv":"2305.11675","DBLP":"journals/corr/abs-2305-11675","DOI":"10.48550/arXiv.2305.11675","CorpusId":258823377},"title":"Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity"},{"paperId":"1058b7f3d420093c111514823d6b9400912b859b","externalIds":{"DBLP":"journals/ijcv/WangYTHZFL25","ArXiv":"2305.10874","DOI":"10.1007/s11263-025-02349-y","CorpusId":258762479},"title":"Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation"},{"paperId":"02bc11de9d2f75bad48166098aa6b30fffee4d70","externalIds":{"ArXiv":"2305.10474","DBLP":"journals/corr/abs-2305-10474","DOI":"10.1109/ICCV51070.2023.02096","CorpusId":258762178},"title":"Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models"},{"paperId":"5f51eda9f7abddca027941d50fb0b6bf6f508eff","externalIds":{"ArXiv":"2305.08850","DBLP":"journals/corr/abs-2305-08850","DOI":"10.48550/arXiv.2305.08850","CorpusId":258686590},"title":"Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts"},{"paperId":"e01ed6611f9c998c237cda814ff8366a5acb6c3d","externalIds":{"ArXiv":"2305.06131","DBLP":"journals/corr/abs-2305-06131","DOI":"10.48550/arXiv.2305.06131","CorpusId":258588157},"title":"Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era"},{"paperId":"8482fb463dfecb4c24b1af7b7f47307e726bcc05","externalIds":{"DBLP":"journals/corr/abs-2305-05464","ArXiv":"2305.05464","DOI":"10.1109/LSP.2024.3398538","CorpusId":258564566},"title":"Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer"},{"paperId":"43c93968ea54ea098879c548732272aa22ce04ab","externalIds":{"DBLP":"journals/ijcv/WangMCCDDQ25","ArXiv":"2305.03989","DOI":"10.1007/s11263-024-02231-3","CorpusId":258557986},"title":"LEO: Generative Latent Image Animator for Human Video Synthesis"},{"paperId":"15efd2755d422c2bd801fdd2bfdc6dca9adf337c","externalIds":{"DBLP":"journals/corr/abs-2305-04001","ArXiv":"2305.04001","DOI":"10.48550/arXiv.2305.04001","CorpusId":258557566},"title":"AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion"},{"paperId":"43f8ab6000fc9e94b60e5fa23fa9a8ec9000b4c6","externalIds":{"DBLP":"journals/corr/abs-2305-02594","ArXiv":"2305.02594","DOI":"10.48550/arXiv.2305.02594","CorpusId":258479785},"title":"Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator"},{"paperId":"16ede3f3ef0afc113bf3282086b8ace0ffe7473c","externalIds":{"DBLP":"journals/corr/abs-2305-01319","ArXiv":"2305.01319","DOI":"10.48550/arXiv.2305.01319","CorpusId":258437212},"title":"Long-Term Rhythmic Video Soundtracker"},{"paperId":"d64755f140ad742495518714ebd457b4d95ce341","externalIds":{"ArXiv":"2304.14404","DBLP":"journals/corr/abs-2304-14404","DOI":"10.48550/arXiv.2304.14404","CorpusId":258352582},"title":"Motion-Conditioned Diffusion Model for Controllable Video Synthesis"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"f5a0c57f90c6abe31482e9f320ccac5ee789b135","externalIds":{"ArXiv":"2304.08818","DBLP":"journals/corr/abs-2304-08818","DOI":"10.1109/CVPR52729.2023.02161","CorpusId":258187553},"title":"Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"},{"paperId":"8ad199f11f386319ebd2706c372562677c98fae3","externalIds":{"DBLP":"journals/corr/abs-2304-08477","ArXiv":"2304.08477","DOI":"10.48550/arXiv.2304.08477","CorpusId":258180320},"title":"Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation"},{"paperId":"9742cb208cdcf2de9bd0ca80cf0ef5fe3d813d46","externalIds":{"ArXiv":"2304.08483","DBLP":"conf/iccv/0003YKWLL23","DOI":"10.1109/ICCV51070.2023.02079","CorpusId":258179463},"title":"Text2Performer: Text-Driven Human Video Generation"},{"paperId":"31a1b96392c65a70ab944e276170c81fdffcc73b","externalIds":{"DBLP":"journals/corr/abs-2304-08551","ArXiv":"2304.08551","DOI":"10.48550/arXiv.2304.08551","CorpusId":258187320},"title":"Generative Disco: Text-to-Video Generation for Music Visualization"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"c3939dcd0d61af653fbc2bdbc5c7ad0d8adda30d","externalIds":{"ArXiv":"2304.06818","DBLP":"journals/corr/abs-2304-06818","DOI":"10.48550/arXiv.2304.06818","CorpusId":258170508},"title":"Soundini: Sound-Guided Diffusion for Natural Video Editing"},{"paperId":"34e95464be6cc3041041f145758493401b8a75e8","externalIds":{"DBLP":"journals/corr/abs-2304-06025","ArXiv":"2304.06025","DOI":"10.1109/ICCV51070.2023.02073","CorpusId":258078892},"title":"DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"},{"paperId":"0de66912edc5d8b4bdcfb8ddc0f6ebbf795831d2","externalIds":{"DBLP":"conf/icip/TurDBR23","ArXiv":"2304.05841","DOI":"10.1109/ICIP49359.2023.10222594","CorpusId":258079336},"title":"Exploring Diffusion Models for Unsupervised Video Anomaly Detection"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"ee73edebd42626d9c2d91e35fd2ed3cdb0fb26d0","externalIds":{"DBLP":"conf/aaai/MaHCWC0C24","ArXiv":"2304.01186","DOI":"10.48550/arXiv.2304.01186","CorpusId":257912672},"title":"Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos"},{"paperId":"fe84ae01b14e5bb050ae3b928f2bdacee196ad67","externalIds":{"DBLP":"conf/iccv/LiuLDJS023","ArXiv":"2303.17959","DOI":"10.1109/ICCV51070.2023.00930","CorpusId":257901159},"title":"Diffusion Action Segmentation"},{"paperId":"782838a8699e10b80a0a359f2f7c448aef2ee429","externalIds":{"ArXiv":"2303.17599","DBLP":"journals/corr/abs-2303-17599","DOI":"10.48550/arXiv.2303.17599","CorpusId":257833877},"title":"Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models"},{"paperId":"2b50e72ffd2db2915dd1c6bddab710195cc64583","externalIds":{"DBLP":"conf/iccv/JiCXHLLLLL23","ArXiv":"2303.17559","DOI":"10.1109/ICCV51070.2023.01987","CorpusId":257833921},"title":"DDP: Diffusion Model for Dense Visual Prediction"},{"paperId":"46a97c83626132db81602becab3379c1cc4edf44","externalIds":{"DBLP":"conf/iclr/GuWYS024","ArXiv":"2303.14897","DOI":"10.48550/arXiv.2303.14897","CorpusId":257766959},"title":"Seer: Language Instructed Video Prediction with Latent Diffusion Models"},{"paperId":"e6b767442bc3e9442651016a7db121777614946d","externalIds":{"ArXiv":"2303.14863","DBLP":"conf/iccv/NagZDSX23","DOI":"10.1109/ICCV51070.2023.00951","CorpusId":257766686},"title":"DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion"},{"paperId":"c8f2aced926707fba8a0535a6df5b5823d394bac","externalIds":{"ArXiv":"2304.06632","DBLP":"journals/corr/abs-2304-06632","DOI":"10.48550/arXiv.2304.06632","CorpusId":258107993},"title":"AI-Generated Content (AIGC): A Survey"},{"paperId":"484d2194ce8459bfa9da906e556f63812c6ca999","externalIds":{"DBLP":"conf/cvpr/YuZJL0W23","ArXiv":"2303.14717","DOI":"10.1109/CVPR52729.2023.01422","CorpusId":257767123},"title":"CelebV-Text: A Large-Scale Facial Text-Video Dataset"},{"paperId":"ff6331ebc5b572ac5cb9f93ef0f06f7ee291bf58","externalIds":{"DBLP":"journals/corr/abs-2303-14676","ArXiv":"2303.14676","DOI":"10.1109/CVPR52729.2023.01425","CorpusId":257767335},"title":"PDPP: Projected Diffusion for Procedure Planning in Instructional Videos"},{"paperId":"b8b5015b153709176385873e34339f9e520d128f","externalIds":{"DBLP":"journals/corr/abs-2303-13744","ArXiv":"2303.13744","DOI":"10.1109/CVPR52729.2023.01769","CorpusId":257757116},"title":"Conditional Image-to-Video Generation with Latent Flow Diffusion Models"},{"paperId":"923a03032014a12c4e8b26511c0394e1b915fe74","externalIds":{"DBLP":"conf/iccv/KhachatryanMTHW23","ArXiv":"2303.13439","DOI":"10.1109/ICCV51070.2023.01462","CorpusId":257687280},"title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"},{"paperId":"0c7a877e7952785216767ad9fa3b850fe77cce52","externalIds":{"ACL":"2023.acl-long.73","DBLP":"conf/acl/YinWYWWNYLL0FGW23","ArXiv":"2303.12346","DOI":"10.48550/arXiv.2303.12346","CorpusId":257663639},"title":"NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation"},{"paperId":"32a3c2fbd3e733bd0eea938517fec2ff8dc7c701","externalIds":{"DBLP":"conf/iccv/CeylanHM23","ArXiv":"2303.12688","DOI":"10.1109/ICCV51070.2023.02121","CorpusId":257663916},"title":"Pix2Video: Video Editing using Image Diffusion"},{"paperId":"00d5ee5c590720b25cf61f0fcc1550fbaa87876e","externalIds":{"DBLP":"conf/miccai/ReynaudQDDRGLK23","ArXiv":"2303.12644","DOI":"10.1007/978-3-031-43999-5_14","CorpusId":257663580},"title":"Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis"},{"paperId":"1b492746ee3a304a13950cad1a59861b9ee44645","externalIds":{"ArXiv":"2303.11717","DBLP":"journals/corr/abs-2303-11717","DOI":"10.48550/arXiv.2303.11717","CorpusId":257636561},"title":"A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?"},{"paperId":"e2d5f5a49d68c80a9108646587eb2f7b898e2831","externalIds":{"ArXiv":"2303.09867","DBLP":"conf/iccv/0001LC0JL0023","DOI":"10.1109/ICCV51070.2023.00234","CorpusId":257622679},"title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model"},{"paperId":"14ccb8bcceb6de10eda6ad08bec242a4f2946497","externalIds":{"DBLP":"conf/iccv/QiCZLWSC23","ArXiv":"2303.09535","DOI":"10.1109/ICCV51070.2023.01460","CorpusId":257557738},"title":"FateZero: Fusing Attentions for Zero-shot Text-based Video Editing"},{"paperId":"3bc8bbf3f3909d83b96759af4831cf175a1a0e6e","externalIds":{"DBLP":"conf/aaai/Danier0B24","ArXiv":"2303.09508","DOI":"10.1609/aaai.v38i2.27912","CorpusId":257557164},"title":"LDMVFI: Video Frame Interpolation with Latent Diffusion Models"},{"paperId":"26c6090b7e7ba4513f82aa28d41360c60770c618","externalIds":{"ArXiv":"2303.08320","DBLP":"journals/corr/abs-2303-08320","DOI":"10.1109/CVPR52729.2023.00984","CorpusId":257532642},"title":"VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"e441f22769447578177b956e8f064dc426f91752","externalIds":{"DBLP":"journals/corr/abs-2303-07945","ArXiv":"2303.07945","DOI":"10.48550/arXiv.2303.07945","CorpusId":257504818},"title":"Edit-A-Video: Single Video Editing with Object-Aware Consistency"},{"paperId":"35ccd924de9e8483bdcf144cbf2edf09be157b7e","externalIds":{"ArXiv":"2303.07909","DBLP":"journals/corr/abs-2303-07909","DOI":"10.48550/arXiv.2303.07909","CorpusId":257505012},"title":"Text-to-image Diffusion Models in Generative AI: A Survey"},{"paperId":"6283502d6900a0b403e2454b1cb1cf16ddefd5a7","externalIds":{"DBLP":"conf/cvpr/LiuZ00J24","ArXiv":"2303.04761","DOI":"10.1109/CVPR52733.2024.00821","CorpusId":257405406},"title":"Video-P2P: Video Editing with Cross-Attention Control"},{"paperId":"31ae79a2273574b0bf9a2568a987c6fca02dc1b5","externalIds":{"DBLP":"journals/corr/abs-2302-13434","ArXiv":"2302.13434","DOI":"10.48550/arXiv.2302.13434","CorpusId":257220024},"title":"Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition"},{"paperId":"3c7582bf1a6682f6d33c47fa0c911e55823a3c44","externalIds":{"DBLP":"conf/cvpr/YuSKS23","ArXiv":"2302.07685","DOI":"10.1109/CVPR52729.2023.01770","CorpusId":256868405},"title":"Video Probabilistic Diffusion Models in Projected Latent Space"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"07be0ec1f45e21a1032616535d0290ee6bfe0f6b","externalIds":{"DBLP":"conf/iccv/EsserCAGG23","ArXiv":"2302.03011","DOI":"10.1109/ICCV51070.2023.00675","CorpusId":256615582},"title":"Structure and Content-Guided Video Synthesis with Diffusion Models"},{"paperId":"9758ddd6ffbaac75aa0447a9664e6989811a05e2","externalIds":{"DBLP":"journals/corr/abs-2302-01329","ArXiv":"2302.01329","DOI":"10.48550/arXiv.2302.01329","CorpusId":256503757},"title":"Dreamix: Video Diffusion Models are General Video Editors"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"304cbe454a0239401f3d88fde55045f99fe90549","externalIds":{"DBLP":"conf/cvpr/LeeJCQ023","ArXiv":"2301.13173","DOI":"10.1109/CVPR52729.2023.01376","CorpusId":256390381},"title":"Shape-Aware Text-Driven Layered Video Editing"},{"paperId":"4febb8643b05c698a94dc147f92be8899222419e","externalIds":{"DBLP":"journals/ivc/BigioiBSZJMC24","ArXiv":"2301.04474","DOI":"10.48550/arXiv.2301.04474","CorpusId":255595497},"title":"Speech Driven Video Editing via an Audio-Conditioned Diffusion Model"},{"paperId":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","externalIds":{"DBLP":"conf/iccv/WuGWLGSHSQS23","ArXiv":"2212.11565","DOI":"10.1109/ICCV51070.2023.00701","CorpusId":254974187},"title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"},{"paperId":"3d94322b049959cac15efd67af22207b73afa245","externalIds":{"DBLP":"journals/corr/abs-2212-09478","ArXiv":"2212.09478","DOI":"10.1109/CVPR52729.2023.00985","CorpusId":254854449},"title":"MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation"},{"paperId":"736973165f98105fec3729b7db414ae4d80fcbeb","externalIds":{"DBLP":"journals/corr/abs-2212-09748","ArXiv":"2212.09748","DOI":"10.1109/ICCV51070.2023.00387","CorpusId":254854389},"title":"Scalable Diffusion Models with Transformers"},{"paperId":"c5349e515efe3513682ba62da122be5756650af1","externalIds":{"ArXiv":"2212.02773","DBLP":"conf/icassp/GuCX24","DOI":"10.1109/ICASSP48485.2024.10447191","CorpusId":254275074},"title":"Diffusioninst: Diffusion Model for Instance Segmentation"},{"paperId":"8240048df28571e36e12aaab9f0ce249d4e7cd37","externalIds":{"ArXiv":"2212.02802","DBLP":"conf/cvpr/KimSKCKY23","DOI":"10.1109/CVPR52729.2023.00590","CorpusId":254275011},"title":"Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding"},{"paperId":"13c7b29a100f67d285eb3625c160d06882d4c092","externalIds":{"DBLP":"conf/aaai/MeiP23","ArXiv":"2212.00235","DOI":"10.48550/arXiv.2212.00235","CorpusId":254125713},"title":"VIDM: Video Implicit Diffusion Models"},{"paperId":"154f9eb2f97cae3a7752cbc4e0261eb4e75008d4","externalIds":{"ArXiv":"2211.15076","DBLP":"conf/aaai/ZhongLCJ0Y23","DOI":"10.48550/arXiv.2211.15076","CorpusId":254044696},"title":"Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning"},{"paperId":"f2db26387e696d12028d41663ed5c6e439794206","externalIds":{"ArXiv":"2211.12824","DBLP":"conf/cvpr/FuYZFSWB23","DOI":"10.1109/CVPR52729.2023.01029","CorpusId":253801972},"title":"Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation"},{"paperId":"672a8c7fe6d079c82758a45c7fdefa741bdc6ef5","externalIds":{"DBLP":"journals/corr/abs-2211-13222","ArXiv":"2211.13222","DOI":"10.1109/CVPR52729.2023.01804","CorpusId":253801978},"title":"SVFormer: Semi-supervised Video Transformer for Action Recognition"},{"paperId":"b000d6865db824af1563708fb7a545ddd65c6b3a","externalIds":{"DBLP":"conf/cvpr/TumanyanGBD23","ArXiv":"2211.12572","DOI":"10.1109/CVPR52729.2023.00191","CorpusId":253801961},"title":"Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation"},{"paperId":"c1ddae8106deaeb15120a6a0f04111a63f4e2a7c","externalIds":{"ArXiv":"2211.11743","DBLP":"journals/corr/abs-2211-11743","DOI":"10.48550/arXiv.2211.11743","CorpusId":253734983},"title":"SinFusion: Training Diffusion Models on a Single Image or Video"},{"paperId":"94b690162ead76af6a487d6e10998ea585c035d1","externalIds":{"DBLP":"journals/corr/abs-2211-11018","ArXiv":"2211.11018","DOI":"10.48550/arXiv.2211.11018","CorpusId":253735209},"title":"MagicVideo: Efficient Video Generation With Latent Diffusion Models"},{"paperId":"bdf4af8311637c681904e71cf50f96fd0026f578","externalIds":{"DBLP":"journals/corr/abs-2211-10440","ArXiv":"2211.10440","DOI":"10.1109/CVPR52729.2023.00037","CorpusId":253708074},"title":"Magic3D: High-Resolution Text-to-3D Content Creation"},{"paperId":"a2d2bbe4c542173662a444b33b76c66992697830","externalIds":{"DBLP":"conf/cvpr/BrooksHE23","ArXiv":"2211.09800","DOI":"10.1109/CVPR52729.2023.01764","CorpusId":253581213},"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions"},{"paperId":"81621c53c6aa421deb81bb1359138ded0fb4e258","externalIds":{"DBLP":"journals/corr/abs-2211-09788","ArXiv":"2211.09788","DOI":"10.1109/ICCV51070.2023.01816","CorpusId":253581633},"title":"DiffusionDet: Diffusion Model for Object Detection"},{"paperId":"72564e9d4af543bc94f844fbb0d030a953a4f87e","externalIds":{"ArXiv":"2211.08428","DBLP":"journals/corr/abs-2211-08428","DOI":"10.48550/arXiv.2211.08428","CorpusId":253553491},"title":"CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming"},{"paperId":"e24f4b28167b05fbf7d29000490fc0a4e4c109c7","externalIds":{"ArXiv":"2211.01324","DBLP":"journals/corr/abs-2211-01324","DOI":"10.48550/arXiv.2211.01324","CorpusId":253254800},"title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers"},{"paperId":"b287a2765e5bceb732de39dafdf70594dc9cd664","externalIds":{"DBLP":"journals/ftcgv/GanLLWLG22","ArXiv":"2210.09263","DOI":"10.48550/arXiv.2210.09263","CorpusId":252918286},"title":"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"},{"paperId":"8f48171bf05474449777d9bbf6766d480332e09f","externalIds":{"DBLP":"journals/corr/abs-2210-06366","ArXiv":"2210.06366","DOI":"10.1109/ICCV51070.2023.00090","CorpusId":252846168},"title":"A Generalist Framework for Panoptic Segmentation of Images and Videos"},{"paperId":"f170754f8ab3187514292c12b1cbb431c0a8a634","externalIds":{"ArXiv":"2210.09292","DBLP":"journals/corr/abs-2210-09292","DOI":"10.48550/arXiv.2210.09292","CorpusId":252918532},"title":"Efficient Diffusion Models for Vision: A Survey"},{"paperId":"625d57bd52c60cd79aa4add6c4420dc2ad3b808a","externalIds":{"DBLP":"journals/corr/abs-2210-03142","ArXiv":"2210.03142","DOI":"10.1109/CVPR52729.2023.01374","CorpusId":252762155},"title":"On Distillation of Guided Diffusion Models"},{"paperId":"aa509ec67f311cd09d109356f7fa37a40072aabb","externalIds":{"DBLP":"journals/corr/abs-2210-02399","ArXiv":"2210.02399","DOI":"10.48550/arXiv.2210.02399","CorpusId":252715594},"title":"Phenaki: Variable Length Video Generation From Open Domain Textual Description"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"1e33716e8820b867d5a8aaebab44c2d3135ea4ac","externalIds":{"DBLP":"conf/iclr/SingerPH00ZHYAG23","ArXiv":"2209.14792","CorpusId":252595919},"title":"Make-A-Video: Text-to-Video Generation without Text-Video Data"},{"paperId":"4c94d04afa4309ec2f06bdd0fe3781f91461b362","externalIds":{"DBLP":"conf/iclr/PooleJBM23","ArXiv":"2209.14988","DOI":"10.48550/arXiv.2209.14988","CorpusId":252596091},"title":"DreamFusion: Text-to-3D using 2D Diffusion"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","externalIds":{"DBLP":"journals/corr/abs-2209-00796","ArXiv":"2209.00796","DOI":"10.1145/3626235","CorpusId":252070859},"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744","externalIds":{"ArXiv":"2208.12242","DBLP":"conf/cvpr/RuizLJPRA23","DOI":"10.1109/CVPR52729.2023.02155","CorpusId":251800180},"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"paperId":"b64537bdf7a103aa01972ba06ea24a9c08f7cd74","externalIds":{"DBLP":"conf/iclr/ChenZH23","ArXiv":"2208.04202","DOI":"10.48550/arXiv.2208.04202","CorpusId":251402961},"title":"Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning"},{"paperId":"04e541391e8dce14d099d00fb2c21dbbd8afe87f","externalIds":{"DBLP":"journals/corr/abs-2208-01626","ArXiv":"2208.01626","DOI":"10.48550/arXiv.2208.01626","CorpusId":251252882},"title":"Prompt-to-Prompt Image Editing with Cross Attention Control"},{"paperId":"af9f365ed86614c800f082bd8eb14be76072ad16","externalIds":{"DBLP":"journals/corr/abs-2207-12598","ArXiv":"2207.12598","DOI":"10.48550/arXiv.2207.12598","CorpusId":249145348},"title":"Classifier-Free Diffusion Guidance"},{"paperId":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","externalIds":{"DBLP":"journals/tmlr/YuXKLBWVKYAHHPLZBW22","ArXiv":"2206.10789","DOI":"10.48550/arXiv.2206.10789","CorpusId":249926846},"title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"},{"paperId":"4175003efc9cbf2781c0fd8ef8e6bcb756316296","externalIds":{"ArXiv":"2206.07696","DBLP":"journals/corr/abs-2206-07696","DOI":"10.48550/arXiv.2206.07696","CorpusId":249674747},"title":"Diffusion Models for Video Prediction and Infilling"},{"paperId":"9438577954f3b7696813ef58b9e0005d216cd400","externalIds":{"DBLP":"journals/corr/abs-2206-03429","ArXiv":"2206.03429","DOI":"10.48550/arXiv.2206.03429","CorpusId":249431456},"title":"Generating Long Videos of Dynamic Scenes"},{"paperId":"c539f6ab5818bde96f61298856cb0c38f6268369","externalIds":{"DBLP":"conf/cvpr/Parmar0Z22","DOI":"10.1109/CVPR52688.2022.01112","CorpusId":249879327},"title":"On Aliased Resizing and Surprising Subtleties in GAN Evaluation"},{"paperId":"2f4c451922e227cbbd4f090b74298445bbd900d0","externalIds":{"DBLP":"journals/corr/abs-2206-00364","ArXiv":"2206.00364","DOI":"10.48550/arXiv.2206.00364","CorpusId":249240415},"title":"Elucidating the Design Space of Diffusion-Based Generative Models"},{"paperId":"707bd332d2c21dc5eb1f02a52d4a0506199aae76","externalIds":{"ArXiv":"2205.15868","DBLP":"journals/corr/abs-2205-15868","DOI":"10.48550/arXiv.2205.15868","CorpusId":249209614},"title":"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers"},{"paperId":"805748eec6be59ae0cd92a48400a902b3b7ed8e6","externalIds":{"DBLP":"conf/nips/HarveyNMWW22","ArXiv":"2205.11495","DOI":"10.48550/arXiv.2205.11495","CorpusId":248986725},"title":"Flexible Diffusion Modeling of Long Videos"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"3890d82362d07064687a4b5e9024fc4c92921998","externalIds":{"DBLP":"journals/corr/abs-2205-09853","ArXiv":"2205.09853","DOI":"10.48550/arXiv.2205.09853","CorpusId":248965384},"title":"MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation"},{"paperId":"75bb9eda70751c63fc54dbe63377c673b7dbdb15","externalIds":{"DBLP":"journals/corr/abs-2204-14217","ArXiv":"2204.14217","DOI":"10.48550/arXiv.2204.14217","CorpusId":248476190},"title":"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"9a714179010e6f636ec507f92fb0d5d6ab930539","externalIds":{"DBLP":"journals/corr/abs-2204-06607","ArXiv":"2204.06607","DOI":"10.48550/arXiv.2204.06607","CorpusId":248177832},"title":"Towards Metrical Reconstruction of Human Faces"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","externalIds":{"DBLP":"conf/nips/HoSGC0F22","ArXiv":"2204.03458","DOI":"10.48550/arXiv.2204.03458","CorpusId":248006185},"title":"Video Diffusion Models"},{"paperId":"21fd946d6841a20feb9f19c8188fcdad6e91f84d","externalIds":{"ArXiv":"2204.03638","DBLP":"journals/corr/abs-2204-03638","DOI":"10.48550/arXiv.2204.03638","CorpusId":248006334},"title":"Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer"},{"paperId":"c0e8812789e96f5a7aa3ad940dba1c237aec822d","externalIds":{"ArXiv":"2204.02491","DBLP":"journals/corr/abs-2204-02491","DOI":"10.1007/978-3-031-19784-0_41","CorpusId":247996703},"title":"Text2LIVE: Text-Driven Layered Image and Video Editing"},{"paperId":"aa1b722485106c84e52c5e35b2d4b2f8c7fb3135","externalIds":{"DBLP":"journals/corr/abs-2204-00679","ArXiv":"2204.00679","DOI":"10.48550/arXiv.2204.00679","CorpusId":247939759},"title":"Learning Audio-Video Modalities from Image Captions"},{"paperId":"1641774b55a471a23eb31b722ee05c2e032fec7a","externalIds":{"PubMedCentral":"10606505","DBLP":"journals/corr/abs-2203-09481","ArXiv":"2203.09481","DOI":"10.3390/e25101469","CorpusId":247519223,"PubMed":"37895590"},"title":"Diffusion Probabilistic Modeling for Video Generation"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"ef4741b12cb8f01bcc68708c8cffcdc4237383f7","externalIds":{"ArXiv":"2202.10571","DBLP":"conf/iclr/YuTMKK0S22","CorpusId":247025714},"title":"Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks"},{"paperId":"82ba96443173da0b8b3e870c5ab8f41109a67203","externalIds":{"DBLP":"journals/corr/abs-2202-00273","ArXiv":"2202.00273","DOI":"10.1145/3528233.3530738","CorpusId":246441861},"title":"StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets"},{"paperId":"3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7","externalIds":{"ArXiv":"2201.11793","DBLP":"journals/corr/abs-2201-11793","CorpusId":246411364},"title":"Denoising Diffusion Restoration Models"},{"paperId":"d97e0adbade91d76b10e8790205a71877a9be42b","externalIds":{"DBLP":"journals/corr/abs-2112-14683","ArXiv":"2112.14683","DOI":"10.1109/CVPR52688.2022.00361","CorpusId":245537141},"title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"7002ae048e4b8c9133a55428441e8066070995cb","externalIds":{"ArXiv":"2112.10741","DBLP":"journals/corr/abs-2112-10741","CorpusId":245335086},"title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"},{"paperId":"6296aa7cab06eaf058f7291040b320b5a83c0091","externalIds":{"DBLP":"journals/corr/abs-2203-00667","ArXiv":"2203.00667","DOI":"10.1109/ICCCNT56998.2023.10306417","CorpusId":1033682},"title":"Generative Adversarial Networks"},{"paperId":"414e554d281d529401c873cb9c97186365ec5dd8","externalIds":{"ArXiv":"2111.14822","DBLP":"journals/corr/abs-2111-14822","DOI":"10.1109/CVPR52688.2022.01043","CorpusId":244714856},"title":"Vector Quantized Diffusion Model for Text-to-Image Synthesis"},{"paperId":"88e8801e4daf404d3d40f1648ef29faeb8e6d58a","externalIds":{"ArXiv":"2111.14818","DBLP":"conf/cvpr/AvrahamiLF22","DOI":"10.1109/CVPR52688.2022.01767","CorpusId":244714366},"title":"Blended Diffusion for Text-driven Editing of Natural Images"},{"paperId":"35356feaaf1a739a7db2b76f32e3e5a71ec74eb5","externalIds":{"DBLP":"journals/corr/abs-2111-13606","ArXiv":"2111.13606","CorpusId":244709128},"title":"Conditional Image Generation with Score-Based Diffusion Models"},{"paperId":"97bee918b08c244eb2e54d41e8ea6da00a3e5dbf","externalIds":{"DBLP":"conf/eccv/WuLJYFJD22","ArXiv":"2111.12417","DOI":"10.1007/978-3-031-19787-1_41","CorpusId":244527261},"title":"N√úWA: Visual Synthesis Pre-training for Neural visUal World creAtion"},{"paperId":"80035bfa3f822364fbc62de6df2d5df13a0c47ff","externalIds":{"DBLP":"conf/eccv/Bond-TaylorH0BW22","ArXiv":"2111.12701","DOI":"10.1007/978-3-031-20050-2_11","CorpusId":244527507},"title":"Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes"},{"paperId":"e1a3e6856b6ac6af3600b5954392e5368603fd1b","externalIds":{"ArXiv":"2111.10337","DBLP":"conf/cvpr/XueHZS00FG22","DOI":"10.1109/CVPR52688.2022.00498","CorpusId":244462849},"title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"848eb8367785910c2fe31372605954ad8f9dfe6c","externalIds":{"DBLP":"conf/cvpr/GraumanWBCFGH0L22","ArXiv":"2110.07058","DOI":"10.1109/CVPR52688.2022.01842","CorpusId":238856888},"title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video"},{"paperId":"6bae20930eaa0d9d489317f3b3b1aaaf18205ef8","externalIds":{"DBLP":"journals/corr/abs-2109-13396","ArXiv":"2109.13396","DOI":"10.15607/rss.2022.xviii.063","CorpusId":237277709},"title":"Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets"},{"paperId":"cda3fbbac6734b603bee363b0938e9baa924aa78","externalIds":{"DBLP":"journals/corr/abs-2108-02938","ArXiv":"2108.02938","DOI":"10.1109/iccv48922.2021.01410","CorpusId":236950721},"title":"ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models"},{"paperId":"5e51ebd5507cf6810b3b5833885115ca91bca356","externalIds":{"ArXiv":"2107.08037","DBLP":"conf/nips/MoingPS21","CorpusId":236034465},"title":"CCVS: Context-aware Controllable Video Synthesis"},{"paperId":"c1ff08b59f00c44f34dfdde55cd53370733a2c19","externalIds":{"MAG":"3174807077","DBLP":"conf/nips/KarrasALHHLA21","ArXiv":"2106.12423","CorpusId":235606261},"title":"Alias-Free Generative Adversarial Networks"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"ad14e11bc97cc2fed0fd344e7cb7d7ce4205bfc6","externalIds":{"ArXiv":"2106.05931","DBLP":"conf/nips/VahdatKK21","CorpusId":235390993},"title":"Score-based Generative Modeling in Latent Space"},{"paperId":"90357a6dc817e2f7cec477a51156675fbf545cf1","externalIds":{"ArXiv":"2106.02636","DBLP":"conf/nips/ZellersLHYPCFC21","CorpusId":235352775},"title":"MERLOT: Multimodal Neural Script Knowledge Models"},{"paperId":"d0829986bd7d8904ee855117c974140de3be742c","externalIds":{"DBLP":"journals/corr/abs-2103-03319","DOI":"10.1109/CVPR46437.2021.01256","CorpusId":232135122},"title":"Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos"},{"paperId":"0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4","externalIds":{"DBLP":"journals/jmlr/HoSCFNS22","ArXiv":"2106.15282","CorpusId":235619773},"title":"Cascaded Diffusion Models for High Fidelity Image Generation"},{"paperId":"6a3ada4790748eaac6507f0fb9a2568b3d174c72","externalIds":{"ArXiv":"2105.02668","DBLP":"journals/corr/abs-2105-02668","DOI":"10.1109/ICCV48922.2021.00786","CorpusId":233864776},"title":"VideoLT: Large-scale Long-tailed Video Recognition"},{"paperId":"3618e503068e5f0e4f17ad1557a9bd6692daea79","externalIds":{"MAG":"3158432584","DBLP":"conf/iclr/TianRCO0MT21","ArXiv":"2104.15069","CorpusId":232275342},"title":"A Good Image Generator Is What You Need for High-Resolution Video Synthesis"},{"paperId":"c64025f83864ec9c40e2970a24314b6b84d4c753","externalIds":{"DBLP":"journals/corr/abs-2104-14806","ArXiv":"2104.14806","CorpusId":233476314},"title":"GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions"},{"paperId":"2d9ae4c167510ed78803735fc57ea67c3cc55a35","externalIds":{"DBLP":"journals/corr/abs-2104-10157","ArXiv":"2104.10157","CorpusId":233307257},"title":"VideoGPT: Video Generation using VQ-VAE and Transformers"},{"paperId":"bc7e6165b00f0c39d40ca2c7a4eb33fcc0e3200d","externalIds":{"DBLP":"journals/pami/SahariaHCSFN23","ArXiv":"2104.07636","DOI":"10.1109/TPAMI.2022.3204461","CorpusId":233241040,"PubMed":"36094974"},"title":"Image Super-Resolution via Iterative Refinement"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"b6382a7351c0c595f91472ac71d3b2d87b3c4844","externalIds":{"DBLP":"journals/corr/abs-2103-15691","ArXiv":"2103.15691","DOI":"10.1109/ICCV48922.2021.00676","CorpusId":232417054},"title":"ViViT: A Video Vision Transformer"},{"paperId":"c32fd8ea1b3f2df410410fb18d569dede102c53a","externalIds":{"DBLP":"journals/corr/abs-2103-01458","ArXiv":"2103.01458","DOI":"10.1109/CVPR46437.2021.00286","CorpusId":232092778},"title":"Diffusion Probabilistic Models for 3D Point Cloud Generation"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"de18baa4964804cf471d85a5a090498242d2e79f","externalIds":{"ArXiv":"2102.09672","DBLP":"conf/icml/NicholD21","CorpusId":231979499},"title":"Improved Denoising Diffusion Probabilistic Models"},{"paperId":"fa08b41ccdfc5d8771adfbc34c176fa237d4646c","externalIds":{"DBLP":"conf/icml/BertasiusWT21","ArXiv":"2102.05095","CorpusId":231861462},"title":"Is Space-Time Attention All You Need for Video Understanding?"},{"paperId":"9cf6f42806a35fd1d410dbc34d8e8df73a29d094","externalIds":{"ArXiv":"2101.09258","DBLP":"conf/nips/SongDME21","CorpusId":235352469},"title":"Maximum Likelihood Training of Score-Based Diffusion Models"},{"paperId":"47f7ec3d0a5e6e83b6768ece35206a94dc81919c","externalIds":{"ArXiv":"2012.09841","MAG":"3111551570","DBLP":"journals/corr/abs-2012-09841","DOI":"10.1109/CVPR46437.2021.01268","CorpusId":229297973},"title":"Taming Transformers for High-Resolution Image Synthesis"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"da55208bc9b56b5f394c242239d8cd0734bd5a87","externalIds":{"ArXiv":"2007.14937","DBLP":"journals/corr/abs-2007-14937","MAG":"3045687178","CorpusId":220845567},"title":"Learning Video Representations from Textual Web Supervision"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"1156e277fa7ec195b043161d3c5c97715da17658","externalIds":{"DBLP":"conf/nips/0011E20","ArXiv":"2006.09011","MAG":"3035384201","CorpusId":219708245},"title":"Improved Techniques for Training Score-Based Generative Models"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","externalIds":{"MAG":"3096609285","DBLP":"conf/eccv/CarionMSUKZ20","ArXiv":"2005.12872","DOI":"10.1007/978-3-030-58452-8_13","CorpusId":218889832},"title":"End-to-End Object Detection with Transformers"},{"paperId":"797389ca052efd160ed759d7ef7adf9c30a917d6","externalIds":{"DBLP":"journals/corr/abs-2003-00196","ArXiv":"2003.00196","MAG":"2970315999","CorpusId":202767986},"title":"First Order Motion Model for Image Animation"},{"paperId":"14fdc18d9c164e5b0d6d946b3238c04e81921358","externalIds":{"MAG":"3035574324","DBLP":"conf/cvpr/KarrasLAHLA20","ArXiv":"1912.04958","DOI":"10.1109/cvpr42600.2020.00813","CorpusId":209202273},"title":"Analyzing and Improving the Image Quality of StyleGAN"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"441555b5cd09703e55c03e70bd2c9f82c0ffcf9b","externalIds":{"MAG":"3014641072","DBLP":"journals/pami/00010CJDZ0MTW0X21","ArXiv":"1908.07919","DOI":"10.1109/TPAMI.2020.2983686","CorpusId":201124533,"PubMed":"32248092"},"title":"Deep High-Resolution Representation Learning for Visual Recognition"},{"paperId":"965359b3008ab50dd04e171551220ec0e7f83aba","externalIds":{"MAG":"2971034910","ArXiv":"1907.05600","DBLP":"conf/nips/SongE19","CorpusId":196470871},"title":"Generative Modeling by Estimating Gradients of the Data Distribution"},{"paperId":"7bd83b055702bc178aa26def5b6df463f8eab7b9","externalIds":{"MAG":"2955639361","DBLP":"journals/corr/abs-1907-01341","ArXiv":"1907.01341","DOI":"10.1109/TPAMI.2020.3019967","CorpusId":195776274,"PubMed":"32853149"},"title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer"},{"paperId":"9311779489e597315488749ee6c386bfa3f3512e","externalIds":{"DBLP":"journals/corr/abs-1906-03327","ArXiv":"1906.03327","MAG":"2948859046","DOI":"10.1109/ICCV.2019.00272","CorpusId":182952863},"title":"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"},{"paperId":"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796","externalIds":{"ArXiv":"1904.03493","DBLP":"conf/iccv/WangWCLWW19","MAG":"2925419377","DOI":"10.1109/ICCV.2019.00468","CorpusId":102352148},"title":"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"},{"paperId":"d4dbdeb772105a7ee780543483c6142743b20298","externalIds":{"MAG":"2965813579","DBLP":"conf/iclr/UnterthinerSKMM19","CorpusId":198489709},"title":"FVD: A new Metric for Video Generation"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","externalIds":{"ArXiv":"1901.02860","DBLP":"conf/acl/DaiYYCLS19","MAG":"2964110616","ACL":"P19-1285","DOI":"10.18653/v1/P19-1285","CorpusId":57759363},"title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"b59233aab8364186603967bc12d88af48cc0992d","externalIds":{"DBLP":"journals/corr/abs-1812-01717","ArXiv":"1812.01717","MAG":"2902437806","CorpusId":54458806},"title":"Towards Accurate Generative Models of Video: A New Metric & Challenges"},{"paperId":"0c5f6d07b2a355312ba50132bab30832d1a4d883","externalIds":{"MAG":"3031246127","ArXiv":"1811.09245","DBLP":"journals/ijcv/SaitoSKK20","DOI":"10.1007/s11263-020-01333-y","CorpusId":218978582},"title":"Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN"},{"paperId":"4bbfd46721c145852e443ae4aad35148b814bf91","externalIds":{"ArXiv":"1811.08383","DBLP":"conf/iccv/LinGH19","MAG":"2990152177","DOI":"10.1109/ICCV.2019.00718","CorpusId":85542740},"title":"TSM: Temporal Shift Module for Efficient Video Understanding"},{"paperId":"f56cb5dc32b5b280546998418fda7769d0858629","externalIds":{"ArXiv":"1811.00347","MAG":"2899274165","DBLP":"journals/corr/abs-1811-00347","CorpusId":53186236},"title":"How2: A Large-scale Dataset for Multimodal Language Understanding"},{"paperId":"c5b55f410365bb889c25a9f0354f2b86ec61c4f0","externalIds":{"MAG":"2963841322","ArXiv":"1808.06601","DBLP":"conf/nips/Wang0ZYTKC18","CorpusId":52049245},"title":"Video-to-Video Synthesis"},{"paperId":"62dccab9ab715f33761a5315746ed02e48eed2a0","externalIds":{"DBLP":"journals/corr/abs-1808-01340","MAG":"2887051120","ArXiv":"1808.01340","CorpusId":51927456},"title":"A Short Note about Kinetics-600"},{"paperId":"fc50c9392fd23b6c88915177c6ae904a498aacea","externalIds":{"MAG":"2949810620","ArXiv":"1804.02748","DBLP":"journals/corr/abs-1804-02748","CorpusId":4710439},"title":"Scaling Egocentric Vision: The EPIC-KITCHENS Dataset"},{"paperId":"96ed8ce9ef9fc475db9e02c79f984dc110409b62","externalIds":{"DBLP":"conf/cvpr/SultaniCS18","MAG":"2951723061","ArXiv":"1801.04264","DOI":"10.1109/CVPR.2018.00678","CorpusId":1610415},"title":"Real-World Anomaly Detection in Surveillance Videos"},{"paperId":"8a6acba7fb2aad1299fcf35701417e063d410ed4","externalIds":{"ArXiv":"1712.09867","MAG":"2777556841","DBLP":"conf/cvpr/LiuLLG18","DOI":"10.1109/CVPR.2018.00684","CorpusId":3865699},"title":"Future Frame Prediction for Anomaly Detection - A New Baseline"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","externalIds":{"MAG":"2963799213","DBLP":"conf/nips/OordVK17","ArXiv":"1711.00937","CorpusId":20282961},"title":"Neural Discrete Representation Learning"},{"paperId":"ebf0615fc4d98cf1dbe527c79146ce1e50dce9af","externalIds":{"MAG":"2767621168","DBLP":"conf/corl/DosovitskiyRCLK17","ArXiv":"1711.03938","CorpusId":5550767},"title":"CARLA: An Open Urban Driving Simulator"},{"paperId":"cf18287e79b1fd73cd333fc914bb24c00a537f4c","externalIds":{"DBLP":"journals/corr/abs-1710-05268","MAG":"2765349170","ArXiv":"1710.05268","CorpusId":28326841},"title":"Self-Supervised Visual Planning with Temporal Skip Connections"},{"paperId":"87a818723a2ada66a1193baf17b0383d9766781b","externalIds":{"MAG":"2950466956","ArXiv":"1709.07592","DBLP":"journals/corr/abs-1709-07592","DOI":"10.1109/CVPR.2018.00251","CorpusId":1504491},"title":"Learning to Generate Time-Lapse Videos Using Multi-stage Dynamic Generative Adversarial Networks"},{"paperId":"ee909ad489244016cf301bb7d7d8eeea423dbf35","externalIds":{"MAG":"2963017553","DBLP":"conf/iccv/HendricksWSSDR17","ArXiv":"1708.01641","DOI":"10.1109/ICCV.2017.618","CorpusId":1061352},"title":"Localizing Moments in Video with Natural Language"},{"paperId":"e76edb86f270c3a77ed9f5a1e1b305461f36f96f","externalIds":{"MAG":"2951910147","DBLP":"conf/cvpr/Tulyakov0YK18","ArXiv":"1707.04993","DOI":"10.1109/CVPR.2018.00165","CorpusId":4475365},"title":"MoCoGAN: Decomposing Motion and Content for Video Generation"},{"paperId":"b68811a9b5cafe4795a11c1048541750068b7ad0","externalIds":{"MAG":"2949901290","ArXiv":"1706.04261","DBLP":"conf/iccv/GoyalKMMWKHFYMH17","DOI":"10.1109/ICCV.2017.622","CorpusId":834612},"title":"The ‚ÄúSomething Something‚Äù Video Database for Learning and Evaluating Visual Common Sense"},{"paperId":"b61a3f8b80bbd44f24544dc915f52fd30bbdf485","externalIds":{"ArXiv":"1705.07750","MAG":"2619082050","DBLP":"conf/cvpr/CarreiraZ17","DOI":"10.1109/CVPR.2017.502","CorpusId":206596127},"title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"},{"paperId":"96dd1fc39a368d23291816d57763bc6eb4f7b8d6","externalIds":{"MAG":"2963916161","DBLP":"journals/corr/KrishnaHRLN17","ArXiv":"1705.00754","DOI":"10.1109/ICCV.2017.83","CorpusId":1026139},"title":"Dense-Captioning Events in Videos"},{"paperId":"49e8fec24cce8b73706bc5fcd2c3f681addb9982","externalIds":{"ArXiv":"1704.00675","MAG":"2916743882","DBLP":"journals/corr/Pont-TusetPCASG17","CorpusId":3619941},"title":"The 2017 DAVIS Challenge on Video Object Segmentation"},{"paperId":"e10a5e0baf2aa87d804795af071808a9377cc80a","externalIds":{"MAG":"2784025607","DBLP":"conf/aaai/ZhouXC18","ArXiv":"1703.09788","DOI":"10.1609/aaai.v32i1.12342","CorpusId":19713015},"title":"Towards Automatic Learning of Procedures From Web Instructional Videos"},{"paperId":"dfad8f616bd2a05c8cae5f61060f743f966ece85","externalIds":{"MAG":"2951856387","ArXiv":"1611.08050","DBLP":"conf/cvpr/CaoSWS17","DOI":"10.1109/CVPR.2017.143","CorpusId":16224674},"title":"Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields"},{"paperId":"b8e2e9f3ba008e28257195ec69a00e07f260131d","externalIds":{"DBLP":"conf/cvpr/XuMYR16","MAG":"2425121537","DOI":"10.1109/CVPR.2016.571","CorpusId":206594535},"title":"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"},{"paperId":"154c22ca5eef149aedc8a986fa684ca1fd14e7dc","externalIds":{"MAG":"3037858973","DBLP":"journals/ijcv/RohrbachTRTPLCS17","ArXiv":"1605.03705","DOI":"10.1007/s11263-016-0987-1","CorpusId":18217052},"title":"Movie Description"},{"paperId":"c8c494ee5488fe20e0aa01bddf3fc4632086d654","externalIds":{"DBLP":"journals/corr/CordtsORREBFRS16","MAG":"2953139137","ArXiv":"1604.01685","DOI":"10.1109/CVPR.2016.350","CorpusId":502946},"title":"The Cityscapes Dataset for Semantic Urban Scene Understanding"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","externalIds":{"MAG":"2183341477","DBLP":"conf/cvpr/SzegedyVISW16","ArXiv":"1512.00567","DOI":"10.1109/CVPR.2016.308","CorpusId":206593880},"title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"2dcef55a07f8607a819c21fe84131ea269cc2e3c","externalIds":{"MAG":"2129069237","DBLP":"journals/corr/Sohl-DicksteinW15","ArXiv":"1503.03585","CorpusId":14888175},"title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"},{"paperId":"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d","externalIds":{"MAG":"2116435618","DBLP":"journals/corr/SrivastavaMS15","ArXiv":"1502.04681","CorpusId":11699847},"title":"Unsupervised Learning of Video Representations using LSTMs"},{"paperId":"d25c65d261ea0e6a458be4c50c40ffe5bc508f77","externalIds":{"MAG":"1522734439","DBLP":"conf/iccv/TranBFTP15","DOI":"10.1109/ICCV.2015.510","CorpusId":1122604},"title":"Learning Spatiotemporal Features with 3D Convolutional Networks"},{"paperId":"185f078accb52be4faa13e4f470a9909cc6fe814","externalIds":{"MAG":"2099614498","DBLP":"conf/cvpr/KuehneAS14","DOI":"10.1109/CVPR.2014.105","CorpusId":9621856},"title":"The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"e018fa4a1c893f964f76cee8ff735573974f87cc","externalIds":{"DBLP":"conf/huc/SteinM13","MAG":"2109698606","DOI":"10.1145/2493432.2493482","CorpusId":2333743},"title":"Combining embedded accelerometers with computer vision for recognizing food preparation activities"},{"paperId":"da9e411fcf740569b6b356f330a1d0fc077c8d7c","externalIds":{"MAG":"24089286","ArXiv":"1212.0402","DBLP":"journals/corr/abs-1212-0402","CorpusId":7197134},"title":"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"},{"paperId":"225f78ae8a44723c136646044fd5c5d7f1d3d15a","externalIds":{"MAG":"2212660284","DBLP":"journals/jmlr/GrettonBRSS12","DOI":"10.5555/2503308.2188410","CorpusId":10742222},"title":"A Kernel Two-Sample Test"},{"paperId":"bc900a187b6f0115417f4b7dfd2cf44c62875bf8","externalIds":{"MAG":"2031688197","DBLP":"conf/cvpr/FathiRR11","DOI":"10.1109/CVPR.2011.5995444","CorpusId":1641563},"title":"Learning to recognize objects in egocentric activities"},{"paperId":"8670e2aaec203332b46abed2237e54fc3e422c11","externalIds":{"MAG":"1973207880","DBLP":"journals/spm/WangB09","DOI":"10.1109/MSP.2008.930649","CorpusId":2492436},"title":"Mean squared error: Love it or leave it? A new look at Signal Fidelity Measures"},{"paperId":"25ad81d2f575d045bd1083c95a02b9b4b68a912e","externalIds":{"DOI":"10.2469/faj.v65.n1.5","CorpusId":218510903},"title":"Models"},{"paperId":"eae2e0fa72e898c289365c0af16daf57a7a6cf40","externalIds":{"MAG":"2133665775","DBLP":"journals/tip/WangBSS04","DOI":"10.1109/TIP.2003.819861","CorpusId":207761262,"PubMed":"15376593"},"title":"Image quality assessment: from error visibility to structural similarity"},{"paperId":"bfb79cb28a3d18b6e7c00822a03b94bb98aa75d7","externalIds":{"DBLP":"journals/corr/abs-2404-13680","DOI":"10.48550/arXiv.2404.13680","CorpusId":273845102},"title":"PoseAnimate: Zero-shot high fidelity pose controllable character animation"},{"paperId":"5a2515ede40b07b8b2c6f83387e8317e41b6d165","externalIds":{"DBLP":"journals/corr/abs-2403-10242","DOI":"10.48550/arXiv.2403.10242","CorpusId":275470087},"title":"FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model"},{"paperId":"d0a7f7fe31e0e0c42b471b4c47a313bd8c8e5206","externalIds":{"DBLP":"journals/corr/abs-2308-13812","DOI":"10.48550/arXiv.2308.13812","CorpusId":261243084},"title":"Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models"},{"paperId":"48a7c60b023ebd2cf0587df1cc09f1309fe51d28","externalIds":{"DBLP":"journals/corr/abs-2305-13840","DOI":"10.48550/arXiv.2305.13840","CorpusId":258841645},"title":"Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models"},{"paperId":"e1d5c8ee59031f19ea9979d05f6e92295a540f88","externalIds":{"DBLP":"journals/corr/abs-2304-11603","DOI":"10.48550/arXiv.2304.11603","CorpusId":281497013},"title":"LaMD: Latent Motion Diffusion for Video Generation"},{"paperId":"90428f3a8caa5082f825ebf3138514ddf273dae3","externalIds":{"CorpusId":253581838},"title":"Supplementary Materials for: NULL-text Inversion for Editing Real Images using Guided Diffusion Models"},{"paperId":"d804cac45bd338ea68e7e39c91c378ddb0518042","externalIds":{"DBLP":"journals/corr/abs-2308-15109","DOI":"10.48550/arXiv.2308.15109","CorpusId":261277027},"title":"DiffusionVMR: Diffusion Model for Video Moment Retrieval"},{"paperId":"e43cb872ce595a85e5b2afe9fc3cf219c2f888ff","externalIds":{"DBLP":"journals/corr/abs-2305-17098","DOI":"10.48550/arXiv.2305.17098","CorpusId":258947175},"title":"ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing"},{"paperId":"a479c6014352def4c15cffde1c1851c5841460d7","externalIds":{"DBLP":"journals/corr/abs-2305-19094","DOI":"10.48550/arXiv.2305.19094","CorpusId":269626649},"title":"DiffMatch: Diffusion Model for Dense Matching"},{"paperId":"eba926e6ddf9a83a50e6ff030f40eccf0fd43b44","externalIds":{"DBLP":"journals/corr/abs-2308-00122","DOI":"10.48550/arXiv.2308.00122","CorpusId":278732049},"title":"DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models"},{"paperId":"735fcf085059f419112b76f7217e7f1407efcbb0","externalIds":{"DBLP":"journals/corr/abs-2211-13221","DOI":"10.48550/arXiv.2211.13221","CorpusId":253802030},"title":"Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths"},{"paperId":"0af6e4b23091b863258176d5765759926a4c85de","externalIds":{"DBLP":"journals/corr/abs-2108-01073","CorpusId":236772794},"title":"SDEdit: Image Synthesis and Editing with Stochastic Differential Equations"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","externalIds":{"ArXiv":"2103.14030","DBLP":"conf/iccv/LiuL00W0LG21","DOI":"10.1109/ICCV48922.2021.00986","CorpusId":232352874},"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"3e3c3e97b79df7dd62f0fc478291ea48c2e004a8","externalIds":{"DOI":"10.1016/b978-0-08-102409-6.00009-2","CorpusId":239265652},"title":"YouTube"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"3dd906bc0947e56d2b7bf9530b11351bbdff2358","externalIds":{"CorpusId":14049355},"title":"Computer Vision and Image Understanding"}]}