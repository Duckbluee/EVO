{"references":[{"paperId":"1033f669e64e179068924720ba4aac6ff5a13fac","externalIds":{"DBLP":"journals/corr/abs-2405-20413","ArXiv":"2405.20413","DOI":"10.48550/arXiv.2405.20413","CorpusId":270199863},"title":"Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters"},{"paperId":"991177a764b5a1d01d4466867238cb97104729fc","externalIds":{"ArXiv":"2405.20773","DBLP":"journals/corr/abs-2405-20773","DOI":"10.48550/arXiv.2405.20773","CorpusId":270199716},"title":"Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Characte"},{"paperId":"4ad33969188555b8303b375e18f5c117a68387c6","externalIds":{"DBLP":"journals/corr/abs-2404-07921","ArXiv":"2404.07921","DOI":"10.48550/arXiv.2404.07921","CorpusId":269043107},"title":"AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs"},{"paperId":"f019c9661b253ddb611e930348e20ddcd350a952","externalIds":{"ArXiv":"2404.03027","CorpusId":268889385},"title":"JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks"},{"paperId":"b773069d893a72cee5d62e8c51ffecbeb0f0b07f","externalIds":{"ArXiv":"2404.00629","DBLP":"journals/corr/abs-2404-00629","DOI":"10.48550/arXiv.2404.00629","CorpusId":268820098},"title":"Against The Achilles' Heel: A Survey on Red Teaming for Generative Models"},{"paperId":"1732a6e7670764ca9b43c871647e619819258880","externalIds":{"ArXiv":"2403.17155","DBLP":"conf/naacl/LyuLZPLJC24","DOI":"10.48550/arXiv.2403.17155","CorpusId":268691877},"title":"Task-Agnostic Detector for Insertion-Based Backdoor Attacks"},{"paperId":"33c8910107f3fcb17d140cc88554652508ae3674","externalIds":{"ArXiv":"2403.14472","DBLP":"conf/acl/Wang0XXDYZY0C24","DOI":"10.48550/arXiv.2403.14472","CorpusId":268553537},"title":"Detoxifying Large Language Models via Knowledge Editing"},{"paperId":"55eed6f9ede6c4187d849224d61e60fda73a54df","externalIds":{"ArXiv":"2403.12171","DBLP":"journals/corr/abs-2403-12171","DOI":"10.48550/arXiv.2403.12171","CorpusId":268531982},"title":"EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models"},{"paperId":"8a25739903cab07c74556b8c2d9743749e1be1e5","externalIds":{"DBLP":"conf/eccv/WangLLCX24","ArXiv":"2403.09513","DOI":"10.48550/arXiv.2403.09513","CorpusId":268385048},"title":"AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting"},{"paperId":"6a6d8794f77dcdabebbf80b8f79956f707f16eb9","externalIds":{"DBLP":"conf/eccv/GouCLHXLYKZ24","ArXiv":"2403.09572","DOI":"10.48550/arXiv.2403.09572","CorpusId":268384950},"title":"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation"},{"paperId":"3bb87d605856411c6f002d480fc29d355c3ba245","externalIds":{"DBLP":"journals/corr/abs-2403-09766","ArXiv":"2403.09766","DOI":"10.48550/arXiv.2403.09766","CorpusId":268510103},"title":"An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models"},{"paperId":"425d3c381dde8f576b6237a6d443b97880dd6bc2","externalIds":{"ACL":"2024.emnlp-main.908","DBLP":"conf/emnlp/XiaoY0C24","ArXiv":"2403.08424","DOI":"10.48550/arXiv.2403.08424","CorpusId":268379559},"title":"Distract Large Language Models for Automatic Jailbreak Attack"},{"paperId":"8ba57771dd6345821a0cbe83c4c7eb50f66b7b65","externalIds":{"ArXiv":"2403.04783","DBLP":"journals/corr/abs-2403-04783","DOI":"10.48550/arXiv.2403.04783","CorpusId":268297202},"title":"AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks"},{"paperId":"60f653b14da8cc6866e5062a9700bb5f85a387ca","externalIds":{"ArXiv":"2403.00867","DBLP":"conf/nips/HuCH24","DOI":"10.48550/arXiv.2403.00867","CorpusId":268230819},"title":"Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes"},{"paperId":"f6fa682b62c7981402336ca57da1196ccbf3fc54","externalIds":{"DBLP":"conf/uss/LiuZZDM024","ArXiv":"2402.18104","DOI":"10.48550/arXiv.2402.18104","CorpusId":268041845},"title":"Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction"},{"paperId":"72f51c3ef967f7905e3194296cf6fd8337b1a437","externalIds":{"DBLP":"journals/corr/abs-2402-16717","ArXiv":"2402.16717","DOI":"10.48550/arXiv.2402.16717","CorpusId":268032340},"title":"CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"},{"paperId":"09e2391ab4c266f96c71cc379c1a7c7ddd40ee33","externalIds":{"DBLP":"conf/emnlp/LiWCZH24","ArXiv":"2402.16914","DOI":"10.48550/arXiv.2402.16914","CorpusId":268031868},"title":"DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers"},{"paperId":"1f9f25aad947030fe3206114fa2ac75e8b590515","externalIds":{"ArXiv":"2402.16192","DBLP":"journals/corr/abs-2402-16192","DOI":"10.48550/arXiv.2402.16192","CorpusId":267938320},"title":"Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing"},{"paperId":"b23cf88fd5a99ae68618a3b64d3cd3b3c0c6a65f","externalIds":{"DBLP":"journals/corr/abs-2402-15911","ArXiv":"2402.15911","DOI":"10.48550/arXiv.2402.15911","CorpusId":267938152},"title":"PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails"},{"paperId":"d0746668f85fcdbe00306bd7486a24f8750207d5","externalIds":{"DBLP":"journals/corr/abs-2402-15302","ArXiv":"2402.15302","DOI":"10.48550/arXiv.2402.15302","CorpusId":267897715},"title":"How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries"},{"paperId":"d82a827fa21cce0b601ad21b6db606b8413ed547","externalIds":{"DBLP":"journals/corr/abs-2402-15180","ArXiv":"2402.15180","DOI":"10.48550/arXiv.2402.15180","CorpusId":267897711},"title":"Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement"},{"paperId":"4ebfb0589ba587d6912661c1fe1082db705476af","externalIds":{"DBLP":"journals/corr/abs-2402-14872","ArXiv":"2402.14872","DOI":"10.48550/arXiv.2402.14872","CorpusId":267897371},"title":"Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs"},{"paperId":"4699f20628ebb318005b3cce9005c78847ca6daf","externalIds":{"ArXiv":"2402.14020","DBLP":"journals/corr/abs-2402-14020","DOI":"10.48550/arXiv.2402.14020","CorpusId":267770475},"title":"Coercing LLMs to do and reveal (almost) anything"},{"paperId":"0a691e58a36cdcdaaf72294e88420f79e61e85c7","externalIds":{"DBLP":"journals/corr/abs-2402-11753","ArXiv":"2402.11753","DOI":"10.48550/arXiv.2402.11753","CorpusId":267750708},"title":"ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"},{"paperId":"09f78ccd20f865c288b421376baddb0096c651be","externalIds":{"ArXiv":"2402.11755","DBLP":"journals/corr/abs-2402-11755","DOI":"10.48550/arXiv.2402.11755","CorpusId":267750421},"title":"SPML: A DSL for Defending Language Models Against Prompt Attacks"},{"paperId":"fbceb7ffd77ca45eaba297f3f421f65df4feb5cf","externalIds":{"DBLP":"journals/corr/abs-2402-12329","ArXiv":"2402.12329","DOI":"10.48550/arXiv.2402.12329","CorpusId":267751131},"title":"Query-Based Adversarial Prompt Generation"},{"paperId":"9f25324c89686495afb697cdb79f98d79092b843","externalIds":{"DBLP":"journals/corr/abs-2402-10753","ArXiv":"2402.10753","DOI":"10.48550/arXiv.2402.10753","CorpusId":267740273},"title":"ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages"},{"paperId":"17c64281e2b9947c7f362b213162c5f1331d0002","externalIds":{"ArXiv":"2402.10601","CorpusId":267740378},"title":"When\"Competency\"in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers"},{"paperId":"ef5da08aad746173b7ddf589068c6abf00205fea","externalIds":{"ArXiv":"2402.09674","DBLP":"journals/corr/abs-2402-09674","DOI":"10.48550/arXiv.2402.09674","CorpusId":267682038},"title":"PAL: Proxy-Guided Black-Box Attack on Large Language Models"},{"paperId":"f2e88c26bc1ebdd4adc5f83ab56cb4276120745d","externalIds":{"ArXiv":"2402.10260","DBLP":"conf/nips/SoulyLBTHPASEWT24","DOI":"10.48550/arXiv.2402.10260","CorpusId":267740669},"title":"A StrongREJECT for Empty Jailbreaks"},{"paperId":"76132c4494ef9d4792b7aeb7a868ffe45ca850a8","externalIds":{"DBLP":"journals/corr/abs-2402-10196","ArXiv":"2402.10196","DOI":"10.48550/arXiv.2402.10196","CorpusId":267682286},"title":"A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents"},{"paperId":"2139e414bdf6a5ea7ec4052d3f65a8d49991494b","externalIds":{"DBLP":"journals/corr/abs-2402-08983","ArXiv":"2402.08983","DOI":"10.48550/arXiv.2402.08983","CorpusId":267658033},"title":"SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"},{"paperId":"f6d0d7c8e815de607e03da73467ba538caad4d92","externalIds":{"ArXiv":"2402.06255","DBLP":"conf/nips/MoWW024","DOI":"10.52202/079017-2049","CorpusId":267617138},"title":"Fight Back Against Jailbreaking via Prompt Adversarial Tuning"},{"paperId":"c7830808c6342d673ebc318a179195018d3b5d11","externalIds":{"ArXiv":"2402.03299","DBLP":"journals/corr/abs-2402-03299","DOI":"10.48550/arXiv.2402.03299","CorpusId":267411743},"title":"GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models"},{"paperId":"24253eb1c425de4c4e4c9946ed1b6eb5bdc8fba9","externalIds":{"DBLP":"journals/corr/abs-2402-02309","ArXiv":"2402.02309","DOI":"10.48550/arXiv.2402.02309","CorpusId":267413270},"title":"Jailbreaking Attack against Multimodal Large Language Model"},{"paperId":"2d83b615f989c8d1e9860a8a2d82628c95e40d22","externalIds":{"ArXiv":"2402.02207","DBLP":"journals/corr/abs-2402-02207","DOI":"10.48550/arXiv.2402.02207","CorpusId":267413047},"title":"Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models"},{"paperId":"dd35405d8e562fa1df4338839878e9c94817cfdd","externalIds":{"DBLP":"journals/corr/abs-2401-17263","ArXiv":"2401.17263","DOI":"10.48550/arXiv.2401.17263","CorpusId":267320750},"title":"Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks"},{"paperId":"3cd81b0123b5f8477f6b5777681030ef6b05dd46","externalIds":{"ArXiv":"2401.13136","DBLP":"conf/acl/ShenTCCZXZKK24","DOI":"10.48550/arXiv.2401.13136","CorpusId":267200158},"title":"The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts"},{"paperId":"f9863e1cb5ab60b0ad2892b0d003ffb2308ff187","externalIds":{"DBLP":"journals/corr/abs-2401-10862","ACL":"2024.blackboxnlp-1.26","ArXiv":"2401.10862","DOI":"10.48550/arXiv.2401.10862","CorpusId":267060803},"title":"Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning"},{"paperId":"8fd29e810540c40846cddce3cbdf5060cd59fb57","externalIds":{"DBLP":"conf/coling/Zhang0ZT25","ArXiv":"2401.06561","CorpusId":266977251},"title":"Intention Analysis Makes LLMs A Good Jailbreak Defender"},{"paperId":"8d28d2ef602e8b518b7daecc39a0f2f8d2caaa09","externalIds":{"ACL":"2024.emnlp-main.895","DBLP":"journals/corr/abs-2401-02906","ArXiv":"2401.02906","DOI":"10.48550/arXiv.2401.02906","CorpusId":266818099},"title":"MLLM-Protector: Ensuring MLLM’s Safety without Hurting Performance"},{"paperId":"e424fb0d5a33900f26bcc607a34ed262b6900ffa","externalIds":{"DOI":"10.61969/jai.1311271","CorpusId":266113031},"title":"Google Bard Generated Literature Review: Metaverse"},{"paperId":"accb2fab67c76d5668908107cd50cbb81110c389","externalIds":{"ArXiv":"2401.00287","DBLP":"journals/corr/abs-2401-00287","DOI":"10.48550/arXiv.2401.00287","CorpusId":266693612},"title":"The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness"},{"paperId":"6a6f6070e765f77422c6d6c14c1e6b41a9ed562a","externalIds":{"ArXiv":"2312.17673","DBLP":"journals/corr/abs-2312-17673","DOI":"10.48550/arXiv.2312.17673","CorpusId":266690784},"title":"Jatmo: Prompt Injection Defense by Task-Specific Finetuning"},{"paperId":"952b2e2aadeec56ee4cffbe23f9f70bcfcb14b53","externalIds":{"DBLP":"journals/corr/abs-2312-10982","ArXiv":"2312.10982","DOI":"10.48550/arXiv.2312.10982","CorpusId":266359311},"title":"A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models"},{"paperId":"f75e29c7035fdb0329b33ce0bd2fac7af59d95b8","externalIds":{"DBLP":"journals/corr/abs-2312-04403","ArXiv":"2312.04403","DOI":"10.48550/arXiv.2312.04403","CorpusId":265703434},"title":"OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization"},{"paperId":"e762f92273cd96f63b7788c0173b9b6450adedd7","externalIds":{"DBLP":"journals/natmi/XieYSCLCXW23","DOI":"10.1038/s42256-023-00765-8","CorpusId":266289038},"title":"Defending ChatGPT against jailbreak attack via self-reminders"},{"paperId":"1a5a79b393b3f00eb5a47243ee031ad799d2f641","externalIds":{"DBLP":"conf/eccv/LiuZGLYQ24","ArXiv":"2311.17600","DOI":"10.1007/978-3-031-72992-8_22","CorpusId":265498692},"title":"MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"},{"paperId":"73f082fc7df9f2b9f3bf7dafb7c4422bb7aae968","externalIds":{"ArXiv":"2311.16101","DBLP":"journals/corr/abs-2311-16101","DOI":"10.48550/arXiv.2311.16101","CorpusId":265456945},"title":"How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs"},{"paperId":"9045649163477319dafba4403dc915c3388dceda","externalIds":{"ArXiv":"2311.14876","DBLP":"conf/bigdataconf/SinghAN23","DOI":"10.1109/BigData59044.2023.10386814","CorpusId":265457196},"title":"Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles"},{"paperId":"391eaeb1092c2b145ff0e5a2fa61637a42921fce","externalIds":{"DBLP":"conf/cvpr/ChenSCJD24","ArXiv":"2311.10081","DOI":"10.1109/CVPR52733.2024.01350","CorpusId":265221232},"title":"DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"},{"paperId":"ecdfc556828dc59d8a0009016fcbe97a06cf7e23","externalIds":{"ArXiv":"2312.00029","DBLP":"journals/corr/abs-2312-00029","DOI":"10.48550/arXiv.2312.00029","CorpusId":265552022},"title":"Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework"},{"paperId":"e92e8ff1becb9a9e4a7dd09878eaacb2a62ffb6b","externalIds":{"ArXiv":"2311.09096","DBLP":"conf/acl/ZhangYKMWH24","DOI":"10.48550/arXiv.2311.09096","CorpusId":265212812},"title":"Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"},{"paperId":"05d2ced6a4fb7efb8d527a228ad792526a202235","externalIds":{"ACL":"2024.naacl-long.152","ArXiv":"2311.09447","DBLP":"conf/naacl/MoWC024","DOI":"10.48550/arXiv.2311.09447","CorpusId":265220739},"title":"How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities"},{"paperId":"c4ff1be5c254b60b96b7455eefcc4ec9583f82ed","externalIds":{"ACL":"2024.naacl-long.118","ArXiv":"2311.08268","DBLP":"conf/naacl/DingKMCXCH24","DOI":"10.48550/arXiv.2311.08268","CorpusId":265664913},"title":"A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"},{"paperId":"709af143f78bc62413c50ea1a7ee75b0702c4f59","externalIds":{"ACL":"2024.naacl-long.107","ArXiv":"2311.07689","DBLP":"journals/corr/abs-2311-07689","DOI":"10.48550/arXiv.2311.07689","CorpusId":265157927},"title":"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming"},{"paperId":"71c117015757a064d1d2e94a7065d0e784a41aa6","externalIds":{"ArXiv":"2311.06237","PubMedCentral":"11734899","DBLP":"journals/corr/abs-2311-06237","DOI":"10.1371/journal.pone.0314658","CorpusId":265128905,"PubMed":"39813184"},"title":"Summon a demon and bind it: A grounded theory of LLM red teaming"},{"paperId":"b78b5ce5f21f46d8149824463f8eebd6103d49aa","externalIds":{"ArXiv":"2311.05608","DBLP":"journals/corr/abs-2311-05608","DOI":"10.48550/arXiv.2311.05608","CorpusId":265067328},"title":"FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"},{"paperId":"ccae9fcb1f344e56a3f7cb05a4b49a6e658f9dd2","externalIds":{"DBLP":"journals/corr/abs-2311-05553","ArXiv":"2311.05553","ACL":"2024.naacl-short.59","DOI":"10.48550/arXiv.2311.05553","CorpusId":265067269},"title":"Removing RLHF Protections in GPT-4 via Fine-Tuning"},{"paperId":"99eee47dad469790042b973bc7cb40cb63a660b1","externalIds":{"DBLP":"journals/corr/abs-2311-03191","ArXiv":"2311.03191","DOI":"10.48550/arXiv.2311.03191","CorpusId":265033222},"title":"DeepInception: Hypnotize Large Language Model to Be Jailbreaker"},{"paperId":"bfc0e3e651cd4b715272fe68add8a180a112293c","externalIds":{"ArXiv":"2311.03348","DBLP":"journals/corr/abs-2311-03348","DOI":"10.48550/arXiv.2311.03348","CorpusId":265043220},"title":"Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation"},{"paperId":"e25b4bdfc3f5a8293ea6cd687a0203e446594188","externalIds":{"DBLP":"conf/iclr/ToyerWMSBWOEADR24","ArXiv":"2311.01011","DOI":"10.48550/arXiv.2311.01011","CorpusId":264935143},"title":"Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game"},{"paperId":"d1b5151231a790c7a60f620e21860593dae9a1c5","externalIds":{"DBLP":"journals/corr/abs-2310-20624","ArXiv":"2310.20624","DOI":"10.48550/arXiv.2310.20624","CorpusId":264808400},"title":"LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"},{"paperId":"33f6b9aa455a8563252b242cb575705e782958f0","externalIds":{"DBLP":"journals/corr/abs-2310-15469","ArXiv":"2310.15469","DOI":"10.1145/3658644.3690325","CorpusId":264439566},"title":"The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks"},{"paperId":"f3de6ea08e2464190673c0ec8f78e5ec1cd08642","externalIds":{"ArXiv":"2311.16119","DBLP":"journals/corr/abs-2311-16119","DOI":"10.48550/arXiv.2311.16119","CorpusId":265466048},"title":"Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition"},{"paperId":"e314d182fd9d35a05870b38a56ee38eb3149b47d","externalIds":{"ArXiv":"2310.12505","DBLP":"conf/emnlp/DengWFDW023","DOI":"10.48550/arXiv.2310.12505","CorpusId":264306378},"title":"Attack Prompt Generation for Red Teaming and Defending Large Language Models"},{"paperId":"4f63c5a89c7299a864c6c48aa1844fb0fe8c9437","externalIds":{"ArXiv":"2310.10844","DBLP":"journals/corr/abs-2310-10844","DOI":"10.48550/arXiv.2310.10844","CorpusId":264172191},"title":"Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"},{"paperId":"4637f79ddfaf923ce569996ffa5b6cda1996faa1","externalIds":{"DBLP":"conf/satml/ChaoRDHP025","ArXiv":"2310.08419","DOI":"10.1109/SaTML64287.2025.00010","CorpusId":263908890},"title":"Jailbreaking Black Box Large Language Models in Twenty Queries"},{"paperId":"ac27dd71af3ee93e1129482ceececbae7dd0d0e8","externalIds":{"DBLP":"journals/corr/abs-2310-06987","ArXiv":"2310.06987","DOI":"10.48550/arXiv.2310.06987","CorpusId":263835408},"title":"Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"},{"paperId":"6b135e922a0c673aeb0b05c5aeecdb6c794791c6","externalIds":{"DBLP":"journals/corr/abs-2310-06387","ArXiv":"2310.06387","DOI":"10.48550/arXiv.2310.06387","CorpusId":263830179,"PubMed":"41628045"},"title":"Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations"},{"paperId":"1a9f394b5b7f5bcdecee487174a3f4fc65d30e33","externalIds":{"ArXiv":"2310.06474","DBLP":"conf/iclr/0010ZPB24","DOI":"10.48550/arXiv.2310.06474","CorpusId":263831094},"title":"Multilingual Jailbreak Challenges in Large Language Models"},{"paperId":"8dd9605fbc9702f08a295cba5ae263f625781856","externalIds":{"DBLP":"journals/corr/abs-2310-04655","ArXiv":"2310.04655","DOI":"10.48550/arXiv.2310.04655","CorpusId":263829712},"title":"VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models"},{"paperId":"8cf9b49698fdb1b754df2556576412a7b44929f6","externalIds":{"DBLP":"journals/tmlr/Robey0HP25","ArXiv":"2310.03684","DOI":"10.48550/arXiv.2310.03684","CorpusId":263671542},"title":"SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"},{"paperId":"0e0e706e13f160e74cac9556f28ab9a358c148d2","externalIds":{"DBLP":"journals/corr/abs-2310-03693","ArXiv":"2310.03693","DOI":"10.48550/arXiv.2310.03693","CorpusId":263671523},"title":"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"},{"paperId":"f3f23f7f9f5369aade19f20bc5d028cce7b9c9aa","externalIds":{"ArXiv":"2310.04451","DBLP":"journals/corr/abs-2310-04451","DOI":"10.48550/arXiv.2310.04451","CorpusId":263831566},"title":"AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"},{"paperId":"764fc56883bf83392cac99a7b5a264ac9fe2cdc5","externalIds":{"ArXiv":"2310.02446","DBLP":"journals/corr/abs-2310-02446","DOI":"10.48550/arXiv.2310.02446","CorpusId":263620377},"title":"Low-Resource Languages Jailbreak GPT-4"},{"paperId":"72e05fac8cf01593f70c63e16385a7bf6fd0fe09","externalIds":{"DBLP":"journals/corr/abs-2310-00905","ArXiv":"2310.00905","DOI":"10.48550/arXiv.2310.00905","CorpusId":263605466},"title":"All Languages Matter: On the Multilingual Safety of Large Language Models"},{"paperId":"46eea7d651420e60f9b1393e3f5eda14cbff7a2a","externalIds":{"DBLP":"conf/iclr/PatilHB24","ArXiv":"2309.17410","DOI":"10.48550/arXiv.2309.17410","CorpusId":263311025},"title":"Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks"},{"paperId":"66bc83b4b321cac7084cc51b3c6178d9de54e9c1","externalIds":{"ArXiv":"2309.11830","CorpusId":262084103},"title":"Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"},{"paperId":"9f4c17aebbb181756fab86ade02deadd90d5d4f9","externalIds":{"ArXiv":"2309.11751","DBLP":"journals/corr/abs-2309-11751","DOI":"10.48550/arXiv.2309.11751","CorpusId":262083772},"title":"How Robust is Google's Bard to Adversarial Image Attacks?"},{"paperId":"d4177489596748e43aa571f59556097f2cc4c8be","externalIds":{"ArXiv":"2309.10253","DBLP":"journals/corr/abs-2309-10253","DOI":"10.48550/arXiv.2309.10253","CorpusId":262055242},"title":"GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts"},{"paperId":"cd29c25c489562b409a60f83365f93f33ee1a0a1","externalIds":{"DBLP":"journals/corr/abs-2309-14348","ArXiv":"2309.14348","DOI":"10.48550/arXiv.2309.14348","CorpusId":262827619},"title":"Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"},{"paperId":"b574245f3db22b5eb7fe64bd8b0a147dab467b60","externalIds":{"DBLP":"conf/iclr/LiWZ0024","ArXiv":"2309.07124","DOI":"10.48550/arXiv.2309.07124","CorpusId":261705563},"title":"RAIN: Your Language Models Can Align Themselves without Finetuning"},{"paperId":"3c784cd3150a359e269c70cfbadd18774d66055d","externalIds":{"DBLP":"journals/corr/abs-2309-05274","ArXiv":"2309.05274","DOI":"10.1109/ICASSP48485.2024.10448041","CorpusId":261681918},"title":"FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models"},{"paperId":"1ab91d6ac7afc1a0121487a9089fa70edc1634d4","externalIds":{"DBLP":"journals/corr/abs-2309-02705","ArXiv":"2309.02705","DOI":"10.48550/arXiv.2309.02705","CorpusId":261557007},"title":"Certifying LLM Safety against Adversarial Prompting"},{"paperId":"f846c0c59608f0a8ff18f4c52adba87bf49dc229","externalIds":{"ArXiv":"2309.01446","DBLP":"journals/corr/abs-2309-01446","DOI":"10.48550/arXiv.2309.01446","CorpusId":261530019},"title":"Open Sesame! Universal Black Box Jailbreaking of Large Language Models"},{"paperId":"5bdaadb84db0cbf72aaebda9f55f4288b63c6e9b","externalIds":{"DBLP":"journals/corr/abs-2309-00236","ArXiv":"2309.00236","DOI":"10.48550/arXiv.2309.00236","CorpusId":261494235},"title":"Image Hijacks: Adversarial Images can Control Generative Models at Runtime"},{"paperId":"3e30a7ac4886b28eb50151f58e14a1d698cccd0e","externalIds":{"ArXiv":"2309.00614","DBLP":"journals/corr/abs-2309-00614","DOI":"10.48550/arXiv.2309.00614","CorpusId":261494182},"title":"Baseline Defenses for Adversarial Attacks Against Aligned Language Models"},{"paperId":"b8b8d5655df1c6a71bbb713387863e34cc055332","externalIds":{"DBLP":"journals/corr/abs-2308-14132","ArXiv":"2308.14132","DOI":"10.48550/arXiv.2308.14132","CorpusId":261245172},"title":"Detecting Language Model Attacks with Perplexity"},{"paperId":"5690e35b8beab92a80055fe2530c29c24e495379","externalIds":{"ArXiv":"2308.10741","DBLP":"conf/iccvw/Schlarmann023","DOI":"10.1109/ICCVW60793.2023.00395","CorpusId":261048835},"title":"On the Adversarial Robustness of Multi-Modal Foundation Models"},{"paperId":"9f859726b3d8dffd96a1f55de4122617751cc1b4","externalIds":{"ArXiv":"2308.09662","DBLP":"journals/corr/abs-2308-09662","DOI":"10.48550/arXiv.2308.09662","CorpusId":261030829},"title":"Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment"},{"paperId":"2cdd5c3dc42c0df40bc8839709869af3560d4bfe","externalIds":{"DBLP":"journals/corr/abs-2308-07308","ArXiv":"2308.07308","DOI":"10.48550/arXiv.2308.07308","CorpusId":260887487},"title":"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked"},{"paperId":"ea1710f4d34950560663210781d11b87225c5d55","externalIds":{"DBLP":"journals/corr/abs-2308-07026","ArXiv":"2308.07026","DOI":"10.1145/3581783.3612454","CorpusId":260887071},"title":"AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning"},{"paperId":"1104d766527dead44a40532e8a89444d9cef5c65","externalIds":{"DBLP":"journals/corr/abs-2308-03825","ArXiv":"2308.03825","DOI":"10.1145/3658644.3670388","CorpusId":260704242},"title":"\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"},{"paperId":"0583d13ae65be640d64f100c87452bbf3ebc86ae","externalIds":{"DBLP":"journals/corr/abs-2307-16851","ArXiv":"2307.16851","DOI":"10.48550/arXiv.2307.16851","CorpusId":260333892},"title":"Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives"},{"paperId":"47030369e97cc44d4b2e3cf1be85da0fd134904a","externalIds":{"DBLP":"journals/corr/abs-2307-15043","ArXiv":"2307.15043","CorpusId":260202961},"title":"Universal and Transferable Adversarial Attacks on Aligned Language Models"},{"paperId":"92b9d8b8c81c4c53ea62000c0924500b2dd11bce","externalIds":{"ArXiv":"2307.14539","DBLP":"conf/iclr/Shayegani0A24","CorpusId":260203143},"title":"Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models"},{"paperId":"132679d2fff8f8aed41bf8388a39b7f0aa30d4fd","externalIds":{"DBLP":"conf/iccv/LuWWGGZ23","ArXiv":"2307.14061","DOI":"10.1109/ICCV51070.2023.00016","CorpusId":260164714},"title":"Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models"},{"paperId":"139a0c7a60667979dcb57eae677f75ff3f0b0196","externalIds":{"ArXiv":"2307.10719","DBLP":"journals/corr/abs-2307-10719","DOI":"10.48550/arXiv.2307.10719","CorpusId":259991450},"title":"LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?"},{"paperId":"929305892d4ddae575a0fc23227a8139f7681632","externalIds":{"DBLP":"journals/corr/abs-2307-02483","ArXiv":"2307.02483","DOI":"10.48550/arXiv.2307.02483","CorpusId":259342528},"title":"Jailbroken: How Does LLM Safety Training Fail?"},{"paperId":"317ad53bea6fb603c20f692bb2f1a01e2dc86161","externalIds":{"ArXiv":"2307.00691","DBLP":"journals/access/GuptaAAPP23","DOI":"10.1109/ACCESS.2023.3300381","CorpusId":259316122},"title":"From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy"},{"paperId":"8724579d3f126e753a0451d98ff57b165f722e72","externalIds":{"ArXiv":"2306.15447","DBLP":"journals/corr/abs-2306-15447","DOI":"10.48550/arXiv.2306.15447","CorpusId":259262181},"title":"Are aligned neural networks adversarially aligned?"},{"paperId":"142e934dd5d6c53f877c30243d436255e3a0dde7","externalIds":{"ArXiv":"2306.13213","DBLP":"conf/aaai/QiHP0WM24","DOI":"10.1609/aaai.v38i19.30150","CorpusId":259244034},"title":"Visual Adversarial Examples Jailbreak Aligned Large Language Models"},{"paperId":"7d22ad3573101337bca2091fb0114b377c4f3db6","externalIds":{"DBLP":"journals/corr/abs-2306-11695","ArXiv":"2306.11695","DOI":"10.48550/arXiv.2306.11695","CorpusId":259203115},"title":"A Simple and Effective Pruning Approach for Large Language Models"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","externalIds":{"ArXiv":"2305.16934","DBLP":"conf/nips/ZhaoPDYLCL23","DOI":"10.48550/arXiv.2305.16934","CorpusId":258947177},"title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"1abfc211793c683972ded8d3268475e3ee7a88b0","externalIds":{"DBLP":"journals/corr/abs-2305-14950","ArXiv":"2305.14950","DOI":"10.48550/arXiv.2305.14950","CorpusId":258865399},"title":"Adversarial Demonstration Attacks on Large Language Models"},{"paperId":"025ca4c125d6ecabc816a56f160e5c992abc76d9","externalIds":{"DBLP":"journals/corr/abs-2304-05197","ArXiv":"2304.05197","DOI":"10.48550/arXiv.2304.05197","CorpusId":258060250},"title":"Multi-step Jailbreaking Privacy Attacks on ChatGPT"},{"paperId":"94b94c545cbb90e2d1ab316758a910c53ed95b66","externalIds":{"ArXiv":"2303.09105","DBLP":"conf/iclr/ChenZDY0024","DOI":"10.48550/arXiv.2303.09105","CorpusId":257557727},"title":"Rethinking Model Ensemble in Transfer-based Adversarial Attacks"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"2f94f03fdac62d05f0f416b7b3855d1f597afee9","externalIds":{"DBLP":"journals/corr/abs-2303-04381","ArXiv":"2303.04381","DOI":"10.48550/arXiv.2303.04381","CorpusId":257405439},"title":"Automatically Auditing Large Language Models via Discrete Optimization"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"0cf694b8f85ab2e11d45595de211a15cfbadcd22","externalIds":{"DBLP":"journals/corr/abs-2302-05733","ArXiv":"2302.05733","DOI":"10.1109/SPW63631.2024.00018","CorpusId":256827239},"title":"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"},{"paperId":"8a778cd7d136a57b20c0864e643d4afe2bc83ffc","externalIds":{"ArXiv":"2206.09391","DBLP":"journals/corr/abs-2206-09391","DOI":"10.1145/3503161.3547801","CorpusId":249888984},"title":"Towards Adversarial Attack on Vision-Language Pre-training Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"a6fdb277d0a4b09899f802bda3359f5c2021a156","externalIds":{"ArXiv":"2109.10862","DBLP":"journals/corr/abs-2109-10862","CorpusId":237593001},"title":"Recursively Summarizing Books with Human Feedback"},{"paperId":"ad3e83c5e5471e9fd388e36adb7950e7586b9b5f","externalIds":{"MAG":"3183199078","ArXiv":"2107.01809","DBLP":"journals/corr/abs-2107-01809","DOI":"10.1007/978-3-031-19772-7_42","CorpusId":235731720},"title":"Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks"},{"paperId":"209f9bde2dee7cf1677801586562ffe56d435d38","externalIds":{"DBLP":"conf/naacl/QinE21","ACL":"2021.naacl-main.410","ArXiv":"2104.06599","MAG":"3166846774","DOI":"10.18653/v1/2021.naacl-main.410","CorpusId":233231453},"title":"Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","externalIds":{"DBLP":"conf/chi/ReynoldsM21","ArXiv":"2102.07350","DOI":"10.1145/3411763.3451760","CorpusId":231925131},"title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","externalIds":{"ArXiv":"2012.15723","DBLP":"conf/acl/GaoFC20","ACL":"2021.acl-long.295","DOI":"10.18653/v1/2021.acl-long.295","CorpusId":229923710},"title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"df7d26339adf4eb0c07160947b9d2973c24911ba","externalIds":{"DBLP":"journals/corr/abs-2012-07805","MAG":"3112689365","ArXiv":"2012.07805","CorpusId":229156229},"title":"Extracting Training Data from Large Language Models"},{"paperId":"4517d2d12fc4d655bc42ea5c374fd178e369bc74","externalIds":{"DBLP":"journals/corr/abs-2009-12046","ArXiv":"2009.12046","MAG":"3103264768","ACL":"2020.findings-emnlp.339","DOI":"10.18653/v1/2020.findings-emnlp.339","CorpusId":221949069},"title":"Controllable Text Generation with Focused Variation"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","externalIds":{"MAG":"3082115681","DBLP":"journals/corr/abs-2009-01325","ArXiv":"2009.01325","CorpusId":221665105},"title":"Learning to summarize from human feedback"},{"paperId":"faa48bc9d554505bb542c03a99bbf52ca2b599e3","externalIds":{"DBLP":"journals/corr/abs-2007-08745","ArXiv":"2007.08745","MAG":"3042368254","DOI":"10.1109/TNNLS.2022.3182979","CorpusId":220633116,"PubMed":"35731760"},"title":"Backdoor Learning: A Survey"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"8ae9a17c87a4518b513e860683a0ef7824be994d","externalIds":{"MAG":"3002104146","ArXiv":"2001.07676","ACL":"2021.eacl-main.20","DBLP":"journals/corr/abs-2001-07676","DOI":"10.18653/v1/2021.eacl-main.20","CorpusId":210838924},"title":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","externalIds":{"MAG":"3035207248","ArXiv":"1910.13267","ACL":"2020.acl-main.170","DBLP":"journals/corr/abs-1910-13267","DOI":"10.18653/v1/2020.acl-main.170","CorpusId":204949631},"title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a15950dc71079285a4eaf195de5aadd87c41b40","externalIds":{"MAG":"2973379954","DBLP":"journals/corr/abs-1909-08593","ArXiv":"1909.08593","CorpusId":202660943},"title":"Fine-Tuning Language Models from Human Preferences"},{"paperId":"3caf34532597683c980134579b156cd0d7db2f40","externalIds":{"ArXiv":"1908.07125","DBLP":"conf/emnlp/WallaceFKGS19","MAG":"2970290563","ACL":"D19-1221","DOI":"10.18653/v1/D19-1221","CorpusId":201698258},"title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"bd4336b6015d4d680a27c25a0ed296df5692ddf1","externalIds":{"DBLP":"journals/access/GuLDG19","MAG":"2942091739","DOI":"10.1109/ACCESS.2019.2909068","CorpusId":131777414},"title":"BadNets: Evaluating Backdooring Attacks on Deep Neural Networks"},{"paperId":"b0b96270a9bbeb9f3ec040e70114d565fbcaaed9","externalIds":{"DBLP":"journals/corr/abs-1901-05415","ArXiv":"1901.05415","MAG":"2951998460","ACL":"P19-1358","DOI":"10.18653/v1/P19-1358","CorpusId":58007087},"title":"Learning from Dialogue after Deployment: Feed Yourself, Chatbot!"},{"paperId":"c6f913e4baa7f2c85363c0625c87003ad3b3a14c","externalIds":{"MAG":"2901707424","ArXiv":"1811.07871","DBLP":"journals/corr/abs-1811-07871","CorpusId":53745764},"title":"Scalable agent alignment via reward modeling: a research direction"},{"paperId":"5a5a1d666e4b7b933bc5aafbbadf179bc447ee67","externalIds":{"ArXiv":"1805.00899","MAG":"2798877128","DBLP":"journals/corr/abs-1805-00899","CorpusId":22050710},"title":"AI safety via debate"},{"paperId":"f78a911f516625d6b7b76a9a33c1eb14613341c4","externalIds":{"MAG":"2962847335","ArXiv":"1803.06978","DBLP":"conf/cvpr/XieZZBWRY19","DOI":"10.1109/CVPR.2019.00284","CorpusId":3972825},"title":"Improving Transferability of Adversarial Examples With Input Diversity"},{"paperId":"8e37a3b227b68953f8067215828dc8b8714cb21b","externalIds":{"DBLP":"conf/cvpr/DongLPS0HL18","MAG":"2774644650","DOI":"10.1109/CVPR.2018.00957","CorpusId":4119221},"title":"Boosting Adversarial Attacks with Momentum"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"99e5a8c10cf92749d4a7c2949691c3a6046e499a","externalIds":{"ArXiv":"1611.02770","DBLP":"conf/iclr/LiuCLS17","MAG":"2570685808","CorpusId":17707860},"title":"Delving into Transferable Adversarial Examples and Black-box Attacks"},{"paperId":"78aa018ee7d52360e15d103390ea1cdb3a0beb41","externalIds":{"DBLP":"journals/corr/PapernotMG16","ArXiv":"1605.07277","MAG":"2408141691","CorpusId":17362994},"title":"Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples"},{"paperId":"36e2ad6c962418ee4e4a5c65111225ced2d9f89b","externalIds":{"DBLP":"journals/corr/abs-2401-18018","DOI":"10.48550/arXiv.2401.18018","CorpusId":269976272},"title":"Prompt-Driven LLM Safeguarding via Directed Representation Optimization"},{"paperId":"8832f073c4d5d7ae26d6b5252b00bf8d8531f2e6","externalIds":{"DBLP":"journals/corr/abs-2402-13457","DOI":"10.48550/arXiv.2402.13457","CorpusId":271746334},"title":"LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study"},{"paperId":"c2cfa4d147d3f1e1fcf9584a7d4947321b4160c6","externalIds":{"DBLP":"journals/corr/abs-2402-14968","DOI":"10.48550/arXiv.2402.14968","CorpusId":271908478},"title":"Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment"},{"paperId":"0015e5dd084475781302040504585a229df7a5db","externalIds":{"DBLP":"journals/corr/abs-2402-16006","DOI":"10.48550/arXiv.2402.16006","CorpusId":276435916},"title":"From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings"},{"paperId":"5f2f6f395b500010cec482776d2b885efc44599c","externalIds":{"DBLP":"journals/corr/abs-2403-07865","DOI":"10.48550/arXiv.2403.07865","CorpusId":273994374},"title":"Exploring Safety Generalization Challenges of Large Language Models via Code"},{"paperId":"6c6154e73f2a5d7aeb09ba7b9fa8742ab64d1e1b","externalIds":{"DBLP":"journals/corr/abs-2305-14965","DOI":"10.48550/arXiv.2305.14965","CorpusId":258865314},"title":"Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks"},{"paperId":"259d07f47c046389d0b4f256139de64736dd9a94","externalIds":{"DBLP":"journals/corr/abs-2307-08715","DOI":"10.48550/arXiv.2307.08715","CorpusId":270441137},"title":"Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots"},{"paperId":"6b17231df7cc4c30fa5e1ea7eb9df0d4875caeac","externalIds":{"DBLP":"journals/corr/abs-2312-10766","DOI":"10.48550/arXiv.2312.10766","CorpusId":271325522},"title":"A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"f33c1381e390ae27e3b625405b869cae1720e24a","externalIds":{"MAG":"2070030996","DOI":"10.1016/j.cellsig.2010.07.004","CorpusId":43821258,"PubMed":"20633638"},"title":"Jailbreak: oncogene-induced senescence and its evasion."}]}