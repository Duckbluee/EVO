{"references":[{"paperId":"00e18c603e60d861c4e99c541e4d65ef442d5945","externalIds":{"DBLP":"conf/acl/AlizadehMBKCMRF24","ArXiv":"2312.11514","DOI":"10.48550/arXiv.2312.11514","CorpusId":266362016},"title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory"},{"paperId":"5851121df5ce46be5faea265c868ec0beabfce96","externalIds":{"DBLP":"journals/corr/abs-2312-03863","ArXiv":"2312.03863","DOI":"10.48550/arXiv.2312.03863","CorpusId":266044196},"title":"Efficient Large Language Models: A Survey"},{"paperId":"7bbc7595196a0606a07506c4fb1473e5e87f6082","externalIds":{"ArXiv":"2312.00752","DBLP":"journals/corr/abs-2312-00752","CorpusId":265551773},"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"},{"paperId":"4ea5ca620122e6a9a2b000444d36491cebf49c7c","externalIds":{"DBLP":"journals/corr/abs-2311-12351","ArXiv":"2311.12351","DOI":"10.48550/arXiv.2311.12351","CorpusId":265308945},"title":"Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey"},{"paperId":"4d76206515d6b33903937474273885476fc2771e","externalIds":{"DBLP":"journals/corr/abs-2311-01282","ArXiv":"2311.01282","DOI":"10.48550/arXiv.2311.01282","CorpusId":264935058},"title":"FlashDecoding++: Faster Large Language Model Inference on GPUs"},{"paperId":"6d8896632ca2af8310273f6774e1dfb3140770f7","externalIds":{"ArXiv":"2310.19240","DBLP":"journals/corr/abs-2310-19240","DOI":"10.48550/arXiv.2310.19240","CorpusId":264820296},"title":"M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models"},{"paperId":"a54761081c2b001c057fb6e1ea9a48058d5aa5e0","externalIds":{"ArXiv":"2310.16450","DBLP":"conf/iclr/Chen0MLB24","DOI":"10.48550/arXiv.2310.16450","CorpusId":264451707},"title":"CLEX: Continuous Length Extrapolation for Large Language Models"},{"paperId":"908dad62c0e43d80e3e3cb3c0402f7c71c70499c","externalIds":{"ArXiv":"2310.08560","DBLP":"journals/corr/abs-2310-08560","DOI":"10.48550/arXiv.2310.08560","CorpusId":263909014},"title":"MemGPT: Towards LLMs as Operating Systems"},{"paperId":"4c0428917aeee6aa7bd434f337d039f35996b736","externalIds":{"DBLP":"journals/corr/abs-2310-06839","ArXiv":"2310.06839","DOI":"10.48550/arXiv.2310.06839","CorpusId":263830692},"title":"LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"},{"paperId":"abdb0f9d1486dbb024c4bc9f8f9dc40464c58715","externalIds":{"DBLP":"journals/corr/abs-2310-06694","ArXiv":"2310.06694","DOI":"10.48550/arXiv.2310.06694","CorpusId":263830786},"title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"},{"paperId":"2392b6d3a5cad9e5cf349169eaeee848266adf6a","externalIds":{"ArXiv":"2310.05736","DBLP":"journals/corr/abs-2310-05736","DOI":"10.48550/arXiv.2310.05736","CorpusId":263830701},"title":"LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"},{"paperId":"b12541867632737e826b7b01c7fbe1c4222d8655","externalIds":{"ArXiv":"2310.06201","DBLP":"journals/corr/abs-2310-06201","DOI":"10.48550/arXiv.2310.06201","CorpusId":263830231},"title":"Compressing Context to Enhance Inference Efficiency of Large Language Models"},{"paperId":"02ad9f3fefe33cb9ca546591bec65dbdf7766c80","externalIds":{"ArXiv":"2310.01889","DBLP":"conf/iclr/0055ZA24","DOI":"10.48550/arXiv.2310.01889","CorpusId":263608461},"title":"Ring Attention with Blockwise Transformers for Near-Infinite Context"},{"paperId":"fdc53c2c10742464087c0525f77e32604827a21d","externalIds":{"DBLP":"conf/iclr/XiaoTCHL24","ArXiv":"2309.17453","DOI":"10.48550/arXiv.2309.17453","CorpusId":263310483},"title":"Efficient Streaming Language Models with Attention Sinks"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"819bbdc2dac9e13d9ca3e2508a6e063186ce5e40","externalIds":{"DBLP":"conf/iclr/PengQFS24","ArXiv":"2309.00071","DOI":"10.48550/arXiv.2309.00071","CorpusId":261493986},"title":"YaRN: Efficient Context Window Extension of Large Language Models"},{"paperId":"338d8f3b199abcebc85f34016b0162ab3a9d5310","externalIds":{"DBLP":"journals/corr/abs-2308-07633","ArXiv":"2308.07633","DOI":"10.1162/tacl_a_00704","CorpusId":260900101},"title":"A Survey on Model Compression for Large Language Models"},{"paperId":"0456cd227edb95e596e3915ebcfd1133bcc8d725","externalIds":{"ArXiv":"2307.15411","DBLP":"journals/corr/abs-2307-15411","DOI":"10.48550/arXiv.2307.15411","CorpusId":260315960},"title":"Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning"},{"paperId":"240103933ffe3dac2179cc160a2bd91299357a53","externalIds":{"DBLP":"journals/corr/abs-2307-08621","ArXiv":"2307.08621","CorpusId":259937453},"title":"Retentive Network: A Successor to Transformer for Large Language Models"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","externalIds":{"DBLP":"journals/tist/NaveedKQSAUABM25","ArXiv":"2307.06435","DOI":"10.1145/3744746","CorpusId":259847443},"title":"A Comprehensive Overview of Large Language Models"},{"paperId":"c12db2c60e8989f646a29ad4f4d24475e860ad91","externalIds":{"ArXiv":"2307.02486","DBLP":"journals/corr/abs-2307-02486","CorpusId":259341682},"title":"LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"},{"paperId":"f5afaccfe90268485a9961c5771ec5e71e9b806c","externalIds":{"ArXiv":"2306.15595","DBLP":"journals/corr/abs-2306-15595","DOI":"10.48550/arXiv.2306.15595","CorpusId":259262376},"title":"Extending Context Window of Large Language Models via Positional Interpolation"},{"paperId":"7d22ad3573101337bca2091fb0114b377c4f3db6","externalIds":{"DBLP":"journals/corr/abs-2306-11695","ArXiv":"2306.11695","DOI":"10.48550/arXiv.2306.11695","CorpusId":259203115},"title":"A Simple and Effective Pruning Approach for Large Language Models"},{"paperId":"aa44b28b7c4c4a56d1f59ab4669215b667822c25","externalIds":{"ArXiv":"2306.02272","DBLP":"conf/aaai/LeeJKKP24","DOI":"10.1609/aaai.v38i12.29237","CorpusId":267095435},"title":"OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models"},{"paperId":"6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2","externalIds":{"DBLP":"conf/acl/LiuO0CSMSKC24","ArXiv":"2305.17888","DOI":"10.48550/arXiv.2305.17888","CorpusId":258959117},"title":"LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"},{"paperId":"026b3396a63ed5772329708b7580d633bb86bec9","externalIds":{"DBLP":"conf/emnlp/PengAAAABCCCDDG23","ArXiv":"2305.13048","DOI":"10.18653/v1/2023.findings-emnlp.936","CorpusId":258832459},"title":"RWKV: Reinventing RNNs for the Transformer Era"},{"paperId":"5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200","externalIds":{"DBLP":"journals/corr/abs-2305-13245","ArXiv":"2305.13245","DOI":"10.48550/arXiv.2305.13245","CorpusId":258833177},"title":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"},{"paperId":"017010b941d902a467f6d329ae5e74fd67e67912","externalIds":{"DBLP":"journals/corr/abs-2305-11627","ArXiv":"2305.11627","DOI":"10.48550/arXiv.2305.11627","CorpusId":258823276},"title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"paperId":"b6d6c33298b852cf63edac233deca70530d69a2a","externalIds":{"ArXiv":"2305.10403","DBLP":"journals/corr/abs-2305-10403","CorpusId":258740735},"title":"PaLM 2 Technical Report"},{"paperId":"3e4085e5869f1b7959707a1e1d7d273b6057eb4e","externalIds":{"DBLP":"journals/tmlr/LiAZMKMMALCLZZW23","ArXiv":"2305.06161","CorpusId":258588247},"title":"StarCoder: may the source be with you!"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"f393aff1593c2d370ec0ae004910d18e40524967","externalIds":{"ArXiv":"2303.06349","DBLP":"journals/corr/abs-2303-06349","CorpusId":257496654},"title":"Resurrecting Recurrent Neural Networks for Long Sequences"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"998ac3e945857cf2676ee7efdbaf443a0c6f820a","externalIds":{"DBLP":"journals/corr/abs-2302-10866","ArXiv":"2302.10866","DOI":"10.48550/arXiv.2302.10866","CorpusId":257050308},"title":"Hyena Hierarchy: Towards Larger Convolutional Language Models"},{"paperId":"52045d4d4ae305aebb9e92fbbcf23104242c4d31","externalIds":{"DBLP":"journals/corr/abs-2302-06461","ArXiv":"2302.06461","DOI":"10.48550/arXiv.2302.06461","CorpusId":256827573},"title":"A Study on ReLU and Softmax in Transformer"},{"paperId":"fe5a72e0a4aeb5ea5058d9e4531858be5548dfe0","externalIds":{"ArXiv":"2302.01107","DBLP":"journals/corr/abs-2302-01107","DOI":"10.48550/arXiv.2302.01107","CorpusId":256503897},"title":"A Survey on Efficient Training of Transformers"},{"paperId":"909ad57ce8caa6b390a65ae09db352d27d8f3996","externalIds":{"DBLP":"journals/corr/abs-2301-00774","ArXiv":"2301.00774","DOI":"10.48550/arXiv.2301.00774","CorpusId":255372747},"title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"},{"paperId":"980e55d9226cac302d0fae7732da4e67b8bc952c","externalIds":{"DBLP":"conf/acl/RatnerLBRMAKSLS23","ACL":"2023.acl-long.352","ArXiv":"2212.10947","DOI":"10.18653/v1/2023.acl-long.352","CorpusId":258686160},"title":"Parallel Context Windows for Large Language Models"},{"paperId":"9575afb5702bc33d7df14c48feeee5901ea00369","externalIds":{"DBLP":"journals/corr/abs-2212-10554","ArXiv":"2212.10554","ACL":"2023.acl-long.816","DOI":"10.48550/arXiv.2212.10554","CorpusId":254877252},"title":"A Length-Extrapolatable Transformer"},{"paperId":"a7ca1bce0af7fe4703f5c3296db2dcc8dc112f20","externalIds":{"DBLP":"conf/acl/JongZAFSSC23","ArXiv":"2212.08153","DOI":"10.48550/arXiv.2212.08153","CorpusId":254823295},"title":"FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference"},{"paperId":"eecb45aa040064cbc0b37fd100706c02e7dc880e","externalIds":{"DBLP":"journals/corr/abs-2212-06713","ArXiv":"2212.06713","DOI":"10.48550/arXiv.2212.06713","CorpusId":254591686},"title":"Structured Prompting: Scaling In-Context Learning to 1, 000 Examples"},{"paperId":"379e42895f6d40ab9e9559609f505aba89145a5d","externalIds":{"DBLP":"conf/mlsys/PopeDCDBHXAD23","ArXiv":"2211.05102","DOI":"10.48550/arXiv.2211.05102","CorpusId":253420623},"title":"Efficiently Scaling Transformer Inference"},{"paperId":"7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6","externalIds":{"DBLP":"journals/corr/abs-2210-17323","ArXiv":"2210.17323","CorpusId":253237200},"title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"},{"paperId":"70e91e16eb321067d9402710e14a40cf28311f73","externalIds":{"DBLP":"conf/iclr/MaZKHGNMZ23","ArXiv":"2209.10655","DOI":"10.48550/arXiv.2209.10655","CorpusId":252439127},"title":"Mega: Moving Average Equipped Gated Attention"},{"paperId":"4be7d1524edb0137599a5cc95f72844b85a52fe1","externalIds":{"DBLP":"journals/corr/abs-2208-07339","ArXiv":"2208.07339","DOI":"10.48550/arXiv.2208.07339","CorpusId":251564521},"title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"},{"paperId":"1966c4df2cda0fb8daf7f36366d909a021b6d5c1","externalIds":{"DBLP":"journals/corr/abs-2206-08898","ArXiv":"2206.08898","DOI":"10.1109/WACV57701.2024.00259","CorpusId":249848110},"title":"SimA: Simple Softmax-free Attention for Vision Transformers"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"27be9c039bdafb058741fa1fdd2764def4721541","externalIds":{"DBLP":"journals/corr/abs-2204-03145","ArXiv":"2204.03145","DOI":"10.1109/TPAMI.2024.3450575","CorpusId":248006129,"PubMed":"39190515"},"title":"DeepTensor: Low-Rank Tensor Decomposition With Deep Network Priors"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"c49ac1f916d6d2edeb187e6619c8d23acd95eb21","externalIds":{"DBLP":"journals/corr/abs-2202-08791","ArXiv":"2202.08791","CorpusId":246904340},"title":"cosFormer: Rethinking Softmax in Attention"},{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","externalIds":{"DBLP":"conf/iclr/GuGR22","ArXiv":"2111.00396","CorpusId":240354066},"title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"2e644c67a697073d561da4f4dad35e5ad5316cfd","externalIds":{"DBLP":"journals/corr/abs-2110-11945","ArXiv":"2110.11945","CorpusId":239616022},"title":"SOFT: Softmax-free Transformer with Linear Complexity"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","externalIds":{"ArXiv":"2108.12409","DBLP":"journals/corr/abs-2108-12409","CorpusId":237347130},"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"fc46ccb83dc121c33de7ab6bdedab7d970780b2f","externalIds":{"ArXiv":"2106.13008","DBLP":"conf/nips/WuXWL21","CorpusId":235623791},"title":"Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting"},{"paperId":"d5e999aae76d5270ef272076979c809817458212","externalIds":{"DBLP":"journals/corr/abs-2105-14103","ArXiv":"2105.14103","CorpusId":235254329},"title":"An Attention Free Transformer"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","externalIds":{"DBLP":"journals/ijon/SuALPBL24","ArXiv":"2104.09864","DOI":"10.1016/j.neucom.2023.127063","CorpusId":233307138},"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"db46b0de44c5113c47f0ec5392eb91d0726497bf","externalIds":{"ACL":"2021.emnlp-main.236","DBLP":"conf/emnlp/ChenTBCCF21","ArXiv":"2104.08698","DOI":"10.18653/v1/2021.emnlp-main.236","CorpusId":243728757},"title":"A Simple and Effective Positional Encoding for Transformers"},{"paperId":"093253653cd0b55970c390d77b75137c4095dc29","externalIds":{"DBLP":"journals/corr/abs-2103-13630","ArXiv":"2103.13630","DOI":"10.1201/9781003162810-13","CorpusId":232352683},"title":"A Survey of Quantization Methods for Efficient Neural Network Inference"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","externalIds":{"DBLP":"conf/iclr/ChoromanskiLDSG21","ArXiv":"2009.14794","MAG":"3091156754","CorpusId":222067132},"title":"Rethinking Attention with Performers"},{"paperId":"0964490205fdc38c2f0980c9d778069089ca92e3","externalIds":{"ArXiv":"2008.07669","MAG":"3099512283","DBLP":"conf/nips/GuDERR20","CorpusId":221150566},"title":"HiPPO: Recurrent Memory with Optimal Polynomial Projections"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","externalIds":{"MAG":"3033529678","DBLP":"journals/corr/abs-2006-04768","ArXiv":"2006.04768","CorpusId":219530577},"title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78","externalIds":{"DBLP":"journals/corr/abs-2006-04862","MAG":"3106009088","ArXiv":"2006.04862","CorpusId":219558319},"title":"$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","externalIds":{"DBLP":"journals/corr/abs-2004-05150","MAG":"3015468748","ArXiv":"2004.05150","CorpusId":215737171},"title":"Longformer: The Long-Document Transformer"},{"paperId":"657329c633709dd1ac34a30d57341b186b1a47c2","externalIds":{"ACL":"2021.tacl-1.4","MAG":"2997517014","DBLP":"journals/tacl/RoySVG21","ArXiv":"2003.05997","DOI":"10.1162/tacl_a_00353","CorpusId":212718077},"title":"Efficient Content-Based Sparse Attention with Routing Transformers"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","externalIds":{"DBLP":"journals/corr/abs-2001-04451","MAG":"2994673210","ArXiv":"2001.04451","CorpusId":209315300},"title":"Reformer: The Efficient Transformer"},{"paperId":"2cf3bd0cc1382f35384e259d99e4f9744eeaed28","externalIds":{"MAG":"3106298483","ArXiv":"1911.02972","ACL":"2020.findings-emnlp.232","DBLP":"journals/corr/abs-1911-02972","DOI":"10.18653/v1/2020.findings-emnlp.232","CorpusId":207847640},"title":"Blockwise Self-Attention for Long Document Understanding"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"49e5b09480189fc9b2316a54f9d1e55cf0097c8b","externalIds":{"MAG":"2982696980","ArXiv":"1910.13923","DBLP":"conf/icassp/WinataCLLF20","DOI":"10.1109/ICASSP40776.2020.9053878","CorpusId":204960988},"title":"Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank Transformer"},{"paperId":"36e30516683032634975c53e60f3737b6e35ff80","externalIds":{"MAG":"2954731415","DBLP":"conf/nips/LiJXZCWY19","ArXiv":"1907.00235","CorpusId":195766887},"title":"Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","externalIds":{"MAG":"2963925437","DBLP":"journals/corr/abs-1803-02155","ACL":"N18-2074","ArXiv":"1803.02155","DOI":"10.18653/v1/N18-2074","CorpusId":3725815},"title":"Self-Attention with Relative Position Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"d0d3f4d1003db0fb637519ef5d8bb140e7df8355","externalIds":{"MAG":"2399631432","CorpusId":37645299,"PubMed":"14513624"},"title":"May the source be with you."},{"paperId":"adc8b62fd2bd644c140c7c42275a9d2d913ad8a8","externalIds":{"DBLP":"conf/nips/LiuA23","CorpusId":266351737},"title":"Blockwise Parallel Transformers for Large Context Models"},{"paperId":"343d24c4dcfaff2132373d218561a23fbd53e934","externalIds":{"DBLP":"journals/corr/abs-2306-02272","DOI":"10.48550/arXiv.2306.02272","CorpusId":259076427},"title":"OWQ: Lessons learned from activation outliers for weight quantization in large language models"},{"paperId":"363668677c459ebc0ff494655f993a93a0251009","externalIds":{"DBLP":"conf/iclr/FrantarAHA23","CorpusId":259298689},"title":"OPTQ: Accurate Quantization for Generative Pre-trained Transformers"},{"paperId":"2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c","externalIds":{"DBLP":"journals/corr/abs-2306-00978","DOI":"10.48550/arXiv.2306.00978","CorpusId":271271084},"title":"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"}]}