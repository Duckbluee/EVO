{"paperId":"92e3b209fe9cad9078810e225f6b6b8b391f304b","externalIds":{"ArXiv":"2407.18921","DBLP":"journals/comsur/QuCWLCH25","DOI":"10.1109/COMST.2025.3527641","CorpusId":271534421},"title":"Mobile Edge Intelligence for Large Language Models: A Contemporary Survey","openAccessPdf":{"url":"http://arxiv.org/pdf/2407.18921","status":"GREEN","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2407.18921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2052085053","name":"Guanqiao Qu"},{"authorId":"2249568437","name":"Qiyuan Chen"},{"authorId":"2292342440","name":"Wei Wei"},{"authorId":"2365643","name":"Zhengyi Lin"},{"authorId":"2249923063","name":"Xianhao Chen"},{"authorId":"2256137676","name":"Kaibin Huang"}],"abstract":"On-device large language models (LLMs), referring to running LLMs on edge devices, have raised considerable interest since they are more cost-effective, latency-efficient, and privacy-preserving compared with the cloud LLM paradigm. Nonetheless unlike cloud LLMs, the performance of on-device LLMs is intrinsically constrained by resource limitations on edge devices. Sitting between cloud and on-device AI, mobile edge intelligence (MEI) may address this dilemma by provisioning AI capabilities at the edge of mobile networks, e.g., on base stations. This article provides a contemporary survey on harnessing MEI for LLM deployment. We begin by illustrating several killer applications to demonstrate the urgent need for deploying LLMs at the network edge. Next, we present the preliminaries of LLMs, MEI, and resource-efficient LLM techniques. We then provide an architectural overview of MEI for LLMs (MEI4LLM), outlining its core components and how it supports LLM deployment. Subsequently, we delve into various aspects of MEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training, and edge LLM inference. Finally, we identify future research opportunities. We hope this article inspires researchers in the field to leverage mobile edge computing to facilitate LLM deployment, thereby unleashing the potential of LLMs across various privacy- and delay-sensitive applications."}