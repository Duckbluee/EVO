{"paperId":"6a546395d2e1ea67c832283ae541b3e67923f581","externalIds":{"ArXiv":"2407.00936","DBLP":"journals/corr/abs-2407-00936","DOI":"10.1007/s41019-025-00285-y","CorpusId":270870607},"title":"Large Language Model Enhanced Knowledge Representation Learning: A Survey","openAccessPdf":{"url":"https://doi.org/10.1007/s41019-025-00285-y","status":"GOLD","license":"CCBY","disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2407.00936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2257336365","name":"Xin Wang"},{"authorId":"2257074965","name":"Zirui Chen"},{"authorId":"2265798026","name":"Haofen Wang"},{"authorId":"2203795843","name":"Leong Hou U"},{"authorId":"2257116443","name":"Zhao Li"},{"authorId":"2266802637","name":"Wenbin Guo"}],"abstract":"Knowledge Representation Learning (KRL) is crucial for enabling applications of symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by projecting knowledge facts into vector spaces. Despite their effectiveness in modeling KG structural information, KRL methods are suffering from the sparseness of KGs. The rise of Large Language Models (LLMs) built on the Transformer architecture presents promising opportunities for enhancing KRL by incorporating textual information to address information sparsity in KGs. LLM-enhanced KRL methods, including three key approaches, encoder-based methods that leverage detailed contextual information, encoder-decoder-based methods that utilize a unified Seq2Seq model for comprehensive encoding and decoding, and decoder-based methods that utilize extensive knowledge from large corpora, have significantly advanced the effectiveness and generalization of KRL in addressing a wide range of downstream tasks. This work provides a broad overview of downstream tasks while simultaneously identifying emerging research directions in these evolving domains."}