{"references":[{"paperId":"0f6fe87afd1a3571f77c790893b03717e5d0422a","externalIds":{"ArXiv":"2310.07289","DBLP":"journals/corr/abs-2310-07289","DOI":"10.48550/arXiv.2310.07289","CorpusId":263834979},"title":"Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators"},{"paperId":"8c9b8ba4a44b9736ea9db94f11c3227d5bb91a09","externalIds":{"ArXiv":"2310.05177","DBLP":"journals/corr/abs-2310-05177","DOI":"10.48550/arXiv.2310.05177","CorpusId":263828951},"title":"Do Large Language Models Know about Facts?"},{"paperId":"be177300487b6d0f25e6cade9a31900454b13281","externalIds":{"DBLP":"journals/corr/abs-2310-03214","ArXiv":"2310.03214","DOI":"10.48550/arXiv.2310.03214","CorpusId":263672149},"title":"FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation"},{"paperId":"9099ee08e59cc33ed1c88d4708cf5c931bf46dc4","externalIds":{"ArXiv":"2309.16289","DBLP":"journals/corr/abs-2309-16289","DOI":"10.48550/arXiv.2309.16289","CorpusId":263134950},"title":"LawBench: Benchmarking Legal Knowledge of Large Language Models"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"8eafec7014d08043517834b5a2ed26384f188873","externalIds":{"ArXiv":"2309.12288","DBLP":"journals/corr/abs-2309-12288","CorpusId":262083829},"title":"The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\""},{"paperId":"4b0b56be0ae9479d2bd5c2f0943db1906343c10f","externalIds":{"DBLP":"journals/corr/abs-2309-11495","ArXiv":"2309.11495","DOI":"10.48550/arXiv.2309.11495","CorpusId":262062565},"title":"Chain-of-Verification Reduces Hallucination in Large Language Models"},{"paperId":"c96297261467b5daa2d01227496a70d444602434","externalIds":{"DBLP":"journals/corr/abs-2309-10305","ArXiv":"2309.10305","DOI":"10.48550/arXiv.2309.10305","CorpusId":261951743},"title":"Baichuan 2: Open Large-scale Language Models"},{"paperId":"a281094d05e96b7cca044fdd87ff7c3c65649e20","externalIds":{"DBLP":"journals/corr/abs-2309-10313","ArXiv":"2309.10313","DOI":"10.48550/arXiv.2309.10313","CorpusId":262055661},"title":"Investigating the Catastrophic Forgetting in Multimodal Large Language Models"},{"paperId":"6988596f88276920a4e555cbe624e1431bc8a9f7","externalIds":{"DBLP":"conf/iclr/KothaSR24","ArXiv":"2309.10105","DOI":"10.48550/arXiv.2309.10105","CorpusId":262054014},"title":"Understanding Catastrophic Forgetting in Language Models via Implicit Inference"},{"paperId":"aa0b3306f7dd827a6fb8487aeb39d832fdcb97a0","externalIds":{"DBLP":"journals/corr/abs-2309-09558","ArXiv":"2309.09558","DOI":"10.48550/arXiv.2309.09558","CorpusId":262044218},"title":"Summarization is (Almost) Dead"},{"paperId":"396305230ddcf915b19a19683a89e34d76321a33","externalIds":{"DBLP":"conf/lkm/YeLZHJ24","ArXiv":"2309.06794","DOI":"10.48550/arXiv.2309.06794","CorpusId":261705916},"title":"Cognitive Mirage: A Review of Hallucinations in Large Language Models"},{"paperId":"71bc0c97c20fffce796a355b16bd202987260029","externalIds":{"ArXiv":"2309.05922","DBLP":"journals/corr/abs-2309-05922","DOI":"10.48550/arXiv.2309.05922","CorpusId":261696947},"title":"A Survey of Hallucination in Large Foundation Models"},{"paperId":"ed5020eeda1fbe8c29b1282d654b34abee22d90f","externalIds":{"DBLP":"conf/iclr/ChuangXLKGH24","ArXiv":"2309.03883","DOI":"10.48550/arXiv.2309.03883","CorpusId":261582463},"title":"DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models"},{"paperId":"28e2ecb4183ebc0eec504b12dddc677f8aef8745","externalIds":{"DBLP":"journals/corr/abs-2309-01431","ArXiv":"2309.01431","DOI":"10.48550/arXiv.2309.01431","CorpusId":261530434},"title":"Benchmarking Large Language Models in Retrieval-Augmented Generation"},{"paperId":"d00735241af700d21762d2f3ca00d920241a15a4","externalIds":{"DBLP":"journals/corr/abs-2309-01219","ArXiv":"2309.01219","DOI":"10.1162/coli.a.16","CorpusId":261530162},"title":"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"},{"paperId":"0a10b5af8d6024b1888ae8faed08d5b4e62b09a6","externalIds":{"ArXiv":"2309.01114","DBLP":"journals/corr/abs-2309-01114","DOI":"10.48550/arXiv.2309.01114","CorpusId":261530957},"title":"MedChatZH: a Better Medical Adviser Learns from Better Instructions"},{"paperId":"bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7","externalIds":{"DBLP":"journals/coling/GallegosRBTKDYZA24","ArXiv":"2309.00770","DOI":"10.1162/coli_a_00524","CorpusId":261530629},"title":"Bias and Fairness in Large Language Models: A Survey"},{"paperId":"4c5b4a8e31d3119c1e3b5753693ff283c9717218","externalIds":{"DBLP":"journals/corr/abs-2308-14346","ArXiv":"2308.14346","DOI":"10.48550/arXiv.2308.14346","CorpusId":261243110},"title":"DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation"},{"paperId":"628b131f8f309d583ccf1da268f520c051169ddd","externalIds":{"DBLP":"conf/aaai/ChenCCLZ24","ArXiv":"2308.13198","DOI":"10.48550/arXiv.2308.13198","CorpusId":261214558},"title":"Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons"},{"paperId":"fb00016c1e048b9373803add001c1ec7e877cb23","externalIds":{"DBLP":"journals/corr/abs-2308-10168","ArXiv":"2308.10168","ACL":"2024.naacl-long.18","DOI":"10.48550/arXiv.2308.10168","CorpusId":261048922},"title":"Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?"},{"paperId":"ac4ab9b23a002e81c9419c520747a603fbcbb40d","externalIds":{"ArXiv":"2308.10173","DBLP":"journals/corr/abs-2308-10173","DOI":"10.48550/arXiv.2308.10173","CorpusId":261048937},"title":"FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt"},{"paperId":"0aa5f3e94fab50e3c19870880fdaf245bfebdfae","externalIds":{"ArXiv":"2308.11462","DBLP":"journals/corr/abs-2308-11462","DOI":"10.48550/arXiv.2308.11462","CorpusId":261064672},"title":"LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models"},{"paperId":"838cd69a0b6c9c244a6eebb0f4742c0625132de6","externalIds":{"DBLP":"journals/corr/abs-2308-08747","ArXiv":"2308.08747","DOI":"10.1109/TASLPRO.2025.3606231","CorpusId":261031244},"title":"An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-Tuning"},{"paperId":"5df24ed6fdf10d1e92885687abce7bd5e56f3f85","externalIds":{"ArXiv":"2308.08833","DBLP":"conf/naacl/WangCS0CXCJLWW024","ACL":"2024.naacl-long.343","DOI":"10.48550/arXiv.2308.08833","CorpusId":261030527},"title":"CMB: A Comprehensive Medical Benchmark in Chinese"},{"paperId":"64e802ea8e9dbe247c31fb06184c04dbf9e55e4e","externalIds":{"ArXiv":"2308.06966","DBLP":"conf/aaai/LiMWHJ0X0J24","DOI":"10.48550/arXiv.2308.06966","CorpusId":260887693},"title":"EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce"},{"paperId":"a50d4fd8f584276c0fd8560255884edd57aa926e","externalIds":{"DBLP":"journals/corr/abs-2308-03549","ArXiv":"2308.03549","DOI":"10.48550/arXiv.2308.03549","CorpusId":260681932},"title":"Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue"},{"paperId":"7cfda398581512b4a95d9874f499a2291bd9dfc0","externalIds":{"ArXiv":"2308.01414","DBLP":"journals/corr/abs-2308-01414","DOI":"10.48550/arXiv.2308.01414","CorpusId":260438392},"title":"HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field"},{"paperId":"21737406f448844220dde7c711565bd44b0c27f0","externalIds":{"ArXiv":"2307.15290","DBLP":"journals/corr/abs-2307-15290","DOI":"10.48550/arXiv.2307.15290","CorpusId":260315830},"title":"ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation"},{"paperId":"e08c1e013681c82a65dd971bfd86d5ae4b48318f","externalIds":{"DBLP":"conf/nlpcc/FanJLL23","ArXiv":"2307.13923","DOI":"10.48550/arXiv.2307.13923","CorpusId":260165014},"title":"GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning"},{"paperId":"7a5b44ea10a51708e18786595c8d70b18950da11","externalIds":{"DBLP":"journals/corr/abs-2307-13528","ArXiv":"2307.13528","DOI":"10.48550/arXiv.2307.13528","CorpusId":260154834},"title":"FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"},{"paperId":"1fdf449c96fbac0789cf8dfae15b788905407fd3","externalIds":{"DBLP":"journals/tacl/CohenBYGG24","ACL":"2024.tacl-1.16","ArXiv":"2307.12976","DOI":"10.1162/tacl_a_00644","CorpusId":260356612},"title":"Evaluating the Ripple Effects of Knowledge Editing in Language Models"},{"paperId":"089f6328085066263fedc083952624ca121ebbf3","externalIds":{"DBLP":"journals/corr/abs-2307-11346","ArXiv":"2307.11346","DOI":"10.48550/arXiv.2307.11346","CorpusId":260091657},"title":"CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study"},{"paperId":"84b77180228051040286423cec82b62c323a8fda","externalIds":{"ArXiv":"2307.11019","DBLP":"journals/corr/abs-2307-11019","DOI":"10.48550/arXiv.2307.11019","CorpusId":259991467},"title":"Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation"},{"paperId":"94ce1d5924e05e8d75e43ce70044293ddcef850a","externalIds":{"DOI":"10.1038/s41591-023-02448-8","CorpusId":259947046,"PubMed":"37460753"},"title":"Large language models in medicine"},{"paperId":"1827dd28ef866eaeb929ddf4bcfa492880aba4c7","externalIds":{"DBLP":"journals/corr/abs-2307-03987","ArXiv":"2307.03987","DOI":"10.48550/arXiv.2307.03987","CorpusId":263699899},"title":"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation"},{"paperId":"1733eb7792f7a43dd21f51f4d1017a1bffd217b5","externalIds":{"DBLP":"journals/tacl/LiuLHPBPL24","ArXiv":"2307.03172","ACL":"2024.tacl-1.9","DOI":"10.1162/tacl_a_00638","CorpusId":259360665},"title":"Lost in the Middle: How Language Models Use Long Contexts"},{"paperId":"888728745dbb769e29ed475d4f7661eebe1a71cf","externalIds":{"DBLP":"journals/tist/ChangWWWYZCYWWYZCYYX24","ArXiv":"2307.03109","DOI":"10.1145/3641289","CorpusId":259360395},"title":"A Survey on Evaluation of Large Language Models"},{"paperId":"228aee5393e7a11e018bbef940fea1c2816b6ec4","externalIds":{"ArXiv":"2306.16092","CorpusId":259274889},"title":"Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"02033e83ff310f35e4623bd339982c52d926f2d5","externalIds":{"DBLP":"journals/tkde/YangCLDW24","ArXiv":"2306.11489","DOI":"10.1109/TKDE.2024.3360454","CorpusId":259203671},"title":"Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling"},{"paperId":"d2eba144323a6704740ffe550b7d435ff679a5f0","externalIds":{"DBLP":"journals/corr/abs-2306-11520","ArXiv":"2306.11520","DOI":"10.48550/arXiv.2306.11520","CorpusId":259203204},"title":"Hallucination is the last thing you need"},{"paperId":"2a68cfffde314b717ca3fc4bd3ffab597f1b6ea9","externalIds":{"ArXiv":"2306.09525","DBLP":"journals/corr/abs-2306-09525","DOI":"10.48550/arXiv.2306.09525","CorpusId":259187949},"title":"Explaining Legal Concepts with Augmented Large Language Models (GPT-4)"},{"paperId":"9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6","externalIds":{"ArXiv":"2306.08302","DBLP":"journals/corr/abs-2306-08302","DOI":"10.1109/TKDE.2024.3352100","CorpusId":259165563},"title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap"},{"paperId":"109929be7890ef982fb3b6be0d78609cfab1ea13","externalIds":{"DBLP":"journals/corr/abs-2306-05443","ArXiv":"2306.05443","DOI":"10.48550/arXiv.2306.05443","CorpusId":259129602},"title":"PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance"},{"paperId":"405f8f5f1c6df1b3343c812832479aad5180b65f","externalIds":{"ArXiv":"2306.03341","DBLP":"journals/corr/abs-2306-03341","CorpusId":259088877},"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"},{"paperId":"5677b3de564fddf06f518c79429697f316d944d4","externalIds":{"DBLP":"conf/acl/HossainD023","ArXiv":"2306.03950","ACL":"2023.acl-long.293","DOI":"10.48550/arXiv.2306.03950","CorpusId":259095770},"title":"MISGENDERED: Limits of Large Language Models in Understanding Pronouns"},{"paperId":"6847b9658f287f430098199cd81bf26308da13f9","externalIds":{"DBLP":"journals/csur/LingZLDZWCLCZZPMPCWLCCW26","ArXiv":"2305.18703","DOI":"10.1145/3764579","CorpusId":259502302},"title":"Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey"},{"paperId":"eb971944bccf9793ac463c3e2f4d4251d4e8e071","externalIds":{"DBLP":"journals/corr/abs-2305-18153","ArXiv":"2305.18153","DOI":"10.48550/arXiv.2305.18153","CorpusId":258959258},"title":"Do Large Language Models Know What They Don't Know?"},{"paperId":"7db7653c581d7823cb9c328f2d742ec70d7a0ce4","externalIds":{"DBLP":"journals/corr/abs-2305-14908","ArXiv":"2305.14908","DOI":"10.48550/arXiv.2305.14908","CorpusId":258865339},"title":"PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions"},{"paperId":"a7977870b58e716cd93f571a3b75a610167a75bd","externalIds":{"DBLP":"conf/emnlp/LuoZCGKWMG23","ArXiv":"2305.15225","DOI":"10.48550/arXiv.2305.15225","CorpusId":258865283},"title":"SAIL: Search-Augmented Instruction Learning"},{"paperId":"5459cab5dcf3c65c6b4f63b3d9f1e376f722bbcb","externalIds":{"DBLP":"journals/corr/abs-2305-15075","ArXiv":"2305.15075","DOI":"10.48550/arXiv.2305.15075","CorpusId":258865566},"title":"HuatuoGPT, towards Taming Language Model to Be a Doctor"},{"paperId":"e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365","externalIds":{"DBLP":"conf/emnlp/GaoYYC23","ArXiv":"2305.14627","DOI":"10.48550/arXiv.2305.14627","CorpusId":258865710},"title":"Enabling Large Language Models to Generate Text with Citations"},{"paperId":"56e952fd463accff09cf2e35432aaabd7c7c57f3","externalIds":{"DBLP":"conf/emnlp/ZhongWMPC23","ArXiv":"2305.14795","DOI":"10.48550/arXiv.2305.14795","CorpusId":258865984},"title":"MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions"},{"paperId":"4780d0a027c5c5a8e01d7cf697f6296880ffc945","externalIds":{"ArXiv":"2305.14325","DBLP":"journals/corr/abs-2305-14325","DOI":"10.48550/arXiv.2305.14325","CorpusId":258841118},"title":"Improving Factuality and Reasoning in Language Models through Multiagent Debate"},{"paperId":"bd5deadc58ee45b5e004378ba1d54a96bc947b4a","externalIds":{"ArXiv":"2305.14251","DBLP":"conf/emnlp/MinKLLYKIZH23","DOI":"10.48550/arXiv.2305.14251","CorpusId":258841470},"title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"},{"paperId":"62b322b0bead56d6a252a2e24de499ea8385ad7f","externalIds":{"DBLP":"journals/corr/abs-2305-13669","DOI":"10.48550/arXiv.2305.13669","CorpusId":258840979},"title":"Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment"},{"paperId":"4f480bae3196dbbc27ab383bce33478ea963f9b3","externalIds":{"ArXiv":"2305.13711","DBLP":"journals/corr/abs-2305-13711","ACL":"2023.nlp4convai-1.5","DOI":"10.48550/arXiv.2305.13711","CorpusId":258841681},"title":"LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models"},{"paperId":"fef6471c4a2a0e7abc4a2261a6cf916e34091d12","externalIds":{"ArXiv":"2305.13661","DBLP":"conf/emnlp/PanPCNKW23","DOI":"10.48550/arXiv.2305.13661","CorpusId":258840876},"title":"On the Risk of Misinformation Pollution with Large Language Models"},{"paperId":"6825ba09383bc758f9a2feaebabe35a6cd4adc4c","externalIds":{"DBLP":"conf/icml/ZhangPMLS24","ArXiv":"2305.13534","DOI":"10.48550/arXiv.2305.13534","CorpusId":258841857},"title":"How Language Model Hallucinations Can Snowball"},{"paperId":"41a41c75ba336dec98d58c563605f261019e5df0","externalIds":{"ACL":"2024.eacl-long.140","DBLP":"conf/eacl/WellerMWLKD24","ArXiv":"2305.13252","DOI":"10.18653/v1/2024.eacl-long.140","CorpusId":258832937},"title":"‚ÄúAccording to . . . ‚Äù: Prompting Language Models Improves Quoting from Pre-Training Data"},{"paperId":"f5c73d9e6641b018b633690102121f5605d34fb0","externalIds":{"DBLP":"conf/emnlp/YaoWT0LDC023","ArXiv":"2305.13172","DOI":"10.48550/arXiv.2305.13172","CorpusId":258833129},"title":"Editing Large Language Models: Problems, Methods, and Opportunities"},{"paperId":"8793066d170b6a742c4fcdb478d4f100c1e4bf17","externalIds":{"DBLP":"journals/corr/abs-2305-12744","ACL":"2023.acl-long.386","ArXiv":"2305.12744","DOI":"10.48550/arXiv.2305.12744","CorpusId":258833449},"title":"Fact-Checking Complex Claims with Program-Guided Reasoning"},{"paperId":"ed0ed87161a2beab9e1bed3e783d7487a5f1062a","externalIds":{"ArXiv":"2305.13281","DBLP":"conf/emnlp/CohenHGG23","DOI":"10.48550/arXiv.2305.13281","CorpusId":258833288},"title":"LM vs LM: Detecting Factual Errors via Cross Examination"},{"paperId":"bcdaf6c98ddbd6809cf6241aa77200d7394db163","externalIds":{"DBLP":"conf/iclr/GouSGSYDC24","ArXiv":"2305.11738","DOI":"10.48550/arXiv.2305.11738","CorpusId":258823123},"title":"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"},{"paperId":"e0384ba36555232c587d4a80d527895a095a9001","externalIds":{"DBLP":"journals/corr/abs-2305-11747","ArXiv":"2305.11747","DOI":"10.48550/arXiv.2305.11747","CorpusId":258832847},"title":"HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"ed1353d705eeabc0e916caba5fbae890eefe4f84","externalIds":{"DBLP":"conf/acl/LiuZXW0QZL23","ArXiv":"2305.10688","ACL":"2023.acl-short.138","DOI":"10.48550/arXiv.2305.10688","CorpusId":258762343},"title":"MolXPT: Wrapping Molecules with Text for Generative Pre-training"},{"paperId":"e0f27336698c84709bd60b6b7f4ce588cbae66bf","externalIds":{"DBLP":"journals/corr/abs-2305-09645","ArXiv":"2305.09645","DOI":"10.48550/arXiv.2305.09645","CorpusId":258714753},"title":"StructGPT: A General Framework for Large Language Model to Reason over Structured Data"},{"paperId":"236c7dafea3df7ecffb5f18ec780d12f2f27d4b0","externalIds":{"DBLP":"conf/nips/HuangBZZZSLLZLF23","ArXiv":"2305.08322","DOI":"10.48550/arXiv.2305.08322","CorpusId":258685666},"title":"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"},{"paperId":"88884b8806262a4095036041e3567d450dba39f7","externalIds":{"DBLP":"journals/corr/abs-2305-06983","ArXiv":"2305.06983","DOI":"10.48550/arXiv.2305.06983","CorpusId":258615731},"title":"Active Retrieval Augmented Generation"},{"paperId":"c3ed333a37a6d9a0fcf1dad3106a114f66a45b99","externalIds":{"DBLP":"journals/corr/abs-2305-06849","ArXiv":"2305.06849","ACL":"2023.acl-long.499","DOI":"10.48550/arXiv.2305.06849","CorpusId":258615343},"title":"WebCPM: Interactive Web Search for Chinese Long-form Question Answering"},{"paperId":"acf90b4d165690fe27c62c4af1a28d540c784000","externalIds":{"DBLP":"journals/corr/abs-2305-06311","ArXiv":"2305.06311","DOI":"10.48550/arXiv.2305.06311","CorpusId":258587884},"title":"Automatic Evaluation of Attribution by Large Language Models"},{"paperId":"1c5bc4f10b95a90d0283d0aacc94332aae508169","externalIds":{"DBLP":"journals/corr/abs-2305-01526","ArXiv":"2305.01526","DOI":"10.48550/arXiv.2305.01526","CorpusId":258436815},"title":"Huatuo-26M, a Large-scale Chinese Medical QA Dataset"},{"paperId":"133b97e40017a9bbbadd10bcd7f13088a97ca3cc","externalIds":{"DBLP":"conf/emnlp/GevaBFG23","ArXiv":"2304.14767","DOI":"10.48550/arXiv.2304.14767","CorpusId":258417932},"title":"Dissecting Recall of Factual Associations in Auto-Regressive Language Models"},{"paperId":"e877d295ca425faf33f0c8e4d8c410c2e9c8a26d","externalIds":{"DBLP":"conf/emnlp/LiuWMSWKSS023","ArXiv":"2304.14399","DOI":"10.48550/arXiv.2304.14399","CorpusId":258352700},"title":"We're Afraid Language Models Aren't Modeling Ambiguity"},{"paperId":"f406aceba4f29cc7cfbe7edb2f52f01374486589","externalIds":{"ArXiv":"2304.13734","DBLP":"journals/corr/abs-2304-13734","DOI":"10.18653/v1/2023.findings-emnlp.68","CorpusId":258352729},"title":"The Internal State of an LLM Knows When its Lying"},{"paperId":"d2b4b4804479714ad0e406e8ab73fa3e72069216","externalIds":{"DBLP":"journals/corr/abs-2304-10453","ArXiv":"2304.10453","DOI":"10.48550/arXiv.2304.10453","CorpusId":258236343},"title":"Phoenix: Democratizing ChatGPT across Languages"},{"paperId":"05e003a34148d4663734d3f39deefa0979d2a0e6","externalIds":{"PubMedCentral":"10153281","DBLP":"journals/corr/abs-2304-09667","ArXiv":"2304.09667","DOI":"10.1093/bioinformatics/btae075","CorpusId":258298113,"PubMed":"38341654"},"title":"GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information"},{"paperId":"e5adc219685c9941b9a3d029480af4a51c0ea05a","externalIds":{"DBLP":"journals/corr/abs-2304-08177","ArXiv":"2304.08177","DOI":"10.48550/arXiv.2304.08177","CorpusId":258180548},"title":"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"},{"paperId":"302ee27524a717ddc21f332ca634b9211c6ec6aa","externalIds":{"ArXiv":"2304.06975","DBLP":"journals/corr/abs-2304-06975","DOI":"10.48550/arXiv.2304.06975","CorpusId":258170497},"title":"HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"},{"paperId":"68c834c19cd126bbd6d25a3572d7205cfed76271","externalIds":{"DBLP":"journals/corr/abs-2304-06364","ArXiv":"2304.06364","DOI":"10.48550/arXiv.2304.06364","CorpusId":258108259},"title":"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"},{"paperId":"3fcc8cb68488cfdfe4c52b81f27a236352fe5582","externalIds":{"DBLP":"journals/corr/abs-2304-03728","ArXiv":"2304.03728","DOI":"10.48550/arXiv.2304.03728","CorpusId":258041307},"title":"Interpretable Unified Language Checking"},{"paperId":"6028780bfb3728292d37c07951e3f463fae0981e","externalIds":{"ACL":"2023.eacl-main.215","ArXiv":"2304.01597","DBLP":"journals/corr/abs-2304-01597","DOI":"10.48550/arXiv.2304.01597","CorpusId":257921894},"title":"Unsupervised Improvement of Factual Knowledge in Language Models"},{"paperId":"bce55193d9a887ad00774a9134df08cd521a85ae","externalIds":{"DBLP":"journals/corr/abs-2304-01097","ArXiv":"2304.01097","DOI":"10.48550/arXiv.2304.01097","CorpusId":257912795},"title":"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","externalIds":{"DBLP":"journals/corr/abs-2303-17564","ArXiv":"2303.17564","CorpusId":257833842},"title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"4a7f6c4e71e20311ade4e76e8d0945d499c31fcd","externalIds":{"PubMedCentral":"10364849","ArXiv":"2303.14070","DOI":"10.7759/cureus.40895","CorpusId":259252045,"PubMed":"37492832"},"title":"ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge"},{"paperId":"8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c","externalIds":{"ArXiv":"2303.12712","DBLP":"journals/corr/abs-2303-12712","CorpusId":257663729},"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"12c826f4195da172b212a529f8fcf10cc79e35da","externalIds":{"ArXiv":"2303.11315","DBLP":"journals/corr/abs-2303-11315","DOI":"10.48550/arXiv.2303.11315","CorpusId":257632259},"title":"Context-faithful Prompting for Large Language Models"},{"paperId":"cff26bda86237d113ed01c812ad8bedd0afbe070","externalIds":{"DBLP":"journals/corr/abs-2303-11032","ArXiv":"2303.11032","DOI":"10.48550/arXiv.2303.11032","CorpusId":257632030},"title":"DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4"},{"paperId":"0671fd553dd670a4e820553a974bc48040ba0819","externalIds":{"DBLP":"conf/nips/ShinnCGNY23","ArXiv":"2303.11366","CorpusId":258833055},"title":"Reflexion: language agents with verbal reinforcement learning"},{"paperId":"348a1efa54376fa39053e5e25d52bd0eb6a0ba68","externalIds":{"ArXiv":"2303.13375","DBLP":"journals/corr/abs-2303-13375","DOI":"10.48550/arXiv.2303.13375","CorpusId":257687695},"title":"Capabilities of GPT-4 on Medical Challenge Problems"},{"paperId":"7c1707db9aafd209aa93db3251e7ebd593d55876","externalIds":{"DBLP":"conf/emnlp/ManakulLG23","ArXiv":"2303.08896","DOI":"10.48550/arXiv.2303.08896","CorpusId":257557820},"title":"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"bfad52fc64ca0169644b6e7e0ea9a46470d51709","externalIds":{"DBLP":"conf/semweb/TanMLLHCQ23","ArXiv":"2303.07992","CorpusId":257505407},"title":"Can ChatGPT Replace Traditional KBQA Models? An In-Depth Analysis of the Question Answering Performance of the GPT LLM Family"},{"paperId":"430aa6966c15c4a20a4fb2d8383e136b9cb6cde7","externalIds":{"ArXiv":"2303.01229","PubMedCentral":"10187428","DOI":"10.21203/rs.3.rs-2883198/v1","CorpusId":258740478,"PubMed":"37205549"},"title":"Almanac: Retrieval-Augmented Language Models for Clinical Medicine"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"e5c72b92c48d68594b290c84a8904da7c8335554","externalIds":{"ArXiv":"2302.12813","DBLP":"journals/corr/abs-2302-12813","DOI":"10.48550/arXiv.2302.12813","CorpusId":257205781},"title":"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback"},{"paperId":"41b360c5b4ae9c5bfcf3891c45319a9e0b3e6d81","externalIds":{"ArXiv":"2302.05729","DBLP":"journals/corr/abs-2302-05729","DOI":"10.48550/arXiv.2302.05729","CorpusId":256827839},"title":"A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3"},{"paperId":"40c318400809abf5e50aba5a5a80c8012a7715d5","externalIds":{"DBLP":"conf/naacl/FuNJ024","ArXiv":"2302.04166","ACL":"2024.naacl-long.365","DOI":"10.18653/v1/2024.naacl-long.365","CorpusId":256662188},"title":"GPTScore: Evaluate as You Desire"},{"paperId":"b026af1b2436fe064e02ca7a00b79713427bc3b2","externalIds":{"PubMedCentral":"9967747","DOI":"10.3390/ijerph20043378","CorpusId":256936867,"PubMed":"36834073"},"title":"Diagnostic Accuracy of Differential-Diagnosis Lists Generated by Generative Pretrained Transformer 3 Chatbot for Clinical Vignettes with Common Chief Complaints: A Pilot Study"},{"paperId":"07b14c24833400b79978b0a5f084803337e30a15","externalIds":{"DBLP":"conf/naacl/ShiMYS0LZY24","ACL":"2024.naacl-long.463","ArXiv":"2301.12652","DOI":"10.48550/arXiv.2301.12652","CorpusId":256389797},"title":"REPLUG: Retrieval-Augmented Black-Box Language Models"},{"paperId":"621b8c4b37d1e22d440262dda2206ac13da6d779","externalIds":{"DOI":"10.1148/radiol.230163","CorpusId":256272939,"PubMed":"36700838"},"title":"ChatGPT and Other Large Language Models Are Double-edged Swords."},{"paperId":"a9be51698e7c2247853b7b6f1f70fc4d6d7ef605","externalIds":{"DBLP":"journals/corr/abs-2301-09785","ArXiv":"2301.09785","DOI":"10.48550/arXiv.2301.09785","CorpusId":256194369},"title":"Transformer-Patcher: One Mistake worth One Neuron"},{"paperId":"7389b6ebbf36f4d869a02e305e2ef52ad2c92264","externalIds":{"PubMedCentral":"9950855","DOI":"10.1093/bioadv/vbad001","CorpusId":255893761,"PubMed":"36845200"},"title":"Applications of transformer-based language models in bioinformatics: a survey"},{"paperId":"30c0cdc414f68211d5d0514df027cec22e005174","externalIds":{"DBLP":"conf/emnlp/Dong0DZMLXX0C0S24","ArXiv":"2301.00234","ACL":"2024.emnlp-main.64","DOI":"10.18653/v1/2024.emnlp-main.64","CorpusId":255372865},"title":"A Survey on In-context Learning"},{"paperId":"490d8006851b1562cfd9ec1f057471f2868289d1","externalIds":{"DBLP":"journals/corr/abs-2301-00303","ArXiv":"2301.00303","DOI":"10.48550/arXiv.2301.00303","CorpusId":255372320},"title":"Rethinking with Retrieval: Faithful Large Language Model Inference"},{"paperId":"6052486bc9144dc1730c12bf35323af3792a1fd0","externalIds":{"ArXiv":"2212.13138","DBLP":"journals/corr/abs-2212-13138","PubMedCentral":"10396962","DOI":"10.1038/s41586-023-06291-2","CorpusId":255124952,"PubMed":"37438534"},"title":"Large language models encode clinical knowledge"},{"paperId":"f208ea909fa7f54fea82def9a92fd81dfc758c39","externalIds":{"ArXiv":"2212.10509","DBLP":"journals/corr/abs-2212-10509","ACL":"2023.acl-long.557","DOI":"10.48550/arXiv.2212.10509","CorpusId":254877499},"title":"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","externalIds":{"ArXiv":"2212.10403","DBLP":"conf/acl/0009C23","DOI":"10.48550/arXiv.2212.10403","CorpusId":254877753},"title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"c6ee979c2da4b55a8486abae4cd720422ab09b26","externalIds":{"ArXiv":"2212.10511","ACL":"2023.acl-long.546","DBLP":"conf/acl/MallenAZDKH23","DOI":"10.18653/v1/2023.acl-long.546","CorpusId":254877603},"title":"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories"},{"paperId":"d7ea898cc97754e06d209df0fd55ab60250601f2","externalIds":{"ArXiv":"2212.10400","DBLP":"conf/aaai/SunSGRRR23","DOI":"10.48550/arXiv.2212.10400","CorpusId":254593969},"title":"Contrastive Learning Reduces Hallucination in Conversations"},{"paperId":"cf1f26e7cbed3958b3c2870656568c299fece6e3","externalIds":{"PubMedCentral":"9931230","DOI":"10.1371/journal.pdig.0000198","CorpusId":254876189,"PubMed":"36812645"},"title":"Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models"},{"paperId":"86da7a99e904481d9146f291088f80eafd181c86","externalIds":{"DBLP":"conf/acl/LiuFLZNHHJWXR23","ACL":"2023.acl-long.228","ArXiv":"2212.07981","DOI":"10.48550/arXiv.2212.07981","CorpusId":254685611},"title":"Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation"},{"paperId":"86c79317afa13ea4a74f656e3c48f012ee1fc326","externalIds":{"DBLP":"journals/corr/abs-2212-03613","ArXiv":"2212.03613","ACL":"2022.emnlp-main.441","DOI":"10.48550/arXiv.2212.03613","CorpusId":254366524},"title":"G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks"},{"paperId":"75f7e9e2b59fb640ef9d1dff94097175daf46c4d","externalIds":{"ArXiv":"2211.08411","DBLP":"journals/corr/abs-2211-08411","CorpusId":253522998},"title":"Large Language Models Struggle to Learn Long-Tail Knowledge"},{"paperId":"56373d3fd0f1354a61f9e577db039cdb187d8d43","externalIds":{"DBLP":"conf/acl/TamMZKBR23","ArXiv":"2211.08412","DOI":"10.18653/v1/2023.findings-acl.322","CorpusId":253523092},"title":"Evaluating the Factual Consistency of Large Language Models Through News Summarization"},{"paperId":"cae223fe3654b0bd3c1361dad204218ac60ed771","externalIds":{"DBLP":"conf/acl/NeemanAHCSA23","ArXiv":"2211.05655","ACL":"2023.acl-long.559","DOI":"10.48550/arXiv.2211.05655","CorpusId":253447228},"title":"DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering"},{"paperId":"ee8de585183763ff64cb3c81ecda2fc75fa81507","externalIds":{"ArXiv":"2211.05110","DBLP":"journals/corr/abs-2211-05110","DOI":"10.48550/arXiv.2211.05110","CorpusId":253420654},"title":"Large Language Models with Controllable Working Memory"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"0882a2b2787b35dbcc6e341c953d964b77abd4df","externalIds":{"ArXiv":"2211.00083","ACL":"2022.emnlp-main.148","DBLP":"conf/emnlp/ShahCESDCRSCY22","DOI":"10.48550/arXiv.2211.00083","CorpusId":253244049},"title":"When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain"},{"paperId":"88b62496cbc52072bfa8f4b29d172b0477b701bc","externalIds":{"DBLP":"conf/acl/LiHFLEHZL23","ArXiv":"2210.15097","ACL":"2023.acl-long.687","DOI":"10.48550/arXiv.2210.15097","CorpusId":253157949},"title":"Contrastive Decoding: Open-ended Text Generation as Optimization"},{"paperId":"5b70e69b65b29d231d37bea354b25c05daec07e2","externalIds":{"ArXiv":"2210.13701","DBLP":"conf/emnlp/ChenZC22","ACL":"2022.emnlp-main.146","DOI":"10.48550/arXiv.2210.13701","CorpusId":253107178},"title":"Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e","externalIds":{"ArXiv":"2210.08726","DBLP":"conf/acl/GaoDPCCFZLLJG23","ACL":"2023.acl-long.910","DOI":"10.18653/v1/2023.acl-long.910","CorpusId":254247260},"title":"RARR: Researching and Revising What Language Models Say, Using Language Models"},{"paperId":"452cf9f8b756995363baa659976b30cb09893c89","externalIds":{"DBLP":"conf/nips/LiGK22","ArXiv":"2210.05758","DOI":"10.48550/arXiv.2210.05758","CorpusId":252846580},"title":"Decoupled Context Processing for Context Augmented Language Modeling"},{"paperId":"7471cb40a33e9d971a922b5dff5ca9b4a73ca609","externalIds":{"DBLP":"journals/corr/abs-2210-03329","ArXiv":"2210.03329","DOI":"10.48550/arXiv.2210.03329","CorpusId":252762125},"title":"Calibrating Factual Knowledge in Pretrained Language Models"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","externalIds":{"DBLP":"conf/iclr/YaoZYDSN023","ArXiv":"2210.03629","CorpusId":252762395},"title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"07955e96cbd778d0ae2a68f09d073b866dd84c2a","externalIds":{"ArXiv":"2210.02406","DBLP":"conf/iclr/KhotTFF0CS23","DOI":"10.48550/arXiv.2210.02406","CorpusId":252715485},"title":"Decomposed Prompting: A Modular Approach for Solving Complex Tasks"},{"paperId":"f8770b2097e9c197a4d60253d696bd117d7ec883","externalIds":{"ACL":"2022.coling-1.138","DBLP":"conf/coling/SenAS22","ArXiv":"2210.01613","DOI":"10.48550/arXiv.2210.01613","CorpusId":252693442},"title":"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering"},{"paperId":"44279244407a64431810f982be6d0c7da4429dd7","externalIds":{"ArXiv":"2210.10341","DBLP":"journals/bib/LuoSXQZPL22","DOI":"10.1093/bib/bbac409","CorpusId":252542956,"PubMed":"36156661"},"title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"},{"paperId":"b2542a738b75ee9b7ce1a13d8b78f9095d212412","externalIds":{"ArXiv":"2209.10063","DBLP":"conf/iclr/0002IWXJ000023","CorpusId":252408513},"title":"Generate rather than Retrieve: Large Language Models are Strong Context Generators"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"ae4422ec1e8746c9c0bc03ccf5a3b828da2889b4","externalIds":{"ArXiv":"2209.02970","DBLP":"journals/corr/abs-2209-02970","DOI":"10.48550/arXiv.2209.02970","CorpusId":252111085},"title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence"},{"paperId":"916be31cbf847faa65cad0549e153f0c25b9f424","externalIds":{"ArXiv":"2208.03299","DBLP":"journals/jmlr/IzacardLLHPSDJRG23","CorpusId":251371732},"title":"Few-shot Learning with Retrieval Augmented Language Models"},{"paperId":"dc5518e1db565a8c52084f27353461df474403d8","externalIds":{"DBLP":"conf/nips/KasaiST0A0RS0I23","ArXiv":"2207.13332","DOI":"10.48550/arXiv.2207.13332","CorpusId":251105205},"title":"RealTime QA: What's the Answer Right Now?"},{"paperId":"1d650f1afd45c59ff907396fe8b678595dcb85ea","externalIds":{"DBLP":"conf/icml/MitchellLBMF22","ArXiv":"2206.06520","CorpusId":249642147},"title":"Memory-Based Model Editing at Scale"},{"paperId":"f12a6168ed8de1aee69fee51b469b1aecd5f903e","externalIds":{"ArXiv":"2206.04624","DBLP":"conf/nips/LeePXPFSC22","DOI":"10.48550/arXiv.2206.04624","CorpusId":249538460},"title":"Factuality Enhanced Language Models for Open-Ended Text Generation"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","externalIds":{"ArXiv":"2206.04615","DBLP":"journals/corr/abs-2206-04615","CorpusId":263625818},"title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"374dd173491a59a10bbb2b3519ebcfe3649f529d","externalIds":{"DBLP":"journals/corr/abs-2205-14334","ArXiv":"2205.14334","DOI":"10.48550/arXiv.2205.14334","CorpusId":249191391},"title":"Teaching Models to Express Their Uncertainty in Words"},{"paperId":"eea2129457fcd78c4071a9020355a2fe1da4d2fd","externalIds":{"DBLP":"journals/corr/abs-2205-08184","ArXiv":"2205.08184","ACL":"2022.naacl-main.113","DOI":"10.18653/v1/2022.naacl-main.113","CorpusId":248834551},"title":"SKILL: Structured Knowledge Infusion for Large Language Models"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"6ae63be462816bdd0da32894985d16f1e8cbdd5c","externalIds":{"DBLP":"journals/corr/abs-2204-10555","ArXiv":"2204.10555","ACL":"2022.naacl-main.379","DOI":"10.48550/arXiv.2204.10555","CorpusId":248366611},"title":"KALA: Knowledge-Augmented Language Model Adaptation"},{"paperId":"fd78047e4b169c8dd67879524275d0ae35903354","externalIds":{"DBLP":"conf/nlpcc/WangLLXHZ23","ArXiv":"2204.07994","DOI":"10.1007/978-3-031-44696-2_35","CorpusId":248227557},"title":"Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base"},{"paperId":"fcd7bdf37cec0845f6960aa1ab87685b73fcfd2f","externalIds":{"ACL":"2022.emnlp-main.566","DBLP":"conf/emnlp/StelmakhLDC22","ArXiv":"2204.06092","DOI":"10.48550/arXiv.2204.06092","CorpusId":248157463},"title":"ASQA: Factoid Questions Meet Long-Form Answers"},{"paperId":"cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","externalIds":{"ArXiv":"2204.06031","DBLP":"journals/corr/abs-2204-06031","DOI":"10.48550/arXiv.2204.06031","CorpusId":248157206},"title":"A Review on Language Models as Knowledge Bases"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cf36236015c9f93f15bfafbf282f69e08bdc9c16","externalIds":{"DBLP":"conf/emnlp/GevaCWG22","ArXiv":"2203.14680","ACL":"2022.emnlp-main.3","DOI":"10.48550/arXiv.2203.14680","CorpusId":247762385},"title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","externalIds":{"DBLP":"conf/iclr/0002WSLCNCZ23","ArXiv":"2203.11171","CorpusId":247595263},"title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"8666f9f379389a5dff31e72fb0f992a37763ba41","externalIds":{"ArXiv":"2203.11147","DBLP":"journals/corr/abs-2203-11147","DOI":"10.48550/arXiv.2203.11147","CorpusId":247594830},"title":"Teaching language models to support answers with verified quotes"},{"paperId":"b0ceba96db8b7b71beb2f0906568ffc143fddbf9","externalIds":{"ArXiv":"2202.11912","DBLP":"journals/corr/abs-2202-11912","CorpusId":247084007},"title":"A Rigorous Study of Integrated Gradients Method and Extensions to Internal Neuron Attributions"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","externalIds":{"DBLP":"conf/nips/MengBAB22","ArXiv":"2202.05262","CorpusId":255825985},"title":"Locating and Editing Factual Associations in GPT"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"07283e1578a50e15ac66efcc35b4ae0cbf2159ef","externalIds":{"DBLP":"journals/corr/abs-2112-12870","ArXiv":"2112.12870","DOI":"10.1162/coli_a_00486","CorpusId":245502761},"title":"Measuring Attribution in Natural Language Generation Models"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","externalIds":{"DBLP":"journals/corr/abs-2112-09332","ArXiv":"2112.09332","CorpusId":245329531},"title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"4f4a409f701f7552d45c46a5b0fea69dca6f8e84","externalIds":{"ArXiv":"2112.09118","DBLP":"journals/tmlr/IzacardCHRBJG22","CorpusId":249097975},"title":"Unsupervised Dense Information Retrieval with Contrastive Learning"},{"paperId":"1acdb41d8a17cff5593495410a7d4b9f44972a82","externalIds":{"ArXiv":"2112.08713","DBLP":"journals/corr/abs-2112-08713","ACL":"2022.naacl-main.415","DOI":"10.18653/v1/2022.naacl-main.415","CorpusId":245218843},"title":"CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning"},{"paperId":"002c256d30d6be4b23d365a8de8ae0e67e4c9641","externalIds":{"DBLP":"journals/corr/abs-2112-04426","ArXiv":"2112.04426","CorpusId":244954723},"title":"Improving language models by retrieving from trillions of tokens"},{"paperId":"68f141724814839d556a989646194be88641b143","externalIds":{"ArXiv":"2112.11446","DBLP":"journals/corr/abs-2112-11446","CorpusId":245353475},"title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"9286ac6e9b1aacd7d93496eb4615ae7678876d2a","externalIds":{"DBLP":"journals/corr/abs-2110-11309","ArXiv":"2110.11309","CorpusId":239050360},"title":"Fast Model Editing at Scale"},{"paperId":"dbeff5429ff0caa85f9e02621928e787e789ca2b","externalIds":{"ArXiv":"2110.08542","DBLP":"conf/acl/Khot0KS22","ACL":"2022.findings-acl.142","DOI":"10.18653/v1/2022.findings-acl.142","CorpusId":248666080},"title":"Hey AI, Can You Solve Complex Tasks by Talking to Agents?"},{"paperId":"238deab37e201c57505a4a47bb854e462af79bd7","externalIds":{"DBLP":"conf/emnlp/LongprePCRD021","ACL":"2021.emnlp-main.565","ArXiv":"2109.05052","DOI":"10.18653/v1/2021.emnlp-main.565","CorpusId":237491581},"title":"Entity-Based Knowledge Conflicts in Question Answering"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","externalIds":{"DBLP":"journals/corr/abs-2109-07958","ACL":"2022.acl-long.229","ArXiv":"2109.07958","DOI":"10.18653/v1/2022.acl-long.229","CorpusId":237532606},"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"b808b6ddf511977e9a33dbe01b412a02b6092ae0","externalIds":{"DBLP":"conf/acl/CaoDC22","ArXiv":"2109.09784","ACL":"2022.acl-long.236","DOI":"10.18653/v1/2022.acl-long.236","CorpusId":244909449},"title":"Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization"},{"paperId":"ec307b17f193b14292206b65a1bcc95bfd8f02ed","externalIds":{"DBLP":"journals/tacl/TrivediBKS22","ACL":"2022.tacl-1.31","ArXiv":"2108.00573","DOI":"10.1162/tacl_a_00475","CorpusId":236771976},"title":"‚ô´ MuSiQue: Multihop Questions via Single-hop Question Composition"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","externalIds":{"ACL":"2022.acl-long.577","DBLP":"journals/corr/abs-2107-06499","ArXiv":"2107.06499","DOI":"10.18653/v1/2022.acl-long.577","CorpusId":235829052},"title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","externalIds":{"DBLP":"conf/nips/YuanNL21","ArXiv":"2106.11520","CorpusId":235593404},"title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"6d4a9f1c41b078846901362ba0dce8295dd6a2a8","externalIds":{"ArXiv":"2106.05346","DBLP":"journals/corr/abs-2106-05346","CorpusId":235390519},"title":"End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering"},{"paperId":"f62acd332fd7a6f35b117ed4ffaf93b19483dcf7","externalIds":{"ACL":"2021.acl-long.251","DBLP":"journals/corr/abs-2106-01561","ArXiv":"2106.01561","DOI":"10.18653/v1/2021.acl-long.251","CorpusId":235313524},"title":"Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?"},{"paperId":"1a1e99514d8d175459f7c61cfd0c394b46e63359","externalIds":{"MAG":"3171434230","ACL":"2021.naacl-main.278","DBLP":"conf/naacl/AgarwalGSA21","DOI":"10.18653/V1/2021.NAACL-MAIN.278","CorpusId":263864711},"title":"Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training"},{"paperId":"2c871df72c52b58f05447fcb3afc838168d94505","externalIds":{"ArXiv":"2104.08696","DBLP":"journals/corr/abs-2104-08696","ACL":"2022.acl-long.581","DOI":"10.18653/v1/2022.acl-long.581","CorpusId":233296761},"title":"Knowledge Neurons in Pretrained Transformers"},{"paperId":"240b0caabb415578bdea4da7d0a32bdff2e8163f","externalIds":{"DBLP":"journals/corr/abs-2104-08164","ArXiv":"2104.08164","ACL":"2021.emnlp-main.522","DOI":"10.18653/v1/2021.emnlp-main.522","CorpusId":233289412},"title":"Editing Factual Knowledge in Language Models"},{"paperId":"370c97aa9521b3052b60369ee6c7b61f120f976d","externalIds":{"ArXiv":"2104.04725","ACL":"2021.naacl-main.32","DBLP":"conf/naacl/EisenschlosDBBB21","MAG":"3172045361","DOI":"10.18653/V1/2021.NAACL-MAIN.32","CorpusId":233210218},"title":"Fool Me Twice: Entailment from Wikipedia Gamification"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","externalIds":{"MAG":"3212496002","DOI":"10.5281/ZENODO.5297715","CorpusId":245758737},"title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"ca2f1088d3e581b2c6c75cf0ebc96506d620f64d","externalIds":{"DBLP":"conf/fat/BenderGMS21","DOI":"10.1145/3442188.3445922","CorpusId":262580630},"title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú"},{"paperId":"346081161bdc8f18e2a4c4af7f51d35452b5cb01","externalIds":{"ArXiv":"2101.02235","DBLP":"journals/tacl/GevaKSKRB21","DOI":"10.1162/tacl_a_00370","CorpusId":230799347},"title":"Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","externalIds":{"DBLP":"journals/corr/abs-2101-00027","ArXiv":"2101.00027","CorpusId":230435736},"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"03cb8234036dedd356901f574c1771a88e3578d8","externalIds":{"DBLP":"conf/acl/Thorne020","ArXiv":"2012.15788","ACL":"2021.acl-long.256","DOI":"10.18653/v1/2021.acl-long.256","CorpusId":235294035},"title":"Evidence-based Factual Error Correction"},{"paperId":"4a54d58a4b20e4f3af25cea3c188a12082a95e02","externalIds":{"DBLP":"conf/emnlp/GevaSBL21","ACL":"2021.emnlp-main.446","ArXiv":"2012.14913","DOI":"10.18653/v1/2021.emnlp-main.446","CorpusId":229923720},"title":"Transformer Feed-Forward Layers Are Key-Value Memories"},{"paperId":"2b4bc49a3b23229a060609380752666b24b435fb","externalIds":{"DBLP":"conf/iclr/IzacardG21","ArXiv":"2012.04584","MAG":"3111425437","CorpusId":227746078},"title":"Distilling Knowledge from Reader to Retriever for Question Answering"},{"paperId":"addd2d86d19c1e7c8854e827fb2656a50c250440","externalIds":{"DBLP":"journals/tacl/HayashiBWANN21","MAG":"3099222060","ArXiv":"2011.07832","DOI":"10.1162/tacl_a_00362","CorpusId":226965071},"title":"WikiAsp: A Dataset for Multi-domain Aspect-based Summarization"},{"paperId":"9001eb3c3d5a96ad3d804410c2437e6f60feade9","externalIds":{"ArXiv":"2011.01060","MAG":"3101823922","ACL":"2020.coling-main.580","DBLP":"journals/corr/abs-2011-01060","DOI":"10.18653/V1/2020.COLING-MAIN.580","CorpusId":226236740},"title":"Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps"},{"paperId":"1f8b1fe3a13169d73a5cdc3e0df47f244c722581","externalIds":{"ArXiv":"2011.03088","ACL":"2020.findings-emnlp.309","DBLP":"conf/emnlp/JiangBZD0B20","MAG":"3105886116","DOI":"10.18653/v1/2020.findings-emnlp.309","CorpusId":226278099},"title":"HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification"},{"paperId":"01a1f2df34d947d7aa5698ca6fb31c03d15a5183","externalIds":{"DBLP":"journals/corr/abs-2011-07127","ACL":"2020.emnlp-main.86","ArXiv":"2011.07127","MAG":"3099524945","DOI":"10.18653/v1/2020.emnlp-main.86","CorpusId":226262208},"title":"IIRC: A Dataset of Incomplete Information Reading Comprehension Questions"},{"paperId":"93d3e45395117e21214d404c8753b578c29266d1","externalIds":{"DBLP":"journals/corr/abs-2010-10439","MAG":"3094282562","ArXiv":"2010.10439","CorpusId":224803601},"title":"Open Question Answering over Tables and Text"},{"paperId":"398a0625e8707a0b41ac58eaec51e8feb87dd7cb","externalIds":{"DBLP":"journals/corr/abs-2010-03768","MAG":"3092516542","ArXiv":"2010.03768","CorpusId":222208810},"title":"ALFWorld: Aligning Text and Embodied Environments for Interactive Learning"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","externalIds":{"MAG":"3088599783","ACL":"2020.findings-emnlp.301","DBLP":"journals/corr/abs-2009-11462","ArXiv":"2009.11462","DOI":"10.18653/v1/2020.findings-emnlp.301","CorpusId":221878771},"title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"3af229f3da1bd24378fd8d76c88fd393b4464058","externalIds":{"MAG":"3083182073","DBLP":"journals/corr/abs-2009-02252","ArXiv":"2009.02252","ACL":"2021.naacl-main.200","DOI":"10.18653/V1/2021.NAACL-MAIN.200","CorpusId":221507798},"title":"KILT: a Benchmark for Knowledge Intensive Language Tasks"},{"paperId":"ea8c46e193d5121e440daf96edfd15a47151c293","externalIds":{"ACL":"2021.eacl-main.74","ArXiv":"2007.01282","MAG":"3039017601","DBLP":"conf/eacl/IzacardG21","DOI":"10.18653/v1/2021.eacl-main.74","CorpusId":220302360},"title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"e23cb51f50f749320b9122fb5f75113b4d192c0a","externalIds":{"DBLP":"journals/corr/abs-2006-14799","ArXiv":"2006.14799","MAG":"3037013468","CorpusId":220128348},"title":"Evaluation of Text Generation: A Survey"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"07c1c2429b63fefdae41eb546c31b40de2a880f7","externalIds":{"MAG":"3035275890","ArXiv":"2005.06117","ACL":"2020.acl-main.210","DBLP":"conf/acl/GuptaMNS20","DOI":"10.18653/v1/2020.acl-main.210","CorpusId":218614095},"title":"INFOTABS: Inference on Tables as Semi-structured Data"},{"paperId":"dbeeca8466e0c177ec67c60d529899232415ca87","externalIds":{"DBLP":"conf/acl/MaynezNBM20","MAG":"3021338988","ArXiv":"2005.00661","ACL":"2020.acl-main.173","DOI":"10.18653/v1/2020.acl-main.173","CorpusId":218487034},"title":"On Faithfulness and Factuality in Abstractive Summarization"},{"paperId":"f2f3c83db919a2429c4fcad2d0a0ed4e5294354a","externalIds":{"MAG":"3104215796","ArXiv":"2004.12651","ACL":"2020.emnlp-main.634","DBLP":"conf/emnlp/ChenHCCLY20","DOI":"10.18653/v1/2020.emnlp-main.634","CorpusId":216553067},"title":"Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting"},{"paperId":"302face5b5a0944cab13665a2d4e07ef3aaf5240","externalIds":{"MAG":"3019779721","DBLP":"conf/emnlp/MinMHZ20","ACL":"2020.emnlp-main.466","ArXiv":"2004.10645","DOI":"10.18653/v1/2020.emnlp-main.466","CorpusId":216056269},"title":"AmbigQA: Answering Ambiguous Open-domain Questions"},{"paperId":"270f3bea8ca801870a6cc56b4d36f7f2019c9ed0","externalIds":{"MAG":"3105163367","DBLP":"journals/corr/abs-2004-09297","ArXiv":"2004.09297","CorpusId":215827489},"title":"MPNet: Masked and Permuted Pre-training for Language Understanding"},{"paperId":"016368185723d0ec99aafa4b5927300590d0647f","externalIds":{"MAG":"3016309009","DBLP":"journals/corr/abs-2004-07202","ACL":"2020.emnlp-main.400","ArXiv":"2004.07202","DOI":"10.18653/v1/2020.emnlp-main.400","CorpusId":215768768},"title":"Entities as Experts: Sparse Memory Access with Entity Supervision"},{"paperId":"4ae52766028e69186052ea8f33a137fbbbdb986a","externalIds":{"MAG":"3035252911","ArXiv":"2004.04696","DBLP":"conf/acl/SellamDP20","ACL":"2020.acl-main.704","DOI":"10.18653/v1/2020.acl-main.704","CorpusId":215548699},"title":"BLEURT: Learning Robust Metrics for Text Generation"},{"paperId":"027946f80f3cb276ea38bc2cf19903052f59cd0e","externalIds":{"MAG":"2983088655","ACL":"2020.findings-emnlp.22","DBLP":"conf/emnlp/MassarelliPPORP20","ArXiv":"1911.03587","DOI":"10.18653/v1/2020.findings-emnlp.22","CorpusId":207852724},"title":"How Decoding Strategies Affect the Verifiability of Generated Text"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","externalIds":{"MAG":"2973727699","ArXiv":"1909.08053","DBLP":"journals/corr/abs-1909-08053","CorpusId":202660670},"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","externalIds":{"DBLP":"journals/corr/abs-1909-01066","MAG":"2996758945","ArXiv":"1909.01066","ACL":"D19-1250","DOI":"10.18653/v1/D19-1250","CorpusId":202539551},"title":"Language Models as Knowledge Bases?"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","externalIds":{"ACL":"Q19-1026","MAG":"2912924812","DBLP":"journals/tacl/KwiatkowskiPRCP19","DOI":"10.1162/tacl_a_00276","CorpusId":86611921},"title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"5963f30b31dbeac0ecf052fa33638f87b2c1b6c7","externalIds":{"DOI":"10.7191/soc.2019.1038","CorpusId":240768975},"title":"Again"},{"paperId":"ebf59587f8f170ff4241c42263bbfb9da5bd2135","externalIds":{"MAG":"2964040452","DBLP":"conf/acl/FanJPGWA19","ACL":"P19-1346","ArXiv":"1907.09190","DOI":"10.18653/v1/P19-1346","CorpusId":196170479},"title":"ELI5: Long Form Question Answering"},{"paperId":"2dd939fe52a336451c83b4e048660f2a5a048265","externalIds":{"DBLP":"conf/naacl/JiangW019","MAG":"2946088473","ACL":"N19-1028","DOI":"10.18653/v1/N19-1028","CorpusId":174800890},"title":"FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"227458886343b86bd15adf58c769be326b4b058a","externalIds":{"MAG":"2898875342","DBLP":"conf/iclr/DinanRSFAW19","ArXiv":"1811.01241","CorpusId":53218829},"title":"Wizard of Wikipedia: Knowledge-Powered Conversational agents"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","externalIds":{"ACL":"D18-1259","DBLP":"journals/corr/abs-1809-09600","MAG":"2952862139","ArXiv":"1809.09600","DOI":"10.18653/v1/D18-1259","CorpusId":52822214},"title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25","externalIds":{"MAG":"2785611959","DBLP":"conf/lrec/ElSaharVRGHLS18","ACL":"L18-1544","CorpusId":4612975},"title":"T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"},{"paperId":"24d0b13f6daf0ec327beed4440d84ea5862383d7","externalIds":{"MAG":"2798898418","DBLP":"conf/www/JiaARSW18","DOI":"10.1145/3184558.3191536","CorpusId":13865788},"title":"TempQuestions: A Benchmark for Temporal Question Answering"},{"paperId":"b1d24e8e08435b7c52335485a0d635abf9bc604c","externalIds":{"MAG":"2789566302","ACL":"N18-1074","ArXiv":"1803.05355","DBLP":"conf/naacl/ThorneVCM18","DOI":"10.18653/v1/N18-1074","CorpusId":4711425},"title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification"},{"paperId":"dd06c454aa84444b87f147ca7a2ef1256dce90ba","externalIds":{"DBLP":"reference/db/X09kh","DOI":"10.1007/978-0-387-39940-9_2328","CorpusId":219541572},"title":"Curation"},{"paperId":"7d5cf22c70484fe217936c66741fb73b2a278bde","externalIds":{"MAG":"2765383698","ACL":"Q18-1021","DBLP":"journals/corr/abs-1710-06481","ArXiv":"1710.06481","DOI":"10.1162/tacl_a_00021","CorpusId":9192723},"title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents"},{"paperId":"fa025e5d117929361bcf798437957762eb5bb6d4","externalIds":{"DBLP":"conf/conll/LevySCZ17","MAG":"2962881743","ACL":"K17-1034","ArXiv":"1706.04115","DOI":"10.18653/v1/K17-1034","CorpusId":793385},"title":"Zero-Shot Relation Extraction via Reading Comprehension"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","externalIds":{"MAG":"2612431505","ArXiv":"1705.03551","ACL":"P17-1147","DBLP":"journals/corr/JoshiCWZ17","DOI":"10.18653/v1/P17-1147","CorpusId":26501419},"title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"339c6e6d46836c173fb6a23b493c724896d4cc70","externalIds":{"MAG":"2963527228","DBLP":"conf/acl/LoweNSABP17","ACL":"P17-1103","ArXiv":"1708.07149","DOI":"10.18653/v1/P17-1103","CorpusId":1880070},"title":"Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","externalIds":{"MAG":"2561529111","DBLP":"journals/corr/SpeerCH16","ArXiv":"1612.03975","DOI":"10.1609/aaai.v31i1.11164","CorpusId":15206880},"title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"efbd381493bb9636f489b965a2034d529cd56bcd","externalIds":{"ArXiv":"1609.07843","MAG":"2525332836","DBLP":"journals/corr/MerityXBS16","CorpusId":16299141},"title":"Pointer Sentinel Mixture Models"},{"paperId":"5ed791f810da580c78df6a052c6b9f2e258f6b0a","externalIds":{"MAG":"2952915793","ArXiv":"1606.06031","DBLP":"conf/acl/PapernoKLPBPBBF16","ACL":"P16-1144","DOI":"10.18653/v1/P16-1144","CorpusId":2381275},"title":"The LAMBADA dataset: Word prediction requiring a broad discourse context"},{"paperId":"604764133befe7a0aaa692919545846197e6e065","externalIds":{"MAG":"2950539144","DBLP":"conf/emnlp/LebretGA16","ACL":"D16-1128","DOI":"10.18653/v1/D16-1128","CorpusId":1238927},"title":"Neural Text Generation from Structured Data with Application to the Biography Domain"},{"paperId":"6447bfcda1dfb2fa8484683711af92b7cbaeca2b","externalIds":{"DBLP":"journals/sigkdd/LiGMLSZFH15","MAG":"2951361729","ArXiv":"1505.02463","DOI":"10.1145/2897350.2897352","CorpusId":9060471},"title":"A Survey on Truth Discovery"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","externalIds":{"MAG":"2080133951","DBLP":"journals/cacm/VrandecicK14","DOI":"10.1145/2629489","CorpusId":14494942},"title":"Wikidata"},{"paperId":"0407b605b8f55db72e2545586bfe8e946b691b70","externalIds":{"MAG":"2113839990","DBLP":"journals/corr/GoodfellowMDCB13","ArXiv":"1312.6211","CorpusId":12730344},"title":"An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","externalIds":{"ACL":"D13-1160","MAG":"2252136820","DBLP":"conf/emnlp/BerantCFL13","DOI":"10.18653/v1/d13-1160","CorpusId":6401679},"title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"7b36c5602930abf08efd2867f92cdb48a1be757a","externalIds":{"DOI":"10.1080/10131752.2012.730185","CorpusId":218498893},"title":"Together"},{"paperId":"38e61d9a65aa483ad0fb4a219fe54d4e6a2f6c36","externalIds":{"MAG":"2735575534","DBLP":"journals/aim/McCarthyMRS06","DOI":"10.1609/aimag.v27i4.1904","CorpusId":19439915},"title":"A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"b1e7bf85c7caf1306fa27802218a8e2cdc8f4268","externalIds":{"DBLP":"journals/cacm/Lenat95","MAG":"2527371576","DOI":"10.1145/219717.219745","CorpusId":16147141},"title":"CYC: a large-scale investment in knowledge infrastructure"},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","externalIds":{"MAG":"2081580037","DBLP":"conf/naacl/Miller92","ACL":"H92-1116","DOI":"10.1145/219717.219748","CorpusId":1671874},"title":"WordNet: A Lexical Database for English"},{"paperId":"4bd970a37c59c97804ff93cbb2c108e081de3a37","externalIds":{"MAG":"2102381086","DOI":"10.1093/IJL/3.4.235","CorpusId":2146137},"title":"Introduction to WordNet: An On-line Lexical Database"},{"paperId":"476a27dd2ebc6ac53d9732788f05586a5ed85ac0","externalIds":{"DOI":"10.1177/0075424290023001-218","CorpusId":262508884},"title":"According to"},{"paperId":"af465996da89a302fae95c2fe22e54d2b79e4ac3","externalIds":{"DBLP":"books/ox/90/NewellS90","MAG":"1968079292","DOI":"10.1145/360018.360022","CorpusId":5581562},"title":"Computer science as empirical inquiry: symbols and search"},{"paperId":"6b6dc1167d8f6f44f230309e9c27e4268578f1f7","externalIds":{"MAG":"2799518931","DOI":"10.2307/2982638","CorpusId":67110416},"title":"Transmission of Information: A Statistical Theory of Communications."},{"paperId":"9e463eefadbcd336c69270a299666e4104d50159","externalIds":{"MAG":"2053154970","DOI":"10.1177/001316446002000104","CorpusId":15926286},"title":"A Coefficient of Agreement for Nominal Scales"},{"paperId":"b78aa1fbe406020f42c193b2fad8d635123b9d2e","externalIds":{"DBLP":"journals/corr/abs-2305-12421","DOI":"10.48550/arXiv.2305.12421","CorpusId":258833523},"title":"Evaluating Open Question Answering Evaluation"},{"paperId":"4972b88f8f324a4fa18e921f62a9857af2b5fc7b","externalIds":{"DBLP":"conf/acl/MuennighoffWSRB23","ACL":"2023.acl-long.891","DOI":"10.18653/v1/2023.acl-long.891","CorpusId":253264914},"title":"Crosslingual Generalization through Multitask Finetuning"},{"paperId":"5e761e9f5cd9672a181b256299cd2916a8079461","externalIds":{"DBLP":"journals/corr/abs-2309-02233","DOI":"10.48550/arXiv.2309.02233","CorpusId":261557644},"title":"Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering"},{"paperId":"54b9261eb9752bc76d890f420638e10f63f1955c","externalIds":{"DBLP":"journals/corr/abs-2306-05064","DOI":"10.48550/arXiv.2306.05064","CorpusId":263886801},"title":"Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization"},{"paperId":"6470b561d3426714847fd9201c8ea4ab8585fb96","externalIds":{"CorpusId":257535768},"title":"GeneTuring tests GPT models in genomics"},{"paperId":"ac771182d1780c863954243809d1e144433919f9","externalIds":{"DBLP":"journals/corr/abs-2307-12966","DOI":"10.48550/arXiv.2307.12966","CorpusId":260356605},"title":"Aligning Large Language Models with Human: A Survey"},{"paperId":"9815f586f329a098df83ca872799880bd5cb1a15","externalIds":{"DBLP":"journals/corr/abs-2305-13300","DOI":"10.48550/arXiv.2305.13300","CorpusId":258832560},"title":"Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes"},{"paperId":"2cad608200ee31884f6392e6290878c236f06248","externalIds":{"DBLP":"conf/acl-clinicalnlp/TangCG23","ACL":"2023.clinicalnlp-1.7","DOI":"10.18653/v1/2023.clinicalnlp-1.7","CorpusId":259833772},"title":"Aligning Factual Consistency for Clinical Studies Summarization through Reinforcement Learning"},{"paperId":"4aae0d84fdc1d069d9cf549ca2a6672554d04f4c","externalIds":{"DBLP":"journals/corr/abs-2306-04136","ACL":"2023.nlrse-1.7","DOI":"10.18653/v1/2023.nlrse-1.7","CorpusId":259095910},"title":"Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering"},{"paperId":"d05fe60fedc803df9b2088203694bf9791487766","externalIds":{"DBLP":"journals/corr/abs-2305-14766","DOI":"10.48550/arXiv.2305.14766","CorpusId":281304760},"title":"BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver"},{"paperId":"9e51474fe259fa08446e998d4efcdf246f66c3be","externalIds":{"ACL":"2022.naacl-main.90","DBLP":"conf/naacl/0003TVB22","DOI":"10.18653/v1/2022.naacl-main.90","CorpusId":250390687},"title":"MultiSpanQA: A Dataset for Multi-Span Question Answering"},{"paperId":"55adc6c9ad132d814e8c6e81b4e229fc9e6bcb82","externalIds":{"DBLP":"journals/corr/abs-2211-00635","DOI":"10.48550/arXiv.2211.00635","CorpusId":263883564},"title":"Preserving In-Context Learning ability in Large Language Model Fine-tuning"},{"paperId":"7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7","externalIds":{"DBLP":"conf/iclr/JongZFSC22","ArXiv":"2110.06176","CorpusId":248545122},"title":"MENTION MEMORY : INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH ENTITY MENTION ATTENTION"},{"paperId":"48151071d50240df76df400910dbcb5c65dabc07","externalIds":{"ACL":"2021.fever-1.1","DOI":"10.18653/v1/2021.fever-1.1","CorpusId":241583576},"title":"The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"920a003217d9dbc4c68e3097b4f9f54dc73c16de","externalIds":{"CorpusId":221493446},"title":"Grounded Response Generation Task at DSTC7"},{"paperId":"0c0a778e6fdf7e36b1750c533dcc916f86608607","externalIds":{"MAG":"2527310337","DBLP":"journals/tkde/XunJGZ17","DOI":"10.1109/TKDE.2016.2614508","CorpusId":13490401},"title":"A Survey on Context Learning"},{"paperId":"5e537c4d988d55f74d0bd5bb5015208977fc52e6","externalIds":{"CorpusId":126210996},"title":"FWDselect : Variable selection algorithm in regression models"},{"paperId":"6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce","externalIds":{"MAG":"2525491769","DOI":"10.1201/b18055-8","CorpusId":208945385},"title":"google,Êàë,Ëê®Â®ú"},{"paperId":"780b7c5e2440459bcc532b251e51c9223037acc2","externalIds":{"MAG":"2620949368","CorpusId":263446282},"title":"Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics"}]}