{"abstract":"This paper introduces a pioneering methodology in autonomous robot control, denoted as LLM-BRAIn, enabling the generation of adaptive behaviors in robots in response to operator commands, while simultaneously considering a multitude of potential future events. Unlike traditional step-by-step behavior generation methods, LLM-BRAIn leverages a behavior tree (BT) format that inherently encompasses various potential outcomes, thus reducing the frequency of model queries and optimizing resource utilization and decision-making time. LLM-BRAIn is a transformer-based Large Language Model (LLM) fine-tuned from the Stanford Alpaca 7B model to generate a BT from text descriptions. The model was trained on 8.5k instruction-following demonstrations generated using GPT-3.5. LLM-BRAIn accurately builds complex robot behavior while remaining small enough to run on the robot’s onboard microcomputer. The model generates structurally and logically correct BTs and can manage instructions not presented in the training set. Experiments revealed no significant subjective differences between BTs generated by LLM-BRAIn and human-created BTs, with participants distinguishing between them correctly in only 4.53 out of 10 cases, indicating performance close to random chance.The proposed approach holds promise for applications across various domains, including mobile robotics, drone operations, robot manipulator systems, and Industry 5.0. This applicability stems from the BT framework’s inherent consideration of numerous potential outcomes, obviating the need for exhaustive deliberation at each step, thereby expediting the robot’s operations. We provide a dataset for LLM-BRAIn replication: huggingface.co/datasets/ArtemLykov/LLM BRAIn dataset."}