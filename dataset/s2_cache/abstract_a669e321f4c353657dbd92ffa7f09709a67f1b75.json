{"abstract":"Knowledge-aware recommendation system has at-tracted considerable interest in academia and industry, which comes in handy to solve the cold-start problem and offer a reliable solution for the business to grow. It's particularly important to consider fairness issues when designing and using those systems. However, we find that though not explicitly introduced to a knowledge graph (KG), sensitive information can be implicitly learned by a recommender and thus leads to unfairness. Most existing debiasing methods require sophisticated model design or can only be applied to specific base models. In this paper, to address the above problems, we propose a method to ensure the fairness of any knowledge-aware recommendation models by introducing a sensitivity graph. Different from the majority of previous studies that only handle a single protected attribute, we also aim to make our method flexible to different combinations of fairness constraints during inference. Specifically, given a knowledge-based recommendation model, we first construct a sensitivity graph by taking protected attributes as nodes and dynamically learned relations between pairs of attributes as edges. Then we merge the sensitivity graph into the original knowledge graph and introduce an adversarial framework to enhance fairness criterion by extracting sensitive information of users from the original KG during the graph representation process, without changing the KG-based recommendation model. Extensive experimental results on two public real-world datasets show that the proposed framework can achieve state-of-the-art performance on improving the fairness of any KG-based recommendation model while only cause trivial overall accuracy declination."}