{"references":[{"paperId":"8a9e443bf8426a881795dc53edf0ea740dcd76d3","externalIds":{"DBLP":"journals/trob/JinSLZYY23","DOI":"10.1109/TRO.2023.3303011","CorpusId":261089205},"title":"Learning a Flexible Neural Energy Function With a Unique Minimum for Globally Stable and Accurate Demonstration Learning"},{"paperId":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","externalIds":{"DBLP":"conf/icml/WangXCWWFEHG24","ArXiv":"2311.01455","DOI":"10.48550/arXiv.2311.01455","CorpusId":264935717},"title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"},{"paperId":"9a8819369283adcb40280dda3528ba0ff39d6247","externalIds":{"DBLP":"journals/tsmc/JiangSHPSL23","DOI":"10.1109/TSMC.2023.3295424","CorpusId":260650124},"title":"Dual Stream Meta Learning for Road Surface Classification and Riding Event Detection on Shared Bikes"},{"paperId":"42706e1e7d1ab5cd55cfcf2249f80eeba79fdec1","externalIds":{"DBLP":"journals/corr/abs-2310-19797","ArXiv":"2310.19797","DOI":"10.48550/arXiv.2310.19797","CorpusId":264825342},"title":"DEFT: Dexterous Fine-Tuning for Real-World Hand Policies"},{"paperId":"54f107c7cdffc43490ba18e3e4313a8b4dff4950","externalIds":{"DBLP":"conf/corl/MandlekarNWANFZ23","ArXiv":"2310.17596","DOI":"10.48550/arXiv.2310.17596","CorpusId":264490765},"title":"MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations"},{"paperId":"dd4dfee7ad7a2ed179f9b2e80b83685b37661dbf","externalIds":{"PubMedCentral":"10620072","DBLP":"journals/nature/LakeB23","DOI":"10.1038/s41586-023-06668-3","CorpusId":264489248,"PubMed":"37880371"},"title":"Human-like systematic generalization through a meta-learning neural network"},{"paperId":"ad4beeecbdfc7dd238eb55e2b19113f62096d95b","externalIds":{"DBLP":"conf/iclr/PuigUSCYPDCHMVG24","ArXiv":"2310.13724","DOI":"10.48550/arXiv.2310.13724","CorpusId":264387711},"title":"Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots"},{"paperId":"45872b94798c3125abfb185b7926689c5e767763","externalIds":{"DBLP":"conf/sigir/Tang00SSCY024","ArXiv":"2310.13023","DOI":"10.1145/3626772.3657775","CorpusId":264405943},"title":"GraphGPT: Graph Instruction Tuning for Large Language Models"},{"paperId":"6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc","externalIds":{"DBLP":"journals/corr/abs-2310-12931","ArXiv":"2310.12931","DOI":"10.48550/arXiv.2310.12931","CorpusId":264306288},"title":"Eureka: Human-Level Reward Design via Coding Large Language Models"},{"paperId":"974f0e1a85c1ece2555718342ff2abb6bcb6a825","externalIds":{"DBLP":"journals/corr/abs-2310-11220","ArXiv":"2310.11220","DOI":"10.48550/arXiv.2310.11220","CorpusId":264172465},"title":"KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models"},{"paperId":"ef7d31137ef06c5be8c2824ecc5af6ce3358cc8f","externalIds":{"DBLP":"journals/corr/abs-2310-08864","ArXiv":"2310.08864","DOI":"10.1109/ICRA57147.2024.10611477","CorpusId":263626099},"title":"Open X-Embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration0"},{"paperId":"c3d14e7a319ab764297a60112ce74af201762a73","externalIds":{"DBLP":"journals/corr/abs-2310-06114","ArXiv":"2310.06114","DOI":"10.48550/arXiv.2310.06114","CorpusId":263830899},"title":"Learning Interactive Real-World Simulators"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","externalIds":{"DBLP":"journals/corr/abs-2310-02239","ArXiv":"2310.02239","DOI":"10.48550/arXiv.2310.02239","CorpusId":263608981},"title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"54814744b42b06c855c97b23de1366e0bcbe775a","externalIds":{"ArXiv":"2309.17421","DBLP":"journals/corr/abs-2309-17421","DOI":"10.48550/arXiv.2309.17421","CorpusId":263310951},"title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)"},{"paperId":"6ec855b6c80a258200e41b65f118c6116ab908a6","externalIds":{"DBLP":"conf/iccv/WangKRPCABFTFJP23","ArXiv":"2309.17024","DOI":"10.1109/ICCV51070.2023.01854","CorpusId":263310718},"title":"HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World"},{"paperId":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","externalIds":{"DBLP":"journals/corr/abs-2309-16609","ArXiv":"2309.16609","DOI":"10.48550/arXiv.2309.16609","CorpusId":263134555},"title":"Qwen Technical Report"},{"paperId":"334d6aa9b6e85d3ed3fb13daf5c2aeda89ee2b27","externalIds":{"ArXiv":"2309.12300","DBLP":"conf/icra/GuzeyDECP24","DOI":"10.1109/ICRA57147.2024.10611407","CorpusId":262084312},"title":"See to Touch: Learning Tactile Dexterity through Visual Incentives"},{"paperId":"a281094d05e96b7cca044fdd87ff7c3c65649e20","externalIds":{"DBLP":"journals/corr/abs-2309-10313","ArXiv":"2309.10313","DOI":"10.48550/arXiv.2309.10313","CorpusId":262055661},"title":"Investigating the Catastrophic Forgetting in Multimodal Large Language Models"},{"paperId":"c96297261467b5daa2d01227496a70d444602434","externalIds":{"DBLP":"journals/corr/abs-2309-10305","ArXiv":"2309.10305","DOI":"10.48550/arXiv.2309.10305","CorpusId":261951743},"title":"Baichuan 2: Open Large-scale Language Models"},{"paperId":"af3ab5da98e0807784b57e321ed887a3666a8ab6","externalIds":{"DBLP":"journals/corr/abs-2309-10020","ArXiv":"2309.10020","DOI":"10.48550/arXiv.2309.10020","CorpusId":262055614},"title":"Multimodal Foundation Models: From Specialists to General-Purpose Assistants"},{"paperId":"570b125788bb43384def312127a0af1b4b13f443","externalIds":{"DBLP":"conf/icra/AnVLHBVKN24","ArXiv":"2309.09818","DOI":"10.1109/ICRA57147.2024.10611277","CorpusId":262045996},"title":"Grasp-Anything: Large-scale Grasp Dataset from Foundation Models"},{"paperId":"0c72450890a54b68d63baa99376131fda8f06cf9","externalIds":{"ArXiv":"2309.07864","DBLP":"journals/corr/abs-2309-07864","DOI":"10.48550/arXiv.2309.07864","CorpusId":261817592},"title":"The Rise and Potential of Large Language Model Based Agents: A Survey"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f","externalIds":{"ArXiv":"2309.05660","DBLP":"journals/corr/abs-2309-05660","DOI":"10.48550/arXiv.2309.05660","CorpusId":261696510},"title":"Hypothesis Search: Inductive Reasoning with Language Models"},{"paperId":"b65144033c6d29103879fb178d8efba610cfcd27","externalIds":{"DBLP":"journals/corr/abs-2309-05689","ArXiv":"2309.05689","DOI":"10.48550/arXiv.2309.05689","CorpusId":261696523},"title":"Large Language Model for Science: A Study on P vs. NP"},{"paperId":"24d52678c887331b9da0368e8a2f58bec07f7203","externalIds":{"DBLP":"journals/corr/abs-2309-04658","ArXiv":"2309.04658","DOI":"10.48550/arXiv.2309.04658","CorpusId":261681932},"title":"Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf"},{"paperId":"f8a2dca1e8fe56e698984c077f7ff58d8ca867e9","externalIds":{"DBLP":"journals/corr/abs-2309-03409","ArXiv":"2309.03409","DOI":"10.48550/arXiv.2309.03409","CorpusId":261582296},"title":"Large Language Models as Optimizers"},{"paperId":"e4bb1b1f97711a7634bf4bff72c56891be2222e6","externalIds":{"DBLP":"journals/corr/abs-2309-02427","ArXiv":"2309.02427","DOI":"10.48550/arXiv.2309.02427","CorpusId":261556862},"title":"Cognitive Architectures for Language Agents"},{"paperId":"316f980cfd2e217234386166a46eb080bf027cdd","externalIds":{"ArXiv":"2309.02561","DBLP":"conf/icra/GaoSXX0IMS24","DOI":"10.1109/ICRA57147.2024.10610090","CorpusId":261556939},"title":"Physically Grounded Vision-Language Models for Robotic Manipulation"},{"paperId":"de7bdb2e79c5298244b6b3ff329ad5e4a77e2dd1","externalIds":{"DBLP":"conf/humanoids/SeoHSBGSZ23","ArXiv":"2309.01952","DOI":"10.1109/Humanoids57100.2023.10375203","CorpusId":261531361},"title":"Deep Imitation Learning for Humanoid Loco-manipulation Through Human Teleoperation"},{"paperId":"cc92496398fbb78646341f95d79e25080ff58b53","externalIds":{"ArXiv":"2309.01352","DBLP":"journals/corr/abs-2309-01352","DOI":"10.48550/arXiv.2309.01352","CorpusId":261530737},"title":"Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning"},{"paperId":"4c2ed9907a3e966b3caadec5dddfde0e4c83f9da","externalIds":{"DBLP":"journals/corr/abs-2309-01093","ArXiv":"2309.01093","DOI":"10.1109/ICCV51070.2023.00285","CorpusId":261531326},"title":"CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection"},{"paperId":"719f574226f34cecc57c72be6082353dd8c2f04d","externalIds":{"DBLP":"journals/natmi/TriantafyllidisALL23","DOI":"10.1038/s42256-023-00709-2","CorpusId":261648252},"title":"Hybrid hierarchical learning for solving complex sequential tasks using the robotic manipulation network ROMAN"},{"paperId":"71f1d46eb773a8d2c2c17a1fba4480c26c51f247","externalIds":{"DOI":"10.1126/science.ade4401","CorpusId":261396009,"PubMed":"37651511"},"title":"A principal odor map unifies diverse tasks in olfactory perception"},{"paperId":"b083a81fcf36e72bc3918bf8efa3de0e6541fc1e","externalIds":{"DBLP":"journals/corr/abs-2308-16741","ArXiv":"2308.16741","DOI":"10.48550/arXiv.2308.16741","CorpusId":261395214},"title":"Socratis: Are large multimodal models emotionally aware?"},{"paperId":"fad72b1e1e3c1a299038f74ce5773ff1152ff914","externalIds":{"DBLP":"conf/corl/ZeYWMGY0LW23","ArXiv":"2308.16891","DOI":"10.48550/arXiv.2308.16891","CorpusId":261396262},"title":"GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields"},{"paperId":"83089d2fd243ce21d13a2430f484a431545f2065","externalIds":{"DBLP":"conf/corl/XieLAJ23","ArXiv":"2308.16893","DOI":"10.48550/arXiv.2308.16893","CorpusId":261396000},"title":"Language-Conditioned Path Planning"},{"paperId":"f1dc0b8b844332f08b7503d9728e4831e8bd3607","externalIds":{"ArXiv":"2308.16493","DBLP":"journals/corr/abs-2308-16493","DOI":"10.48550/arXiv.2308.16493","CorpusId":261397016},"title":"Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception"},{"paperId":"1aee86a54287779617721c97ca9a3692e50cf9c9","externalIds":{"DBLP":"journals/corr/abs-2308-16185","ArXiv":"2308.16185","DOI":"10.1109/ICRA57147.2024.10610498","CorpusId":261339736},"title":"Learning Vision-based Pursuit-Evasion Robot Policies"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","externalIds":{"DBLP":"journals/fcsc/WangMFZYZCTCLZWW24","ArXiv":"2308.11432","DOI":"10.1007/s11704-024-40231-1","CorpusId":261064713},"title":"A survey on large language model based autonomous agents"},{"paperId":"c3925ef53f864e5c30189272f63801248ff1406f","externalIds":{"DBLP":"journals/corr/abs-2308-10141","ArXiv":"2308.10141","DOI":"10.1109/ICCV51070.2023.01444","CorpusId":261048775},"title":"March in Chat: Interactive Prompting for Remote Embodied Referring Expression"},{"paperId":"338d8f3b199abcebc85f34016b0162ab3a9d5310","externalIds":{"DBLP":"journals/corr/abs-2308-07633","ArXiv":"2308.07633","DOI":"10.1162/tacl_a_00704","CorpusId":260900101},"title":"A Survey on Model Compression for Large Language Models"},{"paperId":"451a657dabf80ebc43f6a3be518250b2cd5dfe1a","externalIds":{"DBLP":"journals/corr/abs-2308-07902","ArXiv":"2308.07902","ACL":"2023.ccl-2.8","DOI":"10.48550/arXiv.2308.07902","CorpusId":260899983},"title":"Through the Lens of Core Competency: Survey on Evaluation of Large Language Models"},{"paperId":"3c8444cc4e96bdbe6853b886caf032afd1ee1d20","externalIds":{"ArXiv":"2308.06912","DBLP":"conf/iclr/0002LWGS24","CorpusId":260887420},"title":"CausalLM is not optimal for in-context learning"},{"paperId":"7b4a09463e1fec7d9a49f325035453a67b842773","externalIds":{"DBLP":"journals/corr/abs-2308-07037","ArXiv":"2308.07037","CorpusId":260887475},"title":"Bayesian Flow Networks"},{"paperId":"9b5536c16978e4dbbb6370d75dc0f245db9ed08a","externalIds":{"DBLP":"journals/tcyb/WangZLKS24","DOI":"10.1109/TCYB.2023.3298195","CorpusId":260839978,"PubMed":"37566505"},"title":"Task-Driven Reinforcement Learning With Action Primitives for Long-Horizon Manipulation Skills"},{"paperId":"658cd67a91da86cf451e6f1b015f762b56015172","externalIds":{"DBLP":"conf/aaai/GunjalYB24","ArXiv":"2308.06394","DOI":"10.48550/arXiv.2308.06394","CorpusId":260887222},"title":"Detecting and Preventing Hallucinations in Large Vision Language Models"},{"paperId":"a9a05fdbbc7d469bb4a308c3af39135225a3acba","externalIds":{"DBLP":"conf/nips/MengSPJZQL23","ArXiv":"2308.06262","DOI":"10.48550/arXiv.2308.06262","CorpusId":260866006},"title":"Foundation Model is Efficient Multimodal Multitask Model Selector"},{"paperId":"c60116a51bf66bc363d11b797d97eba84b13cfd7","externalIds":{"DBLP":"journals/corr/abs-2308-04945","ACL":"2024.eacl-demo.23","ArXiv":"2308.04945","DOI":"10.48550/arXiv.2308.04945","CorpusId":260735890},"title":"LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking"},{"paperId":"507acddb0b7f36b83fd7c8bff2f121eb506ac8fb","externalIds":{"DBLP":"journals/corr/abs-2308-04371","ArXiv":"2308.04371","DOI":"10.48550/arXiv.2308.04371","CorpusId":260704428},"title":"Cumulative Reasoning with Large Language Models"},{"paperId":"64a80a33018a0fdc182b06111e32b2e08e186f6a","externalIds":{"ArXiv":"2308.04352","DBLP":"conf/iccv/ZhuMCD0023","DOI":"10.1109/ICCV51070.2023.00272","CorpusId":260704493},"title":"3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment"},{"paperId":"5dbf93a68b7fda600521f046dea35ea8ba9e884f","externalIds":{"DBLP":"journals/corr/abs-2308-03688","ArXiv":"2308.03688","DOI":"10.48550/arXiv.2308.03688","CorpusId":260682249},"title":"AgentBench: Evaluating LLMs as Agents"},{"paperId":"4fb3695d7a3cba3db438cda198c724225ab48a38","externalIds":{"DBLP":"conf/iros/JingZLSYFK23","ArXiv":"2308.03620","DOI":"10.1109/IROS55552.2023.10342201","CorpusId":254198890},"title":"Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods"},{"paperId":"ebbffe5db352a10fde868843b8d5787b87843f09","externalIds":{"ArXiv":"2308.03656","DBLP":"journals/corr/abs-2308-03656","DOI":"10.48550/arXiv.2308.03656","CorpusId":260682960},"title":"Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench"},{"paperId":"863a4cff37f2bbda38bce76689ca3dea5ba04c5b","externalIds":{"DBLP":"journals/corr/abs-2308-03357","ArXiv":"2308.03357","DOI":"10.48550/arXiv.2308.03357","CorpusId":260682539},"title":"Foundation Model based Open Vocabulary Task Planning and Executive System for General Purpose Service Robots"},{"paperId":"7124d130b5a84250c7349f387929f6e065729093","externalIds":{"ArXiv":"2308.03624","DBLP":"conf/iros/YangJWXSCSK23","DOI":"10.1109/IROS55552.2023.10342371","CorpusId":260682913},"title":"MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation"},{"paperId":"ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04","externalIds":{"ArXiv":"2308.03188","DBLP":"journals/corr/abs-2308-03188","DOI":"10.48550/arXiv.2308.03188","CorpusId":260682695},"title":"Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"},{"paperId":"70a75b05a9410198fd71c3a0fe937a77d15d6bc8","externalIds":{"DBLP":"conf/semweb/MihindukulasooriyaTEL23","ArXiv":"2308.02357","DOI":"10.48550/arXiv.2308.02357","CorpusId":260611736},"title":"Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text"},{"paperId":"1e26b42669b060a3850e4766dea0db6e3c85cdec","externalIds":{"DBLP":"journals/corr/abs-2308-01191","ArXiv":"2308.01191","DOI":"10.48550/arXiv.2308.01191","CorpusId":260378810},"title":"Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey"},{"paperId":"7fbc502441d66daf1f53765d5d86a8dfba9ab0ce","externalIds":{"DBLP":"journals/corr/abs-2308-01390","ArXiv":"2308.01390","DOI":"10.48550/arXiv.2308.01390","CorpusId":261043320},"title":"OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"},{"paperId":"5e274869d8b9aaee4fb4af09c3228b711c845d1d","externalIds":{"DBLP":"conf/iros/GuoWYTT0B23","ArXiv":"2308.01477","DOI":"10.1109/IROS55552.2023.10341672","CorpusId":260438513},"title":"HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions"},{"paperId":"7777ffb5f2f1d52b7080725332035b81af2994c9","externalIds":{"ArXiv":"2308.00688","DBLP":"journals/corr/abs-2308-00688","DOI":"10.1109/LRA.2023.3343602","CorpusId":260351368},"title":"AnyLoc: Towards Universal Visual Place Recognition"},{"paperId":"1e6102c981b9464c632ef0b00dbd11dfb0564e4e","externalIds":{"DBLP":"conf/iclr/MiaoTR24","ArXiv":"2308.00436","DOI":"10.48550/arXiv.2308.00436","CorpusId":260350986},"title":"SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","externalIds":{"DBLP":"journals/corr/abs-2308-00692","ArXiv":"2308.00692","DOI":"10.1109/CVPR52733.2024.00915","CorpusId":260351258},"title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"e451bcd5b503d10677188a986f2bdfc3f4f4002d","externalIds":{"DOI":"10.1016/j.eng.2023.07.006","CorpusId":260790651},"title":"The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions"},{"paperId":"7d46a13a1edd02dd6ae2b9f713e6f91ea001dfb4","externalIds":{"DBLP":"journals/corr/abs-2307-16376","ArXiv":"2307.16376","DOI":"10.1007/s11280-024-01276-1","CorpusId":260334118},"title":"When large language models meet personalization: perspectives of challenges and opportunities"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","externalIds":{"DBLP":"journals/corr/abs-2307-16789","ArXiv":"2307.16789","DOI":"10.48550/arXiv.2307.16789","CorpusId":260334759},"title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"38939304bb760473141c2aca0305e44fbe04e6e8","externalIds":{"ArXiv":"2307.15818","DBLP":"conf/corl/ZitkovichYXXXXW23","DOI":"10.48550/arXiv.2307.15818","CorpusId":260293142},"title":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"},{"paperId":"af6d0ba799213cbbcbfceb1fb9b78d2858486308","externalIds":{"DBLP":"journals/corr/abs-2307-14535","ArXiv":"2307.14535","DOI":"10.48550/arXiv.2307.14535","CorpusId":260203080},"title":"Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition"},{"paperId":"e41482f4ee984f17382f6cdd900df094d928be06","externalIds":{"DBLP":"conf/iclr/ZhouX0ZLSCOBF0N24","ArXiv":"2307.13854","DOI":"10.48550/arXiv.2307.13854","CorpusId":260164780},"title":"WebArena: A Realistic Web Environment for Building Autonomous Agents"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","externalIds":{"ArXiv":"2307.13721","DBLP":"journals/corr/abs-2307-13721","DOI":"10.48550/arXiv.2307.13721","CorpusId":260164769},"title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"dd586f20551db661be3e1eeaf4dfc962b0fbdf93","externalIds":{"DBLP":"journals/corr/abs-2307-11343","ArXiv":"2307.11343","DOI":"10.48550/arXiv.2307.11343","CorpusId":260091596},"title":"A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI"},{"paperId":"9f4017de7deded49c032a83d7844efcd9ea1aa21","externalIds":{"DBLP":"conf/nips/GuhaCBMSR23","ArXiv":"2307.11031","DOI":"10.48550/arXiv.2307.11031","CorpusId":259991664},"title":"Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"},{"paperId":"83c48aa341850af478247e3b34ba1ee1db9f1236","externalIds":{"ArXiv":"2307.10802","DBLP":"journals/corr/abs-2307-10802","DOI":"10.48550/arXiv.2307.10802","CorpusId":259991096},"title":"Meta-Transformer: A Unified Framework for Multimodal Learning"},{"paperId":"e01ab53663e5df5961a021506a9cb09f4efc3788","externalIds":{"DBLP":"journals/corr/abs-2307-10169","ArXiv":"2307.10169","DOI":"10.48550/arXiv.2307.10169","CorpusId":259982665},"title":"Challenges and Applications of Large Language Models"},{"paperId":"18b75ea107ed166d7120c12c162b94f02e20b417","externalIds":{"ArXiv":"2307.08689","DBLP":"journals/corr/abs-2307-08689","DOI":"10.48550/arXiv.2307.08689","CorpusId":259936996},"title":"COLLIE: Systematic Construction of Constrained Text Generation Tasks"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","externalIds":{"DBLP":"conf/corl/HuangWZL0023","ArXiv":"2307.05973","DOI":"10.48550/arXiv.2307.05973","CorpusId":259837330},"title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"d64fbe0dabe47bae13bd7a3b0abf838078a375f4","externalIds":{"ArXiv":"2307.05959","DBLP":"journals/corr/abs-2307-05959","DOI":"10.48550/arXiv.2307.05959","CorpusId":259836885},"title":"Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations"},{"paperId":"c5d18dbb92d0cd5393baa1e69de33d6922ac3e57","externalIds":{"DBLP":"conf/icra/MandiJS24","ArXiv":"2307.04738","DOI":"10.1109/ICRA57147.2024.10610855","CorpusId":259501567},"title":"RoCo: Dialectic Multi-Robot Collaboration with Large Language Models"},{"paperId":"d89a1cd530f83f399a0d522215c751c2ca0f3ffd","externalIds":{"DBLP":"conf/rss/ShawAP23","ArXiv":"2309.06440","DOI":"10.15607/RSS.2023.XIX.089","CorpusId":259327055},"title":"LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning"},{"paperId":"3c8de7a0a37fcdea5cabe0f0848319f1bbcfa75a","externalIds":{"DBLP":"conf/icra/LinSMLA24","ArXiv":"2307.03567","DOI":"10.1109/ICRA57147.2024.10610356","CorpusId":259375950},"title":"SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Network"},{"paperId":"888728745dbb769e29ed475d4f7661eebe1a71cf","externalIds":{"DBLP":"journals/tist/ChangWWWYZCYWWYZCYYX24","ArXiv":"2307.03109","DOI":"10.1145/3641289","CorpusId":259360395},"title":"A Survey on Evaluation of Large Language Models"},{"paperId":"587352c3b95c90de6d37f061c8e117f42be0b575","externalIds":{"DBLP":"journals/corr/abs-2307-02485","ArXiv":"2307.02485","DOI":"10.48550/arXiv.2307.02485","CorpusId":259342833},"title":"Building Cooperative Embodied Agents Modularly with Large Language Models"},{"paperId":"d293a75f529d7b1abb161480a95122c9a3ed6376","externalIds":{"DBLP":"journals/tmlr/Deng0LZ23","ArXiv":"2307.01452","DOI":"10.48550/arXiv.2307.01452","CorpusId":259342103},"title":"Causal Reinforcement Learning: A Survey"},{"paperId":"80c698688bb4488beaceaab5c64f701a946cb7ae","externalIds":{"DBLP":"journals/corr/abs-2307-01504","ArXiv":"2307.01504","DOI":"10.1145/3580305.3599256","CorpusId":259341605},"title":"All in One: Multi-Task Prompting for Graph Neural Networks"},{"paperId":"df710c46594c04fb59ef9a93d3b4e1cb387a1b2b","externalIds":{"DBLP":"journals/corr/abs-2307-01848","ArXiv":"2307.01848","DOI":"10.48550/arXiv.2307.01848","CorpusId":259342896},"title":"Embodied Task Planning with Large Language Models"},{"paperId":"d1500f1dbd62e26ef0753f31e845078f58479968","externalIds":{"ArXiv":"2307.01928","DBLP":"conf/corl/RenDBSTBXTXVXSZ23","DOI":"10.48550/arXiv.2307.01928","CorpusId":259342058},"title":"Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners"},{"paperId":"7619a98ef077c8f75e0bfb98953457649209e07e","externalIds":{"ArXiv":"2307.00855","DBLP":"journals/corr/abs-2307-00855","DOI":"10.48550/arXiv.2307.00855","CorpusId":259317124},"title":"Review of Large Vision Models and Visual Prompt Engineering"},{"paperId":"039c49d19493bcda3b0de86fab91f83fa2626c13","externalIds":{"ArXiv":"2307.00666","DBLP":"journals/corr/abs-2307-00666","DOI":"10.48550/arXiv.2307.00666","CorpusId":259316241},"title":"Real-time Vision-based Navigation for a Robot in an Indoor Environment"},{"paperId":"42b920abd44e76d73708859bfe13034555f1f8cb","externalIds":{"ArXiv":"2307.00329","DBLP":"conf/iros/GuoWZC24","DOI":"10.1109/IROS58592.2024.10802284","CorpusId":259317210},"title":"DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment"},{"paperId":"942130a875ccfe55a4c60c27c636f693e25cb13d","externalIds":{"DBLP":"journals/corr/abs-2307-00184","ArXiv":"2307.00184","DOI":"10.48550/arXiv.2307.00184","CorpusId":259317218},"title":"Personality Traits in Large Language Models"},{"paperId":"03251361c1d67c6b5badffc7059fdd7fbfea1fed","externalIds":{"DBLP":"journals/corr/abs-2306-17840","ArXiv":"2306.17840","DOI":"10.1109/ICRA57147.2024.10610634","CorpusId":259309028},"title":"Statler: State-Maintaining Language Models for Embodied Reasoning"},{"paperId":"d87a88548010dd157b20a11e388d5b4f19320cff","externalIds":{"DBLP":"conf/iros/ChenHDLAIKG23","ArXiv":"2306.17157","DOI":"10.1109/IROS55552.2023.10341719","CorpusId":259287275},"title":"FogROS2-SGC: A ROS2 Cloud Robotics Platform for Secure Global Connectivity"},{"paperId":"d9823ffa34f865fb1d0adef95d64a0c352ae125f","externalIds":{"DBLP":"journals/corr/abs-2306-15724","ArXiv":"2306.15724","DOI":"10.48550/arXiv.2306.15724","CorpusId":259274760},"title":"REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction"},{"paperId":"2421734fb9b3f8dfd08c9e078da20b03c406d222","externalIds":{"ArXiv":"2306.15401","CorpusId":259261969},"title":"Explainable Multimodal Emotion Recognition"},{"paperId":"3b6179c293df29e31d31cea46476f104ab6950f2","externalIds":{"DBLP":"conf/iclr/Peng00HHMYW24","ArXiv":"2306.14824","DOI":"10.48550/arXiv.2306.14824","CorpusId":259262263},"title":"Kosmos-2: Grounding Multimodal Large Language Models to the World"},{"paperId":"f94c040b02bdd6cf1b85f374e3912630c66861c3","externalIds":{"ArXiv":"2306.14898","DBLP":"journals/corr/abs-2306-14898","DOI":"10.48550/arXiv.2306.14898","CorpusId":259262186},"title":"InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback"},{"paperId":"c53a121d6c99f8c2add0eddca41262c7fc0bd795","externalIds":{"ArXiv":"2306.14447","DBLP":"journals/corr/abs-2306-14447","DOI":"10.48550/arXiv.2306.14447","CorpusId":259251806},"title":"RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"3efb81de24eb88017d6dbcf22cb4215084223fd8","externalIds":{"ArXiv":"2306.12925","DBLP":"journals/corr/abs-2306-12925","DOI":"10.48550/arXiv.2306.12925","CorpusId":259224345},"title":"AudioPaLM: A Large Language Model That Can Speak and Listen"},{"paperId":"02033e83ff310f35e4623bd339982c52d926f2d5","externalIds":{"DBLP":"journals/tkde/YangCLDW24","ArXiv":"2306.11489","DOI":"10.1109/TKDE.2024.3360454","CorpusId":259203671},"title":"Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling"},{"paperId":"3b0c02955e88f5862e61b560c7f70ba8cf235b1d","externalIds":{"DBLP":"journals/corr/abs-2306-11565","ArXiv":"2306.11565","DOI":"10.48550/arXiv.2306.11565","CorpusId":259203746},"title":"HomeRobot: Open-Vocabulary Mobile Manipulation"},{"paperId":"2562fe379554d201aad312f786903f4c60b68acf","externalIds":{"ArXiv":"2306.11706","CorpusId":259203978},"title":"RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation"},{"paperId":"53aae0a4e001853245800c70bed04b31bc25835b","externalIds":{"ArXiv":"2306.10322","DBLP":"conf/acl/LiangM0HXML24","DOI":"10.18653/v1/2024.findings-acl.745","CorpusId":259202797},"title":"CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot Vision-and-Language Navigation"},{"paperId":"2b806bc0a075f9088021f7362ffa5b8b86fd75ab","externalIds":{"DBLP":"conf/corl/RadosavovicSFGD23","ArXiv":"2306.10007","DOI":"10.48550/arXiv.2306.10007","CorpusId":259187975},"title":"Robot Learning with Sensorimotor Pre-training"},{"paperId":"9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6","externalIds":{"ArXiv":"2306.08302","DBLP":"journals/corr/abs-2306-08302","DOI":"10.1109/TKDE.2024.3352100","CorpusId":259165563},"title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap"},{"paperId":"94bcf0390d5acb1b92323bd15cc1dc311314122c","externalIds":{"DBLP":"conf/corl/0003GFKLACEHHIX23","ArXiv":"2306.08647","DOI":"10.48550/arXiv.2306.08647","CorpusId":259164906},"title":"Language to Rewards for Robotic Skill Synthesis"},{"paperId":"eebb4a3162c1251b51e50ccd83797babc5b776c0","externalIds":{"ArXiv":"2306.05171","DBLP":"journals/corr/abs-2306-05171","DOI":"10.48550/arXiv.2306.05171","CorpusId":259108727},"title":"Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures"},{"paperId":"993df7df129f8d18816877d69923d7df7b347d85","externalIds":{"DBLP":"conf/acl/Jiang0L23","ACL":"2023.acl-long.792","ArXiv":"2306.02561","DOI":"10.48550/arXiv.2306.02561","CorpusId":259075564},"title":"LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"},{"paperId":"3b8871e4c25d3aaca2bee6606c07bc870337253c","externalIds":{"DBLP":"journals/corr/abs-2306-02224","ArXiv":"2306.02224","DOI":"10.48550/arXiv.2306.02224","CorpusId":259075577},"title":"Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions"},{"paperId":"b2cad5ae3b2c7a9d90c2050cf37402b01725446a","externalIds":{"DBLP":"journals/corr/abs-2306-00915","ArXiv":"2306.00915","DOI":"10.48550/arXiv.2306.00915","CorpusId":258999584,"PubMed":"37863713"},"title":"The feasibility of artificial consciousness through the lens of neuroscience"},{"paperId":"b5cfea5990b0f2aa15854a7883266ca5e5e90194","externalIds":{"DBLP":"conf/cvpr/ZhangSLBYJ23","DOI":"10.1109/CVPR52729.2023.01039","CorpusId":260844699},"title":"Layout-based Causal Inference for Object Navigation"},{"paperId":"b15d51d72711b350e5717f18b28d9e469bf056b4","externalIds":{"DBLP":"journals/natmi/StellaSH23","DOI":"10.1038/s42256-023-00669-7","CorpusId":259214978},"title":"How can LLMs transform the robotic design process?"},{"paperId":"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","externalIds":{"DBLP":"journals/corr/abs-2305-19308","ArXiv":"2305.19308","DOI":"10.48550/arXiv.2305.19308","CorpusId":258987743},"title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"},{"paperId":"431b2c3f4c6b2fe1ec4fead396286a48ed5edbbf","externalIds":{"ArXiv":"2306.00942","DBLP":"journals/corr/abs-2306-00942","DOI":"10.1109/ICRA48891.2023.10160594","CorpusId":253122077},"title":"Train Offline, Test Online: A Real Robot Learning Benchmark"},{"paperId":"119d3beca449efd9096d58674cf01a99c793a9a7","externalIds":{"DBLP":"conf/icra/MoZK23","DOI":"10.1109/ICRA48891.2023.10161333","CorpusId":259301056},"title":"Towards Open-World Interactive Disambiguation for Robotic Grasping"},{"paperId":"2d2b49c9de7833f8a306a587229caddd20013d4f","externalIds":{"DBLP":"conf/icra/ElangovanGSWWJL23","DOI":"10.1109/ICRA48891.2023.10161171","CorpusId":259339375},"title":"On Human Grasping and Manipulation in Kitchens: Automated Annotation, Insights, and Metrics for Effective Data Collection"},{"paperId":"8199c9d55dd998f69f703e0ad250ca0697e3ad27","externalIds":{"DBLP":"conf/aaai/ZhouHW24","ArXiv":"2305.16986","DOI":"10.48550/arXiv.2305.16986","CorpusId":258947250},"title":"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models"},{"paperId":"f197bf0fc2f228483f6af3285000d54d8d97f9eb","externalIds":{"ArXiv":"2305.16291","DBLP":"journals/tmlr/WangX0MXZFA24","DOI":"10.48550/arXiv.2305.16291","CorpusId":258887849},"title":"Voyager: An Open-Ended Embodied Agent with Large Language Models"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","externalIds":{"ArXiv":"2305.15021","DBLP":"journals/corr/abs-2305-15021","DOI":"10.48550/arXiv.2305.15021","CorpusId":258865718},"title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"0f416c637a5a78435e6b12ebf1ce891224de0edc","externalIds":{"ArXiv":"2305.13516","DBLP":"journals/jmlr/PratapTSTBKENVF24","DOI":"10.48550/arXiv.2305.13516","CorpusId":258841617},"title":"Scaling Speech Technology to 1, 000+ Languages"},{"paperId":"35631fd55c2545615811fa8072015356ac8198e7","externalIds":{"DBLP":"journals/www/ZhuWCQOYDCZ24","ArXiv":"2305.13168","DOI":"10.1007/s11280-024-01297-w","CorpusId":258833039},"title":"LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities"},{"paperId":"afc5092a4116f27b4c64733c7815cd662bab78f7","externalIds":{"DBLP":"journals/corr/abs-2305-11176","ArXiv":"2305.11176","DOI":"10.48550/arXiv.2305.11176","CorpusId":258762636},"title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","externalIds":{"DBLP":"conf/nips/WangCCWZZLLZQD23","ArXiv":"2305.11175","DOI":"10.48550/arXiv.2305.11175","CorpusId":258762579},"title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"dd7f28d93dc2ec3cbc1cc66c3443c1c17105f1b3","externalIds":{"DBLP":"conf/acl/XuXWLZM24","ArXiv":"2305.08848","DOI":"10.48550/arXiv.2305.08848","CorpusId":258685778},"title":"Small Models are Valuable Plug-ins for Large Language Models"},{"paperId":"1856bebc4cb35e68368d9c83bd2ac2d26cd4bcfa","externalIds":{"DBLP":"journals/corr/abs-2305-08196","ArXiv":"2305.08196","DOI":"10.48550/arXiv.2305.08196","CorpusId":258686670},"title":"A Comprehensive Survey on Segment Anything Model for Vision and Beyond"},{"paperId":"1fbdd5a3ba7d17ab9c24c8c0e6ea93d7ccb90c16","externalIds":{"DBLP":"conf/iccv/0002CWKX23","ArXiv":"2305.06456","DOI":"10.1109/ICCV51070.2023.01000","CorpusId":258615220},"title":"Perpetual Humanoid Control for Real-time Simulated Avatars"},{"paperId":"9bdcf270bce9f680bad5385bc7920536d4fa0c53","externalIds":{"ArXiv":"2305.05706","DBLP":"journals/corr/abs-2305-05706","DOI":"10.1109/CVPR52729.2023.02030","CorpusId":258588439},"title":"DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects"},{"paperId":"e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a","externalIds":{"ArXiv":"2305.05658","DBLP":"journals/arobots/WuAKLZSBRF23","DOI":"10.1007/s10514-023-10139-z","CorpusId":258564887},"title":"TidyBot: Personalized Robot Assistance with Large Language Models"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","externalIds":{"DBLP":"journals/corr/abs-2304-13712","ArXiv":"2304.13712","DOI":"10.1145/3649506","CorpusId":258331833},"title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"da3b810e10638507be9901e1b5cb94db126c8166","externalIds":{"DBLP":"journals/corr/abs-2304-13826","ArXiv":"2304.13826","DOI":"10.48550/arXiv.2304.13826","CorpusId":258352681},"title":"Programmatically Grounded, Compositionally Generalizable Robotic Manipulation"},{"paperId":"003ef1cd670d01af05afa0d3c72d72228f494432","externalIds":{"ArXiv":"2304.11477","DBLP":"journals/corr/abs-2304-11477","DOI":"10.48550/arXiv.2304.11477","CorpusId":258298051},"title":"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"ae6a4cd221684be6ca3082b6f526a7901281490b","externalIds":{"ArXiv":"2304.05332","DBLP":"journals/corr/abs-2304-05332","DOI":"10.48550/arXiv.2304.05332","CorpusId":258059651},"title":"Emergent autonomous scientific research capabilities of large language models"},{"paperId":"f274ce903eca789be99fd6e721dc6e213f8debeb","externalIds":{"DBLP":"journals/corr/abs-2304-04602","ArXiv":"2304.04602","DOI":"10.48550/arXiv.2304.04602","CorpusId":258049361},"title":"Learning a Universal Human Prior for Dexterous Manipulation from Human Preference"},{"paperId":"7a84a26647892daa9d10dbfc97c0382619ac2f4d","externalIds":{"ArXiv":"2304.03893","DBLP":"journals/corr/abs-2304-03893","DOI":"10.1109/ACCESS.2023.3310935","CorpusId":258048920},"title":"ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"311d53dcb0bfe50017959323393dd88ea2b451d8","externalIds":{"ArXiv":"2304.02052","DBLP":"journals/corr/abs-2304-02052","DOI":"10.1109/ICRA48891.2023.10161003","CorpusId":257952540},"title":"Online augmentation of learned grasp sequence policies for more adaptable and data-efficient in-hand manipulation"},{"paperId":"e1bd151a3f670fd0f77580702fe7a85dc78a41cb","externalIds":{"DBLP":"conf/icml/JiaTLCH024","ArXiv":"2304.00776","DOI":"10.48550/arXiv.2304.00776","CorpusId":257912725},"title":"Chain-of-Thought Predictive Control"},{"paperId":"5d5024fae7223a0f8a8b86e1fdf5c5f4b7a9d9c6","externalIds":{"DBLP":"journals/corr/abs-2304-00464","ArXiv":"2304.00464","DOI":"10.1109/ICCV51070.2023.00360","CorpusId":257913440},"title":"UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"9ab6c3c3be48627aba40ad89e5bbc15d7140d873","externalIds":{"DBLP":"journals/corr/abs-2303-17156","ArXiv":"2303.17156","DOI":"10.48550/arXiv.2303.17156","CorpusId":257833965},"title":"MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations"},{"paperId":"bafe023fb072045dc0cd50316382a61c8dcb9fae","externalIds":{"DBLP":"conf/kdd/ZhengXZDWXSW0LS23","ArXiv":"2303.17568","DOI":"10.1145/3580305.3599790","CorpusId":257834177},"title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"},{"paperId":"9fac3d0728a8c833a593446e3e176e90d856df04","externalIds":{"DBLP":"journals/corr/abs-2303-16727","ArXiv":"2303.16727","DOI":"10.1109/CVPR52729.2023.01398","CorpusId":257805127},"title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","externalIds":{"DBLP":"journals/corr/abs-2303-16434","ArXiv":"2303.16434","DOI":"10.48550/arXiv.2303.16434","CorpusId":257804802},"title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"5d332ecf011cfd2338f625909591cb46ec6170d1","externalIds":{"DBLP":"conf/icra/MitashWLTGPN23","ArXiv":"2303.16382","DOI":"10.1109/ICRA48891.2023.10160846","CorpusId":257805049},"title":"ARMBench: An Object-centric Benchmark Dataset for Robotic Manipulation"},{"paperId":"87fa694400cd3d818e7014e36b08745afc64041f","externalIds":{"DBLP":"conf/iros/NguyenNPDX23","ArXiv":"2303.14880","DOI":"10.1109/IROS55552.2023.10342447","CorpusId":257766343},"title":"Toward Human-Like Social Robot Navigation: A Large-Scale, Multi-Modal, Social Human Navigation Dataset"},{"paperId":"17e4b3fb4d8e27f8a4f63b251179ee62ba1eeb51","externalIds":{"DBLP":"conf/cvpr/ZhengZFLY23","ArXiv":"2303.15469","DOI":"10.1109/CVPR52729.2023.00064","CorpusId":257771325},"title":"CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis"},{"paperId":"0975241e605761306e370c13f2cba8d3bb212a8b","externalIds":{"DBLP":"journals/corr/abs-2303-14502","ArXiv":"2303.14502","DOI":"10.1109/IROS55552.2023.10342393","CorpusId":267854518},"title":"VERN: Vegetation-Aware Robot Navigation in Dense Unstructured Outdoor Environments"},{"paperId":"ecceb57a5fe8e771e35b7a00278e7ab0fad2165a","externalIds":{"DBLP":"conf/corl/HanXZR23","ArXiv":"2303.13446","DOI":"10.48550/arXiv.2303.13446","CorpusId":257687207},"title":"On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills"},{"paperId":"dae9be0f0d815b53b46974377a0edf9169a99f3f","externalIds":{"ArXiv":"2303.12076","DBLP":"conf/corl/GuzeyECP23","DOI":"10.48550/arXiv.2303.12076","CorpusId":257636836},"title":"Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play"},{"paperId":"8f2d4758e6d525509ae36bb30224dc9259027e6b","externalIds":{"DBLP":"journals/arobots/LinAMPB23","ArXiv":"2303.12153","DOI":"10.1007/s10514-023-10131-7","CorpusId":257663442},"title":"Text2Motion: from natural language instructions to feasible plans"},{"paperId":"ba38c19d132ec29a40e64bd3734bf4d6b0059637","externalIds":{"DBLP":"journals/corr/abs-2303-10880","ArXiv":"2303.10880","DOI":"10.48550/arXiv.2303.10880","CorpusId":257631785},"title":"Rotating without Seeing: Towards In-hand Dexterity through Touch"},{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","externalIds":{"ArXiv":"2303.10845","DBLP":"journals/corr/abs-2303-10845","CorpusId":257666647},"title":"PanGu-Î£: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"2ebd5df74980a37370b0bcdf16deff958289c041","externalIds":{"ArXiv":"2303.04129","DBLP":"journals/corr/abs-2303-04129","DOI":"10.48550/arXiv.2303.04129","CorpusId":257378587},"title":"Foundation Models for Decision Making: Problems, Methods, and Opportunities"},{"paperId":"a8f842ed0dec8d81e3530ad26fc05512c8958198","externalIds":{"DBLP":"journals/scirobotics/RadosavovicXZDMS24","ArXiv":"2303.03381","DOI":"10.1126/scirobotics.adi9579","CorpusId":257365674,"PubMed":"38630806"},"title":"Real-world humanoid locomotion with reinforcement learning"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"f02d56e630986997e0aea3d92bf53e0f363ce401","externalIds":{"ArXiv":"2303.02506","DBLP":"journals/tmlr/LiuFJYXA24","CorpusId":257365896},"title":"Prismer: A Vision-Language Model with Multi-Task Experts"},{"paperId":"96a19cd83080d70e10f3e6fd5af327a1263f20d6","externalIds":{"ArXiv":"2303.01497","DBLP":"journals/corr/abs-2303-01497","DOI":"10.48550/arXiv.2303.01497","CorpusId":257279829},"title":"Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations"},{"paperId":"777317e5af8742b30408e98778fa067750e69f78","externalIds":{"ArXiv":"2303.01037","DBLP":"journals/corr/abs-2303-01037","DOI":"10.48550/arXiv.2303.01037","CorpusId":257280021},"title":"Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"0ba581718f294db1d7b3dbc159cc3d3380f74606","externalIds":{"DBLP":"journals/access/VempralaBBK24","ArXiv":"2306.17582","DOI":"10.1109/ACCESS.2024.3387941","CorpusId":259141622},"title":"ChatGPT for Robotics: Design Principles and Model Abilities"},{"paperId":"634127f09be5119f4d29a175866f11b14b3821e0","externalIds":{"ArXiv":"2302.04659","DBLP":"journals/corr/abs-2302-04659","DOI":"10.48550/arXiv.2302.04659","CorpusId":256697500},"title":"ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills"},{"paperId":"bf8491bef353df126e2306ad2fe4b898697b906a","externalIds":{"ArXiv":"2302.04023","DBLP":"conf/ijcnlp/BangCLDSWLJYCDXF23","ACL":"2023.ijcnlp-main.45","DOI":"10.18653/v1/2023.ijcnlp-main.45","CorpusId":256662612},"title":"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity"},{"paperId":"d77ec728e45f2a0feb80ddf1e2e6226170ac56e2","externalIds":{"DBLP":"journals/csur/GuoWQL0023","DOI":"10.1145/3583136","CorpusId":256630415},"title":"Recent Trends in Task and Motion Planning for Robotics: A Survey"},{"paperId":"9348656b761f7b76fb65cfe6fac55386b04a3a8a","externalIds":{"DBLP":"journals/corr/abs-2302-00487","ArXiv":"2302.00487","DOI":"10.1109/TPAMI.2024.3367329","CorpusId":256459333,"PubMed":"38407999"},"title":"A Comprehensive Survey of Continual Learning: Theory, Method and Application"},{"paperId":"64c1ba56a32ed9a42f2cac010de56002380c2408","externalIds":{"ArXiv":"2301.12292","DBLP":"journals/corr/abs-2301-12292","DOI":"10.48550/arXiv.2301.12292","CorpusId":256389672},"title":"Zero-shot causal learning"},{"paperId":"b81f8c734331a6fef1de842f7ea5952e26151e22","externalIds":{"DBLP":"journals/ese/GarciaSBFPB23","PubMedCentral":"9789375","DOI":"10.1007/s10664-022-10231-5","CorpusId":252085879,"PubMed":"36588914"},"title":"Software variability in service robotics"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","externalIds":{"ArXiv":"2212.10403","DBLP":"conf/acl/0009C23","DOI":"10.48550/arXiv.2212.10403","CorpusId":254877753},"title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","externalIds":{"DBLP":"journals/corr/abs-2212-09597","ArXiv":"2212.09597","ACL":"2023.acl-long.294","DOI":"10.48550/arXiv.2212.09597","CorpusId":254854219},"title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d","externalIds":{"ArXiv":"2212.06817","DBLP":"conf/rss/BrohanBCCDFGHHH23","DOI":"10.48550/arXiv.2212.06817","CorpusId":254591260},"title":"RT-1: Robotics Transformer for Real-World Control at Scale"},{"paperId":"5666cf5dd2cb1eb55dfa88c1486a6cacdf94c840","externalIds":{"DBLP":"conf/corl/LiZZWLXA0G022","ArXiv":"2212.03858","DOI":"10.48550/arXiv.2212.03858","CorpusId":252411626},"title":"See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation"},{"paperId":"a02fbaf22237a1aedacb1320b6007cd70c1fe6ec","externalIds":{"DBLP":"journals/corr/abs-2212-04356","ArXiv":"2212.04356","CorpusId":252923993},"title":"Robust Speech Recognition via Large-Scale Weak Supervision"},{"paperId":"f403f84183be8660c1c7aa91c98cea74f39d3924","externalIds":{"ArXiv":"2212.00922","DBLP":"journals/corr/abs-2212-00922","DOI":"10.1126/scirobotics.adf6991","CorpusId":254221215,"PubMed":"37379376"},"title":"Navigating to objects in the real world"},{"paperId":"55dff8d4cdeab9c86dccb1c8b739ac0518cdbed0","externalIds":{"DBLP":"conf/rss/XiaoCSWBHLT23","ArXiv":"2211.11736","DOI":"10.48550/arXiv.2211.11736","CorpusId":253734315},"title":"Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"ee5661886abd37718edd5efa38b8bb660e2a7204","externalIds":{"DBLP":"conf/nips/WangLGW22","ArXiv":"2210.16822","DOI":"10.48550/arXiv.2210.16822","CorpusId":253237724},"title":"Towards Versatile Embodied Navigation"},{"paperId":"a451c2226c3cc4f4d0a902f5e9b4eb157ba3138d","externalIds":{"DBLP":"journals/corr/abs-2211-04895","ArXiv":"2211.04895","DOI":"10.1146/annurev-control-062122-025215","CorpusId":253077719},"title":"Grasp Learning: Models, Methods, and Performance"},{"paperId":"b287a2765e5bceb732de39dafdf70594dc9cd664","externalIds":{"DBLP":"journals/ftcgv/GanLLWLG22","ArXiv":"2210.09263","DOI":"10.48550/arXiv.2210.09263","CorpusId":252918286},"title":"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"},{"paperId":"0e34addae55a571d7efd3a5e2543e86dd7d41a83","externalIds":{"DBLP":"journals/corr/abs-2210-06407","ArXiv":"2210.06407","DOI":"10.48550/arXiv.2210.06407","CorpusId":252846090},"title":"Interactive Language: Talking to Robots in Real Time"},{"paperId":"c305ab1bdba79442bec72ec7f5c5ee7c49c2a566","externalIds":{"DBLP":"journals/corr/abs-2210-05714","ArXiv":"2210.05714","DOI":"10.1109/ICRA48891.2023.10160969","CorpusId":252846548},"title":"Visual Language Maps for Robot Navigation"},{"paperId":"bdf99df7839c02abf7253e33e67caafa21eda5fd","externalIds":{"ArXiv":"2210.02697","DBLP":"conf/icra/WangZCXLLW23","DOI":"10.1109/ICRA48891.2023.10160982","CorpusId":252734719},"title":"DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","externalIds":{"DBLP":"conf/iclr/YaoZYDSN023","ArXiv":"2210.03629","CorpusId":252762395},"title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"25425e299101b13ec2872417a14f961f4f8aa18e","externalIds":{"DBLP":"journals/corr/abs-2210-03094","ArXiv":"2210.03094","DOI":"10.48550/arXiv.2210.03094","CorpusId":252735175},"title":"VIMA: General Robot Manipulation with Multimodal Prompts"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"5178858ff4ff5aab811e91cfab654c140f81dd46","externalIds":{"DBLP":"journals/ras/LiuDPRC23","DOI":"10.1016/j.robot.2022.104294","CorpusId":250640399},"title":"A survey of Semantic Reasoning frameworks for robotic systems"},{"paperId":"6902d6f9f4d4c1303f87f0d131ff595aca6a951e","externalIds":{"DBLP":"journals/csur/AntonyshynSGM23","DOI":"10.1145/3564696","CorpusId":252546226},"title":"Multiple Mobile Robot Task and Motion Planning: A Survey"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","externalIds":{"DBLP":"journals/corr/abs-2209-11302","ArXiv":"2209.11302","DOI":"10.1109/ICRA48891.2023.10161317","CorpusId":252519594},"title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"91deaf9d324c8feafc189da0da03e60a60287bca","externalIds":{"ArXiv":"2209.07753","DBLP":"conf/icra/LiangHXXHIFZ23","DOI":"10.1109/ICRA48891.2023.10160591","CorpusId":252355542},"title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"344cba18285949fe409fc0b332831bc186cfeb17","externalIds":{"DBLP":"conf/iclr/GuC0M23","ArXiv":"2209.02778","DOI":"10.48550/arXiv.2209.02778","CorpusId":252110923},"title":"Multi-skill Mobile Manipulation for Object Rearrangement"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","externalIds":{"ArXiv":"2208.10442","DBLP":"journals/corr/abs-2208-10442","DOI":"10.48550/arXiv.2208.10442","CorpusId":251719655},"title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","externalIds":{"DBLP":"journals/corr/abs-2207-10397","ArXiv":"2207.10397","DOI":"10.48550/arXiv.2207.10397","CorpusId":250920542},"title":"CodeT: Code Generation with Generated Tests"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","externalIds":{"ArXiv":"2207.04429","DBLP":"conf/corl/ShahOIL22","DOI":"10.48550/arXiv.2207.04429","CorpusId":250426345},"title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"23525374cfd3af714f3ffb7a203b1ef3253333fe","externalIds":{"ArXiv":"2207.01206","DBLP":"journals/corr/abs-2207-01206","DOI":"10.48550/arXiv.2207.01206","CorpusId":250264533},"title":"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents"},{"paperId":"25bc06b508b2c63b9faf77881e528530b147b988","externalIds":{"DBLP":"conf/corl/WuEHAG22","ArXiv":"2206.14176","CorpusId":250088882},"title":"DayDreamer: World Models for Physical Robot Learning"},{"paperId":"32c9b3859086d15184989454eb878638659e64c6","externalIds":{"ArXiv":"2206.08853","DBLP":"conf/nips/FanWJMYZTHZA22","DOI":"10.48550/arXiv.2206.08853","CorpusId":249848263},"title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","externalIds":{"ArXiv":"2206.04615","DBLP":"journals/corr/abs-2206-04615","CorpusId":263625818},"title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"c2fbc8ad4e2403c795529c2606e4686f5a571215","externalIds":{"DBLP":"journals/natmi/AchtibatDEBWSL23","ArXiv":"2206.03208","DOI":"10.1038/s42256-023-00711-8","CorpusId":249431627},"title":"From attribution maps to human-understandable explanations through Concept Relevance Propagation"},{"paperId":"75b181c0e62b5261e3e0f3e3e96687c128208134","externalIds":{"DBLP":"conf/cvpr/Lin0CLLL22","ArXiv":"2205.15509","DOI":"10.1109/CVPR52688.2022.01496","CorpusId":249209579},"title":"ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts"},{"paperId":"5922f437512158970c417f4413bface021df5f78","externalIds":{"DBLP":"journals/corr/abs-2205-06175","ArXiv":"2205.06175","DOI":"10.48550/arXiv.2205.06175","CorpusId":248722148},"title":"A Generalist Agent"},{"paperId":"9f5120b815fddaaef25c7042035ffe5680507a65","externalIds":{"DBLP":"conf/icra/DownsFKKHRMV22","ArXiv":"2204.11918","DOI":"10.48550/arXiv.2204.11918","CorpusId":248392390},"title":"Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items"},{"paperId":"a58b3f2ab75fdbda082e684d027ab4f552b0b5d3","externalIds":{"DBLP":"journals/corr/abs-2204-05186","ArXiv":"2204.05186","DOI":"10.48550/arXiv.2204.05186","CorpusId":248085271},"title":"Correcting Robot Plans with Natural Language Feedback"},{"paperId":"1bf46fd55008c3fe2dd531c5cdb97dceafd6b217","externalIds":{"DBLP":"journals/corr/abs-2204-05080","ArXiv":"2204.05080","DOI":"10.48550/arXiv.2204.05080","CorpusId":248085427},"title":"Semantic Exploration from Language Abstractions and Pretrained Representations"},{"paperId":"9b97b9649feeef07dda2c0db97f7c933c26f8488","externalIds":{"DBLP":"journals/corr/abs-2204-02389","ArXiv":"2204.02389","DOI":"10.1109/CVPR52688.2022.01034","CorpusId":247958132},"title":"ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","externalIds":{"DBLP":"conf/corl/IchterBCFHHHIIJ22","ArXiv":"2204.01691","CorpusId":247939706},"title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"f1fa3e971b93e123ebb3e76e4832509199b3daa5","externalIds":{"DBLP":"journals/corr/abs-2203-15709","ArXiv":"2203.15709","DOI":"10.1109/CVPR52688.2022.02028","CorpusId":247778464},"title":"OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction"},{"paperId":"521ca2049131f48c804859a5402d3e8ab4b7e2a9","externalIds":{"DBLP":"journals/ral/KarnanNXWPTHBS22","ArXiv":"2203.15041","DOI":"10.1109/LRA.2022.3184025","CorpusId":247778446},"title":"Socially CompliAnt Navigation Dataset (SCAND): A Large-Scale Dataset Of Demonstrations For Social Navigation"},{"paperId":"742b195fb4c2868a4e60012c8e0bf7db43bb5650","externalIds":{"DBLP":"conf/cvpr/GadreWISS23","ArXiv":"2203.10421","DOI":"10.1109/CVPR52729.2023.02219","CorpusId":254636632},"title":"CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation"},{"paperId":"18c302c9d51146ea91784638748e5d737da75e12","externalIds":{"ArXiv":"2203.02764","DBLP":"journals/corr/abs-2203-02764","DOI":"10.1109/CVPR52688.2022.01500","CorpusId":247292395},"title":"Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation"},{"paperId":"2a0b8be3594e8163f9ea4988658223d7c46cfcb3","externalIds":{"DBLP":"conf/cvpr/LiuLJLWSLFWY22","ArXiv":"2203.01577","DOI":"10.1109/CVPR52688.2022.02034","CorpusId":247222786},"title":"HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction"},{"paperId":"3def68bd0f856886d34272840a7f81588f2bc082","externalIds":{"DBLP":"journals/corr/abs-2202-03629","ArXiv":"2202.03629","DOI":"10.1145/3571730","CorpusId":246652372},"title":"Survey of Hallucination in Natural Language Generation"},{"paperId":"e9c16da93e3f3f3fa61954d92afc4983a5eb6ac0","externalIds":{"DBLP":"journals/corr/abs-2202-03631","ArXiv":"2202.03631","CorpusId":246652189},"title":"Robotic Grasping from Classical to Modern: A Survey"},{"paperId":"5eaa7b7db558a4cbef6d73a4ef4be9130df48154","externalIds":{"ArXiv":"2202.00199","CorpusId":258686729},"title":"RFUniverse: A Multiphysics Simulation Platform for Embodied AI"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"9d270c56dd80a2f493e206781faf82a22351a6b2","externalIds":{"DBLP":"journals/corr/abs-2201-09862","ArXiv":"2201.09862","DOI":"10.1109/IROS47612.2022.9981261","CorpusId":246240350},"title":"Learning to Act with Affordance-Aware Multimodal Neural SLAM"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","externalIds":{"DBLP":"journals/corr/abs-2201-08239","ArXiv":"2201.08239","CorpusId":246063428},"title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","externalIds":{"DBLP":"journals/corr/abs-2201-07207","ArXiv":"2201.07207","CorpusId":246035276},"title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"a3184d40d390793232c99c89b57b8f65c16320b2","externalIds":{"DBLP":"journals/corr/abs-2112-12731","ArXiv":"2112.12731","CorpusId":245425057},"title":"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"},{"paperId":"34e82c55ae8920e893d1635c8156193cbcc78ce5","externalIds":{"ArXiv":"2112.13659","DBLP":"journals/corr/abs-2112-13659","DOI":"10.1109/lra.2021.3138527","CorpusId":245502543},"title":"M2DGR: A Multi-Sensor and Multi-Scenario SLAM Dataset for Ground Robots"},{"paperId":"4be02694125b71876552900a53c85c47a2a83614","externalIds":{"DBLP":"journals/ral/MeesHRB22","ArXiv":"2112.03227","DOI":"10.1109/lra.2022.3180108","CorpusId":244908821},"title":"CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks"},{"paperId":"582b6ce5b3266a934f8be7cdcea5e09352d2c389","externalIds":{"PubMedCentral":"8673825","DBLP":"journals/firai/KazanzidesVPDLW21","DOI":"10.3389/frobt.2021.747917","CorpusId":244778883,"PubMed":"34926590"},"title":"Teleoperation and Visualization Interfaces for Remote Intervention in Space"},{"paperId":"826383e18568c9c37b5fc5dd7e2913352db22b47","externalIds":{"DBLP":"journals/corr/abs-2111-09888","ArXiv":"2111.09888","DOI":"10.1109/CVPR52688.2022.01441","CorpusId":244346010},"title":"Simple but Effective: CLIP Embeddings for Embodied AI"},{"paperId":"4a02061f8623f68502991d8bdf7728ae50669091","externalIds":{"DBLP":"journals/scirobotics/DupontNGHMOSVY21","DOI":"10.1126/scirobotics.abi8017","CorpusId":243988461,"PubMed":"34757801"},"title":"A decade retrospective of medical robotics research from 2010 to 2020"},{"paperId":"0ee9b633a0914b51f1eec3ad434752aa58e10149","externalIds":{"ArXiv":"2111.03043","DBLP":"conf/corl/Chen0A21","CorpusId":242757300},"title":"A System for General In-Hand Object Re-Orientation"},{"paperId":"e7eba2aa3c625beec289cb14914e7b5d36469d04","externalIds":{"PubMedCentral":"9038844","ArXiv":"2111.00956","DBLP":"journals/corr/abs-2111-00956","DOI":"10.3389/frobt.2022.799893","CorpusId":240354417,"PubMed":"35494543"},"title":"Robot Learning From Randomized Simulations: A Review"},{"paperId":"ee8984a6712791d4e0f2c776dad8119a3b893dd9","externalIds":{"ArXiv":"2110.14883","DBLP":"conf/icpp/LiLBFHLW023","DOI":"10.1145/3605573.3605613","CorpusId":240070340},"title":"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"},{"paperId":"6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424","externalIds":{"DBLP":"journals/corr/abs-2110-00534","ArXiv":"2110.00534","DOI":"10.1609/aaai.v36i2.20097","CorpusId":238253352},"title":"TEACh: Task-driven Embodied Agents that Chat"},{"paperId":"6bae20930eaa0d9d489317f3b3b1aaaf18205ef8","externalIds":{"DBLP":"journals/corr/abs-2109-13396","ArXiv":"2109.13396","DOI":"10.15607/rss.2022.xviii.063","CorpusId":237277709},"title":"Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets"},{"paperId":"69ee9b3a915951cc84b74599a3a2699a66d4004f","externalIds":{"DBLP":"conf/corl/ShridharMF21","ArXiv":"2109.12098","CorpusId":237396838},"title":"CLIPort: What and Where Pathways for Robotic Manipulation"},{"paperId":"096618aba19bce1d917df488a04891ab239cffdd","externalIds":{"DBLP":"journals/tvcg/ShenSLYHZTW23","ArXiv":"2109.03506","DOI":"10.1109/TVCG.2022.3148007","CorpusId":237439677,"PubMed":"35104221"},"title":"Towards Natural Language Interfaces for Data Visualization: A Survey"},{"paperId":"49142e3e381c0dc7fee0049ea41d2ef02c0340d7","externalIds":{"DBLP":"conf/nips/MakoviychukWGLS21","ArXiv":"2108.10470","CorpusId":237277983},"title":"Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning"},{"paperId":"5c89bc8e91fb85f8af7761e8096d27dd740491d1","externalIds":{"DBLP":"conf/iccv/GuhurTCLS21","ArXiv":"2108.09105","DOI":"10.1109/ICCV48922.2021.00166","CorpusId":237260024},"title":"Airbert: In-domain Pretraining for Vision-and-Language Navigation"},{"paperId":"c0c9f77cb097f2ce53feb91802bcfbae57fcc42f","externalIds":{"DBLP":"conf/corl/Srivastava0LMXV21","ArXiv":"2108.03332","CorpusId":236957374},"title":"BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments"},{"paperId":"5aeaaa5a11697c8b7f6e52080c223ad8564e4855","externalIds":{"DOI":"10.1177/1729881420925283","CorpusId":237393714},"title":"Utilization of multilayer perceptron for determining the inverse kinematics of an industrial robotic manipulator"},{"paperId":"4aa88c1406414cda3ce9cf76c8af0abaa8391760","externalIds":{"ArXiv":"2106.14405","DBLP":"conf/nips/SzotCUWZTMMCMGV21","CorpusId":235658123},"title":"Habitat 2.0: Training Home Assistants to Rearrange their Habitat"},{"paperId":"32feca141fce06c6588b4014d27953a3fc25f19b","externalIds":{"ACL":"2021.acl-long.159","DBLP":"journals/corr/abs-2106-00188","MAG":"3176195078","ArXiv":"2106.00188","DOI":"10.18653/v1/2021.acl-long.159","CorpusId":235266260},"title":"PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World"},{"paperId":"f8677102449c803512c87b240b3989e4e9276cbc","externalIds":{"DBLP":"journals/scirobotics/CuiT21","DOI":"10.1126/scirobotics.abd9461","CorpusId":235204107,"PubMed":"34043539"},"title":"Toward next-generation learned robot manipulation"},{"paperId":"d05fdc686ebe5148196b25b5c7e3880654c3354e","externalIds":{"DOI":"10.1109/RBME.2021.3078190","CorpusId":233998626,"PubMed":"33961564"},"title":"Emerging Wearable Interfaces and Algorithms for Hand Gesture Recognition: A Survey"},{"paperId":"774591fdd988eaaff3917e7c5171d044b0843e63","externalIds":{"ArXiv":"2104.04473","DBLP":"conf/sc/NarayananSCLPKV21","DOI":"10.1145/3458817.3476209","CorpusId":236635565},"title":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"},{"paperId":"21ec9c0f869bdb33b06c7dbc8880169db0397d08","externalIds":{"DBLP":"journals/corr/abs-2103-13009","ArXiv":"2103.13009","DOI":"10.1609/aaai.v35i15.17590","CorpusId":232335877},"title":"UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark"},{"paperId":"f79bc5e08d5d00c486baefb29e2306715d991b69","externalIds":{"DBLP":"journals/thri/MavrogiannisBWZ23","ArXiv":"2103.05668","DOI":"10.1145/3583741","CorpusId":232170677},"title":"Core Challenges of Social Robot Navigation: A Survey"},{"paperId":"9c404d02aefd850ac3d5a8bdc5860738e6cd2b04","externalIds":{"ArXiv":"2103.04918","DBLP":"journals/tetci/DuanYTZT22","DOI":"10.1109/tetci.2022.3141105","CorpusId":232146971},"title":"A Survey of Embodied AI: From Simulators to Research Tasks"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"33428914d6d4bd646054c44610c101b1444f2903","externalIds":{"ArXiv":"2102.12321","DBLP":"journals/corr/abs-2102-12321","CorpusId":232035753},"title":"AGENT: A Benchmark for Core Psychological Reasoning"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","externalIds":{"ArXiv":"2102.03334","DBLP":"journals/corr/abs-2102-03334","CorpusId":231839613},"title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"cda5d891ae6262c55c8711ef5b18ccc1fe748d7a","externalIds":{"DBLP":"conf/iros/ShenX0MFWPBSTTV21","ArXiv":"2012.02924","DOI":"10.1109/IROS51168.2021.9636667","CorpusId":227347434},"title":"iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes"},{"paperId":"604e0c54580a0530392f86b48a6183581a47b66d","externalIds":{"MAG":"3108144224","ArXiv":"2011.13922","DBLP":"conf/cvpr/Hong0QOG21","DOI":"10.1109/CVPR46437.2021.00169","CorpusId":227228335},"title":"VLNâ»BERT: A Recurrent Vision-and-Language BERT for Navigation"},{"paperId":"dab94f11874acdd5ffe63cfb22c5f93723dc519f","externalIds":{"ACL":"2020.emnlp-main.59","DBLP":"conf/emnlp/HahnKBPRLA20","ArXiv":"2011.08277","MAG":"3102192628","DOI":"10.18653/v1/2020.emnlp-main.59","CorpusId":226262201},"title":"Where Are You? Localization from Embodied Dialog"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"398a0625e8707a0b41ac58eaec51e8feb87dd7cb","externalIds":{"DBLP":"journals/corr/abs-2010-03768","MAG":"3092516542","ArXiv":"2010.03768","CorpusId":222208810},"title":"ALFWorld: Aligning Text and Embodied Environments for Interactive Learning"},{"paperId":"94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b","externalIds":{"DBLP":"journals/corr/abs-2009-00236","MAG":"3082269314","ArXiv":"2009.00236","DOI":"10.1145/3472291","CorpusId":221397441},"title":"A Survey of Deep Active Learning"},{"paperId":"f0152a8fd87cb60ad30c296823829802c13a9986","externalIds":{"DBLP":"journals/corr/abs-2007-14626","MAG":"3045665802","ArXiv":"2007.14626","DOI":"10.1007/978-3-030-58607-2_18","CorpusId":220845968},"title":"Object-and-Action Aware Model for Visual Language Navigation"},{"paperId":"d3dd97403d8fdb9aa9541ea84db029c1d6bbc9ed","externalIds":{"DBLP":"conf/eccv/DuY020","MAG":"3044464274","ArXiv":"2007.11018","DOI":"10.1007/978-3-030-58571-6_2","CorpusId":220686422},"title":"Learning Object Relation Graph and Tentative Policy for Visual Navigation"},{"paperId":"fb98a5e61efb13a76bd3b02d18860460ef012655","externalIds":{"MAG":"3035189375","DBLP":"journals/corr/abs-2007-11121","ArXiv":"2007.11121","CorpusId":220686648},"title":"PackIt: A Virtual Environment for Geometric Planning"},{"paperId":"c9f9ee3659c2a855b33ae256e98b05c51b2e30b7","externalIds":{"DBLP":"journals/corr/abs-2007-01851","MAG":"3006646738","ArXiv":"2007.01851","DOI":"10.15607/RSS.2020.XVI.002","CorpusId":214160345},"title":"Swoosh! Rattle! Thump! - Actions that Sound"},{"paperId":"dbaa1b97034d647383f611c1010f8f9ac35aefce","externalIds":{"DBLP":"conf/wacv/Ben-Shabat0SCOL21","ArXiv":"2007.00394","MAG":"3040046528","DOI":"10.1109/WACV48630.2021.00089","CorpusId":220280924},"title":"The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose"},{"paperId":"f76c9834a249a97ebcaf225a216c6076032af99c","externalIds":{"DBLP":"conf/cvpr/FangWGL20","MAG":"3035198432","DOI":"10.1109/cvpr42600.2020.01146","CorpusId":219964473},"title":"GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"89962792268cd60c2b09d8f44030032a0ec94c75","externalIds":{"MAG":"3045497534","DBLP":"journals/corr/abs-2002-10158","ArXiv":"2002.10158","DOI":"10.1007/s11370-020-00324-9","CorpusId":211259358},"title":"Robot perception of static and dynamic objects with an autonomous floor scrubber"},{"paperId":"fd83bfb69b874509a964d9984061fdd6c1634fe6","externalIds":{"MAG":"3043647281","PubMedCentral":"7367864","DOI":"10.1038/s41467-020-17266-6","CorpusId":262399418,"PubMed":"32681088"},"title":"Automated extraction of chemical synthesis actions from experimental procedures"},{"paperId":"f4cf4246f3882aa6337e9c05d5675a3b8463a32e","externalIds":{"MAG":"2993086250","ArXiv":"1912.01734","DBLP":"journals/corr/abs-1912-01734","DOI":"10.1109/cvpr42600.2020.01075","CorpusId":208617407},"title":"ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks"},{"paperId":"0bc855f84668b35cb65618d996d09f6e434d28c9","externalIds":{"MAG":"2981344907","DBLP":"journals/corr/abs-1910-10897","ArXiv":"1910.10897","CorpusId":204852201},"title":"Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"},{"paperId":"3e519d85cdcefdd1d2ad89829d6ad445695d8c58","externalIds":{"DBLP":"journals/corr/abs-1910-11215","ArXiv":"1910.11215","MAG":"2981626359","CorpusId":204851795},"title":"RoboNet: Large-Scale Multi-Robot Learning"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"6d5fd1c40604ebbda2de58b29fbdaa97745ce7d6","externalIds":{"DBLP":"conf/iccv/KongWDKTM19","MAG":"2991451943","DOI":"10.1109/ICCV.2019.00875","CorpusId":207980205},"title":"MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding"},{"paperId":"a2fdfda785b3a2a0178d174daa515377c531f222","externalIds":{"DBLP":"journals/ral/JamesMAD20","MAG":"2975395092","ArXiv":"1909.12271","DOI":"10.1109/LRA.2020.2974707","CorpusId":202889132},"title":"RLBench: The Robot Learning Benchmark & Learning Environment"},{"paperId":"82a6c7aec68d81deeb6fc6858d63d51604fd7c3c","externalIds":{"DBLP":"conf/iros/Kirsanov0GKSVSZ19","MAG":"2976604869","ArXiv":"1909.12146","DOI":"10.1109/IROS40897.2019.8967921","CorpusId":202888975},"title":"DISCOMAN: Dataset of Indoor SCenes for Odometry, Mapping And Navigation"},{"paperId":"33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b","externalIds":{"MAG":"2973525135","DBLP":"conf/iclr/BakerKMWPMM20","ArXiv":"1909.07528","CorpusId":202583612},"title":"Emergent Tool Use From Multi-Agent Autocurricula"},{"paperId":"4a3e9a51f6d7261a64aedae4dba4d097ef7d6a23","externalIds":{"DBLP":"journals/corr/abs-1909-06980","ArXiv":"1909.06980","MAG":"2972998376","DOI":"10.1109/ICRA48506.2021.9561428","CorpusId":202577886},"title":"kPAM-SC: Generalizable Manipulation Planning using KeyPoint Affordance and Shape Completion"},{"paperId":"79bc6e1fe465aec49d7f0252f295c0ad9cdaf389","externalIds":{"DBLP":"conf/emnlp/NguyenD19","ACL":"D19-1063","MAG":"2970340522","ArXiv":"1909.01871","DOI":"10.18653/v1/D19-1063","CorpusId":202538463},"title":"Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning"},{"paperId":"f33ae3a6f47ff3897a7ff12c6a0bacec2223d6d6","externalIds":{"MAG":"2955035422","DBLP":"journals/jmlr/KroemerNK21","ArXiv":"1907.03146","CorpusId":195833499},"title":"A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms"},{"paperId":"fee1e72a23203f00943db68ba9c0e444e12097aa","externalIds":{"MAG":"2950069298","DOI":"10.1126/science.aat8414","CorpusId":195187611,"PubMed":"31221831"},"title":"Trends and challenges in robot manipulation"},{"paperId":"5c09f5b0f22c1fb5fa7035a44ed933da835f5b3f","externalIds":{"MAG":"2952768114","ArXiv":"1906.08236","DBLP":"journals/corr/abs-1906-08236","CorpusId":195069489},"title":"PyRobot: An Open-source Robotics Framework for Research and Benchmarking"},{"paperId":"8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad","externalIds":{"MAG":"2946609015","DBLP":"journals/corr/abs-1905-07830","ACL":"P19-1472","ArXiv":"1905.07830","DOI":"10.18653/v1/P19-1472","CorpusId":159041722},"title":"HellaSwag: Can a Machine Really Finish Your Sentence?"},{"paperId":"1bdd8f6d3900e6e7fe492ea21fcdd87f3d8c857f","externalIds":{"MAG":"3034578524","ArXiv":"1904.10151","DBLP":"conf/cvpr/QiW0WWSH20","DOI":"10.1109/cvpr42600.2020.01000","CorpusId":214264259},"title":"REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments"},{"paperId":"c8c76626db4246c944642e86d19665025fa7deb4","externalIds":{"DBLP":"conf/naacl/TanYB19","MAG":"2951271423","ArXiv":"1904.04195","ACL":"N19-1268","DOI":"10.18653/v1/N19-1268","CorpusId":102352060},"title":"Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout"},{"paperId":"d0bfd3cb732471a0843a39d2d047caf60a844466","externalIds":{"ArXiv":"1903.02741","DBLP":"conf/cvpr/ZhangGJZZ19","MAG":"2953612685","DOI":"10.1109/CVPR.2019.00546","CorpusId":71148268},"title":"RAVEN: A Dataset for Relational and Analogical Visual REasoNing"},{"paperId":"e27e78c33288728f66f7dab2fe2696ddbc5c1026","externalIds":{"MAG":"2922303317","DBLP":"journals/corr/abs-1903-02874","ArXiv":"1903.02874","DOI":"10.1109/CVPR.2019.00130","CorpusId":71147568},"title":"COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis"},{"paperId":"48e86729d2b60f47a6888dd57948ac484678d92c","externalIds":{"MAG":"3003296541","DBLP":"journals/corr/abs-1903-00425","ArXiv":"1903.00425","DOI":"10.1109/IROS40897.2019.8968115","CorpusId":67855717},"title":"Generating Grasp Poses for a High-DOF Gripper Using Neural Networks"},{"paperId":"5d81569e4f35a7afcb23a49bd5185d0e8bd496e3","externalIds":{"MAG":"2900742918","DOI":"10.1016/J.PNUCENE.2018.10.023","CorpusId":126032566},"title":"A review of ground-based robotic systems for the characterization of nuclear environments"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","externalIds":{"MAG":"2919420119","DBLP":"conf/naacl/DuaWDSS019","ACL":"N19-1246","ArXiv":"1903.00161","DOI":"10.18653/v1/N19-1246","CorpusId":67855846},"title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"e41f00c02053aa4aaa1dd5537c549570cab95ccd","externalIds":{"MAG":"2911230430","DBLP":"journals/scirobotics/X19","DOI":"10.1126/scirobotics.aaw3520","CorpusId":59567370,"PubMed":"33137765"},"title":"Robot learningâBeyond imitation"},{"paperId":"57c7ced90b8c635f8346983651363379d0357dc2","externalIds":{"DBLP":"journals/scirobotics/MatlSDDMG19","MAG":"2910474428","DOI":"10.1126/scirobotics.aau4984","CorpusId":58031589,"PubMed":"33137754"},"title":"Learning ambidextrous robot grasping policies"},{"paperId":"05146ae7935f56f8b1d082b490dc8d879b93d5f3","externalIds":{"MAG":"2909047179","DBLP":"journals/scirobotics/FullJFBCCDNT19","DOI":"10.1126/scirobotics.aaw1826","CorpusId":58031576,"PubMed":"33137759"},"title":"Ten robotics technologies of the year"},{"paperId":"3ab84417632f676acc4db2d452c0848f6622fb29","externalIds":{"MAG":"2908802515","DBLP":"journals/scirobotics/Ramirez-AmaroBK19","DOI":"10.1126/scirobotics.aav1530","CorpusId":58031615,"PubMed":"33137756"},"title":"Purposive learning: Robot reasoning about the meanings of human activities"},{"paperId":"f75f0750a00f6f85107985c70eca9c275b5e0962","externalIds":{"MAG":"3042113477","ArXiv":"1811.12354","DBLP":"conf/cvpr/ChenSMSA19","DOI":"10.1109/CVPR.2019.01282","CorpusId":54078068},"title":"TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments"},{"paperId":"c66b8e508718f4b7f14829e5c2cde0add31d2693","externalIds":{"MAG":"2964935470","DBLP":"conf/cvpr/WangHcGSWWZ19","ArXiv":"1811.10092","DOI":"10.1109/CVPR.2019.00679","CorpusId":53735892},"title":"Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation"},{"paperId":"0df7dd6d836a5df84dafd74bbd65df6d0fa94091","externalIds":{"DBLP":"journals/corr/abs-1810-11043","MAG":"2898525034","ArXiv":"1810.11043","CorpusId":53081044},"title":"One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks"},{"paperId":"cda470bede832f2965e594f9bdee79d6973a91e9","externalIds":{"ArXiv":"1811.02790","MAG":"2898634286","DBLP":"conf/corl/MandlekarZGBSTG18","CorpusId":53057199},"title":"ROBOTURK: A Crowdsourcing Platform for Robotic Skill Learning through Imitation"},{"paperId":"0052b31f07eda7737b5e0e2bf3803c3a32f3f728","externalIds":{"MAG":"2896930824","DBLP":"journals/corr/abs-1810-08575","ArXiv":"1810.08575","CorpusId":53041432},"title":"Supervising strong learners by amplifying weak experts"},{"paperId":"cef5a3fab9bf7a576838e550c1de37a7e253a52f","externalIds":{"ArXiv":"1810.07483","PubMedCentral":"8367442","DBLP":"journals/firai/PaulyAHF21","DOI":"10.3389/frobt.2021.686368","CorpusId":236640100,"PubMed":"34409071"},"title":"O2A: One-Shot Observational Learning with Action Vectors"},{"paperId":"1b19f433a3e8497e9d9bd67efb108521d16b5b85","externalIds":{"MAG":"2994943647","ArXiv":"1810.08272","DBLP":"conf/iclr/Chevalier-Boisvert19","CorpusId":59536625},"title":"BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning"},{"paperId":"c7aea4b653d4e12cb47438960f5689f5f835e073","externalIds":{"DBLP":"journals/corr/abs-1810-06543","MAG":"2895915466","ArXiv":"1810.06543","CorpusId":53116049},"title":"Visual Semantic Navigation using Scene Priors"},{"paperId":"7fe203374ba3ddc8462a7c775b276e53f09dd036","externalIds":{"ArXiv":"1808.08374","MAG":"2888030275","DBLP":"journals/corr/abs-1808-08374","CorpusId":52098702},"title":"NavigationNet: A Large-scale Interactive Indoor Navigation Dataset"},{"paperId":"068d2251c62540c5147a713f02caf38224cd1ca3","externalIds":{"DBLP":"journals/scirobotics/Mataric18","MAG":"2888056933","DOI":"10.1126/scirobotics.aat7451","CorpusId":52074708,"PubMed":"33141723"},"title":"Robots for the people, by the people: Personalizing human-machine interaction"},{"paperId":"39e734da43eb8c72e9549b42e96760545036f8e5","externalIds":{"DBLP":"journals/corr/abs-1808-07036","ACL":"D18-1241","MAG":"2951831170","ArXiv":"1808.07036","DOI":"10.18653/v1/D18-1241","CorpusId":52057510},"title":"QuAC: Question Answering in Context"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","externalIds":{"DBLP":"journals/corr/abs-1808-05326","ArXiv":"1808.05326","ACL":"D18-1009","MAG":"2952007233","DOI":"10.18653/v1/D18-1009","CorpusId":52019251},"title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"56a0ead811a1bf15e42be8a9a007b0299636f213","externalIds":{"DBLP":"journals/corr/abs-1807-03367","MAG":"2915066392","ArXiv":"1807.03367","CorpusId":49669712},"title":"Talk the Walk: Navigating New York City through Grounded Dialogue"},{"paperId":"2e19e455327176d02f9c82e43bd9638d97a9908d","externalIds":{"MAG":"2964041256","DBLP":"journals/corr/abs-1807-00858","ArXiv":"1807.00858","DOI":"10.1177/0278364919849091","CorpusId":49564706},"title":"A dataset of daily interactive manipulation"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","externalIds":{"MAG":"2963323070","ACL":"P18-2124","ArXiv":"1806.03822","DBLP":"journals/corr/abs-1806-03822","DOI":"10.18653/v1/P18-2124","CorpusId":47018994},"title":"Know What You Donât Know: Unanswerable Questions for SQuAD"},{"paperId":"7139a5f730652abbeabf9e140009907d2c7da3e5","externalIds":{"MAG":"2799002257","DBLP":"conf/cvpr/PuigRBLWF018","ArXiv":"1806.07011","DOI":"10.1109/CVPR.2018.00886","CorpusId":49317780},"title":"VirtualHome: Simulating Household Activities Via Programs"},{"paperId":"893186c6bc08a17cb3f9f94fa3f14e9ad20b0525","externalIds":{"MAG":"2805984364","DBLP":"conf/nips/FriedHCRAMBSKD18","ArXiv":"1806.02724","CorpusId":46979001},"title":"Speaker-Follower Models for Vision-and-Language Navigation"},{"paperId":"fc50c9392fd23b6c88915177c6ae904a498aacea","externalIds":{"MAG":"2949810620","ArXiv":"1804.02748","DBLP":"journals/corr/abs-1804-02748","CorpusId":4710439},"title":"Scaling Egocentric Vision: The EPIC-KITCHENS Dataset"},{"paperId":"6272f7355e28148e576cad0a14e240c333299d78","externalIds":{"MAG":"2794784791","ArXiv":"1803.11469","DBLP":"journals/corr/abs-1803-11469","DOI":"10.1109/IROS.2018.8593950","CorpusId":215827111},"title":"Jacquard: A Large Scale Dataset for Robotic Grasp Detection"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","externalIds":{"ArXiv":"1803.05457","DBLP":"journals/corr/abs-1803-05457","MAG":"2794325560","CorpusId":3922816},"title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"89c8aad71433f7638d2e2c009e1ea20e039f832d","externalIds":{"DBLP":"journals/corr/abs-1712-05474","MAG":"2776202271","ArXiv":"1712.05474","CorpusId":28328610},"title":"AI2-THOR: An Interactive 3D Environment for Visual AI"},{"paperId":"c02a6b732abc4fbb8ac8237e7ce5f97cc9797845","externalIds":{"MAG":"2769044080","DBLP":"journals/scirobotics/CorrellKP17","DOI":"10.1126/scirobotics.aar4527","CorpusId":9196117,"PubMed":"33157906"},"title":"Will robots be bodies with brains or brains with bodies?"},{"paperId":"c37c23b12e00168833eccff8025a830ce27c5abc","externalIds":{"MAG":"2952157315","DBLP":"conf/cvpr/AndersonWTB0S0G18","ArXiv":"1711.07280","DOI":"10.1109/CVPR.2018.00387","CorpusId":4673790},"title":"Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"},{"paperId":"482c0cbfffa77154e3c879c497f50b605297d5bc","externalIds":{"MAG":"2755546070","ArXiv":"1709.04905","DBLP":"conf/corl/FinnYZAL17","CorpusId":22221787},"title":"One-Shot Visual Imitation Learning via Meta-Learning"},{"paperId":"41f2a087031944f9b990eb102f59b4ff58d6b5ef","externalIds":{"DBLP":"journals/corr/MahlerLNLDLOG17","ArXiv":"1703.09312","MAG":"2600030077","DOI":"10.15607/RSS.2017.XIII.058","CorpusId":6138957},"title":"Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics"},{"paperId":"e10a5e0baf2aa87d804795af071808a9377cc80a","externalIds":{"MAG":"2784025607","DBLP":"conf/aaai/ZhouXC18","ArXiv":"1703.09788","DOI":"10.1609/aaai.v32i1.12342","CorpusId":19713015},"title":"Towards Automatic Learning of Procedures From Web Instructional Videos"},{"paperId":"e296a89be7ce1cad7e4f2a86b2cc6a527442da62","externalIds":{"DBLP":"journals/ijrr/Ruiz-SarmientoG17","MAG":"2606926518","DOI":"10.1177/0278364917695640","CorpusId":23176814},"title":"Robot@Home, a robotic dataset for semantic mapping of home environments"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","externalIds":{"DBLP":"journals/corr/JohnsonHMFZG16","ArXiv":"1612.06890","MAG":"2953212746","DOI":"10.1109/CVPR.2017.215","CorpusId":15458100},"title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"4ab53de69372ec2cd2d90c126b6a100165dc8ed1","externalIds":{"DBLP":"journals/corr/HoE16","ArXiv":"1606.03476","MAG":"2949080919","CorpusId":16153365},"title":"Generative Adversarial Imitation Learning"},{"paperId":"494e2d5b40dcebde349f9872c7317e5003f9c5d2","externalIds":{"MAG":"2962736495","DBLP":"journals/ijrr/LevinePKIQ18","ArXiv":"1603.02199","DOI":"10.1177/0278364917710318","CorpusId":13072941},"title":"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection"},{"paperId":"f03b4ff1b4943691cec703b508c0a91f2d97a881","externalIds":{"MAG":"2201912979","DBLP":"journals/corr/PintoG15","ArXiv":"1509.06825","DOI":"10.1109/ICRA.2016.7487517","CorpusId":3177253},"title":"Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours"},{"paperId":"e0ce0e87edb08038c1432755909bb7270026ec50","externalIds":{"DBLP":"journals/ram/CalliWSSAD15","MAG":"1806263934","DOI":"10.1109/MRA.2015.2448951","CorpusId":8170683},"title":"Benchmarking in Manipulation Research: Using the Yale-CMU-Berkeley Object and Model Set"},{"paperId":"da9e411fcf740569b6b356f330a1d0fc077c8d7c","externalIds":{"MAG":"24089286","ArXiv":"1212.0402","DBLP":"journals/corr/abs-1212-0402","CorpusId":7197134},"title":"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"},{"paperId":"49435aab7cdf259335725acc96691f755e436f55","externalIds":{"MAG":"2019660985","DBLP":"conf/cvpr/RohrbachAAS12","DOI":"10.1109/CVPR.2012.6247801","CorpusId":9349950},"title":"A database for fine grained activity detection of cooking activities"},{"paperId":"bce01bd0f070a4aec6600ba78b031cb8a7e554e3","externalIds":{"MAG":"2081034428","DBLP":"conf/hri/AkgunCYT12","DOI":"10.1145/2157689.2157815","CorpusId":16864200},"title":"Trajectories and keyframes for kinesthetic teaching: A human-robot interaction perspective"},{"paperId":"8b3b8848a311c501e704c45c6d50430ab7068956","externalIds":{"MAG":"2126579184","DBLP":"conf/iccv/KuehneJGPS11","DOI":"10.1109/ICCV.2011.6126543","CorpusId":206769852},"title":"HMDB: A large video database for human motion recognition"},{"paperId":"3c104b0e182a5f514d3aebecc93629bbcf1434ac","externalIds":{"MAG":"2123435073","DBLP":"conf/icra/JiangMS11","DOI":"10.1109/ICRA.2011.5980145","CorpusId":206849653},"title":"Efficient grasping from RGBD images: Learning using a new rectangle representation"},{"paperId":"128cb6b891aee1b5df099acb48e2efecfcff689f","externalIds":{"DBLP":"conf/aaaiss/Levesque11","MAG":"2267020232","CorpusId":15710851},"title":"The Winograd Schema Challenge"},{"paperId":"176f4a67510b8cde4de72a6bf8586ca6957d9922","externalIds":{"MAG":"2152255989","DOI":"10.1016/j.tics.2010.01.001","CorpusId":15142890,"PubMed":"20138795"},"title":"What determines our navigational abilities?"},{"paperId":"9f0ffa7012b7a802535284a232a412f84dc6f7b0","externalIds":{"MAG":"2078046413","DBLP":"conf/iccvw/TenorthBB09","DOI":"10.1109/ICCVW.2009.5457583","CorpusId":10895196},"title":"The TUM Kitchen Data Set of everyday manipulation activities for motion tracking and action recognition"},{"paperId":"8fa9c9568d8de9cd3536d6f99d99fe957d45e0a1","externalIds":{"DBLP":"journals/ijrr/SaxenaDN08","MAG":"2041376653","DOI":"10.1177/0278364907087172","CorpusId":1744418},"title":"Robotic Grasping of Novel Objects using Vision"},{"paperId":"342fe6a6338e73fd4d34c4f37f41e3bbad274dd2","externalIds":{"DOI":"10.1002/9781119991083.ch42","CorpusId":219375210},"title":"Networks"},{"paperId":"70987dd4fb41109c723bcff127818870662aa108","externalIds":{"DBLP":"journals/corr/abs-2404-19456","DOI":"10.48550/arXiv.2404.19456","CorpusId":277742754},"title":"Imitation Learning: A Survey of Learning Methods, Environments and Metrics"},{"paperId":"8a694ac007cad520325cbe1e2e150aa6e7d80d32","externalIds":{"CorpusId":259372701},"title":"Tactile Pose Feedback for Closed-loop Manipulation Tasks"},{"paperId":"62a869c8e9b116d813787102f277ac56fc3db31c","externalIds":{"DBLP":"journals/corr/abs-2307-00595","DOI":"10.48550/arXiv.2307.00595","CorpusId":263898161},"title":"RH20T: A Robotic Dataset for Learning Diverse Skills in One-Shot"},{"paperId":"5b06056345034e1559ef8680190cdccc79a2196d","externalIds":{"DBLP":"conf/icml/JiangG0WDC0AZF23","CorpusId":260844398},"title":"VIMA: Robot Manipulation with Multimodal Prompts"},{"paperId":"5ce94181ea702f69c3651dce721d6bd8026b8106","externalIds":{"DBLP":"journals/corr/abs-2308-03427","DOI":"10.48550/arXiv.2308.03427","CorpusId":265381326},"title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"},{"paperId":"46299fee72ca833337b3882ae1d8316f44b32b3c","externalIds":{"DBLP":"journals/corr/abs-2303-11366","CorpusId":257636839},"title":"Reflexion: an autonomous agent with dynamic memory and self-reflection"},{"paperId":"4747e72c5bc706c50e76953188f0144df18992d0","externalIds":{"DBLP":"journals/corr/abs-2307-07924","DOI":"10.48550/arXiv.2307.07924","CorpusId":259936967},"title":"Communicative Agents for Software Development"},{"paperId":"e6b79c12032884be401da08177a7c33ca02a6985","externalIds":{"DBLP":"journals/corr/abs-2306-05716","DOI":"10.48550/arXiv.2306.05716","CorpusId":268890276},"title":"Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots"},{"paperId":"e13b52a3ee01320165d4ba49f20708f9907ed563","externalIds":{"DBLP":"journals/corr/abs-2306-09896","DOI":"10.48550/arXiv.2306.09896","CorpusId":269043866},"title":"Demystifying GPT Self-Repair for Code Generation"},{"paperId":"69764fcc646e4c608ac08eeb4c784cf8465268d2","externalIds":{"DBLP":"conf/corl/0002ZWGSMWLLSAH22","CorpusId":255198985},"title":"BEHAVIOR-1K: A Benchmark for Embodied AI with 1, 000 Everyday Activities and Realistic Simulation"},{"paperId":"b48713978407f58653323ff94e35b81c624ea345","externalIds":{"DBLP":"journals/corr/abs-2202-00199","CorpusId":246442312},"title":"RFUniverse: A Physics-based Action-centric Interactive Environment for Everyday Household Tasks"},{"paperId":"414a476f83634e3b452b243ed7460c9ef3d1aaa4","externalIds":{"DBLP":"journals/tmlr/WeihsYBFMMK22","CorpusId":252004773},"title":"Benchmarking Progress to Infant-Level Physical Reasoning in AI"},{"paperId":"c1f1ec160744ba4a9526cc6a007f2d637ac74732","externalIds":{"DBLP":"journals/corr/abs-2212-04385","DOI":"10.48550/arXiv.2212.04385","CorpusId":263874325},"title":"BEVBert: Topo-Metric Map Pre-training for Language-guided Navigation"},{"paperId":"d43991b2fc2921c4fff5dea8ecb60b32fc0e1512","externalIds":{"CorpusId":199533486},"title":"Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning"},{"paperId":"b9effa7f0d628ab825f07bee059a3997257348ce","externalIds":{"CorpusId":51446499},"title":"Deep Reinforcement Learning with Skill Library : Exploring with Temporal Abstractions and coarse approximate Dynamics Models"},{"paperId":"cc58c920e2988331e8509e6d7c4279cc6ea85c1d","externalIds":{"CorpusId":45150299},"title":"Robot Learning from Demonstration : Kinesthetic Teaching vs . Teleoperation"},{"paperId":"70d6dfdc40c4681ba5d51d60116db0311b5126ce","externalIds":{"DBLP":"reference/db/Hiemstra18","DOI":"10.1007/978-0-387-39940-9_923","CorpusId":10924669},"title":"Language Models"},{"paperId":"12bdfc10f7c17a0a647589d5b4f10ad2d0049c93","externalIds":{"MAG":"105287674","CorpusId":16721121},"title":"Guide to the Carnegie Mellon University Multimodal Activity (CMU-MMAC) Database"},{"paperId":"97efafdb4a3942ab3efba53ded7413199f79c054","externalIds":{"MAG":"2121863487","DBLP":"journals/tnn/SuttonB98","DOI":"10.1109/TNN.1998.712192","CorpusId":60035920},"title":"Reinforcement Learning: An Introduction"},{"paperId":"cfee1826dd4743eab44c6e27a0cc5970effa4d80","externalIds":{"CorpusId":264403242},"title":"Improving Image Generation with Better Captions"}]}