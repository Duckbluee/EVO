{"paperId":"bc155d2fd40da6d328ae895ff97d067171e7f3ab","externalIds":{"DBLP":"journals/corr/abs-2401-14656","ArXiv":"2401.14656","DOI":"10.1145/3715318","CorpusId":267301629},"title":"Scientific Large Language Models: A Survey on Biological & Chemical Domains","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2401.14656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2254277130","name":"Qiang Zhang"},{"authorId":"2254282405","name":"Keyan Ding"},{"authorId":"2248078321","name":"Tianwen Lv"},{"authorId":"2281680745","name":"Xinda Wang"},{"authorId":"2281644597","name":"Qingyu Yin"},{"authorId":"2281713161","name":"Yiwen Zhang"},{"authorId":"2281881162","name":"Jing Yu"},{"authorId":"2281682186","name":"Yuhao Wang"},{"authorId":"2254278404","name":"Xiaotong Li"},{"authorId":"2281643218","name":"Zhuoyi Xiang"},{"authorId":"2231289043","name":"Zhuang Xiang"},{"authorId":"2255087679","name":"Zeyuan Wang"},{"authorId":"2254241045","name":"Ming Qin"},{"authorId":"2281678525","name":"Mengyao Zhang"},{"authorId":"2268799310","name":"Jinlu Zhang"},{"authorId":"94332696","name":"Jiyu Cui"},{"authorId":"2308997018","name":"Renjun Xu"},{"authorId":"2144211996","name":"Hongyang Chen"},{"authorId":"2152774386","name":"Xiaohui Fan"},{"authorId":"2281643799","name":"Huabin Xing"},{"authorId":"2254343747","name":"Huajun Chen"}],"abstract":"Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this article, we endeavor to methodically delineate the concept of “scientific language,” whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs."}