{"references":[{"paperId":"ce4e101950554c41d3b35f5b297722abd1ce6403","externalIds":{"ArXiv":"2402.09171","DBLP":"conf/sigsoft/AlshahwanCFGHHM24","DOI":"10.1145/3663529.3663839","CorpusId":267657828},"title":"Automated Unit Test Improvement using Large Language Models at Meta"},{"paperId":"1f2a20a6efaf83214861dddae4a38a83ae18fe32","externalIds":{"ArXiv":"2401.14196","DBLP":"journals/corr/abs-2401-14196","DOI":"10.48550/arXiv.2401.14196","CorpusId":267211867},"title":"DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"},{"paperId":"8c1243e089621d09025e1e51e8e01cb2cb20eabf","externalIds":{"ArXiv":"2401.10491","DBLP":"conf/iclr/WanH0QB024","DOI":"10.48550/arXiv.2401.10491","CorpusId":267061245},"title":"Knowledge Fusion of Large Language Models"},{"paperId":"fb4dc0178e5d7347b1615c48caf05347b6e5eb48","externalIds":{"DBLP":"journals/corr/abs-2401-05561","ArXiv":"2401.05561","DOI":"10.48550/arXiv.2401.05561","CorpusId":266933236},"title":"TrustLLM: Trustworthiness in Large Language Models"},{"paperId":"6f657eaca838919ce7283b9fe435fe99ce8961bb","externalIds":{"DBLP":"journals/corr/abs-2401-03568","ArXiv":"2401.03568","DOI":"10.48550/arXiv.2401.03568","CorpusId":266844635},"title":"Agent AI: Surveying the Horizons of Multimodal Interaction"},{"paperId":"560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd","externalIds":{"ArXiv":"2401.02385","DBLP":"journals/corr/abs-2401-02385","DOI":"10.48550/arXiv.2401.02385","CorpusId":266755802},"title":"TinyLlama: An Open-Source Small Language Model"},{"paperId":"c3d1832ed0444f75d44116fabbdda891aebc4b01","externalIds":{"ArXiv":"2401.02415","DBLP":"conf/acl/WuGGLWFSL24","DOI":"10.48550/arXiv.2401.02415","CorpusId":266755997},"title":"LLaMA Pro: Progressive LLaMA with Block Expansion"},{"paperId":"575f403261d5f99526f0b4dfc8644352d6c4467a","externalIds":{"DBLP":"conf/acl/0005RSMBKPNL24","ArXiv":"2401.00908","DOI":"10.48550/arXiv.2401.00908","CorpusId":266725585},"title":"DocLLM: A layout-aware generative language model for multimodal document understanding"},{"paperId":"46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5","externalIds":{"ArXiv":"2312.10997","DBLP":"journals/corr/abs-2312-10997","CorpusId":266359151},"title":"Retrieval-Augmented Generation for Large Language Models: A Survey"},{"paperId":"7bbc7595196a0606a07506c4fb1473e5e87f6082","externalIds":{"ArXiv":"2312.00752","DBLP":"journals/corr/abs-2312-00752","CorpusId":265551773},"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","externalIds":{"DBLP":"conf/eccv/LiuCLZLRZYSZZGL24","ArXiv":"2311.05437","DOI":"10.48550/arXiv.2311.05437","CorpusId":265067489},"title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"cdcf3f36866ef1e16eba26d57c2324362247ba84","externalIds":{"DBLP":"journals/corr/abs-2310-16944","ArXiv":"2310.16944","DOI":"10.48550/arXiv.2310.16944","CorpusId":264490502},"title":"Zephyr: Direct Distillation of LM Alignment"},{"paperId":"c85268696fe1435605ae66a18653cfdcf8153753","externalIds":{"DBLP":"conf/nips/FuAGJETSPRR23","ArXiv":"2310.12109","DOI":"10.48550/arXiv.2310.12109","CorpusId":264288993},"title":"Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"},{"paperId":"db633c6b1c286c0386f0078d8a2e6224e03a6227","externalIds":{"ArXiv":"2310.06825","DBLP":"journals/corr/abs-2310-06825","DOI":"10.48550/arXiv.2310.06825","CorpusId":263830494},"title":"Mistral 7B"},{"paperId":"0c72450890a54b68d63baa99376131fda8f06cf9","externalIds":{"ArXiv":"2309.07864","DBLP":"journals/corr/abs-2309-07864","DOI":"10.48550/arXiv.2309.07864","CorpusId":261817592},"title":"The Rise and Potential of Large Language Model Based Agents: A Survey"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"e26888285436bc7998e5c95102a9beb60144be5e","externalIds":{"DBLP":"journals/corr/abs-2309-05463","ArXiv":"2309.05463","DOI":"10.48550/arXiv.2309.05463","CorpusId":261696657},"title":"Textbooks Are All You Need II: phi-1.5 technical report"},{"paperId":"0b0debb710366cdff461938c80763eace1651af6","externalIds":{"DBLP":"journals/corr/abs-2308-12950","ArXiv":"2308.12950","DOI":"10.48550/arXiv.2308.12950","CorpusId":261100919},"title":"Code Llama: Open Foundation Models for Code"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","externalIds":{"DBLP":"journals/fcsc/WangMFZYZCTCLZWW24","ArXiv":"2308.11432","DOI":"10.1007/s11704-024-40231-1","CorpusId":261064713},"title":"A survey on large language model based autonomous agents"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","externalIds":{"ArXiv":"2308.10792","DBLP":"journals/corr/abs-2308-10792","DOI":"10.1145/3777411","CorpusId":261049152},"title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"2dfb9171e180dcb0af23d305e024d43d311708ab","externalIds":{"DBLP":"journals/corr/abs-2308-10882","ArXiv":"2308.10882","DOI":"10.48550/arXiv.2308.10882","CorpusId":261048876},"title":"Giraffe: Adventures in Expanding Context Lengths in LLMs"},{"paperId":"377d4d6c1be01b9df32edfd94b2c5946971b0108","externalIds":{"ArXiv":"2308.01285","DBLP":"journals/corr/abs-2308-01285","DOI":"10.48550/arXiv.2308.01285","CorpusId":260379133},"title":"Flows: Building Blocks of Reasoning and Collaborating AI"},{"paperId":"e01ab53663e5df5961a021506a9cb09f4efc3788","externalIds":{"DBLP":"journals/corr/abs-2307-10169","ArXiv":"2307.10169","DOI":"10.48550/arXiv.2307.10169","CorpusId":259982665},"title":"Challenges and Applications of Large Language Models"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"888728745dbb769e29ed475d4f7661eebe1a71cf","externalIds":{"DBLP":"journals/tist/ChangWWWYZCYWWYZCYYX24","ArXiv":"2307.03109","DOI":"10.1145/3641289","CorpusId":259360395},"title":"A Survey on Evaluation of Large Language Models"},{"paperId":"b069c32fcd77160f944ab3ba71ab6f0cfb782c68","externalIds":{"DBLP":"conf/nips/TworkowskiSPWMM23","ArXiv":"2307.03170","DOI":"10.48550/arXiv.2307.03170","CorpusId":259360592},"title":"Focused Transformer: Contrastive Training for Context Scaling"},{"paperId":"c12b80b44d9acfe6cd92fdf965264c4b706c367c","externalIds":{"DBLP":"conf/nips/ZhuangYWSZ23","ArXiv":"2306.13304","DOI":"10.48550/arXiv.2306.13304","CorpusId":259243960},"title":"ToolQA: A Dataset for LLM Question Answering with External Tools"},{"paperId":"2922768fd451ecdb45f48c1a83eb57f54a91221b","externalIds":{"DBLP":"journals/corr/abs-2306-11644","ArXiv":"2306.11644","CorpusId":259203998},"title":"Textbooks Are All You Need"},{"paperId":"a65cbc88c5cb7af1103c060e48f63f95ad403b3c","externalIds":{"ArXiv":"2306.08997","DBLP":"journals/corr/abs-2306-08997","CorpusId":259165520},"title":"Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models"},{"paperId":"9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6","externalIds":{"ArXiv":"2306.08302","DBLP":"journals/corr/abs-2306-08302","DOI":"10.1109/TKDE.2024.3352100","CorpusId":259165563},"title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap"},{"paperId":"fbd2c8089870814449f9254a711041bbae145a82","externalIds":{"ArXiv":"2306.04751","DBLP":"journals/corr/abs-2306-04751","DOI":"10.48550/arXiv.2306.04751","CorpusId":259108263},"title":"How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"},{"paperId":"0244aeb7c6927e2fb0c2e668687e160a00737dbe","externalIds":{"ArXiv":"2306.02707","DBLP":"journals/corr/abs-2306-02707","DOI":"10.48550/arXiv.2306.02707","CorpusId":259075316},"title":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4"},{"paperId":"7a1e71cb1310c4a873e7a4e54d1a6dab0553adce","externalIds":{"ArXiv":"2306.01116","DBLP":"journals/corr/abs-2306-01116","DOI":"10.48550/arXiv.2306.01116","CorpusId":259063761},"title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"7e72eb196b7c90b3a5d6385af536fe8e5934fb82","externalIds":{"ArXiv":"2305.17784","DBLP":"journals/corr/abs-2305-17784","DOI":"10.48550/arXiv.2305.17784","CorpusId":258959337},"title":"ConvGenVisMo: Evaluation of Conversational Generative Vision Models"},{"paperId":"7d8905a1fd288068f12c8347caeabefd36d0dd6c","externalIds":{"DBLP":"journals/corr/abs-2305-15334","ArXiv":"2305.15334","DOI":"10.52202/079017-4020","CorpusId":258865184},"title":"Gorilla: Large Language Model Connected with Massive APIs"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"bd5deadc58ee45b5e004378ba1d54a96bc947b4a","externalIds":{"ArXiv":"2305.14251","DBLP":"conf/emnlp/MinKLLYKIZH23","DOI":"10.48550/arXiv.2305.14251","CorpusId":258841470},"title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"},{"paperId":"90027ca7802645671a69b00b65e1fa94e6b63544","externalIds":{"ArXiv":"2305.18323","DBLP":"journals/corr/abs-2305-18323","DOI":"10.48550/arXiv.2305.18323","CorpusId":258967566},"title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"},{"paperId":"2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4","externalIds":{"DBLP":"journals/corr/abs-2305-14552","ArXiv":"2305.14552","DOI":"10.48550/arXiv.2305.14552","CorpusId":258865517},"title":"Sources of Hallucination by Large Language Models on Inference Tasks"},{"paperId":"026b3396a63ed5772329708b7580d633bb86bec9","externalIds":{"DBLP":"conf/emnlp/PengAAAABCCCDDG23","ArXiv":"2305.13048","DOI":"10.18653/v1/2023.findings-emnlp.936","CorpusId":258832459},"title":"RWKV: Reinventing RNNs for the Transformer Era"},{"paperId":"e0384ba36555232c587d4a80d527895a095a9001","externalIds":{"DBLP":"journals/corr/abs-2305-11747","ArXiv":"2305.11747","DOI":"10.48550/arXiv.2305.11747","CorpusId":258832847},"title":"HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models"},{"paperId":"b6d6c33298b852cf63edac233deca70530d69a2a","externalIds":{"ArXiv":"2305.10403","DBLP":"journals/corr/abs-2305-10403","CorpusId":258740735},"title":"PaLM 2 Technical Report"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"ea72fb2a0d340f9d14fbcf300cd5f5fbbe1050bb","externalIds":{"DBLP":"journals/corr/abs-2305-09617","ArXiv":"2305.09617","DOI":"10.48550/arXiv.2305.09617","CorpusId":258715226},"title":"Towards Expert-Level Medical Question Answering with Large Language Models"},{"paperId":"88884b8806262a4095036041e3567d450dba39f7","externalIds":{"DBLP":"journals/corr/abs-2305-06983","ArXiv":"2305.06983","DOI":"10.48550/arXiv.2305.06983","CorpusId":258615731},"title":"Active Retrieval Augmented Generation"},{"paperId":"3e4085e5869f1b7959707a1e1d7d273b6057eb4e","externalIds":{"DBLP":"journals/tmlr/LiAZMKMMALCLZZW23","ArXiv":"2305.06161","CorpusId":258588247},"title":"StarCoder: may the source be with you!"},{"paperId":"e01515c6138bc525f7aec30fc85f2adf028d4156","externalIds":{"DBLP":"journals/corr/abs-2305-03047","ArXiv":"2305.03047","DOI":"10.48550/arXiv.2305.03047","CorpusId":258479665},"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"},{"paperId":"886e0962479ec6dac563666399ca4c96a468fcaa","externalIds":{"ArXiv":"2305.02309","DBLP":"journals/corr/abs-2305-02309","DOI":"10.48550/arXiv.2305.02309","CorpusId":258461229},"title":"CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","externalIds":{"DBLP":"conf/icml/BidermanSABOHKP23","ArXiv":"2304.01373","DOI":"10.48550/arXiv.2304.01373","CorpusId":257921893},"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"a8148b9dce6101b080e499b4c48cbe267d745131","externalIds":{"ArXiv":"2303.17071","DBLP":"conf/acl-clinicalnlp/NairSTK24","ACL":"2024.clinicalnlp-1.12","DOI":"10.48550/arXiv.2303.17071","CorpusId":257833743},"title":"DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents"},{"paperId":"0671fd553dd670a4e820553a974bc48040ba0819","externalIds":{"DBLP":"conf/nips/ShinnCGNY23","ArXiv":"2303.11366","CorpusId":258833055},"title":"Reflexion: language agents with verbal reinforcement learning"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","externalIds":{"ArXiv":"2303.09014","DBLP":"journals/corr/abs-2303-09014","DOI":"10.48550/arXiv.2303.09014","CorpusId":257557449},"title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"7c1707db9aafd209aa93db3251e7ebd593d55876","externalIds":{"DBLP":"conf/emnlp/ManakulLG23","ArXiv":"2303.08896","DOI":"10.48550/arXiv.2303.08896","CorpusId":257557820},"title":"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","externalIds":{"ArXiv":"2302.14045","DBLP":"conf/nips/Huang0WHSML0MPL23","DOI":"10.48550/arXiv.2302.14045","CorpusId":257219775},"title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"e5c72b92c48d68594b290c84a8904da7c8335554","externalIds":{"ArXiv":"2302.12813","DBLP":"journals/corr/abs-2302-12813","DOI":"10.48550/arXiv.2302.12813","CorpusId":257205781},"title":"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback"},{"paperId":"998ac3e945857cf2676ee7efdbaf443a0c6f820a","externalIds":{"DBLP":"journals/corr/abs-2302-10866","ArXiv":"2302.10866","DOI":"10.48550/arXiv.2302.10866","CorpusId":257050308},"title":"Hyena Hierarchy: Towards Larger Convolutional Language Models"},{"paperId":"3599a236f285af48782fc30b1341d13ec7320735","externalIds":{"ArXiv":"2302.09419","DBLP":"journals/corr/abs-2302-09419","DOI":"10.48550/arXiv.2302.09419","CorpusId":257039063},"title":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"},{"paperId":"81fa4bd479191424a5cd9c4c7647e34e49c3078b","externalIds":{"DOI":"10.3390/math11041006","CorpusId":256982101},"title":"A Survey on Evaluation Metrics for Machine Translation"},{"paperId":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","externalIds":{"ArXiv":"2302.07842","DBLP":"journals/tmlr/MialonDLNPRRSDC23","CorpusId":256868474},"title":"Augmented Language Models: a Survey"},{"paperId":"ca75356503c540ec9207ba203abe5884564598af","externalIds":{"ArXiv":"2302.07730","DBLP":"journals/corr/abs-2302-07730","DOI":"10.48550/arXiv.2302.07730","CorpusId":256868804},"title":"Transformer models: an introduction and catalog"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","externalIds":{"DBLP":"journals/corr/abs-2302-04761","ArXiv":"2302.04761","DOI":"10.48550/arXiv.2302.04761","CorpusId":256697342},"title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"6052486bc9144dc1730c12bf35323af3792a1fd0","externalIds":{"ArXiv":"2212.13138","DBLP":"journals/corr/abs-2212-13138","PubMedCentral":"10396962","DOI":"10.1038/s41586-023-06291-2","CorpusId":255124952,"PubMed":"37438534"},"title":"Large language models encode clinical knowledge"},{"paperId":"e965e93e76a9e6c4e4863d145b5c007b540d575d","externalIds":{"ArXiv":"2212.12017","DBLP":"journals/corr/abs-2212-12017","CorpusId":255096269},"title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","externalIds":{"ArXiv":"2212.10403","DBLP":"conf/acl/0009C23","DOI":"10.48550/arXiv.2212.10403","CorpusId":254877753},"title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","externalIds":{"ArXiv":"2211.10435","DBLP":"journals/corr/abs-2211-10435","DOI":"10.48550/arXiv.2211.10435","CorpusId":253708270},"title":"PAL: Program-aided Language Models"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","externalIds":{"DBLP":"journals/corr/abs-2211-09085","ArXiv":"2211.09085","CorpusId":253553203},"title":"Galactica: A Large Language Model for Science"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"4610ffb1b016acaa82a2065ffd1a3adbae1ce722","externalIds":{"DBLP":"journals/corr/abs-2211-01910","ArXiv":"2211.01910","DOI":"10.48550/arXiv.2211.01910","CorpusId":253265328},"title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"1bb6d5761903c7ac978188ae36e2648905e95dc5","externalIds":{"ArXiv":"2210.11399","DBLP":"conf/emnlp/TayWC0SSGZRCZMP23","DOI":"10.48550/arXiv.2210.11399","CorpusId":253018395},"title":"Transcending Scaling Laws with 0.1% Extra Compute"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","externalIds":{"DBLP":"journals/corr/abs-2210-03493","ArXiv":"2210.03493","CorpusId":252762275},"title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","externalIds":{"DBLP":"conf/iclr/YaoZYDSN023","ArXiv":"2210.03629","CorpusId":252762395},"title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","externalIds":{"DBLP":"journals/corr/abs-2209-14375","ArXiv":"2209.14375","DOI":"10.48550/arXiv.2209.14375","CorpusId":252596089},"title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"914254fac74a2da051cccf6ca16afcaad416a079","externalIds":{"DBLP":"journals/corr/abs-2208-01448","ArXiv":"2208.01448","DOI":"10.48550/arXiv.2208.01448","CorpusId":251253416},"title":"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"},{"paperId":"f3cf71c51b882fe3111d71c4bf104297d38197f8","externalIds":{"ArXiv":"2207.05608","DBLP":"conf/corl/HuangXXCLFZTMCS22","DOI":"10.48550/arXiv.2207.05608","CorpusId":250451569},"title":"Inner Monologue: Embodied Reasoning through Planning with Language Models"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","externalIds":{"DBLP":"conf/nips/LewkowyczADDMRS22","ArXiv":"2206.14858","DOI":"10.48550/arXiv.2206.14858","CorpusId":250144408},"title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"a8fd9c1625011741f74401ff9bdc1c584e25c86d","externalIds":{"ArXiv":"2206.06336","DBLP":"journals/corr/abs-2206-06336","DOI":"10.48550/arXiv.2206.06336","CorpusId":249626024},"title":"Language Models are General-Purpose Interfaces"},{"paperId":"aa4d9972af3264d032dbee58501ed4ac49477103","externalIds":{"ArXiv":"2205.10487","DBLP":"journals/corr/abs-2205-10487","DOI":"10.48550/arXiv.2205.10487","CorpusId":248986979},"title":"Scaling Laws and Interpretability of Learning from Repeated Data"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","externalIds":{"DBLP":"journals/corr/abs-2203-15556","ArXiv":"2203.15556","CorpusId":247778764},"title":"Training Compute-Optimal Large Language Models"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"0f733817e82026f7c29909a51cb4df7d2685f0e7","externalIds":{"DBLP":"journals/corr/abs-2203-06566","ArXiv":"2203.06566","DOI":"10.1145/3491101.3519729","CorpusId":247447133},"title":"PromptChainer: Chaining Large Language Model Prompts through Visual Programming"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"3def68bd0f856886d34272840a7f81588f2bc082","externalIds":{"DBLP":"journals/corr/abs-2202-03629","ArXiv":"2202.03629","DOI":"10.1145/3571730","CorpusId":246652372},"title":"Survey of Hallucination in Natural Language Generation"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","externalIds":{"ArXiv":"2201.11990","DBLP":"journals/corr/abs-2201-11990","CorpusId":246411325},"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","externalIds":{"DBLP":"journals/corr/abs-2201-08239","ArXiv":"2201.08239","CorpusId":246063428},"title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"6e3f5d05b9da767df3db251030f9a082b8120e2f","externalIds":{"DOI":"10.1163/2214-8647_bnp_e804780","CorpusId":240498814},"title":"Milvus"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","externalIds":{"DBLP":"journals/corr/abs-2112-09332","ArXiv":"2112.09332","CorpusId":245329531},"title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"002c256d30d6be4b23d365a8de8ae0e67e4c9641","externalIds":{"DBLP":"journals/corr/abs-2112-04426","ArXiv":"2112.04426","CorpusId":244954723},"title":"Improving language models by retrieving from trillions of tokens"},{"paperId":"68f141724814839d556a989646194be88641b143","externalIds":{"ArXiv":"2112.11446","DBLP":"journals/corr/abs-2112-11446","CorpusId":245353475},"title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","externalIds":{"DBLP":"conf/iclr/GuGR22","ArXiv":"2111.00396","CorpusId":240354066},"title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"b4e2e29bf0ea891610618b0615b540d4ed5a6004","externalIds":{"ArXiv":"2110.05456","DBLP":"journals/corr/abs-2110-05456","CorpusId":238583083},"title":"Rome was built in 1776: A Case Study on Factual Correctness in Knowledge-Grounded Response Generation"},{"paperId":"12bc45e2268a5742d21a8a37109f8793417cefcc","externalIds":{"ArXiv":"2109.12264","DBLP":"journals/corr/abs-2109-12264","CorpusId":237940507},"title":"More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","externalIds":{"DBLP":"journals/corr/abs-2109-07958","ACL":"2022.acl-long.229","ArXiv":"2109.07958","DOI":"10.18653/v1/2022.acl-long.229","CorpusId":237532606},"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","externalIds":{"ArXiv":"2108.12409","DBLP":"journals/corr/abs-2108-12409","CorpusId":237347130},"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","externalIds":{"DBLP":"journals/corr/abs-2108-07732","ArXiv":"2108.07732","CorpusId":237142385},"title":"Program Synthesis with Large Language Models"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","externalIds":{"DBLP":"journals/csur/LiuYFJHN23","ArXiv":"2107.13586","DOI":"10.1145/3560815","CorpusId":236493269},"title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"319b84be7a843250bc81d7086f79a4126d550277","externalIds":{"DBLP":"journals/corr/abs-2107-02137","ArXiv":"2107.02137","CorpusId":235731579},"title":"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"feba0c47bf12a02c3a725174bb53df78658a72a8","externalIds":{"ArXiv":"2106.07139","DBLP":"journals/aiopen/HanZDGLHQYZZHHJ21","DOI":"10.1016/j.aiopen.2021.08.002","CorpusId":235421816},"title":"Pre-Trained Models: Past, Present and Future"},{"paperId":"bb3425318de7eed5641cda147d61c9a057b9d054","externalIds":{"DBLP":"journals/corr/abs-2106-04489","ArXiv":"2106.04489","ACL":"2021.acl-long.47","DOI":"10.18653/v1/2021.acl-long.47","CorpusId":235309789},"title":"Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","externalIds":{"ArXiv":"2105.09938","DBLP":"conf/nips/HendrycksBKMAGB21","CorpusId":234790100},"title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"3feeb45cb468550bfa12e2ac8a1a4112d2dbfc1a","externalIds":{"ArXiv":"2105.00071","DBLP":"journals/tacl/DziriRLR22","DOI":"10.1162/tacl_a_00506","CorpusId":233481654},"title":"Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","externalIds":{"DBLP":"journals/ijon/SuALPBL24","ArXiv":"2104.09864","DOI":"10.1016/j.neucom.2023.127063","CorpusId":233307138},"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","externalIds":{"ArXiv":"2104.08773","ACL":"2022.acl-long.244","DBLP":"conf/acl/MishraKBH22","DOI":"10.18653/v1/2022.acl-long.244","CorpusId":237421373},"title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":"36f141fc5bc6813073736cf886e264606d9403bf","externalIds":{"DBLP":"conf/emnlp/HonovichCANSA21","ArXiv":"2104.08202","ACL":"2021.emnlp-main.619","DOI":"10.18653/v1/2021.emnlp-main.619","CorpusId":233289483},"title":"Q^{2}: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","externalIds":{"DBLP":"conf/nips/HendrycksBKABTS21","ArXiv":"2103.03874","CorpusId":232134851},"title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","externalIds":{"ArXiv":"2010.11934","DBLP":"conf/naacl/XueCRKASBR21","MAG":"3169483174","ACL":"2021.naacl-main.41","DOI":"10.18653/V1/2021.NAACL-MAIN.41","CorpusId":225040574},"title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"8256f48f759cf85044db251cc512f965834945b3","externalIds":{"DBLP":"journals/corr/abs-2006-15595","ArXiv":"2006.15595","CorpusId":220249871},"title":"Rethinking Positional Encoding in Language Pre-training"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","externalIds":{"MAG":"3138154797","ArXiv":"2006.05525","DBLP":"journals/ijcv/GouYMT21","DOI":"10.1007/s11263-021-01453-z","CorpusId":219559263},"title":"Knowledge Distillation: A Survey"},{"paperId":"14b65a86c82e38fce0eb3506e0d4084ad5cdb583","externalIds":{"MAG":"3033187248","DBLP":"conf/iclr/HeLGC21","ArXiv":"2006.03654","CorpusId":219531210},"title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"659bf9ce7175e1ec266ff54359e2bd76e0b7ff31","externalIds":{"DBLP":"conf/nips/LewisPPPKGKLYR020","MAG":"3027879771","ArXiv":"2005.11401","CorpusId":218869575},"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"29e86cbeacf1e2235cc320ad240956012b294646","externalIds":{"MAG":"3021150969","DBLP":"conf/acl/WangWAYC20","ArXiv":"2005.00969","ACL":"2020.acl-main.101","DOI":"10.18653/v1/2020.acl-main.101","CorpusId":218487237},"title":"Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","externalIds":{"DBLP":"journals/corr/abs-2004-05150","MAG":"3015468748","ArXiv":"2004.05150","CorpusId":215737171},"title":"Longformer: The Long-Document Transformer"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","externalIds":{"DBLP":"journals/corr/abs-2003-08271","MAG":"3088409176","ArXiv":"2003.08271","DOI":"10.1007/s11431-020-1647-3","CorpusId":212747830},"title":"Pre-trained models for natural language processing: A survey"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"04f4e55e14150b7c48b0287ba77c7443df76ed45","externalIds":{"DBLP":"conf/aaai/BiskZLGC20","MAG":"2998617917","ArXiv":"1911.11641","DOI":"10.1609/AAAI.V34I05.6239","CorpusId":208290939},"title":"PIQA: Reasoning about Physical Commonsense in Natural Language"},{"paperId":"503bbfbc6c9654303ce993cf1dae31034dda308c","externalIds":{"DBLP":"conf/aaai/SongZH020","MAG":"2983160116","ArXiv":"1911.05889","DOI":"10.1609/AAAI.V34I05.6417","CorpusId":208006638},"title":"Generating Persona Consistent Dialogues by Exploiting Natural Language Inference"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","externalIds":{"MAG":"2982399380","ACL":"2020.acl-main.703","DBLP":"journals/corr/abs-1910-13461","ArXiv":"1910.13461","DOI":"10.18653/v1/2020.acl-main.703","CorpusId":204960716},"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","externalIds":{"MAG":"3025935268","DBLP":"conf/sc/RajbhandariRRH20","ArXiv":"1910.02054","DOI":"10.1109/SC41405.2020.00024","CorpusId":269617042},"title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","externalIds":{"MAG":"2996428491","DBLP":"journals/corr/abs-1909-11942","ArXiv":"1909.11942","CorpusId":202888986},"title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","externalIds":{"ACL":"Q19-1026","MAG":"2912924812","DBLP":"journals/tacl/KwiatkowskiPRCP19","DOI":"10.1162/tacl_a_00276","CorpusId":86611921},"title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","externalIds":{"MAG":"2950813464","DBLP":"journals/corr/abs-1906-08237","ArXiv":"1906.08237","CorpusId":195069387},"title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"02cbb0db288af2c83b48a023f245812bd22a2408","externalIds":{"MAG":"2948628052","ArXiv":"1906.01081","ACL":"P19-1483","DBLP":"journals/corr/abs-1906-01081","DOI":"10.18653/v1/P19-1483","CorpusId":174797747},"title":"Handling Divergent Reference Texts when Evaluating Table-to-Text Generation"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","externalIds":{"MAG":"2971274815","ArXiv":"1905.03197","DBLP":"journals/corr/abs-1905-03197","CorpusId":147704286},"title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","externalIds":{"MAG":"2944815030","ArXiv":"1905.02450","DBLP":"journals/corr/abs-1905-02450","CorpusId":146808476},"title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"9770fff7379a7ab9006b48939462354dda9a2053","externalIds":{"MAG":"2953271402","DBLP":"journals/corr/abs-1905-10044","ArXiv":"1905.10044","ACL":"N19-1300","DOI":"10.18653/v1/N19-1300","CorpusId":165163607},"title":"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"},{"paperId":"8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad","externalIds":{"MAG":"2946609015","DBLP":"journals/corr/abs-1905-07830","ACL":"P19-1472","ArXiv":"1905.07830","DOI":"10.18653/v1/P19-1472","CorpusId":159041722},"title":"HellaSwag: Can a Machine Really Finish Your Sentence?"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","externalIds":{"DBLP":"journals/corr/abs-1901-07291","ArXiv":"1901.07291","MAG":"2970049541","CorpusId":58981712},"title":"Cross-lingual Language Model Pretraining"},{"paperId":"bd575d03dd5db721728ecabf6715f9425c593c0f","externalIds":{"DOI":"10.4324/9780429427176-8","CorpusId":239846451},"title":"Manual"},{"paperId":"9065ace5366ef548cf81bd9f239f1d132c1ef412","externalIds":{"ArXiv":"1810.06683","DBLP":"conf/iclr/HuangCY19","MAG":"2896342318","CorpusId":260440513},"title":"FlowQA: Grasping Flow in History for Conversational Machine Comprehension"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","externalIds":{"ACL":"D18-1259","DBLP":"journals/corr/abs-1809-09600","MAG":"2952862139","ArXiv":"1809.09600","DOI":"10.18653/v1/D18-1259","CorpusId":52822214},"title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"39e734da43eb8c72e9549b42e96760545036f8e5","externalIds":{"DBLP":"journals/corr/abs-1808-07036","ACL":"D18-1241","MAG":"2951831170","ArXiv":"1808.07036","DOI":"10.18653/v1/D18-1241","CorpusId":52057510},"title":"QuAC: Question Answering in Context"},{"paperId":"1536e8958697c5364f68b2e2448905dbbeb3a0ca","externalIds":{"DBLP":"journals/corr/abs-1809-02789","MAG":"2952396187","ACL":"D18-1260","ArXiv":"1809.02789","DOI":"10.18653/v1/D18-1260","CorpusId":52183757},"title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"},{"paperId":"99ad0533f84c110da2d0713d5798e6e14080b159","externalIds":{"DBLP":"conf/naacl/KhashabiCRUR18","ACL":"N18-1023","MAG":"2804897457","DOI":"10.18653/v1/N18-1023","CorpusId":5112038},"title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","externalIds":{"ArXiv":"1803.05457","DBLP":"journals/corr/abs-1803-05457","MAG":"2794325560","CorpusId":3922816},"title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","externalIds":{"MAG":"2963925437","DBLP":"journals/corr/abs-1803-02155","ACL":"N18-2074","ArXiv":"1803.02155","DOI":"10.18653/v1/N18-2074","CorpusId":3725815},"title":"Self-Attention with Relative Position Representations"},{"paperId":"cbd569036fc72ae7ff747350b91816440282596b","externalIds":{"MAG":"2751448157","DBLP":"journals/corr/abs-1709-00103","ArXiv":"1709.00103","CorpusId":25156106},"title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","externalIds":{"MAG":"2612431505","ArXiv":"1705.03551","ACL":"P17-1147","DBLP":"journals/corr/JoshiCWZ17","DOI":"10.18653/v1/P17-1147","CorpusId":26501419},"title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"636a79420d838eabe4af7fb25d6437de45ab64e8","externalIds":{"MAG":"2606964149","DBLP":"journals/corr/LaiXLYH17","ArXiv":"1704.04683","ACL":"D17-1082","DOI":"10.18653/v1/D17-1082","CorpusId":6826032},"title":"RACE: Large-scale ReAding Comprehension Dataset From Examinations"},{"paperId":"510e26733aaff585d65701b9f1be7ca9d5afc586","externalIds":{"DBLP":"journals/corr/ShazeerMMDLHD17","MAG":"2952339051","ArXiv":"1701.06538","CorpusId":12462234},"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","externalIds":{"DBLP":"journals/corr/RajpurkarZLL16","MAG":"2963748441","ACL":"D16-1264","ArXiv":"1606.05250","DOI":"10.18653/v1/D16-1264","CorpusId":11816014},"title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"b1e20420982a4f923c08652941666b189b11b7fe","externalIds":{"MAG":"2951813483","DBLP":"conf/acl/ChenBM16","ArXiv":"1606.02858","ACL":"P16-1223","DOI":"10.18653/v1/P16-1223","CorpusId":6360322},"title":"A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task"},{"paperId":"f37076f426023241f19cdc2fb0a0fd733a6fa7fa","externalIds":{"ArXiv":"1602.06023","DBLP":"conf/conll/NallapatiZSGX16","MAG":"2341401723","ACL":"K16-1028","DOI":"10.18653/v1/K16-1028","CorpusId":8928715},"title":"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b","externalIds":{"MAG":"1931639407","ArXiv":"1411.4952","DBLP":"journals/corr/FangGISDDGHMPZZ14","DOI":"10.1109/CVPR.2015.7298754","CorpusId":9254582},"title":"From captions to visual concepts and back"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","externalIds":{"MAG":"1895577753","DBLP":"journals/corr/VinyalsTBE14","ArXiv":"1411.4555","DOI":"10.1109/CVPR.2015.7298935","CorpusId":1169492},"title":"Show and tell: A neural image caption generator"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","externalIds":{"MAG":"2130942839","DBLP":"conf/nips/SutskeverVL14","ArXiv":"1409.3215","CorpusId":7961699},"title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","externalIds":{"DBLP":"journals/corr/ChoMBB14","MAG":"2964199361","ArXiv":"1409.1259","ACL":"W14-4012","DOI":"10.3115/v1/W14-4012","CorpusId":11336213},"title":"On the Properties of Neural Machine Translation: Encoderâ€“Decoder Approaches"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","externalIds":{"MAG":"2136189984","DBLP":"conf/cikm/HuangHGDAH13","DOI":"10.1145/2505515.2505665","CorpusId":8384258},"title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","externalIds":{"MAG":"1810943226","DBLP":"journals/corr/Graves13","ArXiv":"1308.0850","CorpusId":1697424},"title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"1b10a8a53517a2a08b3a27f3b575460739cfab1d","externalIds":{"MAG":"2005478706","DOI":"10.1080/21532974.2012.10784686","CorpusId":153933608},"title":"Facebook"},{"paperId":"cb45e9217fe323fbc199d820e7735488fca2a9b3","externalIds":{"DBLP":"conf/asru/MikolovDPBC11","MAG":"1965154800","DOI":"10.1109/ASRU.2011.6163930","CorpusId":15076873},"title":"Strategies for training large scale neural network language models"},{"paperId":"30c9ba88ad0f2fc62b44674950b36ead105cc12e","externalIds":{"DOI":"10.1212/01.CON.0000368259.07921.b8","CorpusId":220579796,"PubMed":"22810512"},"title":"COMPREHENSION"},{"paperId":"954f688810694e53b136ea9b1c8945dd34091b17","externalIds":{"MAG":"1497303833","CorpusId":118113311},"title":"Finite Mixture Models"},{"paperId":"d4a258df43cc14e46988de9a4a7b2f0ea817529b","externalIds":{"DBLP":"conf/acl/SchwenkDG06","MAG":"2103078213","ACL":"P06-2093","DOI":"10.3115/1273073.1273166","CorpusId":1274371},"title":"Continuous Space Language Models for Statistical Machine Translation"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","externalIds":{"MAG":"2140679639","DBLP":"conf/nips/BengioDV00","CorpusId":221275765},"title":"A Neural Probabilistic Language Model"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f7566f1797eb36429acbb09e581d6b2918a50760","externalIds":{"DBLP":"journals/ir/Kantor01","MAG":"1493708795","DOI":"10.1023/A:1011424425034","CorpusId":115386587},"title":"Foundations of Statistical Natural Language Processing"},{"paperId":"119b5e8927f98e3fea76cbb57d7e053c24ac5c18","externalIds":{"MAG":"196533652","DBLP":"conf/flairs/Mahoney00","CorpusId":10397410},"title":"Fast Text Compression with Neural Networks"},{"paperId":"d4e8bed3b50a035e1eabad614fe4218a34b3b178","externalIds":{"MAG":"2158195707","ACL":"P96-1041","DBLP":"journals/csl/ChenG99","ArXiv":"cmp-lg/9606011","DOI":"10.3115/981863.981904","CorpusId":215842252},"title":"An Empirical Study of Smoothing Techniques for Language Modeling"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","externalIds":{"DBLP":"journals/cogsci/Elman90","MAG":"2110485445","DOI":"10.1207/S15516709COG1402_1","CorpusId":2763403},"title":"Finding Structure in Time"},{"paperId":"319f22bd5abfd67ac15988aa5c7f705f018c3ccd","externalIds":{"MAG":"2154642048","DOI":"10.1016/B978-1-4832-1446-7.50035-2","CorpusId":62245742},"title":"Learning internal representations by error propagation"},{"paperId":"60e3bd7cdf2fc948f2171e9726c4f874fba84a9a","externalIds":{"DOI":"10.1177/002205743011202012","CorpusId":220787191},"title":"Guidance"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","externalIds":{"ArXiv":"2303.17580","DBLP":"journals/corr/abs-2303-17580","DOI":"10.48550/arXiv.2303.17580","CorpusId":257833781},"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","externalIds":{"DBLP":"journals/corr/abs-2308-12966","DOI":"10.48550/arXiv.2308.12966","CorpusId":263875678},"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","externalIds":{"DBLP":"journals/corr/abs-2205-05131","CorpusId":248693539},"title":"Unifying Language Learning Paradigms"},{"paperId":"fe0825f9ddb1cccb545f4249da55b6b55e577bbd","externalIds":{"DBLP":"conf/iclr/SanhWRBSACSRDBX22","CorpusId":276421109},"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"0c0a778e6fdf7e36b1750c533dcc916f86608607","externalIds":{"MAG":"2527310337","DBLP":"journals/tkde/XunJGZ17","DOI":"10.1109/TKDE.2016.2614508","CorpusId":13490401},"title":"A Survey on Context Learning"},{"paperId":"51891710e30da33c4ced4ae7daee1593e0cb5cc4","externalIds":{"MAG":"2306526421","CorpusId":15225610},"title":"Machine Learning: The High Interest Credit Card of Technical Debt"},{"paperId":"ea67efe9866b245ea2b0bbb526239fbd7070f635","externalIds":{"MAG":"142212369","DOI":"10.1007/978-3-642-39314-3_1","CorpusId":262553219},"title":"An Introduction to Information Retrieval"},{"paperId":"9819b600a828a57e1cde047bbe710d3446b30da5","externalIds":{"MAG":"179875071","DBLP":"conf/interspeech/MikolovKBCK10","DOI":"10.21437/Interspeech.2010-343","CorpusId":17048224},"title":"Recurrent neural network based language model"},{"paperId":"231f6de83cfa4d641da1681e97a11b689a48e3aa","externalIds":{"MAG":"2569383081","DOI":"10.2307/2670189","CorpusId":12495425},"title":"Statistical methods for speech recognition"},{"paperId":"c1e3f2d537e50e0d5263e4731ab6c7983acd6687","externalIds":{"MAG":"1844951210","DOI":"10.1109/9780470544242.CH12","CorpusId":9101213},"title":"Prediction and Entropy of Printed English"}]}