{"abstract":"Usually, we train a neural system on a sequence of mini-batches of labeled instances. Each mini-batch is composed of k samples, and each sample will learn a representation vector. M IXUP implicitly generates synthetic samples through linearly interpolating inputs and their corresponding labels of random sample pairs in the same mini-batch. This means that M IXUP only generates new points on the edges connecting every two original points in the representation space. We observed that the new points by the standard M IXUP cover pretty limited regions in the entire space of the mini-batch. In this work, we propose B ATCH M IXUP â€”improving the model learning by interpolating hidden states of the entire mini-batch. B ATCH M IXUP can generate new points scattered throughout the space corresponding to the mini-batch. In experiments, B ATCH M IXUP shows superior performance than competitive baselines in improving the performance of NLP tasks while using different ratios of training data."}