{"abstract":"In the realm of multi-modality, text-guided image retouching techniques emerged with the advent of deep learning. Most currently available text-guided methods, however, rely on object-level supervision to confine the region of interest that may be updated. This not only makes it more challenging to develop these algorithms but also limits how widely deep learning can be used for image retouching. In this paper, we offer a text-guided mask-free image retouching approach that yields consistent results to address this concern. Specifically, we propose a two-stage mask-free training paradigm tailored for text-guided image retouching tasks. In the first stage, an unified mask is proposed according to the query description, and then several candidate images are generated with the provided mask and the conditional description based on diffusion model. Extensive experiments have shown that our method can produce high-quality images based on spoken language."}