{"paperId":"e28307299cf2719dad51d8b8e04abaadfe972257","externalIds":{"ArXiv":"2406.08115","DBLP":"journals/corr/abs-2406-08115","DOI":"10.48550/arXiv.2406.08115","CorpusId":270391462},"title":"Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2406.08115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2295732747","name":"Feng Liang"},{"authorId":"2295793981","name":"Zhen Zhang"},{"authorId":"2115608328","name":"Haifeng Lu"},{"authorId":"2145534529","name":"Chengming Li"},{"authorId":"2264958744","name":"Victor C. M. Leung"},{"authorId":"2296218297","name":"Yanyi Guo"},{"authorId":"2327668560","name":"Xiping Hu"}],"abstract":"With rapidly increasing distributed deep learning workloads in large-scale data centers, efficient distributed deep learning framework strategies for resource allocation and workload scheduling have become the key to high-performance deep learning. The large-scale environment with large volumes of datasets, models, and computational and communication resources raises various unique challenges for resource allocation and workload scheduling in distributed deep learning, such as scheduling complexity, resource and workload heterogeneity, and fault tolerance. To uncover these challenges and corresponding solutions, this survey reviews the literature, mainly from 2019 to 2024, on efficient resource allocation and workload scheduling strategies for large-scale distributed DL. We explore these strategies by focusing on various resource types, scheduling granularity levels, and performance goals during distributed training and inference processes. We highlight critical challenges for each topic and discuss key insights of existing technologies. To illustrate practical large-scale resource allocation and workload scheduling in real distributed deep learning scenarios, we use a case study of training large language models. This survey aims to encourage computer science, artificial intelligence, and communications researchers to understand recent advances and explore future research directions for efficient framework strategies for large-scale distributed deep learning."}