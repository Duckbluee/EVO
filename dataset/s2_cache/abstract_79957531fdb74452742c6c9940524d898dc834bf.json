{"abstract":"Despite the expert-level performance of AI models for various medical imaging tasks, real-world performance failures with disparate outputs for various subgroups limit the usefulness of AI in improving patient lives. Many definitions of fairness have been proposed, with discussions of various tensions that arise in the choice of an appropriate metric to use to evaluate bias - for example, should you aim for individual or group fairness? One central observation is that AI models apply \"shortcut learning\" where spurious features (like chest tubes or portable radiographic markers on ICU CXRs) on the medical images are used for prediction instead of identifying true pathology. Moreover, AI has been shown to have a remarkable ability to detect protected attributes of age, sex and race while the same models demonstrate bias against historically underserved sub-groups of age, sex, and race in disease diagnosis. Therefore, an AI model may take shortcut predictions from these correlations and subsequently generate an outcome that is biased towards certain sub-groups even when protected attributes are not explicitly used as inputs into the model. As a result, these sub-groups became non-privileged sub-groups. In this review paper, we discuss the various types of bias from shortcut learning that may occur at different phases of AI model development, including data bias, modeling bias and inference bias. We thereafter summarize various toolkits that can be used to evaluate and mitigate bias and note that these have largely been applied to non-medical domains and require more evaluation for medical AI. We then summarize current techniques for mitigating bias from preprocessing (data-centric solutions), during model development (computational solutions) and post-processing (recalibration of learning). Ongoing legal changes where the use of a biased model will be penalized highlight the necessity of understanding, detecting and mitigating biases from shortcut learning, and will require diverse research teams looking at the whole AI pipeline."}