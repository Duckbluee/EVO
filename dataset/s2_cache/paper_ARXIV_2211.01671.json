{"paperId":"16bded3768bf78c194c7f0b503b3c517283cab36","externalIds":{"ArXiv":"2211.01671","DOI":"10.1145/3793659","CorpusId":257353863},"title":"Visual Adversarial Attacks and Defenses in the Physical World: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2211.01671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2769710","name":"Xingxing Wei"},{"authorId":"1978725813","name":"Bangzheng Pu"},{"authorId":"2190476711","name":"Shiji Zhao"},{"authorId":"2189578499","name":"Jiefan Lu"},{"authorId":"143905981","name":"Baoyuan Wu"}],"abstract":"Although Deep Neural Networks (DNNs) have been widely applied in various real-world scenarios, they remain vulnerable to adversarial examples. Adversarial attacks in computer vision can be categorized into digital attacks and physical attacks based on their different forms. Compared to digital attacks, which generate perturbations in digital pixels, physical attacks are more practical in real-world settings. Due to the serious security risks posed by physically adversarial examples, many studies have been conducted to evaluate the physically adversarial robustness of DNNs in recent years. In this paper, we provide a comprehensive survey of current physically adversarial attacks and defenses in computer vision. We establish a taxonomy by organizing physical attacks according to attack tasks, attack forms, and attack methods. This approach offers readers a systematic understanding of the topic from multiple perspectives. For physical defenses, we categorize them into pre-processing, in-processing, and post-processing for DNN models to ensure comprehensive coverage of adversarial defenses. Based on this survey, we discuss the challenges facing this research field and provide an outlook on future directions."}