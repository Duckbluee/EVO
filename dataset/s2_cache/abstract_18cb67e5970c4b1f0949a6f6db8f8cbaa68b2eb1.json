{"abstract":"Abstract Recently, attributes that contain high-level semantic information of image are always used as a complementary knowledge to improve image captioning performance. However, the use of attributes in prior works cannot excavate the latent visual concepts effectively. At each time step, the semantic information which is sensitive to the predicted word could be different. In this paper, we propose a Dense Semantic Embedding Network (DSEN) for this task. The distinct operation of this network is to densely embed the attributes with the multi-modal of image and text at each step of word generation. The discriminative semantic information hidden in these attributes is formatted in form of global likelihood probabilities. As a result, this dense embedding can modulate the feature distributions of the image, text modals and the hidden states to explicit semantic representation. Furthermore, to improve the discrimination of attributes, a Threshold ReLU (TReLU) is proposed. In addition, a bidirectional LSTM structure is incorporated into the DSEN to capture both the previous and future contexts. Extensive experiments on the COCO and Flickr30K datasets achieve superior results when compared with the state-of-the-art models for the tasks of both image captioning and image-text cross modal retrieval. Most remarkably, our method obtains outstanding performance on the retrieval task, compared with the state-of-the-art models."}