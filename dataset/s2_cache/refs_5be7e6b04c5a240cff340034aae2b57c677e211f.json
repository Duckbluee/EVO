{"references":[{"paperId":"af35e5cd1b22ae7d3c5f8a95c3d5ebc308fabe72","externalIds":{"DBLP":"conf/osdi/SunHZXZL024","ArXiv":"2406.03243","DOI":"10.48550/arXiv.2406.03243","CorpusId":270258417},"title":"Llumnix: Dynamic Scheduling for Large Language Model Serving"},{"paperId":"a7919a3c6dbdcc524776a3102110d637836ad2e0","externalIds":{"DBLP":"journals/corr/abs-2406-02924","ArXiv":"2406.02924","DOI":"10.48550/arXiv.2406.02924","CorpusId":270257857},"title":"Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models"},{"paperId":"a7ed153f848cd8a299c866f50d74b55f16d606ce","externalIds":{"DBLP":"journals/corr/abs-2405-16406","ArXiv":"2405.16406","DOI":"10.48550/arXiv.2405.16406","CorpusId":270062819},"title":"SpinQuant: LLM quantization with learned rotations"},{"paperId":"48c1bf8bab85f4d6a2490a4d3efc8b1fb4a2b261","externalIds":{"ArXiv":"2405.02803","DBLP":"journals/corr/abs-2405-02803","DOI":"10.48550/arXiv.2405.02803","CorpusId":269605814},"title":"Is Flash Attention Stable?"},{"paperId":"d50c7d1287d93539f5c4cc312ae9cfaaaf570fe6","externalIds":{"ArXiv":"2404.18911","DBLP":"journals/corr/abs-2404-18911","DOI":"10.48550/arXiv.2404.18911","CorpusId":269449474},"title":"Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting"},{"paperId":"59b9d7d8f5bab537b7f9e48307d5436ba4922725","externalIds":{"ArXiv":"2404.14897","DBLP":"journals/corr/abs-2404-14897","DOI":"10.48550/arXiv.2404.14897","CorpusId":269303191},"title":"Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models"},{"paperId":"eb06e95dd3eb5a916e52d2e463f474ef4967d8ca","externalIds":{"DBLP":"conf/sosp/WuLZ0L024","ArXiv":"2404.09526","DOI":"10.1145/3694715.3695948","CorpusId":269149599},"title":"LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism"},{"paperId":"394ff691966b419e0a6f24c661aa3c7d45b66bd2","externalIds":{"DBLP":"journals/corr/abs-2404-05089","ArXiv":"2404.05089","DOI":"10.48550/arXiv.2404.05089","CorpusId":269005367},"title":"SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts"},{"paperId":"dd85e6cab147d237a0b1ab6f674570d3efb4d4a0","externalIds":{"DBLP":"journals/corr/abs-2404-00456","ArXiv":"2404.00456","DOI":"10.48550/arXiv.2404.00456","CorpusId":268819214},"title":"QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"},{"paperId":"114c1120a936dbe4de24bc121997fea150bf4331","externalIds":{"DBLP":"conf/iclr/MaLZLX0W0J24","ArXiv":"2403.12544","DOI":"10.48550/arXiv.2403.12544","CorpusId":268531127},"title":"AffineQuant: Affine Transformation Quantization for Large Language Models"},{"paperId":"2be7acee3dfd9c4eb99755158d8c770ac2a36715","externalIds":{"ArXiv":"2403.11421","DBLP":"journals/corr/abs-2403-11421","DOI":"10.48550/arXiv.2403.11421","CorpusId":268513167},"title":"FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines"},{"paperId":"d2421cffac277e230cb97fc2355b32e03dd8bb1f","externalIds":{"DBLP":"journals/corr/abs-2404-07947","ArXiv":"2404.07947","DOI":"10.1145/3620665.3640383","CorpusId":269042826},"title":"ExeGPT: Constraint-Aware Resource Scheduling for LLM Inference"},{"paperId":"4706711dc4e1e16424db9f454a1f3b092b972785","externalIds":{"DBLP":"journals/corr/abs-2403-07378","ArXiv":"2403.07378","DOI":"10.48550/arXiv.2403.07378","CorpusId":268364208},"title":"SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression"},{"paperId":"20f090e35ad598fba2404e550c2462dc9da03a10","externalIds":{"DBLP":"journals/corr/abs-2403-02310","ArXiv":"2403.02310","DOI":"10.48550/arXiv.2403.02310","CorpusId":268249103},"title":"Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve"},{"paperId":"8b8ac20444ef31690f1ad1d80181d121fbde5bab","externalIds":{"DBLP":"conf/icml/LiNWLSYDY024","ArXiv":"2402.18158","DOI":"10.48550/arXiv.2402.18158","CorpusId":268041618},"title":"Evaluating Quantized Large Language Models"},{"paperId":"02ac355296f001a010b1db115d909c052767ccb3","externalIds":{"DBLP":"journals/corr/abs-2402-17834","ArXiv":"2402.17834","DOI":"10.48550/arXiv.2402.17834","CorpusId":268041521},"title":"Stable LM 2 1.6B Technical Report"},{"paperId":"7351898febca53d01453283c9b1a541b662e1ed3","externalIds":{"DBLP":"journals/corr/abs-2403-00818","ArXiv":"2403.00818","DOI":"10.48550/arXiv.2403.00818","CorpusId":268230534},"title":"DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models"},{"paperId":"dbf829c977c121c3704d070d7800d29fe5914756","externalIds":{"DBLP":"journals/corr/abs-2402-16363","ArXiv":"2402.16363","DOI":"10.48550/arXiv.2402.16363","CorpusId":268032253},"title":"LLM Inference Unveiled: Survey and Roofline Model Insights"},{"paperId":"2cea424c7dce71042c24d43317521abdc4c0ffb4","externalIds":{"ArXiv":"2402.15116","DBLP":"journals/corr/abs-2402-15116","DOI":"10.1007/s44267-025-00093-y","CorpusId":267897830},"title":"Large multimodal agents: a survey"},{"paperId":"f7310dac21abc6ba357bcd5e75fb2e6957a97303","externalIds":{"DBLP":"conf/icml/Liu0ILTFXCSKLC24","ArXiv":"2402.14905","CorpusId":267898017},"title":"MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases"},{"paperId":"b42e5a92890053ef48f794311c28c45e9fe55ddd","externalIds":{"DBLP":"journals/corr/abs-2402-14800","ArXiv":"2402.14800","DOI":"10.48550/arXiv.2402.14800","CorpusId":267782440},"title":"Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models"},{"paperId":"8ed690c620794e6e87393a6a6cd02661fe7f81f1","externalIds":{"ArXiv":"2402.12280","CorpusId":267751185},"title":"Plato: Plan to Efficiently Decode for Large Language Model Inference"},{"paperId":"50caaa898eb3605a58a550c7aebacdc1cff3b6da","externalIds":{"DBLP":"journals/corr/abs-2402-16880","ArXiv":"2402.16880","DOI":"10.48550/arXiv.2402.16880","CorpusId":268032346},"title":"BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation"},{"paperId":"2fe05b1f953da5dcf6ec5fe7bc72bfb3dbd9ea30","externalIds":{"ArXiv":"2402.09748","DBLP":"journals/corr/abs-2402-09748","DOI":"10.48550/arXiv.2402.09748","CorpusId":267682382},"title":"Model Compression and Efficient Inference for Large Language Models: A Survey"},{"paperId":"9da427202cc48370fd66359f5d72ff5ff3bc8b57","externalIds":{"DBLP":"journals/corr/abs-2402-04248","ArXiv":"2402.04248","DOI":"10.48550/arXiv.2402.04248","CorpusId":267499935},"title":"Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks"},{"paperId":"d3a23b571143a45c5d7406fbc395750ac5cfef95","externalIds":{"ArXiv":"2402.04291","DBLP":"journals/corr/abs-2402-04291","DOI":"10.48550/arXiv.2402.04291","CorpusId":267523201},"title":"BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"},{"paperId":"8fbf2eb4587b5c271979c3f96eee1b109496143e","externalIds":{"ArXiv":"2402.04396","DBLP":"journals/corr/abs-2402-04396","DOI":"10.48550/arXiv.2402.04396","CorpusId":267523209,"PubMed":"40894235"},"title":"QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks"},{"paperId":"efb78039a1ac98b1ed6c00173c9268bb7d523661","externalIds":{"ArXiv":"2402.02791","CorpusId":267412334},"title":"PanGu-$\\pi$ Pro:Rethinking Optimization and Architecture for Tiny Language Models"},{"paperId":"a3e000e0d7f64c1d094c2a8bf6f43992cbabe91b","externalIds":{"DBLP":"conf/icml/LiuYJZXBC024","ArXiv":"2402.02750","DOI":"10.13140/RG.2.2.28167.37282","CorpusId":267413049},"title":"KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"},{"paperId":"a74a20be53e5767648b5970e30b2d81a9ba8293f","externalIds":{"ArXiv":"2402.05964","DBLP":"journals/corr/abs-2402-05964","DOI":"10.48550/arXiv.2402.05964","CorpusId":267617160},"title":"A Survey on Transformer Compression"},{"paperId":"f1a9e0830bc36c048fa4659beaa62609869895b5","externalIds":{"DBLP":"conf/icml/FuBS024","ArXiv":"2402.02057","DOI":"10.48550/arXiv.2402.02057","CorpusId":267412730},"title":"Break the Sequential Dependency of LLM Inference Using Lookahead Decoding"},{"paperId":"b085968c4362fb286ad6c5ef71a5db9630da0498","externalIds":{"ArXiv":"2401.18079","DBLP":"journals/corr/abs-2401-18079","DOI":"10.48550/arXiv.2401.18079","CorpusId":267335271},"title":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"},{"paperId":"a7e7fa7ca4a32a92f578adf07c613088fa89f5b0","externalIds":{"ArXiv":"2401.15347","DBLP":"journals/corr/abs-2401-15347","DOI":"10.48550/arXiv.2401.15347","CorpusId":267312283},"title":"A Comprehensive Survey of Compression Algorithms for Language Models"},{"paperId":"7754ac3e8ff1286f17593159781487543cdddaba","externalIds":{"ArXiv":"2401.15024","DBLP":"conf/iclr/AshkboosCNHH24a","DOI":"10.48550/arXiv.2401.15024","CorpusId":267301573},"title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns"},{"paperId":"8f070e301979732e0dd73f6aa6170309cf73aa7d","externalIds":{"DBLP":"journals/corr/abs-2402-01680","ArXiv":"2402.01680","DOI":"10.48550/arXiv.2402.01680","CorpusId":267412980},"title":"Large Language Model based Multi-Agents: A Survey of Progress and Challenges"},{"paperId":"21e53e51ff77a5f34f43cb8ca029909c3ad9f71e","externalIds":{"ArXiv":"2401.11181","DBLP":"journals/corr/abs-2401-11181","DOI":"10.48550/arXiv.2401.11181","CorpusId":267068930},"title":"Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads"},{"paperId":"57e7af0b69325fafb371ef5d502e39ef9c90ef7e","externalIds":{"ArXiv":"2401.10774","DBLP":"journals/corr/abs-2401-10774","DOI":"10.48550/arXiv.2401.10774","CorpusId":267061277},"title":"Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"},{"paperId":"72f77a393079431e4207b3afe678ee80b420e6f8","externalIds":{"DBLP":"conf/osdi/ZhongLCHZL0024","ArXiv":"2401.09670","DOI":"10.48550/arXiv.2401.09670","CorpusId":267034664},"title":"DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"},{"paperId":"38c48a1cd296d16dc9c56717495d6e44cc354444","externalIds":{"DBLP":"conf/icml/ZhuL0W0W24","ArXiv":"2401.09417","DOI":"10.48550/arXiv.2401.09417","CorpusId":267028142},"title":"Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"},{"paperId":"8ac21a1545a907fc64b54cde36bf41415608cd7d","externalIds":{"DBLP":"journals/corr/abs-2401-08092","ArXiv":"2401.08092","DOI":"10.48550/arXiv.2401.08092","CorpusId":267027735},"title":"A Survey of Resource-efficient LLM and Multimodal Foundation Models"},{"paperId":"0cee098244c9978032702862a43a09f468f691a4","externalIds":{"DBLP":"journals/corr/abs-2401-07851","ArXiv":"2401.07851","DOI":"10.48550/arXiv.2401.07851","CorpusId":266999159},"title":"Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding"},{"paperId":"8a824052a8ae2061d67f1b76f3610c20ca301f7f","externalIds":{"DBLP":"journals/corr/abs-2401-06761","ArXiv":"2401.06761","DOI":"10.48550/arXiv.2401.06761","CorpusId":266977377},"title":"APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding"},{"paperId":"06d860a5bbb99a4eafdbbb2d5f6aa8dd5fd32cf4","externalIds":{"ArXiv":"2401.05459","DBLP":"journals/corr/abs-2401-05459","DOI":"10.48550/arXiv.2401.05459","CorpusId":266933252},"title":"Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security"},{"paperId":"3eec0c1a7dc0d364d23e2e4544bf8772f5f8ffa3","externalIds":{"ArXiv":"2401.08671","DBLP":"journals/corr/abs-2401-08671","DOI":"10.48550/arXiv.2401.08671","CorpusId":267027676},"title":"DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference"},{"paperId":"411114f989a3d1083d90afd265103132fee94ebe","externalIds":{"DBLP":"journals/corr/abs-2401-04088","ArXiv":"2401.04088","DOI":"10.48550/arXiv.2401.04088","CorpusId":266844877},"title":"Mixtral of Experts"},{"paperId":"3695739b3a8b0e92b8ae90081124d098ae33b15c","externalIds":{"ArXiv":"2401.03868","DBLP":"conf/fpga/ZengLDY0WMSLHDL24","DOI":"10.1145/3626202.3637562","CorpusId":266844224},"title":"FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs"},{"paperId":"745594bd0dc3e9dc86f74e100cd2c98ed36256c0","externalIds":{"DBLP":"journals/corr/abs-2401-04081","ArXiv":"2401.04081","DOI":"10.48550/arXiv.2401.04081","CorpusId":266844147},"title":"MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts"},{"paperId":"ee802ccb7fc3a322b824310ae6f29fc6a1e4314b","externalIds":{"DBLP":"journals/corr/abs-2401-02669","ArXiv":"2401.02669","DOI":"10.48550/arXiv.2401.02669","CorpusId":266818470},"title":"Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"},{"paperId":"560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd","externalIds":{"ArXiv":"2401.02385","DBLP":"journals/corr/abs-2401-02385","DOI":"10.48550/arXiv.2401.02385","CorpusId":266755802},"title":"TinyLlama: An Open-Source Small Language Model"},{"paperId":"fbce7dee52c869edb99408f4a454bdc8703930f5","externalIds":{"ArXiv":"2401.00588","DBLP":"conf/osdi/0007CLZ0ZGS24","DOI":"10.48550/arXiv.2401.00588","CorpusId":266693477},"title":"Fairness in Serving Large Language Models"},{"paperId":"13261129251c9e8891cff02c3aee15c4df6a5630","externalIds":{"DBLP":"journals/corr/abs-2312-15234","ArXiv":"2312.15234","DOI":"10.1145/3754448","CorpusId":266551872},"title":"Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"},{"paperId":"47beae741f6a4470dbd286d4f3f05ba2031c52d2","externalIds":{"DBLP":"journals/trets/ChenZDXYZCZ25","ArXiv":"2312.15159","DOI":"10.1145/3656177","CorpusId":266551359},"title":"Understanding the Potential of FPGA-based Spatial Acceleration for Large Language Model Inference"},{"paperId":"d7eb6ff1007fbdad402719eba8494283ec575016","externalIds":{"DBLP":"journals/corr/abs-2312-13211","ArXiv":"2312.13211","DOI":"10.48550/arXiv.2312.13211","CorpusId":266375072},"title":"DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization"},{"paperId":"e30666ed82670463aa47686e744f0c6f2a0e083d","externalIds":{"ArXiv":"2312.11462","DBLP":"journals/corr/abs-2312-11462","DOI":"10.48550/arXiv.2312.11462","CorpusId":266359077},"title":"Cascade Speculative Drafting for Even Faster LLM Inference"},{"paperId":"ddacee7382548fd9976e846c92500cfa3b6741db","externalIds":{"DBLP":"conf/sosp/SongMX024","ArXiv":"2312.12456","DOI":"10.1145/3694715.3695964","CorpusId":266375171},"title":"PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"},{"paperId":"a3dd3adc37cdb8991936f3feb109c20b6f892f3d","externalIds":{"DBLP":"conf/acl/FeiNZH0D024","ArXiv":"2312.09571","DOI":"10.48550/arXiv.2312.09571","CorpusId":266335580},"title":"Extending Context Window of Large Language Models via Semantic Compression"},{"paperId":"eb95c327260725498404eb43ec370d419b8d92c7","externalIds":{"DBLP":"conf/nips/ZhengYXS0YCKSGB24","ArXiv":"2312.07104","DOI":"10.52202/079017-2000","CorpusId":266174771},"title":"SGLang: Efficient Execution of Structured Language Model Programs"},{"paperId":"a1bcf68d6ed2fec1ecaf16b67f2d19bc20c00ee6","externalIds":{"DBLP":"journals/corr/abs-2312-05821","ArXiv":"2312.05821","DOI":"10.48550/arXiv.2312.05821","CorpusId":266162471},"title":"ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models"},{"paperId":"5851121df5ce46be5faea265c868ec0beabfce96","externalIds":{"DBLP":"journals/corr/abs-2312-03863","ArXiv":"2312.03863","DOI":"10.48550/arXiv.2312.03863","CorpusId":266044196},"title":"Efficient Large Language Models: A Survey"},{"paperId":"383c598625110e0a4c60da4db10a838ef822fbcf","externalIds":{"ArXiv":"2312.02003","DBLP":"journals/corr/abs-2312-02003","DOI":"10.1016/j.hcc.2024.100211","CorpusId":265609409},"title":"A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"},{"paperId":"7bbc7595196a0606a07506c4fb1473e5e87f6082","externalIds":{"ArXiv":"2312.00752","DBLP":"journals/corr/abs-2312-00752","CorpusId":265551773},"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"},{"paperId":"51fb6598a3ebe36b371b096b4824d718e6e527fb","externalIds":{"DBLP":"journals/corr/abs-2312-00678","ArXiv":"2312.00678","DOI":"10.48550/arXiv.2312.00678","CorpusId":265552007},"title":"The Efficiency Spectrum of Large Language Models: An Algorithmic Survey"},{"paperId":"ad9146d98ae95bbeeef460abe083ecc2c4798672","externalIds":{"DBLP":"journals/corr/abs-2311-18677","ArXiv":"2311.18677","DOI":"10.1109/ISCA59077.2024.00019","CorpusId":265506047},"title":"Splitwise: Efficient Generative LLM Inference Using Phase Splitting"},{"paperId":"2c0312c604f9f7638bb4533b39e0ae81e7f6ab12","externalIds":{"ArXiv":"2311.16867","DBLP":"journals/corr/abs-2311-16867","DOI":"10.48550/arXiv.2311.16867","CorpusId":265466629},"title":"The Falcon Series of Open Language Models"},{"paperId":"929ed6412136fe42e6ef1eeb7ea0b4da693dee37","externalIds":{"DBLP":"journals/corr/abs-2311-15566","ArXiv":"2311.15566","DOI":"10.1145/3620665.3640411","CorpusId":265456256},"title":"SpotServe: Serving Generative Large Language Models on Preemptible Instances"},{"paperId":"4067a6f57f708dec4459d3d4322373e06c2b168c","externalIds":{"ArXiv":"2311.13581","DBLP":"journals/corr/abs-2311-13581","DOI":"10.48550/arXiv.2311.13581","CorpusId":265351429},"title":"PaSS: Parallel Speculative Sampling"},{"paperId":"e52abdd619dbb52cc5c8bb099c6b6cb5603bd61d","externalIds":{"DBLP":"conf/acl/Zhao0DRSLIM24","ArXiv":"2311.08640","DOI":"10.18653/v1/2024.acl-long.766","CorpusId":265213039},"title":"Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation"},{"paperId":"532c2c7a247d9e97d20abec1b2f4612984fdab93","externalIds":{"DBLP":"conf/naacl/0012ZCL024","ArXiv":"2311.08252","ACL":"2024.naacl-long.88","DOI":"10.48550/arXiv.2311.08252","CorpusId":265157884},"title":"REST: Retrieval-Based Speculative Decoding"},{"paperId":"a6348981246adcd42e8ba39acf139da745696eff","externalIds":{"ArXiv":"2311.07052","DBLP":"conf/acl/ZhangLSYG025","DOI":"10.48550/arXiv.2311.07052","CorpusId":265149926},"title":"Towards the Law of Capacity Gap in Distilling Language Models"},{"paperId":"4d76206515d6b33903937474273885476fc2771e","externalIds":{"DBLP":"journals/corr/abs-2311-01282","ArXiv":"2311.01282","DOI":"10.48550/arXiv.2311.01282","CorpusId":264935058},"title":"FlashDecoding++: Faster Large Language Model Inference on GPUs"},{"paperId":"9529e50807f36acf3d2e4af994b5803c47e4746a","externalIds":{"DBLP":"conf/mlsys/ZhaoLZYC0CK0K24","ArXiv":"2310.19102","DOI":"10.48550/arXiv.2310.19102","CorpusId":264828796},"title":"Atom: Low-bit Quantization for Efficient and Accurate LLM Serving"},{"paperId":"a8b995f0da78a79447dfb18c2337972b044f4239","externalIds":{"DBLP":"journals/corr/abs-2310-16836","ArXiv":"2310.16836","DOI":"10.18653/v1/2023.emnlp-main.39","CorpusId":264451753},"title":"LLM-FP4: 4-Bit Floating-Point Quantized Transformers"},{"paperId":"ca53c1d1ba1a1386f860fa13d7729160571e1643","externalIds":{"DBLP":"journals/corr/abs-2310-18356","ArXiv":"2310.18356","DOI":"10.48550/arXiv.2310.18356","CorpusId":264590698},"title":"LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery"},{"paperId":"ef9079f32e806e4d297cee28f36b2321acee9eb3","externalIds":{"DBLP":"conf/emnlp/ChenWQWYZ23","ArXiv":"2310.14747","DOI":"10.48550/arXiv.2310.14747","CorpusId":264436620},"title":"MCC-KD: Multi-CoT Consistent Knowledge Distillation"},{"paperId":"ea1f648988c632a6dbab6d8b88432456aa021cfb","externalIds":{"DBLP":"conf/nips/SunSRBJY23","ArXiv":"2310.15141","DOI":"10.48550/arXiv.2310.15141","CorpusId":264591415},"title":"SpecTr: Fast Speculative Decoding via Optimal Transport"},{"paperId":"ddbd8fe782ac98e9c64dd98710687a962195dd9b","externalIds":{"DBLP":"conf/iclr/AsaiWWSH24","ArXiv":"2310.11511","CorpusId":264288947},"title":"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"},{"paperId":"43017a16dfe593c09533c5fb3c3612c83761a98a","externalIds":{"DBLP":"conf/nips/SahaSP23","ArXiv":"2310.11028","DOI":"10.48550/arXiv.2310.11028","CorpusId":262233736},"title":"Matrix Compression via Randomized Low Rank and Low Precision Factorization"},{"paperId":"51bccfc8d164812cf81b0284a5ec13bf4002ae3c","externalIds":{"DBLP":"conf/icassp/ShaoLQ24","ArXiv":"2310.09499","DOI":"10.1109/ICASSP48485.2024.10445737","CorpusId":264146174},"title":"One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models"},{"paperId":"4880ba8910bc320cb7c1aa943992a500f4c41f07","externalIds":{"DBLP":"conf/iclr/0002ZLSYHT0J24","ArXiv":"2310.08915","DOI":"10.48550/arXiv.2310.08915","CorpusId":264128029},"title":"Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"},{"paperId":"56767c18bb5aaa2b6377624168bed1b6dcc4b94d","externalIds":{"DBLP":"conf/iclr/ZhouLRMRKKA24","ArXiv":"2310.08461","DOI":"10.48550/arXiv.2310.08461","CorpusId":263909387},"title":"DistillSpec: Improving Speculative Decoding via Knowledge Distillation"},{"paperId":"e764ad9ccf0b31a0c91a9220290930f083ad062a","externalIds":{"ArXiv":"2310.08049","DBLP":"conf/iclr/LeeJB24","CorpusId":263909253},"title":"Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability"},{"paperId":"af8123ecdff838f63e4eba0b36b8babe4c5cee65","externalIds":{"ArXiv":"2310.08659","DBLP":"conf/iclr/Li00KHCZ24","DOI":"10.48550/arXiv.2310.08659","CorpusId":264128197},"title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"},{"paperId":"ffdc017b1d2b493feaac9efa854882fe23d50dcf","externalIds":{"ArXiv":"2310.08041","DBLP":"journals/corr/abs-2310-08041","DOI":"10.48550/arXiv.2310.08041","CorpusId":263908852},"title":"QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models"},{"paperId":"ba5261e729c181e28a98dee2c08d7cf5fc7127a2","externalIds":{"DBLP":"conf/icml/LiuHBCDS024","ArXiv":"2310.07177","DOI":"10.48550/arXiv.2310.07177","CorpusId":263835233},"title":"Online Speculative Decoding"},{"paperId":"4c0428917aeee6aa7bd434f337d039f35996b736","externalIds":{"DBLP":"journals/corr/abs-2310-06839","ArXiv":"2310.06839","DOI":"10.48550/arXiv.2310.06839","CorpusId":263830692},"title":"LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"},{"paperId":"abdb0f9d1486dbb024c4bc9f8f9dc40464c58715","externalIds":{"DBLP":"journals/corr/abs-2310-06694","ArXiv":"2310.06694","DOI":"10.48550/arXiv.2310.06694","CorpusId":263830786},"title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"},{"paperId":"2392b6d3a5cad9e5cf349169eaeee848266adf6a","externalIds":{"ArXiv":"2310.05736","DBLP":"journals/corr/abs-2310-05736","DOI":"10.48550/arXiv.2310.05736","CorpusId":263830701},"title":"LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"},{"paperId":"b12541867632737e826b7b01c7fbe1c4222d8655","externalIds":{"ArXiv":"2310.06201","DBLP":"journals/corr/abs-2310-06201","DOI":"10.48550/arXiv.2310.06201","CorpusId":263830231},"title":"Compressing Context to Enhance Inference Efficiency of Large Language Models"},{"paperId":"23af54b82c951317f1fc1841164d8a441a2d8120","externalIds":{"DBLP":"conf/iclr/XuSC24","ArXiv":"2310.04408","DOI":"10.48550/arXiv.2310.04408","CorpusId":263830734},"title":"RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation"},{"paperId":"4e13ecf80443a4135d516b7ba77eca82b5c6d347","externalIds":{"DBLP":"journals/corr/abs-2310-01382","ArXiv":"2310.01382","DOI":"10.48550/arXiv.2310.01382","CorpusId":263605754},"title":"Compressing LLMs: The Truth is Rarely Pure and Never Simple"},{"paperId":"bfeda6c7aa7899a80adb01894555b09d24756a59","externalIds":{"DBLP":"journals/corr/abs-2310-00280","ArXiv":"2310.00280","DOI":"10.48550/arXiv.2310.00280","CorpusId":263334469},"title":"Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"},{"paperId":"fdc53c2c10742464087c0525f77e32604827a21d","externalIds":{"DBLP":"conf/iclr/XiaoTCHL24","ArXiv":"2309.17453","DOI":"10.48550/arXiv.2309.17453","CorpusId":263310483},"title":"Efficient Streaming Language Models with Attention Sinks"},{"paperId":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","externalIds":{"DBLP":"journals/corr/abs-2309-16609","ArXiv":"2309.16609","DOI":"10.48550/arXiv.2309.16609","CorpusId":263134555},"title":"Qwen Technical Report"},{"paperId":"945db0077b6d19b720f5998b3f61300013c4f885","externalIds":{"ArXiv":"2309.14717","DBLP":"conf/iclr/XuXG0CZC0024","DOI":"10.48550/arXiv.2309.14717","CorpusId":262825568},"title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"},{"paperId":"8ec117feff6ee10e3b20a19ac101fee5c99e14d7","externalIds":{"DBLP":"journals/corr/abs-2309-14021","ArXiv":"2309.14021","DOI":"10.48550/arXiv.2309.14021","CorpusId":262460763},"title":"LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression"},{"paperId":"c96297261467b5daa2d01227496a70d444602434","externalIds":{"DBLP":"journals/corr/abs-2309-10305","ArXiv":"2309.10305","DOI":"10.48550/arXiv.2309.10305","CorpusId":261951743},"title":"Baichuan 2: Open Large-scale Language Models"},{"paperId":"0c72450890a54b68d63baa99376131fda8f06cf9","externalIds":{"ArXiv":"2309.07864","DBLP":"journals/corr/abs-2309-07864","DOI":"10.48550/arXiv.2309.07864","CorpusId":261817592},"title":"The Rise and Potential of Large Language Model Based Agents: A Survey"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"e26888285436bc7998e5c95102a9beb60144be5e","externalIds":{"DBLP":"journals/corr/abs-2309-05463","ArXiv":"2309.05463","DOI":"10.48550/arXiv.2309.05463","CorpusId":261696657},"title":"Textbooks Are All You Need II: phi-1.5 technical report"},{"paperId":"464cf829eaaeb2b3bafc84cc9203790e95102049","externalIds":{"ArXiv":"2309.02784","DBLP":"conf/aaai/LiL0C24","DOI":"10.48550/arXiv.2309.02784","CorpusId":261557634},"title":"Norm Tweaking: High-performance Low-bit Quantization of Large Language Models"},{"paperId":"d315ca681e95b73f2a6a6115d1e218dec9720d6f","externalIds":{"DBLP":"journals/corr/abs-2309-01885","ArXiv":"2309.01885","DOI":"10.48550/arXiv.2309.01885","CorpusId":261530763},"title":"QuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive Algorithm"},{"paperId":"a9caf21a845cb0b1b1d453c052188de118006093","externalIds":{"DBLP":"journals/corr/abs-2308-16369","ArXiv":"2308.16369","DOI":"10.48550/arXiv.2308.16369","CorpusId":261395577},"title":"SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"},{"paperId":"eb2c2330177f765038a2b17e2ee3498965865797","externalIds":{"DBLP":"journals/corr/abs-2308-13137","ArXiv":"2308.13137","DOI":"10.48550/arXiv.2308.13137","CorpusId":261214575},"title":"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"},{"paperId":"aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3","externalIds":{"DBLP":"conf/aaai/BestaBKGPGGLNNH24","ArXiv":"2308.09687","DOI":"10.1609/aaai.v38i16.29720","CorpusId":261030303},"title":"Graph of Thoughts: Solving Elaborate Problems with Large Language Models"},{"paperId":"5df422fc18974d687febd171adcac35b3012c50a","externalIds":{"DBLP":"journals/access/JungK24","ArXiv":"2308.08758","DOI":"10.1109/ACCESS.2024.3403426","CorpusId":261030884},"title":"Discrete Prompt Compression With Reinforcement Learning"},{"paperId":"7ac38c3398f2696754bec69f296468e7a8237a64","externalIds":{"ArXiv":"2308.09723","DBLP":"journals/corr/abs-2308-09723","DOI":"10.48550/arXiv.2308.09723","CorpusId":261049460},"title":"FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"},{"paperId":"338d8f3b199abcebc85f34016b0162ab3a9d5310","externalIds":{"DBLP":"journals/corr/abs-2308-07633","ArXiv":"2308.07633","DOI":"10.1162/tacl_a_00704","CorpusId":260900101},"title":"A Survey on Model Compression for Large Language Models"},{"paperId":"43e624ddeed82df944a6cae0dedec3372438e243","externalIds":{"DBLP":"journals/corr/abs-2308-04623","ArXiv":"2308.04623","DOI":"10.48550/arXiv.2308.04623","CorpusId":260735640},"title":"Accelerating LLM Inference with Staged Speculative Decoding"},{"paperId":"1dede9d21db0be1c58208e1f970e57aac4fc45f8","externalIds":{"ArXiv":"2308.02019","DBLP":"journals/corr/abs-2308-02019","DOI":"10.48550/arXiv.2308.02019","CorpusId":260611172},"title":"Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty"},{"paperId":"56b828717f32251a5e0f0be9c0113077f23c8429","externalIds":{"DBLP":"journals/corr/abs-2307-13304","ArXiv":"2307.13304","DOI":"10.48550/arXiv.2307.13304","CorpusId":260154775,"PubMed":"39416859"},"title":"QuIP: 2-Bit Quantization of Large Language Models With Guarantees"},{"paperId":"aeb9454987c3f85563cf7a5d2cb7f3d502d3398d","externalIds":{"DBLP":"journals/corr/abs-2307-09782","ArXiv":"2307.09782","DOI":"10.48550/arXiv.2307.09782","CorpusId":259982476},"title":"ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"},{"paperId":"823ca4778e1027f2f0b356df051d762dcecaaba0","externalIds":{"ArXiv":"2307.08691","DBLP":"journals/corr/abs-2307-08691","DOI":"10.48550/arXiv.2307.08691","CorpusId":259936734},"title":"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"},{"paperId":"240103933ffe3dac2179cc160a2bd91299357a53","externalIds":{"DBLP":"journals/corr/abs-2307-08621","ArXiv":"2307.08621","CorpusId":259937453},"title":"Retentive Network: A Successor to Transformer for Large Language Models"},{"paperId":"60b0476a97c00e355df28ba35422764a7fbe88e8","externalIds":{"DBLP":"conf/iclr/00010WWCW24","ArXiv":"2307.06945","DOI":"10.48550/arXiv.2307.06945","CorpusId":259847425},"title":"In-context Autoencoder for Context Compression in a Large Language Model"},{"paperId":"e28f4687b9ddf562807d12d9799add07aa191d51","externalIds":{"DBLP":"conf/dac/DaiGVK23","DOI":"10.1109/DAC56929.2023.10247993","CorpusId":261900227},"title":"Efficient Transformer Inference with Statically Structured Sparse Attention"},{"paperId":"ce9435c82dc9b576f2037aa2f4357a520be9b2aa","externalIds":{"DBLP":"journals/corr/abs-2307-02628","ArXiv":"2307.02628","DOI":"10.48550/arXiv.2307.02628","CorpusId":259360560},"title":"SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference"},{"paperId":"2fd0aa038cf1009e265e9cbddab8ea6a8e03016a","externalIds":{"ArXiv":"2306.15799","CorpusId":259275074},"title":"FLuRKA: Fast and accurate unified Low-Rank&Kernel Attention"},{"paperId":"7a6a298efb965ce9a351a3212f6f536e94dbbb03","externalIds":{"ArXiv":"2306.14050","DBLP":"conf/acl/LiHYRC023","ACL":"2023.acl-long.150","DOI":"10.48550/arXiv.2306.14050","CorpusId":259251773},"title":"Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step"},{"paperId":"e586a4591ba0303b769f2c07cbddaf1899cb72e4","externalIds":{"DBLP":"conf/nips/Zhang00CZC0TRBW23","ArXiv":"2306.14048","DOI":"10.48550/arXiv.2306.14048","CorpusId":259263947},"title":"H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"},{"paperId":"7d22ad3573101337bca2091fb0114b377c4f3db6","externalIds":{"DBLP":"journals/corr/abs-2306-11695","ArXiv":"2306.11695","DOI":"10.48550/arXiv.2306.11695","CorpusId":259203115},"title":"A Simple and Effective Pruning Approach for Large Language Models"},{"paperId":"bc8428e270a5474cabfaff578d44955f757ccacd","externalIds":{"ArXiv":"2306.11222","DBLP":"journals/corr/abs-2306-11222","DOI":"10.48550/arXiv.2306.11222","CorpusId":259203385},"title":"LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation"},{"paperId":"2922768fd451ecdb45f48c1a83eb57f54a91221b","externalIds":{"DBLP":"journals/corr/abs-2306-11644","ArXiv":"2306.11644","CorpusId":259203998},"title":"Textbooks Are All You Need"},{"paperId":"d2d0371158803df93a249c9f7237ffd79b875816","externalIds":{"DBLP":"journals/corr/abs-2306-11197","ArXiv":"2306.11197","DOI":"10.48550/arXiv.2306.11197","CorpusId":259203690},"title":"Sparse Modular Activation for Efficient Sequence Modeling"},{"paperId":"3f5e63168d0ae1af41c3434e9e3e7e84dda9a5d8","externalIds":{"DBLP":"conf/isca/QinWDZYLW0Y23","DOI":"10.1145/3579371.3589057","CorpusId":259177825},"title":"FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction"},{"paperId":"a5d3a865b71f3f424ba31e037848028f60161478","externalIds":{"ArXiv":"2306.09306","DBLP":"journals/corr/abs-2306-09306","DOI":"10.48550/arXiv.2306.09306","CorpusId":259165330},"title":"Propagating Knowledge Updates to LMs Through Distillation"},{"paperId":"0a067fab18c67d4a386efa846c080f8afff5e8f3","externalIds":{"ArXiv":"2306.09539","DBLP":"conf/nips/PilaultFFPBG23","CorpusId":259187506},"title":"Block-State Transformers"},{"paperId":"275d8bfe7d9c671d1cb4f525434d10a7dbd8778a","externalIds":{"ArXiv":"2306.08543","CorpusId":259164722},"title":"MiniLLM: On-Policy Distillation of Large Language Models"},{"paperId":"3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc","externalIds":{"DBLP":"journals/corr/abs-2306-07629","ArXiv":"2306.07629","DOI":"10.48550/arXiv.2306.07629","CorpusId":259144954},"title":"SqueezeLLM: Dense-and-Sparse Quantization"},{"paperId":"0423fc7bc1880b850d07aec8ebd9217a70626572","externalIds":{"ArXiv":"2306.06000","DBLP":"conf/nips/JinW0W23","DOI":"10.48550/arXiv.2306.06000","CorpusId":259129329},"title":"S3: Increasing GPU Utilization during Generative Inference for Higher Throughput"},{"paperId":"51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df","externalIds":{"DBLP":"journals/corr/abs-2306-03078","ArXiv":"2306.03078","DOI":"10.48550/arXiv.2306.03078","CorpusId":259076379},"title":"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"},{"paperId":"db9507cdd3e2d7d9c90ed185bd831e55c62dcec9","externalIds":{"DBLP":"journals/sigmobile/LinTTYXH24","ArXiv":"2306.00978","DOI":"10.1145/3714983.3714987","CorpusId":258999941},"title":"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"},{"paperId":"8c5a7de7452b61cb81d6f7124ad021997e0a79c1","externalIds":{"ArXiv":"2306.01150","DBLP":"journals/corr/abs-2306-01150","ACL":"2023.acl-long.172","DOI":"10.48550/arXiv.2306.01150","CorpusId":259063796},"title":"Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning"},{"paperId":"d203c764fb5dec2b053be667c8b06e516ea6ef10","externalIds":{"DBLP":"journals/corr/abs-2306-01160","ArXiv":"2306.01160","DOI":"10.48550/arXiv.2306.01160","CorpusId":259063695},"title":"Faster Causal Attention Over Large Sequences Through Sparse Flash Attention"},{"paperId":"b463f5dd8ed6871ddeac03754c1c2d99547b08fe","externalIds":{"ArXiv":"2305.18403","DBLP":"conf/acl/Zhang0SYOYZ24","DOI":"10.18653/v1/2024.findings-acl.178","CorpusId":258967906},"title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"},{"paperId":"c193eb176985a81ae64f63c5e50b2f11cfb7c4e6","externalIds":{"DBLP":"journals/corr/abs-2305-15805","ArXiv":"2305.15805","DOI":"10.48550/arXiv.2305.15805","CorpusId":258888224},"title":"Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"},{"paperId":"2f7364d8e5cf94315bf8905f57de9c5543e9a4bf","externalIds":{"DBLP":"journals/corr/abs-2305-14788","ArXiv":"2305.14788","DOI":"10.48550/arXiv.2305.14788","CorpusId":258865249},"title":"Adapting Language Models to Compress Contexts"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"026b3396a63ed5772329708b7580d633bb86bec9","externalIds":{"DBLP":"conf/emnlp/PengAAAABCCCDDG23","ArXiv":"2305.13048","DOI":"10.18653/v1/2023.findings-emnlp.936","CorpusId":258832459},"title":"RWKV: Reinventing RNNs for the Transformer Era"},{"paperId":"5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200","externalIds":{"DBLP":"journals/corr/abs-2305-13245","ArXiv":"2305.13245","DOI":"10.48550/arXiv.2305.13245","CorpusId":258833177},"title":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"},{"paperId":"02529b2666a536053a2e2940de5b28de36fd594b","externalIds":{"ArXiv":"2305.12870","DBLP":"conf/emnlp/JiangCCW23","DOI":"10.18653/v1/2023.emnlp-main.189","CorpusId":258833333},"title":"Lion: Adversarial Distillation of Proprietary Large Language Models"},{"paperId":"b2ec81b572fd5f0a5f5de843e3c62985b7d9c5a1","externalIds":{"ACL":"2023.acl-long.249","DBLP":"journals/corr/abs-2305-12129","ArXiv":"2305.12129","DOI":"10.48550/arXiv.2305.12129","CorpusId":258833648},"title":"Lifting the Curse of Capacity Gap in Distilling Language Models"},{"paperId":"017010b941d902a467f6d329ae5e74fd67e67912","externalIds":{"DBLP":"journals/corr/abs-2305-11627","ArXiv":"2305.11627","DOI":"10.48550/arXiv.2305.11627","CorpusId":258823276},"title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"paperId":"8ce6ad6d8a73757309d3b9f525cf15cb68e32397","externalIds":{"DBLP":"journals/corr/abs-2305-11170","ArXiv":"2305.11170","DOI":"10.48550/arXiv.2305.11170","CorpusId":258762345},"title":"Efficient Prompting via Dynamic In-Context Learning"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"1fbaf2d8b69ef6e42a38c233f5d01bea70bad5b7","externalIds":{"DBLP":"journals/corr/abs-2305-05920","ArXiv":"2305.05920","DOI":"10.48550/arXiv.2305.05920","CorpusId":258588170},"title":"Fast Distributed Inference Serving for Large Language Models"},{"paperId":"585f8b9725f5f5e5495c3508d39f70d1c053e190","externalIds":{"DBLP":"journals/tmlr/ChenZ024","ArXiv":"2305.05176","DOI":"10.48550/arXiv.2305.05176","CorpusId":258564349},"title":"FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance"},{"paperId":"aad167be3c902388ea625da4117fcae4325b8b7d","externalIds":{"ArXiv":"2305.02301","DBLP":"journals/corr/abs-2305-02301","DOI":"10.48550/arXiv.2305.02301","CorpusId":258461606},"title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"},{"paperId":"56fa65d8dc41708082f9b2ef7752c49cee9ebe01","externalIds":{"DBLP":"conf/acl/WangWLGYR23","ACL":"2023.acl-long.304","ArXiv":"2305.01879","DOI":"10.48550/arXiv.2305.01879","CorpusId":258461058},"title":"SCOTT: Self-Consistent Chain-of-Thought Distillation"},{"paperId":"389ec3e8902a5dcfcde1adec735854e93f845937","externalIds":{"DBLP":"journals/corr/abs-2304-14402","ACL":"2024.eacl-long.57","ArXiv":"2304.14402","DOI":"10.48550/arXiv.2304.14402","CorpusId":258352678},"title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","externalIds":{"DBLP":"journals/corr/abs-2304-13712","ArXiv":"2304.13712","DOI":"10.1145/3649506","CorpusId":258331833},"title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"b9870e130f61ff900fe00dbcc5782c9b31773d32","externalIds":{"DBLP":"journals/corr/abs-2304-08467","ArXiv":"2304.08467","DOI":"10.48550/arXiv.2304.08467","CorpusId":258179012},"title":"Learning to Compress Prompts with Gist Tokens"},{"paperId":"e92a5332390f0ba94615935541da4da9bed56512","externalIds":{"ArXiv":"2304.07493","DBLP":"conf/isca/0003THL00LG023","DOI":"10.1145/3579371.3589038","CorpusId":258179335},"title":"OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization"},{"paperId":"2a44c6b7f291f625314a82ba3131e605009fd533","externalIds":{"ArXiv":"2304.01089","DBLP":"journals/corr/abs-2304-01089","DOI":"10.48550/arXiv.2304.01089","CorpusId":257913374},"title":"RPTQ: Reorder-based Post-training Quantization for Large Language Models"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","externalIds":{"DBLP":"conf/icml/BidermanSABOHKP23","ArXiv":"2304.01373","DOI":"10.48550/arXiv.2304.01373","CorpusId":257921893},"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c","externalIds":{"ArXiv":"2303.12712","DBLP":"journals/corr/abs-2303-12712","CorpusId":257663729},"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"8f48c75e1354c88a84a67abb60789083c12e5037","externalIds":{"ArXiv":"2303.08302","CorpusId":258947018},"title":"ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation"},{"paperId":"42a14d824caa3348046eb34c37e2ab7985faa7a3","externalIds":{"ArXiv":"2303.06865","DBLP":"journals/corr/abs-2303-06865","DOI":"10.48550/arXiv.2303.06865","CorpusId":257495837},"title":"High-throughput Generative Inference of Large Language Models with a Single GPU"},{"paperId":"1462a0e5b7db47301bb0995db56426e1f4a0ac7d","externalIds":{"ArXiv":"2303.01610","DBLP":"journals/corr/abs-2303-01610","DOI":"10.48550/arXiv.2303.01610","CorpusId":257353502},"title":"Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"998ac3e945857cf2676ee7efdbaf443a0c6f820a","externalIds":{"DBLP":"journals/corr/abs-2302-10866","ArXiv":"2302.10866","DOI":"10.48550/arXiv.2302.10866","CorpusId":257050308},"title":"Hyena Hierarchy: Towards Larger Convolutional Language Models"},{"paperId":"2b66cc9e3b46cf2cd30c4ffdca596480c8de6331","externalIds":{"DBLP":"conf/nips/KurticFA23","ArXiv":"2302.04089","CorpusId":256662263},"title":"ZipLM: Inference-Aware Structured Pruning of Language Models"},{"paperId":"a1f8082505c7e90b0a033e1b9da0a97d67aad66c","externalIds":{"DBLP":"journals/corr/abs-2302-01318","ArXiv":"2302.01318","DOI":"10.48550/arXiv.2302.01318","CorpusId":256503945},"title":"Accelerating Large Language Model Decoding with Speculative Sampling"},{"paperId":"07b14c24833400b79978b0a5f084803337e30a15","externalIds":{"DBLP":"conf/naacl/ShiMYS0LZY24","ACL":"2024.naacl-long.463","ArXiv":"2301.12652","DOI":"10.48550/arXiv.2301.12652","CorpusId":256389797},"title":"REPLUG: Retrieval-Augmented Black-Box Language Models"},{"paperId":"909ad57ce8caa6b390a65ae09db352d27d8f3996","externalIds":{"DBLP":"journals/corr/abs-2301-00774","ArXiv":"2301.00774","DOI":"10.48550/arXiv.2301.00774","CorpusId":255372747},"title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"},{"paperId":"5a77b508302771fc083bf24e0bcda8553c9b5421","externalIds":{"DBLP":"journals/corr/abs-2212-14052","ArXiv":"2212.14052","DOI":"10.48550/arXiv.2212.14052","CorpusId":255340454},"title":"Hungry Hungry Hippos: Towards Language Modeling with State Space Models"},{"paperId":"d0a970c06f6405610c607a16e59ea832770ad1bc","externalIds":{"ArXiv":"2212.10534","ACL":"2023.acl-long.302","DBLP":"conf/acl/ChenGBS023","DOI":"10.18653/v1/2023.acl-long.302","CorpusId":254877039},"title":"DISCO: Distilling Counterfactuals with Large Language Models"},{"paperId":"a128b1c47e6842605fb95bceae930d2135fc38fc","externalIds":{"DBLP":"conf/emnlp/WangYGR23","ArXiv":"2212.10544","DOI":"10.48550/arXiv.2212.10544","CorpusId":254877218},"title":"Pretraining Without Attention"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","externalIds":{"ACL":"2023.acl-long.830","DBLP":"conf/acl/HoSY23","ArXiv":"2212.10071","DOI":"10.48550/arXiv.2212.10071","CorpusId":254877399},"title":"Large Language Models Are Reasoning Teachers"},{"paperId":"f9ad1fffa1cc76fd5db3ff758c0839492c5147c4","externalIds":{"DBLP":"journals/corr/abs-2212-10670","ArXiv":"2212.10670","DOI":"10.48550/arXiv.2212.10670","CorpusId":254926556},"title":"In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models"},{"paperId":"126a4776ff8315fd506766cb8f3c722cf746ad9e","externalIds":{"DBLP":"journals/corr/abs-2212-08410","ACL":"2023.acl-short.151","ArXiv":"2212.08410","DOI":"10.48550/arXiv.2212.08410","CorpusId":254823156},"title":"Teaching Small Language Models to Reason"},{"paperId":"397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e","externalIds":{"DBLP":"journals/corr/abs-2212-05055","ArXiv":"2212.05055","DOI":"10.48550/arXiv.2212.05055","CorpusId":254535822},"title":"Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"},{"paperId":"8fd462f6248d5e3f1b6602697c09489086b5655f","externalIds":{"DBLP":"conf/acl/ShridharSS23","ArXiv":"2212.00193","DOI":"10.18653/v1/2023.findings-acl.441","CorpusId":258762841},"title":"Distilling Reasoning Capabilities into Smaller Language Models"},{"paperId":"d8e9f8c8a37cb4cd26b92ad0d942d641cd512644","externalIds":{"DBLP":"journals/corr/abs-2211-17192","ArXiv":"2211.17192","DOI":"10.48550/arXiv.2211.17192","CorpusId":254096365},"title":"Fast Inference from Transformers via Speculative Decoding"},{"paperId":"43014fc85c4860487336579ec98f509fec1803f7","externalIds":{"DBLP":"conf/mlsys/GaleNYZ23","ArXiv":"2211.15841","DOI":"10.48550/arXiv.2211.15841","CorpusId":254069783},"title":"MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"},{"paperId":"2c994fadbb84fb960d8306ee138dbeef41a5b323","externalIds":{"ArXiv":"2211.10438","DBLP":"conf/icml/XiaoLSWDH23","DOI":"10.48550/arXiv.2211.10438","CorpusId":253708271},"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6","externalIds":{"DBLP":"journals/corr/abs-2210-17323","ArXiv":"2210.17323","CorpusId":253237200},"title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"},{"paperId":"b0c5c673c690c644a7d4af73adb783bd98486181","externalIds":{"DBLP":"conf/aaai/FengLJY23","ArXiv":"2210.11794","DOI":"10.48550/arXiv.2210.11794","CorpusId":253080899},"title":"Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences"},{"paperId":"240300b1da360f22bf0b82c6817eacebba6deed4","externalIds":{"DBLP":"conf/iclr/LiCZCD23","ArXiv":"2210.09298","DOI":"10.48550/arXiv.2210.09298","CorpusId":252917984},"title":"What Makes Convolutional Models Great on Long Sequence Modeling?"},{"paperId":"22b58dce1a13382418b8372bbd50ed3b2533f899","externalIds":{"DBLP":"journals/corr/abs-2210-03052","ArXiv":"2210.03052","DOI":"10.1109/IPDPS54959.2023.00042","CorpusId":252734710},"title":"ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"},{"paperId":"4afda39036206dcb3f97829dccb897f1fc80f459","externalIds":{"DBLP":"journals/corr/abs-2210-03162","ArXiv":"2210.03162","DOI":"10.48550/arXiv.2210.03162","CorpusId":252762169},"title":"Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models"},{"paperId":"33be243ac9dd8723e6267dea45fd6a6172d4f6a5","externalIds":{"ArXiv":"2210.01351","DBLP":"journals/corr/abs-2210-01351","DOI":"10.48550/arXiv.2210.01351","CorpusId":252693152},"title":"Less is More: Task-aware Layer-wise Distillation for Language Model Compression"},{"paperId":"b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc","externalIds":{"DBLP":"journals/corr/abs-2209-12951","ArXiv":"2209.12951","DOI":"10.48550/arXiv.2209.12951","CorpusId":252545164},"title":"Liquid Structural State-Space Models"},{"paperId":"13270b9759cf0296b5a346fbb58b706e8ad0a982","externalIds":{"ArXiv":"2209.09570","DBLP":"journals/corr/abs-2209-09570","DOI":"10.1109/MICRO56248.2022.00050","CorpusId":252383196},"title":"Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design"},{"paperId":"30a7390ec0103684eba9fb6bde1983d706fb57b3","externalIds":{"DBLP":"journals/corr/abs-2208-11580","ArXiv":"2208.11580","DOI":"10.48550/arXiv.2208.11580","CorpusId":251765570},"title":"Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"},{"paperId":"86891d00499eebe86d3f1e39143d412addf2652b","externalIds":{"DBLP":"conf/micro/HongMKLKLK22","ArXiv":"2209.10797","DOI":"10.1109/MICRO56248.2022.00051","CorpusId":252439113},"title":"DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation"},{"paperId":"4be7d1524edb0137599a5cc95f72844b85a52fe1","externalIds":{"DBLP":"journals/corr/abs-2208-07339","ArXiv":"2208.07339","DOI":"10.48550/arXiv.2208.07339","CorpusId":251564521},"title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"},{"paperId":"6d7d141c75af752ffc0d8a6184cca3f9323d6c74","externalIds":{"DBLP":"conf/iclr/SmithWL23","ArXiv":"2208.04933","DOI":"10.48550/arXiv.2208.04933","CorpusId":251442769},"title":"Simplified State Space Layers for Sequence Modeling"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","externalIds":{"DBLP":"conf/nips/SchusterFG0B0TM22","ArXiv":"2207.07061","DOI":"10.48550/arXiv.2207.07061","CorpusId":250526382},"title":"Confident Adaptive Language Modeling"},{"paperId":"c022f75b00d795c6297d6a9ea948856ea4d365a1","externalIds":{"DBLP":"conf/sc/AminabadiRALLZRSZRH22","ArXiv":"2207.00032","DOI":"10.1109/SC41404.2022.00051","CorpusId":250243681},"title":"DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"},{"paperId":"eaef083b9d661f42cc0d89d9d8156218f33a91d9","externalIds":{"DBLP":"journals/corr/abs-2206-13947","ArXiv":"2206.13947","DOI":"10.48550/arXiv.2206.13947","CorpusId":250089125},"title":"Long Range Language Modeling via Gated State Spaces"},{"paperId":"76d40153acfbb35a7eb8272a4215854cafa10e78","externalIds":{"DBLP":"conf/icml/ZhangZLBHCZ22","ArXiv":"2206.12562","DOI":"10.48550/arXiv.2206.12562","CorpusId":250072480},"title":"PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance"},{"paperId":"ca444821352a4bd91884413d8070446e2960715a","externalIds":{"DBLP":"journals/corr/abs-2206-11893","ArXiv":"2206.11893","DOI":"10.48550/arXiv.2206.11893","CorpusId":249953875},"title":"On the Parameterization and Initialization of Diagonal State Space Models"},{"paperId":"5eeb828685e44ca5b8ebafb34a9fa4d51c9186df","externalIds":{"ArXiv":"2206.09557","DBLP":"conf/iclr/ParkPKLKKKKLL24","CorpusId":258180013},"title":"LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models"},{"paperId":"2e700ff36108119f5ed19a53bd2eaa22b42ec3d8","externalIds":{"ArXiv":"2206.03382","DBLP":"journals/corr/abs-2206-03382","DOI":"10.48550/arXiv.2206.03382","CorpusId":249431713},"title":"Tutel: Adaptive Mixture-of-Experts at Scale"},{"paperId":"e03609f2587f690867e7ea0bedaf0db25282c548","externalIds":{"DBLP":"conf/nips/YaoAZWLH22","ArXiv":"2206.01861","DOI":"10.48550/arXiv.2206.01861","CorpusId":249395624},"title":"ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"c9550f0d1940ee1adf1549c9a0d699ef896dbefd","externalIds":{"ACL":"2022.acl-long.489","DBLP":"conf/acl/Dai0MZSCW22","ArXiv":"2204.08396","DOI":"10.48550/arXiv.2204.08396","CorpusId":248227505},"title":"StableMoE: Stable Routing Strategy for Mixture of Experts"},{"paperId":"9e82736043eebe3f71eb86cbef6e2ac45306ece5","externalIds":{"ACL":"2022.acl-long.107","DBLP":"conf/acl/XiaZC22","ArXiv":"2204.00408","DOI":"10.48550/arXiv.2204.00408","CorpusId":247922354},"title":"Structured Pruning Learns Compact and Accurate Models"},{"paperId":"fb145e1e49d3269d8223c7710e22b45438613ff0","externalIds":{"DBLP":"journals/corr/abs-2204-09656","ArXiv":"2204.09656","DOI":"10.48550/arXiv.2204.09656","CorpusId":248266822},"title":"A Fast Post-Training Pruning Framework for Transformers"},{"paperId":"71e15a9a52dcafca57bff5f310b95e2c7d0cfc87","externalIds":{"DBLP":"conf/nips/0001GB22","ArXiv":"2203.14343","CorpusId":247762199},"title":"Diagonal State Spaces are as Effective as Structured State Spaces"},{"paperId":"6da9a81b75e7ad02867860753d1aa276673a3a77","externalIds":{"ACL":"2022.emnlp-main.279","DBLP":"conf/emnlp/KurticCNFKFGA22","ArXiv":"2203.07259","DOI":"10.48550/arXiv.2203.07259","CorpusId":247446572},"title":"The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"436278ce3b85265dc5cface29e63c714fe979d23","externalIds":{"ArXiv":"2203.02094","DBLP":"conf/nips/JavaheripiRMSRM22","CorpusId":252968423},"title":"LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models"},{"paperId":"d05141dc0900140f7146bb71e1f7402cf896ea87","externalIds":{"DBLP":"conf/acl/SunLZGWHNXHQ22","ACL":"2022.findings-acl.189","ArXiv":"2203.01670","DOI":"10.48550/arXiv.2203.01670","CorpusId":247222685},"title":"A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation"},{"paperId":"f677ef460670e63a0e9a0bd048cd881b4b55d92f","externalIds":{"ACL":"2022.coling-1.288","ArXiv":"2203.01104","DBLP":"conf/coling/Gao0ZLW22","DOI":"10.48550/arXiv.2203.01104","CorpusId":247218249},"title":"Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models"},{"paperId":"dc0102a51a9d33e104a4a3808a18cf17f057228c","externalIds":{"ArXiv":"2202.10447","DBLP":"journals/corr/abs-2202-10447","CorpusId":247011581},"title":"Transformer Quality in Linear Time"},{"paperId":"bbc57e1b3cf90e09b64377f13de455793bc81ad5","externalIds":{"DBLP":"journals/corr/abs-2202-09368","ArXiv":"2202.09368","CorpusId":247011948},"title":"Mixture-of-Experts with Expert Choice Routing"},{"paperId":"1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3","externalIds":{"ArXiv":"2202.08906","CorpusId":248496391},"title":"ST-MoE: Designing Stable and Transferable Sparse Expert Models"},{"paperId":"802a5d24c78f713e282b003d99b4afd924bd7568","externalIds":{"ACL":"2023.findings-eacl.180","DBLP":"journals/corr/abs-2202-07101","ArXiv":"2202.07101","DOI":"10.18653/v1/2023.findings-eacl.180","CorpusId":246863418},"title":"A Survey on Dynamic Neural Networks for Natural Language Processing"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","externalIds":{"DBLP":"conf/iclr/GuGR22","ArXiv":"2111.00396","CorpusId":240354066},"title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"217913c84a4bdbe5cee3630d70480fda8d44bfb0","externalIds":{"ArXiv":"2111.00230","DBLP":"journals/corr/abs-2111-00230","CorpusId":240354698},"title":"Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning"},{"paperId":"ca9047c78d48b606c4e4f0c456b1dda550de28b2","externalIds":{"DBLP":"conf/nips/GuJGSDRR21","ArXiv":"2110.13985","CorpusId":239998472},"title":"Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"},{"paperId":"561f9f5abb2c0960a886ab6221c821295f0461a1","externalIds":{"DBLP":"conf/acl/ZhangL00S022","ACL":"2022.findings-acl.71","ArXiv":"2110.01786","DOI":"10.18653/v1/2022.findings-acl.71","CorpusId":247958465},"title":"MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"af679d69fcc1d0fcf0f039aba937853bcb50a8de","externalIds":{"ArXiv":"2106.01540","DBLP":"conf/nips/MaKWZMMZ21","CorpusId":235313355},"title":"Luna: Linear Unified Nested Attention"},{"paperId":"5af69480a7ae3b571df6782a11ec4437b386a7d9","externalIds":{"DBLP":"conf/isca/Ham0SKCJL21","DOI":"10.1109/ISCA52012.2021.00060","CorpusId":235414966},"title":"ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"},{"paperId":"03662672662f49e6b06148e94b407b60b0bb72f3","externalIds":{"DBLP":"conf/naacl/LiaoZRSSH21","ACL":"2021.naacl-main.162","MAG":"3170113752","DOI":"10.18653/V1/2021.NAACL-MAIN.162","CorpusId":235097407},"title":"A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models"},{"paperId":"d5e999aae76d5270ef272076979c809817458212","externalIds":{"DBLP":"journals/corr/abs-2105-14103","ArXiv":"2105.14103","CorpusId":235254329},"title":"An Attention Free Transformer"},{"paperId":"dd0a27aa2285bc64798fa76944400ab6d9ce3025","externalIds":{"ArXiv":"2105.14444","MAG":"3167266074","DBLP":"conf/kdd/Xu0LS0QL21","DOI":"10.1145/3447548.3467262","CorpusId":235253768},"title":"NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"},{"paperId":"b15ea460c77a4ee8aa159a30ab0331deedfcf392","externalIds":{"DBLP":"conf/icml/LewisBDGZ21","ArXiv":"2103.16716","CorpusId":232428341},"title":"BASE Layers: Simplifying Training of Large, Sparse Models"},{"paperId":"50796b0f3edf9cb5ff1e447c298b33755378aa4f","externalIds":{"DBLP":"conf/acl/DuQLDQY022","ACL":"2022.acl-long.26","ArXiv":"2103.10360","DOI":"10.18653/v1/2022.acl-long.26","CorpusId":247519241},"title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling"},{"paperId":"9ed25f101f19ea735ca300848948ed64064b97ca","externalIds":{"ArXiv":"2103.02143","DBLP":"journals/corr/abs-2103-02143","CorpusId":232105052},"title":"Random Feature Attention"},{"paperId":"1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a","externalIds":{"DBLP":"journals/corr/abs-2102-02611","ArXiv":"2102.02611","CorpusId":231802365},"title":"CKConv: Continuous Kernel Convolution For Sequential Data"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"73e0f38ab49b19b86321016b773e15f1d02e3a72","externalIds":{"DBLP":"journals/corr/abs-2012-09852","MAG":"3111747337","ArXiv":"2012.09852","DOI":"10.1109/HPCA51647.2021.00018","CorpusId":229298088},"title":"SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","externalIds":{"DBLP":"conf/iclr/ChoromanskiLDSG21","ArXiv":"2009.14794","MAG":"3091156754","CorpusId":222067132},"title":"Rethinking Attention with Performers"},{"paperId":"0964490205fdc38c2f0980c9d778069089ca92e3","externalIds":{"ArXiv":"2008.07669","MAG":"3099512283","DBLP":"conf/nips/GuDERR20","CorpusId":221150566},"title":"HiPPO: Recurrent Memory with Optimal Polynomial Projections"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","externalIds":{"ArXiv":"2007.14062","DBLP":"journals/corr/abs-2007-14062","MAG":"3045733172","CorpusId":220831004},"title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"7c6c31412c5dad22543bb71e31620e8868d644a3","externalIds":{"ArXiv":"2007.08563","MAG":"3047848469","DBLP":"conf/islped/LiPFL0CXWLD20","DOI":"10.1145/3370748.3406567","CorpusId":220633179},"title":"FTRANS: energy-efficient acceleration of transformers using FPGA"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","externalIds":{"MAG":"3040573126","DBLP":"conf/iclr/LepikhinLXCFHKS21","ArXiv":"2006.16668","CorpusId":220265858},"title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"6f68e1bb253925d8431588555d3010419f322e04","externalIds":{"DBLP":"conf/icml/KatharopoulosV020","MAG":"3037798801","ArXiv":"2006.16236","CorpusId":220250819},"title":"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","externalIds":{"MAG":"3033529678","DBLP":"journals/corr/abs-2006-04768","ArXiv":"2006.04768","CorpusId":219530577},"title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8","externalIds":{"MAG":"3101163004","DBLP":"conf/nips/ZhouXGM0W20","ArXiv":"2006.04152","CorpusId":219531455},"title":"BERT Loses Patience: Fast and Robust Inference with Early Exit"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","externalIds":{"MAG":"3033188311","ArXiv":"2006.03236","DBLP":"conf/nips/DaiLY020","CorpusId":219401850},"title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"659bf9ce7175e1ec266ff54359e2bd76e0b7ff31","externalIds":{"DBLP":"conf/nips/LewisPPPKGKLYR020","MAG":"3027879771","ArXiv":"2005.11401","CorpusId":218869575},"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"90a1491ac32e732c93773354e4e665794ed4d490","externalIds":{"MAG":"3035038672","DBLP":"journals/corr/abs-2004-12993","ArXiv":"2004.12993","ACL":"2020.acl-main.204","DOI":"10.18653/v1/2020.acl-main.204","CorpusId":216552850},"title":"DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","externalIds":{"DBLP":"journals/corr/abs-2004-05150","MAG":"3015468748","ArXiv":"2004.05150","CorpusId":215737171},"title":"Longformer: The Long-Document Transformer"},{"paperId":"5d34881ff68bd203ff790187e7e5c9e034389cfa","externalIds":{"MAG":"3014568172","DBLP":"journals/corr/abs-2004-02178","ArXiv":"2004.02178","ACL":"2020.acl-main.537","DOI":"10.18653/v1/2020.acl-main.537","CorpusId":214802887},"title":"FastBERT: a Self-distilling BERT with Adaptive Inference Time"},{"paperId":"1c332cfa211400fc6f56983fb01a6692046116dd","externalIds":{"DBLP":"conf/nips/HouHSJCL20","MAG":"3101731278","ArXiv":"2004.04037","CorpusId":215415863},"title":"DynaBERT: Dynamic BERT with Adaptive Width and Depth"},{"paperId":"657329c633709dd1ac34a30d57341b186b1a47c2","externalIds":{"ACL":"2021.tacl-1.4","MAG":"2997517014","DBLP":"journals/tacl/RoySVG21","ArXiv":"2003.05997","DOI":"10.1162/tacl_a_00353","CorpusId":212718077},"title":"Efficient Content-Based Sparse Attention with Routing Transformers"},{"paperId":"34a4e6818d680875ff0bef9a76de0376118446d1","externalIds":{"MAG":"3034609440","ArXiv":"2002.11296","DBLP":"journals/corr/abs-2002-11296","CorpusId":211505992},"title":"Sparse Sinkhorn Attention"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","externalIds":{"DBLP":"journals/corr/abs-2001-04451","MAG":"2994673210","ArXiv":"2001.04451","CorpusId":209315300},"title":"Reformer: The Efficient Transformer"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"49e5b09480189fc9b2316a54f9d1e55cf0097c8b","externalIds":{"MAG":"2982696980","ArXiv":"1910.13923","DBLP":"conf/icassp/WinataCLLF20","DOI":"10.1109/ICASSP40776.2020.9053878","CorpusId":204960988},"title":"Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank Transformer"},{"paperId":"661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f","externalIds":{"MAG":"2954698171","DBLP":"conf/pldi/TilletKC19","DOI":"10.1145/3315508.3329973","CorpusId":184488182},"title":"Triton: an intermediate language and compiler for tiled neural network computations"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","externalIds":{"ArXiv":"1904.10509","DBLP":"journals/corr/abs-1904-10509","MAG":"2940744433","CorpusId":129945531},"title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"5e04881e91bff952d102d967c4ffb498ec30d4af","externalIds":{"DBLP":"journals/corr/abs-1811-03115","MAG":"2890152612","ArXiv":"1811.03115","CorpusId":53208380},"title":"Blockwise Parallel Decoding for Deep Autoregressive Models"},{"paperId":"6dbb9e4b2e3b67dc4e1634989511f67d41373dd0","externalIds":{"PubMedCentral":"6008460","ArXiv":"1707.04780","DBLP":"journals/corr/MocanuMSNGL17","MAG":"3101584733","DOI":"10.1038/s41467-018-04316-3","CorpusId":49310977,"PubMed":"29921910"},"title":"Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"510e26733aaff585d65701b9f1be7ca9d5afc586","externalIds":{"DBLP":"journals/corr/ShazeerMMDLHD17","MAG":"2952339051","ArXiv":"1701.06538","CorpusId":12462234},"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"},{"paperId":"67d968c7450878190e45ac7886746de867bf673d","externalIds":{"MAG":"2952431534","ArXiv":"1611.01578","DBLP":"journals/corr/ZophL16","CorpusId":12713052},"title":"Neural Architecture Search with Reinforcement Learning"},{"paperId":"a538b05ebb01a40323997629e171c91aa28b8e2f","externalIds":{"MAG":"1665214252","DBLP":"conf/icml/NairH10","CorpusId":15539264},"title":"Rectified Linear Units Improve Restricted Boltzmann Machines"},{"paperId":"e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724","externalIds":{"MAG":"2156150815","DBLP":"conf/icnn/HassibiSW93","DOI":"10.1109/ICNN.1993.298572","CorpusId":61815367},"title":"Optimal Brain Surgeon and general network pruning"},{"paperId":"e0c7559b44997364540a26fb46a92edff56348ad","externalIds":{"MAG":"2117371776","DBLP":"journals/coap/Bertsekas92","DOI":"10.1007/BF00247653","CorpusId":6331577},"title":"Auction algorithms for network flow problems: A tutorial introduction"},{"paperId":"d1a6b3a5efde3783b53f822dc8dd00aaac934b95","externalIds":{"DBLP":"journals/corr/abs-2305-09781","DOI":"10.48550/arXiv.2305.09781","CorpusId":258740799},"title":"SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification"},{"paperId":"81051b830a4f5606106765902a51ba281c9230f9","externalIds":{"DBLP":"journals/corr/abs-2304-09145","DOI":"10.48550/arXiv.2304.09145","CorpusId":258187503},"title":"Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"},{"paperId":"182c6d1d30859f227dca3606c743e178e8ae6780","externalIds":{"CorpusId":261617225},"title":"Structural Pruning of Large Language Models via Neural Architecture Search"},{"paperId":"72c03b873e8c5cd86b15bf73186df341da4731c9","externalIds":{"DBLP":"conf/iclr/SyedGS23","CorpusId":259950394},"title":"Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models"},{"paperId":"51cda783aa6a97e0b3b5915a2bb5a35f31f3c083","externalIds":{"DBLP":"journals/corr/abs-2306-13649","DOI":"10.48550/arXiv.2306.13649","CorpusId":259244026},"title":"GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models"},{"paperId":"0088b2b6f7983a9ac1b53e34a307d68a3383f42c","externalIds":{"DBLP":"conf/acl/TaoHBWJLLW23","DOI":"10.18653/v1/2023.findings-acl.692","CorpusId":259858812},"title":"Structured Pruning for Efficient Generative Pre-trained Language Models"},{"paperId":"3d473cbb7a377cf960abff31748a1a39bb6c7d7c","externalIds":{"DBLP":"journals/corr/abs-2307-15337","DOI":"10.48550/arXiv.2307.15337","CorpusId":260315904},"title":"Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"},{"paperId":"343d24c4dcfaff2132373d218561a23fbd53e934","externalIds":{"DBLP":"journals/corr/abs-2306-02272","DOI":"10.48550/arXiv.2306.02272","CorpusId":259076427},"title":"OWQ: Lessons learned from activation outliers for weight quantization in large language models"},{"paperId":"4b56eef2862f7f553686f1dd190c56017122a6a0","externalIds":{"DBLP":"journals/corr/abs-2310-01655","DOI":"10.48550/arXiv.2310.01655","CorpusId":263609343},"title":"PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels"},{"paperId":"af67be0fff8d087a0d8554b6e8998ab12409bbda","externalIds":{"DBLP":"journals/corr/abs-2307-00526","DOI":"10.48550/arXiv.2307.00526","CorpusId":259316802},"title":"TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"},{"paperId":"55820f684fcd592edfa013633e5704a41b176d23","externalIds":{"DBLP":"journals/corr/abs-2312-08901","DOI":"10.48550/arXiv.2312.08901","CorpusId":273496091},"title":"Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning"},{"paperId":"bb8f7fbec020675d269ccfa0e6e603f02b664c0d","externalIds":{"DBLP":"journals/corr/abs-2305-13888","DOI":"10.48550/arXiv.2305.13888","CorpusId":273994248},"title":"PaD: Program-aided Distillation Specializes Large Models in Reasoning"},{"paperId":"9d7a75601e0e50dd68d40cfb8ef0e891dad797a6","externalIds":{"DBLP":"conf/osdi/YuJKKC22","CorpusId":251734964},"title":"Orca: A Distributed Serving System for Transformer-Based Generative Models"},{"paperId":"4a984ec8286b19bb0c033e6e4df198a0421b0c17","externalIds":{"ACL":"2022.coling-1.414","DBLP":"conf/coling/Kong0YZ22","CorpusId":252818912},"title":"Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting"},{"paperId":"a58ddffb0424021dbc450f3ccdbe3beccb180a05","externalIds":{"DBLP":"conf/nips/XuMLDWZAG22","CorpusId":258509142},"title":"Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models"},{"paperId":"b515de6c2b4a5f08b2f3169f1f1322678deb3257","externalIds":{"CorpusId":252548383},"title":"publicly available"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"0c0a778e6fdf7e36b1750c533dcc916f86608607","externalIds":{"MAG":"2527310337","DBLP":"journals/tkde/XunJGZ17","DOI":"10.1109/TKDE.2016.2614508","CorpusId":13490401},"title":"A Survey on Context Learning"},{"paperId":"5e537c4d988d55f74d0bd5bb5015208977fc52e6","externalIds":{"CorpusId":126210996},"title":"FWDselect : Variable selection algorithm in regression models"},{"paperId":"e7297db245c3feb1897720b173a59fe7e36babb7","externalIds":{"MAG":"2114766824","DBLP":"conf/nips/CunDS89","CorpusId":7785881},"title":"Optimal Brain Damage"},{"paperId":"60a6600258b48c07d60dce4e630dfa9f3cf3b3f7","externalIds":{"CorpusId":281725511},"title":"LLM-MQ: Mixed-precision Quantization for Efﬁcient LLM Deployment"},{"paperId":"9f0fe125af3cfbad99f1f2a6ada0daf61eef92b1","externalIds":{"CorpusId":272768248},"title":"How Long Can Context Length of Open-Source LLMs truly Promise?"}]}