{"paperId":"0307c481f3b125fd87bd7f93fb3c460adf8c7187","externalIds":{"ArXiv":"2310.15676","DBLP":"journals/corr/abs-2310-15676","DOI":"10.48550/arXiv.2310.15676","CorpusId":264439270},"title":"Recent Advances in Multi-modal 3D Scene Understanding: A Comprehensive Survey and Evaluation","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2261432782","name":"Yinjie Lei"},{"authorId":"2261451967","name":"Zixuan Wang"},{"authorId":"2261407288","name":"Feng Chen"},{"authorId":"2261675107","name":"Guoqing Wang"},{"authorId":"2237868559","name":"Peng Wang"},{"authorId":"2261677979","name":"Yang Yang"}],"abstract":"Multi-modal 3D scene understanding has gained considerable attention due to its wide applications in many areas, such as autonomous driving and human-computer interaction. Compared to conventional single-modal 3D understanding, introducing an additional modality not only elevates the richness and precision of scene interpretation but also ensures a more robust and resilient understanding. This becomes especially crucial in varied and challenging environments where solely relying on 3D data might be inadequate. While there has been a surge in the development of multi-modal 3D methods over past three years, especially those integrating multi-camera images (3D+2D) and textual descriptions (3D+language), a comprehensive and in-depth review is notably absent. In this article, we present a systematic survey of recent progress to bridge this gap. We begin by briefly introducing a background that formally defines various 3D multi-modal tasks and summarizes their inherent challenges. After that, we present a novel taxonomy that delivers a thorough categorization of existing methods according to modalities and tasks, exploring their respective strengths and limitations. Furthermore, comparative results of recent approaches on several benchmark datasets, together with insightful analysis, are offered. Finally, we discuss the unresolved issues and provide several potential avenues for future research."}