{"paperId":"523b8635597d6c0bf05fd0e3f35f3a18d2748a24","externalIds":{"ArXiv":"2407.04295","DBLP":"journals/corr/abs-2407-04295","DOI":"10.48550/arXiv.2407.04295","CorpusId":271038633},"title":"Jailbreak Attacks and Defenses Against Large Language Models: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2310316777","name":"Sibo Yi"},{"authorId":"2310274929","name":"Yule Liu"},{"authorId":"2310595263","name":"Zhen Sun"},{"authorId":"2265754853","name":"Tianshuo Cong"},{"authorId":"2295682129","name":"Xinlei He"},{"authorId":"2310387656","name":"Jiaxing Song"},{"authorId":"2307561074","name":"Ke Xu"},{"authorId":"2305736727","name":"Qi Li"}],"abstract":"Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of\"jailbreaking\", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs."}