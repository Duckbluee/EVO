{"references":[{"paperId":"cebf730f71a42a9b2e8e090829e4688771bdd4c6","externalIds":{"DBLP":"journals/corr/abs-2309-07778","ArXiv":"2309.07778","DOI":"10.48550/arXiv.2309.07778","CorpusId":261822584},"title":"Virchow: A Million-Slide Digital Pathology Foundation Model"},{"paperId":"f4dc1886a9ba1def1e9fa230572d21cc85934f7d","externalIds":{"PubMedCentral":"10550819","DOI":"10.1038/s41586-023-06555-x","CorpusId":264168236,"PubMed":"37704728"},"title":"A foundation model for generalizable disease detection from retinal images"},{"paperId":"9298cb4086ba3abaedcfc25f4a1ee1ff69de9a15","externalIds":{"PubMedCentral":"11369198","DBLP":"journals/corr/abs-2309-05904","ArXiv":"2309.05904","DOI":"10.1038/s41467-024-51749-0","CorpusId":261697097,"PubMed":"39223122"},"title":"Enhancing representation in radiography-reports foundation model: a granular alignment algorithm using masked contrastive learning"},{"paperId":"bacdb732960a52dd3b47ad0f73d2343edf808da0","externalIds":{"DBLP":"conf/wacv/AzadNHKAVBM24","ArXiv":"2309.00121","DOI":"10.1109/WACV57701.2024.00132","CorpusId":261493753},"title":"Beyond Self-Attention: Deformable Large Kernel Attention for Medical Image Segmentation"},{"paperId":"f55b95dbe6d89547736a3b54cbff120130f060f1","externalIds":{"DBLP":"conf/miccai/AzadKAAVBM23","ArXiv":"2309.00108","DOI":"10.48550/arXiv.2309.00108","CorpusId":261493786,"PubMed":"38299070"},"title":"Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection"},{"paperId":"0f0024bfef037b97b324b97150ee022c178d6282","externalIds":{"DOI":"10.1038/s41591-023-02504-3","CorpusId":260970273,"PubMed":"37592105"},"title":"A visual–language foundation model for pathology image analysis using medical Twitter"},{"paperId":"f660250f9fcd465acdf2e727d309acf1cc64c780","externalIds":{"ArXiv":"2308.01317","DBLP":"journals/corr/abs-2308-01317","DOI":"10.48550/arXiv.2308.01317","CorpusId":260378981},"title":"ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders"},{"paperId":"6eb3dd2b64db74f9743802acbb9875bdffb9d246","externalIds":{"DBLP":"journals/corr/abs-2307-14901","ArXiv":"2307.14901","DOI":"10.48550/arXiv.2307.14901","CorpusId":260203416},"title":"Text-guided Foundation Model Adaptation for Pathological Image Classification"},{"paperId":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","externalIds":{"ArXiv":"2307.15189","DBLP":"conf/ml4h/MoorHWYDLZRR23","DOI":"10.48550/arXiv.2307.15189","CorpusId":260316059},"title":"Med-Flamingo: a Multimodal Medical Few-shot Learner"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","externalIds":{"ArXiv":"2307.13721","DBLP":"journals/corr/abs-2307-13721","DOI":"10.48550/arXiv.2307.13721","CorpusId":260164769},"title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"9b348715d0311056eee850dd1cce1cdd3c64eec8","externalIds":{"DBLP":"conf/miccai/ChenHXGLY23","ArXiv":"2307.07246","DOI":"10.48550/arXiv.2307.07246","CorpusId":259924614},"title":"Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training"},{"paperId":"25b67873c4bc9afb35224bd984554430fe91d5a7","externalIds":{"DBLP":"conf/miccai/WangLZD23","ArXiv":"2306.16741","DOI":"10.48550/arXiv.2306.16741","CorpusId":259287248},"title":"Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train"},{"paperId":"656e1ab75ff92733b57118d916a320c51c991fd3","externalIds":{"ArXiv":"2306.13465","DBLP":"journals/mia/GongZMLWZHD24","DOI":"10.1016/j.media.2024.103324","CorpusId":259243550,"PubMed":"39213939"},"title":"3DSAM-adapter: Holistic adaptation of SAM from 2D to 3D for promptable tumor segmentation"},{"paperId":"fc754cb6bc93045e9052e1476b76e149c6b1cac2","externalIds":{"ArXiv":"2306.13731","DBLP":"journals/corr/abs-2306-13731","DOI":"10.48550/arXiv.2306.13731","CorpusId":259252329},"title":"How to Efficiently Adapt Large Segmentation Model(SAM) to Medical Images"},{"paperId":"c01e94a36be5b3578fedb17205205b330290a778","externalIds":{"DBLP":"journals/corr/abs-2306-12156","ArXiv":"2306.12156","DOI":"10.48550/arXiv.2306.12156","CorpusId":259212104},"title":"Fast Segment Anything"},{"paperId":"f51017a549c1d858ae7d0650e1bad0186cb17808","externalIds":{"DBLP":"conf/nips/NguyenNDPC0SHAX23","ArXiv":"2306.11925","DOI":"10.48550/arXiv.2306.11925","CorpusId":259212370},"title":"LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching"},{"paperId":"89cefcdd59ae9868f808a7b1058492504e89fa77","externalIds":{"ACL":"2024.bionlp-1.35","DBLP":"conf/bionlp/ThawakarSMCAKLK24","ArXiv":"2306.07971","DOI":"10.18653/v1/2024.bionlp-1.35","CorpusId":259145194},"title":"XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models"},{"paperId":"29cd4e8504df8c762b0b6eef8299584118feeb88","externalIds":{"DBLP":"conf/nips/TschannenKSZHB23","ArXiv":"2306.07915","DOI":"10.48550/arXiv.2306.07915","CorpusId":259145047},"title":"Image Captioners Are Scalable Vision Learners Too"},{"paperId":"fed150a219f9c31bdb4920e615c7c9264c634736","externalIds":{"DBLP":"journals/mia/ZhangM24","ArXiv":"2306.05705","DOI":"10.48550/arXiv.2306.05705","CorpusId":259129811,"PubMed":"37857067"},"title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis"},{"paperId":"a42fc49a300136d60aaebb668369010ee7746150","externalIds":{"DBLP":"conf/cvpr/LuCZWCDLCM23","ArXiv":"2306.07831","DOI":"10.1109/CVPR52729.2023.01893","CorpusId":259144830},"title":"Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","externalIds":{"DBLP":"journals/corr/abs-2306-00890","ArXiv":"2306.00890","DOI":"10.48550/arXiv.2306.00890","CorpusId":258999820},"title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"daf34122a0c38531aeeb55069ba98e564c263d53","externalIds":{"ArXiv":"2305.10799","DBLP":"conf/accv/ChenH24","DOI":"10.48550/arXiv.2305.10799","CorpusId":258762408},"title":"MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts"},{"paperId":"ea72fb2a0d340f9d14fbcf300cd5f5fbbe1050bb","externalIds":{"DBLP":"journals/corr/abs-2305-09617","ArXiv":"2305.09617","DOI":"10.48550/arXiv.2305.09617","CorpusId":258715226},"title":"Towards Expert-Level Medical Question Answering with Large Language Models"},{"paperId":"6f8b9192b1f215254ee7625d752710182c05d2f9","externalIds":{"DBLP":"journals/corr/abs-2305-02677","ArXiv":"2305.02677","DOI":"10.48550/arXiv.2305.02677","CorpusId":258479994},"title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls"},{"paperId":"705315602e9e78b155220169d7704475efeb4a11","externalIds":{"DBLP":"journals/corr/abs-2304-13785","ArXiv":"2304.13785","DOI":"10.48550/arXiv.2304.13785","CorpusId":258352583},"title":"Customized Segment Anything Model for Medical Image Segmentation"},{"paperId":"49f9882d5fd442f02f9c9dff780336f6dce2da4f","externalIds":{"DBLP":"journals/corr/abs-2304-12620","ArXiv":"2304.12620","DOI":"10.48550/arXiv.2304.12620","CorpusId":258309597,"PubMed":"40121809"},"title":"Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation"},{"paperId":"78b03df885cbb57361e8efc5ce5ad5eea211dda6","externalIds":{"PubMedCentral":"10252742","ArXiv":"2304.12637","DBLP":"journals/corr/abs-2304-12637","DOI":"10.3390/diagnostics13111947","CorpusId":258309531,"PubMed":"37296799"},"title":"Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation"},{"paperId":"7ea2ac51f5ea8c4626b7a0f4dc6b23817e26701f","externalIds":{"ArXiv":"2304.09728","DBLP":"journals/corr/abs-2304-09728","DOI":"10.48550/arXiv.2304.09728","CorpusId":258236744},"title":"Any-to-Any Style Transfer: Making Picasso and Da Vinci Collaborate"},{"paperId":"72570016bbb28e8fb15ab4667eb84887f5dd35ad","externalIds":{"ArXiv":"2304.06790","DBLP":"journals/corr/abs-2304-06790","DOI":"10.48550/arXiv.2304.06790","CorpusId":258170322},"title":"Inpaint Anything: Segment Anything Meets Image Inpainting"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"690df0820f35a47e1ce44f90e6ddb4132aa09267","externalIds":{"DBLP":"journals/pami/ZhangHJL24","ArXiv":"2304.00685","DOI":"10.1109/TPAMI.2024.3369699","CorpusId":257913547,"PubMed":"38408000"},"title":"Vision-Language Models for Vision Tasks: A Survey"},{"paperId":"07f07d4d59fdbc3596284f51057cb006779d42c1","externalIds":{"DBLP":"journals/corr/abs-2304-02020","ArXiv":"2304.02020","DOI":"10.1145/3664930","CorpusId":257952516},"title":"A Bibliometric Review of Large Language Models Research from 2017 to 2023"},{"paperId":"9faa2b0e5cb93f20df0555c3c350fab0b2eccf3a","externalIds":{"DOI":"10.1038/s41586-023-05881-4","CorpusId":258083369,"PubMed":"37045921"},"title":"Foundation models for generalist medical artificial intelligence"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"cff26bda86237d113ed01c812ad8bedd0afbe070","externalIds":{"DBLP":"journals/corr/abs-2303-11032","ArXiv":"2303.11032","DOI":"10.48550/arXiv.2303.11032","CorpusId":257632030},"title":"DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4"},{"paperId":"5814bd146b37e13115af4330caf3a751159a156f","externalIds":{"ArXiv":"2303.00915","CorpusId":257280046},"title":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"},{"paperId":"20fb06a4aa4010470d388098618af5d1bea224ad","externalIds":{"PubMedCentral":"10045796","DOI":"10.3390/bioengineering10030380","CorpusId":257683248,"PubMed":"36978771"},"title":"Vision–Language Model for Visual Question Answering in Medical Imagery"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","externalIds":{"DBLP":"conf/iccv/ChenDWLW23","ArXiv":"2302.08958","DOI":"10.1109/ICCV51070.2023.02139","CorpusId":257019905},"title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"5ef821267fa68d3231ed8135ff8ec09f25bb1398","externalIds":{"DBLP":"journals/corr/abs-2302-07257","ArXiv":"2302.07257","DOI":"10.48550/arXiv.2302.07257","CorpusId":256846858},"title":"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"},{"paperId":"6c489a9dc649298aea729c91822a3c89de503729","externalIds":{"ArXiv":"2302.04237","CorpusId":258960116},"title":"Black Box Adversarial Prompting for Foundation Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"74e7edf7c6436bb4992ca9c9df9476c9ccf31919","externalIds":{"ArXiv":"2301.05065","DBLP":"journals/corr/abs-2301-05065","DOI":"10.48550/arXiv.2301.05065","CorpusId":255749525},"title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks"},{"paperId":"84729ec815f0607a4a2370c0969e8c3ba82a9411","externalIds":{"ArXiv":"2301.04558","DBLP":"journals/corr/abs-2301-04558","DOI":"10.1109/CVPR52729.2023.01442","CorpusId":255595629},"title":"Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing"},{"paperId":"8b0357f1bceb9cf7a5629b0ba3acb5660edf90b2","externalIds":{"DBLP":"journals/corr/abs-2301-03505","ArXiv":"2301.03505","DOI":"10.48550/arXiv.2301.03505","CorpusId":255545907,"PubMed":"37883822"},"title":"Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review"},{"paperId":"125632627bfad80c2c688bcbed7f3ee915de7359","externalIds":{"ArXiv":"2301.00785","DBLP":"journals/corr/abs-2301-00785","DOI":"10.1109/ICCV51070.2023.01934","CorpusId":255372928},"title":"CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","externalIds":{"ArXiv":"2212.10403","DBLP":"conf/acl/0009C23","DOI":"10.48550/arXiv.2212.10403","CorpusId":254877753},"title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"3936fd3c6187f606c6e4e2e20b196dbc41cc4654","externalIds":{"DBLP":"journals/corr/abs-2212-08073","ArXiv":"2212.08073","DOI":"10.48550/arXiv.2212.08073","CorpusId":254823489},"title":"Constitutional AI: Harmlessness from AI Feedback"},{"paperId":"9ceaeff7117965832f4c05fd6355d021862d0a82","externalIds":{"DBLP":"conf/cvpr/WangWCS023","ArXiv":"2212.02499","DOI":"10.1109/CVPR52729.2023.00660","CorpusId":254246343},"title":"Images Speak in Images: A Generalist Painter for In-Context Visual Learning"},{"paperId":"714bb3e0c524c040c5426b0d65c805356d26e931","externalIds":{"DBLP":"journals/pami/AzadARJABKCAM24","ArXiv":"2211.14830","DOI":"10.1109/TPAMI.2024.3435571","CorpusId":254043694,"PubMed":"39167505"},"title":"Medical Image Segmentation Review: The Success of U-Net"},{"paperId":"4bae689ade260c1624406b5bf2d58d637a0c5aa9","externalIds":{"DBLP":"conf/icml/Luo0W0023","ArXiv":"2211.14813","DOI":"10.48550/arXiv.2211.14813","CorpusId":254043520},"title":"SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","externalIds":{"DBLP":"journals/corr/abs-2211-09085","ArXiv":"2211.09085","CorpusId":253553203},"title":"Galactica: A Large Language Model for Science"},{"paperId":"b57b8b6b8052bf2e7f6fe5c8e91cdcb385b75ab6","externalIds":{"DBLP":"journals/mia/KazerouniAHAFHM23","ArXiv":"2211.07804","DOI":"10.1016/j.media.2023.102846","CorpusId":253523207,"PubMed":"37295311"},"title":"Diffusion models in medical imaging: A comprehensive survey"},{"paperId":"cdd9c1d23f9e89d5113f3e31821bb174c6a6afed","externalIds":{"DBLP":"journals/corr/abs-2210-10163","ArXiv":"2210.10163","ACL":"2022.emnlp-main.256","DOI":"10.48550/arXiv.2210.10163","CorpusId":252992913,"PubMed":"39144675"},"title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"},{"paperId":"76120de60a9e59c23a372457a056da3c220c64b6","externalIds":{"PubMedCentral":"9792370","DOI":"10.1038/s41551-022-00936-9","CorpusId":252309796,"PubMed":"36109605"},"title":"Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning"},{"paperId":"e015e07c76ff85f624b78112fd58937221f5ea0c","externalIds":{"DBLP":"conf/aaai/YanP22","DOI":"10.1609/aaai.v36i3.20204","CorpusId":250298613},"title":"Clinical-BERT: Vision-Language Pre-training for Radiograph Diagnosis and Reports Generation"},{"paperId":"49b5ffebdbcbd683010a2558a19eaa9b21cd8c34","externalIds":{"ArXiv":"2206.05836","CorpusId":249626342},"title":"GLIPv2: Unifying Localization and Vision-Language Understanding"},{"paperId":"5e76879aaea118b532fb24a50b721076d4c6ae93","externalIds":{"DBLP":"journals/corr/abs-2204-03610","ArXiv":"2204.03610","DOI":"10.1109/CVPR52688.2022.01857","CorpusId":248006101},"title":"Unified Contrastive Learning in Image-Text-Label Space"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b9225c672a5078409d890393780a5eb90f2ec3ca","externalIds":{"ArXiv":"2203.05962","DBLP":"journals/corr/abs-2203-05962","DOI":"10.48550/arXiv.2203.05962","CorpusId":247411040},"title":"Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"e9581d9758062f76e029bd19a58c4ae976cfb414","externalIds":{"ArXiv":"2112.12750","DBLP":"journals/corr/abs-2112-12750","DOI":"10.1007/978-3-031-19809-0_30","CorpusId":245424883},"title":"SLIP: Self-supervision meets Language-Image Pre-training"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","externalIds":{"DBLP":"conf/cvpr/SinghHGCGRK22","ArXiv":"2112.04482","DOI":"10.1109/CVPR52688.2022.01519","CorpusId":244954250},"title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","externalIds":{"DBLP":"journals/corr/abs-2112-03857","ArXiv":"2112.03857","DOI":"10.1109/CVPR52688.2022.01069","CorpusId":244920947},"title":"Grounded Language-Image Pre-training"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","externalIds":{"ArXiv":"2111.07783","DBLP":"journals/corr/abs-2111-07783","CorpusId":244117525},"title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","externalIds":{"DBLP":"journals/corr/abs-2107-07651","ArXiv":"2107.07651","CorpusId":236034189},"title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","externalIds":{"DBLP":"conf/iclr/Bao0PW22","ArXiv":"2106.08254","CorpusId":235436185},"title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","externalIds":{"ArXiv":"2102.05918","DBLP":"conf/icml/JiaYXCPPLSLD21","CorpusId":231879586},"title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2","externalIds":{"MAG":"3100859887","DBLP":"journals/corr/abs-2006-10029","ArXiv":"2006.10029","CorpusId":219721239},"title":"Big Self-Supervised Models are Strong Semi-Supervised Learners"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"acf66668d0c9214aa832c58a84e30bdcc1775ddc","externalIds":{"DBLP":"journals/corr/abs-2002-11328","MAG":"3008906732","ArXiv":"2002.11328","CorpusId":211505957},"title":"Rethinking Bias-Variance Trade-off for Generalization of Neural Networks"},{"paperId":"7af72a461ed7cda180e7eab878efd5f35d79bbf4","externalIds":{"DBLP":"conf/icml/ChenK0H20","MAG":"3034978746","ArXiv":"2002.05709","CorpusId":211096730},"title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","externalIds":{"DBLP":"conf/cvpr/He0WXG20","MAG":"2987283559","ArXiv":"1911.05722","DOI":"10.1109/cvpr42600.2020.00975","CorpusId":207930212},"title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","externalIds":{"MAG":"2842511635","DBLP":"journals/corr/abs-1807-03748","ArXiv":"1807.03748","CorpusId":49670925},"title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"cd2d1a0f73ba8c40f882a386cd367899785fb877","externalIds":{"DBLP":"journals/corr/abs-2304-14454","DOI":"10.48550/arXiv.2304.14454","CorpusId":263888272},"title":"PMC-LLaMA: Further Finetuning LLaMA on Medical Papers"},{"paperId":"7c33f9977cbe3e6205bdd55197c71ddd8efb73ff","externalIds":{"DBLP":"journals/corr/abs-2306-14752","DOI":"10.48550/arXiv.2306.14752","CorpusId":259251500},"title":"MedLSAM: Localize and Segment Anything Model for 3D Medical Images"},{"paperId":"06e34dbee5422b60bdbcf1f984d7c3a55e8aab1f","externalIds":{"DBLP":"journals/corr/abs-2302-00162","DOI":"10.48550/arXiv.2302.00162","CorpusId":256459856},"title":"Continual Segment: Towards a Single, Unified and Accessible Continual Segmentation Model of 143 Whole-body Organs in CT Scans"},{"paperId":"c9dff8253b2e776abf363d0a4836abcaf64ee327","externalIds":{"DBLP":"journals/corr/abs-2303-14070","DOI":"10.48550/arXiv.2303.14070","CorpusId":257756992},"title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge"},{"paperId":"4dd869a2c17cacd03b3e8a9b7efcd48f40651925","externalIds":{"DBLP":"journals/corr/abs-2304-12306","DOI":"10.48550/arXiv.2304.12306","CorpusId":258298289},"title":"Segment Anything in Medical Images"},{"paperId":"06d8562831c32844285a691c5250d04726df3c61","externalIds":{"DBLP":"journals/corr/abs-2307-12980","DOI":"10.48550/arXiv.2307.12980","CorpusId":260357841},"title":"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models"}]}