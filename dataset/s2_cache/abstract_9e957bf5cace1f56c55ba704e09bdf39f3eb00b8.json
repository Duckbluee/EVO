{"abstract":"Nowadays, artificial intelligence models are widely used in financial services, from credit scoring to fraud detection, having a direct impact on our daily lives. Although such models have been developed to try to reduce human bias and thus bring greater fairness to financial services decisions, studies have found that there is still significant discrimination by both face-to-face and algorithmic lenders. In fact, Apple has recently been investigated for gender discrimination in assigning a credit limit to its users, demonstrating that there may still be inherent biases in the development of such algorithms and models. Furthermore, biases in financial services models may not only lead to unfair discrimination but were also linked to health problems and recovery prospects. This project aims to analyse and identify the different types of biases found in AI models and data used in the financial services industry. We propose a method using data analysis and explainable models to explain how these biases emerge throughout the process of developing AI models as well as applying state-of-the-art bias dealing techniques to avoid and mitigate them. Finally, we propose how to evaluate these models according to the business objectives and consider possible trade-offs between different definitions of fairness. Thus, the main questions that this project will try to answer are as follows: - What are the current biases in credit risk and fraud detection models and how to identify them? - In what ways understanding how biases emerge from the data can help us in bias mitigation? - To what extent could credit risk and fraud detection models bias be mitigated, and what are the implications of those mitigation techniques? Answering these questions, we hope to create a pipeline for building these models by understanding the key points where bias can emerge and the appropriate methods to avoid it."}