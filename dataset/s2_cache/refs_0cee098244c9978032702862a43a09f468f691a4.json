{"references":[{"paperId":"f1a9e0830bc36c048fa4659beaa62609869895b5","externalIds":{"DBLP":"conf/icml/FuBS024","ArXiv":"2402.02057","DOI":"10.48550/arXiv.2402.02057","CorpusId":267412730},"title":"Break the Sequential Dependency of LLM Inference Using Lookahead Decoding"},{"paperId":"1b5db3170c195508ff24fee8eda0d4987e806f0b","externalIds":{"DBLP":"journals/corr/abs-2401-15077","ArXiv":"2401.15077","DOI":"10.48550/arXiv.2401.15077","CorpusId":267301131},"title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"},{"paperId":"a8b66565cdb2b8c90556bb98a7fc58ac679c2cec","externalIds":{"ArXiv":"2401.14021","DBLP":"journals/corr/abs-2401-14021","DOI":"10.48550/arXiv.2401.14021","CorpusId":267212215},"title":"Accelerating Retrieval-Augmented Language Model Serving with Speculation"},{"paperId":"57e7af0b69325fafb371ef5d502e39ef9c90ef7e","externalIds":{"ArXiv":"2401.10774","DBLP":"journals/corr/abs-2401-10774","DOI":"10.48550/arXiv.2401.10774","CorpusId":267061277},"title":"Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"},{"paperId":"e30666ed82670463aa47686e744f0c6f2a0e083d","externalIds":{"ArXiv":"2312.11462","DBLP":"journals/corr/abs-2312-11462","DOI":"10.48550/arXiv.2312.11462","CorpusId":266359077},"title":"Cascade Speculative Drafting for Even Faster LLM Inference"},{"paperId":"4067a6f57f708dec4459d3d4322373e06c2b168c","externalIds":{"ArXiv":"2311.13581","DBLP":"journals/corr/abs-2311-13581","DOI":"10.48550/arXiv.2311.13581","CorpusId":265351429},"title":"PaSS: Parallel Speculative Sampling"},{"paperId":"ea1f648988c632a6dbab6d8b88432456aa021cfb","externalIds":{"DBLP":"conf/nips/SunSRBJY23","ArXiv":"2310.15141","DOI":"10.48550/arXiv.2310.15141","CorpusId":264591415},"title":"SpecTr: Fast Speculative Decoding via Optimal Transport"},{"paperId":"f206d34afed6a5705757f96ea97c7bfb9e1a83cd","externalIds":{"ArXiv":"2310.12072","DBLP":"journals/corr/abs-2310-12072","DOI":"10.48550/arXiv.2310.12072","CorpusId":264289340},"title":"SPEED: Speculative Pipelined Execution for Efficient Decoding"},{"paperId":"56767c18bb5aaa2b6377624168bed1b6dcc4b94d","externalIds":{"DBLP":"conf/iclr/ZhouLRMRKKA24","ArXiv":"2310.08461","DOI":"10.48550/arXiv.2310.08461","CorpusId":263909387},"title":"DistillSpec: Improving Speculative Decoding via Knowledge Distillation"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"00e889fcfaf4396a20f37f681cf8b14f3e878879","externalIds":{"ArXiv":"2309.04255","DBLP":"journals/corr/abs-2309-04255","DOI":"10.48550/arXiv.2309.04255","CorpusId":261660737},"title":"LLMCad: Fast and Scalable On-device Large Language Model Inference"},{"paperId":"43e624ddeed82df944a6cae0dedec3372438e243","externalIds":{"DBLP":"journals/corr/abs-2308-04623","ArXiv":"2308.04623","DOI":"10.48550/arXiv.2308.04623","CorpusId":260735640},"title":"Accelerating LLM Inference with Staged Speculative Decoding"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"823ca4778e1027f2f0b356df051d762dcecaaba0","externalIds":{"ArXiv":"2307.08691","DBLP":"journals/corr/abs-2307-08691","DOI":"10.48550/arXiv.2307.08691","CorpusId":259936734},"title":"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"},{"paperId":"d988eb48e8b4e471f5df9d081bfc32db0781e6bf","externalIds":{"DBLP":"journals/tmlr/YangLCP024","ArXiv":"2307.05908","DOI":"10.48550/arXiv.2307.05908","CorpusId":259837553},"title":"Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding"},{"paperId":"3556722b4703a21abafd2f9388743202943f4503","externalIds":{"ACL":"2023.acl-long.689","DBLP":"journals/corr/abs-2305-10427","ArXiv":"2305.10427","DOI":"10.18653/v1/2023.acl-long.689","CorpusId":258741236},"title":"Accelerating Transformer Inference for Translation via Parallel Decoding"},{"paperId":"f0c31511134abdd23f990310e8a2f2eb3a629b62","externalIds":{"ArXiv":"2305.09781","DBLP":"conf/asplos/MiaoOZCWZWZYSSC24","DOI":"10.1145/3620666.3651335","CorpusId":267094734},"title":"SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"b7d12aec8a0152ec4921dfa43ab525a63b334385","externalIds":{"DBLP":"conf/nips/KimMMMMGK23","ArXiv":"2302.07863","CorpusId":256868484},"title":"Speculative Decoding with Big Little Decoder"},{"paperId":"a1f8082505c7e90b0a033e1b9da0a97d67aad66c","externalIds":{"DBLP":"journals/corr/abs-2302-01318","ArXiv":"2302.01318","DOI":"10.48550/arXiv.2302.01318","CorpusId":256503945},"title":"Accelerating Large Language Model Decoding with Speculative Sampling"},{"paperId":"d8e9f8c8a37cb4cd26b92ad0d942d641cd512644","externalIds":{"DBLP":"journals/corr/abs-2211-17192","ArXiv":"2211.17192","DOI":"10.48550/arXiv.2211.17192","CorpusId":254096365},"title":"Fast Inference from Transformers via Speculative Decoding"},{"paperId":"88b62496cbc52072bfa8f4b29d172b0477b701bc","externalIds":{"DBLP":"conf/acl/LiHFLEHZL23","ArXiv":"2210.15097","ACL":"2023.acl-long.687","DOI":"10.48550/arXiv.2210.15097","CorpusId":253157949},"title":"Contrastive Decoding: Open-ended Text Generation as Optimization"},{"paperId":"33389432111bb8c54143c5b6c02680d77d6a5bd4","externalIds":{"DBLP":"conf/coling/DuTWJ22","ArXiv":"2210.03999","ACL":"2022.coling-1.446","DOI":"10.48550/arXiv.2210.03999","CorpusId":263795022},"title":"ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation"},{"paperId":"735bf29aaf13c9420653e271db37614be55154d7","externalIds":{"DBLP":"conf/sigir/0002WL022","DOI":"10.1145/3477495.3532682","CorpusId":250340214},"title":"Recent Advances in Retrieval-Augmented Text Generation"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"ae06a01845948c86210f88dfca9abdd02e4506cf","externalIds":{"ArXiv":"2205.10350","DBLP":"journals/corr/abs-2205-10350","DOI":"10.48550/arXiv.2205.10350","CorpusId":248964928},"title":"Lossless Acceleration for Seq2seq Generation with Aggressive Decoding"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"218c5c69f3cf0c158e9b6af239a2cc62a688c6de","externalIds":{"ArXiv":"2203.16487","DBLP":"conf/emnlp/Xia0WCWS23","DOI":"10.18653/v1/2023.findings-emnlp.257","CorpusId":264724056},"title":"Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation"},{"paperId":"08567bab1d37d166a2ce6c10f7d4132769c438f6","externalIds":{"DBLP":"journals/tist/WangWDLL21","DOI":"10.1145/3474840","CorpusId":245355932},"title":"A Comprehensive Survey of Grammatical Error Correction"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"3efd4b048dd7544333092332bccc3f0aea79f5c7","externalIds":{"DBLP":"conf/acl/SunGWW20","ArXiv":"2106.04970","ACL":"2021.acl-long.462","DOI":"10.18653/v1/2021.acl-long.462","CorpusId":235377417},"title":"Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding"},{"paperId":"e2f679183c504d06767223955a578480ecfe808d","externalIds":{"ArXiv":"2106.05093","DBLP":"conf/icml/DuTJ21","CorpusId":235377210},"title":"Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation"},{"paperId":"33d05a1ff5f15bdc60fc43fa8523a4888af5f116","externalIds":{"DBLP":"journals/corr/abs-2008-07905","ACL":"2021.acl-long.155","ArXiv":"2008.07905","MAG":"3054488230","DOI":"10.18653/v1/2021.acl-long.155","CorpusId":221150562},"title":"Glancing Transformer for Non-Autoregressive Neural Machine Translation"},{"paperId":"659bf9ce7175e1ec266ff54359e2bd76e0b7ff31","externalIds":{"DBLP":"conf/nips/LewisPPPKGKLYR020","MAG":"3027879771","ArXiv":"2005.11401","CorpusId":218869575},"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"b26f2037f769d5ffc5f7bdcec2de8da28ec14bee","externalIds":{"MAG":"3015883388","ArXiv":"2004.04906","DBLP":"conf/emnlp/KarpukhinOMLWEC20","ACL":"2020.emnlp-main.550","DOI":"10.18653/v1/2020.emnlp-main.550","CorpusId":215737187},"title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","externalIds":{"ACL":"Q19-1026","MAG":"2912924812","DBLP":"journals/tacl/KwiatkowskiPRCP19","DOI":"10.1162/tacl_a_00276","CorpusId":86611921},"title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"58d34a4fb936ffe95917d8fb4016ff5e3520429a","externalIds":{"MAG":"2949644922","DBLP":"conf/icml/SternCKU19","ArXiv":"1902.03249","CorpusId":60440448},"title":"Insertion Transformer: Flexible Sequence Generation via Insertion Operations"},{"paperId":"5e04881e91bff952d102d967c4ffb498ec30d4af","externalIds":{"DBLP":"journals/corr/abs-1811-03115","MAG":"2890152612","ArXiv":"1811.03115","CorpusId":53208380},"title":"Blockwise Parallel Decoding for Deep Autoregressive Models"},{"paperId":"15e81c8d1c21f9e928c72721ac46d458f3341454","externalIds":{"DBLP":"journals/corr/abs-1711-02281","MAG":"2767206889","ArXiv":"1711.02281","CorpusId":3480671},"title":"Non-Autoregressive Neural Machine Translation"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","externalIds":{"MAG":"2899575547","DOI":"10.18653/v1/p16-1","CorpusId":65179127},"title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"57a10537978600fd33dcdd48922c791609a4851a","externalIds":{"ACL":"D16-1139","DBLP":"conf/emnlp/KimR16","ArXiv":"1606.07947","MAG":"2463507112","DOI":"10.18653/v1/D16-1139","CorpusId":8451212},"title":"Sequence-Level Knowledge Distillation"},{"paperId":"f37076f426023241f19cdc2fb0a0fd733a6fa7fa","externalIds":{"ArXiv":"1602.06023","DBLP":"conf/conll/NallapatiZSGX16","MAG":"2341401723","ACL":"K16-1028","DOI":"10.18653/v1/K16-1028","CorpusId":8928715},"title":"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"},{"paperId":"843a1567b056c8a1d0deddc8b699e1725194f85c","externalIds":{"MAG":"1999563089","DBLP":"journals/cacm/Patterson04a","DOI":"10.1145/1022594.1022596","CorpusId":30274066},"title":"Latency lags bandwith"},{"paperId":"944adc818b89a543282f9be3a356df67a09faccd","externalIds":{"MAG":"2158992088","DBLP":"journals/tc/Burton85","DOI":"10.1109/TC.1985.6312218","CorpusId":27560619},"title":"Speculative computation, parallelism, and functional programming"},{"paperId":"bccb4ff16f52113a93cde7025a82f581695beb19","externalIds":{"DBLP":"books/daglib/0028244","MAG":"1555915743","CorpusId":60693966},"title":"Computer Architecture - A Quantitative Approach, 5th Edition"},{"paperId":"d1a6b3a5efde3783b53f822dc8dd00aaac934b95","externalIds":{"DBLP":"journals/corr/abs-2305-09781","DOI":"10.48550/arXiv.2305.09781","CorpusId":258740799},"title":"SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification"},{"paperId":"9d7a75601e0e50dd68d40cfb8ef0e891dad797a6","externalIds":{"DBLP":"conf/osdi/YuJKKC22","CorpusId":251734964},"title":"Orca: A Distributed Serving System for Transformer-Based Generative Models"}]}