{"references":[{"paperId":"6631167f0e9baa1963297727566bca3c43573192","externalIds":{"DBLP":"journals/corr/abs-2309-16661","ArXiv":"2309.16661","DOI":"10.48550/arXiv.2309.16661","CorpusId":263134836},"title":"SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation"},{"paperId":"bacdb732960a52dd3b47ad0f73d2343edf808da0","externalIds":{"DBLP":"conf/wacv/AzadNHKAVBM24","ArXiv":"2309.00121","DOI":"10.1109/WACV57701.2024.00132","CorpusId":261493753},"title":"Beyond Self-Attention: Deformable Large Kernel Attention for Medical Image Segmentation"},{"paperId":"f55b95dbe6d89547736a3b54cbff120130f060f1","externalIds":{"DBLP":"conf/miccai/AzadKAAVBM23","ArXiv":"2309.00108","DOI":"10.48550/arXiv.2309.00108","CorpusId":261493786,"PubMed":"38299070"},"title":"Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection"},{"paperId":"740512dfadd91cd52771f2a721509c371efb9b89","externalIds":{"DBLP":"conf/iccv/RenYLW23","ArXiv":"2308.12216","DOI":"10.1109/ICCV51070.2023.00552","CorpusId":261076290},"title":"SG-Former: Self-guided Transformer with Evolving Token Reallocation"},{"paperId":"131ba9932572c92155874db93626cf299659254e","externalIds":{"ArXiv":"2308.00442","DBLP":"journals/corr/abs-2308-00442","DOI":"10.1109/ICCV51070.2023.00548","CorpusId":260351423},"title":"FLatten Transformer: Vision Transformer using Focused Linear Attention"},{"paperId":"d7890d1906d95c4ae4c430b350455156d6d8aed9","externalIds":{"DBLP":"conf/iclr/PodellELBDMPR24","ArXiv":"2307.01952","CorpusId":259341735},"title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"},{"paperId":"37defa9aa690c033f9c4d1510df32ae5e996f716","externalIds":{"DBLP":"journals/tgrs/ChenLCZLZS24","ArXiv":"2306.16269","DOI":"10.1109/TGRS.2024.3356074","CorpusId":259274887},"title":"RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation Based on Visual Foundation Model"},{"paperId":"c306dbb87b9afaa60e1d68c1d20cb21a9c8c01d5","externalIds":{"ArXiv":"2306.04225","DBLP":"journals/corr/abs-2306-04225","DOI":"10.48550/arXiv.2306.04225","CorpusId":259096162},"title":"Efficient Vision Transformer for Human Pose Estimation via Patch Selection"},{"paperId":"9a83aeadc8db65fb6da39ec977360541cddaff5c","externalIds":{"DBLP":"journals/corr/abs-2305-07027","ArXiv":"2305.07027","DOI":"10.1109/CVPR52729.2023.01386","CorpusId":258615318},"title":"EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention"},{"paperId":"c57467e652f3f9131b3e7e40c23059abe395f01d","externalIds":{"DBLP":"conf/wacv/PatroNA25","ArXiv":"2304.06446","DOI":"10.1109/WACV61041.2025.00924","CorpusId":258107943},"title":"SpectFormer: Frequency and Attention is what you need in a Vision Transformer"},{"paperId":"53e5db85e2a7442f20670be2ae25019fcf9d27a2","externalIds":{"DBLP":"conf/cvpr/PanYXSH23","ArXiv":"2304.04237","DOI":"10.1109/CVPR52729.2023.00207","CorpusId":258048654},"title":"Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention"},{"paperId":"e658f8a80f3d859629a0eb0d338e4bf8d6d6c493","externalIds":{"ArXiv":"2303.17605","DBLP":"conf/cvpr/ChenLTYZH23","DOI":"10.1109/CVPR52729.2023.00205","CorpusId":257833511},"title":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer"},{"paperId":"af21d7b29a8b48967d0151a0b86f15d755eba02b","externalIds":{"DBLP":"journals/pami/ZhangZXT24","ArXiv":"2303.15105","DOI":"10.1109/TPAMI.2023.3347693","CorpusId":257767201,"PubMed":"38190690"},"title":"Vision Transformer With Quadrangle Attention"},{"paperId":"3375b5693331c37b899d78115d91215b8b5716e9","externalIds":{"DBLP":"conf/iccv/Shaker0R00K23","ArXiv":"2303.15446","DOI":"10.1109/ICCV51070.2023.01598","CorpusId":257766532},"title":"SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications"},{"paperId":"52cc542b60c99e8866a7fc515c10f443089005f2","externalIds":{"ArXiv":"2303.14189","DBLP":"conf/iccv/VasuGZTR23","DOI":"10.1109/ICCV51070.2023.00532","CorpusId":257756882},"title":"FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization"},{"paperId":"19921cefb2470b2f5d984ab9ce92ebb94aedf2ea","externalIds":{"ArXiv":"2303.13755","DBLP":"conf/cvpr/WeiDJATS23","DOI":"10.1109/CVPR52729.2023.02172","CorpusId":257756933},"title":"Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers"},{"paperId":"643359e68b899a431ff9ccbe68c2e6d3b3624002","externalIds":{"DBLP":"conf/iccv/Lai0023","ArXiv":"2303.09040","DOI":"10.1109/ICCV51070.2023.01201","CorpusId":260704626},"title":"Hybrid Spectral Denoising Transformer with Guided Attention"},{"paperId":"2f4d8f3c016ec53380b376ae7ac516f9c0f07a0d","externalIds":{"DBLP":"conf/cvpr/ZhuWKZL23","ArXiv":"2303.08810","DOI":"10.1109/CVPR52729.2023.00995","CorpusId":257532403},"title":"BiFormer: Vision Transformer with Bi-Level Routing Attention"},{"paperId":"2d4b96178897b39891354908b80dced466071522","externalIds":{"DBLP":"conf/ipmi/ZhouLBHSP23","ArXiv":"2303.06522","DOI":"10.48550/arXiv.2303.06522","CorpusId":257496813,"PubMed":"38680428"},"title":"Token Sparsification for Faster Medical Image Segmentation"},{"paperId":"61e721334296ebfbbf6443b5ed9eb8c83b708c95","externalIds":{"DBLP":"conf/icml/0001DMPHGSCGAJB23","ArXiv":"2302.05442","DOI":"10.48550/arXiv.2302.05442","CorpusId":256808367},"title":"Scaling Vision Transformers to 22 Billion Parameters"},{"paperId":"0cd526723b87ae37981922992992d203448a2014","externalIds":{"DBLP":"journals/corr/abs-2302-01791","ArXiv":"2302.01791","DOI":"10.1109/TMM.2023.3243616","CorpusId":256598233},"title":"DilateFormer: Multi-Scale Dilated Transformer for Visual Recognition"},{"paperId":"8b0357f1bceb9cf7a5629b0ba3acb5660edf90b2","externalIds":{"DBLP":"journals/corr/abs-2301-03505","ArXiv":"2301.03505","DOI":"10.48550/arXiv.2301.03505","CorpusId":255545907,"PubMed":"37883822"},"title":"Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review"},{"paperId":"938414e8b3927ffe8c93c5b3d0bf5177d7e84447","externalIds":{"DBLP":"journals/tmi/HuangDLYF23","DOI":"10.1109/TMI.2022.3230943","CorpusId":254962911,"PubMed":"37015444"},"title":"MISSFormer: An Effective Transformer for 2D Medical Image Segmentation"},{"paperId":"750676b67abef11d102f0a5e7e221bbb56fca2c8","externalIds":{"DBLP":"journals/corr/abs-2212-04497","ArXiv":"2212.04497","DOI":"10.1109/TMI.2024.3398728","CorpusId":254408962,"PubMed":"38722726"},"title":"UNETR++: Delving Into Efficient and Accurate 3D Medical Image Segmentation"},{"paperId":"262ce9a1f9203a103edfc3a7bb88c419982af99e","externalIds":{"DBLP":"conf/iccv/ZhangHW23","ArXiv":"2211.07198","DOI":"10.1109/ICCV51070.2023.00557","CorpusId":257631954},"title":"Fcaformer: Forward Cross Attention in Hybrid Vision Transformer"},{"paperId":"46f7aa7bccd627e0ad43f76e47318cc0c5887f75","externalIds":{"DBLP":"journals/corr/abs-2210-07124","ArXiv":"2210.07124","DOI":"10.48550/arXiv.2210.07124","CorpusId":252873647},"title":"RTFormer: Efficient Design for Real-Time Semantic Segmentation with Transformer"},{"paperId":"ffd91f85d6d19e75309ececfd190afa6bb562284","externalIds":{"DBLP":"conf/iclr/ZhouZHWY0023","ArXiv":"2209.15425","DOI":"10.48550/arXiv.2209.15425","CorpusId":252668422},"title":"Spikformer: When Spiking Neural Network Meets Transformer"},{"paperId":"ec139916edd6feb9b3cb3a0325ca96e21dbb0147","externalIds":{"DBLP":"journals/corr/abs-2209-07484","ArXiv":"2209.07484","DOI":"10.48550/arXiv.2209.07484","CorpusId":252284084},"title":"Hydra Attention: Efficient Attention with Many Heads"},{"paperId":"88b7f8ab3933ee4775eaa200029755e0022f8870","externalIds":{"DBLP":"journals/corr/abs-2208-14657","ArXiv":"2208.14657","DOI":"10.48550/arXiv.2208.14657","CorpusId":251953659},"title":"EViT: Privacy-Preserving Image Retrieval via Encrypted Vision Transformer in Cloud Computing"},{"paperId":"3e448df5aa191f7a3945d0fd609c8bc5966a2333","externalIds":{"ArXiv":"2207.14284","DBLP":"journals/corr/abs-2207-14284","DOI":"10.48550/arXiv.2207.14284","CorpusId":251134855},"title":"HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"},{"paperId":"3de7e0c4bb5648a8368135592e11e991305ac426","externalIds":{"ArXiv":"2207.08518","DBLP":"journals/corr/abs-2207-08518","DOI":"10.1109/WACV56688.2023.00614","CorpusId":250626973},"title":"HiFormer: Hierarchical Multi-scale Representations Using Transformers for Medical Image Segmentation"},{"paperId":"05b7bd47fa5cbe10497c49004b57eb5ab4fdd0b4","externalIds":{"ArXiv":"2206.10589","DBLP":"journals/corr/abs-2206-10589","DOI":"10.48550/arXiv.2206.10589","CorpusId":249890419},"title":"EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications"},{"paperId":"bf6ce546c589fa8054b3972b266532664914bd21","externalIds":{"DBLP":"journals/corr/abs-2205-13213","ArXiv":"2205.13213","DOI":"10.48550/arXiv.2205.13213","CorpusId":249097732},"title":"Fast Vision Transformers with HiLo Attention"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"24de23963bec39fe0e39612e2cacb76c83d66f93","externalIds":{"DBLP":"conf/cvpr/SongYCY22","ArXiv":"2205.03806","DOI":"10.1109/CVPR52688.2022.00859","CorpusId":248572246},"title":"Transformer Tracking with Cyclic Shifting Window Attention"},{"paperId":"c5e6f0c52c1f91086879f46120efa79e96158eba","externalIds":{"ArXiv":"2205.02833","DBLP":"journals/corr/abs-2205-02833","DOI":"10.1109/CVPR52688.2022.01339","CorpusId":248525044},"title":"Cross-view Transformers for real-time Map-view Semantic Segmentation"},{"paperId":"38085be7f53f1cdf5e6399fa170648f30182028b","externalIds":{"DBLP":"journals/remotesensing/SunZGWOWZ22","DOI":"10.3390/rs14091968","CorpusId":248316000},"title":"Fusing Spatial Attention with Spectral-Channel Attention Mechanism for Hyperspectral Image Classification via Encoder-Decoder Networks"},{"paperId":"7ccca665e5438c9e2ce8c94edfd8dd24f3ef2137","externalIds":{"DBLP":"journals/corr/abs-2204-08913","ArXiv":"2204.08913","DOI":"10.1109/CVPRW56347.2022.00107","CorpusId":248239759},"title":"Self-Calibrated Efficient Transformer for Lightweight Super-Resolution"},{"paperId":"ba609e5c83ad9b32723e547b9d3c89d97373f755","externalIds":{"DBLP":"conf/eccv/ZhangXZT22","ArXiv":"2204.08446","DOI":"10.48550/arXiv.2204.08446","CorpusId":248227756},"title":"VSA: Learning Varied-Size Window Attention in Vision Transformers"},{"paperId":"5a71bf38cf409b55b14b2d5159c0b06bef9ad603","externalIds":{"DBLP":"journals/corr/abs-2203-14263","ArXiv":"2203.14263","DOI":"10.1109/TKDE.2021.3126456","CorpusId":243973878},"title":"A General Survey on Attention Mechanisms in Deep Learning"},{"paperId":"859893aadb0d30d38b6f856392056188c18d0c78","externalIds":{"ArXiv":"2203.14186","DBLP":"conf/cvpr/GengLDZ22","DOI":"10.1109/CVPR52688.2022.01692","CorpusId":247762014},"title":"RSTT: Real-time Spatial Temporal Transformer for Space-Time Video Super-Resolution"},{"paperId":"fa717a2e31f0cef4e26921f3b147a98644d2e64c","externalIds":{"DBLP":"journals/corr/abs-2203-11926","ArXiv":"2203.11926","DOI":"10.48550/arXiv.2203.11926","CorpusId":247596882},"title":"Focal Modulation Networks"},{"paperId":"71b646590bbc45f2d79bc9ca1251926beca02e7c","externalIds":{"DBLP":"journals/corr/abs-2203-06697","ArXiv":"2203.06697","DOI":"10.48550/arXiv.2203.06697","CorpusId":247447177},"title":"Efficient Long-Range Attention Network for Image Super-resolution"},{"paperId":"879de0f5fba4c37dfff157672ed23d4e4e6c5fbc","externalIds":{"DBLP":"conf/ijcai/LiuWLG22","ArXiv":"2203.03937","DOI":"10.48550/arXiv.2203.03937","CorpusId":247315103},"title":"Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention"},{"paperId":"9dc481ec44178e797466bbad968071917842156b","externalIds":{"DBLP":"journals/corr/abs-2203-03605","ArXiv":"2203.03605","DOI":"10.48550/arXiv.2203.03605","CorpusId":247292561},"title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"},{"paperId":"ba637c4f1a170f1e2dadeadb71a63cf2b9a46de2","externalIds":{"DBLP":"journals/cvm/GuoLLCH23","ArXiv":"2202.09741","DOI":"10.1007/s41095-023-0364-2","CorpusId":247011300},"title":"Visual attention network"},{"paperId":"577a350ade92578913245e2d474aeabcb576e6d6","externalIds":{"DBLP":"conf/icml/AliSEMMW22","ArXiv":"2202.07304","CorpusId":246863594},"title":"XAI for Transformers: Better Explanations through Conservative Propagation"},{"paperId":"934942934a6a785e2a80daa6421fa79971558b89","externalIds":{"ArXiv":"2202.06268","DBLP":"journals/tnn/LiCLDZN24","DOI":"10.1109/TNNLS.2023.3264730","CorpusId":246822612,"PubMed":"37126636"},"title":"BViT: Broad Attention-Based Vision Transformer"},{"paperId":"b52844a746dafd8a5051cef49abbbda64a312605","externalIds":{"ArXiv":"2201.10801","DBLP":"conf/aaai/WangZTLZ22","DOI":"10.1609/aaai.v36i2.20142","CorpusId":246285566},"title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism"},{"paperId":"3e906906d475c73b6d8ce24ac5ebdac9979fd01b","externalIds":{"DBLP":"journals/pami/SelvaJENMC23","ArXiv":"2201.05991","DOI":"10.1109/TPAMI.2023.3243465","CorpusId":246015436,"PubMed":"37022830"},"title":"Video Transformers: A Survey"},{"paperId":"c2df109689819d6f456c771874208840c98a7a9f","externalIds":{"ArXiv":"2201.01615","DBLP":"journals/corr/abs-2201-01615","CorpusId":245704446},"title":"Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention"},{"paperId":"e5cb26148791b57bfd36aa26ce2401e231d01b57","externalIds":{"DBLP":"journals/corr/abs-2201-00520","ArXiv":"2201.00520","DOI":"10.1109/CVPR52688.2022.00475","CorpusId":245650206},"title":"Vision Transformer with Deformable Attention"},{"paperId":"2a4024163826151303aa0bbb18320b8a67167794","externalIds":{"DBLP":"journals/corr/abs-2112-14000","ArXiv":"2112.14000","DOI":"10.1609/aaai.v36i3.20176","CorpusId":245537158},"title":"Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention"},{"paperId":"72e81bc41ffae1d414836169107910025aaacb75","externalIds":{"DBLP":"journals/corr/abs-2112-10809","ArXiv":"2112.10809","DOI":"10.1109/CVPR52688.2022.01169","CorpusId":245353696},"title":"Lite Vision Transformer with Enhanced Self-Attention"},{"paperId":"5553f9508dd1056ecc20c5b1f367e9a07e2c7e81","externalIds":{"DBLP":"journals/corr/abs-2112-10762","ArXiv":"2112.10762","DOI":"10.1109/CVPR52688.2022.01102","CorpusId":245334475},"title":"StyleSwin: Transformer-based GAN for High-resolution Image Generation"},{"paperId":"87e6f235c7a1fdeceb41605db64419fa11f7b98b","externalIds":{"ArXiv":"2112.05425","DBLP":"journals/corr/abs-2112-05425","CorpusId":245117808},"title":"Couplformer: Rethinking Vision Transformer with Coupling Attention Map"},{"paperId":"658a017302d29e4acf4ca789cb5d9f27983717ff","externalIds":{"DBLP":"conf/cvpr/ChengMSKG22","ArXiv":"2112.01527","DOI":"10.1109/CVPR52688.2022.00135","CorpusId":244799297},"title":"Masked-attention Mask Transformer for Universal Image Segmentation"},{"paperId":"336e06e34eac2eeda8b34d95d545d8ff4dd1b2f9","externalIds":{"DBLP":"conf/eccv/MaazR0KA022","ArXiv":"2111.11430","DOI":"10.1007/978-3-031-20080-9_30","CorpusId":248830840},"title":"Class-Agnostic Object Detection with Multi-modal Transformer"},{"paperId":"45f686be3b96302ede327645227134e1c304dbab","externalIds":{"ArXiv":"2111.07624","DBLP":"journals/cvm/GuoXLLJMZMCH22","DOI":"10.1007/s41095-022-0271-y","CorpusId":244117862},"title":"Attention mechanisms in computer vision: A survey"},{"paperId":"5e2180e4ce9d218cccb1c78a93a863d5f967d907","externalIds":{"DBLP":"journals/cvm/XuWLDSZTDHX22","DOI":"10.1007/s41095-021-0247-3","CorpusId":240006659},"title":"Transformers in computational visual media: A survey"},{"paperId":"da74a10824193be9d3889ce0d6ed4c6f8ee48b9e","externalIds":{"DBLP":"conf/iclr/MehtaR22","ArXiv":"2110.02178","CorpusId":238354201},"title":"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"},{"paperId":"bd9ab344da99022cbbbfd3f5c9c82a0b21c60ad9","externalIds":{"ArXiv":"2109.03201","DBLP":"journals/tip/ZhouGZHYWY23","DOI":"10.1109/TIP.2023.3293771","CorpusId":237431181,"PubMed":"37440404"},"title":"nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer"},{"paperId":"1cd6b0f41d62aca38ba5a69db10e79c05e618c21","externalIds":{"ArXiv":"2108.06152","DBLP":"conf/iccv/MengCFZLYS021","DOI":"10.1109/ICCV48922.2021.00363","CorpusId":237048324},"title":"Conditional DETR for Fast Training Convergence"},{"paperId":"2c4d5b1278125d84c9e66ebe1032af888d9211f3","externalIds":{"DBLP":"journals/corr/abs-2108-02432","ArXiv":"2108.02432","DOI":"10.1145/3474085.3475272","CorpusId":236924300},"title":"Token Shift Transformer for Video Classification"},{"paperId":"d045133e6e022684329ff944d67f91888be1bc3b","externalIds":{"DBLP":"journals/corr/abs-2108-01390","ArXiv":"2108.01390","DOI":"10.1609/aaai.v36i3.20202","CorpusId":236881638},"title":"Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer"},{"paperId":"a5c41f188b0eb0acb444cb4899bf6af378ee9ede","externalIds":{"ArXiv":"2108.00154","DBLP":"conf/iclr/0001YCL00L22","CorpusId":238531695},"title":"CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"},{"paperId":"faa30dfcb1531df4e4d5c219bad06d65f6c860fa","externalIds":{"DBLP":"conf/prcv/XuZHW23","ArXiv":"2107.08623","DOI":"10.1007/978-981-99-8543-2_4","CorpusId":236087532},"title":"LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation"},{"paperId":"800cfb3d23115cdcd4d114234b65bbdf2080f798","externalIds":{"DBLP":"conf/cvpr/DongBCZYYCG22","ArXiv":"2107.00652","DOI":"10.1109/CVPR52688.2022.01181","CorpusId":235694312},"title":"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"},{"paperId":"9b6af0e358e76d22f209c75b1702c3e6ea7815b1","externalIds":{"DBLP":"conf/nips/RaoZZLZ21","ArXiv":"2107.00645","CorpusId":235694359},"title":"Global Filter Networks for Image Classification"},{"paperId":"f1c3cd27f46ab21589c6f2b2cc4179f0d1b9ef2d","externalIds":{"DBLP":"journals/corr/abs-2107-00229","ArXiv":"2107.00229","DOI":"10.1007/978-3-030-87202-1_40","CorpusId":235694653},"title":"E-DSSR: Efficient Dynamic Surgical Scene Reconstruction with Transformer-based Stereoscopic Depth Perception"},{"paperId":"7b664a306b7d2f68dd816ea1d6586cf3472d75c1","externalIds":{"DBLP":"journals/corr/abs-2106-14881","ArXiv":"2106.14881","CorpusId":235658393},"title":"Early Convolutions Help Transformers See Better"},{"paperId":"67040b931c1a384426c44ae73f9553e97f08cf6a","externalIds":{"ArXiv":"2106.13797","DBLP":"journals/cvm/WangXLFSLLLS22","DOI":"10.1007/s41095-022-0274-8","CorpusId":235652212},"title":"PVT v2: Improved baselines with Pyramid Vision Transformer"},{"paperId":"ba1b51e872cdf7744a50b1b2e76ee8b85a0d0dfd","externalIds":{"DBLP":"journals/pami/WuLZC23","ArXiv":"2106.12011","DOI":"10.1109/TPAMI.2022.3202765","CorpusId":235606084,"PubMed":"36040936"},"title":"P2T: Pyramid Pooling Transformer for Scene Understanding"},{"paperId":"7fff8018bf625447df837c2fda5c58a705fbc038","externalIds":{"DBLP":"journals/corr/abs-2106-09681","ArXiv":"2106.09681","CorpusId":235458262},"title":"XCiT: Cross-Covariance Image Transformers"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","externalIds":{"DBLP":"conf/nips/DaiLLT21","ArXiv":"2106.04803","CorpusId":235376986},"title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"6b6ffb94626e672caffafc77097491d9ee7a8682","externalIds":{"DBLP":"conf/iclr/HanFDSC0022","ArXiv":"2106.04263","CorpusId":247450510},"title":"On the Connection between Local Attention and Dynamic Depth-wise Convolution"},{"paperId":"2e8149dafb864ec3675087c99bf5572fcf4eb170","externalIds":{"ArXiv":"2106.02689","DBLP":"journals/corr/abs-2106-02689","CorpusId":235359074},"title":"RegionViT: Regional-to-Local Attention for Vision Transformers"},{"paperId":"dbdcabd0444ad50b68ee09e30f39b66e9068f5d2","externalIds":{"DBLP":"conf/nips/RaoZLLZH21","ArXiv":"2106.02034","CorpusId":235313562},"title":"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"},{"paperId":"07e987364bf0be1949e379f976f8dea675977337","externalIds":{"DBLP":"conf/cvpr/FangXW00022","ArXiv":"2105.15168","DOI":"10.1109/CVPR52688.2022.01175","CorpusId":235254672},"title":"MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens"},{"paperId":"e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60","externalIds":{"DBLP":"conf/nips/XieWYAAL21","ArXiv":"2105.15203","CorpusId":235254713},"title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"},{"paperId":"ea7cfe7f2340584cbe653da6077ee7c213e49b92","externalIds":{"ArXiv":"2105.05537","DBLP":"journals/corr/abs-2105-05537","DOI":"10.1007/978-3-031-25066-8_9","CorpusId":234469981},"title":"Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation"},{"paperId":"cc9f3a61ea4eaabf43cbb30cd1dd718074932679","externalIds":{"ArXiv":"2104.10858","DBLP":"conf/nips/JiangHYZSJWF21","CorpusId":235377078},"title":"All Tokens Matter: Token Labeling for Training Better Vision Transformers"},{"paperId":"18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6","externalIds":{"ArXiv":"2104.11227","DBLP":"conf/iccv/0001XMLYMF21","DOI":"10.1109/ICCV48922.2021.00675","CorpusId":233346705},"title":"Multiscale Vision Transformers"},{"paperId":"97e2eb4d8ddccca8f3c4835ee89ac4617ee52e6a","externalIds":{"DBLP":"journals/tip/ZhouCVR21","ArXiv":"2104.06231","DOI":"10.1109/TIP.2021.3070752","CorpusId":233192832,"PubMed":"33830924"},"title":"Latent Correlation Representation Learning for Brain Tumor Segmentation With Missing MRI Modalities"},{"paperId":"a56bf7ee9a56d8f84079684339a953c2df9ce76b","externalIds":{"MAG":"3146366485","DBLP":"journals/ijon/NiuZY21","DOI":"10.1016/J.NEUCOM.2021.03.091","CorpusId":233562906},"title":"A review on the attention mechanism of deep learning"},{"paperId":"40f4d7fe800810288a80f84cdb357a8f4c28e880","externalIds":{"DBLP":"conf/iccv/HeoYHCCO21","ArXiv":"2103.16302","DOI":"10.1109/ICCV48922.2021.01172","CorpusId":232417451},"title":"Rethinking Spatial Dimensions of Vision Transformers"},{"paperId":"e775e649d815a02373eac840cf5e33a04ff85c95","externalIds":{"ArXiv":"2103.15808","DBLP":"conf/iccv/WuXCLDY021","DOI":"10.1109/ICCV48922.2021.00009","CorpusId":232417787},"title":"CvT: Introducing Convolutions to Vision Transformers"},{"paperId":"b6382a7351c0c595f91472ac71d3b2d87b3c4844","externalIds":{"DBLP":"journals/corr/abs-2103-15691","ArXiv":"2103.15691","DOI":"10.1109/ICCV48922.2021.00676","CorpusId":232417054},"title":"ViViT: A Video Vision Transformer"},{"paperId":"0eff37167876356da2163b2e396df2719adf7de9","externalIds":{"DBLP":"journals/corr/abs-2103-14899","ArXiv":"2103.14899","DOI":"10.1109/ICCV48922.2021.00041","CorpusId":232404237},"title":"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"},{"paperId":"96da196d6f8c947db03d13759f030642f8234abf","externalIds":{"ArXiv":"2103.11886","DBLP":"journals/corr/abs-2103-11886","CorpusId":232307695},"title":"DeepViT: Towards Deeper Vision Transformer"},{"paperId":"70cf7c785952375e8061c92235aa20e94b02ecd4","externalIds":{"ArXiv":"2103.02907","DBLP":"journals/corr/abs-2103-02907","DOI":"10.1109/CVPR46437.2021.01350","CorpusId":232110359},"title":"Coordinate Attention for Efficient Mobile Network Design"},{"paperId":"8fb1c04dab87ca6c116495e4d03c46c9547e4ec3","externalIds":{"DBLP":"journals/corr/abs-2102-12122","ArXiv":"2102.12122","DOI":"10.1109/ICCV48922.2021.00061","CorpusId":232035922},"title":"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"},{"paperId":"367f7f64ded5d18528c1013db9dfa01b075db484","externalIds":{"DBLP":"journals/corr/abs-2102-10662","ArXiv":"2102.10662","DOI":"10.1007/978-3-030-87193-2_4","CorpusId":231986084},"title":"Medical Transformer: Gated Axial-Attention for Medical Image Segmentation"},{"paperId":"24b8a0b02bcb7934967757fc59d273a71ba67e30","externalIds":{"ArXiv":"2102.04306","DBLP":"journals/corr/abs-2102-04306","CorpusId":231847326},"title":"TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation"},{"paperId":"dbe077f8521ecbe0a1477d6148c726d4f053d9c9","externalIds":{"DBLP":"journals/corr/abs-2101-11986","ArXiv":"2101.11986","DOI":"10.1109/ICCV48922.2021.00060","CorpusId":231719476},"title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"},{"paperId":"9afcacef80e4961227439eedf5a3f06e5319b9b7","externalIds":{"DBLP":"conf/aaai/HuangKJLH22","ArXiv":"2101.07434","DOI":"10.1609/aaai.v36i1.19985","CorpusId":237497373},"title":"Channelized Axial Attention - considering Channel Relation within Spatial Attention for Semantic Segmentation"},{"paperId":"3a906b77fa218adc171fecb28bb81c24c14dcc7b","externalIds":{"DBLP":"journals/corr/abs-2101-01169","ArXiv":"2101.01169","DOI":"10.1145/3505244","CorpusId":230435805},"title":"Transformers in Vision: A Survey"},{"paperId":"d40c77c010c8dbef6142903a02f2a73a85012d5d","externalIds":{"ArXiv":"2012.12556","DBLP":"journals/corr/abs-2012-12556","DOI":"10.1109/TPAMI.2022.3152247","CorpusId":236986986,"PubMed":"35180075"},"title":"A Survey on Vision Transformer"},{"paperId":"ad7ddcc14984caae308c397f1a589aae75d4ab71","externalIds":{"ArXiv":"2012.12877","DBLP":"journals/corr/abs-2012-12877","CorpusId":229363322},"title":"Training data-efficient image transformers & distillation through attention"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"39ca8f8ff28cc640e3b41a6bd7814ab85c586504","externalIds":{"DBLP":"journals/corr/abs-2010-04159","MAG":"3092462694","ArXiv":"2010.04159","CorpusId":222208633},"title":"Deformable DETR: Deformable Transformers for End-to-End Object Detection"},{"paperId":"33e738b6ca2a6e0c5020a3ea87c1ebd2a3c7497a","externalIds":{"DBLP":"conf/wacv/MisraNAH21","ArXiv":"2010.03045","MAG":"3092210344","DOI":"10.1109/WACV48630.2021.00318","CorpusId":222177028},"title":"Rotate to Attend: Convolutional Triplet Attention Module"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","externalIds":{"MAG":"3040573126","DBLP":"conf/iclr/LepikhinLXCFHKS21","ArXiv":"2006.16668","CorpusId":220265858},"title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"d920e0710d2a1ae966661d0513b817b6b81dc2b2","externalIds":{"DBLP":"conf/cvpr/LiuHCWF20","MAG":"3034752215","DOI":"10.1109/cvpr42600.2020.01011","CorpusId":219617264},"title":"Improving Convolutional Networks With Self-Calibrated Convolutions"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","externalIds":{"MAG":"3096609285","DBLP":"conf/eccv/CarionMSUKZ20","ArXiv":"2005.12872","DOI":"10.1007/978-3-030-58452-8_13","CorpusId":218889832},"title":"End-to-End Object Detection with Transformers"},{"paperId":"7f31f46193b0b02e19ecdb8e20c9221659ea79b2","externalIds":{"ArXiv":"2005.06803","DBLP":"journals/corr/abs-2005-06803","MAG":"3025409017","DOI":"10.1109/ICCV48922.2021.01345","CorpusId":218630285},"title":"TAM: Temporal Adaptive Module for Video Recognition"},{"paperId":"76a9f336481b39515d6cea2920696f11fb686451","externalIds":{"ACL":"2020.acl-main.385","ArXiv":"2005.00928","DBLP":"journals/corr/abs-2005-00928","MAG":"3022265721","DOI":"10.18653/v1/2020.acl-main.385","CorpusId":218487351},"title":"Quantifying Attention Flow in Transformers"},{"paperId":"416fead21c20b452fbf1f105ee5b4b5646c75a42","externalIds":{"MAG":"3034502973","DBLP":"conf/cvpr/HouZCF20","ArXiv":"2003.13328","DOI":"10.1109/cvpr42600.2020.00406","CorpusId":214713957},"title":"Strip Pooling: Rethinking Spatial Pooling for Scene Parsing"},{"paperId":"26051e0070a59444eefe5bed36627703235405b7","externalIds":{"DOI":"10.1007/1-4020-0613-6_16833","CorpusId":240640357},"title":"Segment"},{"paperId":"173d31ea3ec9db19870fcf2710841b4a3e864c6d","externalIds":{"MAG":"2986451890","DBLP":"journals/corr/abs-1908-10049","DOI":"10.1109/ICCV.2019.00406","CorpusId":201646268},"title":"Global-Local Temporal Representations for Video Person Re-Identification"},{"paperId":"3e70bbe6c4cd98d66599db709e32b748f184a2d4","externalIds":{"MAG":"2982101047","DBLP":"conf/nips/YangBLN19","ArXiv":"1904.04971","CorpusId":202775981},"title":"CondConv: Conditionally Parameterized Convolutions for Efficient Inference"},{"paperId":"8043113812dae0f95f68677ba7ce986321401dd6","externalIds":{"DBLP":"conf/cvpr/ZhangLZJ020","MAG":"3010766832","ArXiv":"1904.02998","DOI":"10.1109/CVPR42600.2020.00325","CorpusId":216639682},"title":"Relation-Aware Global Attention for Person Re-Identification"},{"paperId":"c41a11c0e9b8b92b4faaf97749841170b760760a","externalIds":{"DBLP":"journals/corr/abs-1904-01766","ArXiv":"1904.01766","MAG":"2981851019","DOI":"10.1109/ICCV.2019.00756","CorpusId":102483628},"title":"VideoBERT: A Joint Model for Video and Language Representation Learning"},{"paperId":"7fa4c034518dc272f4d234faa4eff7e1461d4822","externalIds":{"DBLP":"journals/ijon/LiuG19","MAG":"2914767245","DOI":"10.1016/J.NEUCOM.2019.01.078","CorpusId":127325665},"title":"Bidirectional LSTM with attention mechanism and convolutional layer for text classification"},{"paperId":"480687d65e1173eccbd4193f7a2d4469d7a33b32","externalIds":{"MAG":"2910058009","DBLP":"journals/cogcom/LiYXWL19","DOI":"10.1007/s12559-019-9624-y","CorpusId":58020060},"title":"Improving User Attribute Classification with Text and Social Network Attention"},{"paperId":"5f4a22ee70ca613d9c0630eafc96364fe365fdf8","externalIds":{"MAG":"2991526275","ArXiv":"1812.01243","DBLP":"conf/wacv/ShenZZY021","DOI":"10.1109/WACV48630.2021.00357","CorpusId":215999966},"title":"Efficient Attention: Attention with Linear Complexities"},{"paperId":"cd8ddaaf56e38dddafdeac3f9643b9b5e9d35d54","externalIds":{"MAG":"2963984455","DBLP":"conf/nips/HuSASV18","ArXiv":"1810.12348","CorpusId":53116301},"title":"Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks"},{"paperId":"ad655c25e052fa4eeed53421344aca6f239c4c9d","externalIds":{"MAG":"2892219791","DBLP":"journals/corr/abs-1809-02983","ArXiv":"1809.02983","DOI":"10.1109/CVPR.2019.00326","CorpusId":52180375},"title":"Dual Attention Network for Scene Segmentation"},{"paperId":"e9021af7af143d84eec304d59fc6e1009214e592","externalIds":{"ArXiv":"1808.08127","MAG":"2951282455","DBLP":"journals/tmi/RoyNW19","DOI":"10.1109/TMI.2018.2867261","CorpusId":52091306,"PubMed":"30716024"},"title":"Recalibrating Fully Convolutional Networks With Spatial and Channel “Squeeze and Excitation” Blocks"},{"paperId":"de95601d9e3b20ec51aa33e1f27b1880d2c44ef2","externalIds":{"MAG":"2948744165","ArXiv":"1807.06521","DBLP":"conf/eccv/WooPLK18","DOI":"10.1007/978-3-030-01234-2_1","CorpusId":49867180},"title":"CBAM: Convolutional Block Attention Module"},{"paperId":"10bb4ef7a6719ea132e00f0ab5680919a4131d99","externalIds":{"DBLP":"conf/bmvc/ParkWLK18","MAG":"2883579164","ArXiv":"1807.06514","CorpusId":49864419},"title":"BAM: Bottleneck Attention Module"},{"paperId":"136c96810238657bf0c6f0d4b56b0e40e24f3c47","externalIds":{"MAG":"2923378908","DBLP":"conf/iclr/LinsleySES19","CorpusId":108323160},"title":"Learning what and where to attend"},{"paperId":"c52ac453e154953abdb06fc041023e327ea609a4","externalIds":{"DBLP":"journals/corr/abs-1803-09519","ArXiv":"1803.09519","MAG":"2964089206","DOI":"10.21437/Interspeech.2018-1910","CorpusId":4427800},"title":"Self-Attentional Acoustic Models"},{"paperId":"8899094797e82c5c185a0893896320ef77f60e64","externalIds":{"DBLP":"conf/cvpr/0004GGH18","MAG":"2963091558","ArXiv":"1711.07971","DOI":"10.1109/CVPR.2018.00813","CorpusId":4852647},"title":"Non-local Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"4550a4c714920ef57d19878e31c9ebae37b049b2","externalIds":{"MAG":"2952564229","DBLP":"journals/corr/BritzGLL17","ACL":"D17-1151","ArXiv":"1703.03906","DOI":"10.18653/v1/D17-1151","CorpusId":2201909},"title":"Massive Exploration of Neural Machine Translation Architectures"},{"paperId":"88513e738a95840de05a62f0e43d30a67b3c542e","externalIds":{"MAG":"2550553598","ArXiv":"1611.05594","DBLP":"conf/cvpr/ChenZXNSLC17","DOI":"10.1109/CVPR.2017.667","CorpusId":206596371},"title":"SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning"},{"paperId":"de5e7320729f5d3cbb6709eb6329ec41ace8c95d","externalIds":{"ArXiv":"1606.08415","MAG":"2899663614","CorpusId":125617073},"title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b","externalIds":{"MAG":"2963668159","ArXiv":"1606.00061","DBLP":"journals/corr/LuYBP16","CorpusId":868693},"title":"Hierarchical Question-Image Co-Attention for Visual Question Answering"},{"paperId":"d7192b47b259a249f055cd448f6381088601354d","externalIds":{"DBLP":"conf/icann/BinderMLMS16","MAG":"2950394196","ArXiv":"1604.00825","DOI":"10.1007/978-3-319-44781-0_8","CorpusId":15207861},"title":"Layer-Wise Relevance Propagation for Neural Networks with Local Renormalization Layers"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","externalIds":{"MAG":"1902237438","DBLP":"journals/corr/LuongPM15","ArXiv":"1508.04025","ACL":"D15-1166","DOI":"10.18653/v1/D15-1166","CorpusId":1998416},"title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"3056add22b20e3361c38c0472d294a79d4031cb4","externalIds":{"DBLP":"conf/icassp/ChanJLV16","MAG":"2327501763","ArXiv":"1508.01211","DOI":"10.1109/ICASSP.2016.7472621","CorpusId":18165915},"title":"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"},{"paperId":"b92aa7024b87f50737b372e5df31ef091ab54e62","externalIds":{"MAG":"2950621961","DBLP":"journals/corr/SrivastavaGS15a","ArXiv":"1507.06228","CorpusId":2722012},"title":"Training Very Deep Networks"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","externalIds":{"MAG":"2953106684","ArXiv":"1506.01497","DBLP":"journals/pami/RenHG017","DOI":"10.1109/TPAMI.2016.2577031","CorpusId":10328909,"PubMed":"27295650"},"title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","externalIds":{"MAG":"2950178297","DBLP":"conf/icml/XuBKCCSZB15","ArXiv":"1502.03044","CorpusId":1055111},"title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"ac3ee98020251797c2b401e1389461df88e52e62","externalIds":{"MAG":"1924770834","DBLP":"journals/corr/ChungGCB14","ArXiv":"1412.3555","CorpusId":5201925},"title":"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"},{"paperId":"47d2dc34e1d02a8109f5c04bb6939725de23716d","externalIds":{"DBLP":"journals/corr/ChorowskiBCB14","ArXiv":"1412.1602","MAG":"1586532344","CorpusId":453615},"title":"End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","externalIds":{"MAG":"2130942839","DBLP":"conf/nips/SutskeverVL14","ArXiv":"1409.3215","CorpusId":7961699},"title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"8a756d4d25511d92a45d0f4545fa819de993851d","externalIds":{"DBLP":"conf/nips/MnihHGK14","MAG":"2147527908","ArXiv":"1406.6247","CorpusId":17195923},"title":"Recurrent Models of Visual Attention"},{"paperId":"0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3","externalIds":{"DBLP":"conf/nips/LarochelleH10","MAG":"2141399712","CorpusId":9634512},"title":"Learning to combine foveal glimpses with a third-order Boltzmann machine"},{"paperId":"f1eb9f77b6baf1eef537940bd428826a44702944","externalIds":{"MAG":"2112284637","DOI":"10.1037/0096-3445.136.2.217","CorpusId":9361006,"PubMed":"17500647"},"title":"Focusing the spotlight: individual differences in visual attention control."},{"paperId":"29842c1f1d487f0f3b9d8d2afebaff7c2f51f73c","externalIds":{"MAG":"2128272608","DBLP":"journals/pami/IttiKN98","DOI":"10.1109/34.730558","CorpusId":3108956},"title":"A Model of Saliency-Based Visual Attention for Rapid Scene Analysis"},{"paperId":"37b187e3df04fe7dd31293222407b4b86f3089fb","externalIds":{"DOI":"10.1016/S1364-6613(97)82741-6","CorpusId":208785104},"title":"Attention!"},{"paperId":"1d70fed85b2bf8eaac0878b88c3934d62578cb8a","externalIds":{"MAG":"2796668645","DOI":"10.2307/3608620","CorpusId":116601104},"title":"Statistical Theory of Extreme Values and Some Practical Applications"},{"paperId":"c3dd958431836616baf4adda22ecb1507f7d5e98","externalIds":{"DBLP":"journals/tmm/MaJSZWHJ23","DOI":"10.1109/TMM.2022.3164787","CorpusId":247993615},"title":"Knowing What it is: Semantic-Enhanced Dual Attention Transformer"},{"paperId":"585c16a343fe301d28d5bf779162f24a7f3aa0bd","externalIds":{"DBLP":"journals/tgrs/LvWZDZ22","DOI":"10.1109/tgrs.2022.3157671","CorpusId":247331648},"title":"SCViT: A Spatial-Channel Feature Preserving Vision Transformer for Remote Sensing Image Scene Classification"},{"paperId":"b503f607c8e73a117888e0d5f658c6855a11c319","externalIds":{"DBLP":"conf/icml/KimNK22","CorpusId":250340619},"title":"ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","externalIds":{"ArXiv":"2103.14030","DBLP":"conf/iccv/LiuL00W0LG21","DOI":"10.1109/ICCV48922.2021.00986","CorpusId":232352874},"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"1728d2eddd60941bb059b6d6c8f3e6bfc2d15e39","externalIds":{"DBLP":"conf/nips/ChenTLXWQL21","CorpusId":244894878},"title":"Speech-T: Transducer for Text to Speech and Beyond"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"602993051a5e6f1d18c1317ce7fe061f48a1f2c7","externalIds":{"MAG":"2888381511","DOI":"10.1007/978-3-319-73004-2_6","CorpusId":3719231},"title":"Convolutional Neural Networks"}]}