{"abstract":"Machine Learning or Artiﬁcial Intelligence algorithms have gained considerable scrutiny in recent times owing to their propensity towards imitating and amplifying existing prejudices in society. This has led to a niche but growing body of work that identiﬁes and attempts to ﬁx these biases. A ﬁrst step towards making these algorithms more fair is designing metrics that measure unfairness. Most existing work in this ﬁeld deals with either a binary view of fairness (protected vs. unprotected groups) or politically deﬁned categories (race or gender). Such categorization misses the important nuance of intersectionality - biases can often be ampliﬁed in subgroups that combine membership from diﬀerent categories, especially if such a subgroup is particularly underrepresented in historical platforms of opportunity. In this paper, we discuss why fairness metrics need to be looked at under the lens of intersectionality, identify existing work in intersectional fairness, suggest a simple worst case comparison method to expand the deﬁnitions of existing group fairness metrics to incorporate intersectionality, and ﬁnally conclude with the social, legal and political framework to handle intersectional fairness in the modern context."}