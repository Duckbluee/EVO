{"paperId":"105669ec59a58fb2d4dd3021a984af33c227c5ab","externalIds":{"DBLP":"journals/sigkdd/ChenMLJWWWYFLT23","ArXiv":"2307.03393","DOI":"10.1145/3655103.3655110","CorpusId":259375824},"title":"Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs","openAccessPdf":{"url":"http://arxiv.org/pdf/2307.03393","status":"GREEN","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03393, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2109393101","name":"Zhikai Chen"},{"authorId":"2125202063","name":"Haitao Mao"},{"authorId":"2145571830","name":"Hang Li"},{"authorId":"144767914","name":"Wei Jin"},{"authorId":"30580446","name":"Haifang Wen"},{"authorId":"7621447","name":"Xiaochi Wei"},{"authorId":"2386396","name":"Shuaiqiang Wang"},{"authorId":"2136400100","name":"Dawei Yin"},{"authorId":"41031455","name":"Wenqi Fan"},{"authorId":"2146672392","name":"Hui Liu"},{"authorId":"1736632","name":"Jiliang Tang"}],"abstract":"Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at: https://github.com/CurryTang/Graph-LLM ."}