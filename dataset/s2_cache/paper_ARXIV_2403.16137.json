{"paperId":"f67f718288c545164704a3e253e165be5a6da59f","externalIds":{"DBLP":"journals/tkde/ZhaoSLZLZ25","ArXiv":"2403.16137","DOI":"10.1109/TKDE.2025.3568147","CorpusId":268681022},"title":"A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2209282615","name":"Zi-qiang Zhao"},{"authorId":"2313910549","name":"Yixin Su"},{"authorId":"2260170121","name":"Yuhua Li"},{"authorId":"2275592358","name":"Yixiong Zou"},{"authorId":"2283487404","name":"Ruixuan Li"},{"authorId":"2293586635","name":"Rui Zhang"}],"abstract":"The field of graph foundation models (GFMs) has seen a dramatic rise in interest in recent years. Their powerful generalization ability is believed to be endowed by self-supervised pre-training and downstream tuning techniques. There is a wide variety of knowledge patterns embedded in the graph data, such as node properties and clusters, which are crucial for learning generalized representations for GFMs. We present a comprehensive survey of self-supervised GFMs from a novel knowledge-based perspective. Our main contribution is a knowledge-based taxonomy that categorizes self-supervised graph models by the specific graph knowledge utilized: microscopic (nodes, links, etc.), mesoscopic (context, clusters, etc.), and macroscopic (global structure, manifolds, etc.). It covers a total of 9 knowledge categories and 300 references for self-supervised pre-training as well as various downstream tuning strategies. Such a knowledge-based taxonomy allows us to more clearly re-examine potential GFM architectures, including large language models (LLMs), as well as provide deeper insights for constructing future GFMs."}