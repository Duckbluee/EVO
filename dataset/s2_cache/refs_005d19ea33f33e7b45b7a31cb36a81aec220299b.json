{"references":[{"paperId":"d0deaec3e1f74701ac43600d9e64c5c969be7391","externalIds":{"ArXiv":"2404.11912","DBLP":"journals/corr/abs-2404-11912","DOI":"10.48550/arXiv.2404.11912","CorpusId":269214125},"title":"TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding"},{"paperId":"f4fff56d412ab1319daf07cda14b6e2becf86d6c","externalIds":{"ArXiv":"2403.10444","DBLP":"conf/iclr/SunMLARBS25","CorpusId":268510560},"title":"Block Verification Accelerates Speculative Decoding"},{"paperId":"26128d9ec47f044de6f0186f86c0c7fb3ccef77f","externalIds":{"ArXiv":"2402.11809","DBLP":"journals/corr/abs-2402-11809","DOI":"10.48550/arXiv.2402.11809","CorpusId":267750845},"title":"Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding"},{"paperId":"57e7af0b69325fafb371ef5d502e39ef9c90ef7e","externalIds":{"ArXiv":"2401.10774","DBLP":"journals/corr/abs-2401-10774","DOI":"10.48550/arXiv.2401.10774","CorpusId":267061277},"title":"Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"},{"paperId":"13261129251c9e8891cff02c3aee15c4df6a5630","externalIds":{"DBLP":"journals/corr/abs-2312-15234","ArXiv":"2312.15234","DOI":"10.1145/3754448","CorpusId":266551872},"title":"Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"},{"paperId":"27b1b41ac386814af6f7bfd4646781f3f32b8a38","externalIds":{"DBLP":"conf/kdd/ZhaoXLZG24","ArXiv":"2312.12728","DOI":"10.1145/3637528.3671614","CorpusId":266374986},"title":"Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy"},{"paperId":"f4db96c6fe2fcd83b50d54d585cbc558704e3022","externalIds":{"DBLP":"conf/aaai/ZengHDZC24","ArXiv":"2312.11882","DOI":"10.48550/arXiv.2312.11882","CorpusId":266362672},"title":"ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models Inference"},{"paperId":"e30666ed82670463aa47686e744f0c6f2a0e083d","externalIds":{"ArXiv":"2312.11462","DBLP":"journals/corr/abs-2312-11462","DOI":"10.48550/arXiv.2312.11462","CorpusId":266359077},"title":"Cascade Speculative Drafting for Even Faster LLM Inference"},{"paperId":"89d3d19ad717cd534bdd1866d28e2da253147705","externalIds":{"DBLP":"journals/corr/abs-2312-04916","ArXiv":"2312.04916","DOI":"10.48550/arXiv.2312.04916","CorpusId":266149909},"title":"EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism"},{"paperId":"bc1c1b0c4214907e2fc872f54a8e66b327d6aa6b","externalIds":{"DBLP":"journals/corr/abs-2310-18813","ArXiv":"2310.18813","DOI":"10.48550/arXiv.2310.18813","CorpusId":264590298},"title":"The Synergy of Speculative Decoding and Batching in Serving Large Language Models"},{"paperId":"f206d34afed6a5705757f96ea97c7bfb9e1a83cd","externalIds":{"ArXiv":"2310.12072","DBLP":"journals/corr/abs-2310-12072","DOI":"10.48550/arXiv.2310.12072","CorpusId":264289340},"title":"SPEED: Speculative Pipelined Execution for Efficient Decoding"},{"paperId":"56767c18bb5aaa2b6377624168bed1b6dcc4b94d","externalIds":{"DBLP":"conf/iclr/ZhouLRMRKKA24","ArXiv":"2310.08461","DOI":"10.48550/arXiv.2310.08461","CorpusId":263909387},"title":"DistillSpec: Improving Speculative Decoding via Knowledge Distillation"},{"paperId":"564855d475ed9197dd7516594557ff886ff623e5","externalIds":{"ArXiv":"2310.05424","DBLP":"journals/corr/abs-2310-05424","DOI":"10.48550/arXiv.2310.05424","CorpusId":263830054},"title":"Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"},{"paperId":"fdc53c2c10742464087c0525f77e32604827a21d","externalIds":{"DBLP":"conf/iclr/XiaoTCHL24","ArXiv":"2309.17453","DOI":"10.48550/arXiv.2309.17453","CorpusId":263310483},"title":"Efficient Streaming Language Models with Attention Sinks"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"a9caf21a845cb0b1b1d453c052188de118006093","externalIds":{"DBLP":"journals/corr/abs-2308-16369","ArXiv":"2308.16369","DOI":"10.48550/arXiv.2308.16369","CorpusId":261395577},"title":"SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"},{"paperId":"43e624ddeed82df944a6cae0dedec3372438e243","externalIds":{"DBLP":"journals/corr/abs-2308-04623","ArXiv":"2308.04623","DOI":"10.48550/arXiv.2308.04623","CorpusId":260735640},"title":"Accelerating LLM Inference with Staged Speculative Decoding"},{"paperId":"d988eb48e8b4e471f5df9d081bfc32db0781e6bf","externalIds":{"DBLP":"journals/tmlr/YangLCP024","ArXiv":"2307.05908","DOI":"10.48550/arXiv.2307.05908","CorpusId":259837553},"title":"Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding"},{"paperId":"ce9435c82dc9b576f2037aa2f4357a520be9b2aa","externalIds":{"DBLP":"journals/corr/abs-2307-02628","ArXiv":"2307.02628","DOI":"10.48550/arXiv.2307.02628","CorpusId":259360560},"title":"SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference"},{"paperId":"a57ef1f5c3af185af79751855b8033b7fc6d89b3","externalIds":{"ArXiv":"2306.13649","DBLP":"conf/iclr/AgarwalVZSGGB24","CorpusId":263610088},"title":"On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200","externalIds":{"DBLP":"journals/corr/abs-2305-13245","ArXiv":"2305.13245","DOI":"10.48550/arXiv.2305.13245","CorpusId":258833177},"title":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"},{"paperId":"3556722b4703a21abafd2f9388743202943f4503","externalIds":{"ACL":"2023.acl-long.689","DBLP":"journals/corr/abs-2305-10427","ArXiv":"2305.10427","DOI":"10.18653/v1/2023.acl-long.689","CorpusId":258741236},"title":"Accelerating Transformer Inference for Translation via Parallel Decoding"},{"paperId":"eb3415db1b322b1f7bf95f8697aed701b0d40f88","externalIds":{"DBLP":"journals/corr/abs-2304-04487","ArXiv":"2304.04487","DOI":"10.48550/arXiv.2304.04487","CorpusId":258048436},"title":"Inference with Reference: Lossless Acceleration of Large Language Models"},{"paperId":"c6ff0f377d4a8700aff52312e240a32ab12f3316","externalIds":{"DBLP":"conf/ijcai/HuZLT23","ArXiv":"2303.09266","DOI":"10.48550/arXiv.2303.09266","CorpusId":257557293},"title":"SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference"},{"paperId":"b7d12aec8a0152ec4921dfa43ab525a63b334385","externalIds":{"DBLP":"conf/nips/KimMMMMGK23","ArXiv":"2302.07863","CorpusId":256868484},"title":"Speculative Decoding with Big Little Decoder"},{"paperId":"a1f8082505c7e90b0a033e1b9da0a97d67aad66c","externalIds":{"DBLP":"journals/corr/abs-2302-01318","ArXiv":"2302.01318","DOI":"10.48550/arXiv.2302.01318","CorpusId":256503945},"title":"Accelerating Large Language Model Decoding with Speculative Sampling"},{"paperId":"d8e9f8c8a37cb4cd26b92ad0d942d641cd512644","externalIds":{"DBLP":"journals/corr/abs-2211-17192","ArXiv":"2211.17192","DOI":"10.48550/arXiv.2211.17192","CorpusId":254096365},"title":"Fast Inference from Transformers via Speculative Decoding"},{"paperId":"d270288a707bdea971e3d7a1ce0267b0f4379711","externalIds":{"DBLP":"conf/kdd/MangrulkarSS22","DOI":"10.1145/3534678.3539132","CorpusId":250392089},"title":"BE3R: BERT based Early-Exit Using Expert Routing"},{"paperId":"218c5c69f3cf0c158e9b6af239a2cc62a688c6de","externalIds":{"ArXiv":"2203.16487","DBLP":"conf/emnlp/Xia0WCWS23","DOI":"10.18653/v1/2023.findings-emnlp.257","CorpusId":264724056},"title":"Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation"},{"paperId":"cf36236015c9f93f15bfafbf282f69e08bdc9c16","externalIds":{"DBLP":"conf/emnlp/GevaCWG22","ArXiv":"2203.14680","ACL":"2022.emnlp-main.3","DOI":"10.48550/arXiv.2203.14680","CorpusId":247762385},"title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"},{"paperId":"d05141dc0900140f7146bb71e1f7402cf896ea87","externalIds":{"DBLP":"conf/acl/SunLZGWHNXHQ22","ACL":"2022.findings-acl.189","ArXiv":"2203.01670","DOI":"10.48550/arXiv.2203.01670","CorpusId":247222685},"title":"A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation"},{"paperId":"da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed","externalIds":{"DBLP":"conf/nips/JaszczurCMKGMK21","ArXiv":"2111.12763","CorpusId":244709266},"title":"Sparse is Enough in Scaling Transformers"},{"paperId":"84310f76cf8909f87c6a7f2ed30ae28214cc9eab","externalIds":{"MAG":"3176017841","ACL":"2021.acl-long.231","DBLP":"conf/acl/Zhu20","DOI":"10.18653/v1/2021.acl-long.231","CorpusId":236459809},"title":"LeeBERT: Learned Early Exit for BERT with cross-level optimization"},{"paperId":"c3ea8eb80bc8ca0b21efa273b9e4a9fd059c65be","externalIds":{"ArXiv":"2107.07511","DBLP":"journals/corr/abs-2107-07511","CorpusId":235899036},"title":"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification"},{"paperId":"3e8f037d1b2c893f4df37deda1da3dcc577a8bac","externalIds":{"DBLP":"conf/eacl/SuCWVBLC21","MAG":"3154504973","ArXiv":"2102.08220","ACL":"2021.eacl-main.18","DOI":"10.17863/CAM.69916","CorpusId":231933701},"title":"Non-Autoregressive Text Generation with Pre-trained Language Models"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","externalIds":{"ArXiv":"2007.14062","DBLP":"journals/corr/abs-2007-14062","MAG":"3045733172","CorpusId":220831004},"title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8","externalIds":{"MAG":"3101163004","DBLP":"conf/nips/ZhouXGM0W20","ArXiv":"2006.04152","CorpusId":219531455},"title":"BERT Loses Patience: Fast and Robust Inference with Early Exit"},{"paperId":"8747f028acccde9ee7c35c858da8091613d3e574","externalIds":{"DBLP":"conf/aaai/LiuMZCX21","MAG":"3112673818","DOI":"10.1609/aaai.v35i15.17584","CorpusId":229283620},"title":"Faster Depth-Adaptive Transformers"},{"paperId":"90a1491ac32e732c93773354e4e665794ed4d490","externalIds":{"MAG":"3035038672","DBLP":"journals/corr/abs-2004-12993","ArXiv":"2004.12993","ACL":"2020.acl-main.204","DOI":"10.18653/v1/2020.acl-main.204","CorpusId":216552850},"title":"DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"},{"paperId":"5d34881ff68bd203ff790187e7e5c9e034389cfa","externalIds":{"MAG":"3014568172","DBLP":"journals/corr/abs-2004-02178","ArXiv":"2004.02178","ACL":"2020.acl-main.537","DOI":"10.18653/v1/2020.acl-main.537","CorpusId":214802887},"title":"FastBERT: a Self-distilling BERT with Adaptive Inference Time"},{"paperId":"1bebcbfb3b8309c9cea44de52662a911f99bc65b","externalIds":{"MAG":"2996987694","DBLP":"conf/aaai/GuoTXQCL20","ArXiv":"1911.08717","DOI":"10.1609/AAAI.V34I05.6289","CorpusId":208175668},"title":"Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"fe2f254923c72958fb65e025adf38ec6403dd6f8","externalIds":{"MAG":"2981648103","DBLP":"journals/corr/abs-1910-11555","ArXiv":"1910.11555","CorpusId":204916079},"title":"Fast Structured Decoding for Sequence Models"},{"paperId":"4585611042d2be0d997ee135e3fe219d668db9ec","externalIds":{"MAG":"2981757109","DBLP":"conf/iclr/ElbayadGGA20","ArXiv":"1910.10073","CorpusId":204824061},"title":"Depth-Adaptive Transformer"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","externalIds":{"DBLP":"journals/corr/abs-1910-01108","ArXiv":"1910.01108","MAG":"2978017171","CorpusId":203626972},"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"0cbf97173391b0430140117027edcaf1a37968c7","externalIds":{"MAG":"3105966348","ACL":"2020.findings-emnlp.372","DBLP":"conf/emnlp/JiaoYSJCL0L20","ArXiv":"1909.10351","DOI":"10.18653/v1/2020.findings-emnlp.372","CorpusId":202719327},"title":"TinyBERT: Distilling BERT for Natural Language Understanding"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","externalIds":{"MAG":"2973727699","ArXiv":"1909.08053","DBLP":"journals/corr/abs-1909-08053","CorpusId":202660670},"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"75fe6c3ffdea2608794b4f21119c5a4dec07663a","externalIds":{"MAG":"2972034672","ArXiv":"1909.02480","DBLP":"conf/emnlp/MaZLNH19","ACL":"D19-1437","DOI":"10.18653/v1/D19-1437","CorpusId":202539063},"title":"FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow"},{"paperId":"d6e21619df572d04b2b2d97b4c5d1fd604f185fb","externalIds":{"MAG":"2948197522","ArXiv":"1906.02780","DBLP":"journals/corr/abs-1906-02780","ACL":"P19-1122","DOI":"10.18653/v1/P19-1122","CorpusId":174801260},"title":"Syntactically Supervised Transformers for Faster Neural Machine Translation"},{"paperId":"5efadc9019ce3378a0eb6c8f939cdde6c8918b1e","externalIds":{"MAG":"2988975212","DBLP":"conf/emnlp/GhazvininejadLL19","ACL":"D19-1633","DOI":"10.18653/v1/D19-1633","CorpusId":202538740},"title":"Mask-Predict: Parallel Decoding of Conditional Masked Language Models"},{"paperId":"26384278cf5d575fc32cb92c303fb648fa0d5217","externalIds":{"ArXiv":"1902.09574","MAG":"2915589364","DBLP":"journals/corr/abs-1902-09574","CorpusId":67855585},"title":"The State of Sparsity in Deep Neural Networks"},{"paperId":"9d9c4a3b27dab2293ed915ad69662ea09901ccab","externalIds":{"DBLP":"journals/corr/abs-1812-09664","MAG":"2962969034","ArXiv":"1812.09664","DOI":"10.1609/aaai.v33i01.33013723","CorpusId":56895270},"title":"Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input"},{"paperId":"5e04881e91bff952d102d967c4ffb498ec30d4af","externalIds":{"DBLP":"journals/corr/abs-1811-03115","MAG":"2890152612","ArXiv":"1811.03115","CorpusId":53208380},"title":"Blockwise Parallel Decoding for Deep Autoregressive Models"},{"paperId":"6a1c61a0da5f56a3fdfca5515767cfd74529524a","externalIds":{"DBLP":"conf/emnlp/WangZC18","ACL":"D18-1044","MAG":"2890501761","ArXiv":"1808.08583","DOI":"10.18653/v1/D18-1044","CorpusId":52100894},"title":"Semi-Autoregressive Neural Machine Translation"},{"paperId":"29de7c0fb3c09eaf55b20619bceaeafe72fd87a6","externalIds":{"MAG":"2963096510","DBLP":"conf/acl/LewisDF18","ArXiv":"1805.04833","ACL":"P18-1082","DOI":"10.18653/v1/P18-1082","CorpusId":44134226},"title":"Hierarchical Neural Story Generation"},{"paperId":"2d08ed53491053d84b6de89aedbf2178b9c8cf84","externalIds":{"DBLP":"journals/corr/abs-1803-03382","MAG":"2789543585","ArXiv":"1803.03382","CorpusId":4720016},"title":"Fast Decoding in Sequence Models using Discrete Latent Variables"},{"paperId":"9c5c89199114858eafbe50b46d77d38ffd03b28a","externalIds":{"DBLP":"journals/corr/abs-1802-06901","ArXiv":"1802.06901","ACL":"D18-1149","MAG":"2953322184","DOI":"10.18653/v1/D18-1149","CorpusId":3438497},"title":"Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement"},{"paperId":"15e81c8d1c21f9e928c72721ac46d458f3341454","externalIds":{"DBLP":"journals/corr/abs-1711-02281","MAG":"2767206889","ArXiv":"1711.02281","CorpusId":3480671},"title":"Non-Autoregressive Neural Machine Translation"},{"paperId":"896de8418884f4aab1ae4a60027500c9e8baffc3","externalIds":{"MAG":"2610140147","DBLP":"conf/icpr/Teerapittayanon16","ArXiv":"1709.01686","DOI":"10.1109/ICPR.2016.7900006","CorpusId":2916466},"title":"BranchyNet: Fast inference via early exiting from deep neural networks"},{"paperId":"d2e4147eecae6f914e9e1e9aece8fdd2eaed809f","externalIds":{"ArXiv":"1609.07061","MAG":"2524428287","DBLP":"journals/corr/HubaraCSEB16","CorpusId":15817277},"title":"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"},{"paperId":"f0085dc1fe376ef240f233002b8ce57c2cfe0106","externalIds":{"MAG":"2353655624","DBLP":"journals/corr/Cho16","ArXiv":"1605.03835","CorpusId":14830778},"title":"Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","externalIds":{"DBLP":"journals/corr/Graves16","ArXiv":"1603.08983","MAG":"2325237720","CorpusId":8224916},"title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"0f899b92b7fb03b609fee887e4b6f3b633eaf30d","externalIds":{"MAG":"299440670","ArXiv":"1505.05770","DBLP":"journals/corr/RezendeM15","CorpusId":12554042},"title":"Variational Inference with Normalizing Flows"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"ef098e0154da4b210a6ee11b84ca30bd3e445ac6","externalIds":{"DOI":"10.1137/1.9780898719468","CorpusId":39585209},"title":"Iterative solution of nonlinear equations in several variables"},{"paperId":"8de174ab5419b9d3127695405efd079808e956e8","externalIds":{"MAG":"2296073425","DBLP":"conf/icml/BengioLCW09","DOI":"10.1145/1553374.1553380","CorpusId":873046},"title":"Curriculum learning"},{"paperId":"f4ba954b0412773d047dc41231c733de0c1f4926","externalIds":{"DBLP":"conf/icml/LaffertyMP01","MAG":"2147880316","CorpusId":219683473},"title":"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"},{"paperId":"daa63f57c3fbe994c4356f8d986a22e696e776d2","externalIds":{"DBLP":"journals/jgo/JonesSW98","MAG":"1510052597","DOI":"10.1023/A:1008306431147","CorpusId":263864014},"title":"Efficient Global Optimization of Expensive Black-Box Functions"},{"paperId":"ab7b5917515c460b90451e67852171a531671ab8","externalIds":{"MAG":"2006969979","DBLP":"journals/coling/BrownPPM94","ACL":"J93-2003","CorpusId":13259913},"title":"The Mathematics of Statistical Machine Translation: Parameter Estimation"},{"paperId":"944adc818b89a543282f9be3a356df67a09faccd","externalIds":{"MAG":"2158992088","DBLP":"journals/tc/Burton85","DOI":"10.1109/TC.1985.6312218","CorpusId":27560619},"title":"Speculative computation, parallelism, and functional programming"},{"paperId":"bccb4ff16f52113a93cde7025a82f581695beb19","externalIds":{"DBLP":"books/daglib/0028244","MAG":"1555915743","CorpusId":60693966},"title":"Computer Architecture - A Quantitative Approach, 5th Edition"},{"paperId":"3d473cbb7a377cf960abff31748a1a39bb6c7d7c","externalIds":{"DBLP":"journals/corr/abs-2307-15337","DOI":"10.48550/arXiv.2307.15337","CorpusId":260315904},"title":"Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"},{"paperId":"d8d0672e2cb73dcd25e6eee5fdf11aa1cbabecf0","externalIds":{"DBLP":"journals/corr/abs-2203-16266","DOI":"10.48550/arXiv.2203.16266","CorpusId":247793982},"title":"Non-autoregressive Translation with Dependency-Aware Decoder"},{"paperId":"4a984ec8286b19bb0c033e6e4df198a0421b0c17","externalIds":{"ACL":"2022.coling-1.414","DBLP":"conf/coling/Kong0YZ22","CorpusId":252818912},"title":"Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting"},{"paperId":"7b37c0a4976c4d2a5a440d494fbb0f3daede2a00","externalIds":{"ACL":"2021.eacl-main.8","DBLP":"conf/eacl/XinTYL21","DOI":"10.18653/v1/2021.eacl-main.8","CorpusId":233189542},"title":"BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce","externalIds":{"MAG":"2525491769","DOI":"10.1201/b18055-8","CorpusId":208945385},"title":"google,我,萨娜"}]}