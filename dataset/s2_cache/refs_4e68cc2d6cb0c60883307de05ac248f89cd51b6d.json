{"references":[{"paperId":"38687b782053e029ce3773fabed85b9fdeea93ca","externalIds":{"DBLP":"journals/network/ZhuJHG24","DOI":"10.1109/MNET.2024.3368138","CorpusId":267963974},"title":"When In-Network Computing Meets Distributed Machine Learning"},{"paperId":"15d37231657883a1e7022e1e84b5188a94800eca","externalIds":{"DBLP":"conf/sigcomm/QianXCGXGFSZMWW24","DOI":"10.1145/3651890.3672265","CorpusId":271601659},"title":"Alibaba HPN: A Data Center Network for Large Language Model Training"},{"paperId":"fca1ad2e38d3c6784b3eab0629a8e62df8698e3a","externalIds":{"DBLP":"journals/tpds/LiLLLGL24","DOI":"10.1109/TPDS.2024.3406420","CorpusId":270111615},"title":"A Multidimensional Communication Scheduling Method for Hybrid Parallel DNN Training"},{"paperId":"5d1ca53a0b41f4f5c960cd9997556d7180d6b88d","externalIds":{"ArXiv":"2407.08608","DBLP":"journals/corr/abs-2407-08608","DOI":"10.48550/arXiv.2407.08608","CorpusId":271098045},"title":"FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision"},{"paperId":"b149bcbe4bf29cfd1c89543a5d04f020619c88e8","externalIds":{"ArXiv":"2407.05467","DBLP":"journals/corr/abs-2407-05467","DOI":"10.48550/arXiv.2407.05467","CorpusId":271050367},"title":"The infrastructure powering IBM's Gen AI model development"},{"paperId":"9ae763b09322055bb13cbb659140ddc88159b74f","externalIds":{"ArXiv":"2407.00550","CorpusId":270870540},"title":"Ethereal: Divide and Conquer Network Load Balancing in Large-Scale Distributed Training"},{"paperId":"a79c67e5ecddc025ccf705c7b7a40e8844f83a6a","externalIds":{"ArXiv":"2406.18485","DBLP":"journals/corr/abs-2406-18485","DOI":"10.48550/arXiv.2406.18485","CorpusId":270738084},"title":"LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism"},{"paperId":"9b1ef16bb9c7a8e8ce952e724cdaa0478f4b5681","externalIds":{"DBLP":"journals/corr/abs-2406-17145","ArXiv":"2406.17145","DOI":"10.1145/3669940.3707220","CorpusId":270711123},"title":"GraphPipe: Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism"},{"paperId":"e7c9478b9dab56b6113a85d1c53723eb5d09e58f","externalIds":{"DBLP":"conf/mlsys/MeiFLWZ025","ArXiv":"2406.14088","CorpusId":270619751},"title":"ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation"},{"paperId":"758a107293bd962b8363dde2f8dd149f0c2ddfa9","externalIds":{"DBLP":"conf/iwmm/Imanishi024","DOI":"10.1145/3652024.3665508","CorpusId":270617734},"title":"A Heuristic for Periodic Memory Allocation with Little Fragmentation to Train Neural Networks"},{"paperId":"08f8e417faf7a02f2d42c853bfedc21938e2aad2","externalIds":{"ArXiv":"2406.13768","DBLP":"journals/corr/abs-2406-13768","DOI":"10.48550/arXiv.2406.13768","CorpusId":270620561},"title":"FastPersist: Accelerating Model Checkpointing in Deep Learning"},{"paperId":"8082e08349f789fb5fe1e45caede6da118c00b37","externalIds":{"DBLP":"journals/corr/abs-2406-08756","ArXiv":"2406.08756","DOI":"10.48550/arXiv.2406.08756","CorpusId":270440209},"title":"Optimizing Large Model Training through Overlapped Activation Recomputation"},{"paperId":"5d4b80d7efe7a4345ec60dca9ac04d02beb9f1cd","externalIds":{"ArXiv":"2406.04594","DBLP":"conf/hpca/DongLZZFZLCSJLG25","DOI":"10.1109/HPCA61900.2025.00095","CorpusId":270357799},"title":"Enhancing Large-Scale AI Training Efficiency: The C4 Solution for Real-Time Anomaly Detection and Communication Optimization"},{"paperId":"e6dbc9545be9b0cda0925862d355a177511d3b11","externalIds":{"DBLP":"journals/corr/abs-2406-03488","ArXiv":"2406.03488","DOI":"10.48550/arXiv.2406.03488","CorpusId":270258420},"title":"Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training"},{"paperId":"cba8438b8cf288ff7d36fe2439037a3c887a0a3a","externalIds":{"DBLP":"conf/hpdc/MauryaURCN24","ArXiv":"2406.10707","DOI":"10.1145/3625549.3658685","CorpusId":270560603},"title":"DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models"},{"paperId":"773597e8acb8c9ee2a4603e0bd40bb74b7fba871","externalIds":{"DBLP":"journals/tpds/ZhangXWYHZC24","DOI":"10.1109/TPDS.2024.3385639","CorpusId":269024443},"title":"MPMoE: Memory Efficient MoE for Pre-Trained Models With Adaptive Pipeline Parallelism"},{"paperId":"dedcfd974eb7a8ab3d02b42561e4448883f0215a","externalIds":{"DBLP":"conf/podc/JacobsT0ZASRH24","DOI":"10.1109/IPDPSW63119.2024.00208","CorpusId":270287130},"title":"System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"},{"paperId":"dc2108f1fc59c51179a5aaa11a826437a8214f5c","externalIds":{"ArXiv":"2405.15362","DBLP":"conf/nips/QiWAL24","DOI":"10.48550/arXiv.2405.15362","CorpusId":270045100},"title":"Pipeline Parallelism with Controllable Memory"},{"paperId":"d111c5055bbbd50c7669350c386a581ee1d39b2e","externalIds":{"DBLP":"conf/sosp/GandhiZSK24","ArXiv":"2405.14009","DOI":"10.1145/3694715.3695960","CorpusId":269982911},"title":"ReCycle: Resilient Training of Large DNNs using Pipeline Adaptation"},{"paperId":"a941c7948377201abceecc9bcecb7fad09becb9f","externalIds":{"DBLP":"journals/ijautcomp/SunZHLCLYSTZZCZZLZZLYWY24","DOI":"10.1007/s11633-024-1502-8","CorpusId":269947217},"title":"MOSS: An Open Conversational Large Language Model"},{"paperId":"28f59093730d88719b96041f9544c73671f798bd","externalIds":{"DBLP":"journals/corr/abs-2405-07719","ArXiv":"2405.07719","DOI":"10.48550/arXiv.2405.07719","CorpusId":269757781},"title":"USP: A Unified Sequence Parallelism Approach for Long Context Generative AI"},{"paperId":"53a803388e83ae89261624099d7be4287ace67cb","externalIds":{"ArXiv":"2405.04434","DBLP":"journals/corr/abs-2405-04434","DOI":"10.48550/arXiv.2405.04434","CorpusId":269613809},"title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"paperId":"a509670659e8b054e2b7d1b6f8a0bc722398fa62","externalIds":{"DBLP":"conf/asplos/AnselYHGJVBBBBC24","DOI":"10.1145/3620665.3640366","CorpusId":268794728},"title":"PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation"},{"paperId":"3bffaa1da8778405673d174b8bbf28f83d5916d1","externalIds":{"DBLP":"conf/asplos/ChenLZDSZY24","DOI":"10.1145/3620666.3651379","CorpusId":269363779},"title":"Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning"},{"paperId":"2d6bd2c05c05691c8f30eef54e01eab750295f19","externalIds":{"DBLP":"conf/asplos/SunCWF0WC24","DOI":"10.1145/3620666.3651359","CorpusId":269364682},"title":"AdaPipe: Optimizing Pipeline Parallelism with Adaptive Recomputation and Partitioning"},{"paperId":"83858f08aef77ced91428207b8891a82e7087cd2","externalIds":{"DBLP":"conf/eurosys/ShiPWLRHYLC24","DOI":"10.1145/3627703.3650083","CorpusId":269243508},"title":"ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling"},{"paperId":"e28e266c675acbf131c18af5f6f438a4a1d576a2","externalIds":{"DBLP":"conf/eurosys/GuptaKKVGKRS24","DOI":"10.1145/3627703.3650085","CorpusId":269263721},"title":"Just-In-Time Checkpointing: Low Cost Error Recovery from Deep Learning Training Failures"},{"paperId":"bbe2e5cbcb7abaed960708f10fe1c160663b7293","externalIds":{"DBLP":"conf/eurosys/LiuMLSMYBW24","DOI":"10.1145/3627703.3629554","CorpusId":269255886},"title":"Aceso: Efficient Parallel DNN Training through Iterative Bottleneck Alleviation"},{"paperId":"5fb9124b0e817986e8687c322214a828a71b79ce","externalIds":{"DBLP":"conf/euromlsys/StratiEKK24","DOI":"10.1145/3642970.3655843","CorpusId":269257605},"title":"ML Training with Cloud GPU Shortages: Is Cross-Region the Answer?"},{"paperId":"3dcaf781cde9d8fcf9e5870db6baeeeac1a5c303","externalIds":{"DOI":"10.1126/science.adl1203","CorpusId":269043430,"PubMed":"38603505"},"title":"Large-scale photonic chiplet Taichi empowers 160-TOPS/W artificial general intelligence"},{"paperId":"75b2ae5ee35611ecfbd3dc2c3d0799cfb4fd98e4","externalIds":{"DBLP":"journals/corr/abs-2403-17297","ArXiv":"2403.17297","DOI":"10.48550/arXiv.2403.17297","CorpusId":268691939},"title":"InternLM2 Technical Report"},{"paperId":"8046d516d395df434ff2b0c65c15f55e8e85dfe5","externalIds":{"DBLP":"journals/corr/abs-2403-16125","ArXiv":"2403.16125","DOI":"10.48550/arXiv.2403.16125","CorpusId":268681513},"title":"A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters"},{"paperId":"916b4926cda574dc3f9486bb9994b6f2788dd800","externalIds":{"DBLP":"journals/corr/abs-2403-14608","ArXiv":"2403.14608","DOI":"10.48550/arXiv.2403.14608","CorpusId":268553763},"title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey"},{"paperId":"01f6de9e8670b613f4eccf0a699acb45673c19c5","externalIds":{"DBLP":"journals/corr/abs-2403-14097","ArXiv":"2403.14097","DOI":"10.48550/arXiv.2403.14097","CorpusId":268553966},"title":"Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances"},{"paperId":"388df4f4d6577c92a43a9a578ebed844a1b4cc17","externalIds":{"DBLP":"journals/corr/abs-2403-12422","ArXiv":"2403.12422","DOI":"10.48550/arXiv.2403.12422","CorpusId":268532328},"title":"Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization"},{"paperId":"ad65a34460b135a7aaa29f87d58d85a24789f008","externalIds":{"DBLP":"journals/corr/abs-2403-10266","ArXiv":"2403.10266","DOI":"10.48550/arXiv.2403.10266","CorpusId":268510227},"title":"DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers"},{"paperId":"cf93c04b73d50ba097151ee3e2a9f47fda4a8525","externalIds":{"ArXiv":"2403.09347","DBLP":"journals/corr/abs-2403-09347","DOI":"10.48550/arXiv.2403.09347","CorpusId":268384915},"title":"BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences"},{"paperId":"d8599a04f738a42f5d7a2c9bc598c2c7b74669c5","externalIds":{"ArXiv":"2403.08245","DBLP":"journals/corr/abs-2403-08245","DOI":"10.48550/arXiv.2403.08245","CorpusId":268379441},"title":"Scattered Mixture-of-Experts Implementation"},{"paperId":"fb0207353fe923efd26a4fedafde56cd8eda1173","externalIds":{"DBLP":"journals/corr/abs-2403-07648","ArXiv":"2403.07648","DOI":"10.48550/arXiv.2403.07648","CorpusId":268363846},"title":"Characterization of Large Language Model Development in the Datacenter"},{"paperId":"c5d038cfad52a2aa18d072455a2c87003f77029b","externalIds":{"ArXiv":"2403.06504","CorpusId":268357075},"title":"LoHan: Low-Cost High-Performance Framework to Fine-Tune 100B Model on a Consumer GPU"},{"paperId":"1113d9329463048a5b969583a8dd228027264cae","externalIds":{"DBLP":"journals/tjs/WuZCLZLA24","DOI":"10.1007/s11227-024-05890-8","CorpusId":268437153},"title":"SWattention: designing fast and memory-efficient attention for a new Sunway Supercomputer"},{"paperId":"0390f8b6c0b2d377530b385bf5b1c0099eb7d70b","externalIds":{"DBLP":"journals/corr/abs-2403-06664","ArXiv":"2403.06664","DOI":"10.1109/HPCA57654.2024.00034","CorpusId":268357266},"title":"Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System"},{"paperId":"7946c2c8dadbb9e7f5f5c6f341dcdfcde77bcf67","externalIds":{"DBLP":"journals/micro/DaiQCL24","DOI":"10.1109/MM.2024.3360081","CorpusId":267482991},"title":"High-Speed Data Communication With Advanced Networks in Large Language Model Training"},{"paperId":"d68e93cad6a038f233cba5c7f3ca5448a1744d55","externalIds":{"DBLP":"journals/corr/abs-2402-19282","ArXiv":"2402.19282","DOI":"10.48550/arXiv.2402.19282","CorpusId":268063226},"title":"WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset"},{"paperId":"63167c30b06aa6c3d76e09065ced0412090d6c3b","externalIds":{"DBLP":"journals/corr/abs-2402-17764","ArXiv":"2402.17764","DOI":"10.48550/arXiv.2402.17764","CorpusId":268041246},"title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"},{"paperId":"6dc5c6190dfbe55c8b45b7b23800614c21e5b51c","externalIds":{"DBLP":"journals/corr/abs-2402-15627","ArXiv":"2402.15627","DOI":"10.48550/arXiv.2402.15627","CorpusId":267938564},"title":"MegaScale: Scaling Large Language Model Training to More Than 10, 000 GPUs"},{"paperId":"f7ba2bb86f0804e627fcf9a89039360ecca80445","externalIds":{"ArXiv":"2402.09589","DBLP":"journals/corr/abs-2402-09589","DOI":"10.48550/arXiv.2402.09589","CorpusId":267681976},"title":"MLTCP: Congestion Control for DNN Training"},{"paperId":"061a3e91fa84b382847df502220e6930f7cfef7d","externalIds":{"ArXiv":"2402.06194","DBLP":"conf/usenix/0001JYQZLZPZWJP24","CorpusId":267617286},"title":"SuperBench: Improving Cloud AI Infrastructure Reliability with Proactive Validation"},{"paperId":"8ffd570001c45eba70075e25e6637fcdce9f8419","externalIds":{"ArXiv":"2401.16677","DBLP":"journals/corr/abs-2401-16677","DOI":"10.1145/3620665.3640410","CorpusId":267320307},"title":"T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives"},{"paperId":"883b5a6cbbf3c499c8402204a657abf1e836d310","externalIds":{"DBLP":"journals/csur/YeGHSWLZW24","DOI":"10.1145/3638757","CorpusId":266561946},"title":"Deep Learning Workload Scheduling in GPU Datacenters: A Survey"},{"paperId":"b12775806f607aa2415680630037f4ede509bc95","externalIds":{"DBLP":"journals/corr/abs-2401-11202","ArXiv":"2401.11202","DOI":"10.1145/3669940.3707284","CorpusId":267069381},"title":"PartIR: Composing SPMD Partitioning Strategies for Machine Learning"},{"paperId":"38bb1b247ee019fb3d2e0cb53fa4e067970bbf91","externalIds":{"ArXiv":"2401.08156","DBLP":"conf/asplos/0003ZXLLHGWZZZ24","DOI":"10.1145/3620665.3640423","CorpusId":267027901},"title":"GMLake: Efficient and Transparent GPU Memory Defragmentation for Large-scale DNN Training with Virtual Memory Stitching"},{"paperId":"8ac21a1545a907fc64b54cde36bf41415608cd7d","externalIds":{"DBLP":"journals/corr/abs-2401-08092","ArXiv":"2401.08092","DOI":"10.48550/arXiv.2401.08092","CorpusId":267027735},"title":"A Survey of Resource-efficient LLM and Multimodal Foundation Models"},{"paperId":"2712a7c0a8275bd0db91a61790a9e7a7aa7e74b8","externalIds":{"DBLP":"conf/eurosys/ZhangD0CW024","ArXiv":"2401.05965","DOI":"10.1145/3627703.3629580","CorpusId":265222985},"title":"HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis"},{"paperId":"411114f989a3d1083d90afd265103132fee94ebe","externalIds":{"DBLP":"journals/corr/abs-2401-04088","ArXiv":"2401.04088","DOI":"10.48550/arXiv.2401.04088","CorpusId":266844877},"title":"Mixtral of Experts"},{"paperId":"efc5e94635a850ede9c1f8dbce65d5dc536f3bfb","externalIds":{"DBLP":"journals/ijon/LiuHHZLTZWGZPXWLZZHZQLG25","ArXiv":"2401.02038","DOI":"10.48550/arXiv.2401.02038","CorpusId":266755678},"title":"Understanding LLMs: A Comprehensive Overview from Training to Inference"},{"paperId":"6cdf32647b158b6308aac7a650f756a27227f0f7","externalIds":{"ArXiv":"2401.00134","DBLP":"journals/corr/abs-2401-00134","DOI":"10.48550/arXiv.2401.00134","CorpusId":266693533},"title":"Unicron: Economizing Self-Healing LLM Training at Scale"},{"paperId":"13261129251c9e8891cff02c3aee15c4df6a5630","externalIds":{"DBLP":"journals/corr/abs-2312-15234","ArXiv":"2312.15234","DOI":"10.1145/3754448","CorpusId":266551872},"title":"Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"},{"paperId":"d0ad9d1c928e8a119f300e8dd88165060ad8c35e","externalIds":{"DBLP":"journals/corr/abs-2312-12705","ArXiv":"2312.12705","DOI":"10.48550/arXiv.2312.12705","CorpusId":266375194},"title":"Optimizing Distributed Training on Frontier for Large Language Models"},{"paperId":"18a74a76d2ed55df12544c55c43458457a081fd5","externalIds":{"DBLP":"journals/corr/abs-2312-11819","ArXiv":"2312.11819","DOI":"10.48550/arXiv.2312.11819","CorpusId":266362133},"title":"An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training"},{"paperId":"32cfcf171e0eccd1ccf50c5e1dc427ee539cef24","externalIds":{"DBLP":"journals/corr/abs-2312-11918","ArXiv":"2312.11918","DOI":"10.48550/arXiv.2312.11918","CorpusId":266362492},"title":"A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library"},{"paperId":"8f6a87e17e0987ff639638d4538e74c28f9f2b8e","externalIds":{"DBLP":"conf/sosp/ChungGJM0C24","ArXiv":"2312.06902","DOI":"10.1145/3694715.3695970","CorpusId":266174063},"title":"Reducing Energy Bloat in Large Model Training"},{"paperId":"5851121df5ce46be5faea265c868ec0beabfce96","externalIds":{"DBLP":"journals/corr/abs-2312-03863","ArXiv":"2312.03863","DOI":"10.48550/arXiv.2312.03863","CorpusId":266044196},"title":"Efficient Large Language Models: A Survey"},{"paperId":"7e88160ab2b3c9224ce0b9a66bec1740728984bf","externalIds":{"ArXiv":"2401.10241","DBLP":"journals/corr/abs-2401-10241","DOI":"10.48550/arXiv.2401.10241","CorpusId":267060979},"title":"Zero Bubble Pipeline Parallelism"},{"paperId":"8820369cbf53c520924eb84f27f21fb646eb9937","externalIds":{"DBLP":"journals/corr/abs-2311-15269","ArXiv":"2311.15269","DOI":"10.1109/HPCA57654.2024.00067","CorpusId":265456135},"title":"Tessel: Boosting Distributed Execution of Large DNN Models via Flexible Schedule Search"},{"paperId":"4ea5ca620122e6a9a2b000444d36491cebf49c7c","externalIds":{"DBLP":"journals/corr/abs-2311-12351","ArXiv":"2311.12351","DOI":"10.48550/arXiv.2311.12351","CorpusId":265308945},"title":"Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey"},{"paperId":"8b418947454affb80ba47454b65af7e8fea705ae","externalIds":{"DBLP":"conf/eurosys/JiangJ00024","ArXiv":"2311.10418","DOI":"10.1145/3627703.3629585","CorpusId":265214419},"title":"DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines"},{"paperId":"ade22704be8a0fc3730d320cc7934b2ccbcd97e4","externalIds":{"ArXiv":"2311.09431","DBLP":"journals/corr/abs-2311-09431","DOI":"10.48550/arXiv.2311.09431","CorpusId":265220849},"title":"Striped Attention: Faster Ring Attention for Causal Transformers"},{"paperId":"aef4ee743e6e61aec316ceaa301515f048020921","externalIds":{"DBLP":"journals/corr/abs-2311-05034","ArXiv":"2311.05034","DOI":"10.48550/arXiv.2311.05034","CorpusId":265066959},"title":"Just-in-time Quantization with Processing-In-Memory for Efficient ML Training"},{"paperId":"c07ce0954743d93e0e954abbc16f7f132b1ad2b8","externalIds":{"DBLP":"conf/iccd/Chen0BH23","DOI":"10.1109/ICCD58817.2023.00031","CorpusId":266494393},"title":"A Cost-Efficient Failure-Tolerant Scheme for Distributed DNN Training"},{"paperId":"58fc40ba1aaf3694f9c44425524bdc9a2c3aa35b","externalIds":{"ArXiv":"2311.01635","DBLP":"journals/corr/abs-2311-01635","DOI":"10.48550/arXiv.2311.01635","CorpusId":265019050},"title":"RTP: Rethinking Tensor Parallelism with Memory Deduplication"},{"paperId":"57c655bec92a3df3ee2a9be0a209a7350fa8a25b","externalIds":{"DBLP":"journals/corr/abs-2311-00591","ArXiv":"2311.00591","DOI":"10.48550/arXiv.2311.00591","CorpusId":264833029},"title":"Coop: Memory is not a Commodity"},{"paperId":"1094b385d05c36ccf83a110c8b22318e4f6e72f6","externalIds":{"ArXiv":"2311.00257","CorpusId":264832924},"title":"AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training"},{"paperId":"985e4584d8dce5ce1c75860828c92d65cbd58590","externalIds":{"DBLP":"journals/corr/abs-2311-00176","ArXiv":"2311.00176","DOI":"10.48550/arXiv.2311.00176","CorpusId":264833257},"title":"ChipNeMo: Domain-Adapted LLMs for Chip Design"},{"paperId":"25067493e2a969217e6c8b8115462184390bb2bd","externalIds":{"DBLP":"conf/cluster/WangLLLGLSL23","DOI":"10.1109/CLUSTER52292.2023.00015","CorpusId":265354174},"title":"Prophet: Fine-grained Load Balancing for Parallel Training of Large-scale MoE Models"},{"paperId":"eaea6533a6945450a167b4b15688dc0f240fc0ee","externalIds":{"DBLP":"journals/corr/abs-2310-19295","ArXiv":"2310.19295","DOI":"10.48550/arXiv.2310.19295","CorpusId":264806896},"title":"ROAM: memory-efficient large DNN training via optimized operator ordering and memory layout"},{"paperId":"334e8b7597b69da621c9114e3f695c6315c9267e","externalIds":{"DBLP":"journals/corr/abs-2310-18313","ArXiv":"2310.18313","DOI":"10.48550/arXiv.2310.18313","CorpusId":264555252},"title":"FP8-LM: Training FP8 Large Language Models"},{"paperId":"a2eba36b34833621fa70bcc63ba239846bfd529a","externalIds":{"DBLP":"conf/sosp/WangJZZFNW23","DOI":"10.1145/3600006.3613145","CorpusId":263609572},"title":"GEMINI: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints"},{"paperId":"71de9b9cb83dcf53a86c7a3cb3ff7b36b3917978","externalIds":{"DBLP":"conf/sosp/SubramanyaALQJG23","DOI":"10.1145/3600006.3613175","CorpusId":263610098},"title":"Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling"},{"paperId":"c31b8b8b1d1ed299b465cf70ba9c3ad9e023e407","externalIds":{"ArXiv":"2310.12670","CorpusId":264305601},"title":"Fault-Tolerant Hybrid-Parallel Training at Scale with Reliable and Efficient In-memory Checkpointing"},{"paperId":"358b96ce1653ec489e7bf565c0f1afad81e502d0","externalIds":{"ArXiv":"2310.11453","DBLP":"journals/corr/abs-2310-11453","DOI":"10.48550/arXiv.2310.11453","CorpusId":264172438},"title":"BitNet: Scaling 1-bit Transformers for Large Language Models"},{"paperId":"9d78505dd333b4bc77b75b65313ab9f96bcfe198","externalIds":{"ArXiv":"2310.10537","DBLP":"journals/corr/abs-2310-10537","DOI":"10.48550/arXiv.2310.10537","CorpusId":264146384},"title":"Microscaling Data Formats for Deep Learning"},{"paperId":"ffa343b5dce9e82f00fbf380f8b343963109f8ec","externalIds":{"ArXiv":"2310.10046","DBLP":"journals/corr/abs-2310-10046","DOI":"10.48550/arXiv.2310.10046","CorpusId":264145923},"title":"TRANSOM: An Efficient Fault-Tolerant System for Training LLMs"},{"paperId":"6a433d3cd43c22cbe23b700b9a1e0ee5cf631a8e","externalIds":{"DBLP":"journals/corr/abs-2310-06003","ArXiv":"2310.06003","DOI":"10.48550/arXiv.2310.06003","CorpusId":263831568},"title":"Rethinking Memory and Communication Cost for Efficient Large Language Model Training"},{"paperId":"a941874be310395c5778d4f1710ad0635795ad6b","externalIds":{"DBLP":"journals/corr/abs-2310-04607","ArXiv":"2310.04607","DOI":"10.48550/arXiv.2310.04607","CorpusId":263830290},"title":"A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators"},{"paperId":"8511ea96d61593de57cbc2e996910e5cb3dbfe84","externalIds":{"ArXiv":"2310.03294","CorpusId":263671659},"title":"DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training"},{"paperId":"02ad9f3fefe33cb9ca546591bec65dbdf7766c80","externalIds":{"ArXiv":"2310.01889","DBLP":"conf/iclr/0055ZA24","DOI":"10.48550/arXiv.2310.01889","CorpusId":263608461},"title":"Ring Attention with Blockwise Transformers for Near-Infinite Context"},{"paperId":"e4011238f0aee0f1baebea9b1bea0bd85371826f","externalIds":{"DBLP":"journals/corr/abs-2309-16976","ArXiv":"2309.16976","DOI":"10.1145/3624062.3624257","CorpusId":263310443},"title":"Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors"},{"paperId":"e97addc2c9d137ca53a73d41ad59083c1a4cf214","externalIds":{"DBLP":"journals/corr/abs-2309-08125","ArXiv":"2309.08125","DOI":"10.1145/3600006.3613152","CorpusId":262012886},"title":"Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"660380b17d3a37d8132f2e6dcb5cb47092e5b7d1","externalIds":{"ArXiv":"2309.01172","DBLP":"journals/corr/abs-2309-01172","DOI":"10.48550/arXiv.2309.01172","CorpusId":261530813},"title":"FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs"},{"paperId":"a32476f93be0e8707cc1b99c2f506e60d61715a4","externalIds":{"DBLP":"conf/sigcomm/LiuWJ23","DOI":"10.1145/3603269.3604869","CorpusId":261431474},"title":"Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","externalIds":{"ArXiv":"2308.10792","DBLP":"journals/corr/abs-2308-10792","DOI":"10.1145/3777411","CorpusId":261049152},"title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"338d8f3b199abcebc85f34016b0162ab3a9d5310","externalIds":{"DBLP":"journals/corr/abs-2308-07633","ArXiv":"2308.07633","DOI":"10.1162/tacl_a_00704","CorpusId":260900101},"title":"A Survey on Model Compression for Large Language Models"},{"paperId":"02d4096c030d052e1866d52fbc3b83480e1ed9f5","externalIds":{"DBLP":"conf/kdd/DongMXM023","DOI":"10.1145/3580305.3599572","CorpusId":260499677},"title":"Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques"},{"paperId":"dd278797cae2d4ca9725d98a4e0f73b637990381","externalIds":{"ArXiv":"2308.01320","DBLP":"journals/corr/abs-2308-01320","DOI":"10.48550/arXiv.2308.01320","CorpusId":260438723},"title":"DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"},{"paperId":"c095361942179c96cae0c30248fb6ff80b0bc390","externalIds":{"ArXiv":"2308.00852","DBLP":"conf/nsdi/RajasekaranGA24","DOI":"10.48550/arXiv.2308.00852","CorpusId":260378738},"title":"CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters"},{"paperId":"e77c553b71704196b2ea010651dc8f4c42c3e735","externalIds":{"DBLP":"journals/tpds/LiangTZBSLQL23","DOI":"10.1109/TPDS.2023.3281931","CorpusId":259339019},"title":"A Survey on Auto-Parallelism of Large-Scale Deep Learning Training"},{"paperId":"83d60df5c515be6a10b71fb615c224f6ea94ab6e","externalIds":{"DBLP":"journals/ijon/ZhangNDLWDW23","DOI":"10.1016/j.neucom.2023.126661","CorpusId":260651456},"title":"PipePar: Enabling fast DNN pipeline parallel training in heterogeneous GPU clusters"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"823ca4778e1027f2f0b356df051d762dcecaaba0","externalIds":{"ArXiv":"2307.08691","DBLP":"journals/corr/abs-2307-08691","DOI":"10.48550/arXiv.2307.08691","CorpusId":259936734},"title":"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"},{"paperId":"3e5741ee9cfd23c79d2af2e209ebb1b57da96e2a","externalIds":{"DBLP":"journals/tkde/WangJMFZNTC24","ArXiv":"2307.02031","DOI":"10.1109/TKDE.2024.3370614","CorpusId":259342227},"title":"Improving Automatic Parallel Training via Balanced Memory Workload Optimization"},{"paperId":"7505337d88e4d36e2e37891c542947a2e3c9e009","externalIds":{"DBLP":"conf/nips/XiLCZ23","ArXiv":"2306.11987","DOI":"10.48550/arXiv.2306.11987","CorpusId":259212192},"title":"Training Transformers with 4-bit Integers"},{"paperId":"1ca5618423c64f0656d13c2bc0d387cc2006f7b2","externalIds":{"ArXiv":"2306.10209","DBLP":"journals/corr/abs-2306-10209","DOI":"10.48550/arXiv.2306.10209","CorpusId":259203634},"title":"ZeRO++: Extremely Efficient Collective Communication for Giant Model Training"},{"paperId":"bb28712eb3b32bca5ce123bf85d81aaf0e2e037f","externalIds":{"DBLP":"journals/tjs/YinDGWT23","DOI":"10.1007/s11227-023-05479-7","CorpusId":259487340},"title":"Evaluation of pre-training large language models on leadership-class supercomputers"},{"paperId":"de431ca980507e70ef35a14a48ef635747b8edcc","externalIds":{"DBLP":"conf/vlsit/SwaminathanSWLS23","DOI":"10.23919/VLSITechnologyandCir57934.2023.10185224","CorpusId":260152089},"title":"AMD InstinctTM MI250X Accelerator enabled by Elevated Fanout Bridge Advanced Packaging Architecture"},{"paperId":"da0ae0a54c6ec1d116a615fddbd71d190b4060da","externalIds":{"ArXiv":"2305.16121","DBLP":"journals/tpds/LiLLHLGDL25","DOI":"10.1109/TPDS.2025.3583165","CorpusId":258887676},"title":"Oases: Efficient Large-Scale Model Training on Commodity Servers via Overlapped and Automated Tensor Model Parallelism"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200","externalIds":{"DBLP":"journals/corr/abs-2305-13245","ArXiv":"2305.13245","DOI":"10.48550/arXiv.2305.13245","CorpusId":258833177},"title":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"},{"paperId":"479f22891a50a553ae46ca32e7bc7e195d9293fc","externalIds":{"DBLP":"conf/infocom/ShiPCL23","DOI":"10.1109/INFOCOM53939.2023.10228874","CorpusId":261390644},"title":"PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining"},{"paperId":"a8e32285ff09afaa3fcea50a06a421d5b9fe578d","externalIds":{"DBLP":"conf/eurosys/ZhaoHYZ0YZL0QZZ23","DOI":"10.1145/3552326.3567499","CorpusId":258508799},"title":"SiloD: A Co-design of Caching and Scheduling for Deep Learning Clusters"},{"paperId":"114cc72d93c73b97ba03ed8c4e4a8d937b344607","externalIds":{"DBLP":"conf/ipps/XuY23","DOI":"10.1109/IPDPS54959.2023.00031","CorpusId":233210609},"title":"An Efficient 2D Method for Training Super-Large Deep Learning Models"},{"paperId":"ed734578bf467a51b72ea8d0da2b81e1a6364346","externalIds":{"DBLP":"journals/micro/Choquette23","DOI":"10.1109/MM.2023.3256796","CorpusId":257544490},"title":"NVIDIA Hopper H100 GPU: Scaling Performance"},{"paperId":"2d2b3fa757d7a839d6154b709f779366e624b903","externalIds":{"DBLP":"journals/tpds/LiZQCGWZC23","DOI":"10.1109/TPDS.2023.3247883","CorpusId":257655212},"title":"Fold3D: Rethinking and Parallelizing Computational and Communicational Tasks in the Training of Large DNN Models"},{"paperId":"e61462184a6dce9259e76c9069c5747a420b3c0d","externalIds":{"DBLP":"journals/pvldb/MiaoSYCJ23","DOI":"10.14778/3598581.3598604","CorpusId":259246472},"title":"SDPipe: A Semi-Decentralized Framework for Heterogeneity-aware Pipeline-parallel Training"},{"paperId":"4c1faaf1573902bdfee39fd1089d38882e7bd5eb","externalIds":{"DBLP":"conf/icse/GaoSLZWLY23","DOI":"10.1109/ICSE-SEIP58684.2023.00052","CorpusId":259123952},"title":"An Empirical Study on Quality Issues of Deep Learning Platform"},{"paperId":"a0e7c31d723608e03f30fc92ffc2a604a7a039da","externalIds":{"ArXiv":"2304.11277","DBLP":"journals/pvldb/ZhaoGVLHXWSOSDB23","DOI":"10.48550/arXiv.2304.11277","CorpusId":258297871},"title":"PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"},{"paperId":"dbbc5003af690799fa4fe6330fb795311cde106f","externalIds":{"DBLP":"journals/pacmmod/NieMWYXMC023","ArXiv":"2304.03946","DOI":"10.1145/3588964","CorpusId":258048524},"title":"FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"},{"paperId":"ece77610adfb0fb162dd22ef694f2777393c319a","externalIds":{"DBLP":"journals/corr/abs-2304-03208","ArXiv":"2304.03208","DOI":"10.48550/arXiv.2304.03208","CorpusId":257985427},"title":"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster"},{"paperId":"7c25adf2ddb35df05a61c697da97efb8583d77df","externalIds":{"DBLP":"conf/isca/JouppiK0MNNPSST23","ArXiv":"2304.01433","DOI":"10.1145/3579371.3589350","CorpusId":257921908},"title":"TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"443c1bef6a7dc3db941375ae76451c884ceffb8a","externalIds":{"DBLP":"conf/ics/SinghRARHB23","ArXiv":"2303.06318","DOI":"10.1145/3577193.3593704","CorpusId":258686146},"title":"A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training"},{"paperId":"d7e00702bbb5a0cccc97033f0405b634ae9e2d3c","externalIds":{"ArXiv":"2303.02868","DBLP":"journals/corr/abs-2303-02868","DOI":"10.48550/arXiv.2303.02868","CorpusId":257365347},"title":"Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent"},{"paperId":"3822cb0a089a66f2ad88e7960fcae6ebdc4e4427","externalIds":{"ArXiv":"2302.12445","DBLP":"conf/icdcs/ZhangSCWLL23","DOI":"10.1109/ICDCS57875.2023.00054","CorpusId":259164359},"title":"DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining"},{"paperId":"a34384389f74b7b2c31c696b0db0bf813e8bb301","externalIds":{"DBLP":"conf/nips/ChenLWYY22","ArXiv":"2302.09915","DOI":"10.48550/arXiv.2302.09915","CorpusId":257038272},"title":"TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training"},{"paperId":"3599a236f285af48782fc30b1341d13ec7320735","externalIds":{"ArXiv":"2302.09419","DBLP":"journals/corr/abs-2302-09419","DOI":"10.48550/arXiv.2302.09419","CorpusId":257039063},"title":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"},{"paperId":"6c148b4273eeab70ca4499400d5fcf34d813b957","externalIds":{"DBLP":"conf/asplos/ChenYZZZ024","ArXiv":"2302.08005","DOI":"10.1145/3620665.3640399","CorpusId":256901173},"title":"Slapo: A Schedule Language for Progressive Optimization of Large Deep Learning Model Training"},{"paperId":"fc2f87333527ea30f9bdf7391572509c2a6f79c3","externalIds":{"DBLP":"conf/nsdi/LiBVLXMY24","ArXiv":"2302.08545","DOI":"10.48550/arXiv.2302.08545","CorpusId":257020088},"title":"THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression"},{"paperId":"bea8f5d6cf70402b21721b7ddf458286ef7af775","externalIds":{"ArXiv":"2302.02599","DBLP":"journals/corr/abs-2302-02599","DOI":"10.48550/arXiv.2302.02599","CorpusId":257079273},"title":"Colossal-Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models"},{"paperId":"2365de6a5d1085f6225fc42a7cb47d28eaf71985","externalIds":{"DBLP":"conf/hpca/LinCCY23","DOI":"10.1109/HPCA56546.2023.10071043","CorpusId":257720856},"title":"Tensor Movement Orchestration in Multi-GPU Training Systems"},{"paperId":"eef3b5edb30c6b68ccc1d69dea83946d282899e6","externalIds":{"DBLP":"conf/hpca/ZhengCSCLYLLL23","DOI":"10.1109/HPCA56546.2023.10071018","CorpusId":257720824},"title":"Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion"},{"paperId":"43cefce076df7ee54505fd78a8a97129c0f6d36b","externalIds":{"DBLP":"conf/hpca/ZhouWYLBYX23","DOI":"10.1109/HPCA56546.2023.10071077","CorpusId":257720517},"title":"MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism"},{"paperId":"5278b81db686b4d36143941bff1c683bea963a63","externalIds":{"DBLP":"journals/corr/abs-2301-11913","ArXiv":"2301.11913","DOI":"10.48550/arXiv.2301.11913","CorpusId":251542095},"title":"SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient"},{"paperId":"58d6ec0dec4952e93be7cf72c1cebb0216eac9df","externalIds":{"DBLP":"conf/asplos/FengXTWLS23","DOI":"10.1145/3575693.3575703","CorpusId":256391189},"title":"Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers"},{"paperId":"d5fe97309afdf0da633e04b5da4212a054661ecf","externalIds":{"DBLP":"conf/asplos/HuZ00023","DOI":"10.1145/3575693.3575705","CorpusId":256391176},"title":"Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs"},{"paperId":"3995c7a3704db252cfcce18ef56562a89f4cf693","externalIds":{"DBLP":"conf/asplos/GuZZXHCYHJL23","DOI":"10.1145/3575693.3575721","CorpusId":256391186},"title":"ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning"},{"paperId":"cba1fb71b14d211639dbabb2d4cfd4a2295c985f","externalIds":{"DBLP":"journals/tpds/ChenLGYH24","ArXiv":"2301.06813","DOI":"10.1109/TPDS.2024.3397800","CorpusId":269588296},"title":"AutoDDL: Automatic Distributed Deep Learning With Near-Optimal Bandwidth Cost"},{"paperId":"3692f4df9d11af68f9b9c9a526667db3f99e552c","externalIds":{"DBLP":"conf/asplos/WangWSDIHCMMZKG23","DOI":"10.1145/3567955.3567959","CorpusId":254927634},"title":"Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models"},{"paperId":"736973165f98105fec3729b7db414ae4d80fcbeb","externalIds":{"DBLP":"journals/corr/abs-2212-09748","ArXiv":"2212.09748","DOI":"10.1109/ICCV51070.2023.00387","CorpusId":254854389},"title":"Scalable Diffusion Models with Transformers"},{"paperId":"16de2006e2960ba410772c6b6d460b83c0a5cc4b","externalIds":{"ArXiv":"2212.07143","DBLP":"journals/corr/abs-2212-07143","DOI":"10.1109/CVPR52729.2023.00276","CorpusId":254636568},"title":"Reproducible Scaling Laws for Contrastive Language-Image Learning"},{"paperId":"be157d55b4afd5be9c81619d75aa4897f5e201e4","externalIds":{"DBLP":"journals/corr/abs-2212-05339","ArXiv":"2212.05339","DOI":"10.48550/arXiv.2212.05339","CorpusId":254564302},"title":"Elixir: Train a Large Language Model on a Small GPU Cluster"},{"paperId":"43014fc85c4860487336579ec98f509fec1803f7","externalIds":{"DBLP":"conf/mlsys/GaleNYZ23","ArXiv":"2211.15841","DOI":"10.48550/arXiv.2211.15841","CorpusId":254069783},"title":"MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"e2df6ae1b3485449364ce2a5356ab09600fc3632","externalIds":{"DBLP":"journals/corr/abs-2211-13878","ArXiv":"2211.13878","DOI":"10.14778/3570690.3570697","CorpusId":254017888},"title":"Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"},{"paperId":"1a6a7fe065e42365515ccf7d0b5d1225b8088464","externalIds":{"DBLP":"conf/sc/SunWQYHXW22","DOI":"10.1109/SC41404.2022.00076","CorpusId":257158304},"title":"STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training"},{"paperId":"6d088e8d785a57b50c2b0e465e2460e09ced48d7","externalIds":{"DBLP":"conf/usenix/LiJZ0X23","ArXiv":"2210.17223","CorpusId":259859076},"title":"Accelerating Distributed MoE Training and Inference with Lina"},{"paperId":"347bb85574a0152d7273245c4e657965fb325213","externalIds":{"ArXiv":"2210.16691","DBLP":"conf/mlsys/HuangB0W0D023","CorpusId":259922973},"title":"ALCOP: Automatic Load-Compute Pipelining in Deep Learning Compiler for AI-GPUs"},{"paperId":"1e2fd6d64e7eea23d713c98bcc8664c1cf052d35","externalIds":{"DBLP":"conf/nips/LiWXZ22","ArXiv":"2210.07297","DOI":"10.48550/arXiv.2210.07297","CorpusId":252907909},"title":"AMP: Automatically Finding Model Parallel Strategies with Heterogeneity Awareness"},{"paperId":"22b58dce1a13382418b8372bbd50ed3b2533f899","externalIds":{"DBLP":"journals/corr/abs-2210-03052","ArXiv":"2210.03052","DOI":"10.1109/IPDPS54959.2023.00042","CorpusId":252734710},"title":"ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"},{"paperId":"1ad7a323cebbd186a6f3c9a1f0caf0af94ce91bd","externalIds":{"ArXiv":"2209.01346","DBLP":"conf/sc/HoeflerBSGLHBGCS22","DOI":"10.1109/SC41404.2022.00016","CorpusId":252089281},"title":"HammingMesh: A Network Topology for Large-Scale Deep Learning"},{"paperId":"baa467a4dccf87bc7e2c5a4ea6fd5e401d962d39","externalIds":{"DBLP":"conf/cluster/LiuLLDGL22","DOI":"10.1109/CLUSTER51413.2022.00042","CorpusId":252999300},"title":"AutoPipe: A Fast Pipeline Parallelism Approach with Balanced Partitioning and Micro-batch Slicing"},{"paperId":"0437f76a8e6607116659f16c7e4e656851fdd9d5","externalIds":{"DBLP":"conf/cluster/DuanLLLGLL22","DOI":"10.1109/CLUSTER51413.2022.00043","CorpusId":253000093},"title":"HPH: Hybrid Parallelism on Heterogeneous Clusters for Accelerating Large-scale DNNs Training"},{"paperId":"92ffac92cdb5ae7fce881b4d65996458dfe7f241","externalIds":{"DBLP":"conf/sigcomm/ZhaoLPZLJ22","DOI":"10.1145/3544216.3544224","CorpusId":251496057},"title":"Multi-resource interleaving for deep learning training"},{"paperId":"c419c631aa4c271b009d53ef26f94675c87cbf04","externalIds":{"DBLP":"conf/hotchips/Choquette22","DOI":"10.1109/HCS55958.2022.9895592","CorpusId":252549667},"title":"Nvidia Hopper GPU: Scaling Performance"},{"paperId":"3deca3e77cfddb11feb4783f8acb2ceb860800e5","externalIds":{"ArXiv":"2208.06102","DBLP":"journals/corr/abs-2208-06102","DOI":"10.48550/arXiv.2208.06102","CorpusId":251554526},"title":"Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training"},{"paperId":"d5cfc82ac7ad6c5e9c91ef18bba6d2979b632443","externalIds":{"DBLP":"journals/corr/abs-2206-04959","ArXiv":"2206.04959","DOI":"10.1109/TPDS.2023.3247001","CorpusId":249605824},"title":"Merak: An Efficient Distributed DNN Training Framework With Automated 3D Parallelism for Giant Foundation Models"},{"paperId":"2e700ff36108119f5ed19a53bd2eaa22b42ec3d8","externalIds":{"ArXiv":"2206.03382","DBLP":"journals/corr/abs-2206-03382","DOI":"10.48550/arXiv.2206.03382","CorpusId":249431713},"title":"Tutel: Adaptive Mixture-of-Experts at Scale"},{"paperId":"6b117a8dcaa161562b0a69afbb9811e11afb5b3e","externalIds":{"DBLP":"journals/corr/abs-2206-01288","ArXiv":"2206.01288","DOI":"10.48550/arXiv.2206.01288","CorpusId":249375466},"title":"Decentralized Training of Foundation Models in Heterogeneous Environments"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"e8327b84e4c8c631bb11443ad13af18c9f5ee634","externalIds":{"DBLP":"journals/tsc/YuSHGWBDX24","ArXiv":"2205.10034","DOI":"10.1109/TSC.2024.3399654","CorpusId":248965412},"title":"MoESys: A Distributed and Efficient Mixture-of-Experts Training and Inference System for Internet Services"},{"paperId":"cfbfb9eecacce184706f79bda3d90406d806b2ae","externalIds":{"ArXiv":"2205.05243","CorpusId":251467983},"title":"Enabling Fast and Flexible Distributed Deep Learning with Programmable Switches"},{"paperId":"bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96","externalIds":{"ArXiv":"2205.05198","DBLP":"journals/corr/abs-2205-05198","DOI":"10.48550/arXiv.2205.05198","CorpusId":248693351},"title":"Reducing Activation Recomputation in Large Transformer Models"},{"paperId":"c183510bc7b86ee0ed7ecc6fb0698a43bd0e2379","externalIds":{"DBLP":"conf/icde/GuZXCFHDYDCH22","DOI":"10.1109/icde53745.2022.00209","CorpusId":251289622},"title":"Fluid: Dataset Abstraction and Elastic Acceleration for Cloud-native Deep Learning Training Jobs"},{"paperId":"ba377c911b1d601cbe6a7f10f89e5b6175adcd98","externalIds":{"DBLP":"conf/icde/NieMYC22","DOI":"10.1109/icde53745.2022.00241","CorpusId":251291572},"title":"TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting"},{"paperId":"cd733ce920e055415e9a9a7d90d3ec89f8750866","externalIds":{"ArXiv":"2205.00119","DBLP":"journals/pvldb/Zhang0WCKC0022","DOI":"10.14778/3561261.3561265","CorpusId":248496469},"title":"MiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud"},{"paperId":"5ce7d930b87a16ae9a7b5f789181a6e021f4df5d","externalIds":{"DBLP":"journals/corr/abs-2204-12013","ArXiv":"2204.12013","DOI":"10.48550/arXiv.2204.12013","CorpusId":248391844},"title":"Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs"},{"paperId":"f2c9db0279f9acc32d3315fe8daa4dca28fe60a2","externalIds":{"DBLP":"conf/ispd/Fricker22","DOI":"10.1145/3505170.3511036","CorpusId":248150886},"title":"The Cerebras CS-2: Designing an AI Accelerator around the World's Largest 2.6 Trillion Transistor Chip"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"533beb3cc4b99f05ac2f3baae90e1bab07fd93a2","externalIds":{"DBLP":"conf/eurosys/OhLKS22","DOI":"10.1145/3492321.3519563","CorpusId":247765527},"title":"Out-of-order backprop: an effective scheduling technique for deep learning"},{"paperId":"0dab58e476f3f0e6f580a295f7c4756c86f1f198","externalIds":{"DBLP":"conf/ppopp/HeZAWLSL22","DOI":"10.1145/3503221.3508418","CorpusId":247765599},"title":"FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models"},{"paperId":"512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352","externalIds":{"DBLP":"journals/corr/abs-2203-12533","ArXiv":"2203.12533","DOI":"10.48550/arXiv.2203.12533","CorpusId":247618786},"title":"Pathways: Asynchronous Distributed Dataflow for ML"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"f756e993e6eba8d7c3db74838a348ed3a79bc1cc","externalIds":{"DBLP":"journals/cse/ElsterH22","DOI":"10.1109/MCSE.2022.3163817","CorpusId":249474974},"title":"Nvidia Hopper GPU and Grace CPU Highlights"},{"paperId":"7171c5f56543c51ea584ed9edfe8ccd33375167d","externalIds":{"DBLP":"conf/asplos/ZhengYZLZZZLYZS22","DOI":"10.1145/3503222.3507723","CorpusId":247026617},"title":"AStitch: enabling a new multi-dimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures"},{"paperId":"f74b387136d1d57a791c6b7c9d823577a47516aa","externalIds":{"DBLP":"journals/pvldb/LiPMTK22","ArXiv":"2202.01306","DOI":"10.14778/3551793.3551828","CorpusId":246485753},"title":"Harmony: Overcoming the hurdles of GPU memory capacity to train massive DNN models on commodity servers"},{"paperId":"2d9c43e1133f17c73408774840c93382f3cc8f84","externalIds":{"DBLP":"conf/nsdi/WangKZGJM0K23","ArXiv":"2202.00433","CorpusId":252668896},"title":"TopoOpt: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs"},{"paperId":"91b3f376f063dd59060ea5b95e8cb828adec36d7","externalIds":{"ArXiv":"2201.11840","CorpusId":246485856},"title":"GC3: An Optimizing Compiler for GPU Collective Communication"},{"paperId":"7d1e512888a2fa4e838c12a02ae7fce867d322a8","externalIds":{"DBLP":"conf/icml/RajbhandariLYZA22","ArXiv":"2201.05596","CorpusId":245986500},"title":"DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"},{"paperId":"8584a1f52ec8d31f028159c3bc56c68d600c2c1a","externalIds":{"DOI":"10.1109/mspec.2022.9676353","CorpusId":245874814},"title":"The Exascale Era is Upon Us: The Frontier supercomputer may be the first to reach 1,000,000,000,000,000,000 operations per second"},{"paperId":"5137c88a52635013610c7f6215105677ad42dede","externalIds":{"ArXiv":"2112.06095","DBLP":"journals/corr/abs-2112-06095","CorpusId":283273202},"title":"Unlocking the Power of Inline Floating-Point Operations on Programmable Switches"},{"paperId":"53c3940f35b8b45d55ed49056282e1961954513d","externalIds":{"ArXiv":"2112.05682","CorpusId":245117555},"title":"Self-attention Does Not Need $O(n^2)$ Memory"},{"paperId":"ee042a3e299a32c413532e64603de8d3ddb6aa87","externalIds":{"DBLP":"journals/corr/abs-2112-02958","ArXiv":"2112.02958","CorpusId":244908339},"title":"Automap: Towards Ergonomic Automated Parallelism for ML Models"},{"paperId":"cb1cc03b04079b89a1f42d736cba4d2ee3e4f7c3","externalIds":{"ArXiv":"2111.04867","DBLP":"conf/nsdi/ShahCCMMMNS23","CorpusId":250420873},"title":"TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches"},{"paperId":"43332a71939ae7f3bd4756cba2c5ef0763b5cfac","externalIds":{"DBLP":"journals/corr/abs-2111-04007","ArXiv":"2111.04007","DOI":"10.1145/3492321.3519584","CorpusId":243847496},"title":"Varuna: scalable, low-cost training of massive deep learning models"},{"paperId":"f12975df157fe4462054ba1484dd4b4e5aaf5a17","externalIds":{"ArXiv":"2111.00856","DBLP":"journals/corr/abs-2111-00856","CorpusId":240354394},"title":"Large-Scale Deep Learning Optimizations: A Comprehensive Survey"},{"paperId":"ee8984a6712791d4e0f2c776dad8119a3b893dd9","externalIds":{"ArXiv":"2110.14883","DBLP":"conf/icpp/LiLBFHLW023","DOI":"10.1145/3605573.3605613","CorpusId":240070340},"title":"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"},{"paperId":"3186b9dd1331b647cf3304d185c248ea7ec9ad1b","externalIds":{"ArXiv":"2110.15032","DBLP":"journals/corr/abs-2110-15032","CorpusId":240070625},"title":"OneFlow: Redesign the Distributed Deep Learning Framework from Scratch"},{"paperId":"e930b3dc75f9cc36aa78aa640eee639b708a92bd","externalIds":{"DBLP":"conf/mlsys/XieNGV22","ArXiv":"2110.10548","CorpusId":239050101},"title":"Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning"},{"paperId":"ccbe83578d44cb90a79aee05588e4a3323d4403d","externalIds":{"DBLP":"journals/tcasI/LiuZLWDC21","DOI":"10.1109/tcsi.2021.3098841","CorpusId":238221291},"title":"Scalable Fully Pipelined Hardware Architecture for In-Network Aggregated AllReduce Communication"},{"paperId":"ac35dffd21c16b02e140a36726b3a21d266cab0f","externalIds":{"DBLP":"conf/sc/Hu0Y0021","ArXiv":"2109.01313","DOI":"10.1145/3458817.3476223","CorpusId":237417338},"title":"Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters"},{"paperId":"78a5b29d3bdeaf297aee08cab6a8b4f59763481b","externalIds":{"DBLP":"journals/micro/DongWFCPTLLRGGL21","DOI":"10.1109/mm.2021.3091475","CorpusId":237520669},"title":"ACCL: Architecting Highly Scalable Distributed Training Systems With Highly Efficient Collective Communication Library"},{"paperId":"10c0a1d3519dcc7b876d21d614f49d82467c9dc3","externalIds":{"DBLP":"journals/tpds/FangZLSYZY23","ArXiv":"2108.05818","DOI":"10.1109/TPDS.2022.3219819","CorpusId":236986955},"title":"Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management"},{"paperId":"c0c36424f691c9d934aee7b9c6c6c86429a57b45","externalIds":{"DBLP":"conf/sigcomm/ShirkoohiGAZGBV21","DOI":"10.1145/3452296.3472900","CorpusId":236426380},"title":"SiP-ML: high-bandwidth optical network interconnects for machine learning training"},{"paperId":"10f3ca78e194552427ebe9173b19d1b910469e27","externalIds":{"DBLP":"conf/sc/0002H21","ArXiv":"2107.06925","DOI":"10.1145/3458817.3476145","CorpusId":235898937},"title":"Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"659b95b28a9f3b6cfa3bd06fa7bd004165db5663","externalIds":{"DBLP":"conf/icpp/WangXB022","ArXiv":"2105.14500","DOI":"10.1145/3545008.3545087","CorpusId":251979875},"title":"Tesseract: Parallelize the Tensor Parallelism Efficiently"},{"paperId":"d8df456f790381f4ddb388be24a546625bd75ee2","externalIds":{"DBLP":"journals/corr/abs-2105-14450","ArXiv":"2105.14450","CorpusId":235254604},"title":"Maximizing Parallelism in Distributed Training for Huge Neural Networks"},{"paperId":"16e623059ffccab60f4c35be028a2d4f10933515","externalIds":{"ArXiv":"2105.13120","DBLP":"conf/acl/LiXBL023","ACL":"2023.acl-long.134","DOI":"10.18653/v1/2023.acl-long.134","CorpusId":246017095},"title":"Sequence Parallelism: Long Sequence Training from System Perspective"},{"paperId":"91b29761840442005da39bc258e2298b528f31aa","externalIds":{"ArXiv":"2105.05720","DBLP":"conf/asplos/JangdaHLSMMMMS22","DOI":"10.1145/3503222.3507778","CorpusId":237292610},"title":"Breaking the computation and communication abstraction barrier in distributed machine learning workloads"},{"paperId":"509b16378deec0fb6bbec1d7aeb32a4bdeedddb1","externalIds":{"DBLP":"journals/corr/abs-2105-04663","ArXiv":"2105.04663","CorpusId":234357958},"title":"GSPMD: General and Scalable Parallelization for ML Computation Graphs"},{"paperId":"d037f3df7bcad91c18f394855054abf990f7bb40","externalIds":{"DBLP":"conf/asplos/BlocherWES21","DOI":"10.1145/3445814.3446760","CorpusId":232167769},"title":"Switches for HIRE: resource scheduling for data center in-network computing"},{"paperId":"72dd63d67588a42fc817bbb8d655b397f67425df","externalIds":{"DBLP":"journals/corr/abs-2104-07857","ArXiv":"2104.07857","DOI":"10.1145/3458817.3476205","CorpusId":233289729},"title":"ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"},{"paperId":"774591fdd988eaaff3917e7c5171d044b0843e63","externalIds":{"ArXiv":"2104.04473","DBLP":"conf/sc/NarayananSCLPKV21","DOI":"10.1145/3458817.3476209","CorpusId":236635565},"title":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"},{"paperId":"238eb420c472bfdb1b4d34f9f53abec51f307a6b","externalIds":{"DBLP":"journals/corr/abs-2103-13262","ArXiv":"2103.13262","CorpusId":232335691},"title":"FastMoE: A Fast Mixture-of-Expert Training System"},{"paperId":"040ad14a2c97e51510889ae6a0c3c23b29da801d","externalIds":{"ArXiv":"2102.07988","DBLP":"conf/icml/LiZGZZSS21","CorpusId":231934213},"title":"TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models"},{"paperId":"7a485356e538cf5d259912f41a7e54d2370397ca","externalIds":{"DBLP":"conf/isscc/ChoquetteLKBK21","DOI":"10.1109/ISSCC42613.2021.9365803","CorpusId":232153034},"title":"3.2 The A100 Datacenter GPU and Ampere Architecture"},{"paperId":"12b71736392209b4292471b7da0aed71ba2aa545","externalIds":{"DBLP":"conf/usenix/0015RARYZ0H21","MAG":"3121562065","ArXiv":"2101.06840","CorpusId":231632857},"title":"ZeRO-Offload: Democratizing Billion-Scale Model Training"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"6c2294891e79e4f5eb7f000d586cbf0e0ed2d3b5","externalIds":{"DBLP":"conf/conext/TaheriMVFEE20","MAG":"3109063463","DOI":"10.1145/3386367.3431316","CorpusId":227154629},"title":"RoCC: robust congestion control for RDMA"},{"paperId":"2b2056bf5763e32811a69769fa8c223160125f9e","externalIds":{"MAG":"3091055609","DBLP":"journals/corr/abs-2108-13342","ArXiv":"2108.13342","DOI":"10.1145/3453483.3454083","CorpusId":235474532},"title":"DNNFusion: accelerating deep neural networks execution with advanced operator fusion"},{"paperId":"7b829d3e23a6da339e254b6592a2a9ddd1ae526f","externalIds":{"MAG":"3087129161","DBLP":"journals/corr/abs-2009-09736","ArXiv":"2009.09736","CorpusId":221819114},"title":"NetReduce: RDMA-Compatible In-Network Reduction for Distributed DNN Training Acceleration"},{"paperId":"2adcdc9e9e81147499e1372f992d25ac6265fb29","externalIds":{"DBLP":"conf/osdi/QiaoCSNH0GX21","MAG":"3080451848","ArXiv":"2008.12260","CorpusId":221340551},"title":"Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning"},{"paperId":"461a7bf0c14df0fb08d3c59bdec491778a861270","externalIds":{"DBLP":"journals/corr/abs-2008-08708","ArXiv":"2008.08708","MAG":"3072623287","DOI":"10.1145/3437801.3441620","CorpusId":221186601},"title":"Synthesizing optimal collective algorithms"},{"paperId":"2f4d6d3748ac6822711fe0bbd4cf6d2e66fa6613","externalIds":{"MAG":"3096583839","ArXiv":"2008.09213","DBLP":"journals/corr/abs-2008-09213","CorpusId":221246383},"title":"Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads"},{"paperId":"dc9b09b688b4bfc116d1d55df3cdec399c768eb9","externalIds":{"DBLP":"conf/hotchips/NorriePYKLLYJP20","MAG":"3094406919","DOI":"10.1109/HCS49909.2020.9220735","CorpusId":222419775},"title":"Google's Training Chips Revealed: TPUv2 and TPUv3"},{"paperId":"5b1809dce04d222900cc64f39f31e45f65bef6f3","externalIds":{"MAG":"3046470751","DBLP":"conf/sigcomm/KumarDJWWMWSARW20","DOI":"10.1145/3387514.3406591","CorpusId":220872273},"title":"Swift: Delay is Simple and Effective for Congestion Control in the Datacenter"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","externalIds":{"MAG":"3081168214","DBLP":"conf/kdd/RasleyRRH20","DOI":"10.1145/3394486.3406703","CorpusId":221191193},"title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"21a4cd35f19cfe8df1065b066b16edd048d2535d","externalIds":{"DBLP":"conf/ppopp/FanRMCWZWLYXDLL21","ArXiv":"2007.01045","MAG":"3038581078","DOI":"10.1145/3437801.3441593","CorpusId":220302383},"title":"DAPPLE: a pipelined data parallel approach for training large models"},{"paperId":"488128bc81bb96ecdfdff8ca79fb793308d05285","externalIds":{"DBLP":"conf/infocom/BaoPCW20","MAG":"3047537431","DOI":"10.1109/INFOCOM41043.2020.9155446","CorpusId":216609964},"title":"Preemptive All-reduce Scheduling for Expediting Distributed DNN Training"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","externalIds":{"MAG":"3040573126","DBLP":"conf/iclr/LepikhinLXCFHKS21","ArXiv":"2006.16668","CorpusId":220265858},"title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"7208279034eba55f9d4aaab9525f5484b434d019","externalIds":{"MAG":"3039050620","ArXiv":"2006.16423","DBLP":"journals/corr/abs-2006-16423","CorpusId":220265965},"title":"Efficient Algorithms for Device Placement of DNN Graph Operators"},{"paperId":"ab5f0004c5f3317689e8457e1c8d8390ccbee522","externalIds":{"DBLP":"journals/cacm/JouppiYKLPLYP20","MAG":"3036878841","DOI":"10.1145/3360307","CorpusId":219843141},"title":"A domain-specific supercomputer for training deep neural networks"},{"paperId":"cfa6e7ac8bef5b3aadcdc7a27d2a9e9d508b3322","externalIds":{"DBLP":"conf/sigcomm/ZhangCLWAJ20","MAG":"3044837714","ArXiv":"2006.10103","DOI":"10.1145/3405671.3405810","CorpusId":219793032},"title":"Is Network the Bottleneck of Distributed Training?"},{"paperId":"09bda461aa4911d0513e8e46dd39a4113947e450","externalIds":{"DBLP":"conf/osdi/ZhengJSWYHWYZSG20","MAG":"3035582633","ArXiv":"2006.06762","CorpusId":219636278},"title":"Ansor : Generating High-Performance Tensor Programs for Deep Learning"},{"paperId":"9d9dbb4487aca2b62ca3659446d7010ac65aa642","externalIds":{"MAG":"3045098739","DBLP":"conf/usenix/ParkYYNLCNC20","ArXiv":"2005.14038","CorpusId":218971732},"title":"HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism"},{"paperId":"5e2df05a1a45b57d2fc0748bccc45865b9da6913","externalIds":{"DBLP":"conf/ccgrid/NicolaeLWBDC20","MAG":"3021124033","DOI":"10.1109/CCGrid49817.2020.00-76","CorpusId":218969734},"title":"DeepFreeze: Towards Scalable Asynchronous Checkpointing of Deep Learning Models"},{"paperId":"c9060df13d2e7f06b277a2893590aebb653c5995","externalIds":{"DBLP":"journals/corr/abs-2004-13336","MAG":"3021234081","ArXiv":"2004.13336","CorpusId":216562234},"title":"Automatic Cross-Replica Sharding of Weight Update in Data-Parallel Training"},{"paperId":"96a3afc3e3fd6ef727b6aea846f54f59283a473e","externalIds":{"DBLP":"journals/tpds/CaiYMWHCSY22","MAG":"3019389960","ArXiv":"2004.10856","DOI":"10.1109/TPDS.2021.3132413","CorpusId":216080850},"title":"TensorOpt: Exploring the Tradeoffs in Distributed DNN Training with Auto-Parallelism"},{"paperId":"3b2a4cda4a1b9a4c2b2e478f4778719dca52c3fb","externalIds":{"DBLP":"conf/eurosys/ChaudharyRSKV20","MAG":"3022298203","DOI":"10.1145/3342195.3387555","CorpusId":218489707},"title":"Balancing efficiency and fairness in heterogeneous GPU clusters for deep learning"},{"paperId":"d31b6e2991f494373eb27744b097471ffb319c2b","externalIds":{"MAG":"3005665451","DBLP":"conf/fast/KumarS20","CorpusId":211566705},"title":"Quiver: An Informed Storage Cache for Deep Learning"},{"paperId":"6a5c0fc737b6fbd6672fc4265b5e0ca38de17416","externalIds":{"DBLP":"journals/corr/abs-2002-05645","MAG":"3006131567","ArXiv":"2002.05645","CorpusId":211096822},"title":"Training Large Neural Networks with Constant Memory using a New Execution Algorithm"},{"paperId":"65aa97fd4f52f863bbce2fe8a88f762b90086020","externalIds":{"DBLP":"conf/isscc/NaffzigerLPS20","MAG":"3016212306","DOI":"10.1109/ISSCC19947.2020.9063103","CorpusId":215800319},"title":"2.2 AMD Chiplet Architecture for High-Performance Server and Desktop Products"},{"paperId":"cd410e078d091c567a1fb3438f705ac5d75009ec","externalIds":{"DBLP":"conf/hpca/DongCZYWFZLSPGJ20","MAG":"3016395792","DOI":"10.1109/HPCA47549.2020.00056","CorpusId":215817496},"title":"EFLOPS: Algorithm and System Co-Design for a High Performance Distributed Training Platform"},{"paperId":"524f89a2a8f2a260e60f895076a9bd324df0cf25","externalIds":{"MAG":"3016430712","DBLP":"conf/hpca/SongCZQLC20","DOI":"10.1109/HPCA47549.2020.00036","CorpusId":214586807},"title":"AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"5c36a76d181d9dd48923ba68f8b78f20db037af1","externalIds":{"MAG":"2994144272","DBLP":"journals/tbd/SunWHFY22","DOI":"10.1109/tbdata.2019.2957478","CorpusId":213273077},"title":"GradientFlow: Optimizing Network Performance for Large-Scale Distributed DNN Training"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","externalIds":{"MAG":"2970971581","DBLP":"journals/corr/abs-1912-01703","ArXiv":"1912.01703","CorpusId":202786778},"title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"1e009f755503bffd7644fcd0a45939c54b838b37","externalIds":{"DBLP":"journals/ibmrd/ChoFSKH19","MAG":"2981114289","DOI":"10.1147/jrd.2019.2947013","CorpusId":208224662},"title":"BlueConnect: Decomposing all-reduce for deep learning on heterogeneous network hierarchy"},{"paperId":"3fd7c9ba742dd2b435afa75217847e5087e2f2a8","externalIds":{"DBLP":"conf/sosp/NarayananHPSDGG19","MAG":"2969388332","DOI":"10.1145/3341301.3359646","CorpusId":202488191},"title":"PipeDream: generalized pipeline parallelism for DNN training"},{"paperId":"76c929af6735cdff2c4badc9a9c8f39d15ea3e70","externalIds":{"MAG":"2975712713","DBLP":"conf/sosp/PengZCBYLWG19","DOI":"10.1145/3341301.3359642","CorpusId":202741181},"title":"A generic communication scheduler for distributed DNN training acceleration"},{"paperId":"80b362efee95c1759c6dab9219eb77ca3ee44475","externalIds":{"DBLP":"conf/sosp/JiaPTWZA19","MAG":"2981758446","DOI":"10.1145/3341301.3359630","CorpusId":202726856},"title":"TASO: optimizing deep learning computation with automatic generation of graph substitutions"},{"paperId":"fd431005d26100f5453590080683cbae9dc1189f","externalIds":{"DBLP":"journals/corr/abs-1910-02653","ArXiv":"1910.02653","MAG":"3037639655","CorpusId":203837073},"title":"Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","externalIds":{"MAG":"3025935268","DBLP":"conf/sc/RajbhandariRRH20","ArXiv":"1910.02054","DOI":"10.1109/SC41405.2020.00024","CorpusId":269617042},"title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","externalIds":{"MAG":"2973727699","ArXiv":"1909.08053","DBLP":"journals/corr/abs-1909-08053","CorpusId":202660670},"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"382b54869bb92039a7f7c56de67d70fb2421f604","externalIds":{"MAG":"2968108410","DBLP":"conf/sigcomm/LiMLZFTCZKAY19","DOI":"10.1145/3341302.3342085","CorpusId":199661760},"title":"HPCC: high precision congestion control"},{"paperId":"e9343b8322ea4bd2630a81209fe7fc86efbf4d3a","externalIds":{"DBLP":"conf/icpp/ChowdhuryZHPMGM19","MAG":"2963304552","DOI":"10.1145/3337821.3337902","CorpusId":198963479},"title":"I/O Characterization and Performance Evaluation of BeeGFS for Deep Learning"},{"paperId":"4c3ae07aab62ad4aaa7448be0a0ed834b624b5a0","externalIds":{"MAG":"2982356264","ArXiv":"1907.01484","DBLP":"journals/corr/abs-1907-01484","CorpusId":208104372},"title":"Themis: Fair and Efficient GPU Cluster Scheduling for Machine Learning Workloads"},{"paperId":"661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f","externalIds":{"MAG":"2954698171","DBLP":"conf/pldi/TilletKC19","DOI":"10.1145/3315508.3329973","CorpusId":184488182},"title":"Triton: an intermediate language and compiler for tiled neural network computations"},{"paperId":"f238364cff7c2723a2319d42a30323a780926cb5","externalIds":{"DBLP":"conf/isca/LiLYCSH19","MAG":"2949161920","DOI":"10.1145/3307650.3322259","CorpusId":189818249},"title":"Accelerating Distributed Reinforcement learning with In-Switch Computing"},{"paperId":"e65c84e2778d7b13b7541e6b14ff790b624a24ec","externalIds":{"ArXiv":"1905.12322","DBLP":"journals/corr/abs-1905-12322","MAG":"2947629474","CorpusId":168170136},"title":"A Study of BFLOAT16 for Deep Learning Training"},{"paperId":"90cbe7f340a8de92143e5b464e6e963bb95f6129","externalIds":{"DBLP":"conf/mlsys/JayarajanWGFP19","MAG":"2944793600","ArXiv":"1905.03960","CorpusId":85461415},"title":"Priority-based Parameter Propagation for Distributed DNN Training"},{"paperId":"6b39bea0ae1720dcbc1ed19ffa697114c4d356c4","externalIds":{"MAG":"3004495293","ArXiv":"1903.11314","DBLP":"journals/corr/abs-1903-11314","DOI":"10.1145/3363554","CorpusId":219875251},"title":"Scalable Deep Learning on Distributed Infrastructures"},{"paperId":"d25626a852bd6df7502721e54b4330e7566f49ac","externalIds":{"DBLP":"conf/sc/YangCRCA19","ArXiv":"1903.11714","MAG":"2983490771","DOI":"10.1145/3295500.3356149","CorpusId":85542417},"title":"High Performance Monte Carlo Simulation of Ising Model on TPU Clusters"},{"paperId":"46bb5162da1ad9b3591a9e056550135e7a50bc2b","externalIds":{"MAG":"2963436674","ArXiv":"1903.04611","DBLP":"journals/tpds/LiSCLLTB20","DOI":"10.1109/TPDS.2019.2928289","CorpusId":75136160},"title":"Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect"},{"paperId":"dd0de3977cccff806944d5dc28e053fcd8fdda52","externalIds":{"MAG":"2922527104","DBLP":"journals/corr/abs-1903-06701","ArXiv":"1903.06701","CorpusId":78089762},"title":"Scaling Distributed Machine Learning with In-Network Aggregation"},{"paperId":"a97a38f0c25685b1b486968b2b457f2263af4999","externalIds":{"MAG":"2915767208","DBLP":"conf/fpga/YangWMYA19","ArXiv":"1904.04024","DOI":"10.1145/3289602.3293963","CorpusId":67870359},"title":"SwitchAgg: A Further Step Towards In-Network Computation"},{"paperId":"cc14e2e99ef12b01ecb1e869b46b9eb50e2179bd","externalIds":{"DBLP":"conf/hpca/SongMZQLC19","MAG":"2911148381","ArXiv":"1901.02067","DOI":"10.1109/HPCA.2019.00027","CorpusId":57721133},"title":"HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array"},{"paperId":"9a1093af92d315def21b90918faf08665157051a","externalIds":{"MAG":"2953235111","DBLP":"journals/corr/abs-1812-08011","ArXiv":"1812.08011","CorpusId":53977760},"title":"Training Deep Neural Networks with 8-bit Floating Point Numbers"},{"paperId":"d79a26226393f687ddbc375e32055b40b8ad8d38","externalIds":{"MAG":"2991040477","DBLP":"conf/nips/HuangCBFCCLNLWC19","CorpusId":53670168},"title":"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"},{"paperId":"7f12f77cd04dab6116a6ea1db73bf9425d94d8af","externalIds":{"MAG":"2920668770","CorpusId":67877142},"title":"Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash"},{"paperId":"2270b8628fd8ca67ae39d277f45bc3c38ac63d5f","externalIds":{"ArXiv":"1811.02084","DBLP":"journals/corr/abs-1811-02084","MAG":"2963351145","CorpusId":53236433},"title":"Mesh-TensorFlow: Deep Learning for Supercomputers"},{"paperId":"3c07aa4b70ee763b1c2f4b1e745deb5444dc6de5","externalIds":{"DBLP":"journals/corr/abs-1810-09868","ArXiv":"1810.09868","MAG":"2897852008","CorpusId":53039045},"title":"Automatic Full Compilation of Julia Programs and ML Models to Cloud TPUs"},{"paperId":"0606676f16d581fa453f6b7b8a14fc7c4af8d025","externalIds":{"DBLP":"conf/osdi/XiaoBRSKHPPZZYZ18","MAG":"2899071864","CorpusId":52987896},"title":"Gandiva: Introspective Cluster Scheduling for Deep Learning"},{"paperId":"a82fc0115c1802d48d352b35595204738fad84f0","externalIds":{"MAG":"2884711234","DBLP":"journals/corr/abs-1807-11205","ArXiv":"1807.11205","CorpusId":51876267},"title":"Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes"},{"paperId":"7dd7198bb8a61dd22879068e8c4619b32f0470f8","externalIds":{"MAG":"2952760404","DBLP":"conf/eurosys/WangHL19","ArXiv":"1807.08887","DOI":"10.1145/3302424.3303953","CorpusId":50780068},"title":"Supporting Very Large Models using Automatic Dataflow Graph Partitioning"},{"paperId":"dd97a44fdc5923d1d3fd9d7c3dc300fb6f4f04ed","externalIds":{"MAG":"2804682296","DBLP":"conf/icml/GaoCL18","CorpusId":49419207},"title":"Spotlight: Optimizing Device Placement for Training Deep Neural Networks"},{"paperId":"c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd","externalIds":{"ArXiv":"1806.03377","MAG":"2807147113","DBLP":"journals/corr/abs-1806-03377","CorpusId":47016772},"title":"PipeDream: Fast and Efficient Pipeline Parallel DNN Training"},{"paperId":"2229ac756f89c3db017293918548555734d2f891","externalIds":{"MAG":"2894576937","ArXiv":"1803.03288","DBLP":"conf/mlsys/HashemiJC19","CorpusId":52920970},"title":"TicTac: Accelerating Distributed Deep Learning with Communication Scheduling"},{"paperId":"e2c8726d092aea573e69f5b0a2654225883cfacf","externalIds":{"MAG":"2787998955","DBLP":"journals/corr/abs-1802-05799","ArXiv":"1802.05799","CorpusId":3398835},"title":"Horovod: fast and easy distributed deep learning in TensorFlow"},{"paperId":"3ea088eae8637530d1108065acab244f3b6c280d","externalIds":{"MAG":"2786066748","DBLP":"conf/icml/JiaLQA18","ArXiv":"1802.04924","CorpusId":3619071},"title":"Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks"},{"paperId":"e7fd6848cb29ca221a7e17d823e06fb566f1f135","externalIds":{"DBLP":"journals/corr/abs-1710-03740","ArXiv":"1710.03740","MAG":"2963112338","CorpusId":3297437},"title":"Mixed Precision Training"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"bfbd10ebffc9494423770a5bd30ebd0f9cbce66d","externalIds":{"MAG":"2617411258","DBLP":"journals/corr/MirhoseiniPLSLZ17","ArXiv":"1706.04972","CorpusId":3635282},"title":"Device Placement Optimization with Reinforcement Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"079932bf6ff8b99c899172ba60071818f6b5dfcb","externalIds":{"MAG":"2962758826","DBLP":"conf/usenix/ZhangZXDHLHWXX17","ArXiv":"1706.03292","CorpusId":21882993},"title":"Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters"},{"paperId":"d5ea74717b3176aa4bd2963aae00dfb0406a2a5b","externalIds":{"MAG":"2599132532","DBLP":"conf/hpca/ShpinerHEZGZ17","DOI":"10.1109/HiPINEB.2017.11","CorpusId":1911585},"title":"Dragonfly+: Low Cost Topology for Scaling Datacenters"},{"paperId":"b6c13c6b26b5e39f427727365a1ab59151cb4fdb","externalIds":{"DBLP":"conf/conext/ZhuGMP16","MAG":"2557879050","DOI":"10.1145/2999572.2999593","CorpusId":4491781},"title":"ECN or Delay: Lessons Learnt from Analysis of DCQCN and TIMELY"},{"paperId":"c0c352b314e0d972e7eabd35e435789791d407cc","externalIds":{"DBLP":"conf/sc/GrahamBLRSBGDKK16","MAG":"2563521659","DOI":"10.1109/COM-HPC.2016.6","CorpusId":11771799},"title":"Scalable Hierarchical Aggregation Protocol (SHArP): A Hardware Architecture for Efficient Data Reduction"},{"paperId":"60ddf74dd5b443c3bfb59fe876b42f9d6112c4fb","externalIds":{"MAG":"2498764059","DBLP":"conf/sigcomm/GuoWDSYPL16","DOI":"10.1145/2934872.2934908","CorpusId":14105475},"title":"RDMA over Commodity Ethernet at Scale"},{"paperId":"942deb7d865b7782c03176d95e3a0d56cb71009e","externalIds":{"DBLP":"journals/corr/ChenXZG16","ArXiv":"1604.06174","MAG":"2338908902","CorpusId":15865278},"title":"Training Deep Nets with Sublinear Memory Cost"},{"paperId":"07367703f587dbc3313cc613289c4330cebe5c8c","externalIds":{"DBLP":"journals/ccr/ZhuEFGLLPRYZ15","MAG":"2036003010","DOI":"10.1145/2785956.2787484","CorpusId":134703},"title":"Congestion Control for Large-Scale RDMA Deployments"},{"paperId":"327a02b19a60319cc35be860ad0259a5c1aef920","externalIds":{"DBLP":"journals/ccr/MittalLDBWGVWWZ15","MAG":"1968331080","DOI":"10.1145/2785956.2787510","CorpusId":9676894},"title":"TIMELY: RTT-based Congestion Control for the Datacenter"},{"paperId":"9e9191ff842fd86dbb4ae118bfb831ad8b0c532a","externalIds":{"DBLP":"conf/conext/MaiRACMPW14","MAG":"2143295630","DOI":"10.1145/2674005.2674996","CorpusId":9896031},"title":"NetAgg: Using Middleboxes for Application-specific On-path Aggregation in Data Centres"},{"paperId":"d48e362d8c25179cf05f737d64d070c64c57186b","externalIds":{"DBLP":"conf/cloud/LiGZSS14","MAG":"2038157364","DOI":"10.1145/2670979.2670985","CorpusId":9670003},"title":"Tachyon: Reliable, Memory Speed Storage for Cluster Computing Frameworks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"463e7d2a62b1f15cc2daba34230297827e7c6757","externalIds":{"DBLP":"journals/ccr/BosshartDGIMRSTVVW14","MAG":"2949902779","ArXiv":"1312.1719","DOI":"10.1145/2656877.2656890","CorpusId":9077729},"title":"P4: programming protocol-independent packet processors"},{"paperId":"ff3cb3a14ce0d248cdf4b63d8be3c8b220f36a5f","externalIds":{"MAG":"2215735188","DOI":"10.6084/M9.FIGSHARE.791563.V5","CorpusId":109999892},"title":"The MVAPICH Project: Evolution and Sustainability of an Open Source Production Quality MPI Library for HPC"},{"paperId":"62c76ca0b2790c34e85ba1cce09d47be317c7235","externalIds":{"ArXiv":"1308.3432","DBLP":"journals/corr/BengioLC13","MAG":"2242818861","CorpusId":18406556},"title":"Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"},{"paperId":"4d23db55e6671a82c95dacec33b2967a4b8b677d","externalIds":{"MAG":"2055312318","DBLP":"conf/pldi/Ragan-KelleyBAPDA13","DOI":"10.1145/2491956.2462176","CorpusId":5885207},"title":"Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines"},{"paperId":"b0c6c04c406daa61e61016c446d81bbb4b2cd5d3","externalIds":{"MAG":"2132320636","DBLP":"conf/infocom/DixitPHK13","DOI":"10.1109/INFCOM.2013.6567015","CorpusId":5560034},"title":"On the impact of packet spraying in data center networks"},{"paperId":"e764048eb0d28e0351584475e9c44c215fce76d3","externalIds":{"DBLP":"conf/nsdi/CostaDRO12","MAG":"1536639265","CorpusId":13415978},"title":"Camdoop: Exploiting In-network Aggregation for Big Data Applications"},{"paperId":"3ffae44b736b6dbd6d715977bb6381297dd94304","externalIds":{"MAG":"201315547","DBLP":"conf/europar/SolomonikD11","DOI":"10.1007/978-3-642-23397-5_10","CorpusId":13931921},"title":"Communication-Optimal Parallel 2.5D Matrix Multiplication and LU Factorization Algorithms"},{"paperId":"5b8bcb3d2b8e2f4111c61003abbfbb5963187870","externalIds":{"DBLP":"journals/corr/abs-1110-1687","ArXiv":"1110.1687","MAG":"2567769116","CorpusId":6988673},"title":"Jellyfish: Networking Data Centers Randomly"},{"paperId":"677f55089039398ad9f23288fd89e77ff8380de3","externalIds":{"DBLP":"journals/ife/ShainerALLKTSC11","MAG":"1983144994","DOI":"10.1007/s00450-011-0157-1","CorpusId":22771157},"title":"The development of Mellanox/NVIDIA GPUDirect over InfiniBanda new model for GPU to GPU communications"},{"paperId":"8ce4c0ee315d86f32ec7354ccdf8d8996e8ee270","externalIds":{"MAG":"2614670623","DBLP":"conf/mss/ShvachkoKRC10","DOI":"10.1109/MSST.2010.5496972","CorpusId":13925042},"title":"The Hadoop Distributed File System"},{"paperId":"fe33ba23625e0039b6bddf69a63f43dfe22928b1","externalIds":{"MAG":"2126210439","DBLP":"conf/sigcomm/GuoLLWZSTZL09","DOI":"10.1145/1592568.1592577","CorpusId":991482},"title":"BCube: a high performance, server-centric network architecture for modular data centers"},{"paperId":"6f4e48c2a5de9337d147ebbb7d0ff0e555adceca","externalIds":{"DBLP":"journals/jpdc/PatarasukY09","MAG":"2057332538","DOI":"10.1016/j.jpdc.2008.09.002","CorpusId":7433454},"title":"Bandwidth optimal all-reduce algorithms for clusters of workstations"},{"paperId":"bd2e28376aca5a8ef1cf6068a3aeb1ca6d0e1abd","externalIds":{"DBLP":"conf/sigcomm/GuoWTSZL08","MAG":"2143065961","DOI":"10.1145/1402958.1402968","CorpusId":5396194},"title":"Dcell: a scalable and fault-tolerant network structure for data centers"},{"paperId":"b81350c2601441dcd526e656a590ff354dedb220","externalIds":{"MAG":"2155187164","DBLP":"conf/isca/KimDSA08","DOI":"10.1145/1394608.1382129","CorpusId":79473},"title":"Technology-Driven, Highly-Scalable Dragonfly Topology"},{"paperId":"c5de8202da04a88a0122ac2c21780cd9ec3db43b","externalIds":{"DBLP":"conf/osdi/WeilBMLM06","MAG":"1999984505","CorpusId":5159428},"title":"Ceph: a scalable, high-performance distributed file system"},{"paperId":"b27542559edf6cc867afdc1637fe36b9a4e250e6","externalIds":{"MAG":"1825216778","DBLP":"conf/pvm/GabrielFBADSSKBLCDGW04","DOI":"10.1007/978-3-540-30218-6_19","CorpusId":12789460},"title":"Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation"},{"paperId":"ebc09b04a900afc6c3cf53a4b7ff6035f33f02b2","externalIds":{"MAG":"1670978437","DBLP":"conf/pvm/Gropp02","DOI":"10.1007/3-540-45825-5_5","CorpusId":45970576},"title":"MPICH2: A New Start for MPI Implementations"},{"paperId":"cfa5b563d50efc83271883407c98b73a39f959d6","externalIds":{"DBLP":"journals/rfc/rfc2992","MAG":"2169246522","DOI":"10.17487/RFC2992","CorpusId":36760450},"title":"Analysis of an Equal-Cost Multi-Path Algorithm"},{"paperId":"1aa8ad634d1879af9b5ac34b44ecc3de8debd276","externalIds":{"MAG":"2012652661","DBLP":"journals/ibmrd/AgarwalBGJP95","DOI":"10.1147/rd.395.0575","CorpusId":11070208},"title":"A three-dimensional approach to parallel matrix multiplication"},{"paperId":"6326e8ab3d2c2b179c26b1aa368fae7e42a34ed0","externalIds":{"MAG":"2056999868","DBLP":"journals/concurrency/GeijnW97","DOI":"10.1002/(SICI)1096-9128(199704)9:4%3C255::AID-CPE250%3E3.0.CO;2-2","CorpusId":611285},"title":"SUMMA: scalable universal matrix multiplication algorithm"},{"paperId":"c8d90974c3f3b40fa05e322df2905fc16204aa56","externalIds":{"DBLP":"journals/neco/JacobsJNH91","MAG":"2150884987","DOI":"10.1162/neco.1991.3.1.79","CorpusId":572361,"PubMed":"31141872"},"title":"Adaptive Mixtures of Local Experts"},{"paperId":"fa5c7464479bd56f6d4c2c6f42463d37a65686da","externalIds":{"MAG":"2295787458","DOI":"10.1109/C-M.1981.220288","CorpusId":7246181},"title":"Interconnection Networks"},{"paperId":"148c1c241f4b4bd3848b6c0d6ff99e90a534dfce","externalIds":{"MAG":"2096313289","DOI":"10.1002/J.1538-7305.1953.TB01433.X","CorpusId":111089194},"title":"A study of non-blocking switching networks"},{"paperId":"5271353b3eeccd342536818b59105d04d70942da","externalIds":{"DBLP":"conf/usenix/YuanLYZTCSZ24","CorpusId":271217669},"title":"Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism"},{"paperId":"1b5de61ad031cd382436829b6dbd3f47f26dbde1","externalIds":{"DBLP":"conf/nsdi/ZuGDT0HBKRDLWWL24","CorpusId":269240234},"title":"Resiliency at Scale: Managing Google's TPUv4 Machine Learning Supercomputer"},{"paperId":"dcf97a6c0f7947aaee1912f3f1ac7c8806703f95","externalIds":{"DBLP":"journals/corr/abs-2406-18820","DOI":"10.48550/arXiv.2406.18820","CorpusId":270764954},"title":"Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training"},{"paperId":"d8a144b524524091006ea64e77e402e1740a3252","externalIds":{"DBLP":"conf/nsdi/HuangZZQ024","CorpusId":269240072},"title":"DISTMM: Accelerating Distributed Multimodal Model Training"},{"paperId":"353539616338018f4a381b685e719fbf64aba37c","externalIds":{"DBLP":"journals/corr/abs-2407-00611","DOI":"10.48550/arXiv.2407.00611","CorpusId":270870290},"title":"WallFacer: Guiding Transformer Model Training Out of the Long-Context Dark Forest with N-body Problem"},{"paperId":"9deda73da446cbb21550abaa62b41f5a3acd92fc","externalIds":{"DBLP":"journals/corr/abs-2405-11143","DOI":"10.48550/arXiv.2405.11143","CorpusId":269921667},"title":"OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"},{"paperId":"9540e390fa481fa4130733445f240365c22c2367","externalIds":{"DBLP":"conf/nsdi/0116T0WXZ0JW024","CorpusId":269239999},"title":"Towards Domain-Specific Network Transport for Distributed DNN Training"},{"paperId":"adf7b1ed6b4d4ae346888278b7496d55e8e3f13f","externalIds":{"DBLP":"conf/usenix/LeiJZHYZ24","CorpusId":271217708},"title":"PUZZLE: Efficiently Aligning Large Language Models through Light-Weight Context Switch"},{"paperId":"b1d674e82f7579fdf2fb8de1e772ee8610bc2034","externalIds":{"DBLP":"conf/osdi/LinMZ0Z0MCSYXYZ24","CorpusId":271217661},"title":"nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training"},{"paperId":"e51578b433b7cf7cb92febd4fcd8b78035f08ff9","externalIds":{"DBLP":"conf/mlsys/Lamy-Poirier23","DOI":"10.48550/arXiv.2211.05953","CorpusId":253122107},"title":"Breadth-First Pipeline Parallelism"},{"paperId":"aa6ddad0a84eaa004e49142981d05c5f36cc585e","externalIds":{"DBLP":"conf/sc/LiuCZ023","ArXiv":"2308.15762","DOI":"10.1145/3581784.3607073","CorpusId":261339639},"title":"Hanayo: Harnessing Wave-Like Pipeline Parallelism for Enhanced Large Model Training Efficiency"},{"paperId":"33489921dac9fed095332ea13326043c9911a6d3","externalIds":{"DBLP":"conf/osdi/00010XMXMG0Z23","CorpusId":259858911},"title":"Welder: Scheduling Deep Learning Memory Access via Tile-graph"},{"paperId":"283e56e843edc2ab599b0cf8218b936a89a11875","externalIds":{"DBLP":"conf/icml/KimKYC23","CorpusId":260810146},"title":"BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models"},{"paperId":"7889cc4e9565c9e7bd725509d7da4597e6a9b576","externalIds":{"DBLP":"conf/icml/WangLYCLSRZ23","CorpusId":259265981},"title":"CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks"},{"paperId":"6597b53ea7f7c1909870c9be6ed54695ec36d3b6","externalIds":{"DBLP":"conf/osdi/Hu0ZC00023","CorpusId":259858879},"title":"Hydro: Surrogate-Based Hyperparameter Tuning Service in Datacenters"},{"paperId":"cb542f525a3a91bc8d0d3fe69235f59e75822fd4","externalIds":{"DBLP":"conf/nsdi/MahajanC0A23","CorpusId":253648715},"title":"Better Together: Jointly Optimizing ML Collective Scheduling and Execution Planning using SYNDICATE"},{"paperId":"f4945c90c509f9fcc3a416371d4c75c110cbcb66","externalIds":{"DBLP":"conf/usenix/ChoiKAJK23","CorpusId":259858770},"title":"EnvPipe: Performance-preserving DNN Training Framework for Saving Energy"},{"paperId":"adc8b62fd2bd644c140c7c42275a9d2d913ad8a8","externalIds":{"DBLP":"conf/nips/LiuA23","CorpusId":266351737},"title":"Blockwise Parallel Transformers for Large Context Models"},{"paperId":"ac771182d1780c863954243809d1e144433919f9","externalIds":{"DBLP":"journals/corr/abs-2307-12966","DOI":"10.48550/arXiv.2307.12966","CorpusId":260356605},"title":"Aligning Large Language Models with Human: A Survey"},{"paperId":"8f4e626c2783b0fada28d206c3584ed6674ba53f","externalIds":{"DBLP":"conf/usenix/WengYY0TYZ23","CorpusId":259859102},"title":"Beware of Fragmentation: Scheduling GPU-Sharing Workloads with Fragmentation Gradient Descent"},{"paperId":"5c6a17850c9ad6bf6dc8992ec598cd932ce42208","externalIds":{"DBLP":"conf/usenix/ZhaiHMZZZ23","CorpusId":259858759},"title":"SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization"},{"paperId":"ee2c6440d67a7193d4aec6c51f5eb535f637a45b","externalIds":{"DBLP":"conf/europar/DreuningBN22","DOI":"10.1007/978-3-031-12597-3_10","CorpusId":251286860},"title":"mCAP: Memory-Centric Partitioning for Large-Scale Pipeline-Parallel DNN Training"},{"paperId":"7250b2053d05687664029c29c2f1225b48c7ee45","externalIds":{"DBLP":"conf/osdi/ZhuWDKLZXMXC0YZ22","CorpusId":266160403},"title":"ROLLER: Fast and Efficient Tensor Compilation for Deep Learning"},{"paperId":"08041ba871ab80f2f63eddd5ae6f0b6443c0306f","externalIds":{"DBLP":"conf/usenix/He00022","CorpusId":267825505},"title":"Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training"},{"paperId":"da37fb0a9071d1ceda0c7e359a049c6c7e627a01","externalIds":{"DBLP":"conf/osdi/UngerJ0LBNRPMML22","CorpusId":267834645},"title":"Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization"},{"paperId":"ca1402619a80c140c650961d899319c2928744f0","externalIds":{"DBLP":"conf/nips/TarnawskiNP21","CorpusId":244711821},"title":"Piper: Multidimensional Planner for DNN Parallelization"},{"paperId":"d87ceea3f5d0582997840a107f2bd6c5a1aadbde","externalIds":{"DBLP":"conf/mlsys/GebaraGC21","CorpusId":233293424},"title":"In-network Aggregation for Shared Machine Learning Clusters"},{"paperId":"f99dc2d2f20082e7638c6ad31503df7ab0b59828","externalIds":{"DBLP":"conf/fast/PanSZSZSPSWGCPS21","CorpusId":232225564},"title":"Facebook's Tectonic Filesystem: Efficiency from Exascale"},{"paperId":"c90a5375329ea537d4ac266297b1c5cca89af19b","externalIds":{"DBLP":"conf/nsdi/LaoLMCWAS21","CorpusId":233733017},"title":"ATP: In-network Aggregation for Multi-tenant Learning"},{"paperId":"ff1b068b21ff4b721672c42f08476659a123020e","externalIds":{"DBLP":"conf/fast/MohanPC21","CorpusId":233486931},"title":"CheckFreq: Frequent, Fine-Grained DNN Checkpointing"},{"paperId":"57bc3e71a889013981b191b6431bec3dc45eba9f","externalIds":{"MAG":"3096956001","DBLP":"conf/osdi/XiaoRLZHLFLJ20","CorpusId":229197535},"title":"AntMan: Dynamic Scaling on GPU Clusters for Deep Learning"},{"paperId":"e6cc6a7bd4db3e7604bae6a654ec29aa8542dafc","externalIds":{"MAG":"2970821029","DBLP":"conf/nips/SunCCWVSCZG19","CorpusId":202779157},"title":"Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks"},{"paperId":"8d2d560bf1c4c6930d2d1411e48b642bd5b81179","externalIds":{"DBLP":"conf/nsdi/GuCSZJQLG19","MAG":"2919594608","CorpusId":73725044},"title":"Tiresias: A GPU Cluster Manager for Distributed Deep Learning"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"ec3071fb918ad69ec80df1ca9cf1fdeb386a9603","externalIds":{"CorpusId":3296374},"title":"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"},{"paperId":"f5121d0f20ab249d16e8f3473f9179cbe5499ef1","externalIds":{"CorpusId":218476617},"title":"NVIDIA DGX-1 System Architecture White paper"},{"paperId":"551adcf26986bfa27af744b588dbaaff20c7932f","externalIds":{"MAG":"26797728","CorpusId":59635805},"title":"Lustre: Building a File System for 1,000-node Clusters"},{"paperId":"8b52bb4cb8a5ae2ffb573ba33ccc5455069be910","externalIds":{"MAG":"203122272","DBLP":"conf/cmg/Pentakalos02","CorpusId":10948428},"title":"An Introduction to the InfiniBand Architecture"},{"paperId":"162d958ff885f1462aeda91cd72582323fd6a1f4","externalIds":{"MAG":"2112796928","DBLP":"journals/pieee/LeCunBBH98","DOI":"10.1109/5.726791","CorpusId":14542261},"title":"Gradient-based learning applied to document recognition"},{"paperId":"a40614033be2c4f6dd17bcd420a044f6381874c9","externalIds":{"MAG":"206141488","CorpusId":58052736},"title":"MPI: A message - passing interface standard"},{"paperId":"f9c2ee112cd9fa53ace7b75393cdd8baa72f678e","externalIds":{"MAG":"1598993252","CorpusId":60822897},"title":"A cellular computer to implement the kalman filter algorithm"},{"paperId":"1d27a56a8133f947a5a0217b00241d26f585f834","externalIds":{"CorpusId":246411250},"title":"This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."},{"paperId":"ceecad72381aca9dae1ab24c46f014301dfe8b44","externalIds":{"CorpusId":249394456},"title":"This paper is included in the Proceedings of the 2022 USENIX Annual Technical Conference. Whale: Efficient Giant Model Training over Heterogeneous GPUs"},{"paperId":"5af975364d0a228e316278575cadbe41f03b48e4","externalIds":{"CorpusId":251950092},"title":"This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation. Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters"},{"paperId":"4954fa180728932959997a4768411ff9136aac81","externalIds":{"DBLP":"journals/corr/AbadiBCCDDDGIIK16","MAG":"2402144811","ArXiv":"1605.08695","CorpusId":6287870},"title":"TensorFlow: A system for large-scale machine learning"},{"paperId":"684d6a042808a97b6779336f3e3b06a34b1a6a6d","externalIds":{"CorpusId":249070344},"title":"This paper is included in the Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation."}]}