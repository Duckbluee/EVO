{"abstract":"Estimating probability distribution is one of the core issues in the NLP field. \n\nHowever, in both deep learning (DL) and pre-DL eras, unlike the vast applications of linear-chain CRF in sequence labeling tasks, very few works have applied tree-structure CRF to constituency parsing, mainly due to the complexity and inefficiency of the inside-outside algorithm.\n\nThis work presents a fast and accurate neural CRF constituency parser. \n\nThe key idea is to batchify the inside algorithm for loss computation by direct large tensor operations on GPU, and meanwhile avoid the outside algorithm for gradient computation via efficient back-propagation.\n\nWe also propose a simple two-stage bracketing-then-labeling parsing approach to improve efficiency further.\n\nTo improve the parsing performance, inspired by recent progress in dependency parsing, we introduce a new scoring architecture based on boundary representation and biaffine attention, and a beneficial dropout strategy.\n\nExperiments on PTB, CTB5.1, and CTB7 show that our two-stage CRF parser achieves new state-of-the-art performance on both settings of w/o and w/ BERT, and can parse over 1,000 sentences per second.\n\nWe release our code at https://github.com/yzhangcs/crfpar."}