{"references":[{"paperId":"ba2890f012966c69e1399c8a685ee10296a3663f","externalIds":{"DBLP":"journals/ijhci/ZhangYL25","ArXiv":"2309.13965","DOI":"10.1080/10447318.2024.2364986","CorpusId":262464702},"title":"May I Ask a Follow-up Question? Understanding the Benefits of Conversations in Neural Network Explainability"},{"paperId":"7b5cf0af000639de2f3ad16f83b145a87b3d015a","externalIds":{"DBLP":"conf/emnlp/ZhangLLZWYMC22","ArXiv":"2302.00907","DOI":"10.48550/arXiv.2302.00907","CorpusId":256503488},"title":"History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System"},{"paperId":"acab728e7827b0cf7179cc44457b95ff27600c71","externalIds":{"ArXiv":"2301.06989","DBLP":"conf/ijcai/0081PLQZ23","DOI":"10.48550/arXiv.2301.06989","CorpusId":255942778},"title":"Negative Flux Aggregation to Estimate Feature Attributions"},{"paperId":"e4037c51b4bc29d060239a799abe1f914b94f7de","externalIds":{"DBLP":"journals/tkdd/WangWLM23","DOI":"10.1145/3578586","CorpusId":246290213},"title":"Summarizing User-Item Matrix By Group Utility Maximization"},{"paperId":"be0fbb810583930c071d0b9b2c5187fe260783f5","externalIds":{"ArXiv":"2111.09883","DBLP":"journals/corr/abs-2111-09883","DOI":"10.1109/CVPR52688.2022.01170","CorpusId":244346076},"title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"paperId":"f3632a7f829a9686c6f6b57131d2d1e62f798dac","externalIds":{"DBLP":"journals/corr/abs-2111-07668","ArXiv":"2111.07668","CorpusId":244117719},"title":"Fast Axiomatic Attribution for Neural Networks"},{"paperId":"c37b8a7030a013e7aec39e85f0f6f6cc86590970","externalIds":{"DBLP":"journals/tamd/RohlfingCSMBBEG21","MAG":"3111207939","DOI":"10.1109/TCDS.2020.3044366","CorpusId":234243569},"title":"Explanation as a Social Practice: Toward a Conceptual Framework for the Social Design of AI Systems"},{"paperId":"89ed1a2b3da80ff645a0bda32cdeef0d7d68386c","externalIds":{"DBLP":"conf/ijcai/Pan0Z21","DOI":"10.24963/ijcai.2021/396","CorpusId":237100716},"title":"Explaining Deep Neural Network Models with Adversarial Gradient Integration"},{"paperId":"dc32a984b651256a8ec282be52310e6bd33d9815","externalIds":{"PubMedCentral":"8371605","DOI":"10.1038/s41586-021-03819-2","CorpusId":235959867,"PubMed":"34265844"},"title":"Highly accurate protein structure prediction with AlphaFold"},{"paperId":"339b2b711fb5b228d097b03ebc3e62a521779235","externalIds":{"DBLP":"conf/acl/ZakenGR22","ACL":"2022.acl-short.1","ArXiv":"2106.10199","DOI":"10.18653/v1/2022.acl-short.1","CorpusId":231672601},"title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"},{"paperId":"a763be874960999ad5ce9a3830b5eecaa50126b8","externalIds":{"DBLP":"journals/corr/abs-2106-09788","ArXiv":"2106.09788","DOI":"10.1109/CVPR46437.2021.00501","CorpusId":235485504},"title":"Guided Integrated Gradients: an Adaptive Path Method for Removing Noise"},{"paperId":"a9d7c56aeec7d7a898e422bccecd931b1c5f3ea7","externalIds":{"ArXiv":"2103.13533","DBLP":"journals/corr/abs-2103-13533","MAG":"3138271062","CorpusId":232352493},"title":"Symmetry-Preserving Paths in Integrated Gradients"},{"paperId":"60c1ad9bde2306ae3b577abb540c08907ae35fd4","externalIds":{"DBLP":"conf/icml/WangFD22","ArXiv":"2103.11257","CorpusId":238408266},"title":"Robust Models Are More Interpretable Because Attributions Look Normal"},{"paperId":"552b456fab9eebcb80374e6937bb8e6ee98c74ce","externalIds":{"ArXiv":"2102.05228","DBLP":"conf/iccv/JungO21","DOI":"10.1109/iccv48922.2021.00137","CorpusId":235658595},"title":"Towards Better Explanations of Class Activation Mapping"},{"paperId":"d55705658df55e13709518a1e65247224349ffb9","externalIds":{"MAG":"3081876271","DBLP":"journals/ijon/LiangLYLJ21","DOI":"10.1016/j.neucom.2020.08.011","CorpusId":224899505},"title":"Explaining the black-box model: A survey of local interpretation methods for deep neural networks"},{"paperId":"7f05d62928e9d316e5375067f673a590836cd43d","externalIds":{"ArXiv":"2012.03434","DBLP":"conf/aaai/NamCL21","MAG":"3111581355","DOI":"10.1609/aaai.v35i13.17380","CorpusId":227335282},"title":"Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations"},{"paperId":"09a33ec2f03c3e083e26edd0a8c1f8a7def6ca1d","externalIds":{"DBLP":"journals/corr/abs-2011-04917","ArXiv":"2011.04917","MAG":"3105449709","DOI":"10.1145/3461702.3462597","CorpusId":226289857},"title":"Towards Unifying Feature Attribution and Counterfactual Explanations: Different Means to the Same End"},{"paperId":"6936f7e97ed4a587fcb347d0636f1e5cbcadf227","externalIds":{"DBLP":"journals/corr/abs-2010-12697","ArXiv":"2010.12697","MAG":"3094084754","CorpusId":225068956},"title":"Investigating Saturation Effects in Integrated Gradients"},{"paperId":"f7b3e4b5a8e15585d09398a056bdf0b43bed68ca","externalIds":{"MAG":"3093901252","ACL":"2020.findings-emnlp.24","DBLP":"conf/emnlp/WangTW020","ArXiv":"2010.05419","DOI":"10.18653/v1/2020.findings-emnlp.24","CorpusId":222290633},"title":"Gradient-based Analysis of NLP Models is Manipulable"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"ccea6c716c014ab3fb28d142f7e7528fb2741558","externalIds":{"MAG":"3015432706","DBLP":"conf/cvpr/XuVS20","ArXiv":"2004.03383","DOI":"10.1109/cvpr42600.2020.00970","CorpusId":215238396},"title":"Attribution in Scale and Space"},{"paperId":"798ea191aad9401462b405fde1a6cefb4fe53fd5","externalIds":{"DBLP":"journals/corr/abs-2002-04138","ArXiv":"2002.04138","MAG":"3005535506","CorpusId":211075890},"title":"Explaining Explanations: Axiomatic Feature Interactions for Deep Networks"},{"paperId":"5ca973f48d42ab33f798bee42f2af74f9cdd8d26","externalIds":{"MAG":"3006064320","DOI":"10.23915/distill.00022","CorpusId":219874086},"title":"Visualizing the Impact of Feature Attribution Baselines"},{"paperId":"3a083d843f891b3574494c385699c21766ce8b7a","externalIds":{"DBLP":"journals/nature/Senior0JKSGQZNB20","MAG":"2999044305","DOI":"10.1038/s41586-019-1923-7","CorpusId":210221987,"PubMed":"31942072"},"title":"Improved protein structure prediction using potentials from deep learning"},{"paperId":"b0c34618ffd1154f35863e2ce7250ac6b6f2c424","externalIds":{"MAG":"2999362542","DOI":"10.1201/9780367816377-16","CorpusId":209379623},"title":"Interpretable Machine Learning"},{"paperId":"530a059cb48477ad1e3d4f8f4b153274c8997332","externalIds":{"ArXiv":"1910.10045","MAG":"2997428643","DBLP":"journals/inffus/ArrietaRSBTBGGM20","DOI":"10.1016/j.inffus.2019.12.012","CorpusId":204824113},"title":"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI"},{"paperId":"2e257b5f0a0148076e9161202880b022e521ac59","externalIds":{"MAG":"3035253074","ArXiv":"1910.01279","DBLP":"conf/cvpr/WangWDYZDMH20","DOI":"10.1109/CVPRW50498.2020.00020","CorpusId":215745726},"title":"Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks"},{"paperId":"d0919af373ebff5fbb59f53fea17db2fbf5f7e90","externalIds":{"ArXiv":"1910.08485","MAG":"2981149872","DBLP":"journals/corr/abs-1910-08485","DOI":"10.1109/ICCV.2019.00304","CorpusId":201624703},"title":"Understanding Deep Networks via Extremal Perturbations and Smooth Masks"},{"paperId":"ef7483b92959938cd736496c3d8f2a89f04e5c2c","externalIds":{"MAG":"3012430016","DBLP":"journals/corr/abs-1908-04351","ArXiv":"1908.04351","DOI":"10.1109/ICCVW.2019.00513","CorpusId":199552111},"title":"Explaining Convolutional Neural Networks using Softmax Gradient Layer-wise Relevance Propagation"},{"paperId":"d7cb9361d671ac6edaadaef4273c190e8913d86a","externalIds":{"ArXiv":"1908.01224","MAG":"2965696320","DBLP":"journals/corr/abs-1908-01224","CorpusId":199442547},"title":"Smooth Grad-CAM++: An Enhanced Inference Level Visualization Technique for Deep Convolutional Neural Network Models"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"46c266b3d1274dacd7fce27ee8cb4d587f087a58","externalIds":{"MAG":"2964303497","DOI":"10.3390/ELECTRONICS8080832","CorpusId":199659548},"title":"Machine Learning Interpretability: A Survey on Methods and Metrics"},{"paperId":"38f23fe236b152cd4983c8f30d305a568afd0d3e","externalIds":{"ArXiv":"1907.07374","DBLP":"journals/tnn/TjoaG21","MAG":"3094189037","DOI":"10.1109/TNNLS.2020.3027314","CorpusId":197430935,"PubMed":"33079674"},"title":"A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI"},{"paperId":"faf3a22627f198ce56671b6f8a9b3d5cc3164a91","externalIds":{"MAG":"2981998613","ArXiv":"1906.02825","DBLP":"conf/iccv/KapishnikovBVT19","DOI":"10.1109/ICCV.2019.00505","CorpusId":201125339},"title":"XRAI: Better Attributions Through Regions"},{"paperId":"cb2d9b2f171da67f7b47ac3e0eb935a0de223354","externalIds":{"DBLP":"conf/cvpr/PopeKRMH19","MAG":"2979481854","DOI":"10.1109/CVPR.2019.01103","CorpusId":198904065},"title":"Explainability Methods for Graph Convolutional Neural Networks"},{"paperId":"08c6cfc5632a8e7884c661f7cdbea2680bc1a3ec","externalIds":{"MAG":"2914103174","DBLP":"conf/icml/WangZB19","CorpusId":53387762},"title":"Bias Also Matters: Bias Attribution for Deep Neural Network Explanation"},{"paperId":"0e09d47723cccf0a9bad7cb1badaa367ea37fe14","externalIds":{"MAG":"2971620415","ArXiv":"1905.00780","DBLP":"conf/nips/SrinivasF19","CorpusId":202539049},"title":"Full-Gradient Representation for Neural Network Visualization"},{"paperId":"1c9932feb06a3b873361a3318183396214fba4b5","externalIds":{"MAG":"2964612343","ArXiv":"1905.00954","DBLP":"journals/corr/abs-1905-00954","DOI":"10.1609/AAAI.V34I07.6863","CorpusId":145049740},"title":"Visualizing Deep Networks by Optimizing with Integrated Gradients"},{"paperId":"9bc0a4892e33dfe7ae57f9b5c99fb99c54e23557","externalIds":{"DBLP":"conf/iccvw/KimSJKCJ19","ArXiv":"1902.04893","MAG":"2913823268","DOI":"10.1109/ICCVW.2019.00510","CorpusId":61153475},"title":"Why are Saliency Maps Noisy? Cause of and Solution to Noisy Saliency Maps"},{"paperId":"dc016a1c0317b8e4b01716b570e700549eafc43a","externalIds":{"MAG":"2959587146","DBLP":"conf/aies/LakkarajuKCL19","DOI":"10.1145/3306618.3314229","CorpusId":84839090},"title":"Faithful and Customizable Explanations of Black Box Models"},{"paperId":"cbdb103a298c20644553c201c9dfb0399b552b98","externalIds":{"DBLP":"conf/accv/GuYT18","ArXiv":"1812.02100","MAG":"2952176262","DOI":"10.1007/978-3-030-20893-6_8","CorpusId":54459591},"title":"Understanding Individual Decisions of CNNs via Contrastive Backpropagation"},{"paperId":"8dc8f3e0127adc6985d4695e9b69d04717b2fde8","externalIds":{"MAG":"2891612330","ArXiv":"1810.03292","DBLP":"journals/corr/abs-1810-03292","CorpusId":52938797},"title":"Sanity Checks for Saliency Maps"},{"paperId":"21dff47a4142445f83016da0819ffe6dd2947f66","externalIds":{"MAG":"2891503716","DBLP":"journals/access/AdadiB18","DOI":"10.1109/ACCESS.2018.2870052","CorpusId":52965836},"title":"Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)"},{"paperId":"e403f874e94be26bd02299917c1c1551005766e0","externalIds":{"MAG":"2887252986","ArXiv":"1807.07506","DBLP":"conf/nips/DhurandharSLO18","CorpusId":49875174},"title":"Improving Simple Models with Confidence Profiles"},{"paperId":"8a4de50cca1675737990002f32aa5c62621a115a","externalIds":{"DBLP":"conf/fat/MilliSDH19","MAG":"2963560987","ArXiv":"1807.05185","DOI":"10.1145/3287560.3287562","CorpusId":49741763},"title":"Model Reconstruction from Model Explanations"},{"paperId":"5f614777d25efd14b7426e99cb2544f2d6be133e","externalIds":{"MAG":"2970447476","DBLP":"conf/nips/HookerEKK19","ArXiv":"1806.10758","CorpusId":202782699},"title":"A Benchmark for Interpretability Methods in Deep Neural Networks"},{"paperId":"d00c7fc5201405d5411b5ad3da93c5575ce8f10e","externalIds":{"DBLP":"journals/corr/abs-1806-07421","ArXiv":"1806.07421","MAG":"2950772592","CorpusId":49324724},"title":"RISE: Randomized Input Sampling for Explanation of Black-box Models"},{"paperId":"465eff021e837a23881e4a779dbce769c9ba3553","externalIds":{"ArXiv":"1806.03000","DBLP":"journals/corr/abs-1806-03000","MAG":"2806219033","CorpusId":47009499},"title":"Noise-adding Methods of Saliency Map as Series of Higher Order Partial Derivative"},{"paperId":"d7701e78e0bfc92b03a89582e80cfb751ac03f26","externalIds":{"MAG":"2953315354","ArXiv":"1806.00069","DBLP":"conf/dsaa/GilpinBYBSK18","DOI":"10.1109/DSAA.2018.00018","CorpusId":59600034},"title":"Explaining Explanations: An Overview of Interpretability of Machine Learning"},{"paperId":"fd44d398b2945b4c20da8ec3cc32becd5e08100e","externalIds":{"ArXiv":"1805.10820","DBLP":"journals/corr/abs-1805-10820","MAG":"2803532212","CorpusId":44063479},"title":"Local Rule-Based Explanations of Black Box Decision Systems"},{"paperId":"d2d79513f32c4d09b6255b18514d7ad07ebf43fe","externalIds":{"DBLP":"conf/mipro/DosilovicBH18","MAG":"2809925683","DOI":"10.23919/MIPRO.2018.8400040","CorpusId":49560076},"title":"Explainable artificial intelligence: A survey"},{"paperId":"2a0e4eeea84f6352a6ac012f10581e5a228b6a92","externalIds":{"MAG":"2795530988","DBLP":"conf/chi/AbdulVWLK18","DOI":"10.1145/3173574.3174156","CorpusId":5063596},"title":"Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda"},{"paperId":"f7325d232c7ac7d2daaf6605377058db5b5b83cc","externalIds":{"MAG":"2951278035","DBLP":"journals/csur/GuidottiMRTGP19","ArXiv":"1802.01933","DOI":"10.1145/3236009","CorpusId":3342225},"title":"A Survey of Methods for Explaining Black Box Models"},{"paperId":"e6705dd6a795608dda47d0d64a2fc01bc3facd74","externalIds":{"DBLP":"journals/corr/abs-1802-00682","MAG":"2785327160","ArXiv":"1802.00682","CorpusId":3652280},"title":"How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation"},{"paperId":"44cc9fa35da326ecafb02e9bfbd2fbd593b5a43b","externalIds":{"MAG":"2963214037","DBLP":"journals/tvcg/HohmanKPC19","ArXiv":"1801.06889","DOI":"10.1109/TVCG.2018.2843369","CorpusId":9168766,"PubMed":"29993551"},"title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers"},{"paperId":"4aa65bbc0da30f1f761722f5db7d3bffd49ea29b","externalIds":{"DBLP":"conf/chi/HaraAMSCB18","MAG":"2952172018","ArXiv":"1712.05796","DOI":"10.1145/3173574.3174023","CorpusId":5040507},"title":"A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk"},{"paperId":"682b9d2212258fd5edbfca589c86390c31a956b0","externalIds":{"ArXiv":"1711.11279","MAG":"2796885425","DBLP":"conf/icml/KimWGCWVS18","CorpusId":51737170},"title":"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"},{"paperId":"2c1b79a13087a8e9bc2a4446384145e6f85d4820","externalIds":{"MAG":"2899960873","ArXiv":"1710.11063","DBLP":"conf/wacv/ChattopadhyaySH18","DOI":"10.1109/WACV.2018.00097","CorpusId":13678776},"title":"Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks"},{"paperId":"96cf326c91adc96aa59382bf71fbead29ea34c52","externalIds":{"ArXiv":"1710.10547","MAG":"2766912318","DBLP":"journals/corr/abs-1710-10547","DOI":"10.1609/aaai.v33i01.33013681","CorpusId":22172746},"title":"Interpretation of Neural Networks is Fragile"},{"paperId":"58e0ca33ae3068fee7005f339bf6c444fc17d55f","externalIds":{"MAG":"2953012738","ArXiv":"1708.08296","DBLP":"journals/corr/abs-1708-08296","CorpusId":4376915},"title":"Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models"},{"paperId":"f538dca4def5167a32fbc12107b69a05f0c9d832","externalIds":{"DBLP":"journals/corr/SmilkovTKVW17","MAG":"2626639386","ArXiv":"1706.03825","CorpusId":11695878},"title":"SmoothGrad: removing noise by adding noise"},{"paperId":"7380e343dd4547e21d5118b16daf03d021d98c4e","externalIds":{"MAG":"2950438205","DBLP":"conf/iccv/FongV17","ArXiv":"1704.03296","DOI":"10.1109/ICCV.2017.371","CorpusId":1633753},"title":"Interpretable Explanations of Black Boxes by Meaningful Perturbation"},{"paperId":"1a2118bed729579528deb51e745d58dd3629baf6","externalIds":{"MAG":"2952688545","ArXiv":"1704.02685","DBLP":"conf/icml/ShrikumarGK17","CorpusId":3385018},"title":"Learning Important Features Through Propagating Activation Differences"},{"paperId":"f302e136c41db5de1d624412f68c9174cf7ae8be","externalIds":{"MAG":"2949197630","DBLP":"conf/icml/SundararajanTY17","ArXiv":"1703.01365","CorpusId":16747630},"title":"Axiomatic Attribution for Deep Networks"},{"paperId":"5f456ac2ea2126b6af8d9b335eec662a0c1af422","externalIds":{"MAG":"2518775244","DBLP":"conf/eccv/MahendranV16","DOI":"10.1007/978-3-319-46466-4_8","CorpusId":31593154},"title":"Salient Deconvolutional Networks"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","externalIds":{"MAG":"2962858109","DBLP":"conf/iccv/SelvarajuCDVPB17","ArXiv":"1610.02391","DOI":"10.1007/s11263-019-01228-7","CorpusId":15019293},"title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"7361e42c5eb0d5438c4294cc7ea3f9a53d326309","externalIds":{"MAG":"2951260882","DBLP":"conf/eccv/ZhangLBSS16","ArXiv":"1608.00507","DOI":"10.1007/s11263-017-1059-x","CorpusId":15397918},"title":"Top-Down Neural Attention by Excitation Backprop"},{"paperId":"d7192b47b259a249f055cd448f6381088601354d","externalIds":{"DBLP":"conf/icann/BinderMLMS16","MAG":"2950394196","ArXiv":"1604.00825","DOI":"10.1007/978-3-319-44781-0_8","CorpusId":15207861},"title":"Layer-Wise Relevance Propagation for Neural Networks with Local Renormalization Layers"},{"paperId":"c0883f5930a232a9c1ad601c978caede29155979","externalIds":{"DBLP":"conf/naacl/Ribeiro0G16","MAG":"2516809705","ArXiv":"1602.04938","ACL":"N16-3020","DOI":"10.1145/2939672.2939778","CorpusId":13029170},"title":"“Why Should I Trust You?”: Explaining the Predictions of Any Classifier"},{"paperId":"31f9eb39d840821979e5df9f34a6e92dd9c879f2","externalIds":{"ArXiv":"1512.04150","MAG":"2950328304","DBLP":"journals/corr/ZhouKLOT15","DOI":"10.1109/CVPR.2016.319","CorpusId":6789015},"title":"Learning Deep Features for Discriminative Localization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"056713e422a0753c5eb1733d73e9f8185e2015d4","externalIds":{"MAG":"2195388612","DBLP":"journals/corr/MontavonBBSM15","ArXiv":"1512.02479","DOI":"10.1016/j.patcog.2016.11.008","CorpusId":266022338},"title":"Explaining nonlinear classification decisions with deep Taylor decomposition"},{"paperId":"6df11b0bb0244d4d36e8955436067cc5d19734fa","externalIds":{"MAG":"2240067561","ArXiv":"1509.06321","DBLP":"journals/corr/SamekBMBM15","DOI":"10.1109/TNNLS.2016.2599820","CorpusId":7689122,"PubMed":"27576267"},"title":"Evaluating the Visualization of What a Deep Neural Network Has Learned"},{"paperId":"17a273bbd4448083b01b5a9389b3c37f5425aac0","externalIds":{"MAG":"1787224781","PubMedCentral":"4498753","DOI":"10.1371/journal.pone.0130140","CorpusId":9327892,"PubMed":"26161953"},"title":"On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation"},{"paperId":"ec679c45e88fa25fec32c30bc7c1b7d7fd0facec","externalIds":{"DBLP":"conf/cvpr/OquabBLS15","MAG":"1994488211","DOI":"10.1109/CVPR.2015.7298668","CorpusId":206592664},"title":"Is object localization for free? - Weakly-supervised learning with convolutional neural networks"},{"paperId":"fafa541419b3756968fe5b3156c6f0257cb29c23","externalIds":{"DBLP":"conf/naacl/LiCHJ16","MAG":"1601924930","ACL":"N16-1082","ArXiv":"1506.01066","DOI":"10.18653/v1/N16-1082","CorpusId":14099741},"title":"Visualizing and Understanding Neural Models in NLP"},{"paperId":"33af9298e5399269a12d4b9901492fe406af62b4","externalIds":{"DBLP":"journals/corr/SpringenbergDBR14","MAG":"2963382180","ArXiv":"1412.6806","CorpusId":12998557},"title":"Striving for Simplicity: The All Convolutional Net"},{"paperId":"401192b00b650adfa5ac49de59b720e1c81f1410","externalIds":{"MAG":"1899185266","ArXiv":"1412.6856","DBLP":"journals/corr/ZhouKLOT14","CorpusId":8217340},"title":"Object Detectors Emerge in Deep Scene CNNs"},{"paperId":"616b246e332573af1f4859aa91440280774c183a","externalIds":{"DBLP":"journals/ijcv/EveringhamEGWWZ15","MAG":"2037227137","DOI":"10.1007/s11263-014-0733-5","CorpusId":207252270},"title":"The Pascal Visual Object Classes Challenge: A Retrospective"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71","externalIds":{"MAG":"2962851944","ArXiv":"1312.6034","DBLP":"journals/corr/SimonyanVZ13","CorpusId":1450294},"title":"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"},{"paperId":"1a2a770d23b4a171fa81de62a78a3deb0588f238","externalIds":{"DBLP":"journals/corr/ZeilerF13","ArXiv":"1311.2901","MAG":"1849277567","DOI":"10.1007/978-3-319-10590-1_53","CorpusId":3960646},"title":"Visualizing and Understanding Convolutional Networks"},{"paperId":"a90c83628e585ed2b5d88238a772ca71a106e2e1","externalIds":{"MAG":"2950338048","ArXiv":"1309.6392","DOI":"10.1080/10618600.2014.907095","CorpusId":88519447},"title":"Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation"},{"paperId":"260b98e772c4785fa06a5e8fe1c205eb05ec01e2","externalIds":{"MAG":"1999156278","DBLP":"conf/rss/LenzLS13","ArXiv":"1301.3592","DOI":"10.1177/0278364914549607","CorpusId":5240721},"title":"Deep learning for detecting robotic grasps"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"67107f78a84bdb2411053cb54e94fa226eea6d8e","externalIds":{"MAG":"2156387975","DBLP":"journals/jmlr/GlorotBB11","CorpusId":2239473},"title":"Deep Sparse Rectifier Neural Networks"},{"paperId":"1679beddda3a183714d380e944fe6bf586c083cd","externalIds":{"MAG":"1678356000","DOI":"10.1214/AOS/1013203451","CorpusId":39450643},"title":"Greedy function approximation: A gradient boosting machine."},{"paperId":"b806476548e0ba610e3077e7b6e100875ac953eb","externalIds":{"DBLP":"journals/neco/KurkovaK94","MAG":"2106413382","DOI":"10.1162/neco.1994.6.3.543","CorpusId":31012377},"title":"Functionally Equivalent Feedforward Neural Networks"},{"paperId":"a5e8141d4e323ff7f1f49dbbce293b0d6f739464","externalIds":{"MAG":"2132166479","DBLP":"journals/ml/Holte93","DOI":"10.1023/A:1022631118932","CorpusId":6596},"title":"Very Simple Classification Rules Perform Well on Most Commonly Used Datasets"},{"paperId":"ea552f74bf9d3d635734b1e57aaf405c5014da6c","externalIds":{"DBLP":"journals/corr/abs-2306-11706","DOI":"10.48550/arXiv.2306.11706","CorpusId":271771026},"title":"RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation"},{"paperId":"bcf35117a0cb08fba6b4d00f9aa4f499946b57d9","externalIds":{"DBLP":"conf/acl/SikdarBH20","ACL":"2021.acl-long.71","DOI":"10.18653/v1/2021.acl-long.71","CorpusId":236460258},"title":"Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models"},{"paperId":"ef9bbc83dea84df3711de01de56f2b7f91bae068","externalIds":{"DBLP":"series/lncs/MontavonBLSM19","MAG":"2973136764","DOI":"10.1007/978-3-030-28954-6_10","CorpusId":202579539},"title":"Layer-Wise Relevance Propagation: An Overview"},{"paperId":"6ccb34bf2122304af5cbecf54402ee3d970e43f2","externalIds":{"MAG":"2466432605","DOI":"10.1007/BF00116251","CorpusId":13252401},"title":"Induction of decision trees"}]}