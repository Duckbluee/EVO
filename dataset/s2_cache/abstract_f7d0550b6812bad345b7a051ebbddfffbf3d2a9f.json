{"abstract":"In recent years, more biomedical studies have begun to use multimodal data to improve model performance. Many studies have used ablation for explainability, which requires the modification of input data. This can create out-of-distribution samples and lead to incorrect explanations. To avoid this problem, we propose using a gradient-based feature attribution approach, called layer-wise relevance propagation (LRP), to explain the importance of modalities both locally and globally for the first time. We demonstrate the feasibility of the approach with sleep stage classification as our use-case and train a 1-D convolutional neural network with electroencephalogram (EEG), electrooculogram (EOG), and electromyogram (EMG) data. We also analyze the relationship of our local explainability results with clinical and demographic variables to determine whether they affect our classifier. Across all samples, EEG is the most important modality, followed by EOG and EMG. For individual sleep stages, EEG and EOG have higher relevance for awake and non-rapid eye movement 1 (NREM1). EOG is most important for REM, and EEG is most relevant for NREM2-NREM3. Also, LRP gives consistent levels of importance to each modality for the correctly classified samples across folds but inconsistent levels of importance for incorrectly classified samples. Our statistical analyses suggest that medication has a significant effect upon patterns learned for EEG and EOG NREM2 and that subject sex and age significantly affects the EEG and EOG patterns learned, respectively. Our results demonstrate the viability of gradient-based approaches for explaining multimodal electrophysiology classifiers and suggest their generalizability for other multimodal classification domains."}