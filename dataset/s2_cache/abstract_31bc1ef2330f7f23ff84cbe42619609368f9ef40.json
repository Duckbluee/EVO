{"abstract":"In recent years, distributed graph convolutional networks (GCNs) training frameworks have achieved great success in learning the representation of graph-structured data with large sizes. However, existing distributed GCN training frameworks require enormous communication costs since a multitude of dependent graph data need to be transmitted from other processors. To address this issue, we propose a graph augmentation-based distributed GCN framework (GAD). In particular, GAD has two main components: <italic>GAD-Partition</italic> and <italic>GAD-Optimizer</italic>. We first propose an augmentation-based graph partition <italic>(GAD-Partition)</italic> that can divide the input graph into augmented subgraphs to reduce communication by selecting and storing as few significant vertices of other processors as possible. To further speed up distributed GCN training and improve the quality of the training result, we design a subgraph variance-based importance calculation formula and propose a novel weighted global consensus method, collectively referred to as <italic>GAD-Optimizer</italic>. This optimizer adaptively adjusts the importance of subgraphs to reduce the effect of extra variance introduced by GAD-Partition on distributed GCN training. Extensive experiments on four large-scale real-world datasets demonstrate that our framework significantly reduces the communication overhead (<inline-formula> <tex-math notation=\"LaTeX\">$\\approx 50\\%$ </tex-math></inline-formula>), improves the convergence speed (<inline-formula> <tex-math notation=\"LaTeX\">$\\approx 2 \\times $ </tex-math></inline-formula>) of distributed GCN training, and obtains a slight gain in accuracy (<inline-formula> <tex-math notation=\"LaTeX\">$\\approx 0.45\\%$ </tex-math></inline-formula>) based on minimal redundancy compared to the state-of-the-art methods."}