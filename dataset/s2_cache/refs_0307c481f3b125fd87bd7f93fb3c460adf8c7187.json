{"references":[{"paperId":"bb4b48ab3f58a2f4dbe9ad72c5bece52ddce13d9","externalIds":{"DBLP":"journals/tii/WangZY23","DOI":"10.1109/TII.2023.3241585","CorpusId":256532136},"title":"PA3DNet: 3-D Vehicle Detection With Pseudo Shape Segmentation and Adaptive Camera-LiDAR Fusion"},{"paperId":"8ec1eff1157896327bd188c64cca8ffe4c37eaea","externalIds":{"DBLP":"conf/iccv/YangHJZL23","DOI":"10.1109/ICCV51070.2023.01064","CorpusId":265278628},"title":"Zero-Shot Point Cloud Segmentation by Semantic-Visual Aware Synthesis"},{"paperId":"74af5ac1042ed4f921197b342b15108cdb41223f","externalIds":{"DBLP":"journals/corr/abs-2308-07732","ArXiv":"2308.07732","DOI":"10.1109/ICCV51070.2023.00625","CorpusId":260900346},"title":"UniTR: A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation"},{"paperId":"64a80a33018a0fdc182b06111e32b2e08e186f6a","externalIds":{"ArXiv":"2308.04352","DBLP":"conf/iccv/ZhuMCD0023","DOI":"10.1109/ICCV51070.2023.00272","CorpusId":260704493},"title":"3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment"},{"paperId":"ef80a308b548f582ac52419dffaa48f53db8a4f8","externalIds":{"ArXiv":"2308.04556","DBLP":"conf/iccv/ChenYCLAJ023","DOI":"10.1109/ICCV51070.2023.00771","CorpusId":260736073},"title":"FocalFormer3D : Focusing on Hard Instance for 3D Object Detection"},{"paperId":"e1a1bfbf0b7bdfc070bbf330b0ee5f1780c8bf14","externalIds":{"DBLP":"conf/iccv/ZhangZYY0M23","ArXiv":"2308.01686","DOI":"10.1109/ICCV51070.2023.00339","CorpusId":260438352},"title":"LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment"},{"paperId":"b4ad20e79b324007b7f244d19cab95881c2c5c78","externalIds":{"DBLP":"conf/iccv/WangHZLCZYZ23","ArXiv":"2307.09267","DOI":"10.1109/ICCV51070.2023.00251","CorpusId":259950996},"title":"Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding"},{"paperId":"058bcd5f545b6aad74654de56209ade1ec206f07","externalIds":{"DBLP":"journals/ral/CenZPLZLZC24","ArXiv":"2307.04091","DOI":"10.1109/LRA.2023.3335771","CorpusId":259501737},"title":"CMDFusion: Bidirectional Fusion Network With Cross-Modality Knowledge Distillation for LiDAR Semantic Segmentation"},{"paperId":"c0828c270bfa9d633f9727fcb0fdb1be703e362d","externalIds":{"DBLP":"conf/cvpr/JinHYGL23","DOI":"10.1109/CVPR52729.2023.01057","CorpusId":260849452},"title":"Context-aware Alignment and Mutual Masking for 3D-Language Pre-training"},{"paperId":"099dd69a2827cdba5002b5264bb8094ae4d93d6a","externalIds":{"DBLP":"conf/cvpr/YangHJRL23","DOI":"10.1109/CVPR52729.2023.02084","CorpusId":260053688},"title":"Geometry and Uncertainty-Aware 3D Point Cloud Class-Incremental Semantic Segmentation"},{"paperId":"e01ed6611f9c998c237cda814ff8366a5acb6c3d","externalIds":{"ArXiv":"2305.06131","DBLP":"journals/corr/abs-2305-06131","DOI":"10.48550/arXiv.2305.06131","CorpusId":258588157},"title":"Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era"},{"paperId":"e43b81eac814d7467e7e862ce1f3ad4dd85f23cb","externalIds":{"DBLP":"journals/pr/KorbanYA23a","ArXiv":"2305.19624","DOI":"10.1016/j.patcog.2023.109713","CorpusId":258872552},"title":"A Multi-Modal Transformer Network for Action Detection"},{"paperId":"c8ee48ec023511f2b6de5598af763b4db2de87c6","externalIds":{"DBLP":"conf/iccv/XieXRRTKTZ23","ArXiv":"2304.14340","DOI":"10.1109/ICCV51070.2023.01613","CorpusId":258352667},"title":"SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection"},{"paperId":"7db8daabafa2cba01206d5b6b5d4d0697cb0abdc","externalIds":{"ArXiv":"2304.05645","DBLP":"conf/eccv/LinPCZSHZYM24","DOI":"10.48550/arXiv.2304.05645","CorpusId":258079297},"title":"WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language"},{"paperId":"28b8579efe20f32afa0a4db186918a7c793caee9","externalIds":{"DBLP":"conf/cvpr/CardaceRSS23","ArXiv":"2304.02991","DOI":"10.1109/CVPRW59228.2023.00015","CorpusId":257985148},"title":"Exploiting the Complementarity of 2D and 3D Networks to Address Domain-Shift in 3D Semantic Segmentation"},{"paperId":"af861b96ccdbae9b302f949c05ec2bfe8db13453","externalIds":{"DBLP":"conf/cvpr/YangDDW024","ArXiv":"2304.00962","DOI":"10.1109/CVPR52733.2024.01874","CorpusId":257913360},"title":"RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding"},{"paperId":"d314e2467ba132669d4a770eadfe62499a83666d","externalIds":{"DBLP":"journals/corr/abs-2303-17099","ArXiv":"2303.17099","DOI":"10.48550/arXiv.2303.17099","CorpusId":257833799},"title":"BEVFusion4D: Learning LiDAR-Camera Fusion Under Bird's-Eye-View via Cross-Modality Guidance and Temporal Aggregation"},{"paperId":"6e728f4d7c6866b97e1093818c745e5e70704e09","externalIds":{"ArXiv":"2303.13450","DBLP":"conf/iccvw/Cohen-BarRMGC23","DOI":"10.1109/ICCVW60793.2023.00314","CorpusId":257687648},"title":"Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes"},{"paperId":"3c45fd32b56efaa009a3ecef963d233dc5814194","externalIds":{"DBLP":"conf/cvpr/HsuM023","ArXiv":"2303.13483","DOI":"10.1109/CVPR52729.2023.00257","CorpusId":257687234},"title":"NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations"},{"paperId":"fc64ff39e853791704f6b72eb824f4e86306fee1","externalIds":{"DBLP":"journals/corr/abs-2303-12218","ArXiv":"2303.12218","DOI":"10.1109/3DV62453.2024.00026","CorpusId":257663283},"title":"Compositional 3D Scene Generation using Locally Conditioned Diffusion"},{"paperId":"95aa6fa4e42387561cff22378348d528adea37f2","externalIds":{"ArXiv":"2303.11989","DBLP":"conf/iccv/HolleinCO0N23","DOI":"10.1109/ICCV51070.2023.00727","CorpusId":257636653},"title":"Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models"},{"paperId":"0c2fb6f568ece453248f39e48bf58fc33fce5537","externalIds":{"DBLP":"journals/corr/abs-2303-08600","ArXiv":"2303.08600","DOI":"10.1109/CVPR52729.2023.02078","CorpusId":257532589},"title":"MSeg3D: Multi-Modal 3D Semantic Segmentation for Autonomous Driving"},{"paperId":"6b5b25212d92706b92d363681d8b0678e6e671d0","externalIds":{"DBLP":"conf/iccvw/0023DM23","ArXiv":"2303.04748","DOI":"10.1109/ICCVW60793.2023.00219","CorpusId":257404908},"title":"CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP"},{"paperId":"640ef43c4aaa75516acfcef569cd9e7819cef649","externalIds":{"DBLP":"journals/vc/SeniorSYR25","ArXiv":"2303.03761","DOI":"10.1007/s00371-024-03343-0","CorpusId":257378523},"title":"Graph neural networks in vision-language image understanding: a survey"},{"paperId":"03f06493de39c9dd84125b5e8aa5a198c4524045","externalIds":{"DBLP":"journals/corr/abs-2303-03595","ArXiv":"2303.03595","DOI":"10.1109/CVPR52729.2023.01681","CorpusId":257378160},"title":"LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global Cross- Modal Fusion"},{"paperId":"45ce01446a0b85e84aa29c3209f25e0844a1cd45","externalIds":{"DBLP":"journals/corr/abs-2302-01133","ArXiv":"2302.01133","DOI":"10.48550/arXiv.2302.01133","CorpusId":256503775},"title":"SceneScape: Text-Driven Consistent Scene Generation"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"0c17326565266c40a02b230fac3b405a4d3220b9","externalIds":{"DBLP":"journals/corr/abs-2301-04926","ArXiv":"2301.04926","DOI":"10.1109/CVPR52729.2023.00678","CorpusId":255749086},"title":"CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP"},{"paperId":"92d3f7cea95bba8cb905454324c3eeb84d2b6e58","externalIds":{"ArXiv":"2301.02508","DBLP":"journals/corr/abs-2301-02508","DOI":"10.1109/CVPR52729.2023.01070","CorpusId":255522451},"title":"End-to-End 3D Dense Captioning with Vote2Cap-DETR"},{"paperId":"b23b6117ba6749763bff67c530af1c3b83fa2915","externalIds":{"DBLP":"conf/iccv/YanLSJLWZ23","ArXiv":"2301.01283","DOI":"10.1109/ICCV51070.2023.01675","CorpusId":257496090},"title":"Cross Modal Transformer: Towards Fast and Robust 3D Object Detection"},{"paperId":"846f660b68b58bf094f5e6e85c3df5a572edb811","externalIds":{"DBLP":"conf/wacv/QiuYYSKIS23","DOI":"10.1109/WACV56688.2023.00123","CorpusId":256647874},"title":"3D Change Localization and Captioning from Dynamic Scans of Indoor Scenes"},{"paperId":"67db43cb6cc618c873c63fe2c83025c335b7a230","externalIds":{"DBLP":"journals/corr/abs-2212-03588","ArXiv":"2212.03588","DOI":"10.1109/CVPR52729.2023.01075","CorpusId":254366494},"title":"ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation"},{"paperId":"8abaaa14c7b2f089c081bf031cc4b144a60a6759","externalIds":{"ArXiv":"2212.00836","DBLP":"conf/iccv/ChenHCNC23","DOI":"10.1109/ICCV51070.2023.01660","CorpusId":254220973},"title":"UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding"},{"paperId":"64372cac2f962f183af418bc53570c133ff5ed23","externalIds":{"DBLP":"journals/corr/abs-2211-16312","ArXiv":"2211.16312","DOI":"10.1109/CVPR52729.2023.00677","CorpusId":254069374},"title":"PLA: Language-Driven Open-Vocabulary 3D Scene Understanding"},{"paperId":"774408d8848b129d93fb67548ec6571d99b31a2d","externalIds":{"DBLP":"conf/cvpr/PengGJTPF23","ArXiv":"2211.15654","DOI":"10.1109/CVPR52729.2023.00085","CorpusId":254044069},"title":"OpenScene: 3D Scene Understanding with Open Vocabularies"},{"paperId":"bbd5288edbc0e1e4793471250fbf85cc1b40f4ab","externalIds":{"DBLP":"journals/corr/abs-2211-14241","ArXiv":"2211.14241","DOI":"10.48550/arXiv.2211.14241","CorpusId":254017635},"title":"Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding"},{"paperId":"dad224746343e180dc2dec83353fdda9d889e6ca","externalIds":{"DBLP":"journals/air/LiXWRZJ23","DOI":"10.1007/s10462-022-10317-y","CorpusId":253945817},"title":"Automatic targetless LiDAR–camera calibration: a survey"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"c1a4fb211cf995b1af1247a152fcc145594ec13b","externalIds":{"ArXiv":"2210.07474","DBLP":"journals/corr/abs-2210-07474","DOI":"10.48550/arXiv.2210.07474","CorpusId":252907411},"title":"SQA3D: Situated Question Answering in 3D Scenes"},{"paperId":"4c94d04afa4309ec2f06bdd0fe3781f91461b362","externalIds":{"DBLP":"conf/iclr/PooleJBM23","ArXiv":"2209.14988","DOI":"10.48550/arXiv.2209.14988","CorpusId":252596091},"title":"DreamFusion: Text-to-3D using 2D Diffusion"},{"paperId":"7272153cbfe89a8581985919e5cbdc8e7b65a6ce","externalIds":{"ArXiv":"2209.14941","DBLP":"conf/cvpr/WuCZCZ23","DOI":"10.1109/CVPR52729.2023.01843","CorpusId":253734729},"title":"EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding"},{"paperId":"bbd281c29abf5c6974926781da5bdcf0a7aefd12","externalIds":{"ArXiv":"2209.03102","DBLP":"conf/cvpr/JiaoJ0C0J23","DOI":"10.1109/CVPR52729.2023.02073","CorpusId":252111042},"title":"MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection"},{"paperId":"1114863be2a713a14771ccacb5c9436fb4a375e2","externalIds":{"DBLP":"journals/corr/abs-2208-12262","ArXiv":"2208.12262","DOI":"10.1109/CVPR52729.2023.01058","CorpusId":251799827},"title":"MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining"},{"paperId":"54e5843327857ed9041beb627dc0745db20c717a","externalIds":{"DBLP":"journals/corr/abs-2207-13751","ArXiv":"2207.13751","DOI":"10.48550/arXiv.2207.13751","CorpusId":251135101},"title":"GAUDI: A Neural Architect for Immersive 3D Scene Generation"},{"paperId":"18beeed25fbcc9c8c4c6e1b53f75de0dfb50e067","externalIds":{"DBLP":"journals/corr/abs-2207-10316","ArXiv":"2207.10316","DOI":"10.48550/arXiv.2207.10316","CorpusId":250919842},"title":"AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection"},{"paperId":"eabaf3294c05f54e1e2962916517dc2cd9b3bbb2","externalIds":{"DBLP":"conf/eccv/XuGZZZCL22","ArXiv":"2207.04397","DOI":"10.48550/arXiv.2207.04397","CorpusId":250425746},"title":"2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds"},{"paperId":"77cc5356d6c3a025bd18b7cde3fb30699efbaff1","externalIds":{"DBLP":"conf/cvpr/ShenWJ22","DOI":"10.1109/CVPRW56347.2022.00177","CorpusId":250986350},"title":"AAFormer: A Multi-Modal Transformer Network for Aerial Agricultural Images"},{"paperId":"fa50acda0499ad1a1feb000aa897bfa8bcb25257","externalIds":{"DBLP":"conf/cvpr/0001YC00HT22","ArXiv":"2210.01391","DOI":"10.1109/CVPR52688.2022.01180","CorpusId":250616334},"title":"Bridged Transformer for Vision and Point Cloud 3D Object Detection"},{"paperId":"20114810f64a7ca05b3b9acbc873a74d9de9c13a","externalIds":{"DBLP":"conf/nips/LiCQL0J22","ArXiv":"2206.00630","DOI":"10.48550/arXiv.2206.00630","CorpusId":249240467},"title":"Unifying Voxel-based Representation with Transformer for 3D Object Detection"},{"paperId":"8719f0daba699ff00c9d129cfb3a99bc37d8c128","externalIds":{"DBLP":"conf/cvpr/ZengZWMLZH022","DOI":"10.1109/CVPR52688.2022.01666","CorpusId":250602209},"title":"LIFT: Learning 4D LiDAR Image Fusion Transformer for 3D Object Detection"},{"paperId":"fade0ef67bcad3369e83348111a73c0f9578786f","externalIds":{"DBLP":"conf/cvpr/CaiZZSX22","DOI":"10.1109/CVPR52688.2022.01597","CorpusId":250980730},"title":"3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds"},{"paperId":"4e7487374ba0807f233ebe930d2f7ac507e8830a","externalIds":{"DBLP":"journals/corr/abs-2205-13790","ArXiv":"2205.13790","DOI":"10.48550/arXiv.2205.13790","CorpusId":249151966},"title":"BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"ce7b4545b8e6219705363b015564967437a69ca4","externalIds":{"DBLP":"conf/cvpr/YangXYLLH22","ArXiv":"2205.00272","DOI":"10.1109/CVPR52688.2022.00928","CorpusId":248496764},"title":"Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning"},{"paperId":"6fbefc60c862737eeaa9e464b9e8af58e1e78020","externalIds":{"DBLP":"conf/ijcai/0007ZY022","ArXiv":"2204.10688","DOI":"10.48550/arXiv.2204.10688","CorpusId":248366424},"title":"Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds"},{"paperId":"6db6ac70465067c3835c60968a27c28c4045c0a8","externalIds":{"ArXiv":"2204.08721","DBLP":"journals/corr/abs-2204-08721","DOI":"10.1109/CVPR52688.2022.01187","CorpusId":248239629},"title":"Multimodal Token Fusion for Vision Transformers"},{"paperId":"204d6ea3e3d4ab86e95b74aef4207f81c54ef7f4","externalIds":{"DBLP":"journals/corr/abs-2204-07548","ArXiv":"2204.07548","DOI":"10.1109/CVPR52688.2022.00549","CorpusId":248218804},"title":"Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation"},{"paperId":"c0634ed4e6531f637c6b2450ad7919e915737da2","externalIds":{"DBLP":"journals/corr/abs-2204-06272","ArXiv":"2204.06272","DOI":"10.1109/CVPR52688.2022.01596","CorpusId":248157380},"title":"3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection"},{"paperId":"44d1b81911e35e2aa2c03a5347b88ae479602837","externalIds":{"ArXiv":"2204.02174","DBLP":"conf/cvpr/HuangCJ022","DOI":"10.1109/CVPR52688.2022.01508","CorpusId":247957775},"title":"Multi-View Transformer for 3D Visual Grounding"},{"paperId":"8f763954e6a27f4437d27921c2bb796bd2a090b0","externalIds":{"DBLP":"conf/eccv/DingYJQ22","ArXiv":"2204.01599","DOI":"10.1007/978-3-031-19812-0_17","CorpusId":260493353},"title":"DODA: Data-Oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation"},{"paperId":"e8b2d52d703dd66a460a614348679307692b6147","externalIds":{"ArXiv":"2204.00822","DBLP":"conf/cvpr/PengLHG022","DOI":"10.1109/CVPR52688.2022.00262","CorpusId":247939688},"title":"Semantic-Aware Domain Generalized Segmentation"},{"paperId":"cecf50eef55ccecf7a41f60900b63f40e714a6bd","externalIds":{"DBLP":"journals/corr/abs-2204-00325","ArXiv":"2204.00325","DOI":"10.1109/CVPR52688.2022.00098","CorpusId":247922394},"title":"CAT-Det: Contrastively Augmented Transformer for Multimodal 3D Object Detection"},{"paperId":"03a15e2f0a5302727f929637b0ccaf0d8204d3ed","externalIds":{"ArXiv":"2204.00106","CorpusId":247922659},"title":"A Survey of Robust 3D Object Detection Methods in Point Clouds"},{"paperId":"257c1be4cd8990a2727c7341b47de176d5545eef","externalIds":{"ArXiv":"2203.16895","DBLP":"conf/cvpr/JinLA0H22","DOI":"10.1109/CVPR52688.2022.00709","CorpusId":247839150},"title":"Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds"},{"paperId":"add12473900be92c3ff36d07585011ec33e0a736","externalIds":{"DBLP":"journals/corr/abs-2203-16265","ArXiv":"2203.16265","DOI":"10.1007/978-3-031-19833-5_35","CorpusId":247792929},"title":"SeqTR: A Simple yet Universal Network for Visual Grounding"},{"paperId":"54fbb9300530d1deea596ad19807adedf1dc89dc","externalIds":{"DBLP":"conf/cvpr/BaiHZHCFT22","ArXiv":"2203.11496","DOI":"10.1109/CVPR52688.2022.00116","CorpusId":247597200},"title":"TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers"},{"paperId":"de4c059dfacfa993a3e688af140f854303882d7c","externalIds":{"DBLP":"journals/corr/abs-2203-10642","ArXiv":"2203.10642","DOI":"10.1109/CVPRW59228.2023.00022","CorpusId":247594012},"title":"FUTR3D: A Unified Sensor Fusion Framework for 3D Detection"},{"paperId":"7019b573147b3179730d07ed14b315105a3b30b8","externalIds":{"DBLP":"journals/corr/abs-2203-09780","ArXiv":"2203.09780","DOI":"10.1109/CVPR52688.2022.00534","CorpusId":247595289},"title":"Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion"},{"paperId":"de20efa062cb54ce06beab24ac70be9501423f6a","externalIds":{"DBLP":"conf/cvpr/JiangLHSH22","ArXiv":"2203.08481","DOI":"10.1109/CVPR52688.2022.01507","CorpusId":247476021},"title":"Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding"},{"paperId":"5ffca96f4becdab649f085699594caa7c5c03e86","externalIds":{"DBLP":"conf/cvpr/LiYMCNPSLZLYT22","ArXiv":"2203.08195","DOI":"10.1109/CVPR52688.2022.01667","CorpusId":247476162},"title":"DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection"},{"paperId":"8bef2ad7d969f0e37d9c8b78d9c8d2997838f2cf","externalIds":{"DBLP":"journals/corr/abs-2203-05625","ArXiv":"2203.05625","DOI":"10.48550/arXiv.2203.05625","CorpusId":247411100},"title":"PETR: Position Embedding Transformation for Multi-View 3D Object Detection"},{"paperId":"07546f1f0b35012a5d8ca850580438f306f3bdd3","externalIds":{"ArXiv":"2203.05203","DBLP":"conf/eccv/JiaoCJCMJ22","DOI":"10.48550/arXiv.2203.05203","CorpusId":247362768},"title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes"},{"paperId":"a9861b0c4fc76c96493e67f9398cb17e70a19e1a","externalIds":{"ArXiv":"2203.00843","DBLP":"conf/cvpr/YuanYLGLCL22","DOI":"10.1109/CVPR52688.2022.00837","CorpusId":247218430},"title":"X -Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning"},{"paperId":"72da4393710e2f9127237653293dd4381ae59c66","externalIds":{"DBLP":"conf/ijcai/ChenLZFJZZZ22","ArXiv":"2201.06493","DOI":"10.24963/ijcai.2022/116","CorpusId":246015405},"title":"AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection"},{"paperId":"77e190929675f663b9ccfe010dd230258878b150","externalIds":{"DBLP":"journals/pami/LiuHLCWB23","ArXiv":"2112.11088","DOI":"10.1109/TPAMI.2022.3228806","CorpusId":245353414,"PubMed":"37015370"},"title":"EPNet++: Cascade Bi-Directional Fusion for Multi-Modal 3D Object Detection"},{"paperId":"8f6c652a392995bd047a2f7b94474ab1e6e23ff0","externalIds":{"DBLP":"conf/cvpr/AzumaMKK22","ArXiv":"2112.10482","DOI":"10.1109/CVPR52688.2022.01854","CorpusId":245334889},"title":"ScanQA: 3D Question Answering for Spatial Scene Understanding"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"7ec45c59e4fb19065e17967a350524c936cc7144","externalIds":{"ArXiv":"2112.08879","DBLP":"conf/eccv/JainGMF22","DOI":"10.1007/978-3-031-20059-5_24","CorpusId":250921818},"title":"Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds"},{"paperId":"c5c3ad98547202f120aaae4007cc665bdff0f447","externalIds":{"ArXiv":"2112.04446","DBLP":"journals/corr/abs-2112-04446","DOI":"10.1109/CVPR52688.2022.01939","CorpusId":244954552},"title":"Everything at Once – Multi-modal Fusion Transformer for Video Retrieval"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","externalIds":{"DBLP":"journals/corr/abs-2112-03857","ArXiv":"2112.03857","DOI":"10.1109/CVPR52688.2022.01069","CorpusId":244920947},"title":"Grounded Language-Image Pre-training"},{"paperId":"52706e21fea7a583e9d86798a67148ea877fe664","externalIds":{"ArXiv":"2112.01551","DBLP":"conf/eccv/ChenWNC22","DOI":"10.1007/978-3-031-19824-3_29","CorpusId":251018164},"title":"D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding"},{"paperId":"91dc75f94da13452a54ad5c03fab2c5fda87e9ba","externalIds":{"DBLP":"journals/corr/abs-2112-01522","ArXiv":"2112.01522","DOI":"10.1109/CVPR52688.2022.01630","CorpusId":244799261},"title":"Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks"},{"paperId":"eb12359c1bf31edbb43562f5c76c54721954091e","externalIds":{"DBLP":"journals/tmm/ZhuDZJMLZ23","ArXiv":"2111.14382","DOI":"10.1109/TMM.2022.3189778","CorpusId":244714808},"title":"VPFNet: Improving 3D Object Detection With Virtual Point Based LiDAR and Stereo Data Fusion"},{"paperId":"9eaf5cc6f632b456e1d2959a93965c91f5c91fae","externalIds":{"ArXiv":"2111.06881","DBLP":"journals/corr/abs-2111-06881","CorpusId":244102957},"title":"Multimodal Virtual Point 3D Detection"},{"paperId":"a3aabf51b70c414266ad57c4cbe9a902ff94b470","externalIds":{"DBLP":"conf/iccvw/PaigwarGEL21","DOI":"10.1109/ICCVW54120.2021.00327","CorpusId":244305734},"title":"Frustum-PointPillars: A Multi-Stage Approach for 3D Object Detection using RGB Camera and LiDAR"},{"paperId":"69ff4686b6517a0f9ae59503fedd8ed6e7be9983","externalIds":{"DBLP":"conf/iccv/ZhaoCS021","DOI":"10.1109/ICCV48922.2021.00292","CorpusId":244127479},"title":"3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds"},{"paperId":"03a2befad038a9f29859295fdfcdbfa52c564622","externalIds":{"ArXiv":"2109.08141","DBLP":"journals/corr/abs-2109-08141","DOI":"10.1109/ICCV48922.2021.00290","CorpusId":237532397},"title":"An End-to-End Transformer Model for 3D Object Detection"},{"paperId":"b9daeef3594c06c8ac7450e1217e2bd88ac4994a","externalIds":{"DBLP":"journals/corr/abs-2108-07511","ArXiv":"2108.07511","DOI":"10.1109/TMM.2023.3277281","CorpusId":237142513},"title":"LIF-Seg: LiDAR and Camera Image Fusion for 3D LiDAR Semantic Segmentation"},{"paperId":"0a4e7b3b98c1eea83bf253cb0bcb99e19e31659b","externalIds":{"ArXiv":"2108.02388","DBLP":"conf/mm/HeZLHHZ021","DOI":"10.1145/3474085.3475397","CorpusId":236924276},"title":"TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding"},{"paperId":"cf5416bc3fc43250edbf4dbd9afcd3fdc93fc8c2","externalIds":{"ArXiv":"2107.14724","DBLP":"journals/corr/abs-2107-14724","DOI":"10.1109/ICCV48922.2021.00702","CorpusId":236635247},"title":"Sparse-to-dense Feature Matching: Intra and Inter domain Cross-modal Learning in Domain Adaptation for 3D Semantic Segmentation"},{"paperId":"cb56ea2d4de29481e25df6c318afc217eb7e4a7d","externalIds":{"DBLP":"journals/corr/abs-2107-03438","ArXiv":"2107.03438","CorpusId":235765540},"title":"LanguageRefer: Spatial-Language Model for 3D Visual Grounding"},{"paperId":"e3af84d5498fd4ec36bbf476f7d204d57ff0bb8e","externalIds":{"DBLP":"journals/corr/abs-2106-12735","ArXiv":"2106.12735","DOI":"10.1007/s11263-023-01784-z","CorpusId":235624112},"title":"Multi-Modal 3D Object Detection in Autonomous Driving: A Survey"},{"paperId":"19549d117470fd3c45aaf9e182da3c9a66cb81ce","externalIds":{"DBLP":"conf/itsc/XuZFYZZ21","ArXiv":"2106.12449","DOI":"10.1109/itsc48978.2021.9564951","CorpusId":235606016},"title":"FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection"},{"paperId":"ee5cac4c1a8946681f208d6c58a48263b3123a44","externalIds":{"DBLP":"journals/pami/TanZCLJWL24","ArXiv":"2106.15277","DOI":"10.1109/TPAMI.2024.3402232","CorpusId":235670035,"PubMed":"38809744"},"title":"EPMF: Efficient Perception-Aware Multi-Sensor Fusion for 3D Semantic Segmentation"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"747a2bbe3765f488eedac33a2ac7a0069020da57","externalIds":{"DBLP":"conf/cvpr/Wang0ZY21","DOI":"10.1109/CVPR46437.2021.01162","CorpusId":235692920},"title":"PointAugmenting: Cross-Modal Augmentation for 3D Object Detection"},{"paperId":"895524d13843852f25559df0ea867659d1f9a2a5","externalIds":{"DBLP":"journals/pacmhci/BermejoLCPH21","DOI":"10.1145/3457141","CorpusId":235244134},"title":"Exploring Button Designs for Mid-air Interaction in Virtual Reality: A Hexa-metric Evaluation of Key Representations and Multi-modal Cues"},{"paperId":"b784b2023593af98692b1d2063b53eca897512cd","externalIds":{"DBLP":"journals/corr/abs-2105-11450","ArXiv":"2105.11450","DOI":"10.1109/ICCV48922.2021.00187","CorpusId":235166799},"title":"SAT: 2D Semantics Assisted Training for 3D Visual Grounding"},{"paperId":"5f1913828e30c3070f32c154d2d142ec17e91189","externalIds":{"DBLP":"conf/aaai/HuangLCL21","DOI":"10.1609/aaai.v35i2.16253","CorpusId":235306096},"title":"Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation"},{"paperId":"8c8e73765297f0d7a76da9d69ea86c98d5429a74","externalIds":{"DBLP":"journals/tip/JinZCZHZ21","DOI":"10.1109/TIP.2021.3076556","CorpusId":233867982,"PubMed":"33950840"},"title":"Adaptive Spatio-Temporal Graph Enhanced Vision-Language Representation for Video QA"},{"paperId":"7ba9c013988eaff5cd186d73704af329d027872d","externalIds":{"DBLP":"journals/corr/abs-2104-12763","ArXiv":"2104.12763","DOI":"10.1109/ICCV48922.2021.00180","CorpusId":233393962},"title":"MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"},{"paperId":"10354c03ad56421c454164db6cffa5ace7545d49","externalIds":{"DBLP":"conf/wacv/Guan0LCW0M22","ArXiv":"2104.11896","DOI":"10.1109/WACV51458.2022.00235","CorpusId":233394223},"title":"M3DETR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"27ed05b1354ebc2451653a5356702f8260c7f97b","externalIds":{"ArXiv":"2103.16381","DBLP":"journals/corr/abs-2103-16381","DOI":"10.1109/ICCV48922.2021.00370","CorpusId":232417286},"title":"Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud"},{"paperId":"0a3efe0cf3ddb0ddbb560e4ebb01ff9626982ee5","externalIds":{"DBLP":"journals/corr/abs-2103-14326","ArXiv":"2103.14326","DOI":"10.1109/CVPR46437.2021.01414","CorpusId":232379958},"title":"Bidirectional Projection Network for Cross Dimension Scene Understanding"},{"paperId":"5bf7c535e6cc94c9834a7c31cc3d5ed9376757c3","externalIds":{"ArXiv":"2103.12978","DBLP":"conf/iccv/XuZDZSP21","DOI":"10.1109/ICCV48922.2021.01572","CorpusId":232335531},"title":"RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR Point Cloud Segmentation"},{"paperId":"91408ef7aa1c5278e01c2deaf681f2ee7e9343ff","externalIds":{"DBLP":"journals/corr/abs-2103-01128","ArXiv":"2103.01128","DOI":"10.1109/ICCV48922.2021.00181","CorpusId":232092539},"title":"InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"d0584ccef45ad5472259a42e791030aba9953687","externalIds":{"MAG":"3133190857","DBLP":"journals/sensors/YeongVBW21","PubMedCentral":"8003231","DOI":"10.3390/s21062140","CorpusId":232385194,"PubMed":"33803889"},"title":"Sensor and Sensor Fusion Technology in Autonomous Vehicles: A Review"},{"paperId":"2aea5f2332b37b688d71c53b9a3a370a6d057485","externalIds":{"DBLP":"journals/corr/abs-2101-07253","ArXiv":"2101.07253","DOI":"10.1109/TPAMI.2022.3159589","CorpusId":231632110,"PubMed":"35298372"},"title":"Cross-Modal Learning for Domain Adaptation in 3D Semantic Segmentation"},{"paperId":"e374193760d53abd55be0846665dde9e028cc42b","externalIds":{"DBLP":"journals/corr/abs-2012-11409","ArXiv":"2012.11409","DOI":"10.1109/CVPR46437.2021.00738","CorpusId":229339942},"title":"3D Object Detection with Pointformer"},{"paperId":"7a4ba78d377eea9650e5e399a0878e30bd22f648","externalIds":{"DBLP":"conf/cvpr/ChenGNC21","MAG":"3111353235","ArXiv":"2012.02206","DOI":"10.1109/CVPR46437.2021.00321","CorpusId":227305513},"title":"Scan2Cap: Context-aware Dense Captioning in RGB-D Scans"},{"paperId":"8c07a82e8817a66269050f27a9b28fd194c2dd56","externalIds":{"DBLP":"journals/corr/abs-2011-01404","MAG":"3095430094","ArXiv":"2011.01404","DOI":"10.1109/itsc48978.2021.9564990","CorpusId":226237603},"title":"Faraway-Frustum: Dealing with Lidar Sparsity for 3D Object Detection using Fusion"},{"paperId":"e1d082562981a9f51649c60663aa484ee623dbb0","externalIds":{"DBLP":"journals/access/EngelBD21","ArXiv":"2011.00931","DOI":"10.1109/ACCESS.2021.3116304","CorpusId":226227046},"title":"Point Transformer"},{"paperId":"39ca8f8ff28cc640e3b41a6bd7814ab85c586504","externalIds":{"DBLP":"journals/corr/abs-2010-04159","MAG":"3092462694","ArXiv":"2010.04159","CorpusId":222208633},"title":"Deformable DETR: Deformable Transformers for End-to-End Object Detection"},{"paperId":"a59ccac61db09bd4bfb4f0a2f79f7304d5e98e03","externalIds":{"DBLP":"journals/corr/abs-2009-05684","MAG":"3084548064","ArXiv":"2009.05684","DOI":"10.1007/978-3-030-66096-3_6","CorpusId":221655446},"title":"AttnGrounder: Talking to Cars with Attention"},{"paperId":"53794499a3830c3ebb365ecc57f0e8c8a20a682d","externalIds":{"MAG":"3095974555","DBLP":"conf/eccv/AchlioptasAXEG20","DOI":"10.1007/978-3-030-58452-8_25","CorpusId":221378802},"title":"ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes"},{"paperId":"ed77a2e787aa06308072f78c8b06265c0fa23d9e","externalIds":{"MAG":"3042403800","DBLP":"journals/corr/abs-2007-08856","ArXiv":"2007.08856","DOI":"10.1007/978-3-030-58555-6_3","CorpusId":220633447},"title":"EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection"},{"paperId":"1187e4a921beee82ebd9f5f99a79f20397d3549d","externalIds":{"DBLP":"conf/iwssip/PadillaNS20","MAG":"3043995050","DOI":"10.1109/IWSSIP48289.2020.9145130","CorpusId":220734135},"title":"A Survey on Performance Metrics for Object-Detection Algorithms"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"f6ca0f76ef6b69a3c1ef63251c78116af34626b4","externalIds":{"DBLP":"conf/cvpr/HeZH0Z20","MAG":"3034602892","DOI":"10.1109/cvpr42600.2020.01189","CorpusId":219629677},"title":"Structure Aware Single-Stage 3D Object Detection From Point Cloud"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","externalIds":{"MAG":"3096609285","DBLP":"conf/eccv/CarionMSUKZ20","ArXiv":"2005.12872","DOI":"10.1007/978-3-030-58452-8_13","CorpusId":218889832},"title":"End-to-End Object Detection with Transformers"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","externalIds":{"DBLP":"conf/emnlp/KhashabiMKSTCH20","MAG":"3099655892","ArXiv":"2005.00700","ACL":"2020.findings-emnlp.171","DOI":"10.18653/v1/2020.findings-emnlp.171","CorpusId":218487109},"title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"2d4741e76aa6d5b3b6586b1fc69d47f9cfd8c3ab","externalIds":{"DBLP":"conf/eccv/YooKKC20","MAG":"3107819843","ArXiv":"2004.12636","DOI":"10.1007/978-3-030-58583-9_43","CorpusId":216552886},"title":"3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View Spatial Feature Fusion for 3D Object Detection"},{"paperId":"ddb42aace56362ea9725a33815a64bb213d0329a","externalIds":{"DBLP":"journals/corr/abs-2001-10692","ArXiv":"2001.10692","MAG":"3003352491","DOI":"10.1109/CVPR42600.2020.00446","CorpusId":210943036},"title":"ImVoteNet: Boosting 3D Object Detection in Point Clouds With Image Votes"},{"paperId":"0311ace1d499cadd1cc0c515a625d1d045f60d25","externalIds":{"MAG":"2997320365","DBLP":"journals/corr/abs-1912-12033","ArXiv":"1912.12033","DOI":"10.1109/TPAMI.2020.3005434","CorpusId":209501181,"PubMed":"32750799"},"title":"Deep Learning for 3D Point Clouds: A Survey"},{"paperId":"6c161841cdb547f77930942e4ab46f4369751676","externalIds":{"DBLP":"journals/corr/abs-1912-08830","ArXiv":"1912.08830","MAG":"2995439012","DOI":"10.1007/978-3-030-58565-5_13","CorpusId":209414687},"title":"ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language"},{"paperId":"a42c34d26770143df9f2664fe76eafde6cb67ed4","externalIds":{"MAG":"3009257359","DBLP":"conf/wacv/KrispelOWPB20","ArXiv":"1912.08487","DOI":"10.1109/WACV45572.2020.9093584","CorpusId":209405101},"title":"FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data"},{"paperId":"912124ef1f02aeb8506de802a481be5120d12b50","externalIds":{"DBLP":"conf/cvpr/JaritzVCWP20","MAG":"2989909430","ArXiv":"1911.12676","DOI":"10.1109/cvpr42600.2020.01262","CorpusId":208512862},"title":"xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation"},{"paperId":"8648f59b62dda3900d597e944abe8af51a18f665","externalIds":{"DBLP":"journals/corr/abs-1911-10150","MAG":"2989996808","ArXiv":"1911.10150","DOI":"10.1109/CVPR42600.2020.00466","CorpusId":208248084},"title":"PointPainting: Sequential Fusion for 3D Object Detection"},{"paperId":"46cda398d38465f5ef1ae2dc7262c4b4e202a99f","externalIds":{"DBLP":"journals/corr/abs-1911-06084","MAG":"2998254148","ArXiv":"1911.06084","DOI":"10.1609/AAAI.V34I07.6933","CorpusId":208006295},"title":"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module"},{"paperId":"7b1d7d40fed3f03cc106c901a76b61bad439301d","externalIds":{"MAG":"3010797203","ArXiv":"1909.13603","DBLP":"journals/corr/abs-1909-13603","DOI":"10.1109/ICCVW.2019.00494","CorpusId":203593088},"title":"Multi-View PointNet for 3D Scene Understanding"},{"paperId":"d33e9860d37eb09b184d98c627c4ea3859336af8","externalIds":{"MAG":"2969746073","DBLP":"journals/corr/abs-1908-08854","ArXiv":"1908.08854","CorpusId":201645228},"title":"A Review of Point Cloud Semantic Segmentation"},{"paperId":"398db4093f04605ca3494562b3dbf5b1d105f89d","externalIds":{"DBLP":"conf/icra/DouXF19","MAG":"2967654291","DOI":"10.1109/ICRA.2019.8793492","CorpusId":199542504},"title":"SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data"},{"paperId":"7e0d6439b1208bf264a4c79eb69e081a0fa873cf","externalIds":{"DBLP":"conf/cvpr/MeyerCHLV19","MAG":"2963576229","ArXiv":"1904.11466","DOI":"10.1109/CVPRW.2019.00162","CorpusId":131777068},"title":"Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation"},{"paperId":"e60da8d3a79801a3ccbf1abcdd001bb6e001b267","externalIds":{"DBLP":"journals/corr/abs-1904-09664","MAG":"2988715931","ArXiv":"1904.09664","DOI":"10.1109/ICCV.2019.00937","CorpusId":127956465},"title":"Deep Hough Voting for 3D Object Detection in Point Clouds"},{"paperId":"1954a5ef37030002574f1b000cc1192f3bd4cad1","externalIds":{"MAG":"2956106701","ArXiv":"1904.08755","DBLP":"conf/cvpr/ChoyGS19","DOI":"10.1109/CVPR.2019.00319","CorpusId":121123422},"title":"4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks"},{"paperId":"4511f4100decc138031f93212cfd921bf42f72e2","externalIds":{"DBLP":"conf/iccv/BehleyGMQBSG19","MAG":"2965803762","DOI":"10.1109/ICCV.2019.00939","CorpusId":199441943},"title":"SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"e6c278ea0e66a0b62b5a0fa6298d2e7fbd9ec8bf","externalIds":{"ArXiv":"1903.08701","MAG":"2963120444","DBLP":"conf/cvpr/MeyerLKVW19","DOI":"10.1109/CVPR.2019.01296","CorpusId":84842368},"title":"LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving"},{"paperId":"aabced25e0c940e6e1daa5657ec979ca613d5a0a","externalIds":{"DBLP":"conf/iccvw/Joseph-RivlinZK19","ArXiv":"1812.07431","MAG":"3012015905","DOI":"10.1109/ICCVW.2019.00503","CorpusId":56171530},"title":"Momen^et: Flavor the Moments in Learning to Classify Shapes"},{"paperId":"ee134bac4bdd3a4ab1a5045058d7f9314370cce9","externalIds":{"MAG":"2951030570","ArXiv":"1812.07003","DBLP":"journals/corr/abs-1812-07003","DOI":"10.1109/CVPR.2019.00455","CorpusId":56171922},"title":"3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans"},{"paperId":"3bb322718d64a34b91b29c8230c5978de5d7fb7a","externalIds":{"MAG":"2905076052","ArXiv":"1812.05784","DBLP":"conf/cvpr/LangVCZYB19","DOI":"10.1109/CVPR.2019.01298","CorpusId":55701967},"title":"PointPillars: Fast Encoders for Object Detection From Point Clouds"},{"paperId":"c66b8e508718f4b7f14829e5c2cde0add31d2693","externalIds":{"MAG":"2964935470","DBLP":"conf/cvpr/WangHcGSWWZ19","ArXiv":"1811.10092","DOI":"10.1109/CVPR.2019.00679","CorpusId":53735892},"title":"Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation"},{"paperId":"a8fe949f73ad7c0ca5cdabed1a0493be72c4a598","externalIds":{"MAG":"2963094099","DBLP":"conf/nips/ZhuangTZLGWHZ18","ArXiv":"1810.11809","CorpusId":53102564},"title":"Discrimination-aware Channel Pruning for Deep Neural Networks"},{"paperId":"c5a3711ac088d653d0c4bbd9ecdc6861e1afe093","externalIds":{"MAG":"2894567022","ArXiv":"1810.01151","DBLP":"journals/corr/abs-1810-01151","DOI":"10.1007/978-3-030-11015-4_29","CorpusId":52910606},"title":"Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds"},{"paperId":"fafe7c7aa0a19d40bbbf08b9e87d650438b01b67","externalIds":{"ArXiv":"1803.10409","DBLP":"conf/eccv/DaiN18","MAG":"2795014656","DOI":"10.1007/978-3-030-01249-6_28","CorpusId":4408791},"title":"3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation"},{"paperId":"ecc24760bfb423fc9bfd3b34162fea66dea1633c","externalIds":{"ArXiv":"1803.08495","DBLP":"conf/accv/ChenCSCFS18","MAG":"2950536097","DOI":"10.1007/978-3-030-20893-6_7","CorpusId":4707877},"title":"Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings"},{"paperId":"e78e6cb26f4e15101a7f97522b8b20ce9e4f31d5","externalIds":{"DBLP":"journals/ijon/WangD18","MAG":"2952597690","ArXiv":"1802.03601","DOI":"10.1016/J.NEUCOM.2018.05.083","CorpusId":3654323},"title":"Deep Visual Domain Adaptation: A Survey"},{"paperId":"dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4","externalIds":{"MAG":"2949261999","ArXiv":"1801.04381","DBLP":"conf/cvpr/SandlerHZZC18","DOI":"10.1109/CVPR.2018.00474","CorpusId":4555207},"title":"MobileNetV2: Inverted Residuals and Linear Bottlenecks"},{"paperId":"675784f097dbf87cd75a5640019d4469e7bd7905","externalIds":{"DBLP":"journals/corr/abs-1712-02294","MAG":"2774996270","ArXiv":"1712.02294","DOI":"10.1109/IROS.2018.8594049","CorpusId":3763258},"title":"Joint 3D Proposal Generation and Object Detection from View Aggregation"},{"paperId":"7eb89cbdfdde8cb6071a48fb44173a757f51bfd5","externalIds":{"DBLP":"conf/cvpr/KatoUH18","ArXiv":"1711.07566","MAG":"2769112042","DOI":"10.1109/CVPR.2018.00411","CorpusId":32389979},"title":"Neural 3D Mesh Renderer"},{"paperId":"80f5ee8578ee76e2c17824f211762ffec7e029d4","externalIds":{"DBLP":"journals/corr/abs-1711-06396","MAG":"2949954792","ArXiv":"1711.06396","DOI":"10.1109/CVPR.2018.00472","CorpusId":42427078},"title":"VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection"},{"paperId":"4d71873a40063178b3bf894ac87b371f835d658e","externalIds":{"MAG":"2963521187","ArXiv":"1711.00205","DBLP":"journals/corr/abs-1711-00205","DOI":"10.1109/CVPR.2018.00826","CorpusId":11119748},"title":"Towards Effective Low-Bitwidth Convolutional Neural Networks"},{"paperId":"cb8e4bdd69248146bc42aacc45929a9dbfe44081","externalIds":{"MAG":"2962912109","DBLP":"journals/corr/abs-1710-07368","ArXiv":"1710.07368","DOI":"10.1109/ICRA.2018.8462926","CorpusId":206853127},"title":"SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud"},{"paperId":"d734a8365f7dab731a698cf77a8588b44166b4b2","externalIds":{"ArXiv":"1802.01500","MAG":"3103830808","DBLP":"journals/corr/abs-1802-01500","DOI":"10.1109/ICCVW.2017.90","CorpusId":3609891},"title":"Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds"},{"paperId":"fb37561499573109fc2cebb6a7b08f44917267dd","externalIds":{"MAG":"2963420686","DBLP":"journals/corr/abs-1709-01507","ArXiv":"1709.01507","DOI":"10.1109/CVPR.2018.00745","CorpusId":140309863},"title":"Squeeze-and-Excitation Networks"},{"paperId":"8674494bd7a076286b905912d26d47f7501c4046","externalIds":{"DBLP":"conf/nips/QiYSG17","MAG":"2950697424","ArXiv":"1706.02413","CorpusId":1745976},"title":"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"},{"paperId":"4a73a1840945e87583d89ca0216a2c449d50a4a3","externalIds":{"ArXiv":"1703.06211","DBLP":"conf/iccv/DaiQXLZHW17","MAG":"2950477723","DOI":"10.1109/ICCV.2017.89","CorpusId":4028864},"title":"Deformable Convolutional Networks"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"d997beefc0922d97202789d2ac307c55c2c52fba","externalIds":{"MAG":"2950642167","DBLP":"conf/cvpr/QiSMG17","ArXiv":"1612.00593","DOI":"10.1109/CVPR.2017.16","CorpusId":5115938},"title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"},{"paperId":"dc200ab22bf63e10e8b2af328a9e072d82cf75b7","externalIds":{"MAG":"2950952351","DBLP":"journals/corr/ChenMWLX16","ArXiv":"1611.07759","DOI":"10.1109/CVPR.2017.691","CorpusId":707161},"title":"Multi-view 3D Object Detection Network for Autonomous Driving"},{"paperId":"f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f","externalIds":{"DBLP":"journals/corr/AndersonFJG16","MAG":"2950201573","ArXiv":"1607.08822","DOI":"10.1007/978-3-319-46454-1_24","CorpusId":11933981},"title":"SPICE: Semantic Propositional Image Caption Evaluation"},{"paperId":"dc3f8c8513441915408ab0549e9ac5f2f2f31eec","externalIds":{"MAG":"2460657278","DBLP":"conf/cvpr/ArmeniSZJBFS16","DOI":"10.1109/CVPR.2016.170","CorpusId":9649070},"title":"3D Semantic Parsing of Large-Scale Indoor Spaces"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","externalIds":{"DBLP":"journals/corr/RajpurkarZLL16","MAG":"2963748441","ACL":"D16-1264","ArXiv":"1606.05250","DOI":"10.18653/v1/D16-1264","CorpusId":11816014},"title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"9b686d76914befea66377ec79c1f9258d70ea7e3","externalIds":{"MAG":"2190691619","ArXiv":"1512.03012","DBLP":"journals/corr/ChangFGHHLSSSSX15","CorpusId":2554264},"title":"ShapeNet: An Information-Rich 3D Model Repository"},{"paperId":"a72b8bbd039989db39769da836cdb287737deb92","externalIds":{"MAG":"1895989618","DBLP":"conf/cvpr/ChenZ15","DOI":"10.1109/CVPR.2015.7298856","CorpusId":6785090},"title":"Mind's eye: A recurrent visual representation for image caption generation"},{"paperId":"00fe3d95d0fd5f1433d81405bee772c4fe9af9c6","externalIds":{"MAG":"2951954633","DBLP":"conf/cvpr/WuSLDH16","ArXiv":"1506.01144","DOI":"10.1109/CVPR.2016.29","CorpusId":206593820},"title":"What Value Do Explicit High Level Concepts Have in Vision to Language Problems?"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","externalIds":{"MAG":"1895577753","DBLP":"journals/corr/VinyalsTBE14","ArXiv":"1411.4555","DOI":"10.1109/CVPR.2015.7298935","CorpusId":1169492},"title":"Show and tell: A neural image caption generator"},{"paperId":"c1994ba5946456fc70948c549daf62363f13fa2d","externalIds":{"MAG":"125693051","DBLP":"conf/eccv/SilbermanHKF12","DOI":"10.1007/978-3-642-33715-4_54","CorpusId":545361},"title":"Indoor Segmentation and Support Inference from RGBD Images"},{"paperId":"de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42","externalIds":{"DBLP":"conf/cvpr/GeigerLU12","MAG":"2150066425","DOI":"10.1109/CVPR.2012.6248074","CorpusId":6724907},"title":"Are we ready for autonomous driving? The KITTI vision benchmark suite"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"da7c9b95df692349812bd4588c3508a75d75efaa","externalIds":{"DBLP":"conf/accv/LiuYW22","DOI":"10.1007/978-3-031-26293-7_23","CorpusId":257585040},"title":"Application of Multi-modal Fusion Attention Mechanism in Semantic Segmentation"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}]}