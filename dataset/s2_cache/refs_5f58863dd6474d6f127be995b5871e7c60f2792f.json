{"references":[{"paperId":"baa34706923aa79db40907f0763055f3f920444c","externalIds":{"DBLP":"journals/corr/abs-2504-05541","ArXiv":"2504.05541","DOI":"10.48550/arXiv.2504.05541","CorpusId":277627991},"title":"Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting"},{"paperId":"baa31d252c2cdd739d3aba97ce658f6a4946d102","externalIds":{"DBLP":"journals/kais/YadagiriP25","DOI":"10.1007/s10115-024-02310-4","CorpusId":275196637},"title":"Large language models: a survey of their development, capabilities, and applications"},{"paperId":"6e05f2edd1cc348032f9b65bb24205c6bfdb0b6c","externalIds":{"DBLP":"journals/corr/abs-2412-18108","ArXiv":"2412.18108","DOI":"10.1109/CVPR52734.2025.00391","CorpusId":274992055},"title":"Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach"},{"paperId":"047c6d9bb2600fdb91be1b0713c247391b830d02","externalIds":{"DBLP":"journals/tist/LyuHZYMPYWL25","DOI":"10.1145/3709005","CorpusId":265149820},"title":"GPT-4V(ision) as A Social Media Analysis Engine"},{"paperId":"019b14c04e9cba115b969ce707a45770023aa96a","externalIds":{"DBLP":"journals/corr/abs-2412-09919","ArXiv":"2412.09919","DOI":"10.48550/arXiv.2412.09919","CorpusId":274762723},"title":"B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens"},{"paperId":"ada91666fbc89dbfc79de0ddc9e790dc112ec252","externalIds":{"DBLP":"journals/corr/abs-2411-13056","ArXiv":"2411.13056","DOI":"10.48550/arXiv.2411.13056","CorpusId":274150424},"title":"Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale Benchmark"},{"paperId":"50b900f9d4cf428c97253a1f2289d5628d798bba","externalIds":{"ArXiv":"2411.10979","DBLP":"journals/corr/abs-2411-10979","DOI":"10.1109/CVPR52734.2025.00794","CorpusId":274131631,"PubMed":"40917926"},"title":"VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?"},{"paperId":"61f26ff48642f40850fd2c71b00356ad5b2a17f3","externalIds":{"ArXiv":"2410.23782","DBLP":"journals/corr/abs-2410-23782","DOI":"10.48550/arXiv.2410.23782","CorpusId":273707463},"title":"Video Token Merging for Long-form Video Understanding"},{"paperId":"ed458b5d6442aa700131ae4574a143ec1b4cea42","externalIds":{"DBLP":"journals/corr/abs-2410-09733","ArXiv":"2410.09733","DOI":"10.48550/arXiv.2410.09733","CorpusId":273346558},"title":"MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models"},{"paperId":"6f49873f993e2f011293971698042333f7dda0df","externalIds":{"DBLP":"journals/corr/abs-2410-06682","ArXiv":"2410.06682","DOI":"10.48550/arXiv.2410.06682","CorpusId":273229028},"title":"Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization"},{"paperId":"1b050aee277416ce59d9f609fb04a76068501228","externalIds":{"DBLP":"conf/iclr/GuoLLLCT25","ArXiv":"2410.05643","DOI":"10.48550/arXiv.2410.05643","CorpusId":273228232},"title":"TRACE: Temporal Grounding Video LLM via Causal Event Modeling"},{"paperId":"db7541787a071768603f3254b2411fcfa983be26","externalIds":{"DBLP":"conf/nips/0001WZCY24","ArXiv":"2412.19806","DOI":"10.48550/arXiv.2412.19806","CorpusId":270556280},"title":"Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing"},{"paperId":"c0d9455a80ec53d77cbe0665c5fdfdf80e07a2c3","externalIds":{"DBLP":"journals/corr/abs-2409-17523","ArXiv":"2409.17523","DOI":"10.1145/3664647.3681618","CorpusId":272911106},"title":"EAGLE: Egocentric AGgregated Language-video Engine"},{"paperId":"37f8b6e05237be56ca6f5d2652ab000395a0eae2","externalIds":{"DBLP":"journals/corr/abs-2409-16597","ArXiv":"2409.16597","DOI":"10.48550/arXiv.2409.16597","CorpusId":272881053},"title":"EventHallusion: Diagnosing Event Hallucinations in Video LLMs"},{"paperId":"1a4a2c8d18c2ba6f351e0759324e55e248589384","externalIds":{"ArXiv":"2409.15723","DBLP":"journals/corr/abs-2409-15723","DOI":"10.48550/arXiv.2409.15723","CorpusId":272831705},"title":"Federated Large Language Models: Current Progress and Future Directions"},{"paperId":"c89e46567de3ae6f95a7425f472cefbf7eb2224a","externalIds":{"DBLP":"conf/eccv/MoskalenkoBVTZYTLLHMMRS24","ArXiv":"2409.14827","DOI":"10.48550/arXiv.2409.14827","CorpusId":272827049},"title":"AIM 2024 Challenge on Video Saliency Prediction: Methods and Results"},{"paperId":"13377dcc40420c50f9a7bb211a86835bc10fe36f","externalIds":{"ArXiv":"2409.12963","DBLP":"journals/corr/abs-2409-12963","DOI":"10.48550/arXiv.2409.12963","CorpusId":272753674},"title":"Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner"},{"paperId":"8b22c77e57e87552d3e819838f05b609e0d827a5","externalIds":{"ArXiv":"2408.12009","DBLP":"conf/aaai/TangZYLX25","DOI":"10.48550/arXiv.2408.12009","CorpusId":271923960},"title":"CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion"},{"paperId":"9d21874dc08a0b3cef93a0273f0752080a372f4e","externalIds":{"ArXiv":"2407.15841","DBLP":"journals/corr/abs-2407-15841","DOI":"10.48550/arXiv.2407.15841","CorpusId":271329151},"title":"SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models"},{"paperId":"4b39e96841fbd0dca070d4172228ff15f22e9424","externalIds":{"ArXiv":"2407.00634","DBLP":"journals/corr/abs-2407-00634","DOI":"10.48550/arXiv.2407.00634","CorpusId":270870627},"title":"Tarsier: Recipes for Training and Evaluating Large Video Description Models"},{"paperId":"22e959244a704891b60dacc796a6c979704e1130","externalIds":{"ArXiv":"2406.17309","DBLP":"journals/corr/abs-2406-17309","DOI":"10.48550/arXiv.2406.17309","CorpusId":270710917},"title":"Zero-Shot Long-Form Video Understanding through Screenplay"},{"paperId":"d081584960c42f7793502bb496e46f682e3e43b3","externalIds":{"ArXiv":"2406.16852","DBLP":"journals/tmlr/ZhangZLZYZWTLL25","DOI":"10.48550/arXiv.2406.16852","CorpusId":270703489},"title":"Long Context Transfer from Language to Vision"},{"paperId":"28cbee3c16af1e0e3943f5109513aaa73c20201f","externalIds":{"DBLP":"journals/corr/abs-2406-16442","ArXiv":"2406.16442","DOI":"10.48550/arXiv.2406.16442","CorpusId":270703109},"title":"EmoLLM: Multimodal Emotional Understanding Meets Large Language Models"},{"paperId":"ea1f6e28e8c6b08f18424ab4c0291916a750448e","externalIds":{"ArXiv":"2406.16620","DBLP":"journals/corr/abs-2406-16620","ACL":"2024.emnlp-main.559","DOI":"10.48550/arXiv.2406.16620","CorpusId":270702986},"title":"OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer"},{"paperId":"b40033ade7266a99ff80c9c7e954e48845c33cf8","externalIds":{"DBLP":"journals/corr/abs-2406-15704","ArXiv":"2406.15704","DOI":"10.48550/arXiv.2406.15704","CorpusId":270703250},"title":"video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models"},{"paperId":"4407337f4df63816e92866f0deaa842a5001a97d","externalIds":{"DBLP":"journals/corr/abs-2406-14129","ArXiv":"2406.14129","DOI":"10.48550/arXiv.2406.14129","CorpusId":270620047},"title":"Towards Event-oriented Long Video Understanding"},{"paperId":"61a521d47d2ea646a447d50f6a6a64ada8ea62ea","externalIds":{"ArXiv":"2406.12846","DBLP":"journals/corr/abs-2406-12846","DOI":"10.1109/CVPR52734.2025.01764","CorpusId":270562458},"title":"DrVideo: Document Retrieval Based Long Video Understanding"},{"paperId":"50ed4cd141ee4f55fe982c7bd1e81c6e807b532f","externalIds":{"ArXiv":"2406.12235","DBLP":"journals/corr/abs-2406-12235","DOI":"10.48550/arXiv.2406.12235","CorpusId":270562083},"title":"Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM"},{"paperId":"437a119c66698798d57cbe6e4d8f570894f10402","externalIds":{"DBLP":"journals/corr/abs-2406-11303","ArXiv":"2406.11303","DOI":"10.48550/arXiv.2406.11303","CorpusId":270559556},"title":"VideoVista: A Versatile Benchmark for Video Understanding and Reasoning"},{"paperId":"d019f8a137f62cd2c08ef42be54bbb980a8362f2","externalIds":{"DBLP":"conf/cvpr/ChenLWLSGLGMS24","ArXiv":"2406.11816","DOI":"10.1109/CVPR52733.2024.01742","CorpusId":270560262},"title":"VideoLLM-online: Online Video Large Language Model for Streaming Video"},{"paperId":"b9a4c9ae62a3261f1a7afae1d9f4491dd62a0a1d","externalIds":{"DBLP":"journals/corr/abs-2406-10484","ArXiv":"2406.10484","DOI":"10.1109/CVPRW67362.2025.00055","CorpusId":270560751},"title":"Beyond Raw Videos: Understanding Edited Videos with Large Multimodal Model"},{"paperId":"89d86dde79959cfbecbc31cea5646491136c71e9","externalIds":{"DBLP":"conf/cvpr/ReillyCSGWBXD25","ArXiv":"2406.09390","DOI":"10.1109/CVPR52734.2025.02263","CorpusId":270440966},"title":"LLAVIDAL : A Large LAnguage VIsion Model for Daily Activities of Living"},{"paperId":"28ae81290ea4c2306c808a55d0ec0b2cd3537e88","externalIds":{"ArXiv":"2406.09396","DBLP":"journals/corr/abs-2406-09396","DOI":"10.48550/arXiv.2406.09396","CorpusId":270440923},"title":"Too Many Frames, not all Useful: Efficient Strategies for Long-Form Video QA"},{"paperId":"7391bd9f259c7624e23cfac7ddaae94d16893ed9","externalIds":{"ArXiv":"2406.09418","DBLP":"journals/corr/abs-2406-09418","DOI":"10.48550/arXiv.2406.09418","CorpusId":270440582},"title":"VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding"},{"paperId":"8f1c46e0938ed9dd4e106ede775bb8e75a8442f5","externalIds":{"DBLP":"conf/iclr/HeFZLZLFWLYLWWW25","ArXiv":"2406.08407","DOI":"10.48550/arXiv.2406.08407","CorpusId":270391563},"title":"MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos"},{"paperId":"f9cfedb64717235ede7e5921c76f94acb3899ef9","externalIds":{"ArXiv":"2406.08085","DBLP":"journals/corr/abs-2406-08085","DOI":"10.48550/arXiv.2406.08085","CorpusId":270391809},"title":"Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams"},{"paperId":"e3df98b0912db370b473136833228b46287e586d","externalIds":{"DBLP":"journals/corr/abs-2406-08024","ArXiv":"2406.08024","DOI":"10.48550/arXiv.2406.08024","CorpusId":270391828},"title":"Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities in Large Vision-Language Models"},{"paperId":"c6dd31c40836d616b50feab17aff569591c4262e","externalIds":{"DBLP":"journals/corr/abs-2406-07476","ArXiv":"2406.07476","DOI":"10.48550/arXiv.2406.07476","CorpusId":270380326},"title":"VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs"},{"paperId":"2fb2a76be0f261fb2660457a6cec8a8384b33a19","externalIds":{"DBLP":"journals/corr/abs-2406-06040","ArXiv":"2406.06040","DOI":"10.48550/arXiv.2406.06040","CorpusId":270371430},"title":"Vript: A Video Is Worth Thousands of Words"},{"paperId":"a1a6950adf1ec638fdba66eee3561bc1b3864670","externalIds":{"ArXiv":"2406.04264","DBLP":"conf/cvpr/0001S0WLXQYXZH025","DOI":"10.1109/CVPR52734.2025.01278","CorpusId":270286192},"title":"MLVU: Benchmarking Multi-task Long Video Understanding"},{"paperId":"9583cadea300f67aaab0fdf7b6d1f774c3cd55e7","externalIds":{"DBLP":"conf/nips/0016WLD0ZCDB00024","ArXiv":"2406.04325","DOI":"10.48550/arXiv.2406.04325","CorpusId":270285854},"title":"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions"},{"paperId":"0d83e42e5e729d91afbed96b333e9015f09286cf","externalIds":{"DBLP":"conf/nips/QiuZTXMYDYT24","ArXiv":"2406.00258","DOI":"10.48550/arXiv.2406.00258","CorpusId":270213965},"title":"Artemis: Towards Referential Understanding in Complex Videos"},{"paperId":"22552dd0e7789f175a302055f06444a12e22b652","externalIds":{"ArXiv":"2405.21075","DBLP":"conf/cvpr/FuDLLRZWZSZCLLZ25","DOI":"10.1109/CVPR52734.2025.02245","CorpusId":270199408},"title":"Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis"},{"paperId":"438b128445a3e592f2ce7a8b121387044b07fc3c","externalIds":{"DBLP":"conf/mmgr/LuoPVJ24","ArXiv":"2405.20648","DOI":"10.1145/3689091.3690086","CorpusId":270199853},"title":"Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization"},{"paperId":"6a31399c39b8613e558b758439c7c3e146993d86","externalIds":{"DBLP":"journals/corr/abs-2405-20340","ArXiv":"2405.20340","DOI":"10.48550/arXiv.2405.20340","CorpusId":270123086,"PubMed":"41171656"},"title":"MotionLLM: Understanding Human Behaviors from Human Motions and Videos"},{"paperId":"b2092ea6163c06136b807ce4fc13781d94de1510","externalIds":{"ArXiv":"2405.19209","DBLP":"conf/cvpr/WangYSYCBB25","DOI":"10.1109/CVPR52734.2025.00311","CorpusId":270094803},"title":"VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos"},{"paperId":"582bd8742e161ecdfce76cb111692b1265e116d2","externalIds":{"DBLP":"conf/nips/00050WXMF0LCC24","ArXiv":"2405.16886","DOI":"10.48550/arXiv.2405.16886","CorpusId":270063744},"title":"Hawk: Learning to Understand Open-World Video Anomalies"},{"paperId":"db06c9ac677f7d2811643b478a43ebc334cb6800","externalIds":{"DBLP":"journals/corr/abs-2405-16785","ArXiv":"2405.16785","DOI":"10.48550/arXiv.2405.16785","CorpusId":270063131},"title":"PromptFix: You Prompt and We Fix the Photo"},{"paperId":"6df24cfb2ded90dfa443139c2e3fb6317179a358","externalIds":{"DBLP":"conf/nips/QianDZZDLW24","ArXiv":"2405.16009","DOI":"10.48550/arXiv.2405.16009","CorpusId":270063250},"title":"Streaming Long Video Understanding with Large Language Models"},{"paperId":"17d5f3b2b16a6fcd6f13698718a0e489a5da2b46","externalIds":{"DBLP":"journals/corr/abs-2405-13382","ArXiv":"2405.13382","DOI":"10.48550/arXiv.2405.13382","CorpusId":269982104},"title":"VTG-LLM: Integrating Timestamp Knowledge into Video LLMs for Enhanced Video Temporal Grounding"},{"paperId":"53a97a8978f40e8b24b5e14c32eed86faec93d72","externalIds":{"ArXiv":"2405.13911","DBLP":"conf/nips/LiFWK024","DOI":"10.52202/079017-0185","CorpusId":269982812},"title":"TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment"},{"paperId":"e1a4499ec0e70584736f6edf0579a1a12be6a5a8","externalIds":{"ArXiv":"2405.14040","DBLP":"conf/acl/YangZWWG0J24","DOI":"10.48550/arXiv.2405.14040","CorpusId":269983517},"title":"Synchronized Video Storytelling: Generating Video Narrations with Structured Storyline"},{"paperId":"efc50d7ee5bcd8039ba5d686d7d943be3ff199b0","externalIds":{"DBLP":"journals/corr/abs-2405-10739","ArXiv":"2405.10739","DOI":"10.1007/s44267-025-00099-6","CorpusId":269899856},"title":"Efficient multimodal large language models: a survey"},{"paperId":"8e236f3f44ffedfa4b58570fc02b91c59b871018","externalIds":{"ArXiv":"2405.09713","DBLP":"conf/cvpr/WangWCCGLLG24","DOI":"10.1109/CVPR52733.2024.01271","CorpusId":269791023},"title":"SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge"},{"paperId":"039c0f0b4142823fe214ddf2470230e211c7b8ee","externalIds":{"DBLP":"journals/corr/abs-2405-09711","ArXiv":"2405.09711","DOI":"10.48550/arXiv.2405.09711","CorpusId":244907062},"title":"STAR: A Benchmark for Situated Reasoning in Real-World Videos"},{"paperId":"20b65e39d681d901e956a8116353e224ebd4c0fb","externalIds":{"DBLP":"journals/corr/abs-2405-08813","ArXiv":"2405.08813","DOI":"10.48550/arXiv.2405.08813","CorpusId":269761335},"title":"CinePile: A Long Video Question Answering Dataset and Benchmark"},{"paperId":"56c5b5eafd78e316ef1f9f1439ba76b1fdba304c","externalIds":{"DBLP":"journals/corr/abs-2405-03770","ArXiv":"2405.03770","DOI":"10.48550/arXiv.2405.03770","CorpusId":269613897},"title":"Foundation Models for Video Understanding: A Survey"},{"paperId":"9d29da83aba362c728c36f4dea9dde678ae3e2b2","externalIds":{"DBLP":"journals/corr/abs-2404-16994","ArXiv":"2404.16994","DOI":"10.48550/arXiv.2404.16994","CorpusId":269430328},"title":"PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning"},{"paperId":"4b445a06b615ca215951b2d7176d9de09124cdde","externalIds":{"DBLP":"journals/corr/abs-2404-14687","ArXiv":"2404.14687","DOI":"10.48550/arXiv.2404.14687","CorpusId":269303011},"title":"Pegasus-v1 Technical Report"},{"paperId":"55df271485a4b620f5a2b83af46e4ce49fd1a405","externalIds":{"DBLP":"conf/eccv/HuaSKJZCCL24","ArXiv":"2404.14715","DOI":"10.48550/arXiv.2404.14715","CorpusId":269303150},"title":"FINEMATCH: Aspect-based Fine-grained Image and Text Mismatch Detection and Correction"},{"paperId":"3f6661af9fbb993e81add904ce9663f35c2e5b27","externalIds":{"DBLP":"conf/cvpr/HanBNVXZ24","ArXiv":"2404.14412","DOI":"10.1109/CVPR52733.2024.01720","CorpusId":269293975},"title":"AutoAD III: The Prequel - Back to the Pixels"},{"paperId":"454eda5b7dde5e6f24c3ee6f1c5e06c7deef7ba1","externalIds":{"DBLP":"conf/aaai/Hua0X025","ArXiv":"2404.12353","DOI":"10.48550/arXiv.2404.12353","CorpusId":269214225},"title":"V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning"},{"paperId":"beaa06656a9761e7860b0fe4eeb10800bd751320","externalIds":{"ArXiv":"2404.11865","DBLP":"journals/corr/abs-2404-11865","DOI":"10.48550/arXiv.2404.11865","CorpusId":269214381},"title":"From Image to Video, what do we need in multimodal LLMs?"},{"paperId":"d7adb880eefbfc3fd31b3e60be5bb6bc639d98aa","externalIds":{"DBLP":"journals/corr/abs-2404-09933","ArXiv":"2404.09933","DOI":"10.48550/arXiv.2404.09933","CorpusId":269148867},"title":"HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision"},{"paperId":"b2ec9173a97ae3c2fc9f95307048f7dfe2bb3198","externalIds":{"DBLP":"conf/cvpr/KimKMC024","ArXiv":"2404.07610","DOI":"10.1109/CVPR52733.2024.01318","CorpusId":269043099},"title":"Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval"},{"paperId":"12e1d7aabaea5196dc3df60dd182cef3ccfdb7d5","externalIds":{"ArXiv":"2404.06511","DBLP":"journals/corr/abs-2404-06511","DOI":"10.1109/CVPR52733.2024.01257","CorpusId":269010031},"title":"MoReVQA: Exploring Modular Reasoning Models for Video Question Answering"},{"paperId":"f32b5b59fc4a988c49b112e4d4a06d684d4f117a","externalIds":{"DBLP":"conf/cvpr/0004LJJCSSL24","ArXiv":"2404.05726","DOI":"10.1109/CVPR52733.2024.01282","CorpusId":269005185},"title":"MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding"},{"paperId":"b9789545ffb6de3b5b349e672946c8ad574dff17","externalIds":{"DBLP":"conf/cvpr/HeldICGGD22","ArXiv":"2404.06332","DOI":"10.1109/cvprw63382.2024.00332","CorpusId":269009759},"title":"X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models"},{"paperId":"8b3c303cf8f695498cf8fe8d408e544030240a80","externalIds":{"ArXiv":"2404.04346","DBLP":"journals/corr/abs-2404-04346","DOI":"10.1109/CVPR52733.2024.01289","CorpusId":269005942},"title":"Koala: Key Frame-Conditioned Long Video-LLM"},{"paperId":"057fea4eb5bf72c923df2a00296f48a347863591","externalIds":{"ArXiv":"2404.03413","DBLP":"journals/corr/abs-2404-03413","DOI":"10.48550/arXiv.2404.03413","CorpusId":268889883},"title":"MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens"},{"paperId":"c7453d3e481cd419ec103af45aed2a76ac4d55e3","externalIds":{"DBLP":"conf/eccv/WengHHCZ24","ArXiv":"2404.03384","DOI":"10.48550/arXiv.2404.03384","CorpusId":268889590},"title":"LongVLM: Efficient Long Video Understanding via Large Language Models"},{"paperId":"c119bc07d5e9e2bc4e99b9d48c4a9bc7f49763ae","externalIds":{"ArXiv":"2404.01258","DBLP":"journals/corr/abs-2404-01258","DOI":"10.48550/arXiv.2404.01258","CorpusId":268819166},"title":"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward"},{"paperId":"781fa8bb5ab3af8de29b7674e44648aa40bd5e77","externalIds":{"DBLP":"journals/tnsm/AlmodovarSKA24","DOI":"10.1109/TNSM.2024.3358730","CorpusId":267280972},"title":"LogFiT: Log Anomaly Detection Using Fine-Tuned Language Models"},{"paperId":"8fc418e6450d4ef6dabe80e99c3684b228b35430","externalIds":{"ArXiv":"2404.01014","DBLP":"conf/cvpr/ZanellaMM0024","DOI":"10.1109/CVPR52733.2024.01753","CorpusId":268819369},"title":"Harnessing Large Language Models for Training-Free Video Anomaly Detection"},{"paperId":"e5cdc8f4271376de7dc39a5f65de039f0250f02f","externalIds":{"DBLP":"conf/cvpr/ZhouABYMXNS24","ArXiv":"2404.01297","DOI":"10.1109/CVPR52733.2024.01727","CorpusId":268856925},"title":"Streaming Dense Video Captioning"},{"paperId":"516435c1a34adff37f02bb76a62def3ec610f416","externalIds":{"DBLP":"conf/emnlp/ShangYSDH24","ArXiv":"2404.01476","ACL":"2024.emnlp-main.544","DOI":"10.18653/v1/2024.emnlp-main.544","CorpusId":268857079},"title":"TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering"},{"paperId":"d5342fce96175f83550cfae471a0a46d16401481","externalIds":{"DBLP":"journals/corr/abs-2404-00308","ArXiv":"2404.00308","DOI":"10.48550/arXiv.2404.00308","CorpusId":268820223},"title":"ST-LLM: Large Language Models Are Effective Temporal Learners"},{"paperId":"6a2dab08454e94a75d23c68bd1d4b951d44b72db","externalIds":{"ArXiv":"2403.18406","DBLP":"journals/access/KimCLR24","DOI":"10.1109/ACCESS.2024.3517625","CorpusId":268723836},"title":"An Image Grid Can Be Worth a Video: Zero-Shot Video Question Answering Using a VLM"},{"paperId":"d068f7fbed51ffb542f7f54528955f48dace0934","externalIds":{"DBLP":"conf/eccv/HuangLRYMYK24","ArXiv":"2403.19046","DOI":"10.48550/arXiv.2403.19046","CorpusId":268733134},"title":"LITA: Language Instructed Temporal-Localization Assistant"},{"paperId":"5047af20fc63736d3d7988685a26cbfe09dfe890","externalIds":{"DBLP":"conf/cvpr/Wang0LHYWJ24","ArXiv":"2403.17935","DOI":"10.1109/CVPR52733.2024.01724","CorpusId":268691419},"title":"OmniViD: A Generative Framework for Universal Video Understanding"},{"paperId":"f636022c2552305f6ecfa3b366b36ffaab85adfa","externalIds":{"DBLP":"conf/dcoss/BastolaWCR24","ArXiv":"2403.17331","DOI":"10.1109/DCOSS-IoT61029.2024.00025","CorpusId":268692098},"title":"FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling"},{"paperId":"9f206b53ce6816422985c986adbcb25bd66fee41","externalIds":{"DBLP":"conf/iclr/Ranasinghe0KR25","ArXiv":"2403.16998","CorpusId":268681829},"title":"Understanding Long Videos with Multimodal Language Models"},{"paperId":"ab181887fa543033431e2a1e39f561fa2144e211","externalIds":{"ArXiv":"2403.16558","DBLP":"conf/eccv/WangYWNH24","DOI":"10.48550/arXiv.2403.16558","CorpusId":268681556},"title":"Elysium: Exploring Object-level Perception in Videos via MLLM"},{"paperId":"7501dadc2be4e348e8d373c3b2153c48c40ee4d3","externalIds":{"DBLP":"conf/aaai/0002SBFHX25","ArXiv":"2403.16276","DOI":"10.1609/aaai.v39i7.32784","CorpusId":268681759},"title":"Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding"},{"paperId":"bb47823ccdaca2c69f6a3fa77b6072b829ba001b","externalIds":{"ArXiv":"2403.15377","DBLP":"conf/eccv/WangLLYHCPZWSJLXZHQWW24","DOI":"10.1007/978-3-031-73013-9_23","CorpusId":271432386},"title":"InternVideo2: Scaling Foundation Models for Multimodal Video Understanding"},{"paperId":"262389508181493480a5d24871f3ec177d44020f","externalIds":{"ArXiv":"2403.14622","DBLP":"journals/corr/abs-2403-14622","DOI":"10.48550/arXiv.2403.14622","CorpusId":268554224},"title":"Language Repository for Long Video Understanding"},{"paperId":"62edcd3612243ffc82a291b6c8b7f69b66cbba45","externalIds":{"DBLP":"journals/corr/abs-2403-14743","ArXiv":"2403.14743","DOI":"10.48550/arXiv.2403.14743","CorpusId":268667196},"title":"VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding"},{"paperId":"1954f4fad3dd1ddf3f162fbcb6d20c9e7f31cde7","externalIds":{"DBLP":"conf/cvpr/WangTZSW25","ArXiv":"2403.12922","DOI":"10.1109/CVPR52734.2025.00784","CorpusId":268531966},"title":"Contextual AD Narration with Interleaved Multimodal Sequence"},{"paperId":"8cac6e0e5c7c308b5eb33bcfccf8bfffbec9fd8e","externalIds":{"ArXiv":"2403.11481","DBLP":"journals/corr/abs-2403-11481","DOI":"10.48550/arXiv.2403.11481","CorpusId":268513192},"title":"VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding"},{"paperId":"567157a4722d79f5ac019142e5fb1ad10b4e86cb","externalIds":{"ArXiv":"2403.10517","DBLP":"journals/corr/abs-2403-10517","DOI":"10.48550/arXiv.2403.10517","CorpusId":268510077},"title":"VideoAgent: Long-form Video Understanding with Large Language Model as Agent"},{"paperId":"0afefc171b8e37bf27604ab7868acf65fb8faf0d","externalIds":{"DBLP":"journals/corr/abs-2403-10228","ArXiv":"2403.10228","DOI":"10.48550/arXiv.2403.10228","CorpusId":268510318},"title":"HawkEye: Training Video-Text LLMs for Grounding Text in Videos"},{"paperId":"74d1e5c6a96ee487cf5e410237432a4955c08653","externalIds":{"DBLP":"journals/corr/abs-2403-06070","ArXiv":"2403.06070","DOI":"10.48550/arXiv.2403.06070","CorpusId":268357047},"title":"Reframe Anything: LLM Agent for Open World Video Reframing"},{"paperId":"eb6b054789ff8c9edf7c1d50667be5bdd95e019b","externalIds":{"DBLP":"journals/corr/abs-2403-04640","ArXiv":"2403.04640","DOI":"10.48550/arXiv.2403.04640","CorpusId":268264651},"title":"CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios"},{"paperId":"72c54b43a6ed039ea140b753fa7c59b594dbc1a4","externalIds":{"DBLP":"conf/cvpr/NagasingheZGMHK24","ArXiv":"2403.02782","DOI":"10.1109/CVPR52733.2024.01780","CorpusId":268248178},"title":"Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos"},{"paperId":"d4a63e29b204843abd992ac02e8508d2ab79fa7b","externalIds":{"DBLP":"conf/iclr/NiuG00C24","ArXiv":"2403.01599","DOI":"10.48550/arXiv.2403.01599","CorpusId":268248469},"title":"SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos"},{"paperId":"d8ee1434c37f2cb5a7c89027875349b53118743f","externalIds":{"ArXiv":"2403.01437","DBLP":"journals/spl/SunXXSD24","DOI":"10.1109/LSP.2023.3340103","CorpusId":266256689},"title":"GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features"},{"paperId":"ac8089bb7944090cf1de5df25aadf5e6356f3040","externalIds":{"DBLP":"journals/corr/abs-2403-00476","ArXiv":"2403.00476","DOI":"10.48550/arXiv.2403.00476","CorpusId":268201547},"title":"TempCompass: Do Video LLMs Really Understand Videos?"},{"paperId":"0042b9380f7da8335be040a3516e4f6765320834","externalIds":{"ArXiv":"2402.19467","DBLP":"journals/corr/abs-2402-19467","ACL":"2024.emnlp-main.1059","DOI":"10.48550/arXiv.2402.19467","CorpusId":268091324},"title":"TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning"},{"paperId":"44a8afaac107bcecac355d14de7993964f5661c4","externalIds":{"ArXiv":"2402.17128","DBLP":"conf/naacl/NguyenBVTFX24","DOI":"10.48550/arXiv.2402.17128","CorpusId":268032097,"PubMed":"41334549"},"title":"OSCaR: Object State Captioning and State Change Representation"},{"paperId":"efe9bda2508339efae46febf6168b44773c9f5cb","externalIds":{"ArXiv":"2403.02076","DBLP":"journals/corr/abs-2403-02076","DOI":"10.3390/app14051894","CorpusId":268035181},"title":"VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT"},{"paperId":"aa66639895a7dfce1e229293e546686912ba8320","externalIds":{"ArXiv":"2402.13546","CorpusId":271957622},"title":"LLMs Meet Long Video: Advancing Long Video Question Answering with An Interactive Visual Adapter in LLMs"},{"paperId":"4b48fa99d17e130da58b0f72c2c4b50f72d2e146","externalIds":{"DBLP":"conf/cvpr/IslamHYNTB24","ArXiv":"2402.13250","DOI":"10.1109/CVPR52733.2024.01723","CorpusId":267759646},"title":"Video ReCap: Recursive Captioning of Hour-Long Videos"},{"paperId":"71e599dbfe90f6a4bdbdb7dee8464b54a0801379","externalIds":{"DBLP":"journals/corr/abs-2402-13088","ArXiv":"2402.13088","DOI":"10.48550/arXiv.2402.13088","CorpusId":267759602},"title":"Slot-VLM: SlowFast Slots for Video-Language Modeling"},{"paperId":"8f6061730f9964ebda3bac4f99e5a170df2c38c9","externalIds":{"DBLP":"journals/corr/abs-2402-12079","ArXiv":"2402.12079","DOI":"10.48550/arXiv.2402.12079","CorpusId":267750734},"title":"LVCHAT: Facilitating Long Video Comprehension"},{"paperId":"237bfa636f1a575f4784d2ae81a47ac29fa38522","externalIds":{"DBLP":"journals/corr/abs-2402-11435","ArXiv":"2402.11435","DOI":"10.48550/arXiv.2402.11435","CorpusId":267750980},"title":"Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning"},{"paperId":"be4d586ca8add316c14f24397f57f8c77e36cb9d","externalIds":{"ArXiv":"2404.16038","DBLP":"journals/corr/abs-2404-16038","DOI":"10.48550/arXiv.2404.16038","CorpusId":269362618},"title":"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming"},{"paperId":"333381155dfb30e3e05886835b1217c63f1a8100","externalIds":{"DBLP":"conf/mm/WangLSG24","ArXiv":"2401.10711","DOI":"10.1145/3664647.3680826","CorpusId":267060847},"title":"Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","externalIds":{"ArXiv":"2401.08392","DBLP":"conf/icml/YangCLW024","DOI":"10.48550/arXiv.2401.08392","CorpusId":267028201},"title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"090294139835018d2357a7b638fb03b4c7dd5df3","externalIds":{"DBLP":"journals/corr/abs-2401-06071","ArXiv":"2401.06071","DOI":"10.48550/arXiv.2401.06071","CorpusId":266933314},"title":"GroundingGPT:Language Enhanced Multi-modal Grounding Model"},{"paperId":"a05be6905ce7fb4a86dd9b174232362cc50df5af","externalIds":{"DBLP":"journals/corr/abs-2307-05628","DOI":"10.1101/2023.07.11.548628","CorpusId":263868853},"title":"DNAGPT: A Generalized Pre-trained Tool for Multiple DNA Sequence Analysis Tasks"},{"paperId":"0836eab8f8276a18dc2efcffb806103b70039f68","externalIds":{"DBLP":"conf/cvpr/AshutoshXNG24","ArXiv":"2401.01823","DOI":"10.1109/CVPR52733.2024.01779","CorpusId":266741937},"title":"Detours for Navigating Instructional Videos"},{"paperId":"b1b29e274aa116826f8e82177415f40743ba0392","externalIds":{"DBLP":"journals/corr/abs-2401-00849","ArXiv":"2401.00849","DOI":"10.48550/arXiv.2401.00849","CorpusId":266693842},"title":"COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training"},{"paperId":"fcbf1ee82623bce378ca729dd4042c388ebb5f5c","externalIds":{"ArXiv":"2401.00374","DBLP":"conf/cvpr/LiuZBPSZZIZB24","DOI":"10.1109/CVPR52733.2024.00115","CorpusId":266693782},"title":"EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling"},{"paperId":"4641fe56cd44144b6cabea583233ed952f97f4c0","externalIds":{"DBLP":"journals/corr/abs-2312-17235","ACL":"2024.emnlp-main.1209","ArXiv":"2312.17235","DOI":"10.48550/arXiv.2312.17235","CorpusId":266573523},"title":"A Simple LLM Framework for Long-Range Video Question-Answering"},{"paperId":"e4bbd1cfa8e53e529b5e8310838a2e455985137d","externalIds":{"DBLP":"journals/corr/abs-2312-17117","ArXiv":"2312.17117","DOI":"10.48550/arXiv.2312.17117","CorpusId":266573374},"title":"Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos"},{"paperId":"2e4dff705de77ece77f1912032fb33803fe0bad1","externalIds":{"DBLP":"conf/cvpr/0002YJFCW24","ArXiv":"2312.13746","DOI":"10.1109/CVPR52733.2024.02062","CorpusId":266435543},"title":"Video Recognition in Portrait Mode"},{"paperId":"0c4f46e4dcae5527018e6432fb60cfe8c3354e97","externalIds":{"DBLP":"journals/corr/abs-2312-14125","ArXiv":"2312.14125","DOI":"10.48550/arXiv.2312.14125","CorpusId":266435847},"title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation"},{"paperId":"4b1b5e219fb41a7413599c3b2ca6a7fdf045d1a5","externalIds":{"DBLP":"conf/cvpr/SunCZZYWRL0W24","ArXiv":"2312.13286","DOI":"10.1109/CVPR52733.2024.01365","CorpusId":266374640},"title":"Generative Multimodal Models are In-Context Learners"},{"paperId":"fcf66b0af76ce5206139d5a994d18372b013b2ea","externalIds":{"ArXiv":"2312.11782","DBLP":"journals/corr/abs-2312-11782","DOI":"10.1109/CVPR52733.2024.01750","CorpusId":266362245},"title":"Learning Object State Changes in Videos: An Open-World Perspective"},{"paperId":"86227dcaafc29bb7853296eb6504571b486eafc5","externalIds":{"ArXiv":"2312.10300","CorpusId":266348773},"title":"Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos"},{"paperId":"2141ed804636a1cf339d606cd03fd3b3e9582133","externalIds":{"DBLP":"conf/cvpr/LinYP0SH24","ArXiv":"2312.07533","DOI":"10.1109/CVPR52733.2024.02520","CorpusId":266174746},"title":"VILA: On Pre-training for Visual Language Models"},{"paperId":"81bd66d960503106ef969830568016da4f93754a","externalIds":{"DBLP":"conf/cvpr/MaJWXFY24","ArXiv":"2312.08870","DOI":"10.1109/CVPR52733.2024.01249","CorpusId":266209773},"title":"Vista-llama: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens"},{"paperId":"0d13c8a337792729ebfe16500b4d55a365e191f8","externalIds":{"ArXiv":"2312.06363","DBLP":"journals/tomccap/ChenZGLSZLJ25","DOI":"10.1145/3688804","CorpusId":266162879},"title":"MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples"},{"paperId":"1ae9afce62c60fd0bfc9f5b57f8d8a1bbc3641eb","externalIds":{"ArXiv":"2312.06720","DBLP":"journals/corr/abs-2312-06720","DOI":"10.48550/arXiv.2312.06720","CorpusId":266174198},"title":"Audio-Visual LLM for Video Understanding"},{"paperId":"630c40e6573e8e19bf1bc733465d713e38f3315f","externalIds":{"ArXiv":"2312.05269","CorpusId":268793427},"title":"LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos"},{"paperId":"53c3c3984649ca82a2f85629dae01087e9e72991","externalIds":{"DBLP":"conf/cvpr/HanGZ0ZL00024","ArXiv":"2312.03700","DOI":"10.1109/CVPR52733.2024.02510","CorpusId":265709786},"title":"OneLLM: One Framework to Align All Modalities with Language"},{"paperId":"eca8a3e6383e3618e0bc984382e08c09be3cca6c","externalIds":{"DBLP":"journals/corr/abs-2312-02051","ArXiv":"2312.02051","DOI":"10.1109/CVPR52733.2024.01357","CorpusId":265608767},"title":"TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding"},{"paperId":"a4815d80c63092a2b71384570facda6c61e47035","externalIds":{"ArXiv":"2312.02310","DBLP":"journals/corr/abs-2312-02310","DOI":"10.48550/arXiv.2312.02310","CorpusId":265659345},"title":"VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding"},{"paperId":"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","externalIds":{"DBLP":"journals/corr/abs-2312-00937","ArXiv":"2312.00937","DOI":"10.48550/arXiv.2312.00937","CorpusId":265608945},"title":"Zero-Shot Video Question Answering with Procedural Programs"},{"paperId":"17ff6a0844afe74796022e7aaf372553e9303d72","externalIds":{"DBLP":"journals/corr/abs-2311-18445","ArXiv":"2311.18445","DOI":"10.1109/CVPR52733.2024.01353","CorpusId":265506226},"title":"VTimeLLM: Empower LLM to Grasp Video Moments"},{"paperId":"40de3296157b9d7a7882b61f967e37b3cc93f197","externalIds":{"ArXiv":"2312.00589","DBLP":"conf/eccv/YuZWYWKWWGZT24","DOI":"10.48550/arXiv.2312.00589","CorpusId":265552069},"title":"Merlin: Empowering Multimodal LLMs with Foresight Minds"},{"paperId":"5aea65d0d712360bb3357cbc23d43707ec27c461","externalIds":{"DBLP":"journals/corr/abs-2311-17435","ArXiv":"2311.17435","DOI":"10.1109/CVPR52733.2024.01295","CorpusId":265499090},"title":"MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning"},{"paperId":"ea3448eb86a233189631d914721e587d45931b64","externalIds":{"DBLP":"conf/cvpr/0002WH00LWX0L0024","ArXiv":"2311.17005","DOI":"10.1109/CVPR52733.2024.02095","CorpusId":265466214},"title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"},{"paperId":"486c2df78cbb770a90a55f7fa3fe19102fba2c24","externalIds":{"ArXiv":"2311.17043","DBLP":"journals/corr/abs-2311-17043","DOI":"10.48550/arXiv.2311.17043","CorpusId":265466723},"title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"},{"paperId":"b037bb09aa162d8a543e64ec777ca0edc732d2af","externalIds":{"DBLP":"journals/corr/abs-2311-16103","ArXiv":"2311.16103","DOI":"10.48550/arXiv.2311.16103","CorpusId":265456879},"title":"Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models"},{"paperId":"66026c1a25dea275ccc28bcaa6f2049e7ff6a0e9","externalIds":{"DBLP":"conf/mm/WangW0WLL0Z0T24","ArXiv":"2311.16511","DOI":"10.1145/3664647.3681464","CorpusId":265466543},"title":"GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation"},{"paperId":"cb76f7fc35ff289fdf1cf50d9cfe1493342a0dec","externalIds":{"ArXiv":"2311.14906","DBLP":"conf/eccv/ChenLZH24","DOI":"10.48550/arXiv.2311.14906","CorpusId":265456806},"title":"AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering"},{"paperId":"1e838bd5fa2f5bca805493d0f672d03514b36869","externalIds":{"ArXiv":"2311.13627","DBLP":"journals/corr/abs-2311-13627","DOI":"10.48550/arXiv.2311.13627","CorpusId":265445735},"title":"Vamos: Versatile Action Models for Video Understanding"},{"paperId":"4edbb942c2d20a6f5a4e3caa763a9761be953231","externalIds":{"DBLP":"journals/corr/abs-2311-13435","ArXiv":"2311.13435","DOI":"10.48550/arXiv.2311.13435","CorpusId":265351434},"title":"PG-Video-LLaVA: Pixel Grounding Large Video-Language Models"},{"paperId":"7757b3346e6578fabcfbc74754df1354e1d197d8","externalIds":{"ArXiv":"2311.13160","DBLP":"journals/corr/abs-2311-13160","DOI":"10.1109/BigData59044.2023.10386291","CorpusId":265352038},"title":"Large Language Models in Education: Vision and Opportunities"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","externalIds":{"ArXiv":"2311.10122","ACL":"2024.emnlp-main.342","DBLP":"journals/corr/abs-2311-10122","DOI":"10.48550/arXiv.2311.10122","CorpusId":265281544},"title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","externalIds":{"DBLP":"conf/cvpr/0001TZC024","ArXiv":"2311.08046","DOI":"10.1109/CVPR52733.2024.01300","CorpusId":265157455},"title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"e68ab63d505c3ee8d0518c734c3d2b13071cc18d","externalIds":{"DBLP":"journals/corr/abs-2310-19773","ArXiv":"2310.19773","DOI":"10.48550/arXiv.2310.19773","CorpusId":264806489},"title":"MM-VID: Advancing Video Understanding with GPT-4V(ision)"},{"paperId":"bb4516ad6eb7adda97d81f09d4bb92b3ad056c42","externalIds":{"DBLP":"journals/corr/abs-2310-15747","ArXiv":"2310.15747","DOI":"10.48550/arXiv.2310.15747","CorpusId":264438945},"title":"Large Language Models are Temporal and Causal Reasoners for Video Question Answering"},{"paperId":"45872b94798c3125abfb185b7926689c5e767763","externalIds":{"DBLP":"conf/sigir/Tang00SSCY024","ArXiv":"2310.13023","DOI":"10.1145/3626772.3657775","CorpusId":264405943},"title":"GraphGPT: Graph Instruction Tuning for Large Language Models"},{"paperId":"b83593ff620942d176543dbf6a08ead0275ab52d","externalIds":{"ArXiv":"2310.11699","DBLP":"journals/corr/abs-2310-11699","DOI":"10.48550/arXiv.2310.11699","CorpusId":263912881},"title":"MISAR: A Multimodal Instructional System with Augmented Reality"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","externalIds":{"ArXiv":"2310.10647","DBLP":"journals/corr/abs-2310-10647","DOI":"10.1145/3696415","CorpusId":264172934},"title":"A Survey on Video Diffusion Models"},{"paperId":"63baa934679d8b5fec731ec4d75b9db529c369ec","externalIds":{"DOI":"10.3390/vehicles5040076","CorpusId":264178349},"title":"Analysis of Language-Model-Powered Chatbots for Query Resolution in PDF-Based Automotive Manuals"},{"paperId":"2020e6ecaddd06d991421a827207e8992474e701","externalIds":{"DBLP":"journals/corr/abs-2310-05863","ArXiv":"2310.05863","DOI":"10.48550/arXiv.2310.05863","CorpusId":263830927},"title":"Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"8eb99f1ed884356871ddbcf1377b82359071906a","externalIds":{"DBLP":"conf/iclr/ZhuLNYCWPJZLZ0024","ArXiv":"2310.01852","DOI":"10.48550/arXiv.2310.01852","CorpusId":263608698},"title":"LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment"},{"paperId":"696d6b667926a559e63989d45eec53c3a15986be","externalIds":{"ArXiv":"2310.00582","DBLP":"conf/cvpr/XuanGYZ24","DOI":"10.1109/CVPR52733.2024.01313","CorpusId":263333993},"title":"Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs"},{"paperId":"81dccd74199ddda6e975ef2c6957120bad0d5c61","externalIds":{"DBLP":"journals/corr/abs-2310-06838","ArXiv":"2310.06838","DOI":"10.1109/ICCV51070.2023.01255","CorpusId":263829585},"title":"AutoAD II: The Sequel â€“ Who, When, and What in Movie Audio Description"},{"paperId":"16753e0317730e8c1b297338300a8c6163dd06f2","externalIds":{"ArXiv":"2309.15091","DBLP":"journals/corr/abs-2309-15091","DOI":"10.48550/arXiv.2309.15091","CorpusId":262825203},"title":"VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning"},{"paperId":"20f3ca16e2b54aa817c0d9ef3f4dc096de924113","externalIds":{"DBLP":"conf/nips/YangNLSS23","ArXiv":"2309.13952","DOI":"10.48550/arXiv.2309.13952","CorpusId":262465198},"title":"VidChapters-7M: Video Chapters at Scale"},{"paperId":"482665786ce1956fb9ea4b694d2d8e8cf92276fa","externalIds":{"ArXiv":"2309.10228","DBLP":"conf/wacv/CuiMCYW24","DOI":"10.1109/WACVW60836.2024.00101","CorpusId":262054629},"title":"Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles"},{"paperId":"af3ab5da98e0807784b57e321ed887a3666a8ab6","externalIds":{"DBLP":"journals/corr/abs-2309-10020","ArXiv":"2309.10020","DOI":"10.48550/arXiv.2309.10020","CorpusId":262055614},"title":"Multimodal Foundation Models: From Specialists to General-Purpose Assistants"},{"paperId":"8895cfd205a7f3d4ba99caea175dc4f7d0db3b7b","externalIds":{"DBLP":"journals/cluster/AlHawawrehAJ23","DOI":"10.1007/s10586-023-04124-5","CorpusId":261196127},"title":"Chatgpt for cybersecurity: practical applications, challenges, and future directions"},{"paperId":"4172b6f8d18d0d26ba1a1ef8c1303885bb4f8679","externalIds":{"ArXiv":"2308.08849","DBLP":"journals/corr/abs-2308-08849","DOI":"10.48550/arXiv.2308.08849","CorpusId":261030968},"title":"A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation"},{"paperId":"656a6b3c0348d69cf9f98f95cbf68046941a4f29","externalIds":{"DBLP":"journals/corr/abs-2308-09126","ArXiv":"2308.09126","DOI":"10.48550/arXiv.2308.09126","CorpusId":261031047},"title":"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"},{"paperId":"1798c7cc0351957dd1f9551c2c8ddec5a98a25a1","externalIds":{"DBLP":"journals/corr/abs-2308-08544","ArXiv":"2308.08544","DOI":"10.1109/ICCV51070.2023.00254","CorpusId":260926362},"title":"MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions"},{"paperId":"7fbc502441d66daf1f53765d5d86a8dfba9ab0ce","externalIds":{"DBLP":"journals/corr/abs-2308-01390","ArXiv":"2308.01390","DOI":"10.48550/arXiv.2308.01390","CorpusId":261043320},"title":"OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","externalIds":{"DBLP":"journals/corr/abs-2308-00692","ArXiv":"2308.00692","DOI":"10.1109/CVPR52733.2024.00915","CorpusId":260351258},"title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d","externalIds":{"ArXiv":"2307.16449","DBLP":"conf/cvpr/SongCWZZWCG0ZLH24","DOI":"10.1109/CVPR52733.2024.01725","CorpusId":260333927},"title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding"},{"paperId":"6024f320e0a5b9b8fc29b86903aa9a96956b26dd","externalIds":{"DBLP":"conf/iclr/ZhaoW0FDAL024","ArXiv":"2307.16368","DOI":"10.48550/arXiv.2307.16368","CorpusId":260334705},"title":"AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?"},{"paperId":"4c45a3e31084f159c04e14c7439553c962a06e8d","externalIds":{"ArXiv":"2307.12502","DBLP":"conf/iccv/LiZHZ23","DOI":"10.1109/ICCV51070.2023.00128","CorpusId":260926332},"title":"Cross Contrasting Feature Perturbation for Domain Generalization"},{"paperId":"369b449415d50387fba048bbd4d26ee890df84b5","externalIds":{"ArXiv":"2307.06942","DBLP":"conf/iclr/WangH00YML0C00024","DOI":"10.48550/arXiv.2307.06942","CorpusId":259847783},"title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"},{"paperId":"3843469976bd895851bfa08c8208350745bf649f","externalIds":{"DBLP":"conf/corl/RanaHGA0S23","ArXiv":"2307.06135","DOI":"10.48550/arXiv.2307.06135","CorpusId":259837542},"title":"SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning"},{"paperId":"73134ef3ac17961b4947c20aa5198c5b4affcc56","externalIds":{"DBLP":"journals/corr/abs-2307-05463","ArXiv":"2307.05463","DOI":"10.1109/ICCV51070.2023.00487","CorpusId":259766518},"title":"EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone"},{"paperId":"383dc41fbafa9ab8a275013a03646369edb1fee0","externalIds":{"ArXiv":"2307.04827","DBLP":"journals/corr/abs-2307-04827","DOI":"10.48550/arXiv.2307.04827","CorpusId":259765956},"title":"LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad"},{"paperId":"e2a58fd18961c3941102989e3a3d0d27c615e015","externalIds":{"ArXiv":"2306.15195","DBLP":"journals/corr/abs-2306-15195","DOI":"10.48550/arXiv.2306.15195","CorpusId":259262082},"title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"454850fcb311faf1de3f4028a312cfeb781857b4","externalIds":{"ArXiv":"2306.10354","DBLP":"journals/corr/abs-2306-10354","DOI":"10.48550/arXiv.2306.10354","CorpusId":259202917},"title":"LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","externalIds":{"ArXiv":"2306.09093","DBLP":"journals/corr/abs-2306-09093","DOI":"10.48550/arXiv.2306.09093","CorpusId":259165461},"title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","externalIds":{"ArXiv":"2306.08640","DBLP":"journals/corr/abs-2306-08640","DOI":"10.48550/arXiv.2306.08640","CorpusId":259164559},"title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","externalIds":{"DBLP":"journals/corr/abs-2306-07207","ArXiv":"2306.07207","DOI":"10.1145/3796716","CorpusId":259138706},"title":"Valley: Video Assistant with Large Language Model Enhanced Ability"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","externalIds":{"ArXiv":"2306.05424","DBLP":"journals/corr/abs-2306-05424","DOI":"10.48550/arXiv.2306.05424","CorpusId":259108333},"title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"d7a4b09a0e2c2d7b118144cf09895c640896da7b","externalIds":{"DBLP":"journals/corr/abs-2306-04362","ArXiv":"2306.04362","DOI":"10.48550/arXiv.2306.04362","CorpusId":259095579},"title":"Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"f6c494234a818b2aec286b257ee6117f2894bcf7","externalIds":{"DBLP":"conf/icassp/ElizaldeDIW23","DOI":"10.1109/ICASSP49357.2023.10095889","CorpusId":249605738},"title":"CLAP Learning Audio Concepts from Natural Language Supervision"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","externalIds":{"DBLP":"journals/corr/abs-2306-00890","ArXiv":"2306.00890","DOI":"10.48550/arXiv.2306.00890","CorpusId":258999820},"title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"ea720398e02286b1ae55b2a21b52f45a00dca1ba","externalIds":{"DBLP":"conf/cvpr/SongZLCPZKA23","DOI":"10.1109/CVPR52729.2023.01756","CorpusId":260005035},"title":"ObjectStitch: Object Compositing with Diffusion Model"},{"paperId":"4e33c5756aa18d248cf50fef9382acda1e0f65da","externalIds":{"DBLP":"journals/corr/abs-2305-18500","ArXiv":"2305.18500","DOI":"10.48550/arXiv.2305.18500","CorpusId":258967371},"title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","externalIds":{"ArXiv":"2305.16103","DBLP":"journals/corr/abs-2305-16103","DOI":"10.48550/arXiv.2305.16103","CorpusId":258887944},"title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"b82c1b0512d25307e3c81bb8d9df1607267a7a52","externalIds":{"DBLP":"conf/emnlp/HwangS23","ArXiv":"2305.13703","DOI":"10.48550/arXiv.2305.13703","CorpusId":258841099},"title":"MemeCap: A Dataset for Captioning and Interpreting Memes"},{"paperId":"f9bfc6d9ba1665b73af3323d46c7642b852759ef","externalIds":{"DBLP":"journals/corr/abs-2305-13292","ArXiv":"2305.13292","DOI":"10.48550/arXiv.2305.13292","CorpusId":258832930},"title":"VideoLLM: Modeling Video Sequence with Large Language Models"},{"paperId":"4f6aca3e2bcd79711e842c179cb9a4acda653afc","externalIds":{"DBLP":"conf/ijcai/JinLCHWYLC23","ArXiv":"2305.12218","DOI":"10.48550/arXiv.2305.12218","CorpusId":258832331},"title":"Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","externalIds":{"ArXiv":"2305.06988","DBLP":"journals/corr/abs-2305-06988","DOI":"10.48550/arXiv.2305.06988","CorpusId":258615748},"title":"Self-Chained Image-Language Model for Video Localization and Question Answering"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","externalIds":{"DBLP":"journals/corr/abs-2305-06355","ArXiv":"2305.06355","DOI":"10.1007/s11432-024-4321-9","CorpusId":258588306},"title":"VideoChat: chat-centric video understanding"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","externalIds":{"DBLP":"journals/corr/abs-2305-05665","ArXiv":"2305.05665","DOI":"10.1109/CVPR52729.2023.01457","CorpusId":258564264},"title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","externalIds":{"DBLP":"journals/pami/LiZCWPCYLL25","ArXiv":"2305.03726","DOI":"10.1109/TPAMI.2025.3571946","CorpusId":258547300,"PubMed":"40392642"},"title":"Otter: A Multi-Modal Model With In-Context Instruction Tuning"},{"paperId":"6f8b9192b1f215254ee7625d752710182c05d2f9","externalIds":{"DBLP":"journals/corr/abs-2305-02677","ArXiv":"2305.02677","DOI":"10.48550/arXiv.2305.02677","CorpusId":258479994},"title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","externalIds":{"ArXiv":"2304.15010","DBLP":"journals/corr/abs-2304-15010","DOI":"10.48550/arXiv.2304.15010","CorpusId":258418343},"title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"c56a51728678e5b2e3ff95e51caf21d267439c36","externalIds":{"ArXiv":"2304.14407","DBLP":"journals/corr/abs-2304-14407","DOI":"10.48550/arXiv.2304.14407","CorpusId":258352505},"title":"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System"},{"paperId":"687843157b19e750d03e078c3807667c7261a99c","externalIds":{"DBLP":"journals/corr/abs-2304-11431","ArXiv":"2304.11431","DOI":"10.48550/arXiv.2304.11431","CorpusId":258298547,"PubMed":"40030799"},"title":"A Review of Deep Learning for Video Captioning"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"03755613d50e1958a97bfaad2efb976f786fbb70","externalIds":{"DBLP":"journals/pami/LiuCHGZWT25","ArXiv":"2304.08345","DOI":"10.1109/TPAMI.2024.3479776","CorpusId":258179576,"PubMed":"39418158"},"title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","externalIds":{"ArXiv":"2304.04227","DBLP":"journals/corr/abs-2304-04227","DOI":"10.48550/arXiv.2304.04227","CorpusId":258048445},"title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"34859777c597b3b8934338de1af3246cdbf9069e","externalIds":{"ArXiv":"2303.14329","DBLP":"journals/corr/abs-2303-14329","DOI":"10.48550/arXiv.2303.14329","CorpusId":257766587},"title":"Edge-Based Video Analytics: A Survey"},{"paperId":"7ac99ed33afa36c5634a62a32bc61f1fe950e367","externalIds":{"DBLP":"conf/cvpr/HuangTKX23","ArXiv":"2303.13471","DOI":"10.1109/CVPR52729.2023.02194","CorpusId":257687731},"title":"Egocentric Audio-Visual Object Localization"},{"paperId":"031b99e4801d5603181490e8389dfc6391716490","externalIds":{"DBLP":"conf/cvpr/GengW0CZ23","ArXiv":"2303.12930","DOI":"10.1109/CVPR52729.2023.02197","CorpusId":257687331},"title":"Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline"},{"paperId":"730f15f042e13c713325a98b4cf512867c169080","externalIds":{"DBLP":"journals/corr/abs-2303-12060","ArXiv":"2303.12060","DOI":"10.1109/TMM.2023.3335875","CorpusId":257636716},"title":"VideoXum: Cross-Modal Visual and Textural Summarization of Videos"},{"paperId":"a363b989a11e11de76022945390c3eddef769413","externalIds":{"DBLP":"journals/corr/abs-2303-10902","ArXiv":"2303.10902","DOI":"10.1109/CVPR52729.2023.01920","CorpusId":257632461},"title":"Feature Alignment and Uniformity for Test Time Adaptation"},{"paperId":"e2d5f5a49d68c80a9108646587eb2f7b898e2831","externalIds":{"ArXiv":"2303.09867","DBLP":"conf/iccv/0001LC0JL0023","DOI":"10.1109/ICCV51070.2023.00234","CorpusId":257622679},"title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model"},{"paperId":"8c236be5cb8073cb3db317919ceb55130ab66dbe","externalIds":{"DBLP":"journals/corr/abs-2303-09713","ArXiv":"2303.09713","DOI":"10.1109/ICCV51070.2023.01421","CorpusId":257622977},"title":"Champagne: Learning Real-world Conversation from Large-Scale Web Videos"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","externalIds":{"DBLP":"conf/iccv/SurisMV23","ArXiv":"2303.08128","DOI":"10.1109/ICCV51070.2023.01092","CorpusId":257505358},"title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"e4e744cc96da7987a072571fc3817f040d456566","externalIds":{"ArXiv":"2303.06573","DBLP":"conf/emnlp/MaoDMH0Q23","DOI":"10.48550/arXiv.2303.06573","CorpusId":257495903},"title":"Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","externalIds":{"DBLP":"journals/corr/abs-2303-04671","ArXiv":"2303.04671","DOI":"10.48550/arXiv.2303.04671","CorpusId":257404891},"title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"98be3a878abef9ba0fc624a6ada7b8607da26aae","externalIds":{"ArXiv":"2303.02489","DBLP":"journals/corr/abs-2303-02489","DOI":"10.1109/CVPR52729.2023.01462","CorpusId":257365027},"title":"CapDet: Unifying Dense Captioning and Open-World Detection Pretraining"},{"paperId":"8cf01c76b506f6bca5258071ed309fc4430c24d3","externalIds":{"PubMedCentral":"10028514","DOI":"10.2196/46885","CorpusId":257309782,"PubMed":"36863937"},"title":"The Role of ChatGPT, Generative Language Models, and Artificial Intelligence in Medical Education: A Conversation With ChatGPT and a Call for Papers"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"0938d0ccc1c633fa0f8c067d914358b1ef53a44b","externalIds":{"ArXiv":"2302.14115","DBLP":"conf/cvpr/YangNSMPLSS23","DOI":"10.1109/CVPR52729.2023.01032","CorpusId":257232853},"title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning"},{"paperId":"0b411d430c85d234f9df614ef494ce78040b5064","externalIds":{"DBLP":"journals/compsec/MouratidisIPSI23","DOI":"10.1016/j.cose.2023.103139","CorpusId":256932990},"title":"Modelling language for cyber security incident handling for critical infrastructures"},{"paperId":"64caaab51d8339f1b99874d3bddb79debbe661ca","externalIds":{"DBLP":"journals/corr/abs-2302-00402","ArXiv":"2302.00402","DOI":"10.48550/arXiv.2302.00402","CorpusId":256459873},"title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"b909c1905063fe247a7c9359842e8437448f929d","externalIds":{"DBLP":"journals/corr/abs-2212-14546","ArXiv":"2212.14546","DOI":"10.1109/ICCV51070.2023.01413","CorpusId":255340506},"title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training"},{"paperId":"933b37b21e9d61139660088adb032ff3fdf56d86","externalIds":{"DBLP":"conf/cvpr/0006MKG23","ArXiv":"2212.04501","DOI":"10.1109/CVPR52729.2023.00637","CorpusId":254408789},"title":"Learning Video Representations from Large Language Models"},{"paperId":"a02fbaf22237a1aedacb1320b6007cd70c1fe6ec","externalIds":{"DBLP":"journals/corr/abs-2212-04356","ArXiv":"2212.04356","CorpusId":252923993},"title":"Robust Speech Recognition via Large-Scale Weak Supervision"},{"paperId":"33b1dbf1ad913047b919cd090907ffc4199b4178","externalIds":{"DBLP":"journals/corr/abs-2212-00280","ArXiv":"2212.00280","DOI":"10.48550/arXiv.2212.00280","CorpusId":254125445},"title":"GRiT: A Generative Region-to-text Transformer for Object Understanding"},{"paperId":"f6fb6f464efead0c4543f97de793a439c3ca07c6","externalIds":{"ArXiv":"2211.14513","DBLP":"conf/aaai/ZhangLLHHZ23","DOI":"10.48550/arXiv.2211.14513","CorpusId":254043718},"title":"Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation"},{"paperId":"6787e08233ef6716f2a23ed52e28ea8593676810","externalIds":{"ArXiv":"2211.11427","DBLP":"conf/nips/JinHLWGSC022","DOI":"10.48550/arXiv.2211.11427","CorpusId":253735288},"title":"Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations"},{"paperId":"a5cb8f26acb71edd77ff9a143d3ddaab2367eb40","externalIds":{"ArXiv":"2211.09699","DBLP":"journals/corr/abs-2211-09699","DOI":"10.48550/arXiv.2211.09699","CorpusId":253581319},"title":"PromptCap: Prompt-Guided Task-Aware Image Captioning"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","externalIds":{"DBLP":"conf/cvpr/FangWXSWW0WC23","ArXiv":"2211.07636","DOI":"10.1109/CVPR52729.2023.01855","CorpusId":253510587},"title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"fcbed420c30e30e99cce8f1d619dac69e9b046a6","externalIds":{"DBLP":"conf/icassp/WangLCZLX23","ArXiv":"2210.15977","DOI":"10.1109/ICASSP49357.2023.10096019","CorpusId":253224350},"title":"FedVMR: A New Federated Learning Method for Video Moment Retrieval"},{"paperId":"1dff6b1b35e2d45d4db57c8b4e4395486c3e365f","externalIds":{"ArXiv":"2210.09461","DBLP":"conf/iclr/BolyaFDZFH23","DOI":"10.48550/arXiv.2210.09461","CorpusId":252968113},"title":"Token Merging: Your ViT But Faster"},{"paperId":"6ac5e4c7d148634ae994c7854e18c04a4da6bcf0","externalIds":{"DBLP":"conf/nips/SunXS00F22","ArXiv":"2210.06031","DOI":"10.48550/arXiv.2210.06031","CorpusId":252846639},"title":"Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning"},{"paperId":"40b7bc6a863dd67204b20bf44886d21459d802ab","externalIds":{"DBLP":"conf/mm/LiuIZLZBZ22","DOI":"10.1145/3503161.3548400","CorpusId":252782540},"title":"DisCo: Disentangled Implicit Content and Rhythm Learning for Diverse Co-Speech Gestures Synthesis"},{"paperId":"da2529210998a44cb8a8a654d35b3298880bdb8b","externalIds":{"DBLP":"journals/corr/abs-2210-04154","ArXiv":"2210.04154","DOI":"10.48550/arXiv.2210.04154","CorpusId":252780183},"title":"Self-supervised Video Representation Learning with Motion-Aware Masked Autoencoders"},{"paperId":"be5975f7cca9bbf30665d2539619eee01aebddb9","externalIds":{"DBLP":"journals/corr/abs-2209-12164","ArXiv":"2209.12164","DOI":"10.48550/arXiv.2209.12164","CorpusId":252531465},"title":"Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"9e095237a496f20c286eafb9dd966f775f5fbac5","externalIds":{"DBLP":"conf/iclr/XueS0FSLL23","ArXiv":"2209.06430","CorpusId":252222370},"title":"CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Alignment"},{"paperId":"7d82d43261800e4f4dcd3977241fc53307692ef0","externalIds":{"DBLP":"journals/corr/abs-2208-02580","ArXiv":"2208.02580","DOI":"10.48550/arXiv.2208.02580","CorpusId":251320505},"title":"SOMPT22: A Surveillance Oriented Multi-Pedestrian Tracking Dataset"},{"paperId":"a86fe34e17cfc4847a39ab54a2f3adda534eb43d","externalIds":{"DBLP":"conf/iclr/GuptaTZ0M023","ArXiv":"2206.11894","DOI":"10.48550/arXiv.2206.11894","CorpusId":249953817},"title":"MaskViT: Masked Visual Pre-Training for Video Prediction"},{"paperId":"d2425b430fbf5b8ddf9cf2309c36a80a71e5a449","externalIds":{"DBLP":"journals/corr/abs-2206-08356","ArXiv":"2206.08356","DOI":"10.1109/CVPR52729.2023.01003","CorpusId":249712367},"title":"OmniMAE: Single Model Masked Pretraining on Images and Videos"},{"paperId":"c45e3b624d769382c9e0d986c50a37dea6c0e39c","externalIds":{"ACL":"2023.acl-long.29","ArXiv":"2206.03428","DBLP":"journals/corr/abs-2206-03428","DOI":"10.48550/arXiv.2206.03428","CorpusId":249431866},"title":"Revealing Single Frame Bias for Video-and-Language Learning"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","externalIds":{"DBLP":"journals/corr/abs-2205-14100","ArXiv":"2205.14100","DOI":"10.48550/arXiv.2205.14100","CorpusId":249152323},"title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"1457c1ea57ce45ff79d59b689efe376c42540dda","externalIds":{"DBLP":"journals/corr/abs-2205-09113","ArXiv":"2205.09113","DOI":"10.48550/arXiv.2205.09113","CorpusId":248863181},"title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","externalIds":{"ArXiv":"2204.00598","DBLP":"journals/corr/abs-2204-00598","CorpusId":247922520},"title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"efbc6f6d983a189831106aaa8f3f4bd3b4ad8b15","externalIds":{"DBLP":"conf/eccv/WangGYLFS22","ArXiv":"2204.00486","DOI":"10.1007/978-3-031-19833-5_41","CorpusId":250699030},"title":"GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval"},{"paperId":"4990f7542f0600e0501a7e7a931b32eb7cb804d5","externalIds":{"DBLP":"journals/corr/abs-2203-12602","ArXiv":"2203.12602","DOI":"10.48550/arXiv.2203.12602","CorpusId":247619234},"title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"},{"paperId":"85caafcb902e7199a14261a6d887df994992c801","externalIds":{"ArXiv":"2203.10462","DBLP":"journals/air/TuLXLZLY22","DOI":"10.1007/s10462-022-10159-8","CorpusId":247593750},"title":"Optical flow for video super-resolution: a survey"},{"paperId":"b00da02e88a857970d89cd8e69fa77710d03bbfe","externalIds":{"DBLP":"journals/tnn/ShaoHMD25","DOI":"10.1109/TNNLS.2022.3152990","CorpusId":247407181,"PubMed":"35275824"},"title":"Region-Object Relation-Aware Dense Captioning via Transformer"},{"paperId":"ec6b81eadbac2aedcd70e31f383b25be47e96292","externalIds":{"DBLP":"conf/eccv/LiuZIPLZBZ22","ArXiv":"2203.05297","DOI":"10.48550/arXiv.2203.05297","CorpusId":247362872},"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis"},{"paperId":"7366fe7b4e7756dd13220d29077142ff802b41b3","externalIds":{"DBLP":"journals/uais/CosterSHD24","ArXiv":"2202.03086","DOI":"10.1007/s10209-023-00992-1","CorpusId":246634045},"title":"Machine translation from signed to spoken languages: state of the art and challenges"},{"paperId":"c3d086d0f50ff9efa28d56616ed127547d836e55","externalIds":{"ArXiv":"2201.08377","DBLP":"conf/cvpr/GirdharSRMJM22","DOI":"10.1109/CVPR52688.2022.01563","CorpusId":246063865},"title":"Omnivore: A Single Model for Many Visual Modalities"},{"paperId":"80ea0e2882db3347b4fbc83f1a55c6a93e0d9272","externalIds":{"DBLP":"journals/corr/abs-2112-09583","ArXiv":"2112.09583","DOI":"10.1109/CVPR52688.2022.00490","CorpusId":245329792},"title":"Align and Prompt: Video-and-Language Pre-training with Entity Prompts"},{"paperId":"008a428e049003fe768068a0f1fa1416af5c4982","externalIds":{"DBLP":"journals/corr/abs-2112-09133","ArXiv":"2112.09133","DOI":"10.1109/CVPR52688.2022.01426","CorpusId":245218767},"title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training"},{"paperId":"898b65bdec52856cd66b56dabe33e2a62df816f0","externalIds":{"DBLP":"journals/corr/abs-2112-04478","ArXiv":"2112.04478","DOI":"10.1007/978-3-031-19833-5_7","CorpusId":244954623},"title":"Prompting Visual-Language Models for Efficient Video Understanding"},{"paperId":"95c81427a084b02aa0173444f5247463b78e28d9","externalIds":{"DBLP":"journals/corr/abs-2112-00431","ArXiv":"2112.00431","DOI":"10.1109/CVPR52688.2022.00497","CorpusId":244773187},"title":"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions"},{"paperId":"8a349ff8222986274f302bf85c1f53a7ffafbf54","externalIds":{"DBLP":"journals/corr/abs-2111-14690","ArXiv":"2111.14690","DOI":"10.1109/CVPR52688.2022.02032","CorpusId":244714594},"title":"DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion"},{"paperId":"f4b73a1b1de1cbea715e1ed404c0626eac53d976","externalIds":{"ArXiv":"2111.09564","DBLP":"journals/corr/abs-2111-09564","DOI":"10.1016/j.asoc.2023.110689","CorpusId":244346703},"title":"LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"56c0b89b706b0be9fcf456ee703d770ca5efe645","externalIds":{"DBLP":"conf/mm/SongLYDZB21","DOI":"10.1145/3474085.3475196","CorpusId":239011633},"title":"TACR-Net: Editing on Deep Video and Voice Portraits"},{"paperId":"848eb8367785910c2fe31372605954ad8f9dfe6c","externalIds":{"DBLP":"conf/cvpr/GraumanWBCFGH0L22","ArXiv":"2110.07058","DOI":"10.1109/CVPR52688.2022.01842","CorpusId":238856888},"title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video"},{"paperId":"6a84593d8b380c23be5810cf55faff1cd52682b6","externalIds":{"DBLP":"conf/icip/SongYLZY21","MAG":"3193882757","DOI":"10.1109/ICIP42928.2021.9506512","CorpusId":238690301},"title":"Fsft-Net: Face Transfer Video Generation With Few-Shot Views"},{"paperId":"76e64bb7cd283d448740dc1dafb9be69cc34765b","externalIds":{"DBLP":"conf/iccv/WangZLZC021","ArXiv":"2108.07781","DOI":"10.1109/ICCV48922.2021.00677","CorpusId":237142167},"title":"End-to-End Dense Video Captioning with Parallel Decoding"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","externalIds":{"ArXiv":"2106.04560","DBLP":"journals/corr/abs-2106-04560","DOI":"10.1109/CVPR52688.2022.01179","CorpusId":235367962},"title":"Scaling Vision Transformers"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","externalIds":{"DBLP":"conf/cvpr/ZhangLHY0WCG21","DOI":"10.1109/CVPR46437.2021.00553","CorpusId":235692795},"title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"441fa19685f7ba5f5d1b295eec37543a43c09194","externalIds":{"MAG":"3172318343","DBLP":"conf/naacl/HuaLDXL21","ArXiv":"2107.04835","ACL":"2021.naacl-main.258","DOI":"10.18653/V1/2021.NAACL-MAIN.258","CorpusId":235097474},"title":"Noise Stability Regularization for Improving BERT Fine-tuning"},{"paperId":"18f37f62d2bf3c2e34e2bde78545b47e92d7b72d","externalIds":{"DBLP":"journals/corr/abs-2105-09996","ArXiv":"2105.09996","ACL":"2021.findings-acl.370","DOI":"10.18653/v1/2021.findings-acl.370","CorpusId":235125628},"title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding"},{"paperId":"1b937ff4b05e2b56c2c2fcdfa5baa3085cd5a08c","externalIds":{"DBLP":"conf/cvpr/XiaoSYC21","ArXiv":"2105.08276","DOI":"10.1109/CVPR46437.2021.00965","CorpusId":234763093},"title":"NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions"},{"paperId":"32e7df1b63d78ef1f65f4928d8c8c45137e54d3b","externalIds":{"ArXiv":"2105.07404","DBLP":"conf/iccv/LiCH0W021","DOI":"10.1109/ICCV48922.2021.01328","CorpusId":234742617},"title":"MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions"},{"paperId":"8754533bead3996f20440e4a1d0220d4971d00d7","externalIds":{"ArXiv":"2104.11746","DBLP":"journals/corr/abs-2104-11746","DOI":"10.1109/ICCV48922.2021.01332","CorpusId":233387838,"PubMed":"35557988"},"title":"VidTr: Video Transformer Without Convolutions"},{"paperId":"18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6","externalIds":{"ArXiv":"2104.11227","DBLP":"conf/iccv/0001XMLYMF21","DOI":"10.1109/ICCV48922.2021.00675","CorpusId":233346705},"title":"Multiscale Vision Transformers"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"b6382a7351c0c595f91472ac71d3b2d87b3c4844","externalIds":{"DBLP":"journals/corr/abs-2103-15691","ArXiv":"2103.15691","DOI":"10.1109/ICCV48922.2021.00676","CorpusId":232417054},"title":"ViViT: A Video Vision Transformer"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"fa08b41ccdfc5d8771adfbc34c176fa237d4646c","externalIds":{"DBLP":"conf/icml/BertasiusWT21","ArXiv":"2102.05095","CorpusId":231861462},"title":"Is Space-Time Attention All You Need for Video Understanding?"},{"paperId":"aa3b88f1cbfd1545369aa9b836c56a39201e98c6","externalIds":{"DBLP":"journals/corr/abs-2101-10511","ArXiv":"2101.10511","DOI":"10.1109/ICCV48922.2021.00797","CorpusId":231709491},"title":"Generic Event Boundary Detection: A Benchmark for Event Segmentation"},{"paperId":"ee12f02830a5e1f7cadd3623dfed495cf099bc82","externalIds":{"DBLP":"journals/corr/abs-2012-06567","ArXiv":"2012.06567","MAG":"3113370935","CorpusId":228375268},"title":"A Comprehensive Study of Deep Video Action Recognition"},{"paperId":"706bb87043cb169518a94e69a2ee7228feb0ebbe","externalIds":{"DBLP":"journals/tcsv/TangLLLJJYX22","MAG":"3166712493","ArXiv":"2011.05049","DOI":"10.1109/TCSVT.2021.3085907","CorpusId":226289921},"title":"Human-Centric Spatio-Temporal Video Grounding With Visual Transformers"},{"paperId":"2bacd2f2a70d756f108ad889b6bcddc79cc1ce51","externalIds":{"DBLP":"journals/corr/abs-2011-11760","MAG":"3106697459","ArXiv":"2011.11760","ACL":"2020.aacl-main.48","DOI":"10.18653/v1/2020.aacl-main.48","CorpusId":227151289},"title":"Multimodal Pretraining for Dense Video Captioning"},{"paperId":"21ce02f0ef5faaa729130f42a6a3ea39d3d9fa02","externalIds":{"MAG":"3095738461","DBLP":"conf/interspeech/MoritzWHR20","DOI":"10.21437/interspeech.2020-2757","CorpusId":225065016},"title":"All-in-One Transformer: Unifying Speech Recognition, Audio Tagging, and Event Detection"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"0732df185bdfcb9c908ec30bb441252593f58875","externalIds":{"MAG":"3095481265","DBLP":"conf/eccv/HuangXRWL20","ArXiv":"2007.10937","DOI":"10.1007/978-3-030-58548-8_41","CorpusId":220665753},"title":"MovieNet: A Holistic Dataset for Movie Understanding"},{"paperId":"8cda672bd5487ec2c67d5c217dc84ed8fb786640","externalIds":{"ArXiv":"2011.07231","DBLP":"conf/cvpr/ZhuY20a","MAG":"3104220704","DOI":"10.1109/cvpr42600.2020.00877","CorpusId":219617394},"title":"ActBERT: Learning Global-Local Video-Text Representations"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"f26fb60a6f9ad68b3e10fd809a1414deee808c0e","externalIds":{"DBLP":"journals/ijcv/DamenDFFKMMMPPW22","ArXiv":"2006.13256","MAG":"3094711996","DOI":"10.5523/bris.2g1n6qdydwa9u22shpxqzp0t8m","CorpusId":244619848},"title":"Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100"},{"paperId":"5546e6073f3b82967b12c87d6b90ba722c4b85c6","externalIds":{"MAG":"3105232955","ArXiv":"2005.00200","ACL":"2020.emnlp-main.161","DBLP":"conf/emnlp/LiCCGYL20","DOI":"10.18653/v1/2020.emnlp-main.161","CorpusId":218470055},"title":"Hero: Hierarchical Encoder for Video+Language Omni-representation Pre-training"},{"paperId":"1badccbe4a3cbf8662b924a97bbeea14fe2f1ac7","externalIds":{"DBLP":"journals/pami/DamenDFFFKMMPPW21","MAG":"3022491006","ArXiv":"2005.00343","DOI":"10.1109/TPAMI.2020.2991965","CorpusId":218470131,"PubMed":"32365017"},"title":"The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines"},{"paperId":"908cca0abefc35acc38033603714fbb1bcadc49d","externalIds":{"DBLP":"conf/cvpr/Feichtenhofer20","MAG":"3034572008","ArXiv":"2004.04730","DOI":"10.1109/cvpr42600.2020.00028","CorpusId":215548929},"title":"X3D: Expanding Architectures for Efficient Video Recognition"},{"paperId":"55303cc7773e5e0528b1dc579bcc348c0fc38569","externalIds":{"MAG":"3035172263","DBLP":"conf/cvpr/MallaDC20","ArXiv":"2003.13886","DOI":"10.1109/cvpr42600.2020.01120","CorpusId":214727763},"title":"TITAN: Future Forecast Using Action Priors"},{"paperId":"df2f2591054080d069e563cb9ca4e0592bc6df08","externalIds":{"ArXiv":"2002.07442","MAG":"3006408808","DBLP":"conf/iclr/ZhangGHS020","CorpusId":211146346},"title":"V4D: 4D Convolutional Neural Networks for Video-level Representation Learning"},{"paperId":"37dd9c725cb800fd5f336a5227f02c160b8723cb","externalIds":{"DBLP":"journals/corr/abs-2001-09099","ArXiv":"2001.09099","MAG":"3104862079","DOI":"10.1007/978-3-030-58589-1_27","CorpusId":210911606},"title":"TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"3b44345d670d816fb5a3e53262e2d35413a163ec","externalIds":{"DBLP":"conf/cvpr/ZhangZZWLG20","ArXiv":"2001.06891","MAG":"3002007234","DOI":"10.1109/cvpr42600.2020.01068","CorpusId":210839435},"title":"Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2b37cc68c9819cf0f980676935007d4135d8ac8c","externalIds":{"ArXiv":"1910.09387","DBLP":"journals/corr/abs-1910-09387","MAG":"2981182029","DOI":"10.1109/ICASSP40776.2020.9052990","CorpusId":204800739},"title":"Clotho: an Audio Captioning Dataset"},{"paperId":"c5ff974a69fd0c760b4855b819e61e89f31cfffe","externalIds":{"MAG":"2983943451","DBLP":"conf/iccv/0005LZPYZLS19","DOI":"10.1109/ICCV.2019.00852","CorpusId":207967883},"title":"Objects365: A Large-Scale, High-Quality Dataset for Object Detection"},{"paperId":"84c0528cb2aa4bdacad989b5b43441161dd4ecda","externalIds":{"ArXiv":"1907.06987","DBLP":"journals/corr/abs-1907-06987","MAG":"2961193895","CorpusId":196831809},"title":"A Short Note on the Kinetics-700 Human Action Dataset"},{"paperId":"f4327b978dec52f16b089c222c43543f8ecf4717","externalIds":{"DOI":"10.5860/choice.189890","CorpusId":240757309},"title":"arXiv"},{"paperId":"9311779489e597315488749ee6c386bfa3f3512e","externalIds":{"DBLP":"journals/corr/abs-1906-03327","ArXiv":"1906.03327","MAG":"2948859046","DOI":"10.1109/ICCV.2019.00272","CorpusId":182952863},"title":"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"},{"paperId":"4f2c1af57c056102806a184517313804f66e7447","externalIds":{"ArXiv":"1906.02467","DBLP":"conf/aaai/YuXYYZZT19","MAG":"2964220823","DOI":"10.1609/aaai.v33i01.33019127","CorpusId":69645185},"title":"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"},{"paperId":"c4798919e74411d87f7745840e45b8bcf61128ff","externalIds":{"MAG":"2945761034","DBLP":"conf/naacl/KimKLK19","ACL":"N19-1011","DOI":"10.18653/v1/N19-1011","CorpusId":174799768},"title":"AudioCaps: Generating Captions for Audios in The Wild"},{"paperId":"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796","externalIds":{"ArXiv":"1904.03493","DBLP":"conf/iccv/WangWCLWW19","MAG":"2925419377","DOI":"10.1109/ICCV.2019.00468","CorpusId":102352148},"title":"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"},{"paperId":"ca4d965ab8fd07fd236a2ec5b5c7a520077a3085","externalIds":{"ArXiv":"1904.03282","DBLP":"journals/corr/abs-1904-03282","MAG":"2979933490","DOI":"10.1109/CVPR.2019.01186","CorpusId":102351477},"title":"Weakly Supervised Video Moment Retrieval From Text Queries"},{"paperId":"f4852f5385d60e8870e30db5c65392d120e58574","externalIds":{"MAG":"2984287396","DBLP":"conf/iccv/TranWFT19","ArXiv":"1904.02811","DOI":"10.1109/ICCV.2019.00565","CorpusId":102350405},"title":"Video Classification With Channel-Separated Convolutional Networks"},{"paperId":"c41a11c0e9b8b92b4faaf97749841170b760760a","externalIds":{"DBLP":"journals/corr/abs-1904-01766","ArXiv":"1904.01766","MAG":"2981851019","DOI":"10.1109/ICCV.2019.00756","CorpusId":102483628},"title":"VideoBERT: A Joint Model for Video and Language Representation Learning"},{"paperId":"8b47b9c3c35b2b2a78bff7822605b3040f87d699","externalIds":{"MAG":"2990503944","DBLP":"journals/corr/abs-1812-03982","ArXiv":"1812.03982","DOI":"10.1109/ICCV.2019.00630","CorpusId":54463801},"title":"SlowFast Networks for Video Recognition"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","externalIds":{"MAG":"2958882215","ArXiv":"1811.10830","DBLP":"conf/cvpr/ZellersBFC19","DOI":"10.1109/CVPR.2019.00688","CorpusId":53734356},"title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"9eb3584dc1193ea9192be8df6a3b57aebd3b8548","externalIds":{"MAG":"2898200825","DBLP":"journals/corr/abs-1810-11981","ArXiv":"1810.11981","DOI":"10.1109/TPAMI.2019.2957464","CorpusId":53102207,"PubMed":"31804928"},"title":"GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild"},{"paperId":"900ab48d25b44c076e31224b7befa503d9550c53","externalIds":{"MAG":"2954928776","DBLP":"journals/corr/abs-1809-07845","ArXiv":"1809.07845","DOI":"10.1109/CVPR.2019.00552","CorpusId":52350875},"title":"LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking"},{"paperId":"fa1723b216b1f41b085b62b450b7b0bd9f2fd281","externalIds":{"MAG":"2895299763","DBLP":"conf/eccv/LiLR18","DOI":"10.1007/978-3-030-01228-1_38","CorpusId":52233948},"title":"In the Eye of Beholder: Joint Learning of Gaze and Actions in First Person Video"},{"paperId":"cc628fee1e83bfba1d581bfa128c9cb6c28ef8ad","externalIds":{"MAG":"2890447039","ArXiv":"1809.03327","DBLP":"journals/corr/abs-1809-03327","CorpusId":52181738},"title":"YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark"},{"paperId":"e7e1313061b0d56364bd2c41f017deb954bb05db","externalIds":{"DBLP":"journals/corr/abs-1809-01696","ACL":"D18-1167","ArXiv":"1809.01696","MAG":"2951721547","DOI":"10.18653/v1/D18-1167","CorpusId":52171684},"title":"TVQA: Localized, Compositional Video Question Answering"},{"paperId":"62dccab9ab715f33761a5315746ed02e48eed2a0","externalIds":{"DBLP":"journals/corr/abs-1808-01340","MAG":"2887051120","ArXiv":"1808.01340","CorpusId":51927456},"title":"A Short Note about Kinetics-600"},{"paperId":"fe82d072a8d13cfefcd575db893f3374251f04a8","externalIds":{"MAG":"2950599805","DBLP":"conf/eccv/ChenKLYF18","ArXiv":"1807.11195","DOI":"10.1007/978-3-030-01246-5_22","CorpusId":51880217},"title":"Multi-Fiber Networks for Video Recognition"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"85fb0d6cc991cf49ebc4f506b5edd44214979f65","externalIds":{"MAG":"2950637602","DBLP":"conf/eccv/DibaFSAYGG18","ArXiv":"1806.07754","DOI":"10.1007/978-3-030-01225-0_18","CorpusId":49319782},"title":"Spatio-Temporal Channel Correlation Networks for Action Classification"},{"paperId":"554626c58303f9fc6fc0c64595b1ab041147371f","externalIds":{"ArXiv":"1804.09626","DBLP":"journals/corr/abs-1804-09626","MAG":"2798963049","CorpusId":13753288},"title":"Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos"},{"paperId":"aa63893b34f523973d0692dc74ff22512daac322","externalIds":{"ArXiv":"1804.09066","MAG":"2799176631","DBLP":"journals/corr/abs-1804-09066","DOI":"10.1007/978-3-030-01216-8_43","CorpusId":19073975},"title":"ECO: Efficient Convolutional Network for Online Video Understanding"},{"paperId":"35ed258aede3df17ee20a6635364cb5fd2461049","externalIds":{"ArXiv":"1804.00819","DBLP":"conf/cvpr/ZhouZCSX18","MAG":"2949624860","DOI":"10.1109/CVPR.2018.00911","CorpusId":4564155},"title":"End-to-End Dense Video Captioning with Masked Transformer"},{"paperId":"fdf8c9c4c30c6005c2f0e92ce9db3de5ab8b5d29","externalIds":{"DBLP":"conf/cvpr/WangSGCB18","MAG":"2785198463","ArXiv":"1801.07424","DOI":"10.1109/CVPR.2018.00514","CorpusId":4369462},"title":"Revisiting Video Saliency: A Large-Scale Benchmark and a New Model"},{"paperId":"5aeef2c4f3eb125ec1db9c20392f95e64ef62b41","externalIds":{"MAG":"2910159438","DBLP":"conf/iccv/Zhao0TY19","DOI":"10.1109/ICCV.2019.00876","CorpusId":68049510},"title":"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization"},{"paperId":"815aa52cfc02961d82415f080384594639a21984","externalIds":{"MAG":"2951060209","ArXiv":"1712.04851","DBLP":"conf/eccv/XieSHTM18","DOI":"10.1007/978-3-030-01267-0_19","CorpusId":51863579},"title":"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification"},{"paperId":"7002d8c61be9f1ea210f88059df6955c88db62b7","externalIds":{"DBLP":"conf/cvpr/WeiZ0018","MAG":"2951257857","ArXiv":"1711.08565","DOI":"10.1109/CVPR.2018.00016","CorpusId":6258614},"title":"Person Transfer GAN to Bridge Domain Gap for Person Re-identification"},{"paperId":"f5ce640bbb9d6417fd0853ed88a9e7b93d72910d","externalIds":{"MAG":"2769122686","DBLP":"journals/corr/abs-1711-08200","ArXiv":"1711.08200","CorpusId":21069053},"title":"Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification"},{"paperId":"8899094797e82c5c185a0893896320ef77f60e64","externalIds":{"DBLP":"conf/cvpr/0004GGH18","MAG":"2963091558","ArXiv":"1711.07971","DOI":"10.1109/CVPR.2018.00813","CorpusId":4852647},"title":"Non-local Neural Networks"},{"paperId":"057b80e235b10799d03876ad25465208a4c64caf","externalIds":{"DBLP":"conf/mm/XuZX0Z0Z17","MAG":"2765716052","DOI":"10.1145/3123266.3123427","CorpusId":3864050},"title":"Video Question Answering via Gradually Refined Attention over Appearance and Motion"},{"paperId":"024d037d46ae933c7e12fd16af61953c7161773a","externalIds":{"DBLP":"journals/corr/abs-1711-10305","MAG":"2761659801","ArXiv":"1711.10305","DOI":"10.1109/ICCV.2017.590","CorpusId":6070160},"title":"Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks"},{"paperId":"fb37561499573109fc2cebb6a7b08f44917267dd","externalIds":{"MAG":"2963420686","DBLP":"journals/corr/abs-1709-01507","ArXiv":"1709.01507","DOI":"10.1109/CVPR.2018.00745","CorpusId":140309863},"title":"Squeeze-and-Excitation Networks"},{"paperId":"07c83f544d0604e6bab5d741b0bf9a3621d133da","externalIds":{"MAG":"2963616706","DBLP":"journals/corr/abs-1708-07632","ArXiv":"1708.07632","DOI":"10.1109/ICCVW.2017.373","CorpusId":400882},"title":"Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"},{"paperId":"ee909ad489244016cf301bb7d7d8eeea423dbf35","externalIds":{"MAG":"2963017553","DBLP":"conf/iccv/HendricksWSSDR17","ArXiv":"1708.01641","DOI":"10.1109/ICCV.2017.618","CorpusId":1061352},"title":"Localizing Moments in Video with Natural Language"},{"paperId":"7e6cc717311c9c3dcf7279bc44e0c25b29650c15","externalIds":{"MAG":"2952986545","ArXiv":"1707.00836","DBLP":"journals/corr/KimHCZ17","DOI":"10.24963/ijcai.2017/280","CorpusId":9096634},"title":"DeepStory: Video Story QA by Deep Embedded Memory Networks"},{"paperId":"b68811a9b5cafe4795a11c1048541750068b7ad0","externalIds":{"MAG":"2949901290","ArXiv":"1706.04261","DBLP":"conf/iccv/GoyalKMMWKHFYMH17","DOI":"10.1109/ICCV.2017.622","CorpusId":834612},"title":"The â€œSomething Somethingâ€ Video Database for Learning and Evaluating Visual Common Sense"},{"paperId":"b61a3f8b80bbd44f24544dc915f52fd30bbdf485","externalIds":{"ArXiv":"1705.07750","MAG":"2619082050","DBLP":"conf/cvpr/CarreiraZ17","DOI":"10.1109/CVPR.2017.502","CorpusId":206596127},"title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"},{"paperId":"86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6","externalIds":{"DBLP":"journals/corr/KayCSZHVVGBNSZ17","ArXiv":"1705.06950","MAG":"2619947201","CorpusId":27300853},"title":"The Kinetics Human Action Video Dataset"},{"paperId":"e9bd6f0b04a0ddf9fcdf3a5fd1cfe87f8ae9cfff","externalIds":{"DBLP":"conf/iccv/GaoSYN17","MAG":"2964089981","ArXiv":"1705.02101","DOI":"10.1109/ICCV.2017.563","CorpusId":31663499},"title":"TALL: Temporal Activity Localization via Language Query"},{"paperId":"96dd1fc39a368d23291816d57763bc6eb4f7b8d6","externalIds":{"MAG":"2963916161","DBLP":"journals/corr/KrishnaHRLN17","ArXiv":"1705.00754","DOI":"10.1109/ICCV.2017.83","CorpusId":1026139},"title":"Dense-Captioning Events in Videos"},{"paperId":"b2f521c02c6ed3080c5fe123e938cdf4555e6fd2","externalIds":{"DBLP":"conf/cvpr/JangSYKK17","ArXiv":"1704.04497","MAG":"2606982687","DOI":"10.1109/CVPR.2017.149","CorpusId":3030826},"title":"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"},{"paperId":"e10a5e0baf2aa87d804795af071808a9377cc80a","externalIds":{"MAG":"2784025607","DBLP":"conf/aaai/ZhouXC18","ArXiv":"1703.09788","DOI":"10.1609/aaai.v32i1.12342","CorpusId":19713015},"title":"Towards Automatic Learning of Procedures From Web Instructional Videos"},{"paperId":"645de797f936cb19c1b8dba3b862543645510544","externalIds":{"MAG":"2950554226","DBLP":"conf/cvpr/DibaSG17","ArXiv":"1611.06678","DOI":"10.1109/CVPR.2017.168","CorpusId":6709077},"title":"Deep Temporal Linear Encoding Networks"},{"paperId":"f6e0856b4a9199fa968ac00da612a9407b5cb85c","externalIds":{"DBLP":"conf/cvpr/XieGDTH17","MAG":"2953328958","ArXiv":"1611.05431","DOI":"10.1109/CVPR.2017.634","CorpusId":8485068},"title":"Aggregated Residual Transformations for Deep Neural Networks"},{"paperId":"27850781e39df9f750e05409b8072261124068e8","externalIds":{"MAG":"2518876086","DBLP":"conf/eccv/MuellerSG16","DOI":"10.1007/978-3-319-46448-0_27","CorpusId":10184155},"title":"A Benchmark and Simulator for UAV Tracking"},{"paperId":"c9a1e8e1ba2913ef0bdf1c5eaaa1ac0a79be3716","externalIds":{"MAG":"2524365899","DBLP":"journals/corr/Abu-El-HaijaKLN16","ArXiv":"1609.08675","CorpusId":11241677},"title":"YouTube-8M: A Large-Scale Video Classification Benchmark"},{"paperId":"85aefde69e916523d9587b6abd01419420039474","externalIds":{"MAG":"2511791013","DBLP":"journals/corr/RistaniSZCT16","ArXiv":"1609.01775","DOI":"10.1007/978-3-319-48881-3_2","CorpusId":5584770},"title":"Performance Measures and a Data Set for Multi-target, Multi-camera Tracking"},{"paperId":"65ba5f3927633293112cf1bbdf6641d4d15638cc","externalIds":{"DBLP":"conf/eccv/ZengCNS16","MAG":"2949484614","ArXiv":"1608.07068","DOI":"10.1007/978-3-319-46475-6_38","CorpusId":6155397},"title":"Title Generation for User Generated Videos"},{"paperId":"ea3d7de6c0880e14455b9acb28f1bc1234321456","externalIds":{"DBLP":"journals/corr/WangXW0LTG16","MAG":"2507009361","ArXiv":"1608.00859","DOI":"10.1007/978-3-319-46484-8_2","CorpusId":5711057},"title":"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition"},{"paperId":"1b3895364de8a12a43b0a4a258fa755cab17ffb7","externalIds":{"MAG":"1162301944","DBLP":"journals/mta/MironicaDIS16","DOI":"10.1007/s11042-015-2819-7","CorpusId":4117934},"title":"A modified vector of locally aggregated descriptors approach for fast video classification"},{"paperId":"f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f","externalIds":{"DBLP":"journals/corr/AndersonFJG16","MAG":"2950201573","ArXiv":"1607.08822","DOI":"10.1007/978-3-319-46454-1_24","CorpusId":11933981},"title":"SPICE: Semantic Propositional Image Caption Evaluation"},{"paperId":"5bb86aacb5faaea4876a3241841d0c2447a3e640","externalIds":{"MAG":"2315268655","DOI":"10.2307/1513223","CorpusId":59713779},"title":"Auditory Scene Analysis: The Perceptual Organization of Sound by Albert Bregman (review)"},{"paperId":"b8e2e9f3ba008e28257195ec69a00e07f260131d","externalIds":{"DBLP":"conf/cvpr/XuMYR16","MAG":"2425121537","DOI":"10.1109/CVPR.2016.571","CorpusId":206594535},"title":"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"},{"paperId":"05e9e85b5137016c93d042170e82f77bb551a108","externalIds":{"DBLP":"conf/cvpr/PerazziPMGGS16","MAG":"2470139095","DOI":"10.1109/CVPR.2016.85","CorpusId":1949934},"title":"A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation"},{"paperId":"9d9aced120e530484609164c836da64548693484","externalIds":{"ArXiv":"1604.06573","MAG":"2342662179","DBLP":"conf/cvpr/FeichtenhoferPZ16","DOI":"10.1109/CVPR.2016.213","CorpusId":12289712},"title":"Convolutional Two-Stream Network Fusion for Video Action Recognition"},{"paperId":"179462c2a03b1a5c3f98fac86c992c116e152b82","externalIds":{"MAG":"2951864506","DBLP":"journals/pami/VarolLS18","ArXiv":"1604.04494","DOI":"10.1109/TPAMI.2017.2712608","CorpusId":206767788,"PubMed":"28600238"},"title":"Long-Term Temporal Convolutions for Action Recognition"},{"paperId":"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6","externalIds":{"DBLP":"conf/cvpr/LiSCTGJL16","MAG":"2342350679","ArXiv":"1604.02748","DOI":"10.1109/CVPR.2016.502","CorpusId":6262415},"title":"TGIF: A New Dataset and Benchmark on Animated GIF Description"},{"paperId":"c8c494ee5488fe20e0aa01bddf3fc4632086d654","externalIds":{"DBLP":"journals/corr/CordtsORREBFRS16","MAG":"2953139137","ArXiv":"1604.01685","DOI":"10.1109/CVPR.2016.350","CorpusId":502946},"title":"The Cityscapes Dataset for Semantic Urban Scene Understanding"},{"paperId":"21334d1aac5422da88780f8e24e181bfa15ef0e1","externalIds":{"MAG":"2337252826","DBLP":"conf/eccv/SigurdssonVWFLG16","ArXiv":"1604.01753","DOI":"10.1007/978-3-319-46448-0_31","CorpusId":18061547},"title":"Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding"},{"paperId":"ac0d88ca5f75a4a80da90365c28fa26f1a26d4c4","externalIds":{"ArXiv":"1603.00831","DBLP":"journals/corr/MilanL0RS16","MAG":"2291627510","CorpusId":15430338},"title":"MOT16: A Benchmark for Multi-Object Tracking"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"e24c261f5cfcd58a595efb7ca684aedcb2a2f22c","externalIds":{"DBLP":"conf/iccv/ZhengSTWWT15","MAG":"2204750386","DOI":"10.1109/ICCV.2015.133","CorpusId":14991802},"title":"Scalable Person Re-identification: A Benchmark"},{"paperId":"915e68b9de2f982e96c29a56a5610f2bc1bbb34a","externalIds":{"DBLP":"journals/tomccap/MinZGY16","MAG":"2533370895","DOI":"10.1145/2996463","CorpusId":11534076},"title":"Fixation prediction through multimodal analysis"},{"paperId":"98212372180ca3826c74ac5d05a600cc20210ec3","externalIds":{"MAG":"906515803","DBLP":"journals/prl/SekmaMA15","DOI":"10.1016/j.patrec.2015.06.029","CorpusId":27195242},"title":"Human action recognition based on multi-layer Fisher vector encoding method"},{"paperId":"933fe12164f8e4c6c1075dc557b1b3fdcb4f5168","externalIds":{"DBLP":"journals/spic/KoutrasM15","MAG":"1152443276","DOI":"10.1016/j.image.2015.08.004","CorpusId":12057859},"title":"A perceptually based spatio-temporal computational framework for visual saliency estimation"},{"paperId":"cbf89cb4e107fb59e119ae619bcfe48e1964e033","externalIds":{"DBLP":"conf/cvpr/SongVSJ15","MAG":"1924343884","DOI":"10.1109/CVPR.2015.7299154","CorpusId":7675635},"title":"TVSum: Summarizing web videos using titles"},{"paperId":"0a28efacb92d16e6e0dd4d87b5aca91b28be8853","externalIds":{"DBLP":"conf/cvpr/HeilbronEGN15","MAG":"1927052826","DOI":"10.1109/CVPR.2015.7298698","CorpusId":1710722},"title":"ActivityNet: A large-scale video benchmark for human activity understanding"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","externalIds":{"ArXiv":"1505.04870","DBLP":"conf/iccv/PlummerWCCHL15","MAG":"2568262903","DOI":"10.1007/s11263-016-0965-7","CorpusId":6941275},"title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"5418b2a482720e013d487a385c26fae0f017c6a6","externalIds":{"MAG":"1923404803","DBLP":"conf/cvpr/NgHVVMT15","ArXiv":"1503.08909","DOI":"10.1109/CVPR.2015.7299101","CorpusId":4245530},"title":"Beyond short snippets: Deep networks for video classification"},{"paperId":"d25c65d261ea0e6a458be4c50c40ffe5bc508f77","externalIds":{"MAG":"1522734439","DBLP":"conf/iccv/TranBFTP15","DOI":"10.1109/ICCV.2015.510","CorpusId":1122604},"title":"Learning Spatiotemporal Features with 3D Convolutional Networks"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","externalIds":{"ACL":"D14-1086","DBLP":"conf/emnlp/KazemzadehOMB14","MAG":"2251512949","DOI":"10.3115/v1/D14-1086","CorpusId":6308361},"title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"e15cf50aa89fee8535703b9f9512fca5bfc43327","externalIds":{"DBLP":"journals/corr/SzegedyLJSRAEVR14","MAG":"2097117768","ArXiv":"1409.4842","DOI":"10.1109/CVPR.2015.7298594","CorpusId":206592484},"title":"Going deeper with convolutions"},{"paperId":"9d4dbb54439490d17ae99d394725aada1bbb877a","externalIds":{"MAG":"578775697","DBLP":"conf/eccv/ShuYS14","DOI":"10.1007/978-3-319-16178-5_38","CorpusId":15362854},"title":"Action Detection with Improved Dense Trajectories and Sliding Window"},{"paperId":"799bf307438ec2171e6f0bd5b8040f678d5b28da","externalIds":{"MAG":"2595881357","DBLP":"conf/eccv/GygliGRG14","DOI":"10.1007/978-3-319-10584-0_33","CorpusId":2111093},"title":"Creating Summaries from User Videos"},{"paperId":"5c7adde982efb24c3786fa2d1f65f40a64e2afbf","externalIds":{"DBLP":"conf/eccv/SunFS14","MAG":"329267344","DOI":"10.1007/978-3-319-10590-1_51","CorpusId":14560600},"title":"Ranking Domain-Specific Highlights by Analyzing Edited Videos"},{"paperId":"0a9f2486ded05be499a7400ae548f0b90f2d8e34","externalIds":{"MAG":"2155479901","DOI":"10.1167/14.8.5","CorpusId":18287834,"PubMed":"24993019"},"title":"How saliency, faces, and sound influence gaze in dynamic social scenes."},{"paperId":"6d4c9c923e9f145d1c01a2de2afc38ec23c44253","externalIds":{"MAG":"2308045930","DBLP":"conf/cvpr/KarpathyTSLSF14","DOI":"10.1109/CVPR.2014.223","CorpusId":206592218},"title":"Large-Scale Video Classification with Convolutional Neural Networks"},{"paperId":"6bd36e9fd0ef20a3074e1430a6cc601e6d407fc3","externalIds":{"DBLP":"conf/cvpr/LiZXW14","MAG":"1982925187","DOI":"10.1109/CVPR.2014.27","CorpusId":938105},"title":"DeepReID: Deep Filter Pairing Neural Network for Person Re-identification"},{"paperId":"c05d4f9ce61ebb00d12b92bea52ca651c4d2e656","externalIds":{"DBLP":"journals/cviu/SobralV14","MAG":"2071860582","DOI":"10.1016/j.cviu.2013.12.005","CorpusId":12783309},"title":"A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos"},{"paperId":"261a2cb5ac0b550f8174d3d75f436c6d7b73e872","externalIds":{"DBLP":"journals/cviu/BouwmansZ14","MAG":"2091741383","DOI":"10.1016/j.cviu.2013.11.009","CorpusId":38633808},"title":"Robust PCA via Principal Component Pursuit: A review for a comparative evaluation in video surveillance"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"bfba194dfd9c7c27683082aa8331adc4c5963a0d","externalIds":{"MAG":"2089961441","DBLP":"conf/cvpr/WuLY13","DOI":"10.1109/CVPR.2013.312","CorpusId":1660289},"title":"Online Object Tracking: A Benchmark"},{"paperId":"da9e411fcf740569b6b356f330a1d0fc077c8d7c","externalIds":{"MAG":"24089286","ArXiv":"1212.0402","DBLP":"journals/corr/abs-1212-0402","CorpusId":7197134},"title":"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"},{"paperId":"eab7aa438df70ecefc43c0a6abad1a6592a3b2a1","externalIds":{"DBLP":"journals/scholarpedia/Lindeberg12","MAG":"2005750530","DOI":"10.4249/scholarpedia.10491","CorpusId":5900376},"title":"Scale Invariant Feature Transform"},{"paperId":"8b3b8848a311c501e704c45c6d50430ab7068956","externalIds":{"MAG":"2126579184","DBLP":"conf/iccv/KuehneJGPS11","DOI":"10.1109/ICCV.2011.6126543","CorpusId":206769852},"title":"HMDB: A large video database for human motion recognition"},{"paperId":"72729882f8fa3d9084eaece513f6bf9630be5901","externalIds":{"DBLP":"conf/acl/ChenD11","MAG":"2164290393","ACL":"P11-1020","CorpusId":215717103},"title":"Collecting Highly Parallel Data for Paraphrase Evaluation"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"66bf90eb1a737279abf028239bd4d0a318d3328e","externalIds":{"DBLP":"conf/cvpr/MarszalekLS09","MAG":"2163292664","DOI":"10.1109/CVPR.2009.5206557","CorpusId":3155054},"title":"Actions in context"},{"paperId":"67585b9154d5af7e75e26b2c226147d922658ce2","externalIds":{"DBLP":"conf/acivs/HazelhoffHW08","MAG":"1526326465","DOI":"10.1007/978-3-540-88458-3_27","CorpusId":9858145},"title":"Video-Based Fall Detection in the Home Using Principal Component Analysis"},{"paperId":"1c2629d53fd73ee42fb9a67b4d656688ef6a005f","externalIds":{"MAG":"2101194540","DBLP":"conf/cvpr/RodriguezAS08","DOI":"10.1109/CVPR.2008.4587727","CorpusId":83721},"title":"Action MACH a spatio-temporal Maximum Average Correlation Height filter for action recognition"},{"paperId":"cdbb606ae47c64049262dfbd3bb147d3f4ba8420","externalIds":{"MAG":"2185187529","DBLP":"journals/cviu/BayETG08","DOI":"10.1016/j.cviu.2007.09.014","CorpusId":14777911},"title":"Speeded-Up Robust Features (SURF)"},{"paperId":"342fe6a6338e73fd4d34c4f37f41e3bbad274dd2","externalIds":{"DOI":"10.1002/9781119991083.ch42","CorpusId":219375210},"title":"Networks"},{"paperId":"cf97f2359940b547db659be98c1f41a4db5bca85","externalIds":{"DOI":"10.1111/j.1467-8721.2007.00480.x","CorpusId":32043826,"PubMed":"22468032"},"title":"Event Segmentation"},{"paperId":"e8b12467bdc20bde976750b8a28decdb33246d1d","externalIds":{"MAG":"2161969291","DBLP":"conf/cvpr/DalalT05","DOI":"10.1109/CVPR.2005.177","CorpusId":206590483},"title":"Histograms of oriented gradients for human detection"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"82b16ff5e02705df7e77acd1655fd1d51473d51f","externalIds":{"MAG":"2128130395","DOI":"10.1109/ICPR.2004.293","CorpusId":18904386},"title":"Detecting human motion with support vector machines"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"de8e96cdb284b636a81390723c319d92ae2f0611","externalIds":{"MAG":"2141425367","DBLP":"conf/cvpr/LiuC03","DOI":"10.1109/CVPR.2003.1211373","CorpusId":296341},"title":"Video-based face recognition using adaptive hidden Markov models"},{"paperId":"0615a341e2b608fc2151dd6619c4c841f9207391","externalIds":{"MAG":"2170770078","DOI":"10.1109/ICMLC.2002.1167381","CorpusId":61680934},"title":"Automatic video classification using decision tree method"},{"paperId":"75c560508dd4aebd1bc7a35c917c8d2ebdbed9e7","externalIds":{"DOI":"10.1080/00091380009601770","CorpusId":218586724},"title":"In Short"},{"paperId":"90ce5a14cc95e4d249c1257ae196b5a67cbc348e","externalIds":{"DBLP":"journals/corr/abs-2402-16050","DOI":"10.48550/arXiv.2402.16050","CorpusId":273850665},"title":"LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding"},{"paperId":"1959ec1b999db1c2c97d4acc6fe15deaad6376ca","externalIds":{"DBLP":"journals/corr/abs-2406-10889","DOI":"10.48550/arXiv.2406.10889","CorpusId":283285527},"title":"VELOCITI: Can Video-Language Models Bind Semantic Concepts through Time?"},{"paperId":"f329ae90bb7e6655b8ca3a19c4d06dbed7792d6f","externalIds":{"DBLP":"journals/corr/abs-2403-15377","DOI":"10.48550/arXiv.2403.15377","CorpusId":268667436},"title":"InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding"},{"paperId":"912f317d8e9c80134ab93d9796be4b6a2840a712","externalIds":{"DBLP":"journals/corr/abs-2406-19875","DOI":"10.48550/arXiv.2406.19875","CorpusId":270845683},"title":"InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in Very Long Video Understanding"},{"paperId":"069f43a19f66214fa91df6a0ec28020ec2a9046a","externalIds":{"DBLP":"journals/corr/abs-2401-09386","DOI":"10.48550/arXiv.2401.09386","CorpusId":271065048},"title":"Tri2-plane: Volumetric Avatar Reconstruction with Feature Pyramid"},{"paperId":"c44393114047e0f9c32e45b381970d50ed503260","externalIds":{"DBLP":"journals/corr/abs-2402-13546","DOI":"10.48550/arXiv.2402.13546","CorpusId":267770673},"title":"LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs"},{"paperId":"38558608dedc495dd748ea3381b08b0d2ffccf56","externalIds":{"DBLP":"journals/corr/abs-2406-04886","DOI":"10.48550/arXiv.2406.04886","CorpusId":273102382},"title":"Seeing the Unseen: Visual Metaphor Captioning for Videos"},{"paperId":"285e1e01d9bc91eaa7b2fec4fabd8ef5dd081106","externalIds":{"DBLP":"journals/tmm/ShaoHDP24","DOI":"10.1109/TMM.2024.3369863","CorpusId":268049678},"title":"DCMSTRD: End-to-end Dense Captioning via Multi-Scale Transformer Decoding"},{"paperId":"e8f12eb28184dfd0b2e3de58636ca0e566db4a6d","externalIds":{"DBLP":"journals/corr/abs-2403-16276","DOI":"10.48550/arXiv.2403.16276","CorpusId":276421112},"title":"AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue"},{"paperId":"af72203da1d04f344d81496b9d28bf04ed40be8e","externalIds":{"DBLP":"journals/corr/abs-2406-09390","DOI":"10.48550/arXiv.2406.09390","CorpusId":279074945},"title":"LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living"},{"paperId":"c735320ee0e5b2e580043b0a1504e592c0840e24","externalIds":{"DBLP":"journals/corr/abs-2406-04264","DOI":"10.48550/arXiv.2406.04264","CorpusId":276318480},"title":"MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding"},{"paperId":"d8415838384db8c056622c81922a47a68b9cdb26","externalIds":{"DBLP":"journals/corr/abs-2403-01422","DOI":"10.48550/arXiv.2403.01422","CorpusId":280422590},"title":"MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies"},{"paperId":"447e1e50d925c9648af6851d662b7878e0811207","externalIds":{"DBLP":"journals/corr/abs-2406-10221","DOI":"10.48550/arXiv.2406.10221","CorpusId":280662279},"title":"Short Film Dataset (SFD): A Benchmark for Story-Level Video Understanding"},{"paperId":"5b2723ee7a8cac557fbb69c1213c089d066d2f7a","externalIds":{"DBLP":"conf/kes/ZarzaCC23","DOI":"10.1016/j.procs.2023.09.101","CorpusId":266144044},"title":"Socratic Video Understanding on Unmanned Aerial Vehicles"},{"paperId":"ddccc47b30b5e895819dfca15b95d4c125a386f9","externalIds":{"DBLP":"journals/corr/abs-2310-00068","DOI":"10.48550/arXiv.2310.00068","CorpusId":264290069},"title":"Emotional Listener Portrait: Neural Listener Head Generation with Emotion"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","externalIds":{"ArXiv":"2303.17580","DBLP":"journals/corr/abs-2303-17580","DOI":"10.48550/arXiv.2303.17580","CorpusId":257833781},"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"a8c61ebbfd4d69a08e9ea581b93b5dd7b52363ce","externalIds":{"DBLP":"journals/tmm/ShaoHDP23","DOI":"10.1109/TMM.2023.3241517","CorpusId":256565556},"title":"Textual Context-Aware Dense Captioning With Diverse Words"},{"paperId":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","externalIds":{"DBLP":"journals/corr/abs-2312-08870","DOI":"10.48550/arXiv.2312.08870","CorpusId":276770574},"title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens"},{"paperId":"85ec64ab2af6bdfe5eb356facbb54ac04832c18f","externalIds":{"DOI":"10.1007/978-3-031-30566-5","CorpusId":259225335},"title":"XR-Metaverse Cases: Business Application of AR, VR, XR and Metaverse"},{"paperId":"2b3fa12b0eebe7daf82c4a06d71a3370428e6226","externalIds":{"DBLP":"conf/eccv/LiZZFYZ22","DOI":"10.1007/978-3-031-20044-1_30","CorpusId":253099838},"title":"TransVLAD: Focusing on Locally Aggregated Descriptors for Few-Shot Learning"},{"paperId":"045ccef36eabb0230428a343558e773dc6423491","externalIds":{"DBLP":"journals/corr/abs-2206-05658","DOI":"10.48550/arXiv.2206.05658","CorpusId":268126913},"title":"Fine-tuning Pre-trained Language Models with Noise Stability Regularization"},{"paperId":"2362f0730e888d6247edbcfeefc14e8291b174e6","externalIds":{"DBLP":"conf/eccv/KristanLMFPKCDZLDBZZYYCMFBBCCCCCCCCCCC22","DOI":"10.1007/978-3-031-25085-9_25","CorpusId":256903057},"title":"The Tenth Visual Object Tracking VOT2022 Challenge Results"},{"paperId":"2845c94a0cbc85dedb7b6f7b8f0167ee74fa3946","externalIds":{"CorpusId":261887356},"title":"and generation"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"5487c3f9b226a92a30bf214aa37d071a4dd9437c","externalIds":{"MAG":"2498738402","DOI":"10.1007/978-1-4939-3435-5_16","CorpusId":62934357},"title":"Multimodal Saliency Models for Videos"},{"paperId":"10863b5b7c7616c6a53a203e6d5ca2d19e6bacbb","externalIds":{"CorpusId":3033434},"title":"This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Modeling, Clustering, and Segmen"},{"paperId":"070874b011f8eb2b18c8aa521ad0a7a932b4d9ad","externalIds":{"CorpusId":753512},"title":"Author manuscript, published in \"International Conference on Computer Vision (2013)\" Action Recognition with Improved Trajectories"}]}