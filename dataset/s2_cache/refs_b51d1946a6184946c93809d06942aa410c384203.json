{"references":[{"paperId":"599be84b0ffd0eda61a35a2271b241b34f22830f","externalIds":{"ArXiv":"2410.18585","DBLP":"journals/corr/abs-2410-18585","DOI":"10.48550/arXiv.2410.18585","CorpusId":273549433},"title":"Aligning CodeLLMs with Direct Preference Optimization"},{"paperId":"7796f56c7b9c152ac9573c8bb34f716aa78b4e6d","externalIds":{"ArXiv":"2410.18451","DBLP":"journals/corr/abs-2410-18451","DOI":"10.48550/arXiv.2410.18451","CorpusId":273549327},"title":"Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs"},{"paperId":"ebeb20304f0d0a255f5f087ddd99f66d7873d7ad","externalIds":{"DBLP":"conf/iclr/LyuY0YRRR25","ArXiv":"2410.07672","DOI":"10.48550/arXiv.2410.07672","CorpusId":273233831},"title":"MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization"},{"paperId":"158730a16cd4cc4d4842e42e5c2a0843b75d527c","externalIds":{"DBLP":"journals/corr/abs-2410-03742","ArXiv":"2410.03742","DOI":"10.48550/arXiv.2410.03742","CorpusId":273185729},"title":"Beyond Scalar Reward Model: Learning Generative Judge from Preference Data"},{"paperId":"519d5ccbd5aec517ba987209e17afd4741ac9b8a","externalIds":{"DBLP":"journals/corr/abs-2409-20370","ArXiv":"2409.20370","DOI":"10.48550/arXiv.2409.20370","CorpusId":272987127},"title":"The Perfect Blend: Redefining RLHF with Mixture of Judges"},{"paperId":"3707939a856655fcabf0acd5cba1a1009987b439","externalIds":{"DBLP":"journals/corr/abs-2408-15240","ArXiv":"2408.15240","DOI":"10.48550/arXiv.2408.15240","CorpusId":271963324},"title":"Generative Verifiers: Reward Modeling as Next-Token Prediction"},{"paperId":"e1a642026fb46a8b8a868862bcf0728e8d215d7e","externalIds":{"ArXiv":"2408.08152","DBLP":"journals/corr/abs-2408-08152","DOI":"10.48550/arXiv.2408.08152","CorpusId":271874725},"title":"DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search"},{"paperId":"4814f1744ebfbbf0e987ee4242a930dd2d3d09a5","externalIds":{"DBLP":"journals/corr/abs-2408-06195","ArXiv":"2408.06195","DOI":"10.48550/arXiv.2408.06195","CorpusId":271855296},"title":"Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers"},{"paperId":"df90ee11ed6378635f22e6d0061cf67dd0bacd13","externalIds":{"ArXiv":"2407.19594","DBLP":"journals/corr/abs-2407-19594","DOI":"10.48550/arXiv.2407.19594","CorpusId":271533411},"title":"Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge"},{"paperId":"1ef297466d7250b26e47b3bd405d8a8662567731","externalIds":{"ArXiv":"2407.15762","DBLP":"conf/emnlp/WangKSADMGLGDRF24","DOI":"10.18653/v1/2024.findings-emnlp.118","CorpusId":271328237},"title":"Conditional Language Policy: A General Framework For Steerable Multi-Objective Finetuning"},{"paperId":"54fb839f621e3fe787437ab8ca5f37e7e4726bfe","externalIds":{"ArXiv":"2407.10671","DBLP":"journals/corr/abs-2407-10671","DOI":"10.48550/arXiv.2407.10671","CorpusId":271212307},"title":"Qwen2 Technical Report"},{"paperId":"8deb5fd40e310dc4feb27f7db7019e734b44631b","externalIds":{"DBLP":"journals/corr/abs-2407-00215","ArXiv":"2407.00215","DOI":"10.48550/arXiv.2407.00215","CorpusId":270844127},"title":"LLM Critics Help Catch LLM Bugs"},{"paperId":"0c4bf8ba0a17f6066490664e83d28567dd089a40","externalIds":{"ArXiv":"2406.14024","DBLP":"conf/acl/GaoCXWZLLLZXLC25","DOI":"10.48550/arXiv.2406.14024","CorpusId":270620737},"title":"LLM Critics Help Catch Bugs in Mathematics: Towards a Better Mathematical Verifier with Natural Language Feedback"},{"paperId":"a5f26555194d50955f6b3fdafb04d4330cb272dc","externalIds":{"DBLP":"journals/corr/abs-2406-11191","ArXiv":"2406.11191","DOI":"10.48550/arXiv.2406.11191","CorpusId":270560824},"title":"A Survey on Human Preference Learning for Large Language Models"},{"paperId":"394e14ae60bae4c41162056717d9e30a8168abaa","externalIds":{"DBLP":"journals/corr/abs-2406-11176","ACL":"2024.emnlp-main.93","ArXiv":"2406.11176","DOI":"10.48550/arXiv.2406.11176","CorpusId":270558898},"title":"Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement"},{"paperId":"b07880f7b932c7dc8b1091c6c80c9de3e7d64185","externalIds":{"DBLP":"conf/nips/YangDLZZ24","ArXiv":"2406.10216","DOI":"10.48550/arXiv.2406.10216","CorpusId":270521260},"title":"Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs"},{"paperId":"5387445a58a958422a8cfd297e6a611aade0f0e8","externalIds":{"DBLP":"conf/iclr/YanMLZXDY25","ArXiv":"2406.07327","DOI":"10.48550/arXiv.2406.07327","CorpusId":270380285},"title":"3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"},{"paperId":"2c27967c92fda392cdf0654281df5a902bb62240","externalIds":{"DBLP":"journals/corr/abs-2406-07394","ArXiv":"2406.07394","CorpusId":270379580},"title":"Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B"},{"paperId":"cd27f45bc760447fb4de3209e2381ea3493bbd57","externalIds":{"DBLP":"conf/nips/ZhangZHYD024","ArXiv":"2406.03816","DOI":"10.48550/arXiv.2406.03816","CorpusId":270285630},"title":"ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search"},{"paperId":"f32bcc2155997110a7905da050df4c8404867b24","externalIds":{"DBLP":"journals/corr/abs-2406-06592","ArXiv":"2406.06592","CorpusId":270379625},"title":"Improve Mathematical Reasoning in Language Models by Automated Process Supervision"},{"paperId":"f0dece803297c0f368142a33aaa3afdc7f3b42a4","externalIds":{"ArXiv":"2405.21040","DBLP":"journals/corr/abs-2405-21040","DOI":"10.48550/arXiv.2405.21040","CorpusId":270199808},"title":"Direct Alignment of Language Models via Quality-Aware Self-Refinement"},{"paperId":"49d25bf7d5f16da3f201f607d57411dd50bcf5f6","externalIds":{"DBLP":"conf/nips/RameshHCMSBB24","ArXiv":"2405.20304","DOI":"10.48550/arXiv.2405.20304","CorpusId":270123646},"title":"Group Robust Preference Optimization in Reward-free RLHF"},{"paperId":"200a19739ef76cb91c490be72d409f0fb0468901","externalIds":{"DBLP":"journals/corr/abs-2405-20215","ArXiv":"2405.20215","DOI":"10.48550/arXiv.2405.20215","CorpusId":270123022},"title":"TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models"},{"paperId":"126900598c17b4149d4836652116d578507f5033","externalIds":{"DBLP":"journals/tmlr/BadrinathAX25","ArXiv":"2405.17956","CorpusId":270067933},"title":"Unified Preference Optimization: Language Model Alignment Beyond the Preference Frontier"},{"paperId":"a11d3cc04c3293df3e7ea3247c561f42fc36d1c5","externalIds":{"DBLP":"conf/iclr/0002ZLYT25","ArXiv":"2405.14953","CorpusId":270045560},"title":"MallowsPO: Fine-Tune Your LLM with Preference Dispersions"},{"paperId":"c3f1fae241a3c2449e675ab750873d800f95513c","externalIds":{"DBLP":"journals/corr/abs-2405-14734","ArXiv":"2405.14734","DOI":"10.48550/arXiv.2405.14734","CorpusId":269983560},"title":"SimPO: Simple Preference Optimization with a Reference-Free Reward"},{"paperId":"3648515cc35b517cdf60331cc4870e24616f9939","externalIds":{"DBLP":"journals/corr/abs-2405-14333","ArXiv":"2405.14333","DOI":"10.48550/arXiv.2405.14333","CorpusId":269983755},"title":"DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data"},{"paperId":"58bf4853effe89282eb8d8cd1e0dd1b782eee62f","externalIds":{"ArXiv":"2405.13516","DBLP":"journals/corr/abs-2405-13516","DOI":"10.48550/arXiv.2405.13516","CorpusId":269983203},"title":"LIRE: listwise reward enhancement for preference alignment"},{"paperId":"4490d077087df12e47c6f8d77a217179980c7516","externalIds":{"DBLP":"journals/aiethics/JiaoAXP25","ArXiv":"2406.18841","DOI":"10.1007/s43681-025-00814-5","CorpusId":270764497},"title":"Navigating LLM ethics: advancements, challenges, and future directions"},{"paperId":"1e53e98e8709748a6385137d8f240787c12fcfd4","externalIds":{"ArXiv":"2405.07863","DBLP":"journals/tmlr/Dong0000Z0SX024","DOI":"10.48550/arXiv.2405.07863","CorpusId":269757968},"title":"RLHF Workflow: From Reward Modeling to Online RLHF"},{"paperId":"df8c3a325419d63366b9b347739fcbf3e2c4d22c","externalIds":{"DBLP":"journals/corr/abs-2405-00675","ArXiv":"2405.00675","DOI":"10.48550/arXiv.2405.00675","CorpusId":269484698},"title":"Self-Play Preference Optimization for Language Model Alignment"},{"paperId":"38333f6e8f0388968edc4b2ea7a683ce69677e69","externalIds":{"ArXiv":"2405.00451","DBLP":"journals/corr/abs-2405-00451","DOI":"10.48550/arXiv.2405.00451","CorpusId":269484186},"title":"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning"},{"paperId":"fa018aa098c28bd694b316ddf7e213641e7f86ad","externalIds":{"DBLP":"journals/corr/abs-2404-15269","ArXiv":"2404.15269","DOI":"10.48550/arXiv.2404.15269","CorpusId":269302910},"title":"Aligning LLM Agents by Learning Latent Preference from User Edits"},{"paperId":"db407c3a60c6dc768fde8dd1088dab3be951f04e","externalIds":{"DBLP":"journals/corr/abs-2404-14723","ArXiv":"2404.14723","DOI":"10.48550/arXiv.2404.14723","CorpusId":269303161},"title":"Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks"},{"paperId":"315a5cb5dbbbe2be8468b2bb7c62ea72af8930da","externalIds":{"DBLP":"journals/corr/abs-2404-13846","ArXiv":"2404.13846","ACL":"2024.emnlp-main.1266","DOI":"10.48550/arXiv.2404.13846","CorpusId":269293711},"title":"Filtered Direct Preference Optimization"},{"paperId":"370fb62e60f80081015d591f8c10c5a59a56a32d","externalIds":{"DBLP":"journals/corr/abs-2404-09656","ArXiv":"2404.09656","DOI":"10.48550/arXiv.2404.09656","CorpusId":269149572},"title":"Learn Your Reference Model for Real Good Alignment"},{"paperId":"eb375712bd37250c350ecd3f559e1879e87eb3e5","externalIds":{"DBLP":"journals/corr/abs-2404-04475","ArXiv":"2404.04475","DOI":"10.48550/arXiv.2404.04475","CorpusId":269004605},"title":"Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators"},{"paperId":"39d976e2fb52c72fa43f1b52a3d23810e4f171e2","externalIds":{"ArXiv":"2404.04626","DBLP":"journals/corr/abs-2404-04626","DOI":"10.48550/arXiv.2404.04626","CorpusId":269005114},"title":"Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective"},{"paperId":"7647707b6b68c963314de0aab1514176b11732df","externalIds":{"DBLP":"conf/naacl/JinnaiMAA25","ArXiv":"2404.01054","DOI":"10.18653/v1/2025.naacl-long.472","CorpusId":268819814},"title":"Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment"},{"paperId":"9216ec4ce85502e2d81beec89f54423af7810403","externalIds":{"DBLP":"journals/corr/abs-2404-00978","ArXiv":"2404.00978","ACL":"2024.ccl-1.107","DOI":"10.48550/arXiv.2404.00978","CorpusId":268820171},"title":"Prior Constraints-based Reward Model Training for Aligning Large Language Models"},{"paperId":"eabd317bf33a636b099a38f4b49aecca97202661","externalIds":{"DBLP":"conf/coling/0001KSK0KP25","ArXiv":"2403.19270","DOI":"10.48550/arXiv.2403.19270","CorpusId":268732857},"title":"sDPO: Don't Use Your Data All at Once"},{"paperId":"c8c5db6eb1320967acc1dc91816d5ef0f3b5491b","externalIds":{"ArXiv":"2403.14714","DBLP":"journals/corr/abs-2403-14714","DOI":"10.48550/arXiv.2403.14714","CorpusId":268667038},"title":"Compiler generated feedback for Large Language Models"},{"paperId":"b3cdde45d87a3a90d86ebe20c1ee8c4ef0150d1c","externalIds":{"ACL":"2024.lrec-main.1251","ArXiv":"2403.11124","DBLP":"journals/corr/abs-2403-11124","DOI":"10.48550/arXiv.2403.11124","CorpusId":268512826},"title":"Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment"},{"paperId":"973814cd535facbf4f27c3de477b05bf19366030","externalIds":{"DBLP":"journals/corr/abs-2403-07691","ArXiv":"2403.07691","ACL":"2024.emnlp-main.626","DOI":"10.48550/arXiv.2403.07691","CorpusId":268363309},"title":"ORPO: Monolithic Preference Optimization without Reference Model"},{"paperId":"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7","externalIds":{"DBLP":"journals/corr/abs-2403-05530","ArXiv":"2403.05530","CorpusId":268297180},"title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"},{"paperId":"66e7edf09589527ebb58418632418758cee668cd","externalIds":{"ArXiv":"2403.04204","DBLP":"conf/ijcai/0001DYYZ00X0024","DOI":"10.48550/arXiv.2403.04204","CorpusId":268264604},"title":"On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"},{"paperId":"53f4fb0e9972989194368faf288ff8e3cba5bd60","externalIds":{"DBLP":"journals/corr/abs-2403-04132","ArXiv":"2403.04132","DOI":"10.48550/arXiv.2403.04132","CorpusId":268264163},"title":"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference"},{"paperId":"8c785ebee1f34464dbc85ab4113bccafd7a74b0a","externalIds":{"DBLP":"conf/acl/Quan24","ArXiv":"2403.01197","DOI":"10.48550/arXiv.2403.01197","CorpusId":268230489},"title":"DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling"},{"paperId":"6fe7e6ce3cc0ebd038caa456d73fd7472e7d6c38","externalIds":{"DBLP":"conf/emnlp/GuoCY0SSCXZL0024","ACL":"2024.emnlp-main.85","ArXiv":"2402.19085","DOI":"10.48550/arXiv.2402.19085","CorpusId":268063181},"title":"Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment"},{"paperId":"8be81d531dfc4a1145474a1bb2f9c0cf15e19f45","externalIds":{"ArXiv":"2402.16030","DBLP":"journals/corr/abs-2402-16030","DOI":"10.48550/arXiv.2402.16030","CorpusId":267938516},"title":"Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration"},{"paperId":"07038b2d2da9f1785a1e8e260a75c8215bd36ac7","externalIds":{"ArXiv":"2402.13228","DBLP":"journals/corr/abs-2402-13228","DOI":"10.48550/arXiv.2402.13228","CorpusId":267759910},"title":"Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"},{"paperId":"08436b3ddafd2edc798753ebc87f6ceffed6e8df","externalIds":{"ACL":"2024.emnlp-main.805","DBLP":"journals/corr/abs-2402-11176","ArXiv":"2402.11176","DOI":"10.48550/arXiv.2402.11176","CorpusId":267750063},"title":"KnowTuning: Knowledge-aware Fine-tuning for Large Language Models"},{"paperId":"9637ef9019671034912ea0f506ae67c3f2fc4689","externalIds":{"DBLP":"journals/corr/abs-2402-10207","ArXiv":"2402.10207","DOI":"10.48550/arXiv.2402.10207","CorpusId":267682397},"title":"Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"},{"paperId":"041ab8f72343db5d50769eeb725398c689b2850c","externalIds":{"DBLP":"journals/corr/abs-2402-09320","ArXiv":"2402.09320","DOI":"10.48550/arXiv.2402.09320","CorpusId":267658058},"title":"ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization"},{"paperId":"b46d05bcf42295b872f3cebf875643d2e66496a4","externalIds":{"ArXiv":"2402.04792","DBLP":"journals/corr/abs-2402-04792","DOI":"10.48550/arXiv.2402.04792","CorpusId":267522951},"title":"Direct Language Model Alignment from Online AI Feedback"},{"paperId":"b899a28eb553800ce558cf8974a697f65103e591","externalIds":{"DBLP":"journals/corr/abs-2402-06147","ArXiv":"2402.06147","DOI":"10.18653/v1/2025.acl-long.1274","CorpusId":267616998},"title":"DeAL: Decoding-time Alignment for Large Language Models"},{"paperId":"35b142ea69598e6241f0011312128031df55895c","externalIds":{"ArXiv":"2402.03300","DBLP":"journals/corr/abs-2402-03300","DOI":"10.48550/arXiv.2402.03300","CorpusId":267412607},"title":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"},{"paperId":"44162aa2763c88a384d9c51d60eafcc59277a1c9","externalIds":{"DBLP":"conf/icml/LiuGBCBLHDVB24","ArXiv":"2402.02992","DOI":"10.48550/arXiv.2402.02992","CorpusId":267411998},"title":"Decoding-time Realignment of Language Models"},{"paperId":"f977dac98cc603bfccae6ea991cf4b1f83bf139c","externalIds":{"DBLP":"conf/naacl/LiuQWSKJZSBLLW25","ArXiv":"2402.01878","DOI":"10.48550/arXiv.2402.01878","CorpusId":267411871},"title":"LiPO: Listwise Preference Optimization through Learning-to-Rank"},{"paperId":"c0d8e5ee66c279299012cc3b8d0519011b3f4998","externalIds":{"DBLP":"journals/corr/abs-2402-01306","ArXiv":"2402.01306","CorpusId":267406810},"title":"KTO: Model Alignment as Prospect Theoretic Optimization"},{"paperId":"08e84c939b88fc50aaa74ef76e202e61a1ad940b","externalIds":{"ArXiv":"2402.01391","DBLP":"journals/corr/abs-2402-01391","DOI":"10.48550/arXiv.2402.01391","CorpusId":267406244},"title":"StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback"},{"paperId":"65fb348291de709a379a3f0d00b48726a1a674d2","externalIds":{"DBLP":"journals/corr/abs-2401-16635","ArXiv":"2401.16635","DOI":"10.48550/arXiv.2401.16635","CorpusId":267320613},"title":"Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble"},{"paperId":"a54dcfb673e1bbf5c709908160ca2fa70f2b4bb2","externalIds":{"ArXiv":"2401.16657","DBLP":"journals/corr/abs-2401-16657","DOI":"10.48550/arXiv.2401.16657","CorpusId":267320821},"title":"Recovering Mental Representations from Large Language Models with Markov Chain Monte Carlo"},{"paperId":"a16372e000df6bcce7c2acf660dc81c7857784cb","externalIds":{"ArXiv":"2401.12086","CorpusId":267069315},"title":"West-of-N: Synthetic Preferences for Self-Improving Reward Models"},{"paperId":"67f03ac399693393116076c0b8ec8ea05b910685","externalIds":{"DBLP":"conf/icml/RameVHDCBF24","ArXiv":"2401.12187","DOI":"10.48550/arXiv.2401.12187","CorpusId":267068615},"title":"WARM: On the Benefits of Weight Averaged Reward Models"},{"paperId":"04d64be16fb402f28348faffef484bd419c8bd8f","externalIds":{"ArXiv":"2401.10020","DBLP":"journals/corr/abs-2401-10020","DOI":"10.48550/arXiv.2401.10020","CorpusId":267035293},"title":"Self-Rewarding Language Models"},{"paperId":"ebd1c04c61f73f46def3305ca11d038c46665b65","externalIds":{"DBLP":"conf/icml/XuSCTSDM024","ArXiv":"2401.08417","DOI":"10.48550/arXiv.2401.08417","CorpusId":267028540},"title":"Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"},{"paperId":"e360eb07461f2741793f99ece8b97a6c04fb2b68","externalIds":{"ArXiv":"2401.06838","DBLP":"conf/acl/SheZHZLGC24","DOI":"10.48550/arXiv.2401.06838","CorpusId":266998982},"title":"MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization"},{"paperId":"7260442ef9c0448f07ce3803efd49cebaffcebe9","externalIds":{"ArXiv":"2401.02954","DBLP":"journals/corr/abs-2401-02954","CorpusId":266818336},"title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"},{"paperId":"ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5","externalIds":{"DBLP":"conf/icml/ChenDYJG24","ArXiv":"2401.01335","DOI":"10.48550/arXiv.2401.01335","CorpusId":266725672},"title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"},{"paperId":"cdd0e94e51a02bac22ca5e94fa95daa18f36e226","externalIds":{"ArXiv":"2401.00243","DBLP":"journals/corr/abs-2401-00243","DOI":"10.48550/arXiv.2401.00243","CorpusId":266693165},"title":"Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"},{"paperId":"66ea57809a718f2634e4f2065569c0ba24659d44","externalIds":{"DBLP":"journals/corr/abs-2312-15907","ArXiv":"2312.15907","DOI":"10.48550/arXiv.2312.15907","CorpusId":266550919},"title":"Align on the Fly: Adapting Chatbot Behavior to Established Norms"},{"paperId":"4ba57555bef02f988f2ed3bab2f102733dc55221","externalIds":{"ArXiv":"2312.08935","DBLP":"journals/corr/abs-2312-08935","DOI":"10.48550/arXiv.2312.08935","CorpusId":266209760},"title":"Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"},{"paperId":"6b97aa78bcdb88548c44e7e1671c0ed37ed37976","externalIds":{"ArXiv":"2312.09390","DBLP":"conf/icml/BurnsIKBGACEJLS24","DOI":"10.48550/arXiv.2312.09390","CorpusId":266312608},"title":"Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"},{"paperId":"600d9287efc4703bdb99ce39b5e8b37da0baa6f6","externalIds":{"ArXiv":"2312.01552","DBLP":"journals/corr/abs-2312-01552","DOI":"10.48550/arXiv.2312.01552","CorpusId":265608902},"title":"The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning"},{"paperId":"288e64e8adb23d81e291a2cb51e3a56b315023b7","externalIds":{"DBLP":"conf/naacl/YuGW24","ArXiv":"2311.09724","DOI":"10.18653/v1/2024.findings-naacl.55","CorpusId":265221057},"title":"OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning"},{"paperId":"e327ef8d46ea0413316c80ee1404453834d84f05","externalIds":{"DBLP":"conf/acl/ChengLZKWDTH24","ArXiv":"2311.04155","DOI":"10.48550/arXiv.2311.04155","CorpusId":265043631},"title":"Black-Box Prompt Optimization: Aligning Large Language Models without Model Training"},{"paperId":"e5aed8e930b1efa1a5e0aad7ecf3038084cb0a33","externalIds":{"ArXiv":"2310.17022","DBLP":"conf/icml/MudgalLGLWHCCCS24","DOI":"10.48550/arXiv.2310.17022","CorpusId":264491118},"title":"Controlled Decoding from Language Models"},{"paperId":"9c0102443a1b5adc0c2235fab23a80bf8122ce72","externalIds":{"DBLP":"journals/corr/abs-2310-16271","ArXiv":"2310.16271","DOI":"10.48550/arXiv.2310.16271","CorpusId":264487383},"title":"CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment"},{"paperId":"386cebdba39d2d5f2862a9ab43a8d807f3863dae","externalIds":{"DBLP":"journals/corr/abs-2310-13639","ArXiv":"2310.13639","DOI":"10.48550/arXiv.2310.13639","CorpusId":264405839},"title":"Contrastive Preference Learning: Learning from Human Feedback without RL"},{"paperId":"f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f","externalIds":{"DBLP":"journals/corr/abs-2310-12036","ArXiv":"2310.12036","DOI":"10.48550/arXiv.2310.12036","CorpusId":264288854},"title":"A General Theoretical Paradigm to Understand Learning from Human Preferences"},{"paperId":"864a81b053d480b7db11ee1a1979f86554ccecba","externalIds":{"ArXiv":"2310.11593","DBLP":"journals/corr/abs-2310-11593","DOI":"10.48550/arXiv.2310.11593","CorpusId":264289206},"title":"Automated Evaluation of Personalized Text Generation using Large Language Models"},{"paperId":"d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855","externalIds":{"ArXiv":"2310.10505","DBLP":"journals/corr/abs-2310-10505","DOI":"10.48550/arXiv.2310.10505","CorpusId":264146066},"title":"ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"},{"paperId":"e5d0857feca845b474b89565d513ff599629851d","externalIds":{"ArXiv":"2310.09520","DBLP":"journals/corr/abs-2310-09520","DOI":"10.18653/v1/2023.emnlp-main.721","CorpusId":264146589},"title":"Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model"},{"paperId":"9ebf47129c15f61f4b77bbfe305c522480c20347","externalIds":{"DBLP":"conf/iclr/KimS0JLLYSKTS24","ArXiv":"2310.08491","DOI":"10.48550/arXiv.2310.08491","CorpusId":265675839},"title":"Prometheus: Inducing Fine-grained Evaluation Capability in Language Models"},{"paperId":"6f217d984f36499d88ab8a3d89572171552e6f3f","externalIds":{"ArXiv":"2310.07641","DBLP":"journals/corr/abs-2310-07641","DOI":"10.48550/arXiv.2310.07641","CorpusId":263834884},"title":"Evaluating Large Language Models at Evaluating Instruction Following"},{"paperId":"94a5f96308729e31c1ffbc0f0618db87795092fe","externalIds":{"DBLP":"journals/corr/abs-2310-06770","ArXiv":"2310.06770","DOI":"10.48550/arXiv.2310.06770","CorpusId":263829697},"title":"SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"},{"paperId":"5001630bcc65e8e0e621b19625629a2689724743","externalIds":{"DBLP":"conf/iclr/LiSYF0024","ArXiv":"2310.05470","DOI":"10.48550/arXiv.2310.05470","CorpusId":263829791},"title":"Generative Judge for Evaluating Alignment"},{"paperId":"023d462ec6ff84cee0d0716a34d11efc7cde8534","externalIds":{"DBLP":"conf/iclr/CosteAK024","ArXiv":"2310.02743","DOI":"10.48550/arXiv.2310.02743","CorpusId":263620686},"title":"Reward Model Ensembles Help Mitigate Overoptimization"},{"paperId":"e8df1cf6742b50a15500b8dd3dde3942e9c91418","externalIds":{"DBLP":"conf/icml/WanFWM00024","ArXiv":"2309.17179","DOI":"10.48550/arXiv.2309.17179","CorpusId":263310590},"title":"Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"},{"paperId":"860c8de4fdac38695ff6860dd15312f1079c6117","externalIds":{"DBLP":"conf/iclr/WangJYLC24","ArXiv":"2309.16240","DOI":"10.48550/arXiv.2309.16240","CorpusId":263142109},"title":"Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"},{"paperId":"749d59f887c8ac83fd4f5178465e8b03e463358c","externalIds":{"DBLP":"journals/corr/abs-2309-15025","ArXiv":"2309.15025","DOI":"10.48550/arXiv.2309.15025","CorpusId":262824801},"title":"Large Language Model Alignment: A Survey"},{"paperId":"77b1f1c6d1658d120456b9046667cf009ceb39ce","externalIds":{"ArXiv":"2309.12284","DBLP":"journals/corr/abs-2309-12284","CorpusId":262084051},"title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"},{"paperId":"b574245f3db22b5eb7fe64bd8b0a147dab467b60","externalIds":{"DBLP":"conf/iclr/LiWZ0024","ArXiv":"2309.07124","DOI":"10.48550/arXiv.2309.07124","CorpusId":261705563},"title":"RAIN: Your Language Models Can Align Themselves without Finetuning"},{"paperId":"22ab4219371366a4e890382bc0ca606130840ca7","externalIds":{"ArXiv":"2309.06657","DBLP":"conf/iclr/0002ZJKSLL24","DOI":"10.48550/arXiv.2309.06657","CorpusId":261705578},"title":"Statistical Rejection Sampling Improves Preference Optimization"},{"paperId":"1ab91d6ac7afc1a0121487a9089fa70edc1634d4","externalIds":{"DBLP":"journals/corr/abs-2309-02705","ArXiv":"2309.02705","DOI":"10.48550/arXiv.2309.02705","CorpusId":261557007},"title":"Certifying LLM Safety against Adversarial Prompting"},{"paperId":"74b4b993babe99bc5f5c589c27fef0f1baba606b","externalIds":{"DBLP":"journals/corr/abs-2309-02144","ArXiv":"2309.02144","DOI":"10.48550/arXiv.2309.02144","CorpusId":261558535},"title":"Making Large Language Models Better Reasoners with Alignment"},{"paperId":"91206346edbe28abb606d7b3425cd455d4019d4f","externalIds":{"DBLP":"journals/corr/abs-2308-01825","ArXiv":"2308.01825","DOI":"10.48550/arXiv.2308.01825","CorpusId":260438790},"title":"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"},{"paperId":"3705919b880f4f8dc37483a704e14dd078cb9ac4","externalIds":{"ArXiv":"2308.01862","DBLP":"journals/corr/abs-2308-01862","DOI":"10.48550/arXiv.2308.01862","CorpusId":260438863},"title":"Wider and Deeper LLM Networks are Fairer LLM Evaluators"},{"paperId":"e0ca43a635d35fd0414ee76ca1e7c287715f5b00","externalIds":{"ArXiv":"2307.14936","DBLP":"journals/corr/abs-2307-14936","DOI":"10.48550/arXiv.2307.14936","CorpusId":260202985},"title":"PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"a669ea57529f4db630043c8c75d8f840c485d24d","externalIds":{"DBLP":"journals/tmlr/LiuZXF00Y23","ArXiv":"2307.04349","DOI":"10.48550/arXiv.2307.04349","CorpusId":259501019},"title":"RLTF: Reinforcement Learning from Unit Test Feedback"},{"paperId":"130d18d1d455336e1a5b06c85784894bb67d87ec","externalIds":{"ArXiv":"2307.02762","DBLP":"journals/corr/abs-2307-02762","DOI":"10.48550/arXiv.2307.02762","CorpusId":259360619},"title":"PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations"},{"paperId":"929305892d4ddae575a0fc23227a8139f7681632","externalIds":{"DBLP":"journals/corr/abs-2307-02483","ArXiv":"2307.02483","DOI":"10.48550/arXiv.2307.02483","CorpusId":259342528},"title":"Jailbroken: How Does LLM Safety Training Fail?"},{"paperId":"19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e","externalIds":{"DBLP":"conf/aaai/00010LYHLW24","ArXiv":"2306.17492","DOI":"10.48550/arXiv.2306.17492","CorpusId":259308873},"title":"Preference Ranking Optimization for Human Alignment"},{"paperId":"5efec343015f9329c5cd56e2259f68f03c2ef8b5","externalIds":{"DBLP":"journals/corr/abs-2306-16636","ArXiv":"2306.16636","DOI":"10.48550/arXiv.2306.16636","CorpusId":259287423},"title":"CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?"},{"paperId":"bb9a44c94a89dbe00f0061d05c70a45064ff6ea6","externalIds":{"DBLP":"journals/corr/abs-2306-09212","ArXiv":"2306.09212","DOI":"10.48550/arXiv.2306.09212","CorpusId":259164635},"title":"CMMLU: Measuring massive multitask language understanding in Chinese"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"ccd94602e3acecf999d0c9ba62b1a8bc02e9f696","externalIds":{"ArXiv":"2306.05087","DBLP":"journals/corr/abs-2306-05087","DOI":"10.48550/arXiv.2306.05087","CorpusId":259108266},"title":"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization"},{"paperId":"993df7df129f8d18816877d69923d7df7b347d85","externalIds":{"DBLP":"conf/acl/Jiang0L23","ACL":"2023.acl-long.792","ArXiv":"2306.02561","DOI":"10.48550/arXiv.2306.02561","CorpusId":259075564},"title":"LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"},{"paperId":"e2e52461194bc81351da7caa978ac42e9e9549cc","externalIds":{"DBLP":"conf/nips/WuHSDSASOH23","ArXiv":"2306.01693","DOI":"10.48550/arXiv.2306.01693","CorpusId":259064099},"title":"Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"},{"paperId":"4d74a5048b884e8bb3842240abf98915c619c8f8","externalIds":{"ArXiv":"2306.01200","DBLP":"conf/acl/JainKSF0NZ23","DOI":"10.18653/v1/2023.findings-acl.537","CorpusId":259064002},"title":"Multi-Dimensional Evaluation of Text Summarization with In-Context Learning"},{"paperId":"be8db99310602d66bba64bcf41a572c45816fbfc","externalIds":{"ArXiv":"2305.20050","DBLP":"conf/iclr/LightmanKBEBLLS24","DOI":"10.48550/arXiv.2305.20050","CorpusId":258987659},"title":"Let's Verify Step by Step"},{"paperId":"38d64919ba526868a850a0e5f6239d4c474b7e7e","externalIds":{"DBLP":"journals/corr/abs-2305-17926","ArXiv":"2305.17926","DOI":"10.48550/arXiv.2305.17926","CorpusId":258960339},"title":"Large Language Models are not Fair Evaluators"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35","externalIds":{"ArXiv":"2305.15717","DBLP":"journals/corr/abs-2305-15717","DOI":"10.48550/arXiv.2305.15717","CorpusId":258887629},"title":"The False Promise of Imitating Proprietary LLMs"},{"paperId":"a122863d239643453195424c04067e89406246e1","externalIds":{"DBLP":"conf/emnlp/DingCXQHL0Z23","ArXiv":"2305.14233","DOI":"10.48550/arXiv.2305.14233","CorpusId":258840897},"title":"Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"},{"paperId":"cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa","externalIds":{"DBLP":"journals/corr/abs-2305-14387","ArXiv":"2305.14387","DOI":"10.48550/arXiv.2305.14387","CorpusId":258865545},"title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"},{"paperId":"58af2d4fcca54c14334d1efd975554b4eb78cd4d","externalIds":{"ArXiv":"2305.10425","DBLP":"journals/corr/abs-2305-10425","DOI":"10.48550/arXiv.2305.10425","CorpusId":258741082},"title":"SLiC-HF: Sequence Likelihood Calibration with Human Feedback"},{"paperId":"236c7dafea3df7ecffb5f18ec780d12f2f27d4b0","externalIds":{"DBLP":"conf/nips/HuangBZZZSLLZLF23","ArXiv":"2305.08322","DOI":"10.48550/arXiv.2305.08322","CorpusId":258685666},"title":"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"},{"paperId":"03055978e278960de9fbb5c648b1779ef9f26cd1","externalIds":{"ArXiv":"2305.01937","DBLP":"conf/acl/ChiangL23","ACL":"2023.acl-long.870","DOI":"10.48550/arXiv.2305.01937","CorpusId":258461287},"title":"Can Large Language Models Be an Alternative to Human Evaluations?"},{"paperId":"3ab661db57d924f4ff1706e05ac807873ca00e0a","externalIds":{"DBLP":"journals/corr/abs-2304-06767","ArXiv":"2304.06767","DOI":"10.48550/arXiv.2304.06767","CorpusId":258170300},"title":"RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"},{"paperId":"748698bd4387afd08594e0dc8150c2afa210d9ae","externalIds":{"DBLP":"conf/nips/YuanYTWHH23","ArXiv":"2304.05302","DOI":"10.48550/arXiv.2304.05302","CorpusId":258059818},"title":"RRHF: Rank Responses to Align Language Models with Human Feedback without tears"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","externalIds":{"DBLP":"conf/icml/BidermanSABOHKP23","ArXiv":"2304.01373","DOI":"10.48550/arXiv.2304.01373","CorpusId":257921893},"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55","externalIds":{"DBLP":"conf/emnlp/LiuIXWXZ23","ArXiv":"2303.16634","DOI":"10.18653/v1/2023.emnlp-main.153","CorpusId":257804696},"title":"G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"4161ad2d2495d8af1d62dc5e71882bde642cd1c1","externalIds":{"DBLP":"journals/corr/abs-2302-14520","ACL":"2023.eamt-1.19","ArXiv":"2302.14520","DOI":"10.48550/arXiv.2302.14520","CorpusId":257232490},"title":"Large Language Models Are State-of-the-Art Evaluators of Translation Quality"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"cb3125e4f63f3d058a2a39270ecb585e86c3d1ff","externalIds":{"DBLP":"journals/corr/abs-2302-02676","ArXiv":"2302.02676","DOI":"10.48550/arXiv.2302.02676","CorpusId":257038005},"title":"Chain of Hindsight Aligns Language Models with Feedback"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","externalIds":{"DBLP":"journals/corr/abs-2211-14275","ArXiv":"2211.14275","DOI":"10.48550/arXiv.2211.14275","CorpusId":254017497},"title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","externalIds":{"DBLP":"conf/acl/SuzgunSSGTCCLCZ23","ArXiv":"2210.09261","DOI":"10.48550/arXiv.2210.09261","CorpusId":252917648},"title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"891edceb78a274b0c2494d8176bc4d6f6e3f9cbc","externalIds":{"DBLP":"journals/corr/abs-2210-00045","ArXiv":"2210.00045","DOI":"10.48550/arXiv.2210.00045","CorpusId":252683988},"title":"Calibrating Sequence likelihood Improves Conditional Language Generation"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","externalIds":{"DBLP":"journals/corr/abs-2112-09332","ArXiv":"2112.09332","CorpusId":245329531},"title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"6c4bd57f7e70b9c037e44a96840e5ead0513abb0","externalIds":{"DBLP":"conf/icml/EthayarajhCS22","ArXiv":"2110.08420","CorpusId":250340652},"title":"Understanding Dataset Difficulty with V-Usable Information"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","externalIds":{"DBLP":"journals/corr/abs-2108-07732","ArXiv":"2108.07732","CorpusId":237142385},"title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d","externalIds":{"DBLP":"conf/naacl/YangK21","ArXiv":"2104.05218","MAG":"3169017236","ACL":"2021.naacl-main.276","DOI":"10.18653/v1/2021.naacl-main.276","CorpusId":233210709},"title":"FUDGE: Controlled Text Generation With Future Discriminators"},{"paperId":"49f905eb03958c7cfae52ac759ea8978b8b2a6ea","externalIds":{"DBLP":"journals/corr/abs-2103-14659","ArXiv":"2103.14659","CorpusId":232404883},"title":"Alignment of Language Agents"},{"paperId":"346081161bdc8f18e2a4c4af7f51d35452b5cb01","externalIds":{"ArXiv":"2101.02235","DBLP":"journals/tacl/GevaKSKRB21","DOI":"10.1162/tacl_a_00370","CorpusId":230799347},"title":"Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","externalIds":{"MAG":"3082115681","DBLP":"journals/corr/abs-2009-01325","ArXiv":"2009.01325","CorpusId":221665105},"title":"Learning to summarize from human feedback"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","externalIds":{"ACL":"Q19-1026","MAG":"2912924812","DBLP":"journals/tacl/KwiatkowskiPRCP19","DOI":"10.1162/tacl_a_00276","CorpusId":86611921},"title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","externalIds":{"DBLP":"journals/corr/abs-1904-09751","MAG":"2938704169","ArXiv":"1904.09751","CorpusId":127986954},"title":"The Curious Case of Neural Text Degeneration"},{"paperId":"e5a1d41e6212951cb6a831ed61a59d00b7ff6867","externalIds":{"DBLP":"conf/aaai/SahaPKSC18","MAG":"2786472750","ArXiv":"1801.10314","DOI":"10.1609/aaai.v32i1.11332","CorpusId":19240019},"title":"Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph"},{"paperId":"f5265e346382354887340c7b520d639162e2f598","externalIds":{"ACL":"W17-4508","DBLP":"conf/emnlp/VolskePSS17","MAG":"2760781482","DOI":"10.18653/v1/W17-4508","CorpusId":2204603},"title":"TL;DR: Mining Reddit to Learn Automatic Summarization"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","externalIds":{"MAG":"2612431505","ArXiv":"1705.03551","ACL":"P17-1147","DBLP":"journals/corr/JoshiCWZ17","DOI":"10.18653/v1/P17-1147","CorpusId":26501419},"title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","externalIds":{"DBLP":"journals/corr/RajpurkarZLL16","MAG":"2963748441","ACL":"D16-1264","ArXiv":"1606.05250","DOI":"10.18653/v1/D16-1264","CorpusId":11816014},"title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f","externalIds":{"ArXiv":"1211.3711","DBLP":"journals/corr/abs-1211-3711","MAG":"1828163288","CorpusId":17194112},"title":"Sequence Transduction with Recurrent Neural Networks"},{"paperId":"e635d81a617d1239232a9c9a11a196c53dab8240","externalIds":{"MAG":"2744982099","DBLP":"conf/ecml/KocsisS06","DOI":"10.1007/11871842_29","CorpusId":15184765},"title":"Bandit Based Monte-Carlo Planning"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"597ecf3cf9ce315f16dfa6fb7e9c456b6597a44c","externalIds":{"MAG":"2063445365","DOI":"10.1093/BIOMET/44.1-2.114","CorpusId":121527283},"title":"NON-NULL RANKING MODELS. I"},{"paperId":"7d47ee5f84103529f84297c98c21dadb4742e3ff","externalIds":{"MAG":"2013784666","DOI":"10.1093/BIOMET/39.3-4.324","CorpusId":121987403},"title":"RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"},{"paperId":"5edcbf3ea678a7d9977555ecd59370452667597a","externalIds":{"DBLP":"journals/corr/abs-2402-00856","DOI":"10.48550/arXiv.2402.00856","CorpusId":272202119},"title":"Towards Efficient and Exact Optimization of Language Model Alignment"},{"paperId":"d15478c8d0229eac7a86db2a27a896ef9c8b09da","externalIds":{"DBLP":"journals/corr/abs-2403-17141","DOI":"10.48550/arXiv.2403.17141","CorpusId":268692084},"title":"MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models"},{"paperId":"4c8cc2383cec93bd9ea0758692f01b98a035215b","externalIds":{"DBLP":"journals/corr/abs-2310-01377","DOI":"10.48550/arXiv.2310.01377","CorpusId":263605623},"title":"UltraFeedback: Boosting Language Models with High-quality Feedback"},{"paperId":"ac771182d1780c863954243809d1e144433919f9","externalIds":{"DBLP":"journals/corr/abs-2307-12966","DOI":"10.48550/arXiv.2307.12966","CorpusId":260356605},"title":"Aligning Large Language Models with Human: A Survey"},{"paperId":"1cae66341d2d146a4fca402d77325eda4bf7081c","externalIds":{"DBLP":"journals/corr/abs-2311-09835","DOI":"10.48550/arXiv.2311.09835","CorpusId":283773916},"title":"ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks"},{"paperId":"c4c0d6ffd70081d143b32be53b06fec1259b3ad8","externalIds":{"DBLP":"conf/cade/Moura021","DOI":"10.1007/978-3-030-79876-5_37","CorpusId":235800962},"title":"The Lean 4 Theorem Prover and Programming Language"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","externalIds":{"DOI":"10.1023/A:1022672621406","CorpusId":2332513},"title":"Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"},{"paperId":"0002c42e8d7bfeafc431c4ed9f6318f223bbf58b","externalIds":{"CorpusId":266312974},"title":"Practices for Governing Agentic AI Systems"}]}