{"references":[{"paperId":"4f920a4eba80b5ebce35038006330984d1c47e6b","externalIds":{"DBLP":"journals/pami/DingLHYJLJ25","ArXiv":"2512.10945","DOI":"10.1109/TPAMI.2025.3600507","CorpusId":280691154,"PubMed":"40828703"},"title":"MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation"},{"paperId":"3dd290be2b65396cb62b836bb7e1f00cb703ca5f","externalIds":{"DBLP":"journals/corr/abs-2508-00265","ArXiv":"2508.00265","DOI":"10.48550/arXiv.2508.00265","CorpusId":280417283},"title":"Multimodal Referring Segmentation: A Survey"},{"paperId":"2970550366773a6a8bf6ecd82f33ed0deb99ea7b","externalIds":{"DBLP":"journals/corr/abs-2505-21375","ArXiv":"2505.21375","DOI":"10.48550/arXiv.2505.21375","CorpusId":278911260},"title":"GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution"},{"paperId":"e74bd35a38342d746a0a9b8506f49abd6e23b8a3","externalIds":{"DBLP":"conf/cvpr/YangSXHGLYBSH025","ArXiv":"2503.18278","DOI":"10.1109/CVPR52734.2025.01844","CorpusId":277272184},"title":"TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model"},{"paperId":"457e13203a738fb03834cd9b1bad7e634e6b40fe","externalIds":{"ArXiv":"2503.11187","DBLP":"journals/corr/abs-2503-11187","DOI":"10.48550/arXiv.2503.11187","CorpusId":277043430},"title":"FastVID: Dynamic Density Pruning for Fast Video Large Language Models"},{"paperId":"681aa8f04683a124cc312c7a82c62d91f74a8276","externalIds":{"DBLP":"journals/tcsv/XiaoLLYG25","DOI":"10.1109/TCSVT.2024.3457610","CorpusId":275957249},"title":"BinaryViT: Toward Efficient and Accurate Binary Vision Transformers"},{"paperId":"f4966e4fb7cabf892cfa2f19fe547a223ba6ee6a","externalIds":{"ArXiv":"2412.01818","CorpusId":274437586},"title":"Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs"},{"paperId":"449df67152b7368b45126a675af53632e157ee87","externalIds":{"ArXiv":"2411.15024","DBLP":"conf/cvpr/TaoQYSW25","DOI":"10.1109/CVPR52734.2025.01769","CorpusId":274192345},"title":"DyCoke : Dynamic Compression of Tokens for Fast Video Large Language Models"},{"paperId":"63fc2edc907b507d53e0eb2b193caebfd3695c2a","externalIds":{"ArXiv":"2410.19552","DBLP":"journals/corr/abs-2410-19552","DOI":"10.48550/arXiv.2410.19552","CorpusId":273638057},"title":"GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing"},{"paperId":"b0721bd8644fc3cd5d75f2974adf30973234d607","externalIds":{"DBLP":"journals/visintelligence/GaoCCRWZTYHZLLQDW24","ArXiv":"2410.16261","DOI":"10.1007/s44267-024-00067-6","CorpusId":273507856},"title":"Mini-InternVL: a flexible-transfer pocket multi-modal model with 5% parameters and 90% performance"},{"paperId":"a05c0dd5a8bc70814517fb424cf55e94b4208e67","externalIds":{"DBLP":"conf/icml/0020FMZ0CGONKZ25","ArXiv":"2410.04417","DOI":"10.48550/arXiv.2410.04417","CorpusId":273185573},"title":"SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference"},{"paperId":"bd0fae435b93819d407f36d9d52565d5dfb5f87c","externalIds":{"ArXiv":"2408.01800","DBLP":"journals/corr/abs-2408-01800","DOI":"10.48550/arXiv.2408.01800","CorpusId":271709626},"title":"MiniCPM-V: A GPT-4V Level MLLM on Your Phone"},{"paperId":"39065058a01864d434846664216f2c0bfe852bda","externalIds":{"ArXiv":"2406.14555","DBLP":"journals/corr/abs-2406-14555","DOI":"10.48550/arXiv.2406.14555","CorpusId":270620011},"title":"A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models"},{"paperId":"0152e59caf854cd75cef30f99307d41634916653","externalIds":{"DBLP":"journals/corr/abs-2405-12107","ArXiv":"2405.12107","DOI":"10.1109/TMM.2025.3557680","CorpusId":269921376},"title":"Imp: Highly Capable Large Multimodal Models for Mobile Devices"},{"paperId":"e66f54d35d7218dc2d4de380981f49e7e32648ac","externalIds":{"DBLP":"journals/corr/abs-2405-05803","ArXiv":"2405.05803","DOI":"10.48550/arXiv.2405.05803","CorpusId":269635526},"title":"Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference"},{"paperId":"4062c4310c1fa0b6f2136c7bfd5e6dcd265c6a11","externalIds":{"ArXiv":"2405.05615","DBLP":"journals/corr/abs-2405-05615","DOI":"10.48550/arXiv.2405.05615","CorpusId":269635758},"title":"Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning"},{"paperId":"ce68430823b79dd3d478c505cc2761f03cf72b30","externalIds":{"DBLP":"conf/nips/LaurenconTCS24","ArXiv":"2405.02246","DOI":"10.48550/arXiv.2405.02246","CorpusId":269587869},"title":"What matters when building vision-language models?"},{"paperId":"3b62644272dc3a8c3f4698e29aa5d55cbc19e660","externalIds":{"DBLP":"journals/corr/abs-2405-00314","ArXiv":"2405.00314","DOI":"10.48550/arXiv.2405.00314","CorpusId":269484737},"title":"Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey"},{"paperId":"24354722e36c358b69893ab05220d6f2291989d1","externalIds":{"DBLP":"journals/corr/abs-2404-16635","ACL":"2024.emnlp-main.112","ArXiv":"2404.16635","DOI":"10.48550/arXiv.2404.16635","CorpusId":269362640},"title":"TinyChart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging"},{"paperId":"abdceff7d7983cdede9a5aabe6a476d4c72e41a3","externalIds":{"ArXiv":"2404.14219","DBLP":"journals/corr/abs-2404-14219","DOI":"10.48550/arXiv.2404.14219","CorpusId":269293048},"title":"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"},{"paperId":"727282aad52cd4c1ebb3a0a4678d84bd1ec42dc8","externalIds":{"DBLP":"conf/nips/ZongMSSSJL024","ArXiv":"2404.13046","DOI":"10.48550/arXiv.2404.13046","CorpusId":269282964},"title":"MoVA: Adapting Mixture of Vision Experts to Multimodal Context"},{"paperId":"5c0a82558db7a885bf6b174c2ed4e52de558a000","externalIds":{"DBLP":"conf/emnlp/JiangZZJYL24","ArXiv":"2404.10237","DOI":"10.18653/v1/2024.findings-emnlp.221","CorpusId":269157636},"title":"Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models"},{"paperId":"d4dd5be156a0f29f45e15c1df69e24fa7bd751ec","externalIds":{"DBLP":"journals/corr/abs-2404-09204","ArXiv":"2404.09204","DOI":"10.48550/arXiv.2404.09204","CorpusId":269148801},"title":"TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models"},{"paperId":"840ea9e7883ccd276ba26f97b76072686ce6be30","externalIds":{"DBLP":"journals/corr/abs-2404-08856","ArXiv":"2404.08856","DOI":"10.48550/arXiv.2404.08856","CorpusId":269149069},"title":"On Speculative Decoding for Multimodal Large Language Models"},{"paperId":"9738dde55ab77cc26271a5753db7dd7851176fd6","externalIds":{"DBLP":"journals/corr/abs-2404-07204","ArXiv":"2404.07204","DOI":"10.48550/arXiv.2404.07204","CorpusId":269033274},"title":"BRAVE: Broadening the visual encoding of vision-language models"},{"paperId":"d7ffcd4c5225ee752c5af9375e426fb6b5c7f88c","externalIds":{"ArXiv":"2404.06918","DBLP":"conf/cvpr/LiuYCJ0LJSX24","DOI":"10.1109/CVPR52733.2024.01471","CorpusId":269033037},"title":"HRVDA: High-Resolution Visual Document Assistant"},{"paperId":"01ae1c181dcb5117491affae728065e5e62bf074","externalIds":{"ArXiv":"2404.06512","DBLP":"conf/nips/DongZZCWOZDZLYG24","DOI":"10.48550/arXiv.2404.06512","CorpusId":269009700},"title":"InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD"},{"paperId":"49873ee415619efd9e1e4c16f73ee066ff008c1f","externalIds":{"ArXiv":"2404.06395","DBLP":"journals/corr/abs-2404-06395","DOI":"10.48550/arXiv.2404.06395","CorpusId":269009975},"title":"MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"},{"paperId":"3687e55cf21c4d0041d9bf0b74988319bfe3402b","externalIds":{"DBLP":"journals/corr/abs-2404-05719","ArXiv":"2404.05719","DOI":"10.48550/arXiv.2404.05719","CorpusId":269005503},"title":"Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs"},{"paperId":"f32b5b59fc4a988c49b112e4d4a06d684d4f117a","externalIds":{"DBLP":"conf/cvpr/0004LJJCSSL24","ArXiv":"2404.05726","DOI":"10.1109/CVPR52733.2024.01282","CorpusId":269005185},"title":"MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding"},{"paperId":"9efea405e5dfd277472f24f1dd95a26ca5734a6a","externalIds":{"ArXiv":"2404.02132","DBLP":"conf/cvpr/ChenYSYC24","DOI":"10.1109/CVPR52733.2024.01231","CorpusId":268856690},"title":"ViTamin: Designing Scalable Vision Models in the Vision-Language Era"},{"paperId":"a4f48e62aedc705dcb757f5ad8f732c5b364539d","externalIds":{"ArXiv":"2404.01331","DBLP":"journals/corr/abs-2404-01331","DOI":"10.48550/arXiv.2404.01331","CorpusId":268857227},"title":"LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model"},{"paperId":"9f3bc73b27d83a15baf81a4221322c04c522ff2b","externalIds":{"DBLP":"journals/corr/abs-2403-19322","ArXiv":"2403.19322","DOI":"10.48550/arXiv.2403.19322","CorpusId":268732657},"title":"Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models"},{"paperId":"b38845e9adbeeeab37519a2fc30e899411b4a36a","externalIds":{"DBLP":"journals/corr/abs-2403-18814","ArXiv":"2403.18814","DOI":"10.1109/TPAMI.2025.3637265","CorpusId":268724012,"PubMed":"41296951"},"title":"Mini-Gemini: Mining the Potential of Multi-Modality Vision Language Models"},{"paperId":"ab181887fa543033431e2a1e39f561fa2144e211","externalIds":{"ArXiv":"2403.16558","DBLP":"conf/eccv/WangYWNH24","DOI":"10.48550/arXiv.2403.16558","CorpusId":268681556},"title":"Elysium: Exploring Object-level Perception in Videos via MLLM"},{"paperId":"2eb799664c1eb6b4d83cacd43b66348fbe15b58e","externalIds":{"DBLP":"journals/corr/abs-2403-15226","ArXiv":"2403.15226","DOI":"10.48550/arXiv.2403.15226","CorpusId":268667548},"title":"Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models"},{"paperId":"c0ef72d02b93065e77c506e23ce9acbbcd945893","externalIds":{"ArXiv":"2403.15388","DBLP":"journals/corr/abs-2403-15388","DOI":"10.48550/arXiv.2403.15388","CorpusId":268667281},"title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models"},{"paperId":"40e996a7c3e914a67c708704fa9b4c54ea70f36e","externalIds":{"DBLP":"conf/aaai/0008ZZDHW25","ArXiv":"2403.14520","DOI":"10.48550/arXiv.2403.14520","CorpusId":268553791},"title":"Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference"},{"paperId":"6d49ed0ea24b9c218f5ec6731cd261ce618df2ac","externalIds":{"DBLP":"journals/corr/abs-2403-13600","ArXiv":"2403.13600","DOI":"10.48550/arXiv.2403.13600","CorpusId":268537285},"title":"VL-Mamba: Exploring State Space Models for Multimodal Learning"},{"paperId":"155b54d11285a29c4086891da7c1c1975d6bfe39","externalIds":{"ArXiv":"2403.13447","DBLP":"journals/corr/abs-2403-13447","DOI":"10.48550/arXiv.2403.13447","CorpusId":268536660},"title":"HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models"},{"paperId":"dbdfe71fdf641bebac5a052a60de75342871e3df","externalIds":{"DBLP":"conf/eccv/ShiWMWD24","ArXiv":"2403.13043","DOI":"10.48550/arXiv.2403.13043","CorpusId":268536728},"title":"When Do We Not Need Larger Vision Models?"},{"paperId":"b6648235f437c5be722db822f1f29a6a05984cd2","externalIds":{"DBLP":"journals/corr/abs-2403-11703","ArXiv":"2403.11703","DOI":"10.48550/arXiv.2403.11703","CorpusId":268531413},"title":"LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images"},{"paperId":"6675bcf6dc97c87da7afda223938ec7e51ecc3b2","externalIds":{"ArXiv":"2403.09611","DBLP":"journals/corr/abs-2403-09611","CorpusId":268384865},"title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"},{"paperId":"3421d12d98956c36c5d39c0ef023b378b6922068","externalIds":{"DBLP":"journals/corr/abs-2403-06764","ArXiv":"2403.06764","DOI":"10.48550/arXiv.2403.06764","CorpusId":268358224},"title":"An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models"},{"paperId":"b14e5138d4d1f3577b2390541dc7b730a41bb651","externalIds":{"ArXiv":"2403.05525","DBLP":"journals/corr/abs-2403-05525","DOI":"10.48550/arXiv.2403.05525","CorpusId":268297008},"title":"DeepSeek-VL: Towards Real-World Vision-Language Understanding"},{"paperId":"dd9f8b8e28af59512908d8c9f6e51146740f3cff","externalIds":{"ArXiv":"2403.02991","DBLP":"journals/corr/abs-2403-02991","DOI":"10.1109/CVPR52733.2024.01487","CorpusId":268248344},"title":"MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer"},{"paperId":"59d2cfb02f40363fbc48c1ba4d769d0e1c0e93f2","externalIds":{"DBLP":"journals/corr/abs-2402-16918","ArXiv":"2402.16918","DOI":"10.48550/arXiv.2402.16918","CorpusId":268032671},"title":"m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers"},{"paperId":"6a6751f59c5dbc80823b3cf47c3aaae063991b86","externalIds":{"DBLP":"journals/corr/abs-2402-14289","ArXiv":"2402.14289","DOI":"10.48550/arXiv.2402.14289","CorpusId":267782659},"title":"TinyLLaVA: A Framework of Small-scale Large Multimodal Models"},{"paperId":"a1714677252a39d1835824efb185beb0113ca189","externalIds":{"ArXiv":"2402.11530","DBLP":"journals/corr/abs-2402-11530","DOI":"10.48550/arXiv.2402.11530","CorpusId":267751050},"title":"Efficient Multimodal Learning from Data-centric Perspective"},{"paperId":"f6b9ccd7533b58e14d284191f1a576b0c764b3d5","externalIds":{"ArXiv":"2402.11684","CorpusId":267750849},"title":"ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models"},{"paperId":"ec8e2b45c4601730015608a58e33409224a81228","externalIds":{"ArXiv":"2402.05935","DBLP":"conf/icml/LiuZQHLZGLJZSXH24","DOI":"10.48550/arXiv.2402.05935","CorpusId":267547619},"title":"SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models"},{"paperId":"a091bf215c716a146140f81c751712db628c8e20","externalIds":{"ArXiv":"2402.03766","DBLP":"journals/corr/abs-2402-03766","DOI":"10.48550/arXiv.2402.03766","CorpusId":267500104},"title":"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model"},{"paperId":"cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3","externalIds":{"DBLP":"journals/corr/abs-2401-15947","ArXiv":"2401.15947","DOI":"10.48550/arXiv.2401.15947","CorpusId":267311517},"title":"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","externalIds":{"DBLP":"journals/corr/abs-2401-13601","ArXiv":"2401.13601","DOI":"10.48550/arXiv.2401.13601","CorpusId":267199815},"title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"41f12456780aecd204a210ce04b1a92d022b8c4c","externalIds":{"ArXiv":"2401.12503","DBLP":"journals/corr/abs-2401-12503","DOI":"10.48550/arXiv.2401.12503","CorpusId":267095012},"title":"Small Language Model Meets with Reinforced Vision Vocabulary"},{"paperId":"7260442ef9c0448f07ce3803efd49cebaffcebe9","externalIds":{"ArXiv":"2401.02954","DBLP":"journals/corr/abs-2401-02954","CorpusId":266818336},"title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"},{"paperId":"ece33ee67d74c29cd2a83c505e5bf0b818f9c2a1","externalIds":{"DBLP":"journals/corr/abs-2401-02330","ArXiv":"2401.02330","DOI":"10.1145/3688863.3689575","CorpusId":266755915},"title":"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model"},{"paperId":"560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd","externalIds":{"ArXiv":"2401.02385","DBLP":"journals/corr/abs-2401-02385","DOI":"10.48550/arXiv.2401.02385","CorpusId":266755802},"title":"TinyLlama: An Open-Source Small Language Model"},{"paperId":"98ab627dd147db88b5e5cfa9a74f1bd8da110021","externalIds":{"DBLP":"journals/corr/abs-2312-16862","ArXiv":"2312.16862","DOI":"10.48550/arXiv.2312.16862","CorpusId":266572996},"title":"TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones"},{"paperId":"e36f79b2bfbf50cf6362cc563a6e5c261e3f0615","externalIds":{"DBLP":"journals/corr/abs-2312-16886","ArXiv":"2312.16886","DOI":"10.48550/arXiv.2312.16886","CorpusId":266573855},"title":"MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","externalIds":{"ArXiv":"2312.14238","DBLP":"journals/corr/abs-2312-14238","DOI":"10.1109/CVPR52733.2024.02283","CorpusId":266521410},"title":"Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"4b1b5e219fb41a7413599c3b2ca6a7fdf045d1a5","externalIds":{"DBLP":"conf/cvpr/SunCZZYWRL0W24","ArXiv":"2312.13286","DOI":"10.1109/CVPR52733.2024.01365","CorpusId":266374640},"title":"Generative Multimodal Models are In-Context Learners"},{"paperId":"2141ed804636a1cf339d606cd03fd3b3e9582133","externalIds":{"DBLP":"conf/cvpr/LinYP0SH24","ArXiv":"2312.07533","DOI":"10.1109/CVPR52733.2024.02520","CorpusId":266174746},"title":"VILA: On Pre-training for Visual Language Models"},{"paperId":"4f5654ec1dfc04478be42d03eee8e6db6bd9ca14","externalIds":{"DBLP":"conf/cvpr/ChaKMR24","ArXiv":"2312.06742","DOI":"10.1109/CVPR52733.2024.01311","CorpusId":266174127},"title":"Honeybee: Locality-Enhanced Projector for Multimodal LLM"},{"paperId":"c95c4fb96868d6512c32988632a7b101a42c455d","externalIds":{"DBLP":"conf/eccv/MaCSPX24","ArXiv":"2312.00438","DOI":"10.48550/arXiv.2312.00438","CorpusId":265551475},"title":"Dolphins: Multimodal Language Model for Driving"},{"paperId":"7bbc7595196a0606a07506c4fb1473e5e87f6082","externalIds":{"ArXiv":"2312.00752","DBLP":"journals/corr/abs-2312-00752","CorpusId":265551773},"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"},{"paperId":"486c2df78cbb770a90a55f7fa3fe19102fba2c24","externalIds":{"ArXiv":"2311.17043","DBLP":"journals/corr/abs-2311-17043","DOI":"10.48550/arXiv.2311.17043","CorpusId":265466723},"title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"},{"paperId":"b50d19c5c298f6562c3b3c6c3822a351bdc89260","externalIds":{"ArXiv":"2311.16502","DBLP":"journals/corr/abs-2311-16502","DOI":"10.1109/CVPR52733.2024.00913","CorpusId":265466525},"title":"MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"},{"paperId":"40cd34f260d5596263654caf9d911d4355bf4f4e","externalIds":{"DBLP":"journals/corr/abs-2311-16483","ArXiv":"2311.16483","DOI":"10.48550/arXiv.2311.16483","CorpusId":265466206},"title":"ChartLlama: A Multimodal LLM for Chart Understanding and Generation"},{"paperId":"f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560","externalIds":{"DBLP":"conf/eccv/ChenLDZHWZL24","ArXiv":"2311.12793","DOI":"10.48550/arXiv.2311.12793","CorpusId":265308687},"title":"ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","externalIds":{"ArXiv":"2311.10122","ACL":"2024.emnlp-main.342","DBLP":"journals/corr/abs-2311-10122","DOI":"10.48550/arXiv.2311.10122","CorpusId":265281544},"title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"619184447595337a9fe3dca72c4e951e7ab7467c","externalIds":{"DBLP":"journals/corr/abs-2311-07574","ArXiv":"2311.07574","DOI":"10.48550/arXiv.2311.07574","CorpusId":265150580},"title":"To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning"},{"paperId":"bf14244669d5505f63343d4365d99d24aa6c5e82","externalIds":{"ArXiv":"2311.06607","DBLP":"conf/cvpr/LiYLMZYSLB24","DOI":"10.1109/CVPR52733.2024.02527","CorpusId":265150038},"title":"Monkey: Image Resolution and Text Label are Important Things for Large Multi-Modal Models"},{"paperId":"ad13b213681b6f634bc83a264df246e83dd9a9d9","externalIds":{"DBLP":"conf/cvpr/YeXYYHL0Z024","ArXiv":"2311.04257","DOI":"10.1109/CVPR52733.2024.01239","CorpusId":265050943},"title":"mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"},{"paperId":"1ddbd08ad8cf22a5c66c4242194c4286328533bf","externalIds":{"ArXiv":"2310.09478","DBLP":"journals/corr/abs-2310-09478","DOI":"10.48550/arXiv.2310.09478","CorpusId":264146906},"title":"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"8946891e94831adc8cddb0d32311cce2445c96d2","externalIds":{"DBLP":"conf/iclr/LuBX0LH0CG024","ArXiv":"2310.02255","CorpusId":264491155},"title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"},{"paperId":"8eb99f1ed884356871ddbcf1377b82359071906a","externalIds":{"DBLP":"conf/iclr/ZhuLNYCWPJZLZ0024","ArXiv":"2310.01852","DOI":"10.48550/arXiv.2310.01852","CorpusId":263608698},"title":"LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment"},{"paperId":"ccd6f8b6544f112de632e49bfbe592a0a654537d","externalIds":{"ArXiv":"2310.01412","DBLP":"journals/ral/XuZXZGWLZ24","DOI":"10.1109/LRA.2024.3440097","CorpusId":263605524},"title":"DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model"},{"paperId":"f349e5e8f0d18c948c1ffd92d3791db2b0ba2e55","externalIds":{"ArXiv":"2309.17425","DBLP":"journals/corr/abs-2309-17425","DOI":"10.48550/arXiv.2309.17425","CorpusId":263310452},"title":"Data Filtering Networks"},{"paperId":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","externalIds":{"DBLP":"journals/corr/abs-2309-16609","ArXiv":"2309.16609","DOI":"10.48550/arXiv.2309.16609","CorpusId":263134555},"title":"Qwen Technical Report"},{"paperId":"99c6b85cacacd076b24496ab898ce5d700c0180b","externalIds":{"DBLP":"journals/corr/abs-2309-15755","ArXiv":"2309.15755","DOI":"10.1109/TPAMI.2025.3616854","CorpusId":263152299,"PubMed":"41032537"},"title":"CAIT: Triple-Win Compression Toward High Accuracy, Fast Inference, and Favorable Transferability for ViTs"},{"paperId":"af3ab5da98e0807784b57e321ed887a3666a8ab6","externalIds":{"DBLP":"journals/corr/abs-2309-10020","ArXiv":"2309.10020","DOI":"10.48550/arXiv.2309.10020","CorpusId":262055614},"title":"Multimodal Foundation Models: From Specialists to General-Purpose Assistants"},{"paperId":"dcb74fb63acd87d3db0a77de89720300fb28b50a","externalIds":{"DBLP":"journals/corr/abs-2309-02031","ArXiv":"2309.02031","DOI":"10.1109/TPAMI.2024.3392941","CorpusId":261531260,"PubMed":"38656856"},"title":"A Survey on Efficient Vision Transformers: Algorithms, Techniques, and Performance Benchmarking"},{"paperId":"1798c7cc0351957dd1f9551c2c8ddec5a98a25a1","externalIds":{"DBLP":"journals/corr/abs-2308-08544","ArXiv":"2308.08544","DOI":"10.1109/ICCV51070.2023.00254","CorpusId":260926362},"title":"MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions"},{"paperId":"8ce219059d777c2333ee21cb2af2aad71275c98f","externalIds":{"ArXiv":"2308.03303","DBLP":"journals/corr/abs-2308-03303","DOI":"10.48550/arXiv.2308.03303","CorpusId":260683267},"title":"LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","externalIds":{"ArXiv":"2308.02490","DBLP":"journals/corr/abs-2308-02490","DOI":"10.48550/arXiv.2308.02490","CorpusId":260611572},"title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"4309d572a37d655779f9dce6a2c98c66334132de","externalIds":{"DBLP":"journals/corr/abs-2307-16125","ArXiv":"2307.16125","DOI":"10.48550/arXiv.2307.16125","CorpusId":260334888},"title":"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"},{"paperId":"304f8b4edea01fdb5a2f7f8b998c83188deeccff","externalIds":{"ArXiv":"2307.14334","DBLP":"journals/corr/abs-2307-14334","DOI":"10.48550/arXiv.2307.14334","CorpusId":260164663},"title":"Towards Generalist Biomedical AI"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"b37b1dc72b1882858f5120f2cd6883134089a6ed","externalIds":{"ArXiv":"2307.06281","DBLP":"journals/corr/abs-2307-06281","DOI":"10.48550/arXiv.2307.06281","CorpusId":259837088},"title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"451a3f03aca4aa87b93981364842137417549e58","externalIds":{"DBLP":"journals/corr/abs-2307-04087","ArXiv":"2307.04087","DOI":"10.48550/arXiv.2307.04087","CorpusId":259501644},"title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","externalIds":{"ArXiv":"2306.13394","DBLP":"journals/corr/abs-2306-13394","DOI":"10.48550/arXiv.2306.13394","CorpusId":259243928},"title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"948e8cfae92c2004f2dd5c9316f5972f8baaea21","externalIds":{"DBLP":"journals/corr/abs-2306-16527","ArXiv":"2306.16527","DOI":"10.48550/arXiv.2306.16527","CorpusId":259287020},"title":"OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"},{"paperId":"f4bdec0cf595720bc8ee5df2196324bac8f52ab4","externalIds":{"DBLP":"conf/acl/Lv0LGQ24","ArXiv":"2306.09782","DOI":"10.48550/arXiv.2306.09782","CorpusId":259187846},"title":"Full Parameter Fine-tuning for Large Language Models with Limited Resources"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"db9507cdd3e2d7d9c90ed185bd831e55c62dcec9","externalIds":{"DBLP":"journals/sigmobile/LinTTYXH24","ArXiv":"2306.00978","DOI":"10.1145/3714983.3714987","CorpusId":258999941},"title":"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"},{"paperId":"0b6b756bcd3f4b4afa0d2d1fac8eecaf35e79aa1","externalIds":{"ArXiv":"2306.16678","DBLP":"journals/corr/abs-2306-16678","DOI":"10.1109/CVPRW59228.2023.00492","CorpusId":259287125},"title":"BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models"},{"paperId":"21c70bb9093ad46768f8dd36d3a8614bffe2243a","externalIds":{"DBLP":"conf/cvpr/LinPLTR0P23","DOI":"10.1109/CVPR52729.2023.01554","CorpusId":261080989},"title":"Bit-shrinking: Limiting Instantaneous Sharpness for Improving Post-training Quantization"},{"paperId":"ad4b365630f1c13d74d78f0f5d8cee87ef356d41","externalIds":{"DBLP":"conf/nips/MalladiGNDL0A23","ArXiv":"2305.17333","DOI":"10.48550/arXiv.2305.17333","CorpusId":258959274},"title":"Fine-Tuning Language Models with Just Forward Passes"},{"paperId":"c3edb12342b7c6af51f8e36fdef11c1b057b44c2","externalIds":{"ArXiv":"2305.14730","DBLP":"journals/corr/abs-2305-14730","DOI":"10.48550/arXiv.2305.14730","CorpusId":258865478},"title":"BinaryViT: Towards Efficient and Accurate Binary Vision Transformers"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","externalIds":{"DBLP":"journals/corr/abs-2305-15023","ArXiv":"2305.15023","DOI":"10.48550/arXiv.2305.15023","CorpusId":258865326},"title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"026b3396a63ed5772329708b7580d633bb86bec9","externalIds":{"DBLP":"conf/emnlp/PengAAAABCCCDDG23","ArXiv":"2305.13048","DOI":"10.18653/v1/2023.findings-emnlp.936","CorpusId":258832459},"title":"RWKV: Reinventing RNNs for the Transformer Era"},{"paperId":"5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200","externalIds":{"DBLP":"journals/corr/abs-2305-13245","ArXiv":"2305.13245","DOI":"10.48550/arXiv.2305.13245","CorpusId":258833177},"title":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"},{"paperId":"07be590365e7fb76680be4ed67a5505763ec2d96","externalIds":{"DBLP":"conf/cvpr/YuCGF23","ArXiv":"2305.10727","DOI":"10.1109/CVPR52729.2023.02170","CorpusId":258762204},"title":"Boost Vision Transformer with GPU-Friendly Sparsity and Quantization"},{"paperId":"206400aba5f12f734cdd2e4ab48ef6014ea60773","externalIds":{"DBLP":"journals/corr/abs-2305-10355","ArXiv":"2305.10355","DOI":"10.48550/arXiv.2305.10355","CorpusId":258740697},"title":"Evaluating Object Hallucination in Large Vision-Language Models"},{"paperId":"180710a102ed95a6f9d0ee4e8bca30fc3e67b7ac","externalIds":{"ArXiv":"2305.07895","DBLP":"journals/chinaf/LiuLHYYLYLJB24","DOI":"10.1007/s11432-024-4235-6","CorpusId":274769861},"title":"OCRBench: on the hidden mystery of OCR in large multimodal models"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","externalIds":{"DBLP":"journals/corr/abs-2305-06355","ArXiv":"2305.06355","DOI":"10.1007/s11432-024-4321-9","CorpusId":258588306},"title":"VideoChat: chat-centric video understanding"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","externalIds":{"DBLP":"journals/corr/abs-2304-14178","ArXiv":"2304.14178","DOI":"10.48550/arXiv.2304.14178","CorpusId":258352455},"title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"df958800014d310b6df34ad83d771314d68fbb2d","externalIds":{"DBLP":"conf/nips/ZhuHAGDFYSW023","ArXiv":"2304.06939","DOI":"10.48550/arXiv.2304.06939","CorpusId":258170467},"title":"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"bdb68c5e2369633b20e733774ac66eb4600c34d1","externalIds":{"DBLP":"journals/corr/abs-2304-01933","ArXiv":"2304.01933","DOI":"10.48550/arXiv.2304.01933","CorpusId":257921386},"title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"},{"paperId":"35aba190f28b5c39df333c06ca21f46bd4845eba","externalIds":{"DBLP":"journals/corr/abs-2303-15343","ArXiv":"2303.15343","DOI":"10.1109/ICCV51070.2023.01100","CorpusId":257767223},"title":"Sigmoid Loss for Language Image Pre-Training"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"e60b6836b45ad0ae02a5fa663c8c31119f0c0a94","externalIds":{"DBLP":"journals/corr/abs-2303-04935","ArXiv":"2303.04935","DOI":"10.1109/CVPR52729.2023.02333","CorpusId":257427497},"title":"X-Pruner: eXplainable Pruning for Vision Transformers"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"64caaab51d8339f1b99874d3bddb79debbe661ca","externalIds":{"DBLP":"journals/corr/abs-2302-00402","ArXiv":"2302.00402","DOI":"10.48550/arXiv.2302.00402","CorpusId":256459873},"title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"493cf3728f49af4e47c2c2f928510ade1e31cf00","externalIds":{"ArXiv":"2301.01146","DBLP":"conf/iccv/ZhangLL0XZ0HWW23","DOI":"10.1109/ICCV51070.2023.00134","CorpusId":257921138},"title":"Rethinking Mobile Block for Efficient Attention-based Models"},{"paperId":"a05ebb7be3de58b28af75f9c4d340fa03eeb7fb1","externalIds":{"DBLP":"journals/pami/WangWXZL23","DOI":"10.1109/TPAMI.2022.3229313","CorpusId":254909458,"PubMed":"37015428"},"title":"Quantformer: Learning Extremely Low-Precision Vision Transformers"},{"paperId":"f35016b3180808fa97d59acbdecf47d6e2ed2819","externalIds":{"DBLP":"conf/iccv/LiHW0SWTR23","ArXiv":"2212.08059","DOI":"10.1109/ICCV51070.2023.01549","CorpusId":254685631},"title":"Rethinking Vision Transformers for MobileNet Size and Speed"},{"paperId":"8b87d39baf53d982bad7df8ab6c5c8e67c124c67","externalIds":{"ArXiv":"2211.16056","DBLP":"journals/corr/abs-2211-16056","DOI":"10.1109/CVPR52729.2023.01946","CorpusId":254069623},"title":"NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","externalIds":{"DBLP":"conf/cvpr/FangWXSWW0WC23","ArXiv":"2211.07636","DOI":"10.1109/CVPR52729.2023.01855","CorpusId":253510587},"title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"00be5f2cfd099a87674c39dc3557c6e67d4e1bd5","externalIds":{"DBLP":"conf/iccv/HeLZ0WZZ23","ArXiv":"2211.07091","DOI":"10.1109/ICCV51070.2023.00520","CorpusId":253510667},"title":"BiViT: Extremely Compressed Binary Vision Transformers"},{"paperId":"7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6","externalIds":{"DBLP":"journals/corr/abs-2210-17323","ArXiv":"2210.17323","CorpusId":253237200},"title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"},{"paperId":"245445ab41fbd9c96062dba58736dd257f3edc06","externalIds":{"DBLP":"journals/pami/DingLWJ23","ArXiv":"2210.15871","DOI":"10.1109/TPAMI.2022.3217852","CorpusId":258511255,"PubMed":"36306296"},"title":"VLT: Vision-Language Transformer and Query Generation for Referring Segmentation"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"85e959eef45114974c8f8643e88af23936fff3d1","externalIds":{"DBLP":"journals/corr/abs-2210-07558","ACL":"2023.eacl-main.239","ArXiv":"2210.07558","DOI":"10.48550/arXiv.2210.07558","CorpusId":252907428},"title":"DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation"},{"paperId":"3137c3ae4d21cf19d1bec8648f5f2935b5a3378e","externalIds":{"DBLP":"conf/nips/KuznedelevKFA23","ArXiv":"2210.09223","CorpusId":258987321},"title":"CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models"},{"paperId":"dfdb2894d50e095ce97f994ed6cee38554c4c84f","externalIds":{"DBLP":"conf/nips/LiX000G22","ArXiv":"2210.06707","DOI":"10.48550/arXiv.2210.06707","CorpusId":252873138},"title":"Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer"},{"paperId":"b5e71069f091d52f474a2928ed07b6546157af82","externalIds":{"DBLP":"conf/mm/DingQYCLWL22","ArXiv":"2303.14341","DOI":"10.1145/3503161.3547826","CorpusId":252783098},"title":"Towards Accurate Post-Training Quantization for Vision Transformer"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"f27c847e2909f30745f4a3528b574f5acfd76ea7","externalIds":{"DBLP":"conf/fpl/LiSLMYX0LLWLF22","ArXiv":"2208.05163","DOI":"10.1109/FPL57034.2022.00027","CorpusId":251467937},"title":"Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization"},{"paperId":"2fe71acc2c3f1e75b6149dea72838f0b594ad013","externalIds":{"DBLP":"conf/eccv/WuZPLXFY22","ArXiv":"2207.10666","DOI":"10.48550/arXiv.2207.10666","CorpusId":250920355},"title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers"},{"paperId":"d451901a6a12c61179289cac7a4588a86c234112","externalIds":{"DBLP":"conf/aaai/0004HWCCC22","DOI":"10.1609/aaai.v36i3.20222","CorpusId":250294994},"title":"Width & Depth Pruning for Vision Transformers"},{"paperId":"47a67e76ed84260ff19f7a948d764005d1edf1c9","externalIds":{"DBLP":"journals/corr/abs-2206-01718","ArXiv":"2206.01718","DOI":"10.48550/arXiv.2206.01718","CorpusId":249375629},"title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"},{"paperId":"dd1139cfc609c2f3263d02e97176d5275caebc0a","externalIds":{"ArXiv":"2206.01191","DBLP":"journals/corr/abs-2206-01191","DOI":"10.48550/arXiv.2206.01191","CorpusId":249282517},"title":"EfficientFormer: Vision Transformers at MobileNet Speed"},{"paperId":"7cdaa08890895e1ad92afb5fad429690ad7b1dac","externalIds":{"DBLP":"conf/nips/LiuTMMHBR22","ArXiv":"2205.05638","DOI":"10.48550/arXiv.2205.05638","CorpusId":248693283},"title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"3a1dbfb6875bfac8251627d60db313623fbb8b04","externalIds":{"DBLP":"journals/corr/abs-2204-12997","ArXiv":"2204.12997","DOI":"10.1109/CVPR52688.2022.01174","CorpusId":248406101},"title":"DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers"},{"paperId":"58c486ad4020177f5ed3d9f2883f3fc327b55770","externalIds":{"DBLP":"journals/corr/abs-2204-07154","ArXiv":"2204.07154","DOI":"10.1109/CVPR52688.2022.01183","CorpusId":248177918},"title":"MiniViT: Compressing Vision Transformers with Weight Multiplexing"},{"paperId":"dae903091d2c5f82c4595e53e8144ccce93f8fb9","externalIds":{"DBLP":"journals/corr/abs-2203-15380","ArXiv":"2203.15380","DOI":"10.48550/arXiv.2203.15380","CorpusId":247778932},"title":"SepViT: Separable Vision Transformer"},{"paperId":"71e15a9a52dcafca57bff5f310b95e2c7d0cfc87","externalIds":{"DBLP":"conf/nips/0001GB22","ArXiv":"2203.14343","CorpusId":247762199},"title":"Diagonal State Spaces are as Effective as Structured State Spaces"},{"paperId":"28ed0086dd0f51a8965f7e952b6ee933cdf44179","externalIds":{"DBLP":"journals/corr/abs-2203-12217","ArXiv":"2203.12217","DOI":"10.1109/CVPR52688.2022.01062","CorpusId":247619115},"title":"Training-free Transformer Architecture Search"},{"paperId":"b611c501269224702d1a9942c8600a31ec66ab28","externalIds":{"ArXiv":"2203.10244","DBLP":"conf/acl/MasryLTJH22","ACL":"2022.findings-acl.177","DOI":"10.48550/arXiv.2203.10244","CorpusId":247593713},"title":"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"},{"paperId":"4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a","externalIds":{"DBLP":"journals/corr/abs-2203-08243","ArXiv":"2203.08243","DOI":"10.48550/arXiv.2203.08243","CorpusId":247475977},"title":"Unified Visual Transformer Compression"},{"paperId":"202967f77c4384bce80eaf2fa5737259008267d3","externalIds":{"DBLP":"journals/corr/abs-2202-12015","ArXiv":"2202.12015","CorpusId":247084222},"title":"Learning to Merge Tokens in Vision Transformers"},{"paperId":"1b3e38bf2e42ede4aeab2f6fde8f4103562a53f6","externalIds":{"DBLP":"journals/corr/abs-2201-08050","ArXiv":"2201.08050","CorpusId":246063758},"title":"TerViT: An Efficient Ternary Vision Transformer"},{"paperId":"177e957f5cd93229c9794ea652c646d2557b4a69","externalIds":{"ArXiv":"2201.03545","DBLP":"journals/corr/abs-2201-03545","DOI":"10.1109/CVPR52688.2022.01167","CorpusId":245837420},"title":"A ConvNet for the 2020s"},{"paperId":"5ab70d95ca49702a3dd49b39d9396d8136b52311","externalIds":{"DBLP":"conf/cvpr/ChavanS0LCX22","ArXiv":"2201.00814","DOI":"10.1109/CVPR52688.2022.00488","CorpusId":245650313},"title":"Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space"},{"paperId":"722d71a19e4049b30a03d1028158881560432135","externalIds":{"DBLP":"conf/eccv/KongDMMNSSYRTQW22","ArXiv":"2112.13890","DOI":"10.1007/978-3-031-20083-0_37","CorpusId":245537400},"title":"SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning"},{"paperId":"8144ca1f78c045cb001815090bcf8a726e37e0ad","externalIds":{"ArXiv":"2111.15667","DBLP":"conf/eccv/FayyazKJSJSPG22","DOI":"10.1007/978-3-031-20083-0_24","CorpusId":251067151},"title":"Adaptive Token Sampling for Efficient Vision Transformers"},{"paperId":"39a620939887c9fc1f9bdd7ecfabde985a4aad3a","externalIds":{"ArXiv":"2111.12293","DBLP":"conf/eccv/YuanXCWS22","DOI":"10.1007/978-3-031-19775-8_12","CorpusId":244527659},"title":"PTQ4ViT: Post-training Quantization for Vision Transformers with Twin Uniform Quantization"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","externalIds":{"DBLP":"conf/iclr/GuGR22","ArXiv":"2111.00396","CorpusId":240354066},"title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"cbb9446dcb53bb5efda262942f7c7b0f5b3b7195","externalIds":{"DBLP":"journals/corr/abs-2207-05420","ArXiv":"2207.05420","DOI":"10.48550/arXiv.2207.05420","CorpusId":238531567},"title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP"},{"paperId":"a5c41f188b0eb0acb444cb4899bf6af378ee9ede","externalIds":{"ArXiv":"2108.00154","DBLP":"conf/iclr/0001YCL00L22","CorpusId":238531695},"title":"CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"},{"paperId":"c723187a2230749b1e706df2217e928c8271a660","externalIds":{"ArXiv":"2107.01378","DBLP":"conf/nips/HaoGJ0T00022","CorpusId":235731728},"title":"Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation"},{"paperId":"d645bd08fc19d52164695f9cd5ae863345459a06","externalIds":{"ArXiv":"2107.00651","DBLP":"conf/iccv/ChenPFL21","DOI":"10.1109/ICCV48922.2021.01205","CorpusId":235694428},"title":"AutoFormer: Searching Transformers for Visual Recognition"},{"paperId":"c295391129426d89ec58cebb049d1cd2e976deec","externalIds":{"ArXiv":"2106.14156","DBLP":"conf/nips/LiuWHZMG21","CorpusId":235658553},"title":"Post-Training Quantization for Vision Transformer"},{"paperId":"e43eaeca5077d01061a38aebd24f8e3fa5948ad9","externalIds":{"DBLP":"conf/cvpr/RenGHXTHZ22","ArXiv":"2106.12378","DOI":"10.1109/CVPR52688.2022.01627","CorpusId":235606268},"title":"Co-advise: Cross Inductive Bias Distillation"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"efbe9f591090018f78b42c84613c8afda9292fdb","externalIds":{"DBLP":"conf/nips/ChenCGYZW21","ArXiv":"2106.04533","CorpusId":235367934},"title":"Chasing Sparsity in Vision Transformers: An End-to-End Exploration"},{"paperId":"33fd56e5067a1e8a9713378af3e1c1c08d5ce93b","externalIds":{"ArXiv":"2106.02852","DBLP":"journals/corr/abs-2106-02852","DOI":"10.1109/CVPR52688.2022.01185","CorpusId":235358476},"title":"Patch Slimming for Efficient Vision Transformers"},{"paperId":"dbdcabd0444ad50b68ee09e30f39b66e9068f5d2","externalIds":{"DBLP":"conf/nips/RaoZLLZH21","ArXiv":"2106.02034","CorpusId":235313562},"title":"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"},{"paperId":"93efaf8c27940aaef145d8bcbca957be634d26e5","externalIds":{"ArXiv":"2104.08500","CorpusId":233296620},"title":"Vision Transformer Pruning"},{"paperId":"4b06c7e29280b1c6bc05c9df39023b48fef02c93","externalIds":{"ArXiv":"2104.05704","DBLP":"journals/corr/abs-2104-05704","CorpusId":233210459},"title":"Escaping the Big Data Paradigm with Compact Transformers"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"c4708542be64861d140ba95dd92099cd8d604cd7","externalIds":{"ArXiv":"2101.05224","DBLP":"journals/corr/abs-2101-05224","DOI":"10.1109/ICCV48922.2021.00346","CorpusId":231592774},"title":"Big Self-Supervised Models Advance Medical Image Classification"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"ad7ddcc14984caae308c397f1a589aae75d4ab71","externalIds":{"ArXiv":"2012.12877","DBLP":"journals/corr/abs-2012-12877","CorpusId":229363322},"title":"Training data-efficient image transformers & distillation through attention"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"b40bfcf339de3f0dba08fabb2b58b9368ff4c51a","externalIds":{"DBLP":"conf/wacv/MathewKJ21","ArXiv":"2007.00398","MAG":"3040138106","DOI":"10.1109/WACV48630.2021.00225","CorpusId":220280200},"title":"DocVQA: A Dataset for VQA on Document Images"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","externalIds":{"MAG":"3040573126","DBLP":"conf/iclr/LepikhinLXCFHKS21","ArXiv":"2006.16668","CorpusId":220265858},"title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","externalIds":{"MAG":"3033529678","DBLP":"journals/corr/abs-2006-04768","ArXiv":"2006.04768","CorpusId":219530577},"title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5","externalIds":{"MAG":"3033943443","DBLP":"journals/corr/abs-2006-03555","ArXiv":"2006.03555","CorpusId":219401747},"title":"Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","externalIds":{"MAG":"3033188311","ArXiv":"2006.03236","DBLP":"conf/nips/DaiLY020","CorpusId":219401850},"title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"33eadd4e666a894306a22ba0839c5e0cef77280e","externalIds":{"ArXiv":"2003.12462","MAG":"3106859150","DBLP":"journals/corr/abs-2003-12462","DOI":"10.1007/978-3-030-58536-5_44","CorpusId":214693197},"title":"TextCaps: a Dataset for Image Captioning with Reading Comprehension"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","externalIds":{"DBLP":"journals/corr/abs-2001-04451","MAG":"2994673210","ArXiv":"2001.04451","CorpusId":209315300},"title":"Reformer: The Efficient Transformer"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","externalIds":{"MAG":"2988394319","DBLP":"journals/corr/abs-1911-02150","ArXiv":"1911.02150","CorpusId":207880429},"title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"f902a64f7d08aaa6bfca7463e8729952ddc6134e","externalIds":{"DBLP":"conf/cvpr/GuptaDG19","MAG":"2965565989","ArXiv":"1908.03195","DOI":"10.1109/CVPR.2019.00550","CorpusId":195441339},"title":"LVIS: A Dataset for Large Vocabulary Instance Segmentation"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","externalIds":{"DBLP":"conf/cvpr/MarinoRFM19","ArXiv":"1906.00067","MAG":"2947312908","DOI":"10.1109/CVPR.2019.00331","CorpusId":173991173},"title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"af1f7739283bdbd2b7a94903041f6d6afd991907","externalIds":{"MAG":"2936135081","DBLP":"journals/corr/abs-1904-08920","ArXiv":"1904.08920","DOI":"10.1109/CVPR.2019.00851","CorpusId":85553602},"title":"Towards VQA Models That Can Read"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c","externalIds":{"DBLP":"conf/cvpr/Gurari0SGLGLB18","MAG":"2788643321","ArXiv":"1802.08218","DOI":"10.1109/CVPR.2018.00380","CorpusId":3831582},"title":"VizWiz Grand Challenge: Answering Visual Questions from Blind People"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"e65142010431ffc089b272a1174214e00693e503","externalIds":{"MAG":"2963109634","ArXiv":"1511.02283","DBLP":"journals/corr/MaoHTCYM15","DOI":"10.1109/CVPR.2016.9","CorpusId":8745888},"title":"Generation and Comprehension of Unambiguous Object Descriptions"},{"paperId":"51fa7c573fcc98b05c2d15685d64463c40d57cff","externalIds":{"MAG":"2964332173","ArXiv":"1505.00855","DBLP":"journals/corr/SalehE15","DOI":"10.11588/DAH.2016.2.23376","CorpusId":14168099},"title":"Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","externalIds":{"ACL":"D14-1086","DBLP":"conf/emnlp/KazemzadehOMB14","MAG":"2251512949","DOI":"10.3115/v1/D14-1086","CorpusId":6308361},"title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","externalIds":{"DBLP":"conf/nips/OrdonezKB11","MAG":"2109586012","CorpusId":14579301},"title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"a1cb50675b89a0a361cffb94735bcb3d4dddd15f","externalIds":{"DBLP":"journals/corr/abs-2505-12099","DOI":"10.48550/arXiv.2505.12099","CorpusId":283438464},"title":"TinyRS-R1: Compact Multimodal Language Model for Remote Sensing"},{"paperId":"5fd1b67482e4b9fe70b480c0fd0467900d6db436","externalIds":{"DBLP":"conf/nips/DongLWLYTW23","CorpusId":266188707},"title":"PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","externalIds":{"DBLP":"journals/corr/abs-2308-12966","DOI":"10.48550/arXiv.2308.12966","CorpusId":263875678},"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"5c2120fcc911756a1a6560e1934b1662e84f44ef","externalIds":{"DBLP":"books/sp/ZhuW23","DOI":"10.1007/978-3-030-96530-3","CorpusId":257536693},"title":"Computer Vision - Statistical Models for Marr's Paradigm"},{"paperId":"b8a919f4a2aaa97bef19aa43e01f8bc347693b73","externalIds":{"DBLP":"conf/iclr/GongWLCYT0C22","CorpusId":251647856},"title":"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training"},{"paperId":"2f0df7d27b28c80d5b389f30d8d243d62b76bc48","externalIds":{"DOI":"10.1007/978-3-319-32010-6_300155","CorpusId":215989524},"title":"Mixture-of-Experts"},{"paperId":"9af62668cb87f11fffb53a194588c8158fde6b00","externalIds":{"CorpusId":260505212},"title":"DynamicViT: Efcient Vision Transformers with Dynamic Token Sparsication"},{"paperId":"ab88cdefc888fb102b91140bd6e6f8eafef3d135","externalIds":{"CorpusId":277501628},"title":"Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation"}]}