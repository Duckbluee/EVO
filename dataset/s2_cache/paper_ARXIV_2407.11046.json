{"paperId":"291c94b62953e261c94b74516ee997be5511c052","externalIds":{"DBLP":"journals/corr/abs-2407-11046","ArXiv":"2407.11046","DOI":"10.1007/s11704-024-40663-9","CorpusId":271218154},"title":"A survey on LoRA of large language models","openAccessPdf":{"url":"https://doi.org/10.1007/s11704-024-40663-9","status":"HYBRID","license":"CCBY","disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2407.11046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2293238106","name":"Yuren Mao"},{"authorId":"2311640424","name":"Yuhang Ge"},{"authorId":"2280099603","name":"Yijiang Fan"},{"authorId":"2292411692","name":"Wenyi Xu"},{"authorId":"2280064481","name":"Yu Mi"},{"authorId":"2311642443","name":"Zhonghao Hu"},{"authorId":"2292513358","name":"Yunjun Gao"}],"abstract":"Low-Rank Adaptation (LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRAâ€™s performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this survey also discusses the future directions in this field."}