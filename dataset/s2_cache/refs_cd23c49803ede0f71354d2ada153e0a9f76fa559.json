{"references":[{"paperId":"2bb010660dbbd70e4ed3eaba16c3ecd718a091ab","externalIds":{"DBLP":"journals/tog/SunWYG24","DOI":"10.1145/3687757","CorpusId":274251916},"title":"NU-NeRF: Neural Reconstruction of Nested Transparent Objects with Uncontrolled Capture Environment"},{"paperId":"b73b7cbc9e775996ec2fe62826a0dbd1556fbc85","externalIds":{"ArXiv":"2409.18057","DBLP":"journals/corr/abs-2409-18057","DOI":"10.48550/arXiv.2409.18057","CorpusId":272910929},"title":"LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field"},{"paperId":"fdde63b8d0afacbeb9785d1c03b8bb987e7092df","externalIds":{"DBLP":"journals/corr/abs-2407-08414","ArXiv":"2407.08414","DOI":"10.48550/arXiv.2407.08414","CorpusId":271097591},"title":"MeshAvatar: Learning High-quality Triangular Human Avatars from Multi-view Videos"},{"paperId":"c10150d90a69213cae1e37c6fdb0807d3c67c80b","externalIds":{"DBLP":"journals/corr/abs-2410-17741","ArXiv":"2410.17741","DOI":"10.1016/j.patcog.2024.110758","CorpusId":271077049},"title":"Efficient neural implicit representation for 3D human reconstruction"},{"paperId":"83c7067eaaba7c78945e7fa2ee21967c21d90061","externalIds":{"DBLP":"conf/nips/AmaduzziRLSS24","ArXiv":"2406.11840","DOI":"10.48550/arXiv.2406.11840","CorpusId":270560393},"title":"LLaNA: Large Language and NeRF Assistant"},{"paperId":"ab01f985e771145e4de5a51742319e7de4e5fc4c","externalIds":{"DBLP":"journals/corr/abs-2406-07520","ArXiv":"2406.07520","DOI":"10.48550/arXiv.2406.07520","CorpusId":270379694},"title":"Neural Gaffer: Relighting Any Object via Diffusion"},{"paperId":"fa400564bbc3168022be48f029eeaeec5bde485b","externalIds":{"DBLP":"conf/icra/XinYZW24","ArXiv":"2407.18813","DOI":"10.1109/ICRA57147.2024.10610000","CorpusId":271516140},"title":"HERO-SLAM: Hybrid Enhanced Robust Optimization of Neural SLAM"},{"paperId":"f7bea787bfe2b02b90d7f6f6bd8805f67a5e06aa","externalIds":{"DBLP":"conf/cvpr/ChenLP24","ArXiv":"2405.02859","DOI":"10.1109/CVPR52733.2024.00511","CorpusId":269605099},"title":"MVIP-NeRF: Multi-View 3D Inpainting on NeRF Scenes via Diffusion Prior"},{"paperId":"0e6043d25945413118bfe49041145c858bb5e441","externalIds":{"DBLP":"conf/cvpr/CartillierSE22","ArXiv":"2404.11419","DOI":"10.1109/CVPRW63382.2024.00292","CorpusId":269187682},"title":"SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping"},{"paperId":"7bb05ad5161f42475019d8c513a5fc7482452d95","externalIds":{"DBLP":"journals/corr/abs-2404-01543","ArXiv":"2404.01543","DOI":"10.1109/CVPR52733.2024.00193","CorpusId":268856582},"title":"Efficient 3D Implicit Head Avatar With Mesh-Anchored Hash Table Blendshapes"},{"paperId":"9dbd917f17390811a7d0895ad4b4e4c8276ad87e","externalIds":{"DBLP":"journals/corr/abs-2404-01943","ArXiv":"2404.01943","DOI":"10.1109/CVPR52733.2024.01305","CorpusId":268856764},"title":"Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation"},{"paperId":"004c1679e0f69d000ce5d9101123a169974279fe","externalIds":{"DBLP":"conf/cvpr/Lee0L24","ArXiv":"2404.00874","DOI":"10.1109/CVPR52733.2024.01943","CorpusId":268819270},"title":"DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF"},{"paperId":"cf85c821383114c4b5b9ea85e36b6704ff0656ff","externalIds":{"ArXiv":"2403.20002","DBLP":"conf/cvpr/ZhaoFLY24","DOI":"10.1109/CVPR52733.2024.01837","CorpusId":268793571},"title":"Grounding and Enhancing Grid-based Models for Neural Fields"},{"paperId":"10d2480b4b5aab8051611baf3b6ca13b27d0d900","externalIds":{"ArXiv":"2403.16993","DBLP":"journals/corr/abs-2403-16993","DOI":"10.48550/arXiv.2403.16993","CorpusId":268681023},"title":"Comp4D: LLM-Guided Compositional 4D Scene Generation"},{"paperId":"90607c8d66c9e397d697214f326560231f7834ba","externalIds":{"ArXiv":"2403.11134","DBLP":"journals/cvm/WuYZYCYG24","DOI":"10.1007/s41095-024-0436-y","CorpusId":268513119},"title":"Recent advances in 3D Gaussian splatting"},{"paperId":"8475c4c5da4dacb956f85cad1ad24179f1282176","externalIds":{"DBLP":"journals/corr/abs-2403-10335","ArXiv":"2403.10335","DOI":"10.1109/CVPR52733.2024.01899","CorpusId":268509791},"title":"NECA: Neural Customizable Human Avatar"},{"paperId":"dbb62146adcdb24c316689191296bd3ca4fbea73","externalIds":{"DBLP":"journals/tog/TeotiaRPKGET24","DOI":"10.1145/3649889","CorpusId":268214054},"title":"HQ3DAvatar: High-quality Implicit 3D Head Avatar"},{"paperId":"6eb0c5db46a304b68dd6e44735b594585bb91788","externalIds":{"DOI":"10.3390/wevj15030085","CorpusId":268035047},"title":"SLAM Meets NeRF: A Survey of Implicit SLAM Methods"},{"paperId":"33753e552ddf7a4c4fbdc8740869a57d12ca77b0","externalIds":{"DBLP":"journals/tcsv/LiaoZBLL24","ArXiv":"2402.04648","DOI":"10.1109/TCSVT.2024.3439737","CorpusId":267523451},"title":"OV-NeRF: Open-Vocabulary Neural Radiance Fields With Vision and Language Foundation Models for 3D Semantic Understanding"},{"paperId":"06a4f4a9241a6321648ba6695ae8625b93cafd39","externalIds":{"ArXiv":"2401.09101","DBLP":"journals/corr/abs-2401-09101","DOI":"10.1109/TRO.2024.3422055","CorpusId":267028534},"title":"PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency"},{"paperId":"c787cfe707e824d99675b9216d7e51a1f0812921","externalIds":{"DBLP":"conf/wacv/LeeL24","DOI":"10.1109/WACV57701.2024.00494","CorpusId":268695475},"title":"PoseDiff: Pose-conditioned Multimodal Diffusion Model for Unbounded Scene Synthesis from Sparse Inputs"},{"paperId":"eef6c89c37e62406f2584c3aa48f9ef1f689ed0a","externalIds":{"DBLP":"journals/corr/abs-2401-00208","ArXiv":"2401.00208","DOI":"10.48550/arXiv.2401.00208","CorpusId":266693466},"title":"Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with Generative Diffusion Models"},{"paperId":"42128a1bd1a660fc2ff05b7b7a90b54b6b80a875","externalIds":{"DBLP":"journals/corr/abs-2312-12122","ArXiv":"2312.12122","DOI":"10.1016/j.neucom.2024.127714","CorpusId":266362730},"title":"ZS-SRT: An Efficient Zero-Shot Super-Resolution Training Method for Neural Radiance Fields"},{"paperId":"ff2d7822d0dacc3b29c4f000c6a2e8dffca4acdf","externalIds":{"DBLP":"journals/corr/abs-2312-11537","ArXiv":"2312.11537","DOI":"10.1109/WACV57701.2024.00593","CorpusId":266362477},"title":"FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline"},{"paperId":"54fa9f6c32e41e72b230773dcec9e3491df43738","externalIds":{"ArXiv":"2312.02973","DBLP":"conf/cvpr/HuHL24a","DOI":"10.1109/CVPR52733.2024.01930","CorpusId":265659537},"title":"GauHuman: Articulated Gaussian Splatting from Monocular Human Videos"},{"paperId":"114e83828084ed6e82b3984979b2ec8f6e7f9cf7","externalIds":{"DBLP":"journals/corr/abs-2312-02981","ArXiv":"2312.02981","DOI":"10.1109/CVPR52733.2024.02036","CorpusId":265659460},"title":"ReconFusion: 3D Reconstruction with Diffusion Priors"},{"paperId":"b6bc7a200062a2fb7bf28c0dbb127b579bf139f2","externalIds":{"DBLP":"conf/iros/LiNNT24","ArXiv":"2312.00204","DOI":"10.1109/IROS58592.2024.10803056","CorpusId":265551655},"title":"DNS-SLAM: Dense Neural Semantic-Informed SLAM"},{"paperId":"e42ddfabf623e11711de32b1934ccebfd889d5d4","externalIds":{"ArXiv":"2311.17910","DBLP":"conf/cvpr/KocabasCGTR24","DOI":"10.1109/CVPR52733.2024.00055","CorpusId":265498305},"title":"HUGS: Human Gaussian Splats"},{"paperId":"7cc180fd1dc5f0c3f53089514a49796deac7753a","externalIds":{"DBLP":"conf/cvpr/MoreauSDSZP24","ArXiv":"2311.17113","DOI":"10.1109/CVPR52733.2024.00081","CorpusId":265498963},"title":"Human Gaussian Splatting: Real-Time Rendering of Animatable Avatars"},{"paperId":"a047ef26f4830c071b6723bda6f540efc382414d","externalIds":{"DBLP":"journals/corr/abs-2311-13099","ArXiv":"2311.13099","DOI":"10.1109/CVPR52733.2024.00426","CorpusId":265351750},"title":"PIE-NeRF: Physics-Based Interactive Elastodynamics with NeRF"},{"paperId":"218a3611816aa1110afb284c69e2833740b7ce78","externalIds":{"DBLP":"conf/cvpr/LiZDLCL0H24","ArXiv":"2311.11863","DOI":"10.1109/CVPR52733.2024.02051","CorpusId":265294713},"title":"GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding"},{"paperId":"01a6eb35a8c921a6a92b1650f8d114daaab6074f","externalIds":{"ArXiv":"2311.11016","DBLP":"journals/corr/abs-2311-11016","DOI":"10.1109/CVPR52733.2024.02000","CorpusId":265294599},"title":"SNI-SLAM: Semantic Neural Implicit SLAM"},{"paperId":"a0290fd27a7933cf75377452f4299ee9dc94b2ab","externalIds":{"DBLP":"journals/corr/abs-2311-08013","ArXiv":"2311.08013","DOI":"10.48550/arXiv.2311.08013","CorpusId":265157593},"title":"CP-SLAM: Collaborative Neural Point-based SLAM System"},{"paperId":"d79665c73d84e0855bddb27a7933e550610a716f","externalIds":{"DBLP":"journals/tog/DuanWSCC23","ArXiv":"2311.05521","DOI":"10.1145/3618399","CorpusId":265066877},"title":"BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis"},{"paperId":"325561d087b3d1552b52e5d9d33d07119fa62450","externalIds":{"DBLP":"journals/corr/abs-2310-06275","ArXiv":"2310.06275","DOI":"10.48550/arXiv.2310.06275","CorpusId":263829889},"title":"High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field"},{"paperId":"6b15e1ee0785b3aa00f15b85bf02a0af127cbafc","externalIds":{"DBLP":"conf/iccv/XuZCGBG23","DOI":"10.1109/ICCV51070.2023.02064","CorpusId":264523587},"title":"ReNeRF: Relightable Neural Radiance Fields with Nearfield Lighting"},{"paperId":"24958e3427691a273d4f4290847f71d6d66f6cf0","externalIds":{"DBLP":"journals/corr/abs-2309-15426","ArXiv":"2309.15426","DOI":"10.1109/ICCV51070.2023.00386","CorpusId":262941737},"title":"NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions"},{"paperId":"c28fa983d5e59a38f0a74c2fef8244ba04466fff","externalIds":{"ArXiv":"2309.11281","DBLP":"conf/cvpr/ShumKHNY24","DOI":"10.1109/CVPR52733.2024.00495","CorpusId":262064593},"title":"Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates"},{"paperId":"60331af4b8ae9b8147f60877b7b79e011e24b56c","externalIds":{"DBLP":"conf/cvpr/0008P0MYSBZ24","ArXiv":"2308.07903","DOI":"10.1109/CVPR52733.2024.00100","CorpusId":260900158},"title":"Relightable and Animatable Neural Avatar from Sparse-View Video"},{"paperId":"d3fd513594cd2e4cce10b50eb7ea16760b63a2b8","externalIds":{"ArXiv":"2308.03610","DBLP":"journals/corr/abs-2308-03610","DOI":"10.48550/arXiv.2308.03610","CorpusId":260681445},"title":"AvatarVerse: High-quality & Stable 3D Avatar Creation from Text and Pose"},{"paperId":"32d3048a4fe4becc7c4638afd05f2354b631cfca","externalIds":{"DOI":"10.1145/3596711.3596800","CorpusId":5328073},"title":"SMPL: A Skinned Multi-Person Linear Model"},{"paperId":"2cc1d857e86d5152ba7fe6a8355c2a0150cc280a","externalIds":{"DBLP":"journals/tog/KerblKLD23","ArXiv":"2308.04079","DOI":"10.1145/3592433","CorpusId":259267917},"title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering"},{"paperId":"ceced4a2c7736da2b220cc8404b8a4ecff079a75","externalIds":{"DBLP":"journals/tog/ChenXWTSG23","DOI":"10.1145/3592135","CorpusId":260167858},"title":"Dictionary Fields: Learning a Neural Basis Decomposition"},{"paperId":"1799398201d38f527cd0edcd23024b053984c4ee","externalIds":{"DBLP":"journals/corr/abs-2306-09329","ArXiv":"2306.09329","DOI":"10.48550/arXiv.2306.09329","CorpusId":259171750},"title":"DreamHuman: Animatable 3D Avatars from Text"},{"paperId":"06e9b41222dbf2bcea05d3757c73aff68cdcc258","externalIds":{"DBLP":"conf/cvpr/Li0ETU0L23","ArXiv":"2306.03092","DOI":"10.1109/CVPR52729.2023.00817","CorpusId":259075890},"title":"Neuralangelo: High-Fidelity Neural Surface Reconstruction"},{"paperId":"942a0b695f2f789f517f73afac46b4692521b036","externalIds":{"ArXiv":"2304.13386","DBLP":"conf/ijcai/SunZCLJZX23","DOI":"10.48550/arXiv.2304.13386","CorpusId":258331896},"title":"VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs"},{"paperId":"78869b78b5c1e02fcfa39edd034bb7a1bdb24c4d","externalIds":{"DBLP":"journals/corr/abs-2304-06714","ArXiv":"2304.06714","DOI":"10.1109/ICCV51070.2023.00229","CorpusId":258108307},"title":"Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"0fa1501c7378a0dca2ac913fce9dcdcc2b1958a7","externalIds":{"DBLP":"conf/cvpr/CaoC0SW24","ArXiv":"2304.00916","DOI":"10.1109/CVPR52733.2024.00097","CorpusId":257912580},"title":"DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models"},{"paperId":"f74ca3a602b8c02187e0919d8f29c112a693b1c8","externalIds":{"ArXiv":"2303.14435","DBLP":"conf/cvpr/Yan0L23","DOI":"10.1109/CVPR52729.2023.00801","CorpusId":257767015},"title":"NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects"},{"paperId":"611e2100a3f8ed02b2583d4e53fd27a12d223b4c","externalIds":{"ArXiv":"2303.09553","DBLP":"journals/corr/abs-2303-09553","DOI":"10.1109/ICCV51070.2023.01807","CorpusId":257557329},"title":"LERF: Language Embedded Radiance Fields"},{"paperId":"c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0","externalIds":{"DBLP":"conf/eccv/LiuZRLZYJLYSZZ24","ArXiv":"2303.05499","DOI":"10.48550/arXiv.2303.05499","CorpusId":257427307},"title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"},{"paperId":"d763a2af9af6e7dec0fa5c9cf3f89d6d362e305d","externalIds":{"DBLP":"journals/corr/abs-2303-04805","ArXiv":"2303.04805","DOI":"10.1109/CVPR52729.2023.01622","CorpusId":257404819},"title":"X-Avatar: Expressive Human Avatars"},{"paperId":"31bfd63204c861baf2a87fc48f62281d382fd26d","externalIds":{"ArXiv":"2302.12231","DBLP":"conf/cvpr/WynnT23","DOI":"10.1109/CVPR52729.2023.00407","CorpusId":257102507},"title":"DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models"},{"paperId":"84cce9b8aea35e4fa38eef63da439573f21c0728","externalIds":{"ArXiv":"2302.10663","DBLP":"conf/cvpr/Melas-KyriaziL023","DOI":"10.1109/CVPR52729.2023.00816","CorpusId":261092262},"title":"RealFusion 360° Reconstruction of Any Object from a Single Image"},{"paperId":"a8efa7087fd6d84d5d84fd6c7c7cfb9d7ddb6dce","externalIds":{"DBLP":"conf/icml/GuTLSTLR23","ArXiv":"2302.10109","DOI":"10.48550/arXiv.2302.10109","CorpusId":257039008},"title":"NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion"},{"paperId":"7378afa69055e12375d55d063af87177faf37c1b","externalIds":{"ArXiv":"2302.03594","DBLP":"journals/corr/abs-2302-03594","DOI":"10.1109/3DV62453.2024.00096","CorpusId":256627303},"title":"NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM"},{"paperId":"d684fbe07585be651cc93d3c00ae3fe6df3ac877","externalIds":{"DBLP":"conf/icml/SingerSPAMKGVP023","ArXiv":"2301.11280","DOI":"10.48550/arXiv.2301.11280","CorpusId":256274791},"title":"Text-To-4D Dynamic Scene Generation"},{"paperId":"2b4de48703d5278afbce69844d5ed92b5a699ee1","externalIds":{"ArXiv":"2301.10241","DBLP":"journals/corr/abs-2301-10241","DOI":"10.1109/CVPR52729.2023.01201","CorpusId":256194335},"title":"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance"},{"paperId":"5750680aca638c3f90a84f45902e1ef3135c0e98","externalIds":{"ArXiv":"2301.02239","DBLP":"conf/cvpr/0001GMTS0C0023","DOI":"10.1109/CVPR52729.2023.00010","CorpusId":255440362},"title":"Robust Dynamic Radiance Fields"},{"paperId":"e3f5a9251529f34bc15b89e3294e576efbc0af4c","externalIds":{"ArXiv":"2212.03267","DBLP":"journals/corr/abs-2212-03267","DOI":"10.1109/CVPR52729.2023.01977","CorpusId":254366717},"title":"NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors"},{"paperId":"67eb64096c4a0c3aca92f1a1229bdae06eaa73c4","externalIds":{"DBLP":"journals/corr/abs-2212-02469","ArXiv":"2212.02469","DOI":"10.1109/ICCV51070.2023.00824","CorpusId":254246303},"title":"One-shot Implicit Animatable Avatars with Model-based Priors"},{"paperId":"e3a72cc5c29c9b245a0e589384a2adbcfa4a03c0","externalIds":{"ArXiv":"2211.16431","DBLP":"journals/corr/abs-2211-16431","DOI":"10.1109/CVPR52729.2023.00435","CorpusId":254069839},"title":"NeuralLift-360: Lifting an in-the-Wild 2D Photo to A 3D Object with 360° Views"},{"paperId":"6f96b613c8dcb05b3e911aa9ac8e46f64f0b236c","externalIds":{"DBLP":"journals/pami/ChenJSRGBH23","ArXiv":"2211.15601","DOI":"10.1109/TPAMI.2023.3271569","CorpusId":254043854,"PubMed":"37115843"},"title":"Fast-SNARF: A Fast Deformer for Articulated Neural Fields"},{"paperId":"314f1d10c228b32a3d64550f20841786c7f30c40","externalIds":{"DBLP":"conf/cvpr/WangZML23","ArXiv":"2211.12853","DOI":"10.1109/CVPR52729.2023.00406","CorpusId":253802024},"title":"BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields"},{"paperId":"28f045531fddbe63fb61f6ca6e7c4a6b79f72e1d","externalIds":{"ArXiv":"2211.12499","DBLP":"conf/cvpr/ZielonkaBT23","DOI":"10.1109/CVPR52729.2023.00444","CorpusId":253761096},"title":"Instant Volumetric Head Avatars"},{"paperId":"bdf4af8311637c681904e71cf50f96fd0026f578","externalIds":{"DBLP":"journals/corr/abs-2211-10440","ArXiv":"2211.10440","DOI":"10.1109/CVPR52729.2023.00037","CorpusId":253708074},"title":"Magic3D: High-Resolution Text-to-3D Content Creation"},{"paperId":"793939b83e10903f58d8edbb7534963df627a1fe","externalIds":{"DBLP":"journals/corr/abs-2211-07600","ArXiv":"2211.07600","DOI":"10.1109/CVPR52729.2023.01218","CorpusId":253510536},"title":"Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures"},{"paperId":"e24f4b28167b05fbf7d29000490fc0a4e4c109c7","externalIds":{"ArXiv":"2211.01324","DBLP":"journals/corr/abs-2211-01324","DOI":"10.48550/arXiv.2211.01324","CorpusId":253254800},"title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers"},{"paperId":"5974066e3b20b2ea52b980238849c2acfe40613a","externalIds":{"DBLP":"journals/corr/abs-2210-14831","ArXiv":"2210.14831","DOI":"10.48550/arXiv.2210.14831","CorpusId":253116901},"title":"Streaming Radiance Fields for 3D Video Synthesis"},{"paperId":"1fbe23a0d6e45b0bd7861ea33b7e51c27b1a8ef4","externalIds":{"DBLP":"journals/corr/abs-2210-13641","ArXiv":"2210.13641","DOI":"10.1109/IROS55552.2023.10341922","CorpusId":253107311},"title":"NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields"},{"paperId":"11dc60b41f98b96f62f56eb439c9f24fe2efb669","externalIds":{"ArXiv":"2210.08936","DBLP":"conf/nips/YangCCCW22","DOI":"10.48550/arXiv.2210.08936","CorpusId":252917978},"title":"S3-NeRF: Neural Reflectance Field from Shading and Shadow under a Single Viewpoint"},{"paperId":"80ba6fff6d24906d4dba698b0699d62b676f11fd","externalIds":{"ArXiv":"2210.15858","DBLP":"journals/corr/abs-2210-15858","DOI":"10.1109/ISMAR55827.2022.00066","CorpusId":253223971},"title":"Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation"},{"paperId":"4c94d04afa4309ec2f06bdd0fe3781f91461b362","externalIds":{"DBLP":"conf/iclr/PooleJBM23","ArXiv":"2209.14988","DOI":"10.48550/arXiv.2209.14988","CorpusId":252596091},"title":"DreamFusion: Text-to-3D using 2D Diffusion"},{"paperId":"490dd951554bf28cdeb5701052b2eb1ee140acd2","externalIds":{"DBLP":"conf/eccv/SongGPZDYCW22","ArXiv":"2209.10691","DOI":"10.48550/arXiv.2209.10691","CorpusId":252438972},"title":"PREF: Predictability Regularized Neural Motion Fields"},{"paperId":"b0d9a0309b277ed36bf8e80db8a77292d1435d6d","externalIds":{"DBLP":"journals/corr/abs-2208-06787","ArXiv":"2208.06787","DOI":"10.48550/arXiv.2208.06787","CorpusId":251564325},"title":"HDR-Plenoxels: Self-Calibrating High Dynamic Range Radiance Fields"},{"paperId":"b31c2967451722780e92604532e19cdd3f0f49fb","externalIds":{"ArXiv":"2208.00277","DBLP":"journals/corr/abs-2208-00277","DOI":"10.1109/CVPR52729.2023.01590","CorpusId":251223627},"title":"MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures"},{"paperId":"5a365ad81138028dacf989317f34f46852e3e88e","externalIds":{"ArXiv":"2206.08929","DBLP":"conf/eccv/LiTVZGKL22","DOI":"10.48550/arXiv.2206.08929","CorpusId":249848144},"title":"TAVA: Template-free Animatable Volumetric Actors"},{"paperId":"7f313227e68e0eb757d400442262a52c481376d8","externalIds":{"DBLP":"journals/corr/abs-2206-07850","ArXiv":"2206.07850","DOI":"10.48550/arXiv.2206.07850","CorpusId":252438827},"title":"Improved surface reconstruction using high-frequency details"},{"paperId":"781d511d480818ba8d135f5752f45cf5beacd504","externalIds":{"DBLP":"journals/corr/abs-2206-06340","ArXiv":"2206.06340","DOI":"10.48550/arXiv.2206.06340","CorpusId":249626256},"title":"SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data"},{"paperId":"a642118987e374f2a0f0aa3ac26f6b9c8e14e70c","externalIds":{"DBLP":"journals/corr/abs-2206-05737","ArXiv":"2206.05737","DOI":"10.48550/arXiv.2206.05737","CorpusId":249625516},"title":"SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse views"},{"paperId":"21d0f56617c38bd9a5e4794d56b70a1c19d50a09","externalIds":{"ArXiv":"2206.04669","DBLP":"journals/corr/abs-2206-04669","DOI":"10.1109/WACV56688.2023.00086","CorpusId":249538504},"title":"Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields"},{"paperId":"1f1420f059c535667bb32d9b5495518f62dd2ce4","externalIds":{"ArXiv":"2206.00878","DBLP":"journals/corr/abs-2206-00878","DOI":"10.48550/arXiv.2206.00878","CorpusId":249282202},"title":"EfficientNeRF: Efficient Neural Radiance Fields"},{"paperId":"42faf2cd6e6c86d7c18b726ad6dc897202b97be6","externalIds":{"DBLP":"conf/cvpr/AtharXSSS22","ArXiv":"2206.06481","DOI":"10.1109/CVPR52688.2022.01972","CorpusId":249494504},"title":"RigNeRF: Fully Controllable Neural 3D Portraits"},{"paperId":"ddec2b4399d7568fa53525d04df170418b770c0d","externalIds":{"ArXiv":"2205.15848","DBLP":"conf/nips/Fu0OT22","DOI":"10.48550/arXiv.2205.15848","CorpusId":249210038},"title":"Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction"},{"paperId":"6115a34ad9dd868fe886df815071023fd432e7a1","externalIds":{"ArXiv":"2205.15585","DBLP":"journals/corr/abs-2205-15585","DOI":"10.48550/arXiv.2205.15585","CorpusId":249209811},"title":"Decomposing NeRF for Editing via Feature Field Distillation"},{"paperId":"506fdb3d3498b1f2858355b40dac2bb6f07ca25a","externalIds":{"DBLP":"conf/nips/LiuCMZZKSQS22","ArXiv":"2205.15723","DOI":"10.48550/arXiv.2205.15723","CorpusId":249209940},"title":"DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"ed84fd4415a5037b71776c22996c14b9ac3d182d","externalIds":{"DBLP":"journals/corr/abs-2205-05922","ArXiv":"2205.05922","DOI":"10.1109/CVPR52688.2022.01783","CorpusId":248721906},"title":"Ray Priors through Reprojection: Improving Neural Radiance Fields for Novel View Extrapolation"},{"paperId":"d4a265b6008058506a143177422fe192e4fe2090","externalIds":{"DBLP":"journals/corr/abs-2205-04334","ArXiv":"2205.04334","DOI":"10.1109/CVPR52688.2022.01253","CorpusId":248572506},"title":"Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation"},{"paperId":"6f2d01658b8ff1f68d07536775465eb8874f2693","externalIds":{"DBLP":"conf/eccv/ChngRSL22","ArXiv":"2204.05735","DOI":"10.1007/978-3-031-19827-4_16","CorpusId":248118559},"title":"Gaussian Activated Neural Radiance Fields for High Fidelity Reconstruction and Pose Estimation"},{"paperId":"27622ab8eafe2aa886b4398c07456ca1e6074b6a","externalIds":{"DBLP":"conf/cvpr/PearlTK22","ArXiv":"2204.04668","DOI":"10.1109/CVPR52688.2022.01234","CorpusId":248084878},"title":"NAN: Noise-Aware NeRFs for Burst-Denoising"},{"paperId":"710877b1b602ac675280d217372a8ca7d30fb951","externalIds":{"DBLP":"journals/corr/abs-2204-03593","ArXiv":"2204.03593","DOI":"10.1109/CVPR52688.2022.00394","CorpusId":248006097},"title":"AutoRF: Learning 3D Object Radiance Fields from Single View Observations"},{"paperId":"96b935f8579caa31cd8bb785e1c0b740c85b0da2","externalIds":{"DBLP":"conf/cvpr/CoronaHVMSNM22","ArXiv":"2204.01695","DOI":"10.1109/CVPR52688.2022.01988","CorpusId":247939225},"title":"LISA: Learning Implicit Shape and Appearance of Hands"},{"paperId":"084f52f037074c4ef58e8c2a0065a1b1d98a851b","externalIds":{"ArXiv":"2204.00928","DBLP":"journals/corr/abs-2204-00928","DOI":"10.48550/arXiv.2204.00928","CorpusId":247940204},"title":"SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image"},{"paperId":"38d0904bff20007ba2ff8134e68007217ba00439","externalIds":{"ArXiv":"2203.17261","DBLP":"conf/eccv/WangRHOCFT22","DOI":"10.48550/arXiv.2203.17261","CorpusId":247839511},"title":"R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis"},{"paperId":"e9b595ec1d0ff14753626601c832c42fc09a3dd9","externalIds":{"ArXiv":"2203.15224","DBLP":"conf/3dim/FuZCLZZGL22","DOI":"10.1109/3DV57658.2022.00042","CorpusId":247779281},"title":"Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation"},{"paperId":"59d25f507cfe13dd215ef243609dafea9eefae19","externalIds":{"ArXiv":"2203.14478","DBLP":"conf/cvpr/ZhengHYZGL22","DOI":"10.1109/CVPR52688.2022.01543","CorpusId":247762013},"title":"Structured Local Radiance Fields for Human Avatar Modeling"},{"paperId":"17df7e87a5d25e6e83d773e6f686eb0a85f6827e","externalIds":{"DBLP":"conf/eccv/JiangYSTR22","ArXiv":"2203.12575","DOI":"10.48550/arXiv.2203.12575","CorpusId":247618711},"title":"NeuMan: Neural Human Radiance Field from a Single Video"},{"paperId":"647e51a07fc2ff3a6b18f98eb5ba711aeff7bce4","externalIds":{"DBLP":"journals/corr/abs-2203-10821","ArXiv":"2203.10821","DOI":"10.1007/978-3-031-19781-9_42","CorpusId":247593726},"title":"Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields"},{"paperId":"bda0f53cd58a35599cd7f1cb2146d962e8680601","externalIds":{"DBLP":"conf/cvpr/ZhangBS0X22","ArXiv":"2203.11283","DOI":"10.1109/CVPR52688.2022.00537","CorpusId":247596948},"title":"NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction"},{"paperId":"d267731870c41d977c9d51195c1e2fd018846949","externalIds":{"DBLP":"journals/corr/abs-2203-09517","ArXiv":"2203.09517","DOI":"10.48550/arXiv.2203.09517","CorpusId":247519170},"title":"TensoRF: Tensorial Radiance Fields"},{"paperId":"a44889d07ccd2ea99424920270e735978ce652fe","externalIds":{"ArXiv":"2203.08133","DBLP":"journals/pami/ZhouPXDWZSB24","DOI":"10.1109/TPAMI.2024.3355287","CorpusId":250526108,"PubMed":"38231799"},"title":"Animatable Implicit Neural Representations for Creating Realistic Avatars From Videos"},{"paperId":"33d27eee61c5d86d5a561edce10714b91fe42784","externalIds":{"DBLP":"journals/corr/abs-2203-01754","ArXiv":"2203.01754","DOI":"10.1109/CVPR52688.2022.01982","CorpusId":247222837},"title":"PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence"},{"paperId":"4f0c2ec8bcd224a741890109f46b501e00aea35b","externalIds":{"DBLP":"conf/cvpr/CaiODG22","ArXiv":"2202.13162","DOI":"10.1109/CVPR52688.2022.00395","CorpusId":247158374},"title":"Pix2NeRF: Unsupervised Conditional $\\pi$-GAN for Single Image to Neural Radiance Fields Translation"},{"paperId":"34fb43435736cb4b5652360781bd6b846b396ea3","externalIds":{"DBLP":"journals/corr/abs-2202-08614","ArXiv":"2202.08614","DOI":"10.1109/CVPR52688.2022.01316","CorpusId":246904651},"title":"Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time"},{"paperId":"d7d1bbade9453f0348fac8a5c60d131528b87fcf","externalIds":{"DBLP":"conf/cvpr/TancikCYPMSBK22","ArXiv":"2202.05263","DOI":"10.1109/CVPR52688.2022.00807","CorpusId":246706356},"title":"Block-NeRF: Scalable Large Scene Neural View Synthesis"},{"paperId":"055e87ce418a83d6fd555b73aea0d838385dfa85","externalIds":{"DBLP":"journals/corr/abs-2201-08845","ArXiv":"2201.08845","DOI":"10.1109/CVPR52688.2022.00536","CorpusId":246210101},"title":"Point-NeRF: Point-based Neural Radiance Fields"},{"paperId":"60e69982ef2920596c6f31d6fd3ca5e9591f3db6","externalIds":{"ArXiv":"2201.05989","DBLP":"journals/tog/MullerESK22","DOI":"10.1145/3528223.3530127","CorpusId":246016186},"title":"Instant neural graphics primitives with a multiresolution hash encoding"},{"paperId":"fd52fea12a2140219575794bbe9c19cedc905f88","externalIds":{"DBLP":"journals/pami/ZhanYWZLLKTX23","ArXiv":"2112.13592","DOI":"10.1109/TPAMI.2023.3305243","CorpusId":245502474,"PubMed":"37624713"},"title":"Multimodal Image Synthesis and Editing: The Generative AI Era"},{"paperId":"18ba8f0efb362e08903e8e35b062e2c69126e371","externalIds":{"DBLP":"journals/corr/abs-2112-12130","ArXiv":"2112.12130","DOI":"10.1109/CVPR52688.2022.01245","CorpusId":245385791},"title":"NICE-SLAM: Neural Implicit Scalable Encoding for SLAM"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"80d44d92f074683ba8d4c86e1f18f0bc1a29abc4","externalIds":{"DBLP":"conf/cvpr/TurkiRS22","ArXiv":"2112.10703","DOI":"10.1109/CVPR52688.2022.01258","CorpusId":245334780},"title":"Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly- Throughs"},{"paperId":"7c0a7419114db2209c2f386bc1537e90417cf9d4","externalIds":{"DBLP":"conf/cvpr/ChanLCNPMGGTKKW22","ArXiv":"2112.07945","DOI":"10.1109/CVPR52688.2022.01565","CorpusId":245144673},"title":"Efficient Geometry-aware 3D Generative Adversarial Networks"},{"paperId":"792d86d09c2fe992d4ebfb5b80d6dd7ff76e1b3c","externalIds":{"DBLP":"conf/cvpr/ZhengABCBH22","ArXiv":"2112.07471","DOI":"10.1109/CVPR52688.2022.01318","CorpusId":245131174},"title":"I M Avatar: Implicit Morphable Head Avatars from Videos"},{"paperId":"795fecd949592a89c88cc96d22478df04519d4f8","externalIds":{"ArXiv":"2112.05504","DBLP":"conf/eccv/XiangliXPZRTDL22","DOI":"10.1007/978-3-031-19824-3_7","CorpusId":251040899},"title":"BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering"},{"paperId":"c63c28feb43fdfd938e17e707bba06823fb9b7ed","externalIds":{"ArXiv":"2112.05637","DBLP":"journals/corr/abs-2112-05637","DOI":"10.1109/CVPR52688.2022.01973","CorpusId":245117349},"title":"HeadNeRF: A Realtime NeRF-based Parametric Head Model"},{"paperId":"0483be6c3ec6cd41ffe248f86effc7468d3ac7be","externalIds":{"DBLP":"journals/corr/abs-2112-05139","ArXiv":"2112.05139","DOI":"10.1109/CVPR52688.2022.00381","CorpusId":245006265},"title":"CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields"},{"paperId":"e91f73aaef155391b5b07e6612f5346dea888f64","externalIds":{"ArXiv":"2112.05131","DBLP":"conf/cvpr/Fridovich-KeilY22","DOI":"10.1109/CVPR52688.2022.00542","CorpusId":245006364},"title":"Plenoxels: Radiance Fields without Neural Networks"},{"paperId":"40c8c8d8a41c16a0e017cc0d059fae9d346795f0","externalIds":{"DBLP":"journals/corr/abs-2112-03907","ArXiv":"2112.03907","DOI":"10.1109/CVPR52688.2022.00541","CorpusId":244920653},"title":"Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields"},{"paperId":"e59dbb95d71ce16eb329c5f2fa43af5aec910a3f","externalIds":{"DBLP":"journals/corr/abs-2112-03517","ArXiv":"2112.03517","CorpusId":244920682},"title":"CG-NeRF: Conditional Generative Neural Radiance Fields"},{"paperId":"cc52933e2238a80618b958e56cbeb4e4784d7a40","externalIds":{"ArXiv":"2112.02789","DBLP":"conf/cvpr/ZhaoYZLZYX22","DOI":"10.1109/CVPR52688.2022.00759","CorpusId":247748732},"title":"HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs"},{"paperId":"26b1c7ba30879b54c42eef91ee58fa906d7e26cb","externalIds":{"ArXiv":"2112.03288","DBLP":"journals/corr/abs-2112-03288","DOI":"10.1109/CVPR52688.2022.01255","CorpusId":244921004},"title":"Dense Depth Priors for Neural Radiance Fields from Sparse Input Views"},{"paperId":"d14731b2a1e0c77cf246fc82ce76ab6d6b9a047f","externalIds":{"DBLP":"journals/corr/abs-2112-01983","ArXiv":"2112.01983","DOI":"10.1109/CVPR52688.2022.01807","CorpusId":244896417},"title":"CoNeRF: Controllable Neural Radiance Fields"},{"paperId":"98d81b1adcfab59b3174681d0274fd099bd42022","externalIds":{"DBLP":"journals/corr/abs-2112-01759","ArXiv":"2112.01759","DOI":"10.1145/3503161.3547808","CorpusId":244896138},"title":"NeRF-SR: High Quality Neural Radiance Fields using Supersampling"},{"paperId":"13e0adcf727e75f95f7e49243f059b2960037db8","externalIds":{"ArXiv":"2112.01554","DBLP":"conf/cvpr/GrassalPLRNT22","DOI":"10.1109/CVPR52688.2022.01810","CorpusId":244896148},"title":"Neural Head Avatars from Monocular RGB Videos"},{"paperId":"03e1c3b5fdad9b21bbed3d13af7e8d6c73cbcfa6","externalIds":{"DBLP":"journals/corr/abs-2112-01455","ArXiv":"2112.01455","DOI":"10.1109/CVPR52688.2022.00094","CorpusId":244799255},"title":"Zero-Shot Text-Guided Object Generation with Dream Fields"},{"paperId":"7163d171d4671ab8c0fd342e5280db532700999a","externalIds":{"ArXiv":"2112.00724","DBLP":"conf/cvpr/NiemeyerBMS0R22","DOI":"10.1109/CVPR52688.2022.00540","CorpusId":244773517},"title":"RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs"},{"paperId":"a6e7faa007b9307d6b55bd6f633a075215656e2e","externalIds":{"ArXiv":"2111.15234","DBLP":"journals/corr/abs-2111-15234","DOI":"10.1109/CVPR52688.2022.01786","CorpusId":244729083},"title":"NeRFReN: Neural Radiance Fields with Reflections"},{"paperId":"2f983116986e787132a6bfb843c7db2767c9686c","externalIds":{"DBLP":"journals/corr/abs-2111-15135","ArXiv":"2111.15135","DOI":"10.1007/978-3-031-19827-4_9","CorpusId":244729036},"title":"Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs"},{"paperId":"102d29870ba101004afce311823df85a9f304be7","externalIds":{"DBLP":"conf/cvpr/RematasLSBTFF22","ArXiv":"2111.14643","DOI":"10.1109/CVPR52688.2022.01259","CorpusId":244714334},"title":"Urban Radiance Fields"},{"paperId":"7bc683fe1911f4987b845f7d2165d5888f0a50e9","externalIds":{"DBLP":"conf/cvpr/MaL0ZWWS22","ArXiv":"2111.14292","DOI":"10.1109/CVPR52688.2022.01252","CorpusId":244714238},"title":"Deblur-NeRF: Neural Radiance Fields from Blurry Images"},{"paperId":"61d03d3aeb2b264f97d3d6d7220b3b50db180f81","externalIds":{"DBLP":"journals/corr/abs-2111-14451","ArXiv":"2111.14451","DOI":"10.1109/CVPR52688.2022.01785","CorpusId":244714505},"title":"HDR-NeRF: High Dynamic Range Neural Radiance Fields"},{"paperId":"de744193c2c21f5e518b71a804892498a9b76925","externalIds":{"DBLP":"conf/cvpr/MildenhallHMSB22","ArXiv":"2111.13679","DOI":"10.1109/CVPR52688.2022.01571","CorpusId":244709242},"title":"NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images"},{"paperId":"7d5d712b28818f95ed79ce9383a121523cab7bfd","externalIds":{"DBLP":"conf/cvpr/JohariLF22","ArXiv":"2111.13539","DOI":"10.1109/CVPR52688.2022.01782","CorpusId":244708875},"title":"GeoNeRF: Generalizing NeRF with Geometry Priors"},{"paperId":"d1463da5601234d722503c02c2dda588bb4f314f","externalIds":{"DBLP":"conf/cvpr/SajjadiMPBGRVLD22","ArXiv":"2111.13152","DOI":"10.1109/CVPR52688.2022.00613","CorpusId":244709599},"title":"Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations"},{"paperId":"4e1e3efa9218335c1b40573c5c88b41dc49c0da1","externalIds":{"DBLP":"journals/corr/abs-2111-13260","ArXiv":"2111.13260","CorpusId":244708895},"title":"NeSF: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes"},{"paperId":"ec90ffa017a2cc6a51342509ce42b81b478aefb3","externalIds":{"ArXiv":"2111.12077","DBLP":"conf/cvpr/BarronMVSH22","DOI":"10.1109/CVPR52688.2022.00539","CorpusId":244488448},"title":"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields"},{"paperId":"4f7eb65f8d3c1eeb97e30f7ac68977ff16e1e942","externalIds":{"ArXiv":"2111.11215","DBLP":"conf/cvpr/0004SC22","DOI":"10.1109/CVPR52688.2022.00538","CorpusId":244477646},"title":"Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction"},{"paperId":"521026c6097b0c0b44cb97b8a3952261527bea41","externalIds":{"ArXiv":"2111.11426","DBLP":"journals/corr/abs-2111-11426","DOI":"10.1111/cgf.14505","CorpusId":244478496},"title":"Neural Fields in Visual Computing and Beyond"},{"paperId":"2fef692b57e036e84c0f3a561743da5a14e3994d","externalIds":{"DBLP":"conf/cvpr/RebainMYLT22","ArXiv":"2111.09996","DOI":"10.1109/CVPR52688.2022.00161","CorpusId":244463222},"title":"LOLNeRF: Learn from One Look"},{"paperId":"39a1787c29a3e8a74a87ba87e6e0fdb088d47b7b","externalIds":{"DBLP":"journals/corr/abs-2111-10427","ArXiv":"2111.10427","DOI":"10.1109/CVPR52688.2022.01572","CorpusId":244478644},"title":"DIVeR: Real-time and Accurate Neural Radiance Fields with Deterministic Integration for Volume Rendering"},{"paperId":"c3c087ba647465653a49a4be9213a39ac657af11","externalIds":{"DBLP":"conf/corl/IchnowskiAKG21","ArXiv":"2110.14217","CorpusId":239998474},"title":"Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects"},{"paperId":"3fdda879abf2462b09139fb1fd1c2c147c9a0ef0","externalIds":{"DBLP":"journals/corr/abs-2110-08985","ArXiv":"2110.08985","CorpusId":239016913},"title":"StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis"},{"paperId":"edcf7fd9cb17dfd9f6e6d6e97620b31cd7613a8d","externalIds":{"DBLP":"journals/corr/abs-2110-00168","ArXiv":"2110.00168","DOI":"10.1109/LRA.2022.3150497","CorpusId":238253331},"title":"Vision-Only Robot Navigation in a Neural Radiance World"},{"paperId":"0314cfdc5fac4f9a9b9195c73dea9e4a1203b471","externalIds":{"ArXiv":"2109.13410","DBLP":"journals/corr/abs-2109-13410","DOI":"10.1109/TPAMI.2022.3179507","CorpusId":238198653,"PubMed":"35648872"},"title":"KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D"},{"paperId":"8bdfd48579130b50112c03170904d803aafe6ba9","externalIds":{"ArXiv":"2109.01847","DBLP":"conf/iccv/Yang0XLZB0C21","DOI":"10.1109/ICCV48922.2021.01352","CorpusId":237421130},"title":"Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering"},{"paperId":"0b403437a5bd42a717973aaf8f4493695fde240b","externalIds":{"DBLP":"journals/corr/abs-2109-01750","ArXiv":"2109.01750","DOI":"10.1109/ICCV48922.2021.01271","CorpusId":237420378},"title":"CodeNeRF: Disentangled Neural Radiance Fields for Object Categories"},{"paperId":"149343c70c8c506c2e91112746549eea4300817b","externalIds":{"DBLP":"journals/corr/abs-2109-01129","ArXiv":"2109.01129","DOI":"10.1109/iccv48922.2021.00556","CorpusId":237386110},"title":"NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo"},{"paperId":"77327efa13f9e1defba258091bab5d406d047a0b","externalIds":{"DBLP":"conf/iccv/ReizensteinSHSL21","ArXiv":"2109.00512","DOI":"10.1109/ICCV48922.2021.01072","CorpusId":237371959},"title":"Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction"},{"paperId":"45d48c24eb5dce288c7475f91ad1526f3e763a65","externalIds":{"DBLP":"conf/iccv/JeongACACP21","ArXiv":"2108.13826","DOI":"10.1109/ICCV48922.2021.00579","CorpusId":237363825},"title":"Self-Calibrating Neural Radiance Fields"},{"paperId":"3d1b2afb6cd38412126b46a49e6b0e1fcdcaad3a","externalIds":{"ArXiv":"2107.13421","DBLP":"journals/corr/abs-2107-13421","DOI":"10.1109/CVPR52688.2022.00767","CorpusId":236469482},"title":"Neural Rays for Occlusion-aware Image-based Rendering"},{"paperId":"b37bbc5345e638e003e05d5282a938529a1cc514","externalIds":{"DBLP":"journals/corr/abs-2107-12351","ArXiv":"2107.12351","DOI":"10.2312/sr.20211299","CorpusId":235681857},"title":"NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting"},{"paperId":"0b5b6598e3e108147842f35ff66a95d989f9ec89","externalIds":{"DBLP":"journals/corr/abs-2111-05849","MAG":"3185841081","ArXiv":"2111.05849","DOI":"10.1111/cgf.14507","CorpusId":236162433},"title":"Advances in Neural Rendering"},{"paperId":"988952b0e737c8ab9b6c1fbd6d54db86e299d270","externalIds":{"ArXiv":"2107.02791","DBLP":"journals/corr/abs-2107-02791","DOI":"10.1109/CVPR52688.2022.01254","CorpusId":235743051},"title":"Depth-supervised NeRF: Fewer Views and Faster Training for Free"},{"paperId":"cf5647cb2613f5f697729eab567383006dcd4913","externalIds":{"ArXiv":"2106.10689","DBLP":"journals/corr/abs-2106-10689","CorpusId":235490453},"title":"NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction"},{"paperId":"e3a3f61f3ab5f1c310f3062a2e40fed49fc2caa4","externalIds":{"ArXiv":"2106.03798","DBLP":"conf/cvpr/ShaoZZCC0L22","DOI":"10.1109/CVPR52688.2022.01541","CorpusId":244727701},"title":"DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering"},{"paperId":"fb0cd71b23b19a452b6981de3b972ac891a9f60d","externalIds":{"DBLP":"journals/corr/abs-2106-02634","ArXiv":"2106.02634","CorpusId":235352518},"title":"Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering"},{"paperId":"5cba144c945eb77a16b1508dbfbbb3db1d26bf95","externalIds":{"DBLP":"journals/corr/abs-2105-09103","ArXiv":"2105.09103","DOI":"10.1109/TVCG.2022.3204608","CorpusId":234777883,"PubMed":"36194712"},"title":"Recursive-NeRF: An Efficient and Dynamically Growing NeRF"},{"paperId":"61aa9c31b7d00c2bcd45b624f93a7952af20710c","externalIds":{"ArXiv":"2105.06466","DBLP":"journals/corr/abs-2105-06466","DOI":"10.1109/ICCV48922.2021.00572","CorpusId":234482509},"title":"Editing Conditional Radiance Fields"},{"paperId":"e137c4fcdd7be545f1001d5f590538387493f7f1","externalIds":{"ArXiv":"2105.04668","DBLP":"journals/corr/abs-2105-04668","DOI":"10.1109/ICCV48922.2021.01129","CorpusId":234357879},"title":"HuMoR: 3D Human Motion Model for Robust Pose Estimation"},{"paperId":"35e940ace815548f620709d9c1803da34c581e86","externalIds":{"DBLP":"conf/iccv/PengDWZSZB21","ArXiv":"2105.02872","DOI":"10.1109/ICCV48922.2021.01405","CorpusId":238419696},"title":"Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies"},{"paperId":"f184a47adced84a1f7f99bd052d8f30f0587d6db","externalIds":{"DBLP":"journals/corr/abs-2104-10078","ArXiv":"2104.10078","DOI":"10.1109/ICCV48922.2021.00554","CorpusId":233307004},"title":"UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction"},{"paperId":"8b5300bea1e5ec06f14be1dc87ad2eb26951b517","externalIds":{"DBLP":"conf/cvpr/DerksenI21","ArXiv":"2104.09877","MAG":"3176084073","DOI":"10.1109/CVPRW53098.2021.00126","CorpusId":233306958},"title":"Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry"},{"paperId":"802be9ef1c7a80757e6c62cb9f1ab257478eb9e5","externalIds":{"DBLP":"journals/corr/abs-2104-08418","ArXiv":"2104.08418","DOI":"10.1109/3DV53792.2021.00104","CorpusId":233297080},"title":"FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling"},{"paperId":"2d029e88d045fed07ea6a6d01b56a6fc6df38474","externalIds":{"DBLP":"journals/corr/abs-2104-06935","ArXiv":"2104.06935","DOI":"10.1109/CVPR46437.2021.00782","CorpusId":232768311},"title":"Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes"},{"paperId":"33cc02f23c97a3daa835953b9d2784d0e1abf16e","externalIds":{"DBLP":"journals/corr/abs-2104-06405","ArXiv":"2104.06405","DOI":"10.1109/ICCV48922.2021.00569","CorpusId":233219786},"title":"BARF: Bundle-Adjusting Neural Radiance Fields"},{"paperId":"03074d197dda05614f93509c0938d66bf5993236","externalIds":{"ArXiv":"2104.04532","DBLP":"conf/cvpr/AzinovicMGNT22","DOI":"10.1109/CVPR52688.2022.00619","CorpusId":233210013},"title":"Neural RGB-D Surface Reconstruction"},{"paperId":"41638479c1caac6ee62f850e783ac4c3d37fa035","externalIds":{"MAG":"3141043576","DBLP":"journals/corr/abs-2104-00587","ArXiv":"2104.00587","CorpusId":232478805},"title":"NeRF-VAE: A Geometry Aware 3D Scene Generative Model"},{"paperId":"81918488b569df3a43fd998d7b698fa9f6d1ff1b","externalIds":{"DBLP":"conf/iccv/JainTA21","ArXiv":"2104.00677","DOI":"10.1109/ICCV48922.2021.00583","CorpusId":232478424},"title":"Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis"},{"paperId":"be05b97c1a221b12640bac66cf4b322e3a81ca86","externalIds":{"ArXiv":"2103.15875","DBLP":"conf/iccv/ZhiLLD21","DOI":"10.1109/ICCV48922.2021.01554","CorpusId":232417682},"title":"In-Place Scene Labelling and Understanding with Implicit Scene Representation"},{"paperId":"169971b60749264cbbe2b577dc4d2ad23ca4f46c","externalIds":{"DBLP":"journals/corr/abs-2103-15595","ArXiv":"2103.15595","DOI":"10.1109/ICCV48922.2021.01386","CorpusId":232404617},"title":"MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo"},{"paperId":"97b1ec438bba3fab2db21a733bb35c298aed599d","externalIds":{"DBLP":"journals/corr/abs-2103-15606","ArXiv":"2103.15606","DOI":"10.1109/ICCV48922.2021.00629","CorpusId":232404358},"title":"GNeRF: GAN-based Neural Radiance Field without Posed Camera"},{"paperId":"0f138a10bed7fb1266f74d6a02407f9c43b76b6f","externalIds":{"ArXiv":"2103.14910","DBLP":"conf/iccv/LiFSDWL21","DOI":"10.1109/ICCV48922.2021.01235","CorpusId":236635025},"title":"MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis"},{"paperId":"8bd70d3dcfa295d9710922c34c1a9eeb0be48b94","externalIds":{"ArXiv":"2103.14645","DBLP":"journals/corr/abs-2103-14645","DOI":"10.1109/ICCV48922.2021.00582","CorpusId":232379923},"title":"Baking Neural Radiance Fields for Real-Time View Synthesis"},{"paperId":"c041aaed581616e122e790dd2769337216df3d8d","externalIds":{"DBLP":"conf/iccv/ReiserPL021","ArXiv":"2103.13744","DOI":"10.1109/ICCV48922.2021.01407","CorpusId":232352619},"title":"KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs"},{"paperId":"5744fcc21b40327f7ad710de7d947d4584c53012","externalIds":{"ArXiv":"2103.14024","DBLP":"journals/corr/abs-2103-14024","DOI":"10.1109/ICCV48922.2021.00570","CorpusId":232352425},"title":"PlenOctrees for Real-time Rendering of Neural Radiance Fields"},{"paperId":"21336e57dc2ab9ae2171a0f6c35f7d1aba584796","externalIds":{"ArXiv":"2103.13415","DBLP":"journals/corr/abs-2103-13415","DOI":"10.1109/ICCV48922.2021.00580","CorpusId":232352655},"title":"Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields"},{"paperId":"3629e0ba2ffb8406d8aba880685f06aa35e3cd7f","externalIds":{"DBLP":"conf/iccv/SucarLOD21","ArXiv":"2103.12352","DOI":"10.1109/ICCV48922.2021.00617","CorpusId":232320654},"title":"iMAP: Implicit Mapping and Positioning in Real-Time"},{"paperId":"a81ef3138365f762334736769174738413cccdd9","externalIds":{"DBLP":"conf/iccv/GuoCLLBZ21","ArXiv":"2103.11078","DOI":"10.1109/ICCV48922.2021.00573","CorpusId":232307613},"title":"AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis"},{"paperId":"c0ccfbaf073c91e68ccbc57af2114c72c0d0427d","externalIds":{"DBLP":"conf/iccv/GarbinK0SV21","ArXiv":"2103.10380","DOI":"10.1109/ICCV48922.2021.01408","CorpusId":232270138},"title":"FastNeRF: High-Fidelity Neural Rendering at 200FPS"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"7cbc3dd0280b8c4551ac934af42dc227d43754f7","externalIds":{"ArXiv":"2102.13090","DBLP":"conf/cvpr/WangWGSZBMSF21","MAG":"3186630079","DOI":"10.1109/CVPR46437.2021.00466","CorpusId":232045969},"title":"IBRNet: Learning Multi-View Image-Based Rendering"},{"paperId":"6034aa4be2520537e02b707a31abc0d033673c85","externalIds":{"ArXiv":"2102.07064","CorpusId":268127194},"title":"NeRF--: Neural Radiance Fields Without Known Camera Parameters"},{"paperId":"74bb19d1ce2ec9fb9605545a750e68281a067e63","externalIds":{"ArXiv":"2102.06199","DBLP":"conf/nips/SuYZR21","CorpusId":239885761},"title":"A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose"},{"paperId":"af8faec7c0b8f4b2a28d42a86e0e7d499016c560","externalIds":{"DBLP":"journals/corr/abs-2012-15838","ArXiv":"2012.15838","DOI":"10.1109/CVPR46437.2021.00894","CorpusId":229924396},"title":"Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans"},{"paperId":"464890394a85587ca8499f4f56cba0add7dcde9a","externalIds":{"DBLP":"conf/cvpr/AhmadyanZAWG21","ArXiv":"2012.09988","DOI":"10.1109/CVPR46437.2021.00773","CorpusId":229331553},"title":"Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations"},{"paperId":"33d1cb96eb2c77fd64afee5d2ce77d2449c66596","externalIds":{"ArXiv":"2101.05204","DBLP":"journals/corr/abs-2101-05204","CorpusId":231592673},"title":"Neural Volume Rendering: NeRF And Beyond"},{"paperId":"3882aa4b2370cae3f8b7b9c41e9088f8c056cdd9","externalIds":{"ArXiv":"2012.05877","DBLP":"conf/iros/LinFBRIL21","MAG":"3112108866","DOI":"10.1109/IROS51168.2021.9636708","CorpusId":228083990},"title":"iNeRF: Inverting Neural Radiance Fields for Pose Estimation"},{"paperId":"890398bb6364141f8b4a798fbc1c1605a871bd1d","externalIds":{"DBLP":"journals/corr/abs-2012-03065","MAG":"3111632845","ArXiv":"2012.03065","DOI":"10.1109/CVPR46437.2021.00854","CorpusId":227342468},"title":"Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction"},{"paperId":"4365f51fc270c55005adb794002685078a6fca1d","externalIds":{"DBLP":"conf/cvpr/YuYTK21","ArXiv":"2012.02190","MAG":"3109025584","DOI":"10.1109/CVPR46437.2021.00455","CorpusId":227254854},"title":"pixelNeRF: Neural Radiance Fields from One or Few Images"},{"paperId":"4fc6e9545d30539fac6a6a0dcac1418ad4dd9a9f","externalIds":{"DBLP":"journals/corr/abs-2012-01714","MAG":"3109596657","ArXiv":"2012.01714","DOI":"10.1109/CVPR46437.2021.01432","CorpusId":227255123},"title":"AutoInt: Automatic Integration for Fast Neural Volume Rendering"},{"paperId":"8d17d62952f141fe5c4948eeafb8be5a8db9d054","externalIds":{"ArXiv":"2012.00926","DBLP":"conf/cvpr/ChanMK0W21","MAG":"3107517429","DOI":"10.1109/CVPR46437.2021.00574","CorpusId":227247980},"title":"pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis"},{"paperId":"694bdf6e5906992dad2987a3cc8d1a176de691c9","externalIds":{"MAG":"3109476562","DBLP":"journals/corr/abs-2011-13961","ArXiv":"2011.13961","DOI":"10.1109/CVPR46437.2021.01018","CorpusId":227227965},"title":"D-NeRF: Neural Radiance Fields for Dynamic Scenes"},{"paperId":"0f1af3f94f4699cd70a554f68f8f9e2c8e3d53dd","externalIds":{"MAG":"3107486934","DBLP":"journals/corr/abs-2011-12948","ArXiv":"2011.12948","DOI":"10.1109/ICCV48922.2021.00581","CorpusId":234364556},"title":"Nerfies: Deformable Neural Radiance Fields"},{"paperId":"bb8656979f38d95062ba55640c1be65535f57c6a","externalIds":{"MAG":"3109420014","ArXiv":"2011.12100","DBLP":"conf/cvpr/Niemeyer021","DOI":"10.1109/CVPR46437.2021.01129","CorpusId":227151657},"title":"GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"5b0ea2c92ee16fa2f5a3dbc9315cd5c1e4ec1d88","externalIds":{"MAG":"3092948912","DBLP":"journals/corr/abs-2010-07492","ArXiv":"2010.07492","CorpusId":222380037},"title":"NeRF++: Analyzing and Improving Neural Radiance Fields"},{"paperId":"3c2197394f2bf59c07776a853b36f64254bd7a3b","externalIds":{"DBLP":"conf/iccv/Trevithick021","ArXiv":"2010.04595","DOI":"10.1109/ICCV48922.2021.01490","CorpusId":236975860},"title":"GRF: Learning a General Radiance Field for 3D Representation and Rendering"},{"paperId":"c861eaf43aa30b35813e9a84f07a8e1651c9722e","externalIds":{"MAG":"3048423054","DBLP":"journals/corr/abs-2008-04852","ArXiv":"2008.04852","DOI":"10.1007/978-3-030-58539-6_15","CorpusId":221136353},"title":"GeLaTO: Generative Latent Textured Objects"},{"paperId":"691eddbfaebbc71f6a12d3c99d5c155042459434","externalIds":{"DBLP":"journals/corr/abs-2008-02268","ArXiv":"2008.02268","MAG":"3047146825","DOI":"10.1109/CVPR46437.2021.00713","CorpusId":220968781},"title":"NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections"},{"paperId":"17d7767a6ea87f4ab24d9cfaa5039160af9cad76","externalIds":{"ArXiv":"2007.11571","DBLP":"conf/nips/LiuGLCT20","MAG":"3092203888","CorpusId":220686483},"title":"Neural Sparse Voxel Fields"},{"paperId":"62d337dbaead376ca042f23d62c0d4b65ec98546","externalIds":{"DBLP":"journals/corr/abs-2007-02442","ArXiv":"2007.02442","MAG":"3101267796","CorpusId":220364071},"title":"GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis"},{"paperId":"a0dc3135c40e150f0271002a96b7c9680b6cac40","externalIds":{"DBLP":"conf/nips/TancikSMFRSRBN20","ArXiv":"2006.10739","MAG":"3036843665","CorpusId":219791950},"title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains"},{"paperId":"43b1e34451f783fed053c1d539d7560dc4ec16a9","externalIds":{"MAG":"3103313582","DBLP":"conf/nips/SitzmannMBLW20","ArXiv":"2006.09661","CorpusId":219720931},"title":"Implicit Neural Representations with Periodic Activation Functions"},{"paperId":"785eb47a72b579d7da4a99414288349afc025116","externalIds":{"MAG":"3094088110","DBLP":"conf/nips/YarivKMGABL20","CorpusId":225077557},"title":"Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance"},{"paperId":"021f956ab3bc13008e87d24f42feeced61eb5916","externalIds":{"ArXiv":"2003.01587","MAG":"3092233714","DBLP":"journals/ijcv/JinMMMFYT21","DOI":"10.1007/s11263-020-01385-0","CorpusId":211817874},"title":"Image Matching Across Wide Baselines: From Paper to Practice"},{"paperId":"dfc66041b88ccdfc21ae816e0ecee9e8d1ef4731","externalIds":{"DBLP":"journals/corr/abs-1912-07372","MAG":"2995007867","ArXiv":"1912.07372","DOI":"10.1109/cvpr42600.2020.00356","CorpusId":209376368},"title":"Differentiable Volumetric Rendering: Learning Implicit 3D Representations Without 3D Supervision"},{"paperId":"bb3a9fad105717c62195e0c6b99093fd99b777f0","externalIds":{"MAG":"3035654619","DBLP":"conf/cvpr/GenovaCSSF20","DOI":"10.1109/cvpr42600.2020.00491","CorpusId":219631990},"title":"Local Deep Implicit Functions for 3D Shape"},{"paperId":"8406903fd2f0eb25349bf071ccfaae3947e2a9cd","externalIds":{"MAG":"3035172746","DBLP":"conf/cvpr/SunKDCPTGZCCVHN20","ArXiv":"1912.04838","DOI":"10.1109/CVPR42600.2020.00252","CorpusId":209140225},"title":"Scalability in Perception for Autonomous Driving: Waymo Open Dataset"},{"paperId":"14fdc18d9c164e5b0d6d946b3238c04e81921358","externalIds":{"MAG":"3035574324","DBLP":"conf/cvpr/KarrasLAHLA20","ArXiv":"1912.04958","DOI":"10.1109/cvpr42600.2020.00813","CorpusId":209202273},"title":"Analyzing and Improving the Image Quality of StyleGAN"},{"paperId":"e32541301130923df1ae4a83cb8037bd459e5254","externalIds":{"MAG":"3035257660","DBLP":"journals/corr/abs-1911-10127","ArXiv":"1911.10127","DOI":"10.1109/cvpr42600.2020.00186","CorpusId":208248003},"title":"BlendedMVS: A Large-Scale Dataset for Generalized Multi-View Stereo Networks"},{"paperId":"60f511eef446120fe59563ee976f948d5e3f9064","externalIds":{"MAG":"2949830259","DBLP":"journals/corr/abs-1906-05797","ArXiv":"1906.05797","CorpusId":189762191},"title":"The Replica Dataset: A Digital Replica of Indoor Spaces"},{"paperId":"b9d4a1ac5e41570082828b405b289aa6959252a4","externalIds":{"ArXiv":"1906.01618","DBLP":"journals/corr/abs-1906-01618","MAG":"2971278627","CorpusId":174798113},"title":"Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations"},{"paperId":"af39e137818ef8304ccea2ada546700de1dd2f8c","externalIds":{"MAG":"2964288609","DBLP":"journals/tog/MildenhallSCKRN19","ArXiv":"1905.00889","DOI":"10.1145/3306346.3322980","CorpusId":260545139},"title":"Local light field fusion"},{"paperId":"cc6a32280064b061c6ca53467e2d5fb11dcce5c4","externalIds":{"DBLP":"conf/cvpr/LiDCTSLF19","ArXiv":"1904.11111","MAG":"2952647833","DOI":"10.1109/CVPR.2019.00465","CorpusId":131775632},"title":"Learning the Depths of Moving People by Watching Frozen People"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"dd81523b9accdf1c13cd37f76b22ab27d84b7a42","externalIds":{"MAG":"2951382975","DBLP":"journals/corr/abs-1901-05103","ArXiv":"1901.05103","DOI":"10.1109/CVPR.2019.00025","CorpusId":58007025},"title":"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"3175307e30b0fa05b43d5d1f63bc963265f791f9","externalIds":{"DBLP":"journals/pami/ChengWY20","MAG":"2894788161","ArXiv":"1810.02695","DOI":"10.1109/TPAMI.2019.2947374","CorpusId":52934534,"PubMed":"31634121"},"title":"Learning Depth with Convolutional Spatial Propagation Network"},{"paperId":"50890ecfad90328453c9552f68252e324c742711","externalIds":{"MAG":"2803244426","DBLP":"conf/icml/MarinoYM18","ArXiv":"1807.09356","CorpusId":50785699},"title":"Iterative Amortized Inference"},{"paperId":"87ca28235555f7e70cf1edc2a63cda4aef7fee42","externalIds":{"MAG":"2949176509","DBLP":"conf/eccv/YaoLLFQ18","ArXiv":"1804.02505","DOI":"10.1007/978-3-030-01237-3_47","CorpusId":4712004},"title":"MVSNet: Depth Inference for Unstructured Multi-view Stereo"},{"paperId":"02ccfc9b550d381b5df4365a2ae48bb5f7f7578e","externalIds":{"ArXiv":"1803.04189","MAG":"2793146153","DBLP":"journals/corr/abs-1803-04189","CorpusId":3846544},"title":"Noise2Noise: Learning Image Restoration without Clean Data"},{"paperId":"9217e28b2273eb3b26e4e9b7b498b4661e6e09f5","externalIds":{"ArXiv":"1802.02611","MAG":"2787091153","DBLP":"journals/corr/abs-1802-02611","DOI":"10.1007/978-3-030-01234-2_49","CorpusId":3638670},"title":"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"},{"paperId":"cb8f1840a018fddcc7a453dd73cc082ebea82e7b","externalIds":{"DBLP":"conf/icml/KimWMSR18","ArXiv":"1802.02550","MAG":"2952471899","CorpusId":334803},"title":"Semi-Amortized Variational Autoencoders"},{"paperId":"c468bbde6a22d961829e1970e6ad5795e05418d1","externalIds":{"ArXiv":"1801.03924","MAG":"2783879794","DBLP":"journals/corr/abs-1801-03924","DOI":"10.1109/CVPR.2018.00068","CorpusId":4766599},"title":"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"},{"paperId":"ebf0615fc4d98cf1dbe527c79146ce1e50dce9af","externalIds":{"MAG":"2767621168","DBLP":"conf/corl/DosovitskiyRCLK17","ArXiv":"1711.03938","CorpusId":5550767},"title":"CARLA: An Open Urban Driving Simulator"},{"paperId":"8337441971f941716a9e525a67f37088eb01fd13","externalIds":{"MAG":"2949525187","DBLP":"journals/corr/abs-1709-06158","ArXiv":"1709.06158","DOI":"10.1109/3DV.2017.00081","CorpusId":21435690},"title":"Matterport3D: Learning from RGB-D Data in Indoor Environments"},{"paperId":"e2ef0f1a528cd0f415be8265a04466a6d3f74e6c","externalIds":{"MAG":"2963376432","DBLP":"journals/corr/BojanowskiJLS17","ArXiv":"1707.05776","CorpusId":2019311},"title":"Optimizing the Latent Space of Generative Networks"},{"paperId":"8760bc7631c0cb04e7138254e9fd6451b7def8ca","externalIds":{"DBLP":"journals/corr/SunSSG17","MAG":"2734663976","ArXiv":"1707.02968","DOI":"10.1109/ICCV.2017.97","CorpusId":6842201},"title":"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"41f2a087031944f9b990eb102f59b4ff58d6b5ef","externalIds":{"DBLP":"journals/corr/MahlerLNLDLOG17","ArXiv":"1703.09312","MAG":"2600030077","DOI":"10.15607/RSS.2017.XIII.058","CorpusId":6138957},"title":"Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"d997beefc0922d97202789d2ac307c55c2c52fba","externalIds":{"MAG":"2950642167","DBLP":"conf/cvpr/QiSMG17","ArXiv":"1612.00593","DOI":"10.1109/CVPR.2017.16","CorpusId":5115938},"title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"},{"paperId":"4603cb8e05258bb0572ae912ad20903b8f99f4b1","externalIds":{"DBLP":"journals/corr/GuoZHHG16","ArXiv":"1607.08221","MAG":"2952419167","DOI":"10.1007/978-3-319-46487-9_6","CorpusId":2908606},"title":"MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition"},{"paperId":"e7d0c37f4f3589a3b787f39e8307704da5ed8d6c","externalIds":{"DBLP":"conf/cvpr/SchonbergerF16","MAG":"2471962767","DOI":"10.1109/CVPR.2016.445","CorpusId":1728538},"title":"Structure-from-Motion Revisited"},{"paperId":"ba11b4feb04a472cb5e5962697ed6faa653dc647","externalIds":{"MAG":"2906447902","DBLP":"journals/cacm/ThiesZSTN19","ArXiv":"2007.14808","DOI":"10.1145/3292039","CorpusId":52858569},"title":"Face2Face: Real-Time Face Capture and Reenactment of RGB Videos"},{"paperId":"09879f7956dddc2a9328f5c1472feeb8402bcbcf","externalIds":{"MAG":"2950173439","DBLP":"journals/corr/DinhSB16","ArXiv":"1605.08803","CorpusId":8768364},"title":"Density estimation using Real NVP"},{"paperId":"b8d9e2bb5b517f5b307045efd0cc3a9bf4967419","externalIds":{"DBLP":"journals/tog/YucerSWS16","MAG":"2320359495","DOI":"10.1145/2876504","CorpusId":15061054},"title":"Efficient 3D Object Segmentation from Densely Sampled Light Fields with Applications to 3D Reconstruction"},{"paperId":"592d2e65489f23ebd993dbdc0c84eda9ac8aadbe","externalIds":{"ArXiv":"1602.07360","DBLP":"journals/corr/IandolaMAHDK16","MAG":"2279098554","CorpusId":14136028},"title":"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size"},{"paperId":"bc6fa661298e2226c15725308da70e93f0998e83","externalIds":{"MAG":"2253156915","ArXiv":"1602.02481","DBLP":"journals/corr/ChoiZMK16","CorpusId":13129136},"title":"A Large Dataset of Object Scans"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"9b686d76914befea66377ec79c1f9258d70ea7e3","externalIds":{"MAG":"2190691619","ArXiv":"1512.03012","DBLP":"journals/corr/ChangFGHHLSSSSX15","CorpusId":2554264},"title":"ShapeNet: An Information-Rich 3D Model Repository"},{"paperId":"586eb9f2eb5d963ef95c266eab2723bd8d7911a3","externalIds":{"DBLP":"conf/iccv/JooLTGNMKNS15","MAG":"2215643317","DOI":"10.1109/ICCV.2015.381","CorpusId":15005115},"title":"Panoptic Studio: A Massively Multiview System for Social Motion Capture"},{"paperId":"edf455c3b5b8d1c6337c72e39940125036354d03","externalIds":{"MAG":"1921093919","DBLP":"conf/cvpr/MenzeG15","DOI":"10.1109/CVPR.2015.7298925","CorpusId":12986049},"title":"Object scene flow for autonomous vehicles"},{"paperId":"2dcef55a07f8607a819c21fe84131ea269cc2e3c","externalIds":{"MAG":"2129069237","DBLP":"journals/corr/Sohl-DicksteinW15","ArXiv":"1503.03585","CorpusId":14888175},"title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","externalIds":{"MAG":"2949429431","ArXiv":"1409.1556","DBLP":"journals/corr/SimonyanZ14a","CorpusId":14124313},"title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"736baa2d935ebffca0683bdc729dff36eba15d68","externalIds":{"MAG":"2085905957","DBLP":"conf/cvpr/JensenDVTA14","DOI":"10.1109/CVPR.2014.59","CorpusId":18412989},"title":"Large Scale Multi-view Stereopsis Evaluation"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","externalIds":{"MAG":"2950635152","DBLP":"conf/emnlp/ChoMGBBSB14","ACL":"D14-1179","ArXiv":"1406.1078","DOI":"10.3115/v1/D14-1179","CorpusId":5590763},"title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"a5fa91dbc72f200970e70debe88375b71ddef40b","externalIds":{"DBLP":"conf/itsc/FritschKG13","MAG":"2167222293","DOI":"10.1109/ITSC.2013.6728473","CorpusId":14769670},"title":"A new performance measure and evaluation benchmark for road detection algorithms"},{"paperId":"79b949d9b35c3f51dd20fb5c746cc81fc87147eb","externalIds":{"MAG":"2115579991","DBLP":"journals/ijrr/GeigerLSU13","DOI":"10.1177/0278364913491297","CorpusId":9455111},"title":"Vision meets robotics: The KITTI dataset"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42","externalIds":{"DBLP":"conf/cvpr/GeigerLU12","MAG":"2150066425","DOI":"10.1109/CVPR.2012.6248074","CorpusId":6724907},"title":"Are we ready for autonomous driving? The KITTI vision benchmark suite"},{"paperId":"aa88eb88dbe1a24e92b85e66c4e8059192aa2aee","externalIds":{"DBLP":"journals/tog/ParkerBDFHLMMMRS10","MAG":"2050513283","DOI":"10.1145/1833349.1778803","CorpusId":5947188},"title":"OptiX: a general purpose ray tracing engine"},{"paperId":"93b8b54c709a798f6958ce77b4652810d9027ce5","externalIds":{"MAG":"2147334734","DBLP":"conf/eccv/ZhangST08","DOI":"10.1007/978-3-540-88693-8_59","CorpusId":2441648},"title":"Cat Head Detection - How to Effectively Exploit Shape and Texture Features"},{"paperId":"79ad463104c7b7afeab11c2046fe7c18d5108ac6","externalIds":{"DOI":"10.1080/10131750485310161","CorpusId":218497666},"title":"Pattern"},{"paperId":"eae2e0fa72e898c289365c0af16daf57a7a6cf40","externalIds":{"MAG":"2133665775","DBLP":"journals/tip/WangBSS04","DOI":"10.1109/TIP.2003.819861","CorpusId":207761262,"PubMed":"15376593"},"title":"Image quality assessment: from error visibility to structural similarity"},{"paperId":"6e42d48098a3fe5434a78e4249e99f8f28e9241f","externalIds":{"MAG":"2063366997","DBLP":"conf/siggraph/LevoyH96","DOI":"10.1145/237170.237199","CorpusId":1363510},"title":"Light field rendering"},{"paperId":"2a737fbad8dd29730313c89ae1123efeab48786d","externalIds":{"MAG":"2294985758","DBLP":"conf/siggraph/GortlerGSC96","DOI":"10.1145/237170.237200","CorpusId":2036193},"title":"The lumigraph"},{"paperId":"d92f735b0773b4e697e7e72798eccae2f647acd6","externalIds":{"MAG":"2229412420","DBLP":"conf/siggraph/LorensenC87","DOI":"10.1145/37401.37422","CorpusId":15545924},"title":"Marching cubes: A high resolution 3D surface construction algorithm"},{"paperId":"1daaace5526aeec3c2e099a5f2431ba48a2a43c0","externalIds":{"MAG":"2026058811","DBLP":"conf/siggraph/KajiyaH84","DOI":"10.1145/800031.808594","CorpusId":6722621},"title":"Ray tracing volume densities"},{"paperId":"dd3fc94c060e3e4e7f9735e594e9061c8eaf4b64","externalIds":{"DBLP":"journals/tim/WuLTLC24","DOI":"10.1109/TIM.2024.3378264","CorpusId":268526133},"title":"KN-SLAM: Keypoints and Neural Implicit Encoding SLAM"},{"paperId":"f6df6c5255110519a62812db4d940e085b92b79b","externalIds":{"CorpusId":265040090},"title":"NeRF: Neural Radiance Field in 3D Vision, Introduction and Review"},{"paperId":"1cf3210b0ba92e8e12bb0b05193e35333ce8589b","externalIds":{"CorpusId":265038004},"title":"Structured Local Radiance Fields for Human Avatar Modeling"},{"paperId":"435addc4fccf1cf43476d79fd60d4adfc3b8e5aa","externalIds":{"CorpusId":265040201},"title":"DoubleField: Bridging the Neural Surface and Radiance Fields for High-ﬁdelity Human Reconstruction and Rendering"},{"paperId":"b62ee8223006c9b78db6a000078308dc83767a1a","externalIds":{"DBLP":"conf/corl/KerrFHATIKG22","CorpusId":257432841},"title":"Evo-NeRF: Evolving NeRF for Sequential Robot Grasping of Transparent Objects"},{"paperId":"b1b8158f16c4cab16d62349102c201baea920869","externalIds":{"DBLP":"conf/eccv/ChenLHWP22","DOI":"10.1007/978-3-031-19790-1_20","CorpusId":253120829},"title":"GeoAug: Data Augmentation for Few-Shot NeRF with Geometry Constraints"},{"paperId":"6caf3307096a15832ace34a0d54cd28413503f8b","externalIds":{"DBLP":"journals/corr/abs-2102-07064","CorpusId":231924858},"title":"NeRF-: Neural Radiance Fields Without Known Camera Parameters"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"52d2a6110e3bc2215d0347a04c421fb094044557","externalIds":{"MAG":"2955307700","CorpusId":29683894},"title":"- LEVEL ACCURACY WITH 50 X FEWER PARAMETERS AND < 0 . 5 MB MODEL SIZE"},{"paperId":"6b9311d0447c784174d52da38c6f802bf42c47b3","externalIds":{"CorpusId":267812988},"title":"OptiX TM : A General Purpose Ray Tracing Engine"},{"paperId":"b99e7d8237e2116f81bf65acc7ca631cb86581c5","externalIds":{"MAG":"1479709852","CorpusId":54145653},"title":"Camera calibration toolbox for matlab"},{"paperId":"5410086e3c7cb4cc48f73f75e843b4109befe092","externalIds":{"CorpusId":282134003},"title":"Representing Scenes as Neural Radiance Fields for View Synthesis"}]}