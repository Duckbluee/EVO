{"abstract":"In this paper, we study personalized federated 001 learning for text classification with Pretrained 002 Language Models (PLMs). We identify two 003 challenges in efficiently leveraging PLMs for 004 personalized federated learning: 1) Communi-005 cation. PLMs are usually large in size, e.g. , 006 with hundreds of millions of parameters, induc-007 ing huge communication cost in a federated 008 setting. 2) Local Training. Training with PLMs 009 generally requires back-propagation, during 010 which memory consumption can be several 011 times that of the forward-propagation. This 012 may not be affordable when the PLMs are 013 trained locally on the clients, since the clients 014 may be resource constrained, e.g. , mobile de-015 vices with limited access to memory resources. 016 Additionally, the PLMs can be provided as con-017 cealed APIs, for which the back-propagation 018 operations may not be available. For the first 019 challenge, we adopt prompt tuning for PLMs 020 that only train with the prompt parameters, 021 while the pretrained parameters are frozen. 022 We further propose a compression method for 023 the learned prompts to reduce communication 024 cost. For the second challenge, we propose 025 a gradient-free approach based on discrete lo-026 cal search with natural language tokens, cir-027 cumventing gradient computation with back-028 propagation, while also reducing the communi-029 cation cost. Experiments on multiple datasets 030 demonstrates the effectiveness of our method. 031"}