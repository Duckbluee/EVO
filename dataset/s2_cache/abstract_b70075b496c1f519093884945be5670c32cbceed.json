{"abstract":"Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively predict low-level control actions for robots without task-specific fine-tuning. We utilize LLMs as a controller, diverging from the conventional approach of employing them primarily as planners. Simulation experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can function as low-level feedback controllers for dynamic motion control, even in high-dimensional robotic systems. The project website and source code can be found at: prompt2walk.github.io."}