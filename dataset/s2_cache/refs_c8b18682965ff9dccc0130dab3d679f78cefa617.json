{"references":[{"paperId":"dd3268b6fb685b7e31ca576f2a629467da90d662","externalIds":{"ArXiv":"2406.11409","DBLP":"journals/corr/abs-2406-11409","DOI":"10.48550/arXiv.2406.11409","CorpusId":270560319},"title":"CodeGemma: Open Code Models Based on Gemma"},{"paperId":"abdceff7d7983cdede9a5aabe6a476d4c72e41a3","externalIds":{"ArXiv":"2404.14219","DBLP":"journals/corr/abs-2404-14219","DOI":"10.48550/arXiv.2404.14219","CorpusId":269293048},"title":"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"},{"paperId":"b617b4f744bcb077a94328392fc8c630e17e4af0","externalIds":{"DBLP":"conf/icse/Du0WWL0FS0L24","DOI":"10.1145/3597503.3639219","CorpusId":269128474},"title":"Evaluating Large Language Models in Class-Level Code Generation"},{"paperId":"aa51dece70f90faa0cb49d8b108c8a382ff15489","externalIds":{"ArXiv":"2404.05520","DBLP":"conf/icse/ParasaramYYFQZB25","DOI":"10.1109/ICSE55347.2025.00162","CorpusId":269005612},"title":"The Fact Selection Problem in LLM-Based Program Repair"},{"paperId":"3940c3071e343e00d0a1d8c129854eee9430e3fb","externalIds":{"DBLP":"journals/corr/abs-2404-05019","ArXiv":"2404.05019","DOI":"10.48550/arXiv.2404.05019","CorpusId":269004450},"title":"Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts"},{"paperId":"c5cb68ac59f98fafc7cb96b86fca27e662e0cba8","externalIds":{"DBLP":"journals/corr/abs-2404-02183","ArXiv":"2404.02183","DOI":"10.48550/arXiv.2404.02183","CorpusId":268875891},"title":"Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization"},{"paperId":"a6b624ada5db7bfc7e074eff35e246c08a940576","externalIds":{"ArXiv":"2404.01226","DBLP":"journals/corr/abs-2404-01226","DOI":"10.48550/arXiv.2404.01226","CorpusId":268819785},"title":"Stable Code Technical Report"},{"paperId":"f3c339ab479cbd4782807bf47254961bc60bf293","externalIds":{"ArXiv":"2404.00599","DBLP":"journals/corr/abs-2404-00599","DOI":"10.48550/arXiv.2404.00599","CorpusId":268819731},"title":"EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories"},{"paperId":"eabd317bf33a636b099a38f4b49aecca97202661","externalIds":{"DBLP":"conf/coling/0001KSK0KP25","ArXiv":"2403.19270","DOI":"10.48550/arXiv.2403.19270","CorpusId":268732857},"title":"sDPO: Don't Use Your Data All at Once"},{"paperId":"9aa6a885754a27fe42a87e4dfaed87d618fd8518","externalIds":{"DBLP":"conf/acl/Bi0W0GLZS0S24","ArXiv":"2403.16792","DOI":"10.48550/arXiv.2403.16792","CorpusId":268680448},"title":"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback"},{"paperId":"030522cc114b93ec6cec078697caf241c407c4ce","externalIds":{"ArXiv":"2403.07506","DBLP":"journals/corr/abs-2403-07506","DOI":"10.48550/arXiv.2403.07506","CorpusId":268364103},"title":"Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code"},{"paperId":"cf841544a9cb18f9fee3ed632862e08f6667815e","externalIds":{"ArXiv":"2403.03894","DBLP":"journals/corr/abs-2403-03894","DOI":"10.48550/arXiv.2403.03894","CorpusId":268253688},"title":"IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators"},{"paperId":"18e7ab056c16928d8f9539509a4b366889106d97","externalIds":{"DBLP":"journals/corr/abs-2402-19173","ArXiv":"2402.19173","DOI":"10.48550/arXiv.2402.19173","CorpusId":268063676},"title":"StarCoder 2 and The Stack v2: The Next Generation"},{"paperId":"c6c584ea3983627329eebff7c78951938b227351","externalIds":{"ArXiv":"2402.16906","DBLP":"conf/acl/Zhong0S24","DOI":"10.18653/v1/2024.findings-acl.49","CorpusId":268032812},"title":"Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step by Step"},{"paperId":"5eac2a40422a7085cb6f03285ad08210b6f6744b","externalIds":{"ArXiv":"2402.14658","DBLP":"journals/corr/abs-2402-14658","DOI":"10.48550/arXiv.2402.14658","CorpusId":267782452},"title":"OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement"},{"paperId":"0874581dabb0af63971fa3026779d4e1e4ea5f61","externalIds":{"ArXiv":"2402.12317","DBLP":"conf/emnlp/SuJLWSLL024","DOI":"10.18653/v1/2024.findings-emnlp.143","CorpusId":267750919},"title":"EvoR: Evolving Retrieval for Code Generation"},{"paperId":"34efdeaf0a78d6906b4da7335afc7182df11f639","externalIds":{"DBLP":"journals/corr/abs-2402-09739","ArXiv":"2402.09739","DOI":"10.48550/arXiv.2402.09739","CorpusId":267681974},"title":"QuRating: Selecting High-Quality Data for Training Language Models"},{"paperId":"08e84c939b88fc50aaa74ef76e202e61a1ad940b","externalIds":{"ArXiv":"2402.01391","DBLP":"journals/corr/abs-2402-01391","DOI":"10.48550/arXiv.2402.01391","CorpusId":267406244},"title":"StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback"},{"paperId":"78fbb6e7a1c568a04e8c935aa9909d0c942ea5f6","externalIds":{"DBLP":"journals/corr/abs-2402-01030","ArXiv":"2402.01030","DOI":"10.48550/arXiv.2402.01030","CorpusId":267406155},"title":"Executable Code Actions Elicit Better LLM Agents"},{"paperId":"1f2a20a6efaf83214861dddae4a38a83ae18fe32","externalIds":{"ArXiv":"2401.14196","DBLP":"journals/corr/abs-2401-14196","DOI":"10.48550/arXiv.2401.14196","CorpusId":267211867},"title":"DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"},{"paperId":"e8ee8dc3a806495c6084389323ca09aca3238836","externalIds":{"ArXiv":"2401.10314","CorpusId":267061180},"title":"LangProp: A code optimization framework using Large Language Models applied to driving"},{"paperId":"fef0393e997ec51b184e39c712be63197d99fd46","externalIds":{"ArXiv":"2401.08406","DBLP":"journals/corr/abs-2401-08406","DOI":"10.48550/arXiv.2401.08406","CorpusId":267027552},"title":"RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture"},{"paperId":"1566d96346927ad4dced85de4d55356f6aee6fb6","externalIds":{"DBLP":"journals/corr/abs-2401-08500","ArXiv":"2401.08500","DOI":"10.48550/arXiv.2401.08500","CorpusId":267028402},"title":"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering"},{"paperId":"9b7324a56ecb5600a9701c34028af2e5953d7275","externalIds":{"ArXiv":"2401.06391","DBLP":"journals/tosem/WangZFLSLP25","DOI":"10.1145/3714462","CorpusId":266977002},"title":"Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation"},{"paperId":"fbd1b4f09b19bd23c16b54525347c6643bb06322","externalIds":{"DBLP":"conf/icml/GongEC24","ArXiv":"2401.03003","DOI":"10.48550/arXiv.2401.03003","CorpusId":266844632},"title":"AST-T5: Structure-Aware Pretraining for Code Generation and Understanding"},{"paperId":"8554b7c4ec2466326f5bd55335082edd83183f94","externalIds":{"ArXiv":"2401.00788","DBLP":"journals/corr/abs-2401-00788","DOI":"10.48550/arXiv.2401.00788","CorpusId":266693763},"title":"Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models"},{"paperId":"ab7d320cbae173aef86c31faa087780cba44551f","externalIds":{"ArXiv":"2312.15166","DBLP":"conf/naacl/KimKPLSKKKLKAYLPGCLK24","DOI":"10.48550/arXiv.2312.15166","CorpusId":266550918},"title":"SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling"},{"paperId":"15a764731c3ecd4e43cdefbed717527f5e0b7cc8","externalIds":{"DBLP":"conf/icse/Al-KaswanID24","ArXiv":"2312.11658","DOI":"10.1145/3597503.3639133","CorpusId":266362863},"title":"Traces of Memorisation in Large Language Models for Code"},{"paperId":"46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5","externalIds":{"ArXiv":"2312.10997","DBLP":"journals/corr/abs-2312-10997","CorpusId":266359151},"title":"Retrieval-Augmented Generation for Large Language Models: A Survey"},{"paperId":"b512451d431df9e411bea4c99f7135d010275445","externalIds":{"DBLP":"journals/corr/abs-2312-05934","ArXiv":"2312.05934","ACL":"2024.emnlp-main.15","DOI":"10.48550/arXiv.2312.05934","CorpusId":266162497},"title":"Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs"},{"paperId":"392e5714da57d522040c6d4f394fee1874e66a16","externalIds":{"DBLP":"conf/apsec/ShirafujiOSMW23","ArXiv":"2311.11690","DOI":"10.1109/APSEC60848.2023.00025","CorpusId":265295529},"title":"Refactoring Programs Using Large Language Models with Few-Shot Examples"},{"paperId":"f3e83c544a001f72ebd0e8131368cbf52070ab2b","externalIds":{"ArXiv":"2311.07989","CorpusId":265157902},"title":"Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code"},{"paperId":"0a27dde07d28ca8d92ed46cecc71585e1c9693f2","externalIds":{"DBLP":"journals/corr/abs-2311-02303","ArXiv":"2311.02303","DOI":"10.48550/arXiv.2311.02303","CorpusId":265033510},"title":"MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"},{"paperId":"1bfe2a9a40a5f34c5c6b99c182d37a6e93f95aa9","externalIds":{"DBLP":"journals/corr/abs-2310-17680","ArXiv":"2310.17680","DOI":"10.48550/arXiv.2310.17680","CorpusId":264555385},"title":"CodeFusion: A Pre-trained Diffusion Model for Code Generation"},{"paperId":"42016f91e5b1da63174d45acb96bc89b64aa124d","externalIds":{"DBLP":"journals/csur/WangZLZCL25","ArXiv":"2310.16218","DOI":"10.1145/3698590","CorpusId":264487359},"title":"Knowledge Editing for Large Language Models: A Survey"},{"paperId":"f1bd7ea3a63b78a60b5d90d91fdb4a1d7ac0de8e","externalIds":{"DBLP":"journals/corr/abs-2310-11248","ArXiv":"2310.11248","DOI":"10.48550/arXiv.2310.11248","CorpusId":264172238},"title":"CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion"},{"paperId":"94a5f96308729e31c1ffbc0f0618db87795092fe","externalIds":{"DBLP":"journals/corr/abs-2310-06770","ArXiv":"2310.06770","DOI":"10.48550/arXiv.2310.06770","CorpusId":263829697},"title":"SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"},{"paperId":"700bd9681f1b9e9e2212e10415d27b11c7e6836b","externalIds":{"ArXiv":"2310.04406","DBLP":"journals/corr/abs-2310-04406","DOI":"10.48550/arXiv.2310.04406","CorpusId":263829963},"title":"Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models"},{"paperId":"0e0e706e13f160e74cac9556f28ab9a358c148d2","externalIds":{"DBLP":"journals/corr/abs-2310-03693","ArXiv":"2310.03693","DOI":"10.48550/arXiv.2310.03693","CorpusId":263671523},"title":"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"},{"paperId":"12db3efff4cc9e16822dd64bb1cad66f3f034f3b","externalIds":{"DBLP":"journals/tacl/NiYZRFSYLYXJZRCC24","ArXiv":"2309.17446","DOI":"10.1162/tacl_a_00705","CorpusId":263310373},"title":"L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models"},{"paperId":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","externalIds":{"DBLP":"journals/corr/abs-2309-16609","ArXiv":"2309.16609","DOI":"10.48550/arXiv.2309.16609","CorpusId":263134555},"title":"Qwen Technical Report"},{"paperId":"f81a1b4510631d14b5b565c4701ee056f8d5c72f","externalIds":{"DBLP":"journals/corr/abs-2309-12499","ArXiv":"2309.12499","DOI":"10.1145/3643757","CorpusId":262217135},"title":"CodePlan: Repository-Level Coding using LLMs and Planning"},{"paperId":"0c72450890a54b68d63baa99376131fda8f06cf9","externalIds":{"ArXiv":"2309.07864","DBLP":"journals/corr/abs-2309-07864","DOI":"10.48550/arXiv.2309.07864","CorpusId":261817592},"title":"The Rise and Potential of Large Language Model Based Agents: A Survey"},{"paperId":"e26888285436bc7998e5c95102a9beb60144be5e","externalIds":{"DBLP":"journals/corr/abs-2309-05463","ArXiv":"2309.05463","DOI":"10.48550/arXiv.2309.05463","CorpusId":261696657},"title":"Textbooks Are All You Need II: phi-1.5 technical report"},{"paperId":"600ff4c4ae9fc506c86673c5ecce4fa90803e987","externalIds":{"DBLP":"conf/icml/0001PMMFLBHCRP24","ArXiv":"2309.00267","CorpusId":261493811},"title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"},{"paperId":"0b0debb710366cdff461938c80763eace1651af6","externalIds":{"DBLP":"journals/corr/abs-2308-12950","ArXiv":"2308.12950","DOI":"10.48550/arXiv.2308.12950","CorpusId":261100919},"title":"Code Llama: Open Foundation Models for Code"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","externalIds":{"DBLP":"journals/fcsc/WangMFZYZCTCLZWW24","ArXiv":"2308.11432","DOI":"10.1007/s11704-024-40231-1","CorpusId":261064713},"title":"A survey on large language model based autonomous agents"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","externalIds":{"ArXiv":"2308.10792","DBLP":"journals/corr/abs-2308-10792","DOI":"10.1145/3777411","CorpusId":261049152},"title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"703035b483c181953de1b55b5fd59cd4cd4cf211","externalIds":{"DBLP":"journals/corr/abs-2308-00352","ArXiv":"2308.00352","DOI":"10.48550/arXiv.2308.00352","CorpusId":260351380},"title":"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework"},{"paperId":"e0ca43a635d35fd0414ee76ca1e7c287715f5b00","externalIds":{"ArXiv":"2307.14936","DBLP":"journals/corr/abs-2307-14936","DOI":"10.48550/arXiv.2307.14936","CorpusId":260202985},"title":"PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"a669ea57529f4db630043c8c75d8f840c485d24d","externalIds":{"DBLP":"journals/tmlr/LiuZXF00Y23","ArXiv":"2307.04349","DOI":"10.48550/arXiv.2307.04349","CorpusId":259501019},"title":"RLTF: Reinforcement Learning from Unit Test Feedback"},{"paperId":"2a09ebbfcca1a6994eeb472cd4159f5f3858dbf9","externalIds":{"DBLP":"conf/icml/GuoXD0M23","ArXiv":"2306.14893","DOI":"10.48550/arXiv.2306.14893","CorpusId":259262301},"title":"LongCoder: A Long-Range Pre-trained Language Model for Code Completion"},{"paperId":"2922768fd451ecdb45f48c1a83eb57f54a91221b","externalIds":{"DBLP":"journals/corr/abs-2306-11644","ArXiv":"2306.11644","CorpusId":259203998},"title":"Textbooks Are All You Need"},{"paperId":"2bbb16eb8e85c64608af9712724951f070e01910","externalIds":{"ArXiv":"2306.10998","DBLP":"journals/corr/abs-2306-10998","DOI":"10.48550/arXiv.2306.10998","CorpusId":259203440},"title":"RepoFusion: Training Code Models to Understand Your Repository"},{"paperId":"454c8fef2957aa2fb13eb2c7a454393a2ee83805","externalIds":{"DBLP":"journals/corr/abs-2306-08568","ArXiv":"2306.08568","CorpusId":259164815},"title":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"a4929de687f3c6937dabbf733258af635781d3c4","externalIds":{"DBLP":"conf/acl/BabeNZGFA24","ArXiv":"2306.04556","DOI":"10.48550/arXiv.2306.04556","CorpusId":259095478},"title":"StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code"},{"paperId":"f97413a497d47c739d41d237917e6566154647b4","externalIds":{"ArXiv":"2306.03091","DBLP":"conf/iclr/0003XM24","DOI":"10.48550/arXiv.2306.03091","CorpusId":259075246},"title":"RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"5dbffedcabe3fa43060ebbe2b1789500edfd871f","externalIds":{"DBLP":"conf/emnlp/HaoGMHWWH23","ArXiv":"2305.14992","DOI":"10.48550/arXiv.2305.14992","CorpusId":258865812},"title":"Reasoning with Language Model is Planning with World Model"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"5deaacd4c1a3ae6691a7ae9f4442bc8e3c09b6b2","externalIds":{"ArXiv":"2305.09235","DBLP":"journals/corr/abs-2305-09235","DOI":"10.48550/arXiv.2305.09235","CorpusId":258714748},"title":"Synthetic data, real errors: how (not) to publish and use synthetic data"},{"paperId":"9ada8fa11b1cdece31f253acae50b62df8d5f823","externalIds":{"DBLP":"journals/corr/abs-2305-07922","ArXiv":"2305.07922","DOI":"10.48550/arXiv.2305.07922","CorpusId":258685677},"title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation"},{"paperId":"886e0962479ec6dac563666399ca4c96a468fcaa","externalIds":{"ArXiv":"2305.02309","DBLP":"journals/corr/abs-2305-02309","DOI":"10.48550/arXiv.2305.02309","CorpusId":258461229},"title":"CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"},{"paperId":"b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a","externalIds":{"ArXiv":"2305.01210","DBLP":"journals/corr/abs-2305-01210","CorpusId":258437095},"title":"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation"},{"paperId":"1c44e9ec5d2ed0b486f22fcb5a54923096b3591b","externalIds":{"DBLP":"conf/icse/ChenA23","DOI":"10.1109/ICSE48619.2023.00198","CorpusId":259859878},"title":"Duetcs: Code Style Transfer through Generation and Retrieval"},{"paperId":"19ea368b7f88279899c40813a797dda7adc50c07","externalIds":{"DBLP":"journals/corr/abs-2305-00909","ArXiv":"2305.00909","DOI":"10.48550/arXiv.2305.00909","CorpusId":258426713},"title":"Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation"},{"paperId":"fbe90d2864ffdbefd1fc0a7c6f65ac10452052f2","externalIds":{"ArXiv":"2304.14317","ACL":"2024.findings-eacl.148","DBLP":"conf/eacl/Zhuo24","DOI":"10.18653/v1/2024.findings-eacl.148","CorpusId":258352761},"title":"ICE-Score: Instructing Large Language Models to Evaluate Code"},{"paperId":"08a80cb34d785258c770acecd302ab41ead46eed","externalIds":{"DBLP":"conf/iclr/XuSZG0FTLJ24","ArXiv":"2304.12244","CorpusId":258298159},"title":"WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"},{"paperId":"0ffd57884d7957f6b5634b9fa24843dc3759668f","externalIds":{"DBLP":"conf/chi/HamalainenTK23","DOI":"10.1145/3544548.3580688","CorpusId":258218228},"title":"Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study"},{"paperId":"ae736662f64d56f3ab1894fbd9c45f8f37251843","externalIds":{"ArXiv":"2304.07327","DBLP":"conf/nips/KopfKRATSBNSNES23","DOI":"10.48550/arXiv.2304.07327","CorpusId":258179434},"title":"OpenAssistant Conversations - Democratizing Large Language Model Alignment"},{"paperId":"9e3c493fb09dcd61bb05e8c5659f23327b7b6340","externalIds":{"ArXiv":"2304.05128","DBLP":"journals/corr/abs-2304-05128","DOI":"10.48550/arXiv.2304.05128","CorpusId":258059885},"title":"Teaching Large Language Models to Self-Debug"},{"paperId":"748698bd4387afd08594e0dc8150c2afa210d9ae","externalIds":{"DBLP":"conf/nips/YuanYTWHH23","ArXiv":"2304.05302","DOI":"10.48550/arXiv.2304.05302","CorpusId":258059818},"title":"RRHF: Rank Responses to Align Language Models with Human Feedback without tears"},{"paperId":"bafe023fb072045dc0cd50316382a61c8dcb9fae","externalIds":{"DBLP":"conf/kdd/ZhengXZDWXSW0LS23","ArXiv":"2303.17568","DOI":"10.1145/3580305.3599790","CorpusId":257834177},"title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"},{"paperId":"3aaf6a2cbad5850ad81ab5c163599cb3d523436f","externalIds":{"DBLP":"journals/corr/abs-2303-17651","ArXiv":"2303.17651","DOI":"10.48550/arXiv.2303.17651","CorpusId":257900871},"title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"af5c7848417882012203ac21399977ebda695a2b","externalIds":{"DBLP":"conf/emnlp/ZhangCZKLZMLC23","ArXiv":"2303.12570","DOI":"10.48550/arXiv.2303.12570","CorpusId":257663528},"title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"0671fd553dd670a4e820553a974bc48040ba0819","externalIds":{"DBLP":"conf/nips/ShinnCGNY23","ArXiv":"2303.11366","CorpusId":258833055},"title":"Reflexion: language agents with verbal reinforcement learning"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"59fe7cb560651281cfc5db6b8940da0e3ba9dea6","externalIds":{"DBLP":"journals/corr/abs-2302-08468","ArXiv":"2302.08468","DOI":"10.48550/arXiv.2302.08468","CorpusId":256900680},"title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"74013b7cfa0fc524803350fca51341004565eb22","externalIds":{"DBLP":"conf/nips/XieS0L23","ArXiv":"2302.03169","DOI":"10.48550/arXiv.2302.03169","CorpusId":256627727},"title":"Data Selection for Language Models via Importance Resampling"},{"paperId":"30c0cdc414f68211d5d0514df027cec22e005174","externalIds":{"DBLP":"conf/emnlp/Dong0DZMLXX0C0S24","ArXiv":"2301.00234","ACL":"2024.emnlp-main.64","DOI":"10.18653/v1/2024.emnlp-main.64","CorpusId":255372865},"title":"A Survey on In-context Learning"},{"paperId":"e965e93e76a9e6c4e4863d145b5c007b540d575d","externalIds":{"ArXiv":"2212.12017","DBLP":"journals/corr/abs-2212-12017","CorpusId":255096269},"title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"},{"paperId":"7b00c4deb35471194bbbbd338191165d53167fe5","externalIds":{"DBLP":"journals/corr/abs-2212-10007","ArXiv":"2212.10007","ACL":"2024.lrec-main.305","DOI":"10.48550/arXiv.2212.10007","CorpusId":254877371},"title":"CoCoMIC: Code Completion by Jointly Modeling In-file and Cross-file Context"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","externalIds":{"ArXiv":"2212.10403","DBLP":"conf/acl/0009C23","DOI":"10.48550/arXiv.2212.10403","CorpusId":254877753},"title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","externalIds":{"DBLP":"conf/acl/0002LQYWSKTRBNR23","ArXiv":"2212.10264","ACL":"2023.acl-long.773","DOI":"10.48550/arXiv.2212.10264","CorpusId":254877229},"title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"1bed34f2c23b97fd18de359cf62cd92b3ba612c3","externalIds":{"DBLP":"journals/corr/abs-2212-10481","ArXiv":"2212.10481","DOI":"10.48550/arXiv.2212.10481","CorpusId":254877069},"title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"4f939f0751e5484f54089f6a97598e39afdcb3b5","externalIds":{"ArXiv":"2212.09420","ACL":"2023.acl-long.411","DBLP":"conf/acl/ZanCZLWGWL23","DOI":"10.18653/v1/2023.acl-long.411","CorpusId":258557362},"title":"Large Language Models Meet NL2Code: A Survey"},{"paperId":"3936fd3c6187f606c6e4e2e20b196dbc41cc4654","externalIds":{"DBLP":"journals/corr/abs-2212-08073","ArXiv":"2212.08073","DOI":"10.48550/arXiv.2212.08073","CorpusId":254823489},"title":"Constitutional AI: Harmlessness from AI Feedback"},{"paperId":"e1b732e02cd6f41e4e1eb793ec4b356cee2587f1","externalIds":{"DBLP":"journals/corr/abs-2212-06742","ArXiv":"2212.06742","DOI":"10.48550/arXiv.2212.06742","CorpusId":254591305},"title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"e4f8cb7bb933d95bec8d6eaeeb9d1815ed095f21","externalIds":{"ArXiv":"2212.11140","DBLP":"conf/date/ThakurAFPTKDG23","DOI":"10.23919/DATE56975.2023.10137086","CorpusId":254926675},"title":"Benchmarking Large Language Models for Automated Verilog RTL Code Generation"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","externalIds":{"DBLP":"journals/corr/abs-2211-15533","ArXiv":"2211.15533","DOI":"10.48550/arXiv.2211.15533","CorpusId":254044610},"title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","externalIds":{"DBLP":"journals/corr/abs-2211-11501","ArXiv":"2211.11501","DOI":"10.48550/arXiv.2211.11501","CorpusId":253734939},"title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","externalIds":{"ArXiv":"2211.09374","ACL":"2022.dash-1.5","DBLP":"journals/corr/abs-2211-09374","DOI":"10.48550/arXiv.2211.09374","CorpusId":253581341},"title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","externalIds":{"DBLP":"journals/corr/abs-2211-05100","ArXiv":"2211.05100","DOI":"10.48550/arXiv.2211.05100","CorpusId":253420279},"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","externalIds":{"DBLP":"conf/iclr/AthiwaratkunGWL23","ArXiv":"2210.14868","DOI":"10.48550/arXiv.2210.14868","CorpusId":253116642},"title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"b070174f9d955ec52e6852314dde0cb64d02ba28","externalIds":{"ArXiv":"2210.14306","DBLP":"journals/corr/abs-2210-14306","DOI":"10.48550/arXiv.2210.14306","CorpusId":253117056},"title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","externalIds":{"DBLP":"conf/iclr/YaoZYDSN023","ArXiv":"2210.03629","CorpusId":252762395},"title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","externalIds":{"DBLP":"journals/corr/abs-2207-11280","ArXiv":"2207.11280","DOI":"10.48550/arXiv.2207.11280","CorpusId":251040785},"title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","externalIds":{"DBLP":"journals/corr/abs-2207-10397","ArXiv":"2207.10397","DOI":"10.48550/arXiv.2207.10397","CorpusId":250920542},"title":"CodeT: Code Generation with Generated Tests"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","externalIds":{"ArXiv":"2207.05987","DBLP":"conf/iclr/Zhou0XJN23","CorpusId":252734952},"title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"fb9a88ac3c4795e3fcf5de926516c59463b42e67","externalIds":{"DBLP":"journals/pacmpl/BarkeJP23","ArXiv":"2206.15000","DOI":"10.1145/3586030","CorpusId":250144196},"title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"1aa206426a20b0b549cda5068b90b8da353b3434","externalIds":{"DBLP":"journals/corr/abs-2206-12839","ArXiv":"2206.12839","DOI":"10.48550/arXiv.2206.12839","CorpusId":250072448},"title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","externalIds":{"DBLP":"conf/ijcai/ZanCYLKGWCL22","ArXiv":"2206.06888","DOI":"10.48550/arXiv.2206.06888","CorpusId":249642442},"title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"7cdaa08890895e1ad92afb5fad429690ad7b1dac","externalIds":{"DBLP":"conf/nips/LiuTMMHBR22","ArXiv":"2205.05638","DOI":"10.48550/arXiv.2205.05638","CorpusId":248693283},"title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"},{"paperId":"65fd4085a8498928e675d4cb50c0a31092f410fb","externalIds":{"ArXiv":"2204.09191","DBLP":"conf/icse/Li0WW0NW22","DOI":"10.1145/3510003.3510217","CorpusId":248266620},"title":"Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","externalIds":{"ACL":"2022.bigscience-1.9","DBLP":"journals/corr/abs-2204-06745","ArXiv":"2204.06745","DOI":"10.48550/arXiv.2204.06745","CorpusId":248177957},"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","externalIds":{"ArXiv":"2204.05999","DBLP":"conf/iclr/FriedAL0WSZYZL23","DOI":"10.48550/arXiv.2204.05999","CorpusId":248157108},"title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"9a6730534295335247eebdec59b7decdeb83d59a","externalIds":{"DBLP":"journals/corr/abs-2204-09653","ArXiv":"2204.09653","DOI":"10.1145/3524610.3527917","CorpusId":248266381},"title":"On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","externalIds":{"DBLP":"conf/iclr/0002WSLCNCZ23","ArXiv":"2203.11171","CorpusId":247595263},"title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","externalIds":{"DBLP":"conf/acl/LuDHGHS22","ArXiv":"2203.07722","ACL":"2022.acl-long.431","DOI":"10.48550/arXiv.2203.07722","CorpusId":247450969},"title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"7a1069dafaeb484e22f2473d5545f1e45ce30656","externalIds":{"DBLP":"journals/corr/abs-2203-05132","ArXiv":"2203.05132","ACL":"2022.findings-acl.2","DOI":"10.48550/arXiv.2203.05132","CorpusId":247362946},"title":"Compilable Neural Code Generation with Compiler Feedback"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","externalIds":{"DBLP":"journals/corr/abs-2203-03850","ACL":"2022.acl-long.499","ArXiv":"2203.03850","DOI":"10.48550/arXiv.2203.03850","CorpusId":247315559},"title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","externalIds":{"DBLP":"journals/corr/abs-2202-13169","ArXiv":"2202.13169","DOI":"10.1145/3520312.3534862","CorpusId":247158549},"title":"A systematic evaluation of large language models of code"},{"paperId":"23c265ba884b92ecbd9d18641078d964697e4590","externalIds":{"ArXiv":"2202.04538","DBLP":"journals/corr/abs-2202-04538","CorpusId":246680398},"title":"Generating Training Data with Language Models: Towards Zero-Shot Language Understanding"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","externalIds":{"ArXiv":"2203.07814","DBLP":"journals/corr/abs-2203-07814","DOI":"10.1126/science.abq1158","CorpusId":246527904,"PubMed":"36480631"},"title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","externalIds":{"DBLP":"journals/corr/abs-2201-12901","ArXiv":"2201.12901","CorpusId":246430316},"title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","externalIds":{"ArXiv":"2110.08207","DBLP":"journals/corr/abs-2110-08207","CorpusId":239009562},"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"ce828f9986b196308a3e40b1de58af1e8e68d728","externalIds":{"DBLP":"journals/corr/abs-2110-03215","ArXiv":"2110.03215","CorpusId":238419458},"title":"Towards Continual Knowledge Learning of Language Models"},{"paperId":"1561cdeb6dd7000f3810aabb9e6828f14799464c","externalIds":{"ArXiv":"2109.15102","DBLP":"journals/corr/abs-2109-15102","DOI":"10.1109/ICCV48922.2021.00366","CorpusId":238226716},"title":"Fake it till you make it: face analysis in the wild using synthetic data alone"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","externalIds":{"DBLP":"conf/emnlp/0034WJH21","ACL":"2021.emnlp-main.685","ArXiv":"2109.00859","DOI":"10.18653/v1/2021.emnlp-main.685","CorpusId":237386541},"title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","externalIds":{"ArXiv":"2108.12409","DBLP":"journals/corr/abs-2108-12409","CorpusId":237347130},"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","externalIds":{"DBLP":"conf/emnlp/ParvezACRC21","ArXiv":"2108.11601","DOI":"10.18653/v1/2021.findings-emnlp.232","CorpusId":237304122},"title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","externalIds":{"DBLP":"journals/corr/abs-2108-07732","ArXiv":"2108.07732","CorpusId":237142385},"title":"Program Synthesis with Large Language Models"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"339b2b711fb5b228d097b03ebc3e62a521779235","externalIds":{"DBLP":"conf/acl/ZakenGR22","ACL":"2022.acl-short.1","ArXiv":"2106.10199","DOI":"10.18653/v1/2022.acl-short.1","CorpusId":231672601},"title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"7ccd7de2f17bc0afad46492ac7c4ee9323fce95b","externalIds":{"ArXiv":"2105.04144","DBLP":"journals/corr/abs-2105-04144","CorpusId":234336067},"title":"Transitioning from Real to Synthetic data: Quantifying the bias in model"},{"paperId":"70c3be778323b896f5575a5770f934ee6f399441","externalIds":{"DBLP":"conf/icse/PeitekAPBS21","DOI":"10.1109/ICSE43902.2021.00056","CorpusId":231941039},"title":"Program Comprehension and Code Complexity Metrics: An fMRI Study"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","externalIds":{"DBLP":"journals/ijon/SuALPBL24","ArXiv":"2104.09864","DOI":"10.1016/j.neucom.2023.127063","CorpusId":233307138},"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","externalIds":{"MAG":"3212496002","DOI":"10.5281/ZENODO.5297715","CorpusId":245758737},"title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"560d4747ed94786cf96ef542a1a3f2b9089f0145","externalIds":{"DBLP":"journals/corr/abs-2103-09499","ArXiv":"2103.09499","DOI":"10.1609/aaai.v35i16.17650","CorpusId":232257721},"title":"Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs"},{"paperId":"c09c36470a73a4190c1d1c255c59bd7c26844801","externalIds":{"MAG":"3170092793","ACL":"2021.naacl-main.211","ArXiv":"2103.06333","DBLP":"journals/corr/abs-2103-06333","DOI":"10.18653/V1/2021.NAACL-MAIN.211","CorpusId":232185260},"title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","externalIds":{"DBLP":"conf/nips/HendrycksBKABTS21","ArXiv":"2103.03874","CorpusId":232134851},"title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"eed70db53a05fba20a30a27aac9d9a9984c436c4","externalIds":{"DBLP":"conf/icse/JiangL021","ArXiv":"2103.00073","DOI":"10.1109/ICSE43902.2021.00107","CorpusId":232076119},"title":"CURE: Code-Aware Neural Machine Translation for Automatic Program Repair"},{"paperId":"3cb7ef02ed8c1b9983e110386fa5168eb9cb992a","externalIds":{"MAG":"1545603529","DOI":"10.1142/9789814261432_0005","CorpusId":239320018},"title":"Representations"},{"paperId":"870ff1dde0c103c3d90be51880f984628e77a8d6","externalIds":{"DBLP":"conf/nips/LuGRHSBCDJTLZSZ21","ArXiv":"2102.04664","CorpusId":231855531},"title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","externalIds":{"DBLP":"journals/corr/abs-2101-00027","ArXiv":"2101.00027","CorpusId":230435736},"title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"df7d26339adf4eb0c07160947b9d2973c24911ba","externalIds":{"DBLP":"journals/corr/abs-2012-07805","MAG":"3112689365","ArXiv":"2012.07805","CorpusId":229156229},"title":"Extracting Training Data from Large Language Models"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","externalIds":{"MAG":"3091798252","DBLP":"journals/corr/abs-2010-03150","ACL":"2020.emnlp-main.728","ArXiv":"2010.03150","DOI":"10.18653/v1/2020.emnlp-main.728","CorpusId":222178041},"title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","externalIds":{"DBLP":"journals/corr/abs-2009-10297","MAG":"3089307846","ArXiv":"2009.10297","CorpusId":221836101},"title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","externalIds":{"MAG":"3086007799","ArXiv":"2009.08366","DBLP":"journals/corr/abs-2009-08366","CorpusId":221761146},"title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"f820283830d8978dd6d5accad2402c40cce165c2","externalIds":{"MAG":"3046989849","DBLP":"journals/sp/ArditoCBV20","DOI":"10.1155/2020/8840389","CorpusId":221543812},"title":"A Tool-Based Perspective on Software Code Maintainability Metrics: A Systematic Literature Review"},{"paperId":"7c0b558bf433c5aaaf774cd5d3c767bfd3dbe123","externalIds":{"ArXiv":"2006.05405","CorpusId":234487049},"title":"Retrieval-Augmented Generation for Code Summarization via Hybrid GNN"},{"paperId":"df56748cd4f52a58973b4ac52c0bf9156c5f52f0","externalIds":{"MAG":"3033638351","DBLP":"journals/corr/abs-2006-03511","ArXiv":"2006.03511","CorpusId":219401607},"title":"Unsupervised Translation of Programming Languages"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"659bf9ce7175e1ec266ff54359e2bd76e0b7ff31","externalIds":{"DBLP":"conf/nips/LewisPPPKGKLYR020","MAG":"3027879771","ArXiv":"2005.11401","CorpusId":218869575},"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"a4c6cb385a90441061218f50abe2871d5e384a2e","externalIds":{"DBLP":"conf/sigsoft/SvyatkovskiyDFS20","ArXiv":"2005.08025","MAG":"3108032709","DOI":"10.1145/3368089.3417058","CorpusId":218673683},"title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","externalIds":{"ArXiv":"2002.08155","DBLP":"conf/emnlp/FengGTDFGS0LJZ20","MAG":"3008088841","ACL":"2020.findings-emnlp.139","DOI":"10.18653/v1/2020.findings-emnlp.139","CorpusId":211171605},"title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","externalIds":{"MAG":"2973529529","DBLP":"journals/corr/abs-1909-09436","ArXiv":"1909.09436","CorpusId":202712680},"title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","externalIds":{"ArXiv":"1905.13319","MAG":"2945720633","DBLP":"journals/corr/abs-1905-13319","ACL":"N19-1245","DOI":"10.18653/v1/N19-1245","CorpusId":173188048},"title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","externalIds":{"DBLP":"journals/corr/abs-1904-09751","MAG":"2938704169","ArXiv":"1904.09751","CorpusId":127986954},"title":"The Curious Case of Neural Text Degeneration"},{"paperId":"7f9b874d77ee2d0974908448cbead7a4b0af11cd","externalIds":{"DBLP":"conf/msr/MarkovtsevLMSB19","MAG":"2955375655","ArXiv":"1904.00935","DOI":"10.1109/MSR.2019.00073","CorpusId":90260632},"title":"Style-Analyzer: Fixing Code Style Inconsistencies with Interpretable Unsupervised Algorithms"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","externalIds":{"DBLP":"conf/emnlp/YuZYYWLMLYRZR18","ACL":"D18-1425","ArXiv":"1809.08887","MAG":"2890431379","DOI":"10.18653/v1/D18-1425","CorpusId":52815560},"title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","externalIds":{"MAG":"2888925928","ACL":"D18-1192","ArXiv":"1808.09588","DBLP":"conf/emnlp/IyerKCZ18","DOI":"10.18653/v1/D18-1192","CorpusId":263797641},"title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","externalIds":{"MAG":"2964315653","DBLP":"conf/msr/YinDCVN08","ArXiv":"1805.08949","DOI":"10.1145/3196398.3196408","CorpusId":43922261},"title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","externalIds":{"MAG":"2963925437","DBLP":"journals/corr/abs-1803-02155","ACL":"N18-2074","ArXiv":"1803.02155","DOI":"10.18653/v1/N18-2074","CorpusId":3725815},"title":"Self-Attention with Relative Position Representations"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","externalIds":{"MAG":"2963982902","DBLP":"conf/nips/ChenLS18","ArXiv":"1802.03691","CorpusId":600040},"title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"510e26733aaff585d65701b9f1be7ca9d5afc586","externalIds":{"DBLP":"journals/corr/ShazeerMMDLHD17","MAG":"2952339051","ArXiv":"1701.06538","CorpusId":12462234},"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","externalIds":{"MAG":"3037932933","ArXiv":"1607.06450","DBLP":"journals/corr/BaKH16","CorpusId":8236317},"title":"Layer Normalization"},{"paperId":"eea4f7f3167e4c7e3430bdc5f57383040c58e133","externalIds":{"MAG":"2484773337","DBLP":"journals/csur/KokkinosKLV16","DOI":"10.1145/2940295","CorpusId":18574777},"title":"Survey"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"cb670415715d331af5097e46890f524ac1d82ac6","externalIds":{"MAG":"2530984502","DBLP":"conf/sp/NappaJBCD15","DOI":"10.1109/SP.2015.48","CorpusId":15715668},"title":"The Attack of the Clones: A Study of the Impact of Shared Code on Vulnerability Patching"},{"paperId":"3c902294cb3230f81c504b63afffbad41cc302a1","externalIds":{"MAG":"1512847993","ArXiv":"1409.5718","DBLP":"journals/corr/MouLJZW14","CorpusId":85467156},"title":"TBCNN: A Tree-Based Convolutional Neural Network for Programming Language Processing"},{"paperId":"7f013f172a45824d907f68481e92a22e0188ea0b","externalIds":{"MAG":"2949589607","DBLP":"conf/sigsoft/AllamanisS14","ArXiv":"1404.0417","DOI":"10.1145/2635868.2635901","CorpusId":2923536},"title":"Mining idioms from source code"},{"paperId":"d34b626663d672d42fcde50569050b35f7e43925","externalIds":{"MAG":"1986711365","DBLP":"conf/icsm/RaemaekersDV12","DOI":"10.1109/ICSM.2012.6405296","CorpusId":11783445},"title":"Measuring software library stability through historical version analysis"},{"paperId":"18b8ef71bc01b8658b4ef2c8b9a9e4e6e5c2a07b","externalIds":{"DBLP":"conf/fmcad/Gulwani10","MAG":"2187557002","DOI":"10.1145/1836089.1836091","CorpusId":3015769},"title":"Dimensions in program synthesis"},{"paperId":"1a2b8aa0ed7f24ca001508654f506ea010b18a5e","externalIds":{"DBLP":"journals/tse/BuseW10","MAG":"2159739762","DOI":"10.1109/TSE.2009.70","CorpusId":650129},"title":"Learning a Metric for Code Readability"},{"paperId":"b24df4edc61dcb50b53814baaeb54d85e3590817","externalIds":{"MAG":"2134734244","DBLP":"conf/icse/JhaGST10","DOI":"10.1145/1806799.1806833","CorpusId":6344783},"title":"Oracle-guided component-based program synthesis"},{"paperId":"fd07159f48b358e912d0bd6b81ca87710f463734","externalIds":{"MAG":"2157874452","DBLP":"journals/jmlr/CohnBG10","DOI":"10.5555/1756006.1953031","CorpusId":17181517},"title":"Inducing Tree-Substitution Grammars"},{"paperId":"3960dda299e0f8615a7db675b8e6905b375ecf8a","externalIds":{"DBLP":"conf/tacas/MouraB08","MAG":"1480909796","DOI":"10.1007/978-3-540-78800-3_24","CorpusId":15912959},"title":"Z3: An Efficient SMT Solver"},{"paperId":"77479ca664947420f85f0818953a693602db1c4e","externalIds":{"DBLP":"conf/nips/LinsteadRBLB07","MAG":"2141102925","CorpusId":1742535},"title":"Mining Internet-Scale Software Repositories"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"8d350f2d767a70d55275a17d0b3dfcc80b2e0fee","externalIds":{"MAG":"2058373514","DOI":"10.1121/1.2016299","CorpusId":121680873},"title":"Perplexityâ€”a measure of the difficulty of speech recognition tasks"},{"paperId":"d7fc8fc5510f31469ba263253ec54edd7821cf8d","externalIds":{"DBLP":"journals/corr/abs-2403-06095","DOI":"10.48550/arXiv.2403.06095","CorpusId":268359001},"title":"RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion"},{"paperId":"222f289cb96ac4dfef7849cd068af6af02233c52","externalIds":{"DBLP":"conf/naacl/PatelRBD24","ACL":"2024.naacl-long.161","DOI":"10.18653/v1/2024.naacl-long.161","CorpusId":265220768},"title":"Evaluating In-Context Learning of Libraries for Code Generation"},{"paperId":"1db56be01aeb44ca0f3fcebb45180cab1e4cd82e","externalIds":{"DBLP":"journals/corr/abs-2312-13010","DOI":"10.48550/arXiv.2312.13010","CorpusId":266374622},"title":"AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation"},{"paperId":"1a4c6856292b8c64d19a812a77f0aa6fd47cb96c","externalIds":{"DBLP":"journals/corr/abs-2308-08155","CorpusId":260925901},"title":"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework"},{"paperId":"5ef82a8c8aa50f99285f2143b57ca4e82da1af80","externalIds":{"DBLP":"journals/corr/abs-2303-10512","DOI":"10.48550/arXiv.2303.10512","CorpusId":257631760},"title":"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"},{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","externalIds":{"DBLP":"journals/corr/abs-2303-17780","DOI":"10.48550/arXiv.2303.17780","CorpusId":263897368},"title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"4cbe5288b7bedf88ea188f536214778325fda311","externalIds":{"DBLP":"journals/corr/abs-2310-02003","DOI":"10.48550/arXiv.2310.02003","CorpusId":263609227},"title":"L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation"},{"paperId":"362cae35fb65710af8230a4ee8e1aad1f275a4e7","externalIds":{"DBLP":"journals/corr/abs-2312-14187","DOI":"10.48550/arXiv.2312.14187","CorpusId":266521384},"title":"WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation"},{"paperId":"3689b7ca7b07924b6135b8a71b9f1b7937b0a3d5","externalIds":{"DBLP":"journals/corr/abs-2312-17044","DOI":"10.48550/arXiv.2312.17044","CorpusId":266573748},"title":"Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding"},{"paperId":"ac771182d1780c863954243809d1e144433919f9","externalIds":{"DBLP":"journals/corr/abs-2307-12966","DOI":"10.48550/arXiv.2307.12966","CorpusId":260356605},"title":"Aligning Large Language Models with Human: A Survey"},{"paperId":"1012d2a3281dbb40c22e25652b57fc532180f59d","externalIds":{"DBLP":"journals/corr/abs-2303-03004","DOI":"10.48550/arXiv.2303.03004","CorpusId":257365592},"title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"12b0ed898224f0ac6054e4eaa437e0e456620954","externalIds":{"DBLP":"journals/corr/abs-2306-01220","DOI":"10.48550/arXiv.2306.01220","CorpusId":280034819},"title":"Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","externalIds":{"DBLP":"journals/corr/abs-2208-08227","DOI":"10.48550/arXiv.2208.08227","CorpusId":251622387},"title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"1459ed38b154648d1375b29f39891f94d459c64c","externalIds":{"MAG":"2122845366","CorpusId":15424967},"title":"A Formalism for Dependency Grammar Based on Tree Adjoining Grammar"},{"paperId":"469c4f553ddf7b5195530d4cf10adbbcc9b18df8","externalIds":{"MAG":"1494910745","DOI":"10.1184/R1/6605324.V1","CorpusId":1995619},"title":"Evaluation Metrics For Language Models"},{"paperId":"de8ba9b01c9ab7cbabf5c33b80b7bbc618857627","externalIds":{"CorpusId":268232499},"title":"The Claude 3 Model Family: Opus, Sonnet, Haiku"}]}