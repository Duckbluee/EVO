{"references":[{"paperId":"2896f07cb07a0c519f25e7c645014c6fbfd71ba4","externalIds":{"DBLP":"journals/tpds/ChenXWLLCZ24","DOI":"10.1109/TPDS.2023.3241965","CorpusId":256595290},"title":"Synchronize Only the Immature Parameters: Communication-Efficient Federated Learning By Freezing Parameters Adaptively"},{"paperId":"02af185257ddf835d49f821f1cf2f099b774d98e","externalIds":{"DBLP":"journals/ton/LiuLXLWM24","DOI":"10.1109/TNET.2023.3329005","CorpusId":265166809},"title":"YOGA: Adaptive Layer-Wise Model Aggregation for Decentralized Federated Learning"},{"paperId":"929279fb0536bcf883457c69d0ce0eb28e7b1915","externalIds":{"DBLP":"journals/iotj/SongZLFK24","DOI":"10.1109/JIOT.2023.3324079","CorpusId":265771371},"title":"ResFed: Communication0Efficient Federated Learning With Deep Compressed Residuals"},{"paperId":"95e218a72c9942f9f60fe209f8afdbce5c43a7d1","externalIds":{"DBLP":"journals/ton/LiaoXXYWQ24","DOI":"10.1109/TNET.2023.3299851","CorpusId":261150579},"title":"Accelerating Federated Learning With Data and Model Parallelism in Edge Computing"},{"paperId":"bb17b201d9f3411707cb7374fd19bdb74719087e","externalIds":{"DBLP":"journals/tpds/WangXXJLC24","DOI":"10.1109/TPDS.2023.3334398","CorpusId":265328451},"title":"FAST: Enhancing Federated Learning Through Adaptive Data Sampling and Local Training"},{"paperId":"883b5a6cbbf3c499c8402204a657abf1e836d310","externalIds":{"DBLP":"journals/csur/YeGHSWLZW24","DOI":"10.1145/3638757","CorpusId":266561946},"title":"Deep Learning Workload Scheduling in GPU Datacenters: A Survey"},{"paperId":"30c493ed26e38786e7ddfbec279fa121566b198f","externalIds":{"DBLP":"journals/tcc/FilippiniAAG24","DOI":"10.1109/TCC.2023.3336540","CorpusId":265434101},"title":"A Stochastic Approach for Scheduling AI Training Jobs in GPU-Based Systems"},{"paperId":"10cb1af640c855c17dd9e66d34be91a92b0d3c72","externalIds":{"DBLP":"journals/tc/ShenTZZJYZL24","DOI":"10.1109/TC.2023.3315847","CorpusId":262099566},"title":"ASHL: An Adaptive Multi-Stage Distributed Deep Learning Training Scheme for Heterogeneous Environments"},{"paperId":"174f881f77cb7d7adfadb956e00193d927ac06f6","externalIds":{"DBLP":"journals/tpds/YuDL23","DOI":"10.1109/TPDS.2023.3323282","CorpusId":263848726},"title":"Communication Optimization Algorithms for Distributed Deep Learning Systems: A Survey"},{"paperId":"250dc8f7059e4a23985c06edd7b5b2ca9291a693","externalIds":{"DBLP":"journals/ijon/LiuZSCWYG24","DOI":"10.1016/j.neucom.2023.127009","CorpusId":265144366},"title":"Topologies in distributed machine learning: Comprehensive survey, recommendations and future directions"},{"paperId":"d6a1cb0abd8f082498634e5e7ee5e5fa5bf8170a","externalIds":{"DBLP":"journals/tii/WuYZLLCG23","DOI":"10.1109/TII.2023.3245673","CorpusId":256979649},"title":"To Transmit or Predict: An Efficient Industrial Data Transmission Scheme With Deep Learning and Cloud-Edge Collaboration"},{"paperId":"8fae24ba59f06b5097ed4f8d6e2f2efde144280a","externalIds":{"DBLP":"conf/cloud/ChengCYMBGX23","DOI":"10.1145/3620678.3624661","CorpusId":264907681},"title":"Towards GPU Memory Efficiency for Distributed Training at Scale"},{"paperId":"3b2ba249a942209525c2c6ef6131b0364b0aca3f","externalIds":{"DBLP":"journals/tmc/XuLXMWL23","DOI":"10.1109/TMC.2022.3186936","CorpusId":250124963},"title":"Adaptive Control of Local Updating and Model Compression for Efficient Federated Learning"},{"paperId":"628e918632edb5f60ada2cb6562799a97205ca9f","externalIds":{"DBLP":"journals/ton/FangZXWY23","DOI":"10.1109/TNET.2023.3244794","CorpusId":257144071},"title":"GRID: Gradient Routing With In-Network Aggregation for Distributed Training"},{"paperId":"e97addc2c9d137ca53a73d41ad59083c1a4cf214","externalIds":{"DBLP":"journals/corr/abs-2309-08125","ArXiv":"2309.08125","DOI":"10.1145/3600006.3613152","CorpusId":262012886},"title":"Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates"},{"paperId":"795c9f8571f01162eefd30c4773d95c47cb6025c","externalIds":{"DBLP":"journals/jsa/WangWXZWZ23","DOI":"10.1016/j.sysarc.2023.102927","CorpusId":259541124},"title":"Communication compression techniques in distributed deep learning: A survey"},{"paperId":"56ee5f6b57f2d93c4fea55b696236a92fc2dcb83","externalIds":{"DBLP":"journals/tmc/FengCWWZH24","ArXiv":"2308.16835","DOI":"10.1109/TMC.2023.3311188","CorpusId":261394644},"title":"FedDD: Toward Communication-Efficient Federated Learning With Differential Parameter Dropout"},{"paperId":"dda9e433ac572636b57d05c1a1bf529fb4df2973","externalIds":{"DBLP":"conf/icpp/GuZWCG23","ArXiv":"2309.00558","DOI":"10.1145/3605573.3605638","CorpusId":261494131},"title":"FaST-GShare: Enabling Efficient Spatio-Temporal GPU Sharing in Serverless Computing for Deep Learning Inference"},{"paperId":"63c1052d76098022b1d3a3811e1bfcaca9077bca","externalIds":{"DBLP":"journals/tc/YangWXWZZ23","DOI":"10.1109/TC.2023.3242200","CorpusId":256596661},"title":"Hydra: Deadline-Aware and Efficiency-Oriented Scheduling for Deep Learning Jobs on Heterogeneous GPUs"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"4c55dd37ab4c41565226a3c8194166d7412b0a02","externalIds":{"ArXiv":"2307.02779","DBLP":"journals/cm/ShenSZLPLZL24","DOI":"10.1109/MCOM.001.2300550","CorpusId":259360434},"title":"Large Language Models Empowered Autonomous Edge AI for Connected Intelligence"},{"paperId":"74bd049ccab069131f0ff16968a73765262b34d4","externalIds":{"DBLP":"journals/tmc/WangXLXHZ23","DOI":"10.1109/TMC.2022.3147792","CorpusId":246535722},"title":"Accelerating Federated Learning With Cluster Construction and Hierarchical Aggregation"},{"paperId":"6f36cc547e9465c749f765d2434e08d8a19173f0","externalIds":{"DBLP":"journals/tie/YanQLMK23","DOI":"10.1109/TIE.2022.3203761","CorpusId":252189883},"title":"Mapless Navigation With Safety-Enhanced Imitation Learning"},{"paperId":"2880f2ad2a331332be17cad1de77c7c89d50bba6","externalIds":{"ArXiv":"2306.16926","DBLP":"journals/corr/abs-2306-16926","DOI":"10.1145/3605573.3605650","CorpusId":259287477},"title":"OSP: Boosting Distributed Model Training with 2-stage Synchronization"},{"paperId":"78e826afdfea6c10d73bb963f84a217c33a031c1","externalIds":{"DBLP":"conf/ics/GuoHWHPSTLHG23","DOI":"10.1145/3577193.3593724","CorpusId":259205776},"title":"Software-Hardware Co-design of Heterogeneous SmartNIC System for Recommendation Models Inference and Training"},{"paperId":"64e4aff81788ca0b31d94a7d592a1870ffa2389d","externalIds":{"DBLP":"conf/iwqos/FangZXYSX23","DOI":"10.1109/IWQoS57198.2023.10188783","CorpusId":260254304},"title":"GOAT: Gradient Scheduling with Collaborative In-Network Aggregation for Distributed Training"},{"paperId":"343867970e6feea44fb134244168d04f0f3d164b","externalIds":{"DBLP":"conf/infocom/ZhaoLDCNW23","DOI":"10.1109/INFOCOM53939.2023.10228956","CorpusId":261372226},"title":"Enabling Switch Memory Management for Distributed Training with In-Network Aggregation"},{"paperId":"d3562d9e310873f49a3f10c93dc1db2e4e0b93ec","externalIds":{"DBLP":"conf/eurosys/Li0LXZLW23","DOI":"10.1145/3552326.3587436","CorpusId":258508770},"title":"A2TP: Aggregator-aware In-network Aggregation for Multi-tenant Learning"},{"paperId":"ac54fee4b4160e6cdf56fa95652a102abacf91d5","externalIds":{"DBLP":"journals/tpds/DuanPWXLWL23","DOI":"10.1109/TPDS.2023.3250462","CorpusId":257276320},"title":"Accelerating Distributed DNN Training via Transport Layer Scheduling"},{"paperId":"bb4720ca4cc773e3ad09415e8c49a46418418125","externalIds":{"DBLP":"conf/icde/ChenXXH23","DOI":"10.1109/ICDE55515.2023.00177","CorpusId":260172527},"title":"Enhancing Decentralized Federated Learning for Non-IID Data on Heterogeneous Devices"},{"paperId":"7a945907336c80a6f5df34664afbcfe62d61f0ba","externalIds":{"DBLP":"journals/wc/ZhouLWYYWMJ23","DOI":"10.1109/MWC.004.2200381","CorpusId":258219719},"title":"Decentralized P2P Federated Learning for Privacy-Preserving and Resilient Mobile Robotic Systems"},{"paperId":"5a6e344dfa48e20cec414fb0691698beefa8cd3e","externalIds":{"DBLP":"journals/jsac/CaoBDELPZ23a","DOI":"10.1109/JSAC.2023.3242710","CorpusId":256654957},"title":"Communication-Efficient Distributed Learning: An Overview"},{"paperId":"f9b9f93aa9ba91fbfe03c926abad42d49f33e98b","externalIds":{"DBLP":"journals/cn/YangZZX23","DOI":"10.1016/j.comnet.2023.109777","CorpusId":258244900},"title":"DFS: Joint data formatting and sparsification for efficient communication in Distributed Machine Learning"},{"paperId":"f8ae23caf5047f21acd178496da29d71a10602bd","externalIds":{"DBLP":"journals/tcc/MaoSZCGL23","DOI":"10.1109/TCC.2022.3194128","CorpusId":251142102},"title":"Elastic Resource Management for Deep Learning Applications in a Container Cluster"},{"paperId":"c59abe4ec214b34364a87a9aa0e4a53e7804461e","externalIds":{"DBLP":"journals/corr/abs-2303-17885","ArXiv":"2303.17885","DOI":"10.48550/arXiv.2303.17885","CorpusId":257901019},"title":"Accelerating Wireless Federated Learning via Nesterov's Momentum and Distributed Principle Component Analysis"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","externalIds":{"DBLP":"journals/corr/abs-2303-17564","ArXiv":"2303.17564","CorpusId":257833842},"title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"74e3ce19755b1d812573d42dac54c08d3f972b9f","externalIds":{"DBLP":"conf/asplos/LiuWZWLLXCCH23","DOI":"10.1145/3582016.3582037","CorpusId":257632835},"title":"In-Network Aggregation with Transport Transparency for Distributed Training"},{"paperId":"116f63722179f665696d8720ddc3911d80fd7b78","externalIds":{"DBLP":"journals/corr/abs-2303-13610","ArXiv":"2303.13610","DOI":"10.1038/s41591-023-02252-4","CorpusId":257717265,"PubMed":"36959422"},"title":"Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging"},{"paperId":"f8e95d07e78369d3b792753734a58d1c14dd2f24","externalIds":{"PubMedCentral":"10033450","DOI":"10.1038/s41591-023-02225-7","CorpusId":257309821,"PubMed":"36864252"},"title":"A deep-learning algorithm to classify skin lesions from mpox virus infection"},{"paperId":"e14403acf5e5602bf8d566336d909b31cb717b04","externalIds":{"DBLP":"journals/tpds/TangSLC23","DOI":"10.1109/TPDS.2022.3230938","CorpusId":255046867},"title":"GossipFL: A Decentralized Federated Learning Framework With Sparsified and Adaptive Communication"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"3822cb0a089a66f2ad88e7960fcae6ebdc4e4427","externalIds":{"ArXiv":"2302.12445","DBLP":"conf/icdcs/ZhangSCWLL23","DOI":"10.1109/ICDCS57875.2023.00054","CorpusId":259164359},"title":"DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining"},{"paperId":"34e14fdf011987ecf8abac4dc18c776e1554c51c","externalIds":{"DBLP":"journals/tpds/WangTWGNWGY23","DOI":"10.1109/TPDS.2022.3228733","CorpusId":254664666},"title":"FSP: Towards Flexible Synchronous Parallel Frameworks for Distributed Machine Learning"},{"paperId":"2359f07311c32f64ae5f7a701a8490c69be90f82","externalIds":{"DBLP":"journals/iotj/BiXTLC23","DOI":"10.1109/JIOT.2022.3212464","CorpusId":252812413},"title":"Achieving Lightweight and Privacy-Preserving Object Detection for Connected Autonomous Vehicles"},{"paperId":"ffdd341df1e84caca4d717959af2a82d009aa14c","externalIds":{"DBLP":"journals/tvt/SaikiaBSL23","DOI":"10.1109/TVT.2022.3211652","CorpusId":252728720},"title":"Signal Detection in GSM-Based In-Band Full-Duplex Communication Using DNN"},{"paperId":"5278b81db686b4d36143941bff1c683bea963a63","externalIds":{"DBLP":"journals/corr/abs-2301-11913","ArXiv":"2301.11913","DOI":"10.48550/arXiv.2301.11913","CorpusId":251542095},"title":"SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient"},{"paperId":"3e9e607210e278d33f5b32b340bb5eb7482dfef5","externalIds":{"DBLP":"conf/asplos/CowanMMSX23","DOI":"10.1145/3575693.3575724","CorpusId":256391230},"title":"MSCCLang: Microsoft Collective Communication Language"},{"paperId":"fc426ebe45e0b7dac5052947d7b482847a197da8","externalIds":{"DBLP":"conf/icpads/ChenWCH22","DOI":"10.1109/ICPADS56603.2022.00117","CorpusId":257783094},"title":"Tereis: A Package-Based Scheduling in Deep Learning Systems"},{"paperId":"e6a5fc5c430e2ac0d3a4cc318bc2fd5d281881e4","externalIds":{"DBLP":"journals/tii/AlzubiASR23","DOI":"10.1109/TII.2022.3189170","CorpusId":250381557},"title":"Cloud-IIoT-Based Electronic Health Record Privacy-Preserving by CNN and Blockchain-Enabled Federated Learning"},{"paperId":"637c212a287c394e50710ccd1f2f8e45677fe793","externalIds":{"ArXiv":"2212.09668","DBLP":"journals/corr/abs-2212-09668","DOI":"10.1109/MWC.006.2200494","CorpusId":254854349},"title":"Task-Oriented Communications for NextG: End-to-end Deep Learning and AI Security Aspects"},{"paperId":"d06f82ffeb481027571935bef019f7bc8bf22d1a","externalIds":{"DBLP":"journals/corr/abs-2212-08272","ArXiv":"2212.08272","DOI":"10.1109/INFOCOM53939.2023.10228970","CorpusId":254823375},"title":"Communication-Efficient Federated Learning for Heterogeneous Edge Devices Based on Adaptive Gradient Quantization"},{"paperId":"10166aba7157880473a264f87a96c33e8fe24589","externalIds":{"DBLP":"journals/tii/ZhangYYH22","DOI":"10.1109/TII.2022.3178410","CorpusId":249137609},"title":"Distributed Real-Time Scheduling in Cloud Manufacturing by Deep Reinforcement Learning"},{"paperId":"4a09fdb268a342198279a74ddfe1ed7150e89e20","externalIds":{"DBLP":"journals/tpds/JinXGHY22","DOI":"10.1109/TPDS.2022.3200518","CorpusId":251755530},"title":"PS+: A Simple yet Effective Framework for Fast Training on Parameter Server"},{"paperId":"22bc6d8c6fe96758ccf7099fcf482fce5525889f","externalIds":{"DBLP":"conf/cloud/GaoS0022","DOI":"10.1145/3542929.3563460","CorpusId":253385756},"title":"Titan: a scheduler for foundation model fine-tuning workloads"},{"paperId":"d54ebb7b83e81013dc75e12b31fd9e29e587cf4c","externalIds":{"DBLP":"journals/corr/abs-2211-01713","ArXiv":"2211.01713","DOI":"10.1109/TPDS.2022.3232715","CorpusId":253264995},"title":"iGniter: Interference-Aware GPU Resource Provisioning for Predictable DNN Inference in the Cloud"},{"paperId":"f70719547a33377ac22cfc106768233c45278eba","externalIds":{"DBLP":"journals/comcom/FengDLMYW23","DOI":"10.1016/j.comcom.2022.11.004","CorpusId":253516065},"title":"In-network aggregation for data center networks: A survey"},{"paperId":"5ada70309e4a16b90994546c954b2ff4ece7c9fd","externalIds":{"DOI":"10.1016/j.jmsy.2022.08.013","CorpusId":253012066},"title":"Solving task scheduling problems in cloud manufacturing via attention mechanism and deep reinforcement learning"},{"paperId":"e2f20d95928dcaa2293a0970e431adbf3d1635e0","externalIds":{"MAG":"3106918569","DBLP":"journals/tcc/LiCLQXZ22","DOI":"10.1109/TCC.2020.3040312","CorpusId":229606876},"title":"Efficient Online Scheduling for Coflow-Aware Machine Learning Clusters"},{"paperId":"fc3375362be027c3ec411b399a377b29dee878cd","externalIds":{"DBLP":"journals/ijis/TanLLCG22","DOI":"10.1002/int.23060","CorpusId":252427047},"title":"Adaptive synchronous strategy for distributed machine learning"},{"paperId":"1ad7a323cebbd186a6f3c9a1f0caf0af94ce91bd","externalIds":{"ArXiv":"2209.01346","DBLP":"conf/sc/HoeflerBSGLHBGCS22","DOI":"10.1109/SC41404.2022.00016","CorpusId":252089281},"title":"HammingMesh: A Network Topology for Large-Scale Deep Learning"},{"paperId":"baa467a4dccf87bc7e2c5a4ea6fd5e401d962d39","externalIds":{"DBLP":"conf/cluster/LiuLLDGL22","DOI":"10.1109/CLUSTER51413.2022.00042","CorpusId":252999300},"title":"AutoPipe: A Fast Pipeline Parallelism Approach with Balanced Partitioning and Micro-batch Slicing"},{"paperId":"692d4af64f8ab534134591a043fbb748b669b8cb","externalIds":{"DBLP":"journals/jsac/YanLHLS22","DOI":"10.1109/JSAC.2022.3192050","CorpusId":250941277},"title":"AC-SGD: Adaptively Compressed SGD for Communication-Efficient Distributed Learning"},{"paperId":"e66384bfc81d0c9357ce5e692a1b42a30bbe48e1","externalIds":{"DBLP":"conf/icpp/LiuYD22","DOI":"10.1145/3545008.3545027","CorpusId":255775678},"title":"Adaptive and Efficient GPU Time Sharing for Hyperparameter Tuning in Cloud"},{"paperId":"84ac0401115752b9e5c67f550f489307ab2859e6","externalIds":{"DBLP":"conf/icpp/Li0LZJ022","DOI":"10.1145/3545008.3545024","CorpusId":255775677},"title":"HSP: Hybrid Synchronous Parallelism for Fast Distributed Deep Learning"},{"paperId":"007737b9ed27743944263d42763f00262e368502","externalIds":{"DBLP":"journals/corr/abs-2104-06023","DOI":"10.1145/3510587","CorpusId":233219481},"title":"Communication-Efficient Federated Learning with Adaptive Quantization"},{"paperId":"192bafbddba47ffc89b7201473ca640240727955","externalIds":{"DBLP":"conf/fpl/GuoGZHW0LLH22","DOI":"10.1109/FPL57034.2022.00071","CorpusId":256878188},"title":"A Framework for Neural Network Inference on FPGA-Centric SmartNICs"},{"paperId":"7bc444a2063d2334fb8f0a66b363dc13ec1b71b0","externalIds":{"DBLP":"journals/jpdc/ParkYYO22","DOI":"10.1016/j.jpdc.2022.07.009","CorpusId":251174569},"title":"AMBLE: Adjusting mini-batch and local epoch for federated learning with heterogeneous devices"},{"paperId":"b3ef89e0ff008bdad2fd4ea247a4b30424ed2441","externalIds":{"DBLP":"journals/cn/WangYBT22","DOI":"10.2139/ssrn.4072553","CorpusId":247926966},"title":"OSDL: Dedicated optical slice provisioning in support of distributed deep learning"},{"paperId":"d479efb49f3aae2ab1b3a0b89755a5a7af40fdf2","externalIds":{"ArXiv":"2206.08307","DBLP":"journals/corr/abs-2206-08307","DOI":"10.48550/arXiv.2206.08307","CorpusId":249712190},"title":"Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning"},{"paperId":"72127bcfd54da6281874f689ccd7ad8edac7e54e","externalIds":{"DBLP":"conf/nips/MishchenkoBEW22","ArXiv":"2206.07638","DOI":"10.48550/arXiv.2206.07638","CorpusId":249674816},"title":"Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays"},{"paperId":"ff749d42b2822b6124f404c19c8bf5fd904f8cd9","externalIds":{"DBLP":"conf/dac/LiuGZCL22","DOI":"10.1145/3489517.3530474","CorpusId":251744185},"title":"Sniper: Cloud-Edge Collaborative Inference Scheduling with Neural Network Similarity Modeling"},{"paperId":"6b117a8dcaa161562b0a69afbb9811e11afb5b3e","externalIds":{"DBLP":"journals/corr/abs-2206-01288","ArXiv":"2206.01288","DOI":"10.48550/arXiv.2206.01288","CorpusId":249375466},"title":"Decentralized Training of Foundation Models in Heterogeneous Environments"},{"paperId":"b2bc82192fdbb58d0d244768addac5fa93c9962e","externalIds":{"DBLP":"journals/tnsm/HeCZSLYG22","DOI":"10.1109/tnsm.2021.3132361","CorpusId":244886208},"title":"Beamer: Stage-Aware Coflow Scheduling to Accelerate Hyper-Parameter Tuning in Deep Learning Clusters"},{"paperId":"9cbe8abf59c7a2717cfcb78c96e5bd5dcac7ba3e","externalIds":{"DBLP":"journals/eswa/MaZQXTC22","DOI":"10.1016/j.eswa.2022.117508","CorpusId":248787475},"title":"A privacy-preserving content-based image retrieval method based on deep learning in cloud computing"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"533beb3cc4b99f05ac2f3baae90e1bab07fd93a2","externalIds":{"DBLP":"conf/eurosys/OhLKS22","DOI":"10.1145/3492321.3519563","CorpusId":247765527},"title":"Out-of-order backprop: an effective scheduling technique for deep learning"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"1ff32b615497085a999f17192a09914628ebf77c","externalIds":{"ArXiv":"2203.07320","DBLP":"conf/infocom/LiuXYWL22","DOI":"10.1109/INFOCOM48880.2022.9796721","CorpusId":247447326},"title":"The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining"},{"paperId":"4c397c32fe121f32445bb5ca90e3a8d07b9e785e","externalIds":{"DBLP":"journals/comsur/XuNLKXNYSM23","ArXiv":"2203.05471","DOI":"10.1109/COMST.2022.3221119","CorpusId":247362975},"title":"A Full Dive Into Realizing the Edge-Enabled Metaverse: Visions, Enabling Technologies, and Challenges"},{"paperId":"d4a06a8ce3df7165c18abdd02129b46c60af945b","externalIds":{"DBLP":"journals/iot/DesaiCKPAWGB22","DOI":"10.1016/j.iot.2021.100485","CorpusId":245166300},"title":"HealthCloud: A system for monitoring health status of heart patients using machine learning and cloud computing"},{"paperId":"fdacbdcc5e998c250bfc8ea00bda34d898a7798b","externalIds":{"DBLP":"journals/tgcn/BouachirAOA22","DOI":"10.1109/tgcn.2022.3140978","CorpusId":245808391},"title":"FederatedGrids: Federated Learning and Blockchain-Assisted P2P Energy Sharing"},{"paperId":"15822ae7035dec57b3126dac84386a415cb4d59e","externalIds":{"ArXiv":"2202.07896","DBLP":"journals/corr/abs-2202-07896","DOI":"10.1145/3552326.3587445","CorpusId":258508802},"title":"Lyra: Elastic Scheduling for Deep Learning Clusters"},{"paperId":"2d9c43e1133f17c73408774840c93382f3cc8f84","externalIds":{"DBLP":"conf/nsdi/WangKZGJM0K23","ArXiv":"2202.00433","CorpusId":252668896},"title":"TopoOpt: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","externalIds":{"ArXiv":"2201.11990","DBLP":"journals/corr/abs-2201-11990","CorpusId":246411325},"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","externalIds":{"DBLP":"journals/corr/abs-2201-08239","ArXiv":"2201.08239","CorpusId":246063428},"title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"9e4a8177823d044deca512f5849a3f61af553a15","externalIds":{"ArXiv":"2201.07598","DBLP":"conf/ppopp/0002H22","DOI":"10.1145/3503221.3508399","CorpusId":246035776},"title":"Near-optimal sparse allreduce for distributed deep learning"},{"paperId":"754fbf0b104717641f5bdac9c6efdfb7474e24f8","externalIds":{"DBLP":"journals/corr/abs-2201-06539","ArXiv":"2201.06539","DOI":"10.1109/lra.2022.3146635","CorpusId":246016150},"title":"Spatiotemporal Costmap Inference for MPC Via Deep Inverse Reinforcement Learning"},{"paperId":"d1e3f04fb2c6a84c35d1ee3be6cb5fbd2b029043","externalIds":{"DBLP":"journals/corr/abs-2201-01684","ArXiv":"2201.01684","DOI":"10.1109/TPDS.2021.3137867","CorpusId":245457305},"title":"Dynamic GPU Energy Optimization for Machine Learning Training Workloads"},{"paperId":"8b67d9535efd60194d37f6c76b67d53bd8015be5","externalIds":{"DBLP":"journals/tpds/YeungBYFHG22","DOI":"10.1109/TPDS.2021.3079202","CorpusId":235692248},"title":"Horus: Interference-Aware and Prediction-Based Scheduling in Deep Learning Systems"},{"paperId":"88b1ebd97bf135bffca3f14085a8a927b4d9474f","externalIds":{"DBLP":"conf/bigdataconf/Nukada21","DOI":"10.1109/BigData52589.2021.9672073","CorpusId":245935375},"title":"Performance Optimization of Allreduce Operation for Multi-GPU Systems"},{"paperId":"157bddb85786b980c962e649bed33d8ebbf7b4a1","externalIds":{"DBLP":"journals/corr/abs-2111-06061","ArXiv":"2111.06061","DOI":"10.1109/TKDE.2022.3178211","CorpusId":243985820},"title":"Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI"},{"paperId":"cb1cc03b04079b89a1f42d736cba4d2ee3e4f7c3","externalIds":{"ArXiv":"2111.04867","DBLP":"conf/nsdi/ShahCCMMMNS23","CorpusId":250420873},"title":"TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches"},{"paperId":"43332a71939ae7f3bd4756cba2c5ef0763b5cfac","externalIds":{"DBLP":"journals/corr/abs-2111-04007","ArXiv":"2111.04007","DOI":"10.1145/3492321.3519584","CorpusId":243847496},"title":"Varuna: scalable, low-cost training of massive deep learning models"},{"paperId":"e581a00152e3764d56277a409bc2c46f6092c484","externalIds":{"DBLP":"conf/cloud/GaoYS0021","DOI":"10.1145/3472883.3486978","CorpusId":239890151},"title":"Chronus: A Novel Deadline-aware Scheduler for Deep Learning Training Jobs"},{"paperId":"5b8f5a218ceecf164886135741a7416553dce7e2","externalIds":{"DBLP":"conf/cloud/WangYYWLSHZ21","DOI":"10.1145/3472883.3486987","CorpusId":239890145},"title":"Morphling: Fast, Near-Optimal Auto-Configuration for Cloud-Native Model Serving"},{"paperId":"eeb66c3463857244d8e8efff8922757ea509de63","externalIds":{"DBLP":"journals/corr/abs-2109-07735","ArXiv":"2109.07735","CorpusId":237266173},"title":"Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning"},{"paperId":"ac35dffd21c16b02e140a36726b3a21d266cab0f","externalIds":{"DBLP":"conf/sc/Hu0Y0021","ArXiv":"2109.01313","DOI":"10.1145/3458817.3476223","CorpusId":237417338},"title":"Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters"},{"paperId":"d9548bd87381749c8822d34abc786de8c7e305f7","externalIds":{"DBLP":"conf/icpp/ZhangQSCX21","DOI":"10.1145/3472456.3472467","CorpusId":238359072},"title":"Prophet: Speeding up Distributed DNN Training with Predictable Communication Scheduling"},{"paperId":"c0c36424f691c9d934aee7b9c6c6c86429a57b45","externalIds":{"DBLP":"conf/sigcomm/ShirkoohiGAZGBV21","DOI":"10.1145/3452296.3472900","CorpusId":236426380},"title":"SiP-ML: high-bandwidth optical network interconnects for machine learning training"},{"paperId":"0738451518ae3f6941261230d3143f5e61404cca","externalIds":{"DBLP":"conf/nips/SahuDABCK21","ArXiv":"2108.00951","CorpusId":236772471},"title":"Rethinking gradient sparsification as total error minimization"},{"paperId":"10f3ca78e194552427ebe9173b19d1b910469e27","externalIds":{"DBLP":"conf/sc/0002H21","ArXiv":"2107.06925","DOI":"10.1145/3458817.3476145","CorpusId":235898937},"title":"Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"},{"paperId":"d79e8947f481c8ad5703611964a9f6c9bddce551","externalIds":{"DBLP":"conf/icdcs/ChenXWLLCZ21","DOI":"10.1109/ICDCS51616.2021.00010","CorpusId":234101214},"title":"Communication-Efficient Federated Learning with Adaptive Parameter Freezing"},{"paperId":"3d2861d4fa3816afb672014eaee03978b965a1e0","externalIds":{"DBLP":"journals/corr/abs-2106-11879","ArXiv":"2106.11879","CorpusId":235592902},"title":"Asynchronous Stochastic Optimization Robust to Arbitrary Delays"},{"paperId":"fecfd8fe8b6ad03ac81467a838d8878ee650de48","externalIds":{"MAG":"3175663678","DBLP":"journals/tits/LiTZL022","DOI":"10.1109/tits.2021.3081560","CorpusId":237925933},"title":"Privacy-Preserved Federated Learning for Autonomous Driving"},{"paperId":"cf30fb61a5943781144c8442563e3ef9c38df871","externalIds":{"DBLP":"conf/icml/Li0GK21","ArXiv":"2106.07476","CorpusId":235421912},"title":"Training Graph Neural Networks with 1000 Layers"},{"paperId":"25890b3be92e8037585377320fbb09c0c679595c","externalIds":{"DBLP":"conf/nips/SpiridonoffOP21","ArXiv":"2106.04759","CorpusId":235376763},"title":"Communication-efficient SGD: From Local SGD to One-Shot Averaging"},{"paperId":"88d21b9a5ae3aeac6a509c6a19f84d7ab39ad964","externalIds":{"MAG":"3170708168","DBLP":"journals/tits/ArthursGK0HM22","DOI":"10.1109/TITS.2021.3084396","CorpusId":236300974},"title":"A Taxonomy and Survey of Edge Cloud Computing for Intelligent Transportation Systems and Connected Vehicles"},{"paperId":"9591957bb641996aab63cd2f483b54308cdb22af","externalIds":{"DBLP":"journals/comsur/RodriguezOGC21","MAG":"3169084863","DOI":"10.1109/COMST.2021.3086296","CorpusId":236250406},"title":"A Survey of Deep Learning Techniques for Cybersecurity in Mobile Networks"},{"paperId":"cc4294064134d32a075048a43be698ce6832cfab","externalIds":{"MAG":"3175487073","DBLP":"conf/aaai/Say21","DOI":"10.1609/aaai.v35i6.16635","CorpusId":235306075},"title":"A Unified Framework for Planning with Learned Neural Network Transition Models"},{"paperId":"546264399f3914e87f1ad6aaf2f1a132aba6073c","externalIds":{"DBLP":"conf/infocom/YuWJL21","ArXiv":"2105.13855","DOI":"10.1109/INFOCOM42981.2021.9488916","CorpusId":231978223},"title":"A Sum-of-Ratios Multi-Dimensional-Knapsack Decomposition for DNN Resource Scheduling"},{"paperId":"1f47fc5d783e517147f66c4c1a1465e8bab35c63","externalIds":{"DBLP":"journals/comsur/BaccourMAEMHG22","ArXiv":"2105.01798","DOI":"10.1109/COMST.2022.3200740","CorpusId":251850079},"title":"Pervasive AI for IoT Applications: A Survey on Resource-Efficient Distributed Artificial Intelligence"},{"paperId":"f938cffd498ffb81ee9d66b4cd473e82c2e12c72","externalIds":{"DBLP":"journals/corr/abs-2104-14362","ArXiv":"2104.14362","DOI":"10.1007/s10115-022-01664-x","CorpusId":233444166},"title":"From distributed machine learning to federated learning: a survey"},{"paperId":"a3893072ac8aaaa01cbd754a59fabc4d6c1adb2c","externalIds":{"DOI":"10.1073/pnas.2024789118","CorpusId":233371133,"PubMed":"33888586"},"title":"Communication-efficient federated learning"},{"paperId":"181198db16562aec7d0409871345027b406242a9","externalIds":{"MAG":"3105148963","ArXiv":"2104.11125","DBLP":"conf/nips/ChenNLCCSWVSZG20","CorpusId":227276415},"title":"ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training"},{"paperId":"774591fdd988eaaff3917e7c5171d044b0843e63","externalIds":{"ArXiv":"2104.04473","DBLP":"conf/sc/NarayananSCLPKV21","DOI":"10.1145/3458817.3476209","CorpusId":236635565},"title":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"},{"paperId":"a025ad05916ddf3281a2bca0cdffbaed99a2c9dd","externalIds":{"DBLP":"journals/iotj/NguyenDPPLSLNP21","MAG":"3156226702","ArXiv":"2104.01776","DOI":"10.1109/JIOT.2021.3072611","CorpusId":233025108},"title":"Federated Learning Meets Blockchain in Edge Computing: Opportunities and Challenges"},{"paperId":"16a7dcb0332e02c502174a6210e8cfcde9f8762a","externalIds":{"ArXiv":"2103.10481","DBLP":"journals/jsac/LinHABM21","DOI":"10.1109/jsac.2021.3118344","CorpusId":237293624},"title":"Semi-Decentralized Federated Learning With Cooperative D2D Local Model Aggregations"},{"paperId":"43db70f525556f868559a1e71e632612fb57dcd0","externalIds":{"ArXiv":"2103.08870","DBLP":"journals/corr/abs-2103-08870","DOI":"10.1109/TNNLS.2021.3084806","CorpusId":232240523,"PubMed":"34111008"},"title":"Learned Gradient Compression for Distributed Deep Learning"},{"paperId":"fdf8681679cf89f9f7b1f0cc560b7cc950e97b09","externalIds":{"MAG":"3139096080","DBLP":"journals/pieee/HautPMPRP21","DOI":"10.1109/JPROC.2021.3063258","CorpusId":234322055},"title":"Distributed Deep Learning for Remote Sensing Data Interpretation"},{"paperId":"2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6","externalIds":{"ArXiv":"2103.03239","DBLP":"conf/nips/RyabininGPP21","CorpusId":232110560},"title":"Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"},{"paperId":"68faf57a657e6e4e73a912a00cfe4245571b1c1d","externalIds":{"DBLP":"conf/icml/EsfandiariTJBHH21","ArXiv":"2103.02051","CorpusId":232105117},"title":"Cross-Gradient Aggregation for Decentralized Learning from Non-IID data"},{"paperId":"7b39b5f1e7b0822a627b1b2c3a37dbdeaa383f0e","externalIds":{"MAG":"3134742951","DBLP":"journals/tcc/SunGGQWL22","DOI":"10.1109/TCC.2021.3062398","CorpusId":233911304},"title":"GSSP: Eliminating Stragglers Through Grouping Synchronous for Distributed Deep Learning in Heterogeneous Cluster"},{"paperId":"1836c0c53bc8c77aa56d45834c3a3ff910089bc2","externalIds":{"ArXiv":"2102.08374","DBLP":"conf/iclr/MishchenkoWKR22","CorpusId":247594371},"title":"IntSGD: Adaptive Floatless Compression of Stochastic Gradients"},{"paperId":"4066d78b637c2b8e57de5ffd53950134a551de85","externalIds":{"DBLP":"conf/icml/TangGARLLLZH21","ArXiv":"2102.02888","CorpusId":231839831},"title":"1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed"},{"paperId":"994a75c50c0ab67f43250f2ead45cd841c378e28","externalIds":{"MAG":"3092359860","DBLP":"journals/isci/XiaoMKCGW21","DOI":"10.1016/j.ins.2020.05.121","CorpusId":225133242},"title":"EGC: Entropy-based gradient compression for distributed deep learning"},{"paperId":"8b2e1137639aa764aa3c6ecbd1c1aa9787126c7e","externalIds":{"DBLP":"journals/corr/abs-2101-10761","ArXiv":"2101.10761","CorpusId":231709524},"title":"An Efficient Statistical-based Gradient Compression Technique for Distributed Training Systems"},{"paperId":"f030656c4441093852ca0ad0d533b156a7c8a884","externalIds":{"DBLP":"journals/corr/abs-2012-07450","ArXiv":"2012.07450","DOI":"10.1109/TMC.2020.3045266","CorpusId":229156428},"title":"FedHome: Cloud-Edge Based Personalized Federated Learning for In-Home Health Monitoring"},{"paperId":"ce5ff3b5f8595ed10155f6626e7127aa9ef0ac66","externalIds":{"DBLP":"conf/icfpt/ScheltenSSS20","DOI":"10.1109/ICFPT51103.2020.00042","CorpusId":233991434},"title":"A High-Throughput, Resource-Efficient Implementation of the RoCEv2 Remote DMA Protocol for Network-Attached Hardware Accelerators"},{"paperId":"6d0c42fb3fe2160708b8afe4d130521e46fb902c","externalIds":{"MAG":"3102606411","DBLP":"journals/jpdc/OuyangDXX21","DOI":"10.1016/j.jpdc.2020.11.005","CorpusId":227143064},"title":"Communication optimization strategies for distributed deep neural network training: A survey"},{"paperId":"af9806c22e8b78d9d995c5f8f85cf7bf367a873a","externalIds":{"DBLP":"conf/conext/0002LS20","MAG":"3108033633","DOI":"10.1145/3386367.3432588","CorpusId":227154870},"title":"Job scheduling for large-scale machine learning clusters"},{"paperId":"a9bebc22b682906a9ebb4d4cce4b31d230131385","externalIds":{"ArXiv":"2010.12460","DBLP":"conf/nips/FaghriTMA0R20","MAG":"3106325597","CorpusId":225062069},"title":"Adaptive Gradient Quantization for Data-Parallel SGD"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"f57db358459390590bc838663025dae0f8d51ebf","externalIds":{"DBLP":"journals/corr/abs-2010-10458","MAG":"3094326709","ArXiv":"2010.10458","CorpusId":224803615},"title":"Towards Scalable Distributed Training of Deep Learning on Public Cloud Clusters"},{"paperId":"9d64a1dc36c851a334bd60e0721588af443adb63","externalIds":{"DBLP":"conf/cloud/DhakalKR20","MAG":"3097411828","DOI":"10.1145/3419111.3421284","CorpusId":222296326},"title":"GSLICE: controlled spatial sharing of GPUs for a scalable inference platform"},{"paperId":"ff56d18c46b6aa3c4fc03927ee0402ae0410522c","externalIds":{"DBLP":"journals/corr/abs-2010-04116","MAG":"3092309115","ArXiv":"2010.04116","CorpusId":222208763},"title":"Interlocking Backpropagation: Improving depthwise model-parallelism"},{"paperId":"cc7c2b1e6eb6475f066982ba21a4cbb0e75a1acf","externalIds":{"DBLP":"conf/IEEEcloud/KangYYY20","DOI":"10.1109/CLOUD49709.2020.00014","CorpusId":229357897},"title":"TensorExpress: In-Network Communication Scheduling for Distributed Deep Learning"},{"paperId":"54a0ae308d380106f7417f62b196d93d56cfb839","externalIds":{"DBLP":"journals/iotj/ChiuSPWWC20","MAG":"3026202797","DOI":"10.1109/JIOT.2020.2995162","CorpusId":219405842},"title":"Semisupervised Distributed Learning With Non-IID Data for AIoT Service Platform"},{"paperId":"68a439902fd7d8005222f75d354a41fba6d60741","externalIds":{"MAG":"3091097978","DBLP":"conf/sigcomm/Fei0SCS21","DOI":"10.1145/3452296.3472904","CorpusId":224928291},"title":"Efficient sparse collective communication and its application to accelerate distributed deep learning"},{"paperId":"6de9eb6cb68185dd311be3093375527e5b56330d","externalIds":{"MAG":"3087041179","DBLP":"journals/tcyb/VuCNNHD22","DOI":"10.1109/TCYB.2020.3013416","CorpusId":221798943,"PubMed":"32946404"},"title":"Learning Latent Representation for IoT Anomaly Detection"},{"paperId":"1dcd3ce2221eff4213487249978f7ec844f1c611","externalIds":{"ArXiv":"2009.05766","DBLP":"journals/corr/abs-2009-05766","MAG":"3084523204","DOI":"10.1109/ICDE51399.2021.00040","CorpusId":221655553},"title":"Communication-efficient Decentralized Machine Learning over Heterogeneous Networks"},{"paperId":"23fd84cb4422850dcbae301a53cd51ea25facd03","externalIds":{"MAG":"3085277063","DBLP":"journals/esticas/NabavinejadBCPK20","DOI":"10.1109/JETCAS.2020.3022920","CorpusId":221912176},"title":"An Overview of Efficient Interconnection Networks for Deep Neural Network Accelerators"},{"paperId":"2adcdc9e9e81147499e1372f992d25ac6265fb29","externalIds":{"DBLP":"conf/osdi/QiaoCSNH0GX21","MAG":"3080451848","ArXiv":"2008.12260","CorpusId":221340551},"title":"Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning"},{"paperId":"461a7bf0c14df0fb08d3c59bdec491778a861270","externalIds":{"DBLP":"journals/corr/abs-2008-08708","ArXiv":"2008.08708","MAG":"3072623287","DOI":"10.1145/3437801.3441620","CorpusId":221186601},"title":"Synthesizing optimal collective algorithms"},{"paperId":"7c064f21163f085476b65cc1f12bd4f823c7b99c","externalIds":{"MAG":"3069398144","DBLP":"journals/remotesensing/AspriTT20","DOI":"10.3390/rs12172670","CorpusId":221082953},"title":"Distributed Training and Inference of Deep Learning Models for Multi-Modal Land Cover Classification"},{"paperId":"2f4d6d3748ac6822711fe0bbd4cf6d2e66fa6613","externalIds":{"MAG":"3096583839","ArXiv":"2008.09213","DBLP":"journals/corr/abs-2008-09213","CorpusId":221246383},"title":"Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads"},{"paperId":"fd2a2caefd0c986701aa676d14839c723e570edf","externalIds":{"DBLP":"conf/icpp/SultanaCXY20","MAG":"3047783725","DOI":"10.1145/3404397.3404415","CorpusId":221070168},"title":"E-LAS: Design and Analysis of Completion-Time Agnostic Scheduling for Distributed Deep Learning Cluster"},{"paperId":"4b6661347d5b58250130b89145dbd34ce310f2a0","externalIds":{"DBLP":"conf/nips/WangLLJP20","MAG":"3043723611","ArXiv":"2007.07481","CorpusId":220525591},"title":"Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization"},{"paperId":"23d69b7b5eb1c7c2423d213c8ccbdad96c329918","externalIds":{"MAG":"3047002910","DBLP":"conf/infocom/WangLG20","DOI":"10.1109/infocom41043.2020.9155282","CorpusId":221087223},"title":"Geryon: Accelerating Distributed CNN Training by Network-Level Flow Scheduling"},{"paperId":"9222062c20aa6a7fad8899da0453269fb804d6ac","externalIds":{"DBLP":"conf/infocom/ShiWCLQLZ20","MAG":"3047357290","DOI":"10.1109/infocom41043.2020.9155269","CorpusId":221091887},"title":"Communication-Efficient Distributed Deep Learning with Merged Gradient Sparsification on GPUs"},{"paperId":"31f8c61248d6994c6f53942436756356cbefc332","externalIds":{"MAG":"3047371394","DBLP":"conf/infocom/LiHZLT20","DOI":"10.1109/infocom41043.2020.9155267","CorpusId":219965190},"title":"Automating Cloud Deployment for Deep Learning Inference of Real-time Online Services"},{"paperId":"3488b8dfa3d98b4e4f999ecc7753dabc77d096a0","externalIds":{"DBLP":"conf/hpdc/WangWZLBHF20","MAG":"3038104246","DOI":"10.1145/3369583.3392681","CorpusId":219958314},"title":"FFT-based Gradient Sparsification for the Distributed Training of Deep Neural Networks"},{"paperId":"49a049dc85e2380dde80501a984878341dd8efdf","externalIds":{"ArXiv":"2006.11477","DBLP":"conf/nips/BaevskiZMA20","MAG":"3036601975","CorpusId":219966759},"title":"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"},{"paperId":"4d622c861cb481abef684e7a9db90dc555631505","externalIds":{"MAG":"3036987943","DBLP":"journals/ton/WangLCGWWXW20","DOI":"10.1109/TNET.2020.2999377","CorpusId":221280099},"title":"A Scalable, High-Performance, and Fault-Tolerant Network Architecture for Distributed Machine Learning"},{"paperId":"20438e2a38a0c4723fbd9de50b44b7335f6f43cb","externalIds":{"MAG":"3031420959","ArXiv":"2006.00719","DBLP":"conf/aaai/YaoGSMKM21","DOI":"10.1609/aaai.v35i12.17275","CorpusId":219176656},"title":"ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"9d9dbb4487aca2b62ca3659446d7010ac65aa642","externalIds":{"MAG":"3045098739","DBLP":"conf/usenix/ParkYYNLCNC20","ArXiv":"2005.14038","CorpusId":218971732},"title":"HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism"},{"paperId":"ee81584f7e426b891765fe122655fe466b00a072","externalIds":{"DBLP":"journals/tifs/FengHLWC20","MAG":"3032769455","DOI":"10.1109/TIFS.2020.2997134","CorpusId":219877325},"title":"SecureNLP: A System for Multi-Party Privacy-Preserving Natural Language Processing"},{"paperId":"7c8351ac6e11327a12cb7a0c24debd3a4ce81b7d","externalIds":{"MAG":"3012536640","DBLP":"journals/jnca/ZhouHLYS20","DOI":"10.1016/j.jnca.2020.102590","CorpusId":215865690},"title":"JPAS: Job-progress-aware flow scheduling for deep learning clusters"},{"paperId":"a87900762e93ff3448fc5468dd35b7983ca79070","externalIds":{"DBLP":"conf/aaai/GeorgeG20","MAG":"2998299603","DOI":"10.1609/AAAI.V34I05.6206","CorpusId":214088285},"title":"Distributed Stochastic Gradient Descent with Event-Triggered Communication"},{"paperId":"94580a0ae3f4b2fe0ab8ae0359c1afbb6ab50dc5","externalIds":{"MAG":"3024862772","DBLP":"journals/ojcs/LiuCZL20","ArXiv":"2003.09876","DOI":"10.1109/OJCOMS.2020.2994737","CorpusId":214612498},"title":"HierTrain: Fast Hierarchical Edge AI Learning With Hybrid Parallelism in Mobile-Edge-Cloud Computing"},{"paperId":"b6e8d81bc70af67a71d288563f5eb0daf67efc3b","externalIds":{"DBLP":"journals/corr/abs-2004-00363","ArXiv":"2004.00363","MAG":"3014216009","DOI":"10.1109/GLOBECOM42002.2020.9348191","CorpusId":214743008},"title":"DNN-based Localization from Channel Estimates: Feature Design and Experimental Results"},{"paperId":"e6196f0fb4867d2f2186c6df666ae687689f174a","externalIds":{"MAG":"3038098022","CorpusId":219939507},"title":"PLink: Discovering and Exploiting Locality for Accelerated Distributed Training on the public Cloud"},{"paperId":"69170faf00279bdcdfd91290a3756d539f9fb6e6","externalIds":{"DBLP":"journals/corr/abs-2003-06307","MAG":"3012220622","CorpusId":212718058},"title":"Communication-Efficient Distributed Deep Learning: A Comprehensive Survey"},{"paperId":"47a8355e76c3675c481f135cce3a5911c74aeac3","externalIds":{"MAG":"3007279920","DBLP":"journals/comsur/ShiYJZL20","ArXiv":"2002.09668","DOI":"10.1109/COMST.2020.3007787","CorpusId":211258847},"title":"Communication-Efficient Edge AI: Algorithms and Systems"},{"paperId":"a6e731a3b7dc4f34a95201c37cc4309ab92318f0","externalIds":{"DBLP":"conf/icml/WoodworthPSDBMS20","ArXiv":"2002.07839","MAG":"3035507578","CorpusId":211171485},"title":"Is Local SGD Better than Minibatch SGD?"},{"paperId":"da93d65ce761de78992989dae8d2af8aeeada54d","externalIds":{"DBLP":"conf/icdcs/HanWL20","MAG":"2998890488","ArXiv":"2001.04756","DOI":"10.1109/ICDCS47774.2020.00026","CorpusId":210472881},"title":"Adaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach"},{"paperId":"67b6fa1b19113619a70eb443f056ffdabaa92ca7","externalIds":{"DBLP":"journals/tpds/ChenQWFYZL20","MAG":"2964667342","DOI":"10.1109/TPDS.2019.2931558","CorpusId":201142972},"title":"Deep Learning Research and Development Platform: Characterizing and Scheduling with QoS Guarantees on GPU Clusters"},{"paperId":"f9a855ae59579d16dca6a5133cd8daddd3305582","externalIds":{"MAG":"2996075766","DBLP":"journals/corr/abs-1912-09789","ArXiv":"1912.09789","DOI":"10.1145/3377454","CorpusId":209439571},"title":"A Survey on Distributed Machine Learning"},{"paperId":"3ea5bde61362d0dfcbe2751da8dd380adc9f7945","externalIds":{"DBLP":"conf/sc/QinZZYZ019","MAG":"2986864338","DOI":"10.1145/3295500.3356164","CorpusId":207953474},"title":"Swift Machine Learning Model Serving Scheduling: A Region Based Reinforcement Learning Approach"},{"paperId":"c125df9f9ea11890f049477a265be00f13d7fad1","externalIds":{"MAG":"2983255219","DBLP":"journals/twc/YangCSHS21","ArXiv":"1911.02417","DOI":"10.1109/TWC.2020.3037554","CorpusId":207880723},"title":"Energy Efficient Federated Learning Over Wireless Communication Networks"},{"paperId":"1e009f755503bffd7644fcd0a45939c54b838b37","externalIds":{"DBLP":"journals/ibmrd/ChoFSKH19","MAG":"2981114289","DOI":"10.1147/jrd.2019.2947013","CorpusId":208224662},"title":"BlueConnect: Decomposing all-reduce for deep learning on heterogeneous network hierarchy"},{"paperId":"cb340ba0b1b3c56e5003bfeb51ab6e4b60148364","externalIds":{"DBLP":"conf/nips/HaddadpourKMC19","MAG":"2982408624","ArXiv":"1910.13598","CorpusId":202790015},"title":"Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization"},{"paperId":"3fd7c9ba742dd2b435afa75217847e5087e2f2a8","externalIds":{"DBLP":"conf/sosp/NarayananHPSDGG19","MAG":"2969388332","DOI":"10.1145/3341301.3359646","CorpusId":202488191},"title":"PipeDream: generalized pipeline parallelism for DNN training"},{"paperId":"8b9b2f1ec32040c30d2c296ac1688a6627a8fc12","externalIds":{"DBLP":"conf/sosp/ShenCJZKPKS19","MAG":"2982157693","DOI":"10.1145/3341301.3359658","CorpusId":204812163},"title":"Nexus: a GPU cluster engine for accelerating DNN-based video analysis"},{"paperId":"8221b23813858ee536997385097c1ef611184346","externalIds":{"DBLP":"journals/corr/abs-1910-04940","ArXiv":"1910.04940","MAG":"2980047166","CorpusId":204402840},"title":"Blink: Fast and Generic Collectives for Distributed ML"},{"paperId":"e20848622d5145744127b82367f9b671c7ddb08e","externalIds":{"DBLP":"journals/corr/abs-1910-00643","MAG":"2977517840","ArXiv":"1910.00643","CorpusId":203626531},"title":"SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum"},{"paperId":"b213b1a2a80e4d1fb0daeabbdd7196ee4b745fae","externalIds":{"MAG":"2982674764","DBLP":"conf/icccs/JahaniLCAA019","DOI":"10.1109/CCCS.2019.8888151","CorpusId":207833426},"title":"Optimizing on-demand GPUs in the Cloud for Deep Learning Applications Training"},{"paperId":"2734952fe7a88de7cfb16be2dba3cb655c87022d","externalIds":{"DBLP":"journals/corr/abs-1909-06526","MAG":"2972392561","ArXiv":"1909.06526","DOI":"10.1145/3361525.3361538","CorpusId":202577606},"title":"FfDL: A Flexible Multi-tenant Deep Learning Platform"},{"paperId":"40fb2468c3a77c68fe703a6e614f4ad25bd4e3dd","externalIds":{"MAG":"3014943181","DBLP":"journals/corr/abs-1909-00560","ArXiv":"1909.00560","DOI":"10.1109/JIOT.2020.2984887","CorpusId":202541084},"title":"Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence"},{"paperId":"1955dcdd21eeaf0f275ecfbd8266766957d77468","externalIds":{"ArXiv":"1908.06077","MAG":"3168853077","DBLP":"journals/jmlr/Ramezani-Kebrya21","CorpusId":213023083},"title":"NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization"},{"paperId":"d3bf41750875a1a3ef5c4b4ad73917457065acbc","externalIds":{"MAG":"2963149401","DBLP":"conf/icpp/ZhengXCZL19","DOI":"10.1145/3337821.3337873","CorpusId":198943512},"title":"Cynthia: Cost-Efficient Cloud Resource Provisioning for Predictable Distributed Deep Neural Network Training"},{"paperId":"44ea486d6d9d33ea654ba880404c99086e603d7d","externalIds":{"MAG":"2977714483","DBLP":"conf/hpcc/TangWLWH19","DOI":"10.1109/HPCC/SmartCity/DSS.2019.00334","CorpusId":203655977},"title":"Nanily: A QoS-Aware Scheduling for DNN Inference Workload in Clouds"},{"paperId":"3e9a40a567c4a95b591530ff5771296b478a0f0c","externalIds":{"DBLP":"journals/corr/abs-1908-00080","ArXiv":"1908.00080","MAG":"2966086598","DOI":"10.1145/3469029","CorpusId":199064201},"title":"Machine Learning at the Network Edge: A Survey"},{"paperId":"8000a4a63ac97ffb84917f910e2ce747e48d409f","externalIds":{"ArXiv":"1906.02367","MAG":"2948092338","DBLP":"journals/corr/abs-1906-02367","DOI":"10.1109/JSAIT.2020.2985917","CorpusId":174803546},"title":"Qsparse-Local-SGD: Distributed SGD With Quantization, Sparsification, and Local Computations"},{"paperId":"8dbd03d92fb9556a14cb1eed0ee85951bab0c06b","externalIds":{"DBLP":"conf/msml/HorvathHHSCR22","ArXiv":"1905.10988","MAG":"2946198813","CorpusId":166227974},"title":"Natural Compression for Distributed Deep Learning"},{"paperId":"928cd808aba140ec298508df87c5579811ff2f41","externalIds":{"DBLP":"journals/pieee/ZhouCLZLZ19","MAG":"2950865323","ArXiv":"1905.10083","DOI":"10.1109/JPROC.2019.2918951","CorpusId":165163986},"title":"Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing"},{"paperId":"2f682a99bd8ff2e5c7b418760a6d36eb0247e311","externalIds":{"ArXiv":"1905.04346","DBLP":"conf/icml/YuJ19","MAG":"2944305377","CorpusId":152282343},"title":"On the Computation and Communication Complexity of Parallel SGD with Dynamic Batch Sizes for Stochastic Non-Convex Optimization"},{"paperId":"b4a632a7097e7d0631250884dfc6e1f76b376996","externalIds":{"MAG":"2970289928","DBLP":"journals/corr/abs-1905-13727","ArXiv":"1905.13727","CorpusId":173188890},"title":"PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","externalIds":{"ArXiv":"1904.00962","DBLP":"conf/iclr/YouLRHKBSDKH20","MAG":"2995435108","CorpusId":165163737},"title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"2ec9681a021ffed564ac364def44e9fbcedacd48","externalIds":{"MAG":"2919897868","DBLP":"conf/infocom/BaoPW19","DOI":"10.1109/INFOCOM.2019.8737460","CorpusId":86466941},"title":"Deep Learning-based Job Placement in Distributed Machine Learning Clusters"},{"paperId":"5133edcf640bc4cf9beb773059fa79f6e04dc7c9","externalIds":{"DBLP":"conf/infocom/WangLGGC19","MAG":"2919632853","DOI":"10.1109/INFOCOM.2019.8737595","CorpusId":86832271},"title":"Impact of Network Topology on the Performance of DML: Theoretical Analysis and Practical Factors"},{"paperId":"48dec1ce3cfdef6667208cee20eee8a64c9d3302","externalIds":{"MAG":"2920397365","DBLP":"conf/infocom/ChenWL19","DOI":"10.1109/INFOCOM.2019.8737587","CorpusId":86397234},"title":"Round-Robin Synchronization: Mitigating Communication Bottlenecks in Parameter Servers"},{"paperId":"6b39bea0ae1720dcbc1ed19ffa697114c4d356c4","externalIds":{"MAG":"3004495293","ArXiv":"1903.11314","DBLP":"journals/corr/abs-1903-11314","DOI":"10.1145/3363554","CorpusId":219875251},"title":"Scalable Deep Learning on Distributed Infrastructures"},{"paperId":"84c932fcd7226a6d84a4643dddc97a1f795a4922","externalIds":{"MAG":"2926543120","DBLP":"conf/eurosys/XueMCWZZ19","DOI":"10.1145/3302424.3303975","CorpusId":85519203},"title":"Fast Distributed Deep Learning over RDMA"},{"paperId":"db86f5553d23c66c128a29075e8866c4831c4a1f","externalIds":{"DBLP":"journals/tnn/ChenSJ20","MAG":"2998045710","ArXiv":"1903.07424","DOI":"10.1109/TNNLS.2019.2953131","CorpusId":209677470,"PubMed":"31899435"},"title":"Communication-Efficient Federated Deep Learning With Layerwise Asynchronous Model Update and Temporally Weighted Aggregation"},{"paperId":"1fb744bacb54c3ab4c1884e0c6da2e9e3cc666e4","externalIds":{"DBLP":"journals/access/AmeriniLC19","MAG":"2921916843","DOI":"10.1109/ACCESS.2019.2903876","CorpusId":88486572},"title":"Social Network Identification Through Image Classification With CNN"},{"paperId":"46bb5162da1ad9b3591a9e056550135e7a50bc2b","externalIds":{"MAG":"2963436674","ArXiv":"1903.04611","DBLP":"journals/tpds/LiSCLLTB20","DOI":"10.1109/TPDS.2019.2928289","CorpusId":75136160},"title":"Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect"},{"paperId":"29081e5c63d24fa8952e938dba68956cd47ac81d","externalIds":{"MAG":"2989289980","ArXiv":"1903.02891","DBLP":"journals/tnn/SattlerWMS20","DOI":"10.1109/TNNLS.2019.2944481","CorpusId":71147030,"PubMed":"31689214"},"title":"Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data"},{"paperId":"dd0de3977cccff806944d5dc28e053fcd8fdda52","externalIds":{"MAG":"2922527104","DBLP":"journals/corr/abs-1903-06701","ArXiv":"1903.06701","CorpusId":78089762},"title":"Scaling Distributed Machine Learning with In-Network Aggregation"},{"paperId":"1f5ce22d0bef55ee414fbcaadc2d66a396761979","externalIds":{"MAG":"2912234349","DBLP":"journals/corr/abs-1902-04610","ArXiv":"1902.04610","CorpusId":61153642},"title":"Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications"},{"paperId":"79cf9462a583e1889781868cbf8c31e43b36dd2f","externalIds":{"DBLP":"journals/corr/abs-1902-01046","MAG":"3038028469","ArXiv":"1902.01046","CorpusId":59599820},"title":"Towards Federated Learning at Scale: System Design"},{"paperId":"7c22a6a07e89461178b794681c675b209332ee15","externalIds":{"DBLP":"journals/corr/abs-1901-09847","MAG":"2950826569","ArXiv":"1901.09847","CorpusId":59316785},"title":"Error Feedback Fixes SignSGD and other Gradient Compression Schemes"},{"paperId":"54ddd67944520f249b906ba4e817188686eae94d","externalIds":{"MAG":"2953494469","ArXiv":"1901.05758","DBLP":"conf/usenix/JeonVPQXY19","CorpusId":58014231},"title":"Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads"},{"paperId":"22c844d939f755f07b4e78ab830586f10cb0fa6d","externalIds":{"ArXiv":"1901.04359","DBLP":"conf/icdcs/ShiWZTWHC19","MAG":"2908905630","DOI":"10.1109/ICDCS.2019.00220","CorpusId":58004761},"title":"A Distributed Synchronous SGD Algorithm with Global Top-k Sparsification for Low Bandwidth Networks"},{"paperId":"1284ed4bf6a043ecf8cebca09e4811f1e3b83b65","externalIds":{"ArXiv":"1812.06127","DBLP":"conf/mlsys/LiSZSTS20","MAG":"3038022836","CorpusId":59316566},"title":"Federated Optimization in Heterogeneous Networks"},{"paperId":"7f7bb204806ed819b323953cf6ca04cc65a27698","externalIds":{"MAG":"2904648777","DBLP":"journals/pieee/ParkSBD19","ArXiv":"1812.02858","DOI":"10.1109/JPROC.2019.2941458","CorpusId":54457410},"title":"Wireless Network Intelligence at the Edge"},{"paperId":"d84e53711e5d61c61d832febbd75b21f4a8a2956","externalIds":{"MAG":"2891735383","DBLP":"conf/nips/JiangA18","CorpusId":54014664},"title":"A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication"},{"paperId":"d79a26226393f687ddbc375e32055b40b8ad8d38","externalIds":{"MAG":"2991040477","DBLP":"conf/nips/HuangCBFCCLNLWC19","CorpusId":53670168},"title":"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"},{"paperId":"b2c8e834ac5f7be68b9ca3691d39925036dd74a3","externalIds":{"DBLP":"journals/jmlr/ShallueLASFD19","ArXiv":"1811.03600","MAG":"2959980595","CorpusId":53214190},"title":"Measuring the Effects of Data Parallelism on Neural Network Training"},{"paperId":"2582c54766c387f7ee80a21b91a7f5bae262896a","externalIds":{"DBLP":"journals/corr/abs-1810-10161","MAG":"2896827527","ArXiv":"1810.10161","DOI":"10.1109/MCOM.2019.1800155","CorpusId":53085984},"title":"Deep Learning with Long Short-Term Memory for Time Series Prediction"},{"paperId":"6445ddea3defe714a60ca2c933a1f140de4b362b","externalIds":{"DBLP":"journals/corr/abs-1810-08313","MAG":"2896404430","ArXiv":"1810.08313","CorpusId":53043345},"title":"Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD"},{"paperId":"79c8f930bb66c82421b84617e4b6c0b2855cd063","externalIds":{"MAG":"2952941635","DBLP":"journals/corr/abs-1810-07862","ArXiv":"1810.07862","DOI":"10.1109/COMST.2019.2916583","CorpusId":53018231},"title":"Applications of Deep Reinforcement Learning in Communications and Networking: A Survey"},{"paperId":"0606676f16d581fa453f6b7b8a14fc7c4af8d025","externalIds":{"DBLP":"conf/osdi/XiaoBRSKHPPZZYZ18","MAG":"2899071864","CorpusId":52987896},"title":"Gandiva: Introspective Cluster Scheduling for Deep Learning"},{"paperId":"63c52246e0d4dfd5a3f9b6c6c2a1acc48815af42","externalIds":{"DOI":"10.14778/3282495.3282499","CorpusId":240100041},"title":"Rafiki"},{"paperId":"2d8c43aa050203e2b49cd8021d0f65c7d2cca00e","externalIds":{"ArXiv":"1809.10505","DBLP":"journals/corr/abs-1809-10505","MAG":"2949169446","CorpusId":52879241},"title":"The Convergence of Sparsified Gradient Methods"},{"paperId":"38f1ef8ab96e5e0195abcd197bf6df47eb308e8a","externalIds":{"DBLP":"journals/corr/abs-1809-07599","MAG":"2950819300","ArXiv":"1809.07599","CorpusId":52307874},"title":"Sparsified SGD with Memory"},{"paperId":"93ef5b740fa1b54929ead6eb177e0698d7f19719","externalIds":{"ArXiv":"1808.07217","DBLP":"conf/iclr/LinSPJ20","MAG":"2888206291","CorpusId":52071640},"title":"Don't Use Large Mini-Batches, Use Local SGD"},{"paperId":"da8a949f9c9f1df3a38f12c2cac97b789c705465","externalIds":{"MAG":"2885012953","ArXiv":"1807.11023","DBLP":"journals/comsur/Al-GaradiMADAG20","DOI":"10.1109/COMST.2020.2988293","CorpusId":51877788},"title":"A Survey of Machine and Deep Learning Methods for Internet of Things (IoT) Security"},{"paperId":"e0e152748ea0badfbd798dfed3ac743abb58af26","externalIds":{"DBLP":"conf/aaai/YuYZ19","ArXiv":"1807.06629","MAG":"2951862792","DOI":"10.1609/AAAI.V33I01.33015693","CorpusId":53285773},"title":"Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning"},{"paperId":"d29a248bfe7ab08eb99d841f5fd6287f64b8d394","externalIds":{"MAG":"2803965032","DBLP":"journals/corr/abs-1806-08054","ArXiv":"1806.08054","CorpusId":49349763},"title":"Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization"},{"paperId":"b611636f3cfe7b9aa41a606bec1d9fa72e1359ae","externalIds":{"DBLP":"journals/corr/abs-1806-04090","ArXiv":"1806.04090","MAG":"2964267428","CorpusId":47020104},"title":"ATOMO: Communication-efficient Learning via Atomic Sparsification"},{"paperId":"fae2a5101789afd51c1ececb28c75537c88734ec","externalIds":{"ArXiv":"1805.11046","DBLP":"journals/corr/abs-1805-11046","MAG":"2949164106","CorpusId":44071489},"title":"Scalable Methods for 8-bit Training of Neural Networks"},{"paperId":"ccf18d7ead5402dfcf3f2775b40bc78a9a27087f","externalIds":{"MAG":"2963773265","DBLP":"journals/corr/abs-1805-09965","ArXiv":"1805.09965","CorpusId":44061071},"title":"LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning"},{"paperId":"7cfa76a82be96c74b2eff514265b7fd271a179cd","externalIds":{"MAG":"2963179579","DBLP":"journals/corr/abs-1805-09767","ArXiv":"1805.09767","CorpusId":43964415},"title":"Local SGD Converges Fast and Communicates Little"},{"paperId":"93a06eb066fe58ed7d036e46e4cee53483e16bb8","externalIds":{"DBLP":"conf/eurosys/PengBCWG18","MAG":"2798515322","DOI":"10.1145/3190508.3190517","CorpusId":4949076},"title":"Optimus: an efficient dynamic resource scheduler for deep learning clusters"},{"paperId":"e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da","externalIds":{"MAG":"2952696465","ArXiv":"1804.05271","DBLP":"journals/jsac/WangTSLMHC19","DOI":"10.1109/JSAC.2019.2904348","CorpusId":51921962},"title":"Adaptive Federated Learning in Resource Constrained Edge Computing Systems"},{"paperId":"42f7bd35df5a280ccd47115a30901afab9f0776b","externalIds":{"ArXiv":"1803.07068","MAG":"2951371825","DBLP":"conf/icml/TangLYZL18","CorpusId":3988570},"title":"D2: Decentralized Training over Decentralized Data"},{"paperId":"8a5d0579590465494c9aba58a857af43b190b6a6","externalIds":{"DBLP":"journals/comsur/ZhangPH19","ArXiv":"1803.04311","MAG":"2793198093","DOI":"10.1109/COMST.2019.2904897","CorpusId":3887658},"title":"Deep Learning in Mobile and Wireless Networking: A Survey"},{"paperId":"d8c09661b1bebfb690f0566167c87d64c5628d73","externalIds":{"MAG":"2788193959","ArXiv":"1802.09941","DBLP":"journals/csur/Ben-NunH19","DOI":"10.1145/3320060","CorpusId":220247313},"title":"Demystifying Parallel and Distributed Deep Learning"},{"paperId":"e0c0043c43c03de6e2c26a605dc8b8e08872a8a0","externalIds":{"ArXiv":"1802.08021","DBLP":"conf/sc/RenggliAAAH19","MAG":"2789218400","DOI":"10.1145/3295500.3356222","CorpusId":3451069},"title":"SparCML: High-Performance Sparse Communication for Machine Learning"},{"paperId":"33416f2dc49db24cca520a3b234f02463a4e833e","externalIds":{"DBLP":"journals/corr/abs-1802-08246","MAG":"2952436334","ArXiv":"1802.08246","CorpusId":3484600},"title":"Characterizing Implicit Bias in Terms of Optimization Geometry"},{"paperId":"99b3d005763e699613a58b2b7eac5b4ba466567d","externalIds":{"MAG":"2789197829","DBLP":"conf/mlsys/LimAK19","ArXiv":"1802.07389","CorpusId":3392638},"title":"3LC: Lightweight and Effective Traffic Compression for Distributed Machine Learning"},{"paperId":"97884ff15e0a4e83f534b7b13979e519d1c50a54","externalIds":{"MAG":"2786602455","DBLP":"journals/corr/abs-1802-04434","ArXiv":"1802.04434","CorpusId":7763588},"title":"signSGD: compressed optimisation for non-convex problems"},{"paperId":"2e5cf1a6ab44ac44acbec9daa1a870194f1e5553","externalIds":{"MAG":"2952743448","DBLP":"journals/corr/abs-1801-03744","ArXiv":"1801.03744","CorpusId":26093303},"title":"Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?"},{"paperId":"92495abbac86394cb759bec15a763dbf49a8e590","externalIds":{"MAG":"2963803379","DBLP":"conf/iclr/LinHM0D18","ArXiv":"1712.01887","CorpusId":38796293},"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training"},{"paperId":"f7b4139930a42642c71c2af04a013989b7d99be1","externalIds":{"DBLP":"conf/sc/AmaralPCSS17","MAG":"2767239597","DOI":"10.1145/3126908.3126933","CorpusId":207666144},"title":"Topology-Aware GPU Scheduling for Learning Workloads in Cloud Environments"},{"paperId":"db0cc2f21b20cbc0ab8946090967399c25709614","externalIds":{"MAG":"2949130532","DBLP":"journals/iacr/BonawitzIKMMPRS17","DOI":"10.1145/3133956.3133982","CorpusId":3833774},"title":"Practical Secure Aggregation for Privacy-Preserving Machine Learning"},{"paperId":"1caff87770b1cbddcf94edc0a9b1bce029324765","externalIds":{"MAG":"2963540381","ArXiv":"1710.09854","DBLP":"journals/corr/abs-1710-09854","CorpusId":13070099},"title":"Gradient Sparsification for Communication-Efficient Distributed Optimization"},{"paperId":"36f13179cdfc13017df535fdee582d58067301f3","externalIds":{"DBLP":"journals/soco/LotfollahiSZS20","MAG":"2963516518","ArXiv":"1709.02656","DOI":"10.1007/s00500-019-04030-2","CorpusId":35187639},"title":"Deep packet: a novel approach for encrypted traffic classification using deep learning"},{"paperId":"e6e64043c66b83a8787a69a70d7c0a85fd9d23c3","externalIds":{"DBLP":"journals/corr/abs-1708-03888","MAG":"2749988060","CorpusId":5919268},"title":"Scaling SGD Batch Size to 32K for ImageNet Training"},{"paperId":"b07018924334ddc6ec0e8cb07ba0093d5e8c7997","externalIds":{"MAG":"2744599870","ArXiv":"1708.01012","DBLP":"conf/ijcai/ZhouC18","DOI":"10.24963/ijcai.2018/447","CorpusId":3384938},"title":"On the convergence properties of a K-step averaging stochastic gradient descent algorithm for nonconvex optimization"},{"paperId":"7f0d3991feb75f6f2fd50d0f2f80d820f20054cc","externalIds":{"MAG":"2741269719","DBLP":"conf/icml/Zhang0KALZ17","CorpusId":344319},"title":"ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"0d57ba12a6d958e178d83be4c84513f7e42b24e5","externalIds":{"MAG":"2622263826","DBLP":"journals/corr/GoyalDGNWKTJH17","ArXiv":"1706.02677","CorpusId":13905106},"title":"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"},{"paperId":"3f1ab8b484f7881a68c8562ff908390742e4ba90","externalIds":{"MAG":"2950830975","DBLP":"conf/nips/LianZZHZL17","ArXiv":"1705.09056","CorpusId":1467846},"title":"Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent"},{"paperId":"4bdb91a6e47385292ab7a18e8901a6a25f50cc6b","externalIds":{"DBLP":"journals/corr/WenXYWWCL17","MAG":"2617766261","ArXiv":"1705.07878","CorpusId":3747520},"title":"TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning"},{"paperId":"e8437e3b32f091f62f3796556435e139db130f90","externalIds":{"DBLP":"conf/emnlp/AjiH17","MAG":"3101036738","ACL":"D17-1045","ArXiv":"1704.05021","DOI":"10.18653/v1/D17-1045","CorpusId":2140766},"title":"Sparse Communication for Distributed Gradient Descent"},{"paperId":"0a5ff7336879c99513dca6fce6ef44984ebf3f55","externalIds":{"MAG":"2950636297","DBLP":"journals/corr/CrankshawWZFGS16","ArXiv":"1612.03079","CorpusId":1701442},"title":"Clipper: A Low-Latency Online Prediction Serving System"},{"paperId":"f173803acc6bbb11dffa7342d3fab5f4a381230f","externalIds":{"DBLP":"journals/fams/KonecnyR18","MAG":"2950308151","ArXiv":"1611.07555","DOI":"10.3389/fams.2018.00062","CorpusId":17068156},"title":"Randomized Distributed Mean Estimation: Accuracy vs. Communication"},{"paperId":"dbcee766687b947548803d41c66e7962f2aaebf7","externalIds":{"DBLP":"conf/icml/SureshYKM17","ArXiv":"1611.00429","MAG":"2950346378","CorpusId":479800},"title":"Distributed Mean Estimation with Limited Communication"},{"paperId":"c9d64aaa2007b60ef7814acc895dd90f15578a20","externalIds":{"MAG":"2769644379","DBLP":"conf/nips/AlistarhG0TV17","CorpusId":263894534},"title":"QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding"},{"paperId":"d2e4147eecae6f914e9e1e9aece8fdd2eaed809f","externalIds":{"ArXiv":"1609.07061","MAG":"2524428287","DBLP":"journals/corr/HubaraCSEB16","CorpusId":15817277},"title":"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"},{"paperId":"07f5bae91cd45eafe82f3548a43268eb5c84df7a","externalIds":{"ArXiv":"1608.04636","MAG":"2952651122","DBLP":"journals/corr/KarimiNS16","DOI":"10.1007/978-3-319-46128-1_50","CorpusId":9321581},"title":"Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-ojasiewicz Condition"},{"paperId":"8b053389eb8c18c61b84d7e59a95cb7e13f205b7","externalIds":{"MAG":"2469490737","ArXiv":"1606.06160","DBLP":"journals/corr/ZhouNZWWZ16","CorpusId":14395129},"title":"DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"},{"paperId":"ffa641a39315830fcb73ca78b09df69fbc180ce9","externalIds":{"MAG":"2410569288","ArXiv":"1605.09774","DBLP":"journals/corr/MitliagkasZHR16","DOI":"10.1109/ALLERTON.2016.7852343","CorpusId":6668563},"title":"Asynchrony begets momentum, with an application to deep learning"},{"paperId":"d1dbf643447405984eeef098b1b320dee0b3b8a7","externalIds":{"MAG":"2950745363","DBLP":"conf/aistats/McMahanMRHA17","ArXiv":"1602.05629","CorpusId":14955348},"title":"Communication-Efficient Learning of Deep Networks from Decentralized Data"},{"paperId":"dc8bd1f2fd5662c97e3777a695e70ed52550dc86","externalIds":{"ArXiv":"1512.09295","MAG":"2209405713","DBLP":"journals/corr/XingHXD15","DOI":"10.1016/J.ENG.2016.02.008","CorpusId":17010385},"title":"Strategies and Principles of Distributed Machine Learning on Big Data"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"7547eb271adbcb860bd47bc2da1bc8a7fce01ebc","externalIds":{"MAG":"3006487403","DBLP":"journals/ccr/ChowdhuryS15","DOI":"10.1145/2785956.2787480","CorpusId":1042322},"title":"Efficient Coflow Scheduling Without Prior Knowledge"},{"paperId":"b7cf49e30355633af2db19f35189410c8515e91f","externalIds":{"DBLP":"journals/corr/GuptaAGN15","MAG":"2950826343","ArXiv":"1502.02551","CorpusId":2547043},"title":"Deep Learning with Limited Numerical Precision"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","externalIds":{"MAG":"2964121744","DBLP":"journals/corr/KingmaB14","ArXiv":"1412.6980","CorpusId":6628106},"title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d1e4365de165463e51134f10bf3939f2b00a6667","externalIds":{"MAG":"2950678211","DBLP":"journals/corr/ZhangCL14a","ArXiv":"1412.6651","CorpusId":1275282},"title":"Deep learning with Elastic Averaging SGD"},{"paperId":"4922cd952592c854fbed1ebfaf794ed8595acba0","externalIds":{"DBLP":"conf/nips/McMahanS14","MAG":"2133941677","CorpusId":421118},"title":"Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning"},{"paperId":"04ca5de59edbdd49a9c0502c58331524d220bc8c","externalIds":{"MAG":"2127941149","DBLP":"conf/nips/LiASY14","CorpusId":2235165},"title":"Communication Efficient Distributed Machine Learning with the Parameter Server"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"3439a127e45fb763881f03ef3ec735a1db0e0ccc","externalIds":{"DBLP":"conf/interspeech/SeideFDLY14","MAG":"2407022425","DOI":"10.21437/Interspeech.2014-274","CorpusId":2189412},"title":"1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"},{"paperId":"a058935fd019c2367fd32c16cd1ce6983a29aafb","externalIds":{"MAG":"2072566913","DBLP":"conf/kdd/LiZCS14","DOI":"10.1145/2623330.2623612","CorpusId":1809834},"title":"Efficient mini-batch training for stochastic optimization"},{"paperId":"acbd13c7be621a7284da4ab9d8caa40f1a558ce2","externalIds":{"DBLP":"conf/nips/HoCCLKGGGX13","MAG":"2132737349","CorpusId":2221833,"PubMed":"25400488"},"title":"More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server"},{"paperId":"47c141bd9dc9f59090705b6336aed68bfa7ddb47","externalIds":{"MAG":"2117884704","DBLP":"conf/sigcomm/AlizadehYSKMPS13","DOI":"10.1145/2486001.2486031","CorpusId":6292321},"title":"pFabric: minimal near-optimal datacenter transport"},{"paperId":"d1208ac421cf8ff67b27d93cd19ae42b8d596f95","externalIds":{"MAG":"2162390675","DBLP":"conf/icml/CoatesHWWCN13","CorpusId":8604637},"title":"Deep learning with COTS HPC systems"},{"paperId":"aa7bfd2304201afbb19971ebde87b17e40242e91","externalIds":{"MAG":"104184427","DBLP":"conf/icml/SutskeverMDH13","CorpusId":10940950},"title":"On the importance of initialization and momentum in deep learning"},{"paperId":"3127190433230b3dc1abd0680bb58dced4bcd90e","externalIds":{"DBLP":"conf/nips/DeanCMCDLMRSTYN12","MAG":"2168231600","CorpusId":372467},"title":"Large Scale Distributed Deep Networks"},{"paperId":"84069287da0a6b488b8c933f3cb5be759cb6237e","externalIds":{"ArXiv":"1211.5063","DBLP":"conf/icml/PascanuMB13","MAG":"2949190276","CorpusId":14650762},"title":"On the difficulty of training recurrent neural networks"},{"paperId":"5b8bcb3d2b8e2f4111c61003abbfbb5963187870","externalIds":{"DBLP":"journals/corr/abs-1110-1687","ArXiv":"1110.1687","MAG":"2567769116","CorpusId":6988673},"title":"Jellyfish: Networking Data Centers Randomly"},{"paperId":"413c1142de9d91804d6d11c67ff3fed59c9fc279","externalIds":{"MAG":"2405601855","DBLP":"journals/jmlr/DuchiHS11","DOI":"10.5555/1953048.2021068","CorpusId":538820},"title":"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"},{"paperId":"a825470434b4cf7ae528b40616fd979a3b2678c7","externalIds":{"DBLP":"conf/sigcomm/FarringtonPRBSFPV10","MAG":"2119638333","DOI":"10.1145/1851182.1851223","CorpusId":954214},"title":"Helios: a hybrid electrical/optical switch architecture for modular data centers"},{"paperId":"63850d0911a0e7971e6897f6e2182729c1e2d729","externalIds":{"DBLP":"conf/sigcomm/WangAKPNKR10","MAG":"2097926925","DOI":"10.1145/1851182.1851222","CorpusId":1704947},"title":"c-Through: part-time optics in data centers"},{"paperId":"f7f38f4d4a0fc0e0da963b5166730e6321b6c171","externalIds":{"MAG":"2164740236","DBLP":"conf/sigcomm/AlizadehGMPPPSS10","DOI":"10.1145/1851182.1851192","CorpusId":2952752},"title":"Data center TCP (DCTCP)"},{"paperId":"fe33ba23625e0039b6bddf69a63f43dfe22928b1","externalIds":{"MAG":"2126210439","DBLP":"conf/sigcomm/GuoLLWZSTZL09","DOI":"10.1145/1592568.1592577","CorpusId":991482},"title":"BCube: a high performance, server-centric network architecture for modular data centers"},{"paperId":"bd2e28376aca5a8ef1cf6068a3aeb1ca6d0e1abd","externalIds":{"DBLP":"conf/sigcomm/GuoWTSZL08","MAG":"2143065961","DOI":"10.1145/1402958.1402968","CorpusId":5396194},"title":"Dcell: a scalable and fault-tolerant network structure for data centers"},{"paperId":"704ee1ed2c95bedd7808a92e879bd30cba818739","externalIds":{"DBLP":"conf/sigcomm/Al-FaresLV08","MAG":"2130531694","DOI":"10.1145/1402958.1402967","CorpusId":65842},"title":"A scalable, commodity data center network architecture"},{"paperId":"145dcba6ff585990ff051e9e0dbd52296ebda6c6","externalIds":{"DBLP":"journals/ibmrd/AdigaBCCGGHSSTTV05","MAG":"2099952176","DOI":"10.1147/rd.492.0265","CorpusId":14261404},"title":"Blue Gene/L torus interconnection network"},{"paperId":"3fe5ceaad03325a16241d3fac806911249f8beb5","externalIds":{"MAG":"1994211684","DOI":"10.1109/PROC.1967.5493","CorpusId":62491962},"title":"Results of a prototype television bandwidth compression scheme"},{"paperId":"31d3c657987119aaa1e0b8306c0abbe84bf35222","externalIds":{"DBLP":"journals/comsur/TangYWWCXGG24","DOI":"10.1109/COMST.2023.3329027","CorpusId":264938778},"title":"A Survey on Scheduling Techniques in Computing and Network Convergence"},{"paperId":"7889cc4e9565c9e7bd725509d7da4597e6a9b576","externalIds":{"DBLP":"conf/icml/WangLYCLSRZ23","CorpusId":259265981},"title":"CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks"},{"paperId":"6597b53ea7f7c1909870c9be6ed54695ec36d3b6","externalIds":{"DBLP":"conf/osdi/Hu0ZC00023","CorpusId":259858879},"title":"Hydro: Surrogate-Based Hyperparameter Tuning Service in Datacenters"},{"paperId":"9674364f0f53801b3f2ad6efcc11b32578ca7366","externalIds":{"DBLP":"conf/nsdi/WuZBL023","CorpusId":258559210},"title":"Transparent GPU Sharing in Container Clouds for Deep Learning Workloads"},{"paperId":"b6299b08bfecc60245de0cea6cebfa82c27a51b7","externalIds":{"DBLP":"conf/dasfaa/ZhouSDWLZ23","DOI":"10.1007/978-3-031-30678-5_35","CorpusId":258440287},"title":"AntTune: An Efficient Distributed Hyperparameter Optimization System for Large-Scale Data"},{"paperId":"8f4e626c2783b0fada28d206c3584ed6674ba53f","externalIds":{"DBLP":"conf/usenix/WengYY0TYZ23","CorpusId":259859102},"title":"Beware of Fragmentation: Scheduling GPU-Sharing Workloads with Fragmentation Gradient Descent"},{"paperId":"6322985bb03f3fcc2982ec8d2eba4c93e38865f0","externalIds":{"DBLP":"journals/comsur/DuanWRLZWS23","DOI":"10.1109/COMST.2022.3218527","CorpusId":253364873},"title":"Distributed Artificial Intelligence Empowered by End-Edge-Cloud Computing: A Survey"},{"paperId":"d541bd870ae3579e4b9b31645ba919c457cb3557","externalIds":{"DBLP":"journals/tpds/ZhangW22a","DOI":"10.1109/TPDS.2022.3154387","CorpusId":247131464},"title":"MIPD: An Adaptive Gradient Sparsification Framework for Distributed DNNs Training"},{"paperId":"15ee22a56b654d51a93dd01a3f2293bd6c446b9e","externalIds":{"DBLP":"conf/icml/WangXWSN22","CorpusId":250360826},"title":"DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks"},{"paperId":"22cf24bad4fa50d4a54cb2b6fce7a1d22d980058","externalIds":{"DBLP":"conf/icml/0013M22","CorpusId":250340800},"title":"Communication-efficient Distributed Learning for Large Batch Optimization"},{"paperId":"781e0b5148c5d5b710674d7274f573ccc323b3aa","externalIds":{"DBLP":"conf/icml/Yi0022","CorpusId":250360874},"title":"QSFL: A Two-Level Uplink Communication Optimization Framework for Federated Learning"},{"paperId":"da37fb0a9071d1ceda0c7e359a049c6c7e627a01","externalIds":{"DBLP":"conf/osdi/UngerJ0LBNRPMML22","CorpusId":267834645},"title":"Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization"},{"paperId":"b099ca8410efd3ebbb43259832318ead30f3249a","externalIds":{"DBLP":"conf/mlsys/ChoTCS22","CorpusId":249019912},"title":"SLA-Driven ML Inference Framework for Clouds with Hetergeneous Accelerators"},{"paperId":"de036947d8cfc1b6164d55b2470fd8020f086c4c","externalIds":{"DBLP":"conf/usenix/ChoiLKPKH22","CorpusId":252819800},"title":"Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing"},{"paperId":"167530160d46955035172f1b90ed2a94d6e13f72","externalIds":{"DBLP":"journals/jmlr/WangJ21","CorpusId":238863264},"title":"Cooperative SGD: A Unified Framework for the Design and Analysis of Local-Update SGD Algorithms"},{"paperId":"ca1402619a80c140c650961d899319c2928744f0","externalIds":{"DBLP":"conf/nips/TarnawskiNP21","CorpusId":244711821},"title":"Piper: Multidimensional Planner for DNN Parallelization"},{"paperId":"100e224fd1c43538caa34d535725dc3b539fa9bb","externalIds":{"DBLP":"journals/tpds/GuCLDCZCH22","DOI":"10.1109/TPDS.2021.3138825","CorpusId":245558327},"title":"Liquid: Intelligent Resource Estimation and Network-Efficient Scheduling for Deep Learning Jobs on Distributed GPU Clusters"},{"paperId":"920241246c82176173d442fd43214b2762be6e56","externalIds":{"DBLP":"conf/nsdi/HwangKKSP21","CorpusId":232096362},"title":"Elastic Resource Sharing for Distributed Deep Learning"},{"paperId":"d87ceea3f5d0582997840a107f2bd6c5a1aadbde","externalIds":{"DBLP":"conf/mlsys/GebaraGC21","CorpusId":233293424},"title":"In-network Aggregation for Shared Machine Learning Clusters"},{"paperId":"775a0d2671905999474cef4a76b397b266741868","externalIds":{"MAG":"3182301081","DBLP":"conf/usenix/LimAXKJ21","CorpusId":236992562},"title":"Zico: Efficient GPU Memory Sharing for Concurrent DNN Training"},{"paperId":"752afa8e951c21b52a8288700b8760e22dd1dd52","externalIds":{"DBLP":"conf/nips/XuKDLNK21","CorpusId":245011225},"title":"DeepReduce: A Sparse-tensor Communication Framework for Federated Deep Learning"},{"paperId":"021f6b1e8c328a4ce0e90dbd0d74e11e90aa4224","externalIds":{"DBLP":"conf/usenix/Romero0YK21","CorpusId":236922757},"title":"INFaaS: Automated Model-less Inference Serving"},{"paperId":"4043ad5c730d4d2f09d71aa563ff8fce7b834e35","externalIds":{"DBLP":"conf/mlsys/YuLC21","CorpusId":234502137},"title":"Fluid: Resource-aware Hyperparameter Tuning Engine"},{"paperId":"c90a5375329ea537d4ac266297b1c5cca89af19b","externalIds":{"DBLP":"conf/nsdi/LaoLMCWAS21","CorpusId":233733017},"title":"ATP: In-network Aggregation for Multi-tenant Learning"},{"paperId":"a46f01e587dc0ecb5ed3afb2b55711a0ec1472e8","externalIds":{"DBLP":"conf/icml/AvivHSL21","CorpusId":235826259},"title":"Asynchronous Distributed Learning : Adapting to Gradient Delays without Prior Knowledge"},{"paperId":"96acd6b1c1528d3f50f71083c88d84d619b5b8b8","externalIds":{"DBLP":"conf/osdi/BaiZZJ20","MAG":"3096484587","CorpusId":227177645},"title":"PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications"},{"paperId":"57bc3e71a889013981b191b6431bec3dc45eba9f","externalIds":{"MAG":"3096956001","DBLP":"conf/osdi/XiaoRLZHLFLJ20","CorpusId":229197535},"title":"AntMan: Dynamic Scaling on GPU Clusters for Deep Learning"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"6e8001afb2e24b648ae9ceb4e00c20ded4a5ee99","externalIds":{"DBLP":"conf/iclr/NeyshaburLBLS19","MAG":"2912260645","CorpusId":263864766},"title":"The role of over-parametrization in generalization of neural networks"},{"paperId":"8d2d560bf1c4c6930d2d1411e48b642bd5b81179","externalIds":{"DBLP":"conf/nsdi/GuCSZJQLG19","MAG":"2919594608","CorpusId":73725044},"title":"Tiresias: A GPU Cluster Manager for Distributed Deep Learning"},{"paperId":"124399d90a678e7652194c58cff197e5646f02fc","externalIds":{"MAG":"2975257729","DOI":"10.1007/978-3-030-26622-6_24","CorpusId":204077696},"title":"Amazons Machine Learning Toolkit: Sagemaker"},{"paperId":"9ee76c41dd161df75cb50ac06d2868afec63b0db","externalIds":{"MAG":"2405578611","DBLP":"conf/interspeech/Strom15","DOI":"10.21437/Interspeech.2015-354","CorpusId":9338808},"title":"Scalable distributed DNN training using commodity GPU cloud computing"},{"paperId":"74309511fab3a7947a45ab228c1f1c675782dd8a","externalIds":{"MAG":"2502916351","DOI":"10.1016/B978-0-12-496952-0.50023-4","CorpusId":124782887},"title":"Methods of Simultaneous Iteration for Calculating Eigenvectors of Matrices"},{"paperId":"146ac1952faf171a1c198b2023200be1d1fcd14a","externalIds":{"CorpusId":249059077},"title":"This paper is included in the Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation."}]}