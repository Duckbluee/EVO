{"abstract":"We explore the task of language-guided video segmentation (LVS). Previous algorithms mostly adopt 3D CNNs to learn video representation, struggling to capture long-term context and easily suffering from visual-linguistic misalignment. In light of this, we present <sc>Locater</sc> (<underline>lo</underline>cal-global <underline>c</underline>ontext <underline>a</underline>ware <underline>T</underline>ransform<underline>er</underline>), which augments the Transformer architecture with a finite memory so as to query the entire video with the language expression in an efficient manner. The memory is designed to involve two components â€“ one for persistently preserving global video content, and one for dynamically gathering local temporal context and segmentation history. Based on the memorized local-global context and the particular content of each frame, <sc>Locater</sc> holistically and flexibly comprehends the expression as an adaptive query vector for each frame. The vector is used to query the corresponding frame for mask generation. The memory also allows <sc>Locater</sc> to process videos with linear time complexity and constant size memory, while Transformer-style self-attention computation scales quadratically with sequence length. To thoroughly examine the visual grounding capability of LVS models, we contribute a new LVS dataset, A2D-S<inline-formula><tex-math notation=\"LaTeX\">$^+$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"yang-ieq1-3262578.gif\"/></alternatives></inline-formula>, which is built upon A2D-S dataset but poses increased challenges in disambiguating among similar objects. Experiments on three LVS datasets and our A2D-S<inline-formula><tex-math notation=\"LaTeX\">$^+$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"yang-ieq2-3262578.gif\"/></alternatives></inline-formula> show that <sc>Locater</sc> outperforms previous state-of-the-arts. Further, we won the 1<italic>st</italic> place in the Referring Video Object Segmentation Track of the 3<italic>rd</italic> Large-scale Video Object Segmentation Challenge, where <sc>Locater</sc> served as the foundation for the winning solution."}