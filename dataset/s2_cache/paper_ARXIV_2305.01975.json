{"paperId":"1a3efe3595ca2f31da443e57d1b2dba585dba6b8","externalIds":{"DBLP":"journals/corr/abs-2305-01975","ArXiv":"2305.01975","DOI":"10.48550/arXiv.2305.01975","CorpusId":258461406},"title":"A Survey on Dataset Distillation: Approaches, Applications and Future Directions","openAccessPdf":{"url":"https://arxiv.org/pdf/2305.01975","status":"GREEN","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2305.01975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2093923066","name":"Jiahui Geng"},{"authorId":"2109296817","name":"Zongxiong Chen"},{"authorId":"7707938","name":"Yuandou Wang"},{"authorId":"2215923457","name":"Herbert Woisetschlaeger"},{"authorId":"3446997","name":"Sonja Schimmler"},{"authorId":"2215923565","name":"Ruben Mayer"},{"authorId":"2146628744","name":"Zhiming Zhao"},{"authorId":"2056440506","name":"Chunming Rong"}],"abstract":"Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research."}