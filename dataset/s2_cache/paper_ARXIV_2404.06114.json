{"paperId":"d83f751e56af189402e3041e82562a342cf67aff","externalIds":{"DBLP":"journals/corr/abs-2404-06114","ArXiv":"2404.06114","DOI":"10.48550/arXiv.2404.06114","CorpusId":269009419},"title":"Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2404.06114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2295732747","name":"Feng Liang"},{"authorId":"2295793981","name":"Zhen Zhang"},{"authorId":"2115608328","name":"Haifeng Lu"},{"authorId":"2264958744","name":"Victor C. M. Leung"},{"authorId":"2296218297","name":"Yanyi Guo"},{"authorId":"2327668560","name":"Xiping Hu"}],"abstract":"With the rapid growth in the volume of data sets, models, and devices in the domain of deep learning, there is increasing attention on large-scale distributed deep learning. In contrast to traditional distributed deep learning, the large-scale scenario poses new challenges that include fault tolerance, scalability of algorithms and infrastructures, and heterogeneity in data sets, models, and resources. Due to intensive synchronization of models and sharing of data across GPUs and computing nodes during distributed training and inference processes, communication efficiency becomes the bottleneck for achieving high performance at a large scale. This article surveys the literature over the period of 2018-2023 on algorithms and technologies aimed at achieving efficient communication in large-scale distributed deep learning at various levels, including algorithms, frameworks, and infrastructures. Specifically, we first introduce efficient algorithms for model synchronization and communication data compression in the context of large-scale distributed training. Next, we introduce efficient strategies related to resource allocation and task scheduling for use in distributed training and inference. After that, we present the latest technologies pertaining to modern communication infrastructures used in distributed deep learning with a focus on examining the impact of the communication overhead in a large-scale and heterogeneous setting. Finally, we conduct a case study on the distributed training of large language models at a large scale to illustrate how to apply these technologies in real cases. This article aims to offer researchers a comprehensive understanding of the current landscape of large-scale distributed deep learning and to reveal promising future research directions toward communication-efficient solutions in this scope."}