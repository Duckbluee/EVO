{"paperId":"d048c9ad2fdf0a5155110271358e30cd24f53bda","externalIds":{"DBLP":"journals/corr/abs-2409-03752","PubMedCentral":"11873009","ArXiv":"2409.03752","DOI":"10.1016/j.patter.2025.101176","CorpusId":272423770,"PubMed":"40041856"},"title":"Attention heads of large language models","openAccessPdf":{"url":"","status":null,"license":"CCBY","disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2409.03752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2302370745","name":"Zifan Zheng"},{"authorId":"2299332188","name":"Yezhaohui Wang"},{"authorId":"2266815897","name":"Yuxin Huang"},{"authorId":"2268434524","name":"Shichao Song"},{"authorId":"2268400606","name":"Bo Tang"},{"authorId":"2268399953","name":"Feiyu Xiong"},{"authorId":"2268429641","name":"Zhiyu Li"}],"abstract":"Summary Large language models (LLMs) have demonstrated performance approaching human levels in tasks such as long-text comprehension and mathematical reasoning, but they remain black-box systems. Understanding the reasoning bottlenecks of LLMs remains a critical challenge, as these limitations are deeply tied to their internal architecture. Attention heads play a pivotal role in reasoning and are thought to share similarities with human brain functions. In this review, we explore the roles and mechanisms of attention heads to help demystify the internal reasoning processes of LLMs. We first introduce a four-stage framework inspired by the human thought process. Using this framework, we review existing research to identify and categorize the functions of specific attention heads. Additionally, we analyze the experimental methodologies used to discover these special heads and further summarize relevant evaluation methods and benchmarks. Finally, we discuss the limitations of current research and propose several potential future directions."}