{"references":[{"paperId":"559583b7cf7ca50416e2a776dc3ae3c5fb6ba700","externalIds":{"ArXiv":"2409.01652","DBLP":"journals/corr/abs-2409-01652","DOI":"10.48550/arXiv.2409.01652","CorpusId":272367253},"title":"ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation"},{"paperId":"9fbade2b7936b805f808825c46aebd604883fca9","externalIds":{"DBLP":"conf/corl/DoshiWMDL24","ArXiv":"2408.11812","DOI":"10.48550/arXiv.2408.11812","CorpusId":271915701},"title":"Scaling Cross-Embodied Learning: One Policy for Manipulation, Navigation, Locomotion and Aviation"},{"paperId":"3d148ec9c8fd2f7c204fbe4e55df9fa54f5b8f1e","externalIds":{"DBLP":"journals/corr/abs-2408-06265","ArXiv":"2408.06265","DOI":"10.1109/IROS58592.2024.10802778","CorpusId":271855106},"title":"EyeSight Hand: Design of a Fully-Actuated Dexterous Robot Hand with Integrated Vision-Based Tactile Sensors and Compliant Actuation"},{"paperId":"c3653e0f7166f3b8bb55b29a714a5ab0844b1ed8","externalIds":{"DBLP":"conf/iros/0002KL24","ArXiv":"2408.04760","DOI":"10.1109/IROS58592.2024.10801562","CorpusId":271843259},"title":"Embodied Uncertainty-Aware Object Segmentation"},{"paperId":"bb481fdcb05e8bac766814d3cae81caf54686b69","externalIds":{"ArXiv":"2407.06939","DBLP":"journals/corr/abs-2407-06939","DOI":"10.48550/arXiv.2407.06939","CorpusId":271064240},"title":"Towards Open-World Mobile Manipulation in Homes: Lessons from the Neurips 2023 HomeRobot Open Vocabulary Mobile Manipulation Challenge"},{"paperId":"8a4498bdc31ca56803db14c72a562542b79b2bc0","externalIds":{"DBLP":"conf/icra/ArenasXSJRVVHLK24","DOI":"10.1109/ICRA57147.2024.10610784","CorpusId":271800417},"title":"How to Prompt Your Robot: A PromptBook for Manipulation Skills with Code as Policies"},{"paperId":"e1f213f89df082e360fca9ed9518383ef6a90db0","externalIds":{"DBLP":"journals/corr/abs-2403-12943","ArXiv":"2403.12943","DOI":"10.48550/arXiv.2403.12943","CorpusId":268532100},"title":"Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers"},{"paperId":"b547c3888a331417a1441a243a83fed8aaa25b3e","externalIds":{"ArXiv":"2403.07563","DBLP":"conf/iros/QiuHSYFYMYASW25","DOI":"10.1109/IROS60139.2025.11246780","CorpusId":268363856},"title":"Learning Generalizable Feature Fields for Mobile Manipulation"},{"paperId":"5e878c1fe9567a65b1938ce7c930d38307ff7ff3","externalIds":{"DBLP":"journals/corr/abs-2402-19469","ArXiv":"2402.19469","DOI":"10.48550/arXiv.2402.19469","CorpusId":268091333},"title":"Humanoid Locomotion as Next Token Prediction"},{"paperId":"f1bf556dd34739fe7873047ef153439d6fdf5e97","externalIds":{"DBLP":"conf/rss/LiuOV0SP24","ArXiv":"2401.12202","DOI":"10.15607/RSS.2024.XX.091","CorpusId":267068760},"title":"Demonstrating OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics"},{"paperId":"dc4c9ae8c0cfc08ff6392aff69b0fd170da398a4","externalIds":{"ArXiv":"2312.08344","DBLP":"journals/corr/abs-2312-08344","DOI":"10.1109/CVPR52733.2024.01692","CorpusId":266191252},"title":"FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects"},{"paperId":"5a1e56d242226495e7036c83d270eb329399623c","externalIds":{"DBLP":"journals/corr/abs-2312-04549","ArXiv":"2312.04549","DOI":"10.48550/arXiv.2312.04549","CorpusId":266055193},"title":"PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play"},{"paperId":"a6c92dd8c537bc4a0c91cef8558e4a4f25416091","externalIds":{"DBLP":"conf/cvpr/KeethaKJYSRL24","ArXiv":"2312.02126","DOI":"10.1109/CVPR52733.2024.02018","CorpusId":265609060},"title":"SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM"},{"paperId":"7508634ac1312a7a975cbdf06fe754db2a1a3c09","externalIds":{"ArXiv":"2312.00267","CorpusId":265552111},"title":"Sample Efficient Preference Alignment in LLMs via Active Exploration"},{"paperId":"de2a19f2b0331e89689baf24c29868f068b16657","externalIds":{"ArXiv":"2311.16098","DBLP":"journals/corr/abs-2311-16098","DOI":"10.48550/arXiv.2311.16098","CorpusId":265455972},"title":"On Bringing Robots Home"},{"paperId":"337f421a364a6fa8ca423afd627bee2426f69395","externalIds":{"ArXiv":"2311.14379","DBLP":"journals/corr/abs-2311-14379","DOI":"10.48550/arXiv.2311.14379","CorpusId":265445462},"title":"Robot Learning in the Era of Foundation Models: A Survey"},{"paperId":"18a3ec6c7aca5fc6e21455db46b6aaff22bc1a35","externalIds":{"DBLP":"journals/corr/abs-2311-03287","ArXiv":"2311.03287","DOI":"10.48550/arXiv.2311.03287","CorpusId":265033982},"title":"Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges"},{"paperId":"1ba3adf8f2049e672c0b8786c18a1f2ffcd21fa0","externalIds":{"ArXiv":"2311.01977","DBLP":"journals/corr/abs-2311-01977","DOI":"10.48550/arXiv.2311.01977","CorpusId":265018996},"title":"RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches"},{"paperId":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","externalIds":{"DBLP":"conf/icml/WangXCWWFEHG24","ArXiv":"2311.01455","DOI":"10.48550/arXiv.2311.01455","CorpusId":264935717},"title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"},{"paperId":"0f82929fcfc9958d442009a7d50e6794f024b7f1","externalIds":{"DBLP":"journals/corr/abs-2310-17722","ArXiv":"2310.17722","DOI":"10.48550/arXiv.2310.17722","CorpusId":264555578},"title":"Large Language Models as Generalizable Policies for Embodied Tasks"},{"paperId":"043f57a38ef5da59444a92831676ea4b4c6655ab","externalIds":{"DBLP":"journals/corr/abs-2310-16299","ArXiv":"2310.16299","DOI":"10.48550/arXiv.2310.16299","CorpusId":264487194},"title":"FoundLoc: Vision-based Onboard Aerial Localization in the Wild"},{"paperId":"6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc","externalIds":{"DBLP":"journals/corr/abs-2310-12931","ArXiv":"2310.12931","DOI":"10.48550/arXiv.2310.12931","CorpusId":264306288},"title":"Eureka: Human-Level Reward Design via Coding Large Language Models"},{"paperId":"b24a191abe87ac6cb159711a56d658c75c22ecec","externalIds":{"DBLP":"conf/iclr/DuYFXWISYATKZT24","ArXiv":"2310.10625","DOI":"10.48550/arXiv.2310.10625","CorpusId":264172935},"title":"Video Language Planning"},{"paperId":"d6bd400073090b88ea535a6166ca9c164b8015b7","externalIds":{"ArXiv":"2310.10639","DBLP":"journals/corr/abs-2310-10639","DOI":"10.48550/arXiv.2310.10639","CorpusId":264172455},"title":"Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models"},{"paperId":"c3d14e7a319ab764297a60112ce74af201762a73","externalIds":{"DBLP":"journals/corr/abs-2310-06114","ArXiv":"2310.06114","DOI":"10.48550/arXiv.2310.06114","CorpusId":263830899},"title":"Learning Interactive Real-World Simulators"},{"paperId":"23cde1c9c8f4fe79e1ba5fc8f9c17bd1967f0a79","externalIds":{"DBLP":"conf/icra/YamazakiHVPTDNL24","ArXiv":"2310.03923","DOI":"10.1109/ICRA57147.2024.10610193","CorpusId":263829565},"title":"Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation"},{"paperId":"97f173e67ef937f3318f1ef41ae9fdd6a521de97","externalIds":{"DBLP":"journals/corr/abs-2310-01361","ArXiv":"2310.01361","DOI":"10.48550/arXiv.2310.01361","CorpusId":263605851},"title":"GenSim: Generating Robotic Simulation Tasks via Large Language Models"},{"paperId":"ee4395ec938e8861d6646113ddd6f4aede0d2605","externalIds":{"DOI":"10.1109/MCS.2023.3291885","CorpusId":263229790},"title":"Data-Driven Safety Filters: Hamilton-Jacobi Reachability, Control Barrier Functions, and Predictive Methods for Uncertain Systems"},{"paperId":"5d3c629c949b1293592fac13341ea5164c03f4fc","externalIds":{"ArXiv":"2310.00156","DBLP":"conf/iros/QiWYLJLH24","DOI":"10.1109/IROS58592.2024.10801653","CorpusId":263334247},"title":"Learning Generalizable Tool-use Skills through Trajectory Generation"},{"paperId":"174a3290ff0040e0f6a7a0b43bcd752c456350a0","externalIds":{"DBLP":"conf/icra/GuKMJSARPECGMTT24","ArXiv":"2309.16650","DOI":"10.1109/ICRA57147.2024.10610243","CorpusId":263134620},"title":"ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning"},{"paperId":"a265c48fd035091ff039bbfe5f2c05b9682fa341","externalIds":{"DBLP":"conf/icra/KassabMZF24","ArXiv":"2309.15065","DOI":"10.1109/ICRA57147.2024.10610341","CorpusId":262825630},"title":"Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding"},{"paperId":"8ffd6ec9c51576c84e1e04dac17c3652fe564037","externalIds":{"ArXiv":"2309.14975","DBLP":"conf/icra/FangFWRCZWL24","DOI":"10.1109/ICRA57147.2024.10610799","CorpusId":262822580},"title":"AirExo: Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild"},{"paperId":"019a2316e92de2050fda38bdb6f35e4644d93c0e","externalIds":{"DBLP":"journals/corr/abs-2309-14341","ArXiv":"2309.14341","DOI":"10.1109/ICRA57147.2024.10610200","CorpusId":262826068},"title":"Extreme Parkour with Legged Robots"},{"paperId":"b70075b496c1f519093884945be5670c32cbceed","externalIds":{"DBLP":"conf/cdc/WangZCS24","ArXiv":"2309.09969","DOI":"10.1109/CDC56724.2024.10885862","CorpusId":262044060},"title":"Prompt a Robot to Walk with Large Language Models"},{"paperId":"ee2e0077ec46704f2cb930958c9bf3739a904227","externalIds":{"DBLP":"conf/icra/YangRST24","ArXiv":"2309.09919","DOI":"10.1109/ICRA57147.2024.10611447","CorpusId":262044464},"title":"Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents"},{"paperId":"8035a247980cb18abf2bb7b9d96e7d4c63622ef2","externalIds":{"ArXiv":"2309.10103","DBLP":"journals/corr/abs-2309-10103","DOI":"10.48550/arXiv.2309.10103","CorpusId":261945162},"title":"Reasoning about the Unseen for Efficient Outdoor Object Navigation"},{"paperId":"bf89341e16fc0a379ab5e4c6370cf7ea4e9afd03","externalIds":{"DBLP":"journals/corr/abs-2309-10150","ArXiv":"2309.10150","DOI":"10.48550/arXiv.2309.10150","CorpusId":262054345},"title":"Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions"},{"paperId":"69b8cd15966c4c9c3e44e71769e557f1c87fb3f9","externalIds":{"DBLP":"conf/icra/TatiyaFWBS24","ArXiv":"2309.08508","DOI":"10.1109/ICRA57147.2024.10609998","CorpusId":262012665},"title":"MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception"},{"paperId":"2368d84435085af4cb680bcacf68a8c2fce721dc","externalIds":{"ArXiv":"2309.05837","DBLP":"journals/corr/abs-2309-05837","DOI":"10.48550/arXiv.2309.05837","CorpusId":261697421},"title":"The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems"},{"paperId":"316f980cfd2e217234386166a46eb080bf027cdd","externalIds":{"ArXiv":"2309.02561","DBLP":"conf/icra/GaoSXX0IMS24","DOI":"10.1109/ICRA57147.2024.10610090","CorpusId":261556939},"title":"Physically Grounded Vision-Language Models for Robotic Manipulation"},{"paperId":"fad72b1e1e3c1a299038f74ce5773ff1152ff914","externalIds":{"DBLP":"conf/corl/ZeYWMGY0LW23","ArXiv":"2308.16891","DOI":"10.48550/arXiv.2308.16891","CorpusId":261396262},"title":"GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields"},{"paperId":"6a375be82efc01ec4ed73334655935a56ba82d38","externalIds":{"ArXiv":"2308.12952","DBLP":"conf/corl/WalkeBZVZHHMKDL23","DOI":"10.48550/arXiv.2308.12952","CorpusId":261100981},"title":"BridgeData V2: A Dataset for Robot Learning at Scale"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","externalIds":{"DBLP":"journals/fcsc/WangMFZYZCTCLZWW24","ArXiv":"2308.11432","DOI":"10.1007/s11704-024-40231-1","CorpusId":261064713},"title":"A survey on large language model based autonomous agents"},{"paperId":"aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3","externalIds":{"DBLP":"conf/aaai/BestaBKGPGGLNNH24","ArXiv":"2308.09687","DOI":"10.1609/aaai.v38i16.29720","CorpusId":261030303},"title":"Graph of Thoughts: Solving Elaborate Problems with Large Language Models"},{"paperId":"7777ffb5f2f1d52b7080725332035b81af2994c9","externalIds":{"ArXiv":"2308.00688","DBLP":"journals/corr/abs-2308-00688","DOI":"10.1109/LRA.2023.3343602","CorpusId":260351368},"title":"AnyLoc: Towards Universal Visual Place Recognition"},{"paperId":"38939304bb760473141c2aca0305e44fbe04e6e8","externalIds":{"ArXiv":"2307.15818","DBLP":"conf/corl/ZitkovichYXXXXW23","DOI":"10.48550/arXiv.2307.15818","CorpusId":260293142},"title":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"},{"paperId":"64a257d1e5d8a18a0fbef5b25fe19413d87ef4d9","externalIds":{"DBLP":"journals/corr/abs-2308-07931","ArXiv":"2308.07931","DOI":"10.48550/arXiv.2308.07931","CorpusId":260926035},"title":"Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation"},{"paperId":"af6d0ba799213cbbcbfceb1fb9b78d2858486308","externalIds":{"DBLP":"journals/corr/abs-2307-14535","ArXiv":"2307.14535","DOI":"10.48550/arXiv.2307.14535","CorpusId":260203080},"title":"Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","externalIds":{"ArXiv":"2307.13721","DBLP":"journals/corr/abs-2307-13721","DOI":"10.48550/arXiv.2307.13721","CorpusId":260164769},"title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"3c1e43b7d3f5fd42a06c65e3aafe6d8f4a606d5c","externalIds":{"DBLP":"journals/corr/abs-2307-12698","ArXiv":"2307.12698","DOI":"10.48550/arXiv.2307.12698","CorpusId":260125019},"title":"MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features"},{"paperId":"b54798dd87b442aaa6888a1502200e9fe7a190ab","externalIds":{"DBLP":"journals/corr/abs-2307-08927","ArXiv":"2307.08927","DOI":"10.1109/TRO.2024.3353075","CorpusId":259951009},"title":"Multistage Cable Routing Through Hierarchical Imitation Learning"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","externalIds":{"DBLP":"conf/corl/HuangWZL0023","ArXiv":"2307.05973","DOI":"10.48550/arXiv.2307.05973","CorpusId":259837330},"title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"3843469976bd895851bfa08c8208350745bf649f","externalIds":{"DBLP":"conf/corl/RanaHGA0S23","ArXiv":"2307.06135","DOI":"10.48550/arXiv.2307.06135","CorpusId":259837542},"title":"SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning"},{"paperId":"d89a1cd530f83f399a0d522215c751c2ca0f3ffd","externalIds":{"DBLP":"conf/rss/ShawAP23","ArXiv":"2309.06440","DOI":"10.15607/RSS.2023.XIX.089","CorpusId":259327055},"title":"LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning"},{"paperId":"d1500f1dbd62e26ef0753f31e845078f58479968","externalIds":{"ArXiv":"2307.01928","DBLP":"conf/corl/RenDBSTBXTXVXSZ23","DOI":"10.48550/arXiv.2307.01928","CorpusId":259342058},"title":"Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners"},{"paperId":"c8ef5b72b608a9e51b506aed28fb41fce7e13a86","externalIds":{"DBLP":"conf/icra/FangFTLW0ZL24","ArXiv":"2307.00595","DOI":"10.1109/ICRA57147.2024.10611615","CorpusId":259282800},"title":"RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot"},{"paperId":"eb9fc16bd99443af8bca10f9f01f92854ab7cbbc","externalIds":{"ArXiv":"2306.17817","DBLP":"conf/corl/GervetXGF23","CorpusId":259308821},"title":"Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation"},{"paperId":"f52293edcaccf33cc16c372175a69ffa63cf3460","externalIds":{"DBLP":"journals/pami/WuLXYDY0ZT0GT24","ArXiv":"2306.15880","DOI":"10.1109/TPAMI.2024.3361862","CorpusId":259274648,"PubMed":"38315601"},"title":"Towards Open Vocabulary Learning: A Survey"},{"paperId":"dae6a9c393115ba36391d249918d743709e8aee8","externalIds":{"DBLP":"journals/corr/abs-2306-15864","ArXiv":"2306.15864","DOI":"10.48550/arXiv.2306.15864","CorpusId":259274881},"title":"What Went Wrong? Closing the Sim-to-Real Gap via Differentiable Causal Discovery"},{"paperId":"d9823ffa34f865fb1d0adef95d64a0c352ae125f","externalIds":{"DBLP":"journals/corr/abs-2306-15724","ArXiv":"2306.15724","DOI":"10.48550/arXiv.2306.15724","CorpusId":259274760},"title":"REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction"},{"paperId":"d77e806cd177a162fd20445ed6df566e08d58ced","externalIds":{"DBLP":"journals/corr/abs-2306-14846","ArXiv":"2306.14846","DOI":"10.48550/arXiv.2306.14846","CorpusId":259262396},"title":"ViNT: A Foundation Model for Visual Navigation"},{"paperId":"94bcf0390d5acb1b92323bd15cc1dc311314122c","externalIds":{"DBLP":"conf/corl/0003GFKLACEHHIX23","ArXiv":"2306.08647","DOI":"10.48550/arXiv.2306.08647","CorpusId":259164906},"title":"Language to Rewards for Robotic Skill Synthesis"},{"paperId":"cedfdde4b9d01530bf2932554561bb25623890e5","externalIds":{"DBLP":"journals/corr/abs-2305-18438","ArXiv":"2305.18438","DOI":"10.48550/arXiv.2305.18438","CorpusId":258967689},"title":"Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism"},{"paperId":"d2bf230a0229a98066b47f8635ec0b45e26b86e1","externalIds":{"DBLP":"journals/corr/abs-2305-16925","ArXiv":"2305.16925","DOI":"10.48550/arXiv.2305.16925","CorpusId":258947447},"title":"How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers"},{"paperId":"b39f0bebd19ab2e253997af95f34c78042c3c585","externalIds":{"DBLP":"journals/corr/abs-2305-16309","ArXiv":"2305.16309","DOI":"10.48550/arXiv.2305.16309","CorpusId":258887633},"title":"Imitating Task and Motion Planning with Visuomotor Transformers"},{"paperId":"5dbffedcabe3fa43060ebbe2b1789500edfd871f","externalIds":{"DBLP":"conf/emnlp/HaoGMHWWH23","ArXiv":"2305.14992","DOI":"10.48550/arXiv.2305.14992","CorpusId":258865812},"title":"Reasoning with Language Model is Planning with World Model"},{"paperId":"4367911d9d28d83fafbcf6c908698dd981ddbe9e","externalIds":{"DBLP":"journals/corr/abs-2305-15363","ArXiv":"2305.15363","DOI":"10.48550/arXiv.2305.15363","CorpusId":258865730},"title":"Inverse Preference Learning: Preference-based RL without a Reward Function"},{"paperId":"ad22af138fa1d1490cda0301abf8159a7c30c5a2","externalIds":{"DBLP":"conf/nips/DeshmukhESW23","ArXiv":"2305.11834","DOI":"10.48550/arXiv.2305.11834","CorpusId":258823141},"title":"Pengi: An Audio Language Model for Audio Tasks"},{"paperId":"afc5092a4116f27b4c64733c7815cd662bab78f7","externalIds":{"DBLP":"journals/corr/abs-2305-11176","ArXiv":"2305.11176","DOI":"10.48550/arXiv.2305.11176","CorpusId":258762636},"title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"},{"paperId":"5cac6430bd379c9d2fe13137dfd6ae7721a2679f","externalIds":{"DBLP":"conf/emnlp/ZhangLZZWZQ23","ArXiv":"2305.11000","DOI":"10.48550/arXiv.2305.11000","CorpusId":258762683},"title":"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"998d03198aa924791dea4ba2b8f44b0c9beb7b7a","externalIds":{"DBLP":"journals/corr/abs-2306-06211","ArXiv":"2306.06211","DOI":"10.48550/arXiv.2306.06211","CorpusId":259138732},"title":"A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","externalIds":{"DBLP":"journals/corr/abs-2305-05665","ArXiv":"2305.05665","DOI":"10.1109/CVPR52729.2023.01457","CorpusId":258564264},"title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a","externalIds":{"ArXiv":"2305.05658","DBLP":"journals/arobots/WuAKLZSBRF23","DOI":"10.1007/s10514-023-10139-z","CorpusId":258564887},"title":"TidyBot: Personalized Robot Assistance with Large Language Models"},{"paperId":"987d908732fcbaad6b50dcc6793eed0d538e102b","externalIds":{"DBLP":"journals/corr/abs-2305-03270","ArXiv":"2305.03270","DOI":"10.48550/arXiv.2305.03270","CorpusId":258547005},"title":"Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators"},{"paperId":"36877d3608fb391bad5a22fabd81c6669e721e69","externalIds":{"DBLP":"conf/nips/Hollmann0H23","ArXiv":"2305.03403","CorpusId":258547322},"title":"Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering"},{"paperId":"1857dc2570eb0da12b710da8eed195ce3643625d","externalIds":{"DBLP":"journals/ral/TruongZCBZTY24","ArXiv":"2305.01098","DOI":"10.1109/LRA.2024.3385611","CorpusId":258436994},"title":"IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors Without Any Outdoor Experience"},{"paperId":"5e0f2264b872288afa8c069cda381379897e13e4","externalIds":{"ArXiv":"2305.00729","DBLP":"conf/iclr/ParkKH0Y23","DOI":"10.48550/arXiv.2305.00729","CorpusId":258426737},"title":"What Do Self-Supervised Vision Transformers Learn?"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","externalIds":{"DBLP":"journals/corr/abs-2304-13712","ArXiv":"2304.13712","DOI":"10.1145/3649506","CorpusId":258331833},"title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"b3f096fe78333dd62f8d761020316078028ae196","externalIds":{"ArXiv":"2304.13089","DBLP":"journals/corr/abs-2304-13089","DOI":"10.48550/arXiv.2304.13089","CorpusId":258331753},"title":"Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations"},{"paperId":"91eb20f923ea3b0246868902aef4e9bea572b800","externalIds":{"DBLP":"journals/corr/abs-2304-13705","ArXiv":"2304.13705","DOI":"10.48550/arXiv.2304.13705","CorpusId":258331658},"title":"Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware"},{"paperId":"003ef1cd670d01af05afa0d3c72d72228f494432","externalIds":{"ArXiv":"2304.11477","DBLP":"journals/corr/abs-2304-11477","DOI":"10.48550/arXiv.2304.11477","CorpusId":258298051},"title":"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"},{"paperId":"099c2f6509391246152fbb5c2cd8757dc164ed65","externalIds":{"DBLP":"journals/corr/abs-2304-09831","ArXiv":"2304.09831","DOI":"10.48550/arXiv.2304.09831","CorpusId":258212888},"title":"FastRLAP: A System for Learning High-Speed Driving via Deep RL and Autonomous Practicing"},{"paperId":"253b41369d003952874c6a47a6038277b165cfa0","externalIds":{"DBLP":"journals/corr/abs-2304-08488","ArXiv":"2304.08488","DOI":"10.1109/CVPR52729.2023.01324","CorpusId":258180471},"title":"Affordances from Human Videos as a Versatile Representation for Robotics"},{"paperId":"bf153e35503081182aaa5a73bcf616ea95aaa210","externalIds":{"DBLP":"conf/iros/MirjaliliKB23","ArXiv":"2304.07058","DOI":"10.1109/IROS55552.2023.10342439","CorpusId":258170340},"title":"FM-Loc: Using Foundation Models for Improved Vision-Based Localization"},{"paperId":"bd05f81167ca3f77460f4a1da3bf5ade9febb15b","externalIds":{"DBLP":"conf/icml/HuS23","ArXiv":"2304.07297","DOI":"10.48550/arXiv.2304.07297","CorpusId":258179085},"title":"Language Instructed Reinforcement Learning for Human-AI Coordination"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"326f6a8011e43322c433751b9cc31fd56564621c","externalIds":{"ArXiv":"2303.18240","DBLP":"conf/nips/MajumdarYAMCSJB23","DOI":"10.48550/arXiv.2303.18240","CorpusId":257901087},"title":"Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?"},{"paperId":"8f2d4758e6d525509ae36bb30224dc9259027e6b","externalIds":{"DBLP":"journals/arobots/LinAMPB23","ArXiv":"2303.12153","DOI":"10.1007/s10514-023-10131-7","CorpusId":257663442},"title":"Text2Motion: from natural language instructions to feasible plans"},{"paperId":"dae9be0f0d815b53b46974377a0edf9169a99f3f","externalIds":{"ArXiv":"2303.12076","DBLP":"conf/corl/GuzeyECP23","DOI":"10.48550/arXiv.2303.12076","CorpusId":257636836},"title":"Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"00a48c76e123ab77f301bf4dfd88b9b376b234c6","externalIds":{"DBLP":"conf/iros/ZhaoLWHW23","ArXiv":"2303.08268","DOI":"10.1109/IROS55552.2023.10342363","CorpusId":257532768},"title":"Chat with the Environment: Interactive Multimodal Perception Using Large Language Models"},{"paperId":"35ccd924de9e8483bdcf144cbf2edf09be157b7e","externalIds":{"ArXiv":"2303.07909","DBLP":"journals/corr/abs-2303-07909","DOI":"10.48550/arXiv.2303.07909","CorpusId":257505012},"title":"Text-to-image Diffusion Models in Generative AI: A Survey"},{"paperId":"e4be613cc875e61b8c1c6c60d958f1c20d12d6c0","externalIds":{"ArXiv":"2303.06247","DBLP":"conf/iros/Ding0P023","DOI":"10.1109/IROS55552.2023.10342169","CorpusId":257496672},"title":"Task and Motion Planning with Large Language Models for Object Rearrangement"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","externalIds":{"ArXiv":"2303.05510","DBLP":"conf/iclr/ZhangCSDTG23","DOI":"10.48550/arXiv.2303.05510","CorpusId":257427177},"title":"Planning with Large Language Models for Code Generation"},{"paperId":"323400245885e08ad498cd108e30e18020662278","externalIds":{"DBLP":"conf/cvpr/XuLVBWM23","ArXiv":"2303.04803","DOI":"10.1109/CVPR52729.2023.00289","CorpusId":257405338},"title":"Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models"},{"paperId":"6536fb1be8eb17dc765f88e86a3050c6da0a4a0d","externalIds":{"DBLP":"journals/corr/abs-2303-04023","ArXiv":"2303.04023","DOI":"10.48550/arXiv.2303.04023","CorpusId":257378071},"title":"Cross-Tool and Cross-Behavior Perceptual Knowledge Transfer for Grounded Object Recognition"},{"paperId":"2ebd5df74980a37370b0bcdf16deff958289c041","externalIds":{"ArXiv":"2303.04129","DBLP":"journals/corr/abs-2303-04129","DOI":"10.48550/arXiv.2303.04129","CorpusId":257378587},"title":"Foundation Models for Decision Making: Problems, Methods, and Opportunities"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"062de1d00920686dbacde7d97970a4d1d208bbed","externalIds":{"DBLP":"conf/icra/SiYMMY23","ArXiv":"2303.02858","DOI":"10.1109/ICRA48891.2023.10161321","CorpusId":257365858},"title":"RobotSweater: Scalable, Generalizable, and Customizable Machine-Knitted Tactile Skins for Robots"},{"paperId":"9976672fd95dd1b7579117a01957f5a0c46e9d01","externalIds":{"ArXiv":"2303.00905","DBLP":"conf/corl/StoneXLGLVWKZXF23","DOI":"10.48550/arXiv.2303.00905","CorpusId":257280290},"title":"Open-World Object Manipulation using Pre-trained Vision-Language Models"},{"paperId":"81703ec2b62d5457117f71c2fb4e5f5aafd15cbf","externalIds":{"DBLP":"journals/corr/abs-2303-01284","ArXiv":"2303.01284","DOI":"10.1109/IROS55552.2023.10342226","CorpusId":257279769},"title":"NeU-NBV: Next Best View Planning Using Uncertainty Estimation in Image-Based Neural Rendering"},{"paperId":"a68db57f08f6d72c0d3b22d451d2606dca880f94","externalIds":{"DBLP":"journals/corr/abs-2302-12422","ArXiv":"2302.12422","DOI":"10.48550/arXiv.2302.12422","CorpusId":257205825},"title":"MimicPlay: Long-Horizon Imitation Learning by Watching Human Play"},{"paperId":"e701e4c02a32da186d25b08373ada12d83b73b3d","externalIds":{"DBLP":"conf/rss/YuXTSWBSTMPHIX23","ArXiv":"2302.11550","DOI":"10.48550/arXiv.2302.11550","CorpusId":257079001},"title":"Scaling Robot Learning with Semantically Imagined Experience"},{"paperId":"5e2bceb56f116e98baf7e418208057bc0e1c1861","externalIds":{"DBLP":"journals/corr/abs-2302-07241","ArXiv":"2302.07241","DOI":"10.48550/arXiv.2302.07241","CorpusId":256846496},"title":"ConceptFusion: Open-set Multimodal 3D Mapping"},{"paperId":"89e184d2bc830af568e439db9476caa0c047e11a","externalIds":{"ArXiv":"2302.06692","DBLP":"journals/corr/abs-2302-06692","DOI":"10.48550/arXiv.2302.06692","CorpusId":256846700},"title":"Guiding Pretraining in Reinforcement Learning with Large Language Models"},{"paperId":"7378afa69055e12375d55d063af87177faf37c1b","externalIds":{"ArXiv":"2302.03594","DBLP":"journals/corr/abs-2302-03594","DOI":"10.1109/3DV62453.2024.00096","CorpusId":256627303},"title":"NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM"},{"paperId":"0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3","externalIds":{"ArXiv":"2302.02662","DBLP":"journals/corr/abs-2302-02662","DOI":"10.48550/arXiv.2302.02662","CorpusId":256615643},"title":"Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning"},{"paperId":"da2fe6cd385194b0274d04d04ee72e8caf3854d4","externalIds":{"DBLP":"journals/corr/abs-2302-00111","ArXiv":"2302.00111","DOI":"10.48550/arXiv.2302.00111","CorpusId":256459809},"title":"Learning Universal Policies via Text-Guided Video Generation"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"4f0bfeadd39e64456d15d400fda8ecc2197c3265","externalIds":{"ArXiv":"2301.12842","DBLP":"conf/nips/AnLZKKS23","CorpusId":256389836},"title":"Direct Preference-based Policy Optimization without Reward Modeling"},{"paperId":"ee57e4d7a125f4ca8916284a857c3760d7d378d3","externalIds":{"DBLP":"journals/corr/abs-2301-08243","ArXiv":"2301.08243","DOI":"10.1109/CVPR52729.2023.01499","CorpusId":255999752},"title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture"},{"paperId":"eebe62c685bffd1c8880fb3ce70e3b6d42b4b9e9","externalIds":{"DBLP":"journals/corr/abs-2212-09744","ArXiv":"2212.09744","DOI":"10.48550/arXiv.2212.09744","CorpusId":254854290},"title":"DSI++: Updating Transformer Memory with New Documents"},{"paperId":"6e1c0a5f083db4ad691f54878aed36f284bde019","externalIds":{"DBLP":"journals/corr/abs-2212-08333","ArXiv":"2212.08333","DOI":"10.1109/TRO.2023.3281153","CorpusId":254823166},"title":"AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains"},{"paperId":"ec739f7e9e27792398ab446927a49c6b65295d86","externalIds":{"DBLP":"journals/corr/abs-2212-08729","ArXiv":"2212.08729","DOI":"10.48550/arXiv.2212.08729","CorpusId":254854294},"title":"Distribution-aware Goal Prediction and Conformant Model-based Planning for Safe Autonomous Driving"},{"paperId":"c77404122600594ff2f45ec50d3e36a7eacd30a3","externalIds":{"DBLP":"journals/corr/abs-2212-08244","ArXiv":"2212.08244","DOI":"10.48550/arXiv.2212.08244","CorpusId":254823478},"title":"Offline Reinforcement Learning for Visual Navigation"},{"paperId":"fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d","externalIds":{"ArXiv":"2212.06817","DBLP":"conf/rss/BrohanBCCDFGHHH23","DOI":"10.48550/arXiv.2212.06817","CorpusId":254591260},"title":"RT-1: Robotics Transformer for Real-World Control at Scale"},{"paperId":"668ef8248bf0ecfaf36cc6a6c65a4f136b976858","externalIds":{"DBLP":"conf/icml/0001YZMR0XW23","ArXiv":"2212.05749","DOI":"10.48550/arXiv.2212.05749","CorpusId":254222854},"title":"On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline"},{"paperId":"960236c4380656fa254f7e367ceb4b14cbeda45c","externalIds":{"DBLP":"journals/corr/abs-2212-00541","ArXiv":"2212.00541","DOI":"10.48550/arXiv.2212.00541","CorpusId":254125106},"title":"Predictive Sampling: Real-time Behaviour Synthesis with MuJoCo"},{"paperId":"c8fdccbbf9748636c8bc90a23bcdd5c0c1d8a43a","externalIds":{"ArXiv":"2211.11744","DBLP":"journals/scirobotics/ChenTWKAA23","DOI":"10.1126/scirobotics.adc9244","CorpusId":253734517,"PubMed":"37992192"},"title":"Visual dexterity: In-hand reorientation of novel and complex object shapes"},{"paperId":"6b3e939d93c82c269f552e7e2050524c3ad9b73b","externalIds":{"DBLP":"journals/corr/abs-2211-09790","ArXiv":"2211.09790","DOI":"10.1109/CVPR52729.2023.01440","CorpusId":253581517},"title":"ConStruct-VL: Data-Free Continual Structured VL Concepts Learning*"},{"paperId":"30477855d76058a9b542cabea3058aad1a837d51","externalIds":{"DBLP":"journals/corr/abs-2210-14739","ArXiv":"2210.14739","DOI":"10.48550/arXiv.2210.14739","CorpusId":253116622},"title":"A Case for Business Process-Specific Foundation Models"},{"paperId":"1fbe23a0d6e45b0bd7861ea33b7e51c27b1a8ef4","externalIds":{"DBLP":"journals/corr/abs-2210-13641","ArXiv":"2210.13641","DOI":"10.1109/IROS55552.2023.10341922","CorpusId":253107311},"title":"NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields"},{"paperId":"9a11350189c68e11ccf1000c0c1651dadcb00aad","externalIds":{"DBLP":"journals/corr/abs-2210-12209","ArXiv":"2210.12209","DOI":"10.48550/arXiv.2210.12209","CorpusId":253098016},"title":"Motion Policy Networks"},{"paperId":"8a79ac36b26348c1293433b68bc5e538706f321b","externalIds":{"DBLP":"journals/corr/abs-2210-10044","ArXiv":"2210.10044","DOI":"10.48550/arXiv.2210.10044","CorpusId":252968218},"title":"Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion"},{"paperId":"2763b888dbff47c7c6e095287453edb94941cf82","externalIds":{"DBLP":"journals/corr/abs-2210-08217","ArXiv":"2210.08217","DOI":"10.48550/arXiv.2210.08217","CorpusId":252918390},"title":"PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale"},{"paperId":"0e34addae55a571d7efd3a5e2543e86dd7d41a83","externalIds":{"DBLP":"journals/corr/abs-2210-06407","ArXiv":"2210.06407","DOI":"10.48550/arXiv.2210.06407","CorpusId":252846090},"title":"Interactive Language: Talking to Robots in Real Time"},{"paperId":"c305ab1bdba79442bec72ec7f5c5ee7c49c2a566","externalIds":{"DBLP":"journals/corr/abs-2210-05714","ArXiv":"2210.05714","DOI":"10.1109/ICRA48891.2023.10160969","CorpusId":252846548},"title":"Visual Language Maps for Robot Navigation"},{"paperId":"05c4bfd0b16fd0ab0e72e0866d0a5bfec5ad7ded","externalIds":{"DBLP":"conf/rss/KumarSENYFL23","ArXiv":"2210.05178","DOI":"10.15607/RSS.2023.XIX.019","CorpusId":252816019},"title":"Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials"},{"paperId":"9cf66efb5ddc0eef574f909fd4e1fa09994c0184","externalIds":{"DBLP":"journals/corr/abs-2210-05663","ArXiv":"2210.05663","DOI":"10.15607/RSS.2023.XIX.074","CorpusId":252815898},"title":"CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory"},{"paperId":"3ac400f1ca96a7ccb5a1b7790684abcb00464871","externalIds":{"DBLP":"journals/corr/abs-2210-03370","ArXiv":"2210.03370","DOI":"10.1109/ICRA48891.2023.10161227","CorpusId":252762097},"title":"GNM: A General Navigation Model to Drive Any Robot"},{"paperId":"979810ca765695a481c37126103b8ba256ee2192","externalIds":{"ArXiv":"2210.03109","DBLP":"journals/corr/abs-2210-03109","DOI":"10.48550/arXiv.2210.03109","CorpusId":252718704},"title":"Real-World Robot Learning with Masked Visual Pre-training"},{"paperId":"25425e299101b13ec2872417a14f961f4f8aa18e","externalIds":{"DBLP":"journals/corr/abs-2210-03094","ArXiv":"2210.03094","DOI":"10.48550/arXiv.2210.03094","CorpusId":252735175},"title":"VIMA: General Robot Manipulation with Multimodal Prompts"},{"paperId":"4526307d07588c94567999e32412c835259f8ebc","externalIds":{"DBLP":"journals/corr/abs-2210-01116","ArXiv":"2210.01116","DOI":"10.48550/arXiv.2210.01116","CorpusId":252683130},"title":"That Sounds Right: Auditory Self-Supervision for Dynamic Robot Manipulation"},{"paperId":"626d405aa05d96f8c1f27fa09e93b292084670b0","externalIds":{"DBLP":"journals/corr/abs-2209-11133","ArXiv":"2209.11133","DOI":"10.1109/IROS55552.2023.10342381","CorpusId":252438701},"title":"PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","externalIds":{"DBLP":"journals/corr/abs-2209-11302","ArXiv":"2209.11302","DOI":"10.1109/ICRA48891.2023.10161317","CorpusId":252519594},"title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"e00867c7108395d56b823bc5c75d2f4591e2878d","externalIds":{"DBLP":"conf/icra/ChenXIRGRSK23","ArXiv":"2209.09874","DOI":"10.1109/ICRA48891.2023.10161534","CorpusId":252383216},"title":"Open-vocabulary Queryable Scene Representations for Real World Planning"},{"paperId":"bd3a0bbabae3260098e06bfb615147fb6d34e55a","externalIds":{"DBLP":"journals/corr/abs-2209-08959","ArXiv":"2209.08959","DOI":"10.48550/arXiv.2209.08959","CorpusId":252367910},"title":"Latent Plans for Task-Agnostic Offline Reinforcement Learning"},{"paperId":"91deaf9d324c8feafc189da0da03e60a60287bca","externalIds":{"ArXiv":"2209.07753","DBLP":"conf/icra/LiangHXXHIFZ23","DOI":"10.1109/ICRA48891.2023.10160591","CorpusId":252355542},"title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"2d7c6d8bb6f4ca665f406d36738d4958a8671d15","externalIds":{"DBLP":"conf/icra/TatiyaFS23","ArXiv":"2209.06890","DOI":"10.1109/ICRA48891.2023.10160811","CorpusId":252280428},"title":"Transferring Implicit Knowledge of Non-Visual Object Properties Across Heterogeneous Robot Morphologies"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","externalIds":{"ArXiv":"2207.04429","DBLP":"conf/corl/ShahOIL22","DOI":"10.48550/arXiv.2207.04429","CorpusId":250426345},"title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"c2366e759a97c2b21e9fc35c5e2f6b377eca38ab","externalIds":{"DBLP":"journals/corr/abs-2207-02442","ArXiv":"2207.02442","DOI":"10.48550/arXiv.2207.02442","CorpusId":250311260},"title":"Transformers are Adaptable Task Planners"},{"paperId":"d28b9f65c849eba9ba2b27f7e91906f46fbe7fa1","externalIds":{"DBLP":"journals/corr/abs-2207-09450","ArXiv":"2207.09450","DOI":"10.15607/rss.2022.xviii.026","CorpusId":248941578},"title":"Human-to-Robot Imitation in the Wild"},{"paperId":"06f73c93d7902355752abbc1de330436caa9e35f","externalIds":{"ArXiv":"2206.11693","DBLP":"journals/corr/abs-2206-11693","DOI":"10.48550/arXiv.2206.11693","CorpusId":249953916},"title":"Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations"},{"paperId":"01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8","externalIds":{"ArXiv":"2206.11251","DBLP":"journals/corr/abs-2206-11251","DOI":"10.48550/arXiv.2206.11251","CorpusId":249926747},"title":"Behavior Transformers: Cloning k modes with one stone"},{"paperId":"efa4fa4120e3efe5d7384a7b3428f886de9b6a29","externalIds":{"ArXiv":"2206.08686","DBLP":"conf/nips/ChenWWFJLMDZY22","DOI":"10.48550/arXiv.2206.08686","CorpusId":249848184},"title":"Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","externalIds":{"DBLP":"journals/corr/abs-2205-14100","ArXiv":"2205.14100","DOI":"10.48550/arXiv.2205.14100","CorpusId":249152323},"title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"9c5f056c4e7986064722bb522e46e3546be8da51","externalIds":{"ArXiv":"2205.10330","DBLP":"journals/corr/abs-2205-10330","DOI":"10.48550/arXiv.2205.10330","CorpusId":248965265},"title":"A Review of Safe Reinforcement Learning: Methods, Theory and Applications"},{"paperId":"5922f437512158970c417f4413bface021df5f78","externalIds":{"DBLP":"journals/corr/abs-2205-06175","ArXiv":"2205.06175","DOI":"10.48550/arXiv.2205.06175","CorpusId":248722148},"title":"A Generalist Agent"},{"paperId":"0a40a405a1ea603530b8e933e620a0a0da4cb72e","externalIds":{"DBLP":"journals/corr/abs-2205-02953","ArXiv":"2205.02953","DOI":"10.48550/arXiv.2205.02953","CorpusId":248562557},"title":"Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"3f6d6ed110f3262fdc194184c54dd63701b3bca9","externalIds":{"DBLP":"conf/l4dc/CuiNGKR22","ArXiv":"2204.11134","DOI":"10.48550/arXiv.2204.11134","CorpusId":248377285},"title":"Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"426e8449e900f6e16a77809b23cf3507b6f89917","externalIds":{"DBLP":"journals/ral/HuGWKS23","ArXiv":"2204.03140","DOI":"10.1109/LRA.2023.3271520","CorpusId":255941966},"title":"Off-Policy Evaluation With Online Adaptation for Robot Exploration in Challenging Environments"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","externalIds":{"DBLP":"conf/corl/IchterBCFHHHIIJ22","ArXiv":"2204.01691","CorpusId":247939706},"title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"7b3d26bd1d65ed5937c76043b5cd058260d8469f","externalIds":{"DBLP":"conf/icml/ParisiRP022","ArXiv":"2203.03580","DOI":"10.48550/arXiv.2203.03580","CorpusId":247292805},"title":"The Unsurprising Effectiveness of Pre-Trained Vision Models for Control"},{"paperId":"a8cd8d3e76ce4d986f938764a086e1a3d4706973","externalIds":{"ArXiv":"2202.11762","DBLP":"journals/trob/DawsonGF23","DOI":"10.1109/TRO.2022.3232542","CorpusId":247084062},"title":"Safe Control With Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction Methods for Robotics and Control"},{"paperId":"29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d","externalIds":{"DBLP":"conf/iclr/KumarRJ0L22","ArXiv":"2202.10054","CorpusId":247011290},"title":"Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution"},{"paperId":"04248a087a834af24bfe001c9fc9ea28dab63c26","externalIds":{"DBLP":"conf/ijcai/DuLLZ22","ArXiv":"2202.10936","DOI":"10.24963/ijcai.2022/762","CorpusId":247026006},"title":"A Survey of Vision-Language Pre-Trained Models"},{"paperId":"1d803f07e4591bd67c358eef715bcd443e821894","externalIds":{"ArXiv":"2202.02005","DBLP":"journals/corr/abs-2202-02005","CorpusId":237257594},"title":"BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning"},{"paperId":"b9b220b485d2add79118ffdc2aaa148b67fa53ef","externalIds":{"DBLP":"journals/corr/abs-2202-01771","ArXiv":"2202.01771","CorpusId":246485514},"title":"Pre-Trained Language Models for Interactive Decision-Making"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","externalIds":{"DBLP":"journals/corr/abs-2201-07207","ArXiv":"2201.07207","CorpusId":246035276},"title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"cc9826c222ac1e81b4b374dd9e0df130f298b1e8","externalIds":{"ArXiv":"2201.03546","DBLP":"journals/corr/abs-2201-03546","CorpusId":245836975},"title":"Language-driven Semantic Segmentation"},{"paperId":"7002ae048e4b8c9133a55428441e8066070995cb","externalIds":{"ArXiv":"2112.10741","DBLP":"journals/corr/abs-2112-10741","CorpusId":245335086},"title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"},{"paperId":"b2d1fb4f78d24f03119f28a516ccabfc9591e71f","externalIds":{"DBLP":"journals/jmlr/MehtaPCS23","ArXiv":"2112.09153","CorpusId":245329773},"title":"An Empirical Investigation of the Role of Pre-training in Lifelong Learning"},{"paperId":"6296aa7cab06eaf058f7291040b320b5a83c0091","externalIds":{"DBLP":"journals/corr/abs-2203-00667","ArXiv":"2203.00667","DOI":"10.1109/ICCCNT56998.2023.10306417","CorpusId":1033682},"title":"Generative Adversarial Networks"},{"paperId":"3966121cb48c2195b5e6d63e3de4122284c67309","externalIds":{"ArXiv":"2112.05814","DBLP":"journals/corr/abs-2112-05814","CorpusId":245123845},"title":"Deep ViT Features as Dense Visual Descriptors"},{"paperId":"5e39c030eb7e3ad516cbe1bdcd7b8c4a9e51a6a9","externalIds":{"DBLP":"journals/corr/abs-2112-01511","ArXiv":"2112.01511","DOI":"10.15607/rss.2022.xviii.010","CorpusId":244799284},"title":"The Surprising Effectiveness of Representation Learning for Visual Imitation"},{"paperId":"4d70f10934a832a0e1121b10ec86f698d5b84836","externalIds":{"DOI":"10.1016/j.jsr.2021.12.024","CorpusId":245574717,"PubMed":"35249625"},"title":"Characterizing accident narratives with word embeddings: Improving accuracy, richness, and generalizability."},{"paperId":"776096f53cf7b1d7ad46f912b7aceacef9f6d9c8","externalIds":{"ArXiv":"2111.15150","DBLP":"journals/corr/abs-2111-15150","DOI":"10.1109/CVPR52688.2022.00822","CorpusId":244729825},"title":"AirObject: A Temporally Evolving Graph Embedding for Object Identification"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","externalIds":{"DBLP":"conf/cvpr/HeCXLDG22","ArXiv":"2111.06377","DOI":"10.1109/CVPR52688.2022.01553","CorpusId":243985980},"title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"bfba05093314e52317536b6cfc8b7fded8371e02","externalIds":{"ArXiv":"2111.03205","DBLP":"journals/corr/abs-2111-03205","CorpusId":243832934},"title":"LILA: Language-Informed Latent Actions"},{"paperId":"0ee9b633a0914b51f1eec3ad434752aa58e10149","externalIds":{"ArXiv":"2111.03043","DBLP":"conf/corl/Chen0A21","CorpusId":242757300},"title":"A System for General In-Hand Object Re-Orientation"},{"paperId":"0c41b086c2ad9958efe5adcd4975704ccf82b224","externalIds":{"ArXiv":"2111.03062","DBLP":"journals/corr/abs-2111-03062","CorpusId":242757335},"title":"Generalization in Dexterous Manipulation via Geometry-Aware Multi-Task Learning"},{"paperId":"fd399c7068512858b27535f75c8c31d2442dbaac","externalIds":{"DBLP":"conf/icra/ZhaoLLA22","ArXiv":"2110.13423","DOI":"10.1109/icra46639.2022.9812450","CorpusId":239885606},"title":"Towards More Generalizable One-shot Visual Imitation Learning"},{"paperId":"a88f35662af6a2bfe5c1e361b11010b2928c354d","externalIds":{"ArXiv":"2110.07699","CorpusId":244773757},"title":"Safe Autonomous Racing via Approximate Reachability on Ego-vision"},{"paperId":"348a855fe01f3f4273bf0ecf851ca688686dbfcc","externalIds":{"ArXiv":"2110.06169","DBLP":"journals/corr/abs-2110-06169","CorpusId":238634325},"title":"Offline Reinforcement Learning with Implicit Q-Learning"},{"paperId":"bef63d4f7656393b7bceb2ec704e86577c286166","externalIds":{"DBLP":"journals/corr/abs-2110-07342","ArXiv":"2110.07342","CorpusId":238857090},"title":"FILM: Following Instructions in Language with Modular Methods"},{"paperId":"61d4b1bb56322c85e961a0aa8b3eca84b6637982","externalIds":{"DBLP":"conf/corl/LeeDZLBSBAGKFCR21","ArXiv":"2110.06192","CorpusId":235651503},"title":"Beyond Pick-and-Place: Tackling Robotic Stacking of Diverse Shapes"},{"paperId":"b9ce8783893503c067040afbb3d3955d6773e25e","externalIds":{"ArXiv":"2109.14700","DBLP":"conf/icra/TianSBTD22","DOI":"10.1109/icra46639.2022.9812048","CorpusId":238226680},"title":"Safety Assurances for Human-Robot Interaction via Confidence-aware Game-theoretic Human Models"},{"paperId":"6bae20930eaa0d9d489317f3b3b1aaaf18205ef8","externalIds":{"DBLP":"journals/corr/abs-2109-13396","ArXiv":"2109.13396","DOI":"10.15607/rss.2022.xviii.063","CorpusId":237277709},"title":"Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets"},{"paperId":"69ee9b3a915951cc84b74599a3a2699a66d4004f","externalIds":{"DBLP":"conf/corl/ShridharMF21","ArXiv":"2109.12098","CorpusId":237396838},"title":"CLIPort: What and Where Pathways for Robotic Manipulation"},{"paperId":"67515d1f7df144683b059e684da7974e40aeaca1","externalIds":{"DBLP":"journals/corr/abs-2108-10869","ArXiv":"2108.10869","CorpusId":237278112},"title":"DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"8a90fd6370c6a6f5aa997bc0cdf183c6cacd980c","externalIds":{"DBLP":"conf/icra/CurtisFKLG22","ArXiv":"2108.04145","DOI":"10.1109/icra46639.2022.9812057","CorpusId":236956974},"title":"Long-Horizon Manipulation of Unknown Objects via Task and Motion Planning with Estimated Affordances"},{"paperId":"c3ea8eb80bc8ca0b21efa273b9e4a9fd059c65be","externalIds":{"ArXiv":"2107.07511","DBLP":"journals/corr/abs-2107-07511","CorpusId":235899036},"title":"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification"},{"paperId":"04e81615b6439dda3d0b09c6053c9e5b4abcefb4","externalIds":{"DBLP":"conf/rss/CaoZCZ21","DOI":"10.15607/RSS.2021.XVII.018","CorpusId":235399646},"title":"TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D Environments"},{"paperId":"1ca5ff6555d9fc634d3858d1fda9b3de2a91b13a","externalIds":{"DBLP":"conf/rss/KumarFPM21","ArXiv":"2107.04034","DOI":"10.15607/RSS.2021.XVII.011","CorpusId":235650916},"title":"RMA: Rapid Motor Adaptation for Legged Robots"},{"paperId":"fc70db46738fff97d9ee3d66c6f9c57794d7b4fa","externalIds":{"DBLP":"journals/corr/abs-2107-03342","ArXiv":"2107.03342","DOI":"10.1007/s10462-023-10562-9","CorpusId":235755082},"title":"A survey of uncertainty in deep neural networks"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"4ad03eb4041f18bb10cea85f6775f1f5c2393ce7","externalIds":{"DBLP":"journals/corr/abs-2106-14973","ArXiv":"2106.14973","DOI":"10.15607/RSS.2021.XVII.060","CorpusId":235650930},"title":"GIFT: Generalizable Interaction-aware Functional Tool Affordances without Labels"},{"paperId":"ada0e2e476523714a5109c8bb19588140e2314e7","externalIds":{"DBLP":"journals/jair/FrancisKLLNO22","ArXiv":"2106.13948","DOI":"10.1613/jair.1.13646","CorpusId":235658604},"title":"Core Challenges in Embodied Vision-Language Planning"},{"paperId":"7795a7a839309066694399ec35a8fce59cb1388b","externalIds":{"DBLP":"journals/corr/abs-2105-03296","ArXiv":"2105.03296","CorpusId":234094033},"title":"VIRAL SLAM: Tightly Coupled Camera-IMU-UWB-Lidar SLAM"},{"paperId":"a1bf607cdadfeb07adf16ec85199880c0f567533","externalIds":{"ArXiv":"2104.14938","DBLP":"journals/corr/abs-2104-14938","DOI":"10.1109/IROS51168.2021.9635862","CorpusId":233476522},"title":"Super Odometry: IMU-centric LiDAR-Visual-Inertial Estimator for Challenging Environments"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"cf9b8da26d9b92e75ba49616ed2a1033f59fce14","externalIds":{"DBLP":"conf/iclr/GuLKC22","ArXiv":"2104.13921","CorpusId":238744187},"title":"Open-vocabulary Object Detection via Vision and Language Knowledge Distillation"},{"paperId":"7d70e49d027634e5a070096b2735dee90ab36d26","externalIds":{"ArXiv":"2104.09025","DBLP":"journals/corr/abs-2104-09025","DOI":"10.1109/HUMANOIDS47582.2021.9555782","CorpusId":233296675},"title":"The MIT Humanoid Robot: Design, Motion Planning, and Control For Acrobatic Behaviors"},{"paperId":"3e85d208b1b927fdb69ecf8336c70995818aaebd","externalIds":{"ArXiv":"2104.08212","DBLP":"journals/corr/abs-2104-08212","CorpusId":233289968},"title":"MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale"},{"paperId":"19beac25f312ae79684d188a25430ae4e814ddcf","externalIds":{"DBLP":"journals/corr/abs-2103-11575","ArXiv":"2103.11575","DOI":"10.1109/ICCV48922.2021.00965","CorpusId":232307611},"title":"Learn-to-Race: A Multimodal Control Environment for Autonomous Racing"},{"paperId":"3d9f067d97cf21f3b0c4d406ccff98b06abafb5c","externalIds":{"DBLP":"journals/corr/abs-2103-10689","ArXiv":"2103.10689","DOI":"10.1007/s10115-022-01756-8","CorpusId":232290756},"title":"Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","externalIds":{"ArXiv":"2102.05918","DBLP":"conf/icml/JiaYXCPPLSLD21","CorpusId":231879586},"title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"4fe71923d1bf145379e809248b5ad254d1a84c75","externalIds":{"DBLP":"conf/icra/SaxenaSL21","ArXiv":"2102.04324","DOI":"10.1109/ICRA48506.2021.9561221","CorpusId":231846605},"title":"Manipulation Planning Among Movable Obstacles Using Physics-Based Adaptive Motion Primitives"},{"paperId":"08299cd7a55a544332f3815df2fe56ef9e7136d7","externalIds":{"DBLP":"conf/icra/HerbertCSGST21","ArXiv":"2101.05916","DOI":"10.1109/ICRA48506.2021.9561561","CorpusId":231627503},"title":"Scalable Learning of Safety Guarantees for Autonomous Systems using Hamilton-Jacobi Reachability"},{"paperId":"197037c0d6612d1510f98eb2a1719da05bede103","externalIds":{"ArXiv":"2101.02692","DBLP":"journals/corr/abs-2101-02692","DOI":"10.1109/ICCV48922.2021.00674","CorpusId":230800148},"title":"Where2Act: From Pixels to Actions for Articulated 3D Objects"},{"paperId":"c32a6fc742b927e72ed496743a8dcf9ca1cc415c","externalIds":{"DBLP":"journals/corr/abs-2012-11547","ArXiv":"2012.11547","CorpusId":229340500},"title":"Offline Reinforcement Learning from Images with Latent Space Models"},{"paperId":"cff6566e92e71c8fcc5cfa5d16eef34e95b1a1f3","externalIds":{"MAG":"3101483449","ArXiv":"2011.07213","DBLP":"journals/corr/abs-2011-07213","CorpusId":226964355},"title":"PLAS: Latent Action Space for Offline Reinforcement Learning"},{"paperId":"8d0eeb8aee3ce93c9c04f0662ee058e8eefee6bf","externalIds":{"MAG":"3098108114","ArXiv":"2011.05970","DBLP":"conf/corl/DasariG20","CorpusId":226299546},"title":"Transformers for One-Shot Visual Imitation"},{"paperId":"d6df8bfcdb5aea5b5e9545e51ae52bf85d775615","externalIds":{"ArXiv":"2011.00359","DBLP":"conf/corl/WangHS20","MAG":"3096240490","CorpusId":226226415},"title":"TartanVO: A Generalizable Learning-based VO"},{"paperId":"eb6b9bc4ff3e4e2cf1724324d79ce7de43131478","externalIds":{"ArXiv":"2010.14406","DBLP":"conf/corl/ZengFTWCAAKDSL20","MAG":"3096099141","CorpusId":225076003},"title":"Transporter Networks: Rearranging the Visual World for Robotic Manipulation"},{"paperId":"5ac3abe2c236642fcaff93190cb1056fb62d90a2","externalIds":{"ArXiv":"2012.03455","DBLP":"journals/corr/abs-2012-03455","MAG":"3111912394","DOI":"10.1109/IROS45743.2020.9341716","CorpusId":227334165},"title":"TP-TIO: A Robust Thermal-Inertial Odometry with Deep ThermalPoint"},{"paperId":"6f2bfacb71c4a22b45d48d4db35b98fa27b4b0a4","externalIds":{"DBLP":"conf/iros/TatiyaSES20","DOI":"10.1109/IROS45743.2020.9340770","CorpusId":231915248},"title":"Haptic Knowledge Transfer Between Heterogeneous Robots using Kernel Manifold Alignment"},{"paperId":"7ca4abace88db259faed67686ed7bba02b46eb82","externalIds":{"MAG":"3098201885","DBLP":"conf/nips/StepputtisCPLBA20","ArXiv":"2010.12083","CorpusId":225062560},"title":"Language-Conditioned Imitation Learning for Robot Manipulation Tasks"},{"paperId":"eadbe2e4f9de47dd357589cf59e3d1f0199e5075","externalIds":{"MAG":"3104876774","DBLP":"journals/scirobotics/HwangboWK020","ArXiv":"2010.11251","DOI":"10.1126/scirobotics.abc5986","CorpusId":224828219,"PubMed":"33087482"},"title":"Learning quadrupedal locomotion over challenging terrain"},{"paperId":"3adff9fb9b54600ba557cbb5f8efa52b19636101","externalIds":{"DBLP":"journals/firai/TatiyaHHS20","PubMedCentral":"7805839","MAG":"3094678614","DOI":"10.3389/frobt.2020.522141","CorpusId":222211163,"PubMed":"33501303"},"title":"A Framework for Sensorimotor Cross-Perception and Cross-Behavior Knowledge Transfer for Object Categorization"},{"paperId":"8be4f7216f2124603b4477d61f5ddb577a640fe5","externalIds":{"DBLP":"journals/corr/abs-2010-01083","MAG":"3090293438","ArXiv":"2010.01083","DOI":"10.1146/ANNUREV-CONTROL-091420-084139","CorpusId":222124824},"title":"Integrated Task and Motion Planning"},{"paperId":"5b6936905a1fcef9af31bdab49ec99581c03fd83","externalIds":{"MAG":"3044326989","DBLP":"journals/pr/DangMWPLM20","DOI":"10.1016/j.patcog.2020.107561","CorpusId":221765898},"title":"Sensor-based and vision-based human activity recognition: A comprehensive survey"},{"paperId":"e49902d212374c81b48d23c3cac1cee6e2339edd","externalIds":{"DBLP":"conf/iros/ShanEM0RR20","MAG":"3040008529","ArXiv":"2007.00258","DOI":"10.1109/IROS45743.2020.9341176","CorpusId":220280471},"title":"LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping"},{"paperId":"40c7e60a1e79954b0fe85f865ed67f09a85d0a7f","externalIds":{"DBLP":"journals/ijrr/LeungAP23","ArXiv":"2008.00097","MAG":"3046650501","DOI":"10.1177/02783649221082115","CorpusId":219557687},"title":"Backpropagation through signal temporal logic specifications: Infusing logical structure into gradient-based methods"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"38f93092ece8eee9771e61c1edaf11b1293cae1b","externalIds":{"MAG":"3101821705","DBLP":"conf/nips/GrillSATRBDPGAP20","ArXiv":"2006.07733","CorpusId":219687798},"title":"Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning"},{"paperId":"2f45d8ce0781819ff3b364886f238a3f3e6d1c04","externalIds":{"MAG":"3038825904","DBLP":"journals/corr/abs-2006-05768","ArXiv":"2006.05768","DOI":"10.15607/rss.2020.xvi.040","CorpusId":219559096},"title":"Deep Drone Acrobatics"},{"paperId":"28db20a81eec74a50204686c3cf796c42a020d2e","externalIds":{"ArXiv":"2006.04779","DBLP":"journals/corr/abs-2006-04779","MAG":"3102848167","CorpusId":219530894},"title":"Conservative Q-Learning for Offline Reinforcement Learning"},{"paperId":"dea0f1c5949f8d898b9b6ff68226a781558e413c","externalIds":{"ArXiv":"2005.13239","MAG":"3101192004","DBLP":"journals/corr/abs-2005-13239","CorpusId":218900501},"title":"MOPO: Model-based Offline Policy Optimization"},{"paperId":"1301e9d11b728268ed1ff3f1a9adc155308d5250","externalIds":{"ArXiv":"2005.07648","DBLP":"conf/rss/LynchS21","DOI":"10.15607/RSS.2021.XVII.047","CorpusId":235657751},"title":"Language Conditioned Imitation Learning Over Unstructured Data"},{"paperId":"5e7bc93622416f14e6948a500278bfbe58cd3890","externalIds":{"DBLP":"journals/corr/abs-2005-01643","ArXiv":"2005.01643","MAG":"3022566517","CorpusId":218486979},"title":"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"},{"paperId":"db3010eedc3a19ed1b4ae06f9006179db05f34bc","externalIds":{"ArXiv":"2004.05937","MAG":"3015735225","DBLP":"journals/pami/WangY22","DOI":"10.1109/TPAMI.2021.3055564","CorpusId":215745611,"PubMed":"33513099"},"title":"Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks"},{"paperId":"1803722f786b901a744bc363c0ebdc51902ceceb","externalIds":{"ArXiv":"2004.00784","MAG":"3014488508","DBLP":"journals/corr/abs-2004-00784","DOI":"10.15607/rss.2020.xvi.064","CorpusId":214775281},"title":"Learning Agile Robotic Locomotion Skills by Imitating Animals"},{"paperId":"e0d2852be2a3f5952a6600eaa933f23bee448210","externalIds":{"MAG":"3035624836","ArXiv":"2003.08515","DBLP":"conf/cvpr/XiangQMXZLLJYWY20","DOI":"10.1109/cvpr42600.2020.01111","CorpusId":213175506},"title":"SAPIEN: A SimulAted Part-Based Interactive ENvironment"},{"paperId":"f50cebbad6543a7fcde3e92df843b20f108cbb0e","externalIds":{"ArXiv":"2101.07148","MAG":"3012352454","DBLP":"journals/ijrr/IslamSAL21","DOI":"10.1177/02783649211027194","CorpusId":226208928},"title":"Provably constant-time planning and replanning for real-time grasping objects off a conveyor belt"},{"paperId":"8406903fd2f0eb25349bf071ccfaae3947e2a9cd","externalIds":{"MAG":"3035172746","DBLP":"conf/cvpr/SunKDCPTGZCCVHN20","ArXiv":"1912.04838","DOI":"10.1109/CVPR42600.2020.00252","CorpusId":209140225},"title":"Scalability in Perception for Autonomous Driving: Waymo Open Dataset"},{"paperId":"8d814620a1ca77e745bc8a33b96b86148f2804fe","externalIds":{"DBLP":"journals/corr/abs-1912-01588","MAG":"3034946435","ArXiv":"1912.01588","CorpusId":208547624},"title":"Leveraging Procedural Generation to Benchmark Reinforcement Learning"},{"paperId":"7fbeca0aaeb41b597d128c322294f5d4279ef44d","externalIds":{"MAG":"2990485036","DBLP":"journals/corr/abs-1912-00915","ArXiv":"1912.00915","DOI":"10.1609/AAAI.V34I03.5627","CorpusId":208527038},"title":"Just Ask: An Interactive Learning Framework for Vision and Language Navigation"},{"paperId":"36ccafdc1c53c17000cca27009a36da09b983685","externalIds":{"DBLP":"journals/corr/abs-1911-01103","MAG":"2984165007","ArXiv":"1911.01103","DOI":"10.1109/LRA.2020.2977835","CorpusId":207869744},"title":"Learning One-Shot Imitation From Humans Without Humans"},{"paperId":"e26f5517a6ff40529b297af26b771f46a3a1dee6","externalIds":{"DBLP":"journals/corr/abs-1911-13101","ArXiv":"1911.13101","MAG":"3037740263","DOI":"10.1609/icaps.v30i1.6754","CorpusId":208512881},"title":"Learning Domain-Independent Planning Heuristics with Hypergraph Networks"},{"paperId":"4b8a046f37b7451770e05250113fbb77de59b1f7","externalIds":{"DBLP":"conf/iros/JiangYZS19","MAG":"3004011906","DOI":"10.1109/IROS40897.2019.8967680","CorpusId":204808109},"title":"Task-Motion Planning with Reinforcement Learning for Adaptable Mobile Service Robots"},{"paperId":"9db2273fd874a70c92031d7242211db300b71d1c","externalIds":{"DBLP":"journals/corr/abs-1910-11977","ArXiv":"1910.11977","MAG":"3091470805","DOI":"10.1109/ICRA40945.2020.9196971","CorpusId":204907250},"title":"KETO: Learning Keypoint Representations for Tool Manipulation"},{"paperId":"320b227027030fc291de2896fc3c6da49d7614be","externalIds":{"DBLP":"journals/corr/abs-1910-07113","MAG":"2981030070","ArXiv":"1910.07113","CorpusId":204734323},"title":"Solving Rubik's Cube with a Robot Hand"},{"paperId":"7a450675968d31b8363e21fb5d5b72474c128076","externalIds":{"ArXiv":"1909.11652","DBLP":"journals/corr/abs-1909-11652","MAG":"2975909688","CorpusId":202750286},"title":"Deep Dynamics Models for Learning Dexterous Manipulation"},{"paperId":"4d40f7df809576d4db22b95c4ca9cc4c66e6928d","externalIds":{"MAG":"2996354442","DBLP":"conf/iclr/NairF20","ArXiv":"1909.05829","CorpusId":202565422},"title":"Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","externalIds":{"MAG":"2966715458","DBLP":"journals/corr/abs-1908-02265","ArXiv":"1908.02265","CorpusId":199453025},"title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"914d656fe5e51ef5d871d5e17f05b12870e6afef","externalIds":{"DBLP":"conf/icdl-epirob/TatiyaHHS19","MAG":"2978555422","DOI":"10.1109/DEVLRN.2019.8850715","CorpusId":203605393},"title":"Sensorimotor Cross-Behavior Knowledge Transfer for Grounded Category Recognition"},{"paperId":"d596bd32dff9a801a905c60cc871ec37c0351e47","externalIds":{"ArXiv":"1907.06013","MAG":"2961188780","DBLP":"journals/trob/QureshiMSY21","DOI":"10.1109/TRO.2020.3006716","CorpusId":196622676},"title":"Motion Planning Networks: Bridging the Gap Between Learning-Based and Classical Motion Planners"},{"paperId":"00753de4e5553de8a569e951f531bd683d8dcb16","externalIds":{"MAG":"2948199445","DBLP":"conf/iclr/ZhouJKHKWBKLF20","ArXiv":"1906.03352","CorpusId":182952687},"title":"Watch, Try, Learn: Meta-Learning from Demonstrations and Reward"},{"paperId":"326f3f55b27274f214b30aa2f7ea84ef63db7637","externalIds":{"MAG":"2937713253","DBLP":"journals/corr/abs-1904-06577","ArXiv":"1904.06577","DOI":"10.1109/TRO.2020.2991614","CorpusId":118648883},"title":"Direct Sparse Mapping"},{"paperId":"b4a35e548de27b6924e5f2ee41d37238a5c4a1d5","externalIds":{"MAG":"2929928372","ArXiv":"1904.01201","DBLP":"journals/corr/abs-1904-01201","DOI":"10.1109/ICCV.2019.00943","CorpusId":91184540},"title":"Habitat: A Platform for Embodied AI Research"},{"paperId":"2a9fd99d95d857b4d9283ea80cb6f31f72569bc6","externalIds":{"DBLP":"conf/eucc/AmesCENST19","ArXiv":"1903.11199","MAG":"2968945909","DOI":"10.23919/ECC.2019.8796030","CorpusId":85530121},"title":"Control Barrier Functions: Theory and Applications"},{"paperId":"99a7df93a2e16bd7ac3349d52cc34417cda7909d","externalIds":{"MAG":"2919872398","DBLP":"journals/corr/abs-1903-01973","ArXiv":"1903.01973","CorpusId":67877011},"title":"Learning Latent Plans from Play"},{"paperId":"bb0ee42d406f2361fee89cf1274073185a0e9eec","externalIds":{"DBLP":"journals/scirobotics/LeeDBTKH19","ArXiv":"1901.08652","MAG":"3105522894","DOI":"10.1126/scirobotics.aau5872","CorpusId":58031572,"PubMed":"33137755"},"title":"Learning agile and dynamic motor skills for legged robots"},{"paperId":"7d85e83ae00f2d19b3cdb01f5feeb92a2102104f","externalIds":{"DBLP":"conf/corl/JamesBD18","MAG":"2895558617","ArXiv":"1810.03237","CorpusId":52942444},"title":"Task-Embedded Control Networks for Few-Shot Imitation Learning"},{"paperId":"1b19f433a3e8497e9d9bd67efb108521d16b5b85","externalIds":{"MAG":"2994943647","ArXiv":"1810.08272","DBLP":"conf/iclr/Chevalier-Boisvert19","CorpusId":59536625},"title":"BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning"},{"paperId":"dd5bdbbab7077764bdc70bf24d74ed50a38d2efc","externalIds":{"DBLP":"journals/corr/abs-1808-01111","MAG":"2884962765","ArXiv":"1808.01111","DOI":"10.1109/IROS.2018.8593376","CorpusId":51921013},"title":"LDSO: Direct Sparse Odometry with Loop Closure"},{"paperId":"9691b75caa2ff4035a58f1534bbe70356a1518a6","externalIds":{"MAG":"2899757280","DOI":"10.1109/ROMAN.2018.8525627","CorpusId":53283793},"title":"A New Concept of Safety Affordance Map for Robots Object Manipulation*"},{"paperId":"e4660f26d3d752349ec72c1f7ad6d14b13ab95c9","externalIds":{"MAG":"2950413825","DBLP":"conf/iros/GraterWL18","ArXiv":"1807.07524","DOI":"10.1109/IROS.2018.8594394","CorpusId":49878621},"title":"LIMO: Lidar-Monocular Visual Odometry"},{"paperId":"eb37e7b76d26b75463df22b2a3aa32b6a765c672","externalIds":{"ArXiv":"1806.10293","DBLP":"journals/corr/abs-1806-10293","MAG":"2951747857","CorpusId":49470584},"title":"QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"},{"paperId":"e89a4fe6e8286eccedd702216153f0f248adb151","externalIds":{"DBLP":"conf/cvpr/XiaZHSMS18","ArXiv":"1808.10654","MAG":"2799034341","DOI":"10.1109/CVPR.2018.00945","CorpusId":49358881},"title":"Gibson Env: Real-World Perception for Embodied Agents"},{"paperId":"56136aa0b2c347cbcf3d50821f310c4253155026","externalIds":{"MAG":"2963960193","ArXiv":"1805.12114","DBLP":"conf/nips/ChuaCML18","CorpusId":44177328},"title":"Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"},{"paperId":"beddac10ad8dcda206088421ffeb167f491c4ae8","externalIds":{"DBLP":"journals/corr/abs-1805-07830","ArXiv":"1805.07830","MAG":"2804726947","DOI":"10.1609/aaai.v33i01.33016128","CorpusId":29168494},"title":"Learning to Teach in Cooperative Multiagent Reinforcement Learning"},{"paperId":"cf4d2e27da4ff7ee6dbd0e7a844db833444437d1","externalIds":{"MAG":"2963709735","DBLP":"conf/ijcai/YangLLG18","ArXiv":"1804.07779","DOI":"10.24963/ijcai.2018/675","CorpusId":5068169},"title":"PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making"},{"paperId":"ebc96892b9bcbf007be9a1d7844e4b09fde9d961","externalIds":{"ArXiv":"1804.02767","DBLP":"journals/corr/abs-1804-02767","MAG":"2796347433","CorpusId":4714433},"title":"YOLOv3: An Incremental Improvement"},{"paperId":"f4eff7c0127a2ef92c441f028c3bb15b64cabcc8","externalIds":{"DBLP":"journals/corr/abs-1802-01557","ArXiv":"1802.01557","MAG":"2963703448","DOI":"10.15607/RSS.2018.XIV.002","CorpusId":3618072},"title":"One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning"},{"paperId":"adfc4a14e319bd4a49b88452c74b03d7ae4400eb","externalIds":{"MAG":"2787066086","DBLP":"journals/jair/KonidarisKL18","DOI":"10.1613/jair.5575","CorpusId":31918172},"title":"From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning"},{"paperId":"a9a3ed69c94a3e1c08ef1f833d9199f57736238b","externalIds":{"MAG":"2781585732","ArXiv":"1801.00690","DBLP":"journals/corr/abs-1801-00690","CorpusId":6315299},"title":"DeepMind Control Suite"},{"paperId":"89c8aad71433f7638d2e2c009e1ea20e039f832d","externalIds":{"DBLP":"journals/corr/abs-1712-05474","MAG":"2776202271","ArXiv":"1712.05474","CorpusId":28328610},"title":"AI2-THOR: An Interactive 3D Environment for Visual AI"},{"paperId":"c37c23b12e00168833eccff8025a830ce27c5abc","externalIds":{"MAG":"2952157315","DBLP":"conf/cvpr/AndersonWTB0S0G18","ArXiv":"1711.07280","DOI":"10.1109/CVPR.2018.00387","CorpusId":4673790},"title":"Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"},{"paperId":"2980a8df6954c6b796c205dc13581d85c2708606","externalIds":{"MAG":"2768174095","DBLP":"journals/access/Robla-GomezBLGT17","DOI":"10.1109/ACCESS.2017.2773127","CorpusId":31216129},"title":"Working Together: A Review on Safe Human-Robot Collaboration in Industrial Environments"},{"paperId":"4d2c4cbb535801549371d9783a98d1e43bddf4e5","externalIds":{"MAG":"2951513524","DBLP":"journals/corr/abs-1710-09767","ArXiv":"1710.09767","CorpusId":1369182},"title":"Meta Learning Shared Hierarchies"},{"paperId":"d3c54f5fde45966865d517118506cbb66d8522f8","externalIds":{"DBLP":"conf/corl/MaedaEOB017","MAG":"2775751232","CorpusId":26344505},"title":"Active Incremental Learning of Robot Movement Primitives"},{"paperId":"1d3aa6e84299f8d36629e147679614ae09e55d2e","externalIds":{"MAG":"2978230470","DBLP":"conf/rss/PanCSLYTB18","DOI":"10.15607/RSS.2018.XIV.056","CorpusId":53873353},"title":"Agile Autonomous Driving using End-to-End Deep Imitation Learning"},{"paperId":"2d0fc3b6c8bca95f290c05ff5c64235e4c06fad4","externalIds":{"MAG":"2950432902","DBLP":"journals/corr/abs-1709-07523","ArXiv":"1709.07523","DOI":"10.1109/CDC.2017.8263977","CorpusId":35768454},"title":"Hamilton-Jacobi reachability: A brief overview and recent advances"},{"paperId":"8337441971f941716a9e525a67f37088eb01fd13","externalIds":{"MAG":"2949525187","DBLP":"journals/corr/abs-1709-06158","ArXiv":"1709.06158","DOI":"10.1109/3DV.2017.00081","CorpusId":21435690},"title":"Matterport3D: Learning from RGB-D Data in Indoor Environments"},{"paperId":"482c0cbfffa77154e3c879c497f50b605297d5bc","externalIds":{"MAG":"2755546070","ArXiv":"1709.04905","DBLP":"conf/corl/FinnYZAL17","CorpusId":22221787},"title":"One-Shot Visual Imitation Learning via Meta-Learning"},{"paperId":"01ae8022893eb7ccc87f54ab81eb6693aa527c28","externalIds":{"MAG":"2949674290","ArXiv":"1709.01490","DBLP":"conf/nips/AndersenK17","CorpusId":28593063,"PubMed":"31656387"},"title":"Active Exploration for Learning Symbolic Representations"},{"paperId":"be6712aba5b1d35c1d4062664fbd6bcdad06c71f","externalIds":{"MAG":"2753873991","ArXiv":"1709.00507","DBLP":"journals/corr/abs-1709-00507","DOI":"10.1109/CVPR.2018.00135","CorpusId":13517773},"title":"Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks"},{"paperId":"957c804e62a053911f5d2c188b71427f0ee7ca9c","externalIds":{"MAG":"2981336200","DBLP":"conf/iros/DongYA17","ArXiv":"1708.00922","DOI":"10.1109/IROS.2017.8202149","CorpusId":3580261},"title":"Improved GelSight tactile sensor for measuring geometry and slip"},{"paperId":"600bfe5f0597ebd84898f0c4270ddfb3750594f5","externalIds":{"MAG":"2949980803","DBLP":"conf/nips/RacaniereWRBGRB17","ArXiv":"1707.06203","CorpusId":13301124},"title":"Imagination-Augmented Agents for Deep Reinforcement Learning"},{"paperId":"a7680e975d395891522d3c10e3bf892f9b618048","externalIds":{"DBLP":"journals/corr/RahmatizadehABL17","MAG":"2964157221","ArXiv":"1707.02920","DOI":"10.1109/ICRA.2018.8461076","CorpusId":5043670},"title":"Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-to-End Learning from Demonstration"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"8674494bd7a076286b905912d26d47f7501c4046","externalIds":{"DBLP":"conf/nips/QiYSG17","MAG":"2950697424","ArXiv":"1706.02413","CorpusId":1745976},"title":"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"},{"paperId":"3abf64d10a5d9a426d864bcfd68daed370d6904c","externalIds":{"DBLP":"journals/corr/ZhouBSL17","MAG":"2609883120","ArXiv":"1704.07813","DOI":"10.1109/CVPR.2017.700","CorpusId":11977588},"title":"Unsupervised Learning of Depth and Ego-Motion from Video"},{"paperId":"5c57bb5630835a05eb1c3d0df3e12d6180d75de2","externalIds":{"DBLP":"journals/corr/DuanASHSSAZ17","MAG":"2951639441","ArXiv":"1703.07326","CorpusId":8270841},"title":"One-Shot Imitation Learning"},{"paperId":"7113503c645d14975667dd91f48469d2829e2082","externalIds":{"DBLP":"journals/ijrr/CalliSBWKSAD17","MAG":"2603737562","DOI":"10.1177/0278364917700714","CorpusId":6522002},"title":"Yale-CMU-Berkeley dataset for robotic manipulation research"},{"paperId":"345afa0e85cb2f5cb438ae44027499ff2c392409","externalIds":{"MAG":"2593768305","DBLP":"conf/iclr/TzengHSD17","ArXiv":"1702.05464","DOI":"10.1109/CVPR.2017.316","CorpusId":4357800},"title":"Adversarial Discriminative Domain Adaptation"},{"paperId":"ddc1d2830c5979bb07ca631fa711e1e384d5ddfa","externalIds":{"MAG":"2584986912","DOI":"10.2514/1.G001921","CorpusId":64010044},"title":"Model Predictive Path Integral Control: From Theory to Parallel Computation"},{"paperId":"60c67a6d529e8e6f55c239f5a16c80c6c4996d89","externalIds":{"MAG":"2598706937","DBLP":"conf/icra/WangCWT17","ArXiv":"1709.08429","DOI":"10.1109/ICRA.2017.7989236","CorpusId":9114952},"title":"DeepVO: Towards end-to-end visual odometry with deep Recurrent Convolutional Neural Networks"},{"paperId":"3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7","externalIds":{"MAG":"2553882142","DBLP":"conf/icml/AndreasKL17","ArXiv":"1611.01796","CorpusId":14711954},"title":"Modular Multitask Reinforcement Learning with Policy Sketches"},{"paperId":"8f92b4ea04758df2acfb49bd46a4cde923c3ddcb","externalIds":{"MAG":"2953317238","ArXiv":"1610.00696","DBLP":"conf/icra/FinnL17","DOI":"10.1109/ICRA.2017.7989324","CorpusId":2780699},"title":"Deep visual foresight for planning robot motion"},{"paperId":"d2e4147eecae6f914e9e1e9aece8fdd2eaed809f","externalIds":{"ArXiv":"1609.07061","MAG":"2524428287","DBLP":"journals/corr/HubaraCSEB16","CorpusId":15817277},"title":"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"},{"paperId":"8dd691b69dc3b58f6da9196ab02bbb1810651dc3","externalIds":{"DBLP":"books/cu/GNT2016","MAG":"2479648088","DOI":"10.1017/cbo9781139583923","CorpusId":21174343},"title":"Automated Planning and Acting"},{"paperId":"72e7f4edad44394d2bf63bb134707c8f991761c6","externalIds":{"MAG":"2950971522","DBLP":"journals/pami/EngelKC18","ArXiv":"1607.02565","DOI":"10.1109/TPAMI.2017.2658577","CorpusId":3299195,"PubMed":"28422651"},"title":"Direct Sparse Odometry"},{"paperId":"4ab53de69372ec2cd2d90c126b6a100165dc8ed1","externalIds":{"DBLP":"journals/corr/HoE16","ArXiv":"1606.03476","MAG":"2949080919","CorpusId":16153365},"title":"Generative Adversarial Imitation Learning"},{"paperId":"f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb","externalIds":{"MAG":"2953052133","DBLP":"conf/nips/FinnGL16","ArXiv":"1605.07157","CorpusId":2659157},"title":"Unsupervised Learning for Physical Interaction through Video Prediction"},{"paperId":"528a8ef7277d81d337c8b4c4fe0a9df483031773","externalIds":{"DBLP":"conf/icra/WilliamsDGRT16","MAG":"2410617946","DOI":"10.1109/ICRA.2016.7487277","CorpusId":18322548},"title":"Aggressive driving with model predictive path integral control"},{"paperId":"579735c1e5b2b0ae7fb42fcb9e2433f3118afd20","externalIds":{"DBLP":"conf/icra/HessKRA16","MAG":"2411093439","DOI":"10.1109/ICRA.2016.7487258","CorpusId":11892586},"title":"Real-time loop closure in 2D LIDAR SLAM"},{"paperId":"2d323cc0162c90cac29e62fbf4704bf2ce149643","externalIds":{"DBLP":"conf/icra/ChoudhuryGBSS16","MAG":"2416313483","DOI":"10.1109/ICRA.2016.7487615","CorpusId":14416831},"title":"Regionally accelerated batch informed trees (RABIT*): A framework to integrate local information into optimal path planning"},{"paperId":"7995dddea80c84c193aede353536f730e13fedca","externalIds":{"MAG":"2343568200","DBLP":"journals/tiv/PadenCYYF16","ArXiv":"1604.07446","DOI":"10.1109/TIV.2016.2578706","CorpusId":1229096},"title":"A Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles"},{"paperId":"1fa9e326274b74aba5b9973ec057dd679cbab785","externalIds":{"MAG":"3105287169","DBLP":"journals/trob/BohgHSBKSS17","ArXiv":"1604.03670","DOI":"10.1109/TRO.2017.2721939","CorpusId":6144047},"title":"Interactive Perception: Leveraging Action in Perception and Perception in Action"},{"paperId":"d93ececdc44e7700cd84fc75ca069125022d8c9d","externalIds":{"DBLP":"conf/eccv/PintoGHPG16","MAG":"2338684808","ArXiv":"1604.01360","DOI":"10.1007/978-3-319-46475-6_1","CorpusId":7469546},"title":"The Curious Robot: Learning Visual Representations via Physical Interactions"},{"paperId":"494e2d5b40dcebde349f9872c7317e5003f9c5d2","externalIds":{"MAG":"2962736495","DBLP":"journals/ijrr/LevinePKIQ18","ArXiv":"1603.02199","DOI":"10.1177/0278364917710318","CorpusId":13072941},"title":"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection"},{"paperId":"846aedd869a00c09b40f1f1f35673cb22bc87490","externalIds":{"DBLP":"journals/nature/SilverHMGSDSAPL16","MAG":"2257979135","DOI":"10.1038/nature16961","CorpusId":515925,"PubMed":"26819042"},"title":"Mastering the game of Go with deep neural networks and tree search"},{"paperId":"88d5f63af893be00ab021f918e944f58f5f04abe","externalIds":{"MAG":"2143502460","DOI":"10.1146/annurev-psych-122414-033634","CorpusId":33609451,"PubMed":"26393868"},"title":"Modular Brain Networks."},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","externalIds":{"MAG":"2953106684","ArXiv":"1506.01497","DBLP":"journals/pami/RenHG017","DOI":"10.1109/TPAMI.2016.2577031","CorpusId":10328909,"PubMed":"27295650"},"title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"ac7215af97ed57f073dd3bb03188ac313e62e5fb","externalIds":{"MAG":"1512698229","DBLP":"conf/icra/ZhangS15","DOI":"10.1109/ICRA.2015.7139486","CorpusId":6054487},"title":"Visual-lidar odometry and mapping: low-drift, robust, and fast"},{"paperId":"b6b8a1b80891c96c28cc6340267b58186157e536","externalIds":{"MAG":"2952125531","DBLP":"journals/jmlr/LevineFDA16","ArXiv":"1504.00702","DOI":"10.5555/2946645.2946684","CorpusId":7242892},"title":"End-to-End Training of Deep Visuomotor Policies"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"6933c70c747e6a8103f68f1a1db80185401d537b","externalIds":{"DBLP":"journals/corr/Mur-ArtalMT15","MAG":"1612997784","ArXiv":"1502.00956","DOI":"10.1109/TRO.2015.2463671","CorpusId":206775100},"title":"ORB-SLAM: A Versatile and Accurate Monocular SLAM System"},{"paperId":"6ef28af882e408ff63f83ca670392a008d203fbc","externalIds":{"DBLP":"conf/icra/SinapovSS14","MAG":"2070338807","DOI":"10.1109/ICRA.2014.6907696","CorpusId":10709733},"title":"Learning relational object categories using behavioral exploration and multimodal perception"},{"paperId":"c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2","externalIds":{"MAG":"612478963","DBLP":"conf/eccv/EngelSC14","DOI":"10.1007/978-3-319-10605-2_54","CorpusId":14547347},"title":"LSD-SLAM: Large-Scale Direct Monocular SLAM"},{"paperId":"e0dcca4fa447a14496f0143c211fd40a54039b98","externalIds":{"MAG":"2075673359","DBLP":"conf/iros/GemiciS14","DOI":"10.1109/IROS.2014.6942626","CorpusId":16338892},"title":"Learning haptic representation for manipulating deformable food objects"},{"paperId":"4d0fc815dced9db3954ed21060e9894e1555ee3d","externalIds":{"MAG":"2296228853","DBLP":"conf/rss/ZhangS14","DOI":"10.15607/RSS.2014.X.007","CorpusId":18612391},"title":"LOAM: Lidar Odometry and Mapping in Real-time"},{"paperId":"2b1e7849bf426ef0d2c069fd6e0e386cbbb1bf3e","externalIds":{"DBLP":"journals/ijrr/AineSNHL16","MAG":"1620286322","DOI":"10.1177/0278364915594029","CorpusId":1842081},"title":"Multi-Heuristic A*"},{"paperId":"36ec49dfdbf237fb80c1d655ff0ad981df6de87d","externalIds":{"MAG":"1814533834","ArXiv":"1405.5848","DBLP":"conf/icra/GammellSB15","DOI":"10.1109/ICRA.2015.7139620","CorpusId":15776115},"title":"Batch Informed Trees (BIT*): Sampling-based optimal planning via the heuristically guided search of implicit random geometric graphs"},{"paperId":"b4ab2555d5690e8e6fb1cf23c995a120181698a6","externalIds":{"DBLP":"journals/ras/SinapovSSSS14","MAG":"2054228171","DOI":"10.1016/j.robot.2012.10.007","CorpusId":29066714},"title":"Grounding semantic categories in behavioral interactions: Experiments with 100 objects"},{"paperId":"864b8e56e09285fa75611f83cdc8fb6ea0142f35","externalIds":{"MAG":"2119562960","DBLP":"journals/ijrr/CohenCL14","DOI":"10.1177/0278364913507983","CorpusId":10263949},"title":"Single- and dual-arm motion planning with heuristic search"},{"paperId":"2319a491378867c7049b3da055c5df60e1671158","externalIds":{"DBLP":"journals/corr/MnihKSGAWR13","MAG":"1757796397","ArXiv":"1312.5602","CorpusId":15238391},"title":"Playing Atari with Deep Reinforcement Learning"},{"paperId":"b8de958fead0d8a9619b55c7299df3257c624a96","externalIds":{"DBLP":"journals/corr/DonahueJVHZTD13","MAG":"2155541015","ArXiv":"1310.1531","CorpusId":6161478},"title":"DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"},{"paperId":"79b949d9b35c3f51dd20fb5c746cc81fc87147eb","externalIds":{"MAG":"2115579991","DBLP":"journals/ijrr/GeigerLSU13","DOI":"10.1177/0278364913491297","CorpusId":9455111},"title":"Vision meets robotics: The KITTI dataset"},{"paperId":"65438e0ba226c1f97bd8a36333ebc3297b1a32fd","externalIds":{"DBLP":"journals/ijrr/KoberBP13","MAG":"1977655452","DOI":"10.1177/0278364913495721","CorpusId":1932843},"title":"Reinforcement learning in robotics: A survey"},{"paperId":"f700a44dd55960f249734d6c19bbdcf899453ba3","externalIds":{"MAG":"1975492090","DBLP":"conf/icra/MacAllisterBKPL13","DOI":"10.1109/ICRA.2013.6631131","CorpusId":8024380},"title":"Path planning for non-circular micro aerial vehicles in constrained environments"},{"paperId":"5854a5bce7ae24358c5f14c519a7adf9d7a19579","externalIds":{"DBLP":"conf/iros/NarayananPL12","MAG":"1971648593","DOI":"10.1109/IROS.2012.6386191","CorpusId":8617955},"title":"Anytime Safe Interval Path Planning for dynamic environments"},{"paperId":"b354ee518bfc1ac0d8ac447eece9edb69e92eae1","externalIds":{"MAG":"2158782408","DBLP":"conf/iros/TodorovET12","DOI":"10.1109/IROS.2012.6386109","CorpusId":5230692},"title":"MuJoCo: A physics engine for model-based control"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"f82e4ff4f003581330338aaae71f60316e58dd26","externalIds":{"ArXiv":"1207.4708","DBLP":"journals/jair/BellemareNVB13","MAG":"2150468603","DOI":"10.1613/jair.3912","CorpusId":1552061},"title":"The Arcade Learning Environment: An Evaluation Platform for General Agents"},{"paperId":"81f21574260d03fbe68ac73bc6a1da2c9645301d","externalIds":{"MAG":"1967740178","DOI":"10.1109/SSRR.2011.6106777","CorpusId":14113246},"title":"A flexible and scalable SLAM system with full 3D motion estimation"},{"paperId":"4326d7e9933c77ff9dc53056c62ef6712d90c633","externalIds":{"DBLP":"journals/ijrr/KaramanF11","MAG":"2952108591","ArXiv":"1105.1186","DOI":"10.1177/0278364911406761","CorpusId":14876957},"title":"Sampling-based algorithms for optimal motion planning"},{"paperId":"79ab3c49903ec8cb339437ccf5cf998607fc313e","externalIds":{"MAG":"2962957031","DBLP":"journals/jmlr/RossGB11","ArXiv":"1011.0686","CorpusId":103456},"title":"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"},{"paperId":"6938d305ee3a51b4197580d1174c355248d2c0ff","externalIds":{"MAG":"2021947606","PubMedCentral":"3000003","DOI":"10.3389/fnins.2010.00200","CorpusId":2803311,"PubMed":"21151783"},"title":"Modular and Hierarchically Modular Organization of Brain Networks"},{"paperId":"ec4baaa80606883c0e582aff5f925c993fd138fc","externalIds":{"MAG":"2162457740","DBLP":"journals/tamd/CangelosiMSNNFTBSNFWRTDSZ10","DOI":"10.1109/TAMD.2010.2053034","CorpusId":171437},"title":"Integration of Action and Language Knowledge: A Roadmap for Developmental Robotics"},{"paperId":"c8221c054459e37edbf313668523d667fe5c1536","externalIds":{"DBLP":"conf/aaai/ZiebartMBD08","MAG":"2098774185","CorpusId":336219},"title":"Maximum Entropy Inverse Reinforcement Learning"},{"paperId":"d79e789878d1467dbd74cf53e7edb8e850ee5f7c","externalIds":{"DBLP":"journals/arobots/SkubicABPS07","MAG":"2043242772","DOI":"10.1007/s10514-007-9023-1","CorpusId":1932786},"title":"Using a hand-drawn sketch to control a team of robots"},{"paperId":"df0295715f8b43a8cbaa19f74a3dceb06e610eaf","externalIds":{"MAG":"2167340365","DBLP":"conf/iros/KoenigH04","DOI":"10.1109/IROS.2004.1389727","CorpusId":206941306},"title":"Design and use paradigms for Gazebo, an open-source multi-robot simulator"},{"paperId":"f65020fc3b1692d7989e099d6b6e698be5a50a93","externalIds":{"MAG":"1999874108","DBLP":"conf/icml/PieterN04","DOI":"10.1145/1015330.1015430","CorpusId":207155342},"title":"Apprenticeship learning via inverse reinforcement learning"},{"paperId":"be2f6bb4e262d7015b931c12eb1559827ba597dc","externalIds":{"DBLP":"journals/ras/BillardS04","MAG":"2034600737","DOI":"10.1016/J.ROBOT.2004.03.001","CorpusId":36966019},"title":"Robot learning from demonstration"},{"paperId":"2b82038cde0c75e3ecd8917a8b75a78129d6e60c","externalIds":{"MAG":"2218328592","DOI":"10.5040/9781350033337.0009","CorpusId":203958713},"title":"/ Collaboration "},{"paperId":"0f98f000da79132ef80a521ab8fad0396b82c7f5","externalIds":{"DBLP":"conf/icra/KuffnerL00","MAG":"2141664020","DOI":"10.1109/ROBOT.2000.844730","CorpusId":17124403},"title":"RRT-connect: An efficient approach to single-query path planning"},{"paperId":"7e81dac6260c4768af3a28ac21c78c5a38a5f7d0","externalIds":{"MAG":"2128990851","DBLP":"journals/trob/KavrakiSLO96","DOI":"10.1109/70.508439","CorpusId":18974595},"title":"Probabilistic roadmaps for path planning in high-dimensional configuration spaces"},{"paperId":"c7842aa48e030230a66813562884120bcc58c845","externalIds":{"DOI":"10.5694/j.1326-5377.1994.tb127444.x","CorpusId":54147632,"PubMed":"27653138"},"title":"What's on where"},{"paperId":"221aa3be55a4ead8fc2aa83b12aac370bfba72f5","externalIds":{"MAG":"2183506185","DBLP":"journals/tssc/HartNR68","DOI":"10.1109/TSSC.1968.300136","CorpusId":206799161},"title":"A Formal Basis for the Heuristic Determination of Minimum Cost Paths"},{"paperId":"06d8562831c32844285a691c5250d04726df3c61","externalIds":{"DBLP":"journals/corr/abs-2307-12980","DOI":"10.48550/arXiv.2307.12980","CorpusId":260357841},"title":"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models"},{"paperId":"33af17e7cc5cd4ee18c9e6e8c5fca7e224592ec0","externalIds":{"DBLP":"journals/corr/abs-2303-00855","DOI":"10.48550/arXiv.2303.00855","CorpusId":257279977},"title":"Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control"},{"paperId":"c52b30a60fda5f23cc0d2241c4e127f5191bbb2d","externalIds":{"DBLP":"journals/corr/abs-2309-11489","DOI":"10.48550/arXiv.2309.11489","CorpusId":262053612},"title":"Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning"},{"paperId":"130dc560a0f57fc70f865e3313d28b14401ef2e2","externalIds":{"DBLP":"journals/corr/abs-2311-00530","DOI":"10.48550/arXiv.2311.00530","CorpusId":270491208},"title":"The Development of LLMs for Embodied Navigation"},{"paperId":"775f42ed458b8c5b0f2094ea4ff5b64c557b1a34","externalIds":{"CorpusId":251881108},"title":"A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27"},{"paperId":"bbdfe5cc3ac04a6a9da3c817d14bf39888e8a302","externalIds":{"DBLP":"phd/us/Francis22a","DOI":"10.1184/r1/21708182.v1","CorpusId":279310117},"title":"Knowledge-enhanced Representation Learning for Multiview Context Understanding"},{"paperId":"3be5199010012448af287767297d81aa4a87567c","externalIds":{"DBLP":"conf/corl/ZhaoHL21","CorpusId":237407553},"title":"Model-free Safe Control for Zero-Violation Reinforcement Learning"},{"paperId":"7094b1f86944517d0b03cf466be63ddf5780c59d","externalIds":{"DBLP":"conf/corl/KimS19","MAG":"3031205423","CorpusId":204875284},"title":"Learning value functions with relational state representations for guiding task-and-motion planning"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"b7c1bdf7d9c92f7d38aeb2ca106189bffc903f5c","externalIds":{"DBLP":"books/lib/SuttonB2018","CorpusId":277058247},"title":"Reinforcement learning - an introduction, 2nd Edition"},{"paperId":"a97f3d2313affd35c889c57f2ebe21e7ba2b5bbb","externalIds":{"MAG":"2765757095","DBLP":"conf/fsr/MaturanaCUS17","DOI":"10.1007/978-3-319-67361-5_22","CorpusId":19216802},"title":"Real-Time Semantic Mapping for Autonomous Off-Road Navigation"},{"paperId":"4159a063e6b9583c5620af0b9e17c683600a84b1","externalIds":{"MAG":"2489898654","DBLP":"reference/robo/MarshallBNS16","DOI":"10.1007/978-3-319-32552-1_59","CorpusId":19796381},"title":"Robotics in Mining"},{"paperId":"d967d9550f831a8b3f5cb00f8835a4c866da60ad","externalIds":{"MAG":"131069610","CorpusId":14744621},"title":"Rapidly-exploring random trees : a new tool for path planning"},{"paperId":"76f4fbab7781c4e68989055add8a8cce127c5f5f","externalIds":{"DOI":"10.1093/ptj/47.9.873","CorpusId":43410754,"PubMed":"18737841"},"title":"WHAT'S THE ANSWER?"}]}