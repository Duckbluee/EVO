{"references":[{"paperId":"c8a93db7e0d56adbc18217de5138fe0bde570c01","externalIds":{"ArXiv":"2404.11978","DBLP":"journals/corr/abs-2404-11978","DOI":"10.48550/arXiv.2404.11978","CorpusId":269214199},"title":"EVIT: Event-Oriented Instruction Tuning for Event Reasoning"},{"paperId":"a1677ca8f7db7deceeae70eec0e8a02287cc528b","externalIds":{"DBLP":"journals/corr/abs-2404-10429","ArXiv":"2404.10429","DOI":"10.48550/arXiv.2404.10429","CorpusId":269157587},"title":"MEEL: Multi-Modal Event Evolution Learning"},{"paperId":"f3c339ab479cbd4782807bf47254961bc60bf293","externalIds":{"ArXiv":"2404.00599","DBLP":"journals/corr/abs-2404-00599","DOI":"10.48550/arXiv.2404.00599","CorpusId":268819731},"title":"EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories"},{"paperId":"094b6847434be00e41686529f45d895bd632f68a","externalIds":{"ArXiv":"2403.20046","DBLP":"conf/acl/TongLWWTS24","DOI":"10.48550/arXiv.2403.20046","CorpusId":268793529},"title":"Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning"},{"paperId":"7c8851cce662351c49da94fa4512e2a6d2c1ace0","externalIds":{"ArXiv":"2403.15042","DBLP":"journals/corr/abs-2403-15042","DOI":"10.48550/arXiv.2403.15042","CorpusId":268666980},"title":"LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement"},{"paperId":"828f98e0feba2baa55a5486f354fd074cca0880c","externalIds":{"DBLP":"journals/natmi/AkibaSTSH25","ArXiv":"2403.13187","DOI":"10.1038/s42256-024-00975-8","CorpusId":268537290},"title":"Evolutionary optimization of model merging recipes"},{"paperId":"77e3b253c2deeb2661ffcb9fb9f952ceb700c4db","externalIds":{"DBLP":"journals/corr/abs-2403-08715","ArXiv":"2403.08715","DOI":"10.48550/arXiv.2403.08715","CorpusId":268379635},"title":"SOTOPIA-π: Interactive Learning of Socially Intelligent Language Agents"},{"paperId":"a8b1484d3ee6b9ad6e4c48d6bcdbd1048493599d","externalIds":{"ArXiv":"2403.08281","DBLP":"journals/corr/abs-2403-08281","DOI":"10.48550/arXiv.2403.08281","CorpusId":268379027},"title":"Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models"},{"paperId":"9ce1b7190601bf67673716e70dc4fa9fe2ec872e","externalIds":{"ArXiv":"2403.07952","DBLP":"journals/corr/abs-2403-07952","DOI":"10.48550/arXiv.2403.07952","CorpusId":268379752},"title":"AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production"},{"paperId":"95dad62c52800600e571b4197314578fd441ca28","externalIds":{"DBLP":"journals/corr/abs-2403-03101","ArXiv":"2403.03101","DOI":"10.48550/arXiv.2403.03101","CorpusId":268248897},"title":"KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents"},{"paperId":"002a2f9da012bcf37e59c7ea0406f6bf11e5ab55","externalIds":{"ArXiv":"2403.02757","DBLP":"journals/corr/abs-2403-02757","DOI":"10.48550/arXiv.2403.02757","CorpusId":268248582},"title":"In-Memory Learning: A Declarative Learning Framework for Large Language Models"},{"paperId":"f95da5b7be2fac2381eb5dfe26dc7dc5bc2d9a90","externalIds":{"DBLP":"journals/corr/abs-2403-02502","ArXiv":"2403.02502","DOI":"10.48550/arXiv.2403.02502","CorpusId":268249221},"title":"Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents"},{"paperId":"015f62d7a59f7a4301c0cdbe997460c38148d07b","externalIds":{"DBLP":"conf/acl/HuangCWYLSYS24","ArXiv":"2403.01244","DOI":"10.48550/arXiv.2403.01244","CorpusId":268230393},"title":"Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal"},{"paperId":"789485978d69e248832df358ee0fb062012925b8","externalIds":{"ArXiv":"2402.17574","DBLP":"conf/acl/ZhangTWW0HTLZ024","DOI":"10.48550/arXiv.2402.17574","CorpusId":268032624},"title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization"},{"paperId":"ac7c359ce903cdc104d82298537c61447d8cfe81","externalIds":{"DBLP":"journals/corr/abs-2402-17358","ArXiv":"2402.17358","DOI":"10.48550/arXiv.2402.17358","CorpusId":268032032},"title":"SoFA: Shielded On-the-fly Alignment via Priority Rule Following"},{"paperId":"c6c584ea3983627329eebff7c78951938b227351","externalIds":{"ArXiv":"2402.16906","DBLP":"conf/acl/Zhong0S24","DOI":"10.18653/v1/2024.findings-acl.49","CorpusId":268032812},"title":"Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step by Step"},{"paperId":"c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a","externalIds":{"ArXiv":"2402.13669","DBLP":"conf/acl/YangPFWCZL24","DOI":"10.48550/arXiv.2402.13669","CorpusId":267769989},"title":"Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning"},{"paperId":"de6ddb30b07f192f2be142062c4c6c817e508d96","externalIds":{"ArXiv":"2402.11907","DBLP":"journals/corr/abs-2402-11907","DOI":"10.48550/arXiv.2402.11907","CorpusId":267750144},"title":"Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation"},{"paperId":"9be1b9b8b6e97e31c3e16c0263c54b337f46baa2","externalIds":{"ArXiv":"2402.11792","DBLP":"journals/corr/abs-2402-11792","DOI":"10.48550/arXiv.2402.11792","CorpusId":267750355},"title":"SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction"},{"paperId":"7938da5e88245d1769c23fbe0a3b32ce83fa25e1","externalIds":{"DBLP":"journals/corr/abs-2402-11778","ArXiv":"2402.11778","DOI":"10.48550/arXiv.2402.11778","CorpusId":267751048},"title":"Towards Theoretical Understandings of Self-Consuming Generative Models"},{"paperId":"eb6eba90a399e6a8f5810f9b1fde6db551b4a009","externalIds":{"DBLP":"journals/corr/abs-2402-11633","ArXiv":"2402.11633","DOI":"10.48550/arXiv.2402.11633","CorpusId":267750805},"title":"Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs"},{"paperId":"ee85c7c666135f4aae32336968f09584029b6a35","externalIds":{"DBLP":"conf/acl/LiCCHGZ24","ArXiv":"2402.10110","DOI":"10.48550/arXiv.2402.10110","CorpusId":267682220},"title":"Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning"},{"paperId":"32c5b515cab893e5e4bf3f90c8b6c8262bd7ac09","externalIds":{"ArXiv":"2402.09267","DBLP":"conf/acl/ZhangPTZJSMM24","DOI":"10.48550/arXiv.2402.09267","CorpusId":267657805},"title":"Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation"},{"paperId":"38472e4242e0aa632ed594c3b0ed9c0bd6429c41","externalIds":{"DBLP":"journals/tiis/SchoeneggerPKTT25","ArXiv":"2402.07862","DOI":"10.1145/3707649","CorpusId":267627630},"title":"AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy"},{"paperId":"c16f5a6e4c51be0baa49275df441748134d67234","externalIds":{"DBLP":"journals/corr/abs-2402-07610","ArXiv":"2402.07610","DOI":"10.48550/arXiv.2402.07610","CorpusId":267627906},"title":"Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping"},{"paperId":"4fe4f0f9d39d708a6c3d7b8dfbfa2616cd376e1e","externalIds":{"DBLP":"journals/corr/abs-2402-06457","ArXiv":"2402.06457","DOI":"10.48550/arXiv.2402.06457","CorpusId":267617275},"title":"V-STaR: Training Verifiers for Self-Taught Reasoners"},{"paperId":"512c3a8b0d5462fe8a98a1713a02d1a3186e3aae","externalIds":{"ArXiv":"2402.05200","DBLP":"journals/corr/abs-2402-05200","DOI":"10.48550/arXiv.2402.05200","CorpusId":267547479},"title":"Are LLMs Ready for Real-World Materials Discovery?"},{"paperId":"f503b95c0a64f6a84eb1d90e5ea1e094b1e1892b","externalIds":{"DBLP":"journals/corr/abs-2402-04049","ArXiv":"2402.04049","ACL":"2024.emnlp-main.16","DOI":"10.18653/v1/2024.emnlp-main.16","CorpusId":267499945},"title":"Systematic Biases in LLM Simulations of Debates"},{"paperId":"bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e","externalIds":{"ArXiv":"2402.01364","DBLP":"journals/corr/abs-2402-01364","DOI":"10.48550/arXiv.2402.01364","CorpusId":267406164},"title":"Continual Learning for Large Language Models: A Survey"},{"paperId":"42445823fb0156afddc8c72eaa5ee81dded5b965","externalIds":{"ACL":"2024.eacl-srw.17","DBLP":"conf/eacl/AhnVLLZY24","ArXiv":"2402.00157","DOI":"10.48550/arXiv.2402.00157","CorpusId":267365459},"title":"Large Language Models for Mathematical Reasoning: Progresses and Challenges"},{"paperId":"5a9f79660472e894e482bae011c122f35f9a5095","externalIds":{"ArXiv":"2401.17167","DBLP":"journals/corr/abs-2401-17167","DOI":"10.48550/arXiv.2401.17167","CorpusId":267320882},"title":"Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios"},{"paperId":"ce37a3c0a233019da9b97a6f607aff5c1997ec91","externalIds":{"DBLP":"journals/corr/abs-2401-13996","ArXiv":"2401.13996","DOI":"10.48550/arXiv.2401.13996","CorpusId":267211602},"title":"Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution"},{"paperId":"e99e88257c01c4690ee5b4388a3b074da7911671","externalIds":{"ArXiv":"2401.12474","DBLP":"conf/acl/Lu0ZZ24","DOI":"10.18653/v1/2024.acl-long.423","CorpusId":267095369},"title":"Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment"},{"paperId":"6d9d552af11f333b56158b4c4a3ccc236820eca1","externalIds":{"ArXiv":"2401.12292","DBLP":"journals/corr/abs-2401-12292","DOI":"10.48550/arXiv.2401.12292","CorpusId":267094842},"title":"GRATH: Gradual Self-Truthifying for Large Language Models"},{"paperId":"67f03ac399693393116076c0b8ec8ea05b910685","externalIds":{"DBLP":"conf/icml/RameVHDCBF24","ArXiv":"2401.12187","DOI":"10.48550/arXiv.2401.12187","CorpusId":267068615},"title":"WARM: On the Benefits of Weight Averaged Reward Models"},{"paperId":"8c1243e089621d09025e1e51e8e01cb2cb20eabf","externalIds":{"ArXiv":"2401.10491","DBLP":"conf/iclr/WanH0QB024","DOI":"10.48550/arXiv.2401.10491","CorpusId":267061245},"title":"Knowledge Fusion of Large Language Models"},{"paperId":"04d64be16fb402f28348faffef484bd419c8bd8f","externalIds":{"ArXiv":"2401.10020","DBLP":"journals/corr/abs-2401-10020","DOI":"10.48550/arXiv.2401.10020","CorpusId":267035293},"title":"Self-Rewarding Language Models"},{"paperId":"09d4808857397bc858c6cb6fc46989a9776819c0","externalIds":{"ArXiv":"2401.07950","DBLP":"conf/nips/ZhangHZDYWYD024","DOI":"10.52202/079017-0046","CorpusId":266999634},"title":"SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models"},{"paperId":"8223f81e8cc126b83d2774fe2da19ead290c144d","externalIds":{"DBLP":"journals/corr/abs-2401-05033","ArXiv":"2401.05033","DOI":"10.48550/arXiv.2401.05033","CorpusId":266902624},"title":"Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk"},{"paperId":"cfcf8ab7c595c1849e8396167a29f3bd3359107c","externalIds":{"DBLP":"journals/corr/abs-2401-04620","ArXiv":"2401.04620","DOI":"10.48550/arXiv.2401.04620","CorpusId":266899694},"title":"Agent Alignment in Evolving Social Norms"},{"paperId":"4bebe389dfa85423e5cc089edf20b2c3f572f38c","externalIds":{"DBLP":"conf/acl/ZhangSWPWZ024","ArXiv":"2401.02009","DOI":"10.48550/arXiv.2401.02009","CorpusId":266755862},"title":"Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives"},{"paperId":"ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5","externalIds":{"DBLP":"conf/icml/ChenDYJG24","ArXiv":"2401.01335","DOI":"10.48550/arXiv.2401.01335","CorpusId":266725672},"title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"},{"paperId":"e35426fd81c78b044258cf419be6b7e5093b71c0","externalIds":{"DBLP":"journals/corr/abs-2312-10003","ArXiv":"2312.10003","DOI":"10.48550/arXiv.2312.10003","CorpusId":266335848},"title":"ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent"},{"paperId":"6b97aa78bcdb88548c44e7e1671c0ed37ed37976","externalIds":{"ArXiv":"2312.09390","DBLP":"conf/icml/BurnsIKBGACEJLS24","DOI":"10.48550/arXiv.2312.09390","CorpusId":266312608},"title":"Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"},{"paperId":"48362b169a235ca650918c489c8cea4c597da645","externalIds":{"DBLP":"journals/tmlr/SinghCAAPGLH0XP24","ArXiv":"2312.06585","DOI":"10.48550/arXiv.2312.06585","CorpusId":266163375},"title":"Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"},{"paperId":"15b8b6a8028b2b6e75b67dfb6aebaede36826cf8","externalIds":{"DBLP":"journals/corr/abs-2312-07551","ArXiv":"2312.07551","DOI":"10.48550/arXiv.2312.07551","CorpusId":266151093},"title":"Language Model Alignment with Elastic Reset"},{"paperId":"7d5ea474aa1cf732bb1d00faa07a5bb2c9119cfd","externalIds":{"ArXiv":"2311.09807","DBLP":"conf/naacl/GuoSVC24","DOI":"10.48550/arXiv.2311.09807","CorpusId":265221240},"title":"The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text"},{"paperId":"bddb9d818b73de0a06197a6966673c7eb63c9146","externalIds":{"ArXiv":"2311.08719","DBLP":"journals/corr/abs-2311-08719","DOI":"10.48550/arXiv.2311.08719","CorpusId":265212826},"title":"Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory"},{"paperId":"91e8ce403704a38bee8a5df90d99979a796d1741","externalIds":{"ArXiv":"2311.08182","DBLP":"journals/corr/abs-2311-08182","DOI":"10.48550/arXiv.2311.08182","CorpusId":265157588},"title":"Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning"},{"paperId":"c0230760f644f6b7538d93e4296a5e9aa7028e45","externalIds":{"DBLP":"conf/icml/Yu0Y0L24","ArXiv":"2311.03099","DOI":"10.48550/arXiv.2311.03099","CorpusId":265034087},"title":"Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch"},{"paperId":"ab90b84b42d43c3077c374cd34b3a48a881faf43","externalIds":{"DBLP":"journals/corr/abs-2310-15746","ArXiv":"2310.15746","DOI":"10.48550/arXiv.2310.15746","CorpusId":264438951},"title":"Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation"},{"paperId":"f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969","externalIds":{"DBLP":"journals/corr/abs-2310-11667","ArXiv":"2310.11667","DOI":"10.48550/arXiv.2310.11667","CorpusId":264289186},"title":"SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents"},{"paperId":"908dad62c0e43d80e3e3cb3c0402f7c71c70499c","externalIds":{"ArXiv":"2310.08560","DBLP":"journals/corr/abs-2310-08560","DOI":"10.48550/arXiv.2310.08560","CorpusId":263909014},"title":"MemGPT: Towards LLMs as Operating Systems"},{"paperId":"f05c288caeb9a14ef387e6867934ced3d2200259","externalIds":{"ArXiv":"2310.05910","DBLP":"conf/iclr/SunSZZCCYG24","CorpusId":263831633},"title":"SALMON: Self-Alignment with Instructable Reward Models"},{"paperId":"6f39442852656f8a9decc95854a2ed461b3a83ab","externalIds":{"DBLP":"conf/emnlp/CuiW24","ArXiv":"2310.04484","DOI":"10.48550/arXiv.2310.04484","CorpusId":263829618},"title":"Ada-Instruct: Adapting Instruction Generators for Complex Reasoning"},{"paperId":"6d4bacb69923e1e94fb4de468b939ce6db32fb51","externalIds":{"DBLP":"conf/iclr/0009CMZYSZ24","ArXiv":"2310.01798","DOI":"10.48550/arXiv.2310.01798","CorpusId":263609132},"title":"Large Language Models Cannot Self-Correct Reasoning Yet"},{"paperId":"3fe940a1f121f083cb90c568fc6fa2951bb27dda","externalIds":{"ArXiv":"2310.02304","DBLP":"journals/corr/abs-2310-02304","DOI":"10.48550/arXiv.2310.02304","CorpusId":263620781},"title":"Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation"},{"paperId":"7fe071ea76e49bc3e573beb53f07721630954247","externalIds":{"ArXiv":"2309.16797","DBLP":"conf/icml/FernandoBMOR24","DOI":"10.48550/arXiv.2309.16797","CorpusId":263310323},"title":"Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"},{"paperId":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","externalIds":{"DBLP":"journals/corr/abs-2309-16609","ArXiv":"2309.16609","DOI":"10.48550/arXiv.2309.16609","CorpusId":263134555},"title":"Qwen Technical Report"},{"paperId":"77b1f1c6d1658d120456b9046667cf009ceb39ce","externalIds":{"ArXiv":"2309.12284","DBLP":"journals/corr/abs-2309-12284","CorpusId":262084051},"title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"},{"paperId":"31ead991a433f233d490a7efd6c61d5fae98c327","externalIds":{"DBLP":"journals/corr/abs-2309-08395","ArXiv":"2309.08395","DOI":"10.48550/arXiv.2309.08395","CorpusId":148381102},"title":"Learning by Self-Explaining"},{"paperId":"6f8c4311e65efebb9da5a542c0405684e82a77cc","externalIds":{"ACL":"2024.emnlp-main.35","DBLP":"conf/emnlp/LinL0DLZP00ZDPZ24","ArXiv":"2309.06256","DOI":"10.48550/arXiv.2309.06256","CorpusId":261697277},"title":"Mitigating the Alignment Tax of RLHF"},{"paperId":"24d52678c887331b9da0368e8a2f58bec07f7203","externalIds":{"DBLP":"journals/corr/abs-2309-04658","ArXiv":"2309.04658","DOI":"10.48550/arXiv.2309.04658","CorpusId":261681932},"title":"Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf"},{"paperId":"dcf87f11e245b76437c2f551c1ff6a7842585811","externalIds":{"ArXiv":"2308.13724","DBLP":"conf/icra/ZhouSYS024","DOI":"10.1109/ICRA57147.2024.10610065","CorpusId":261245497},"title":"ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning"},{"paperId":"0b0debb710366cdff461938c80763eace1651af6","externalIds":{"DBLP":"journals/corr/abs-2308-12950","ArXiv":"2308.12950","DOI":"10.48550/arXiv.2308.12950","CorpusId":261100919},"title":"Code Llama: Open Foundation Models for Code"},{"paperId":"e3052ebca5eeae6a8a73e44517903d39746f5f3a","externalIds":{"ArXiv":"2308.12032","ACL":"2024.naacl-long.421","DBLP":"journals/corr/abs-2308-12032","DOI":"10.18653/v1/2024.naacl-long.421","CorpusId":261076515},"title":"From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"},{"paperId":"aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3","externalIds":{"DBLP":"conf/aaai/BestaBKGPGGLNNH24","ArXiv":"2308.09687","DOI":"10.1609/aaai.v38i16.29720","CorpusId":261030303},"title":"Graph of Thoughts: Solving Elaborate Problems with Large Language Models"},{"paperId":"182c7b40ff7560a5545764814338f55a2098e441","externalIds":{"ArXiv":"2308.08998","DBLP":"journals/corr/abs-2308-08998","CorpusId":261031028},"title":"Reinforced Self-Training (ReST) for Language Modeling"},{"paperId":"ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7","externalIds":{"ArXiv":"2308.07201","DBLP":"journals/corr/abs-2308-07201","DOI":"10.48550/arXiv.2308.07201","CorpusId":260887105},"title":"ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate"},{"paperId":"5dbf93a68b7fda600521f046dea35ea8ba9e884f","externalIds":{"DBLP":"journals/corr/abs-2308-03688","ArXiv":"2308.03688","DOI":"10.48550/arXiv.2308.03688","CorpusId":260682249},"title":"AgentBench: Evaluating LLMs as Agents"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"ad022cd5da75637ac3e0a8f8cc4f0d394ba5ff7a","externalIds":{"DBLP":"conf/iclr/AlemohammadCLHB24","ArXiv":"2307.01850","DOI":"10.52591/lxai202312101","CorpusId":259341801},"title":"Self-Consuming Generative Models Go MAD"},{"paperId":"2562fe379554d201aad312f786903f4c60b68acf","externalIds":{"ArXiv":"2306.11706","CorpusId":259203978},"title":"RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation"},{"paperId":"454c8fef2957aa2fb13eb2c7a454393a2ee83805","externalIds":{"DBLP":"journals/corr/abs-2306-08568","ArXiv":"2306.08568","CorpusId":259164815},"title":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"bf4810017b54e50354cccffd8966121c7166cb17","externalIds":{"ArXiv":"2306.03856","DBLP":"journals/corr/abs-2306-03856","ACL":"2024.eamt-1.17","DOI":"10.48550/arXiv.2306.03856","CorpusId":259088848},"title":"Iterative Translation Refinement with Large Language Models"},{"paperId":"88b9a3e5882e5dc6dc56d3476948d1c5be67d798","externalIds":{"ArXiv":"2306.01694","DBLP":"journals/corr/abs-2306-01694","DOI":"10.48550/arXiv.2306.01694","CorpusId":259063728},"title":"Evaluating Language Models for Mathematics through Interactions"},{"paperId":"2651f0179874bd010f58d2c9fa7d118807c80977","externalIds":{"ArXiv":"2306.01708","DBLP":"conf/nips/YadavTCRB23","CorpusId":259064039},"title":"TIES-Merging: Resolving Interference When Merging Models"},{"paperId":"34e1a8a75bf6f35084ac6d714a136f39d02c649e","externalIds":{"ArXiv":"2306.00024","DBLP":"journals/corr/abs-2306-00024","DOI":"10.48550/arXiv.2306.00024","CorpusId":258999642},"title":"Self-Verification Improves Few-Shot Clinical Information Extraction"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","externalIds":{"ArXiv":"2305.18752","DBLP":"conf/nips/YangSLZGLS23","DOI":"10.48550/arXiv.2305.18752","CorpusId":258967184},"title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"155aec5cff650263a4c71136f97570611d1bba7a","externalIds":{"DBLP":"journals/corr/abs-2305-17493","ArXiv":"2305.17493","CorpusId":258987240},"title":"The Curse of Recursion: Training on Generated Data Makes Models Forget"},{"paperId":"87177a718bb12d9dc7389e7762bc859730803b91","externalIds":{"ArXiv":"2305.15275","DBLP":"conf/acl/Zhong0L0T23","DOI":"10.48550/arXiv.2305.15275","CorpusId":258865774},"title":"Self-Evolution Learning for Discriminative Language Model Pretraining"},{"paperId":"c226a4acb42912054d498bcf771023b0ba2da001","externalIds":{"DBLP":"conf/iclr/PangWLC0Z024","ArXiv":"2305.14483","DOI":"10.48550/arXiv.2305.14483","CorpusId":258865735},"title":"Language Model Self-improvement by Reinforcement Learning Contemplation"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"a122863d239643453195424c04067e89406246e1","externalIds":{"DBLP":"conf/emnlp/DingCXQHL0Z23","ArXiv":"2305.14233","DOI":"10.48550/arXiv.2305.14233","CorpusId":258840897},"title":"Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"},{"paperId":"2b786901bb5538a41670b0d0e7e1bbaf5cf6c0c0","externalIds":{"DBLP":"conf/emnlp/ZhengZ0TNW0T23","ArXiv":"2305.13547","DOI":"10.48550/arXiv.2305.13547","CorpusId":258840913},"title":"Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks"},{"paperId":"cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa","externalIds":{"DBLP":"journals/corr/abs-2305-14387","ArXiv":"2305.14387","DOI":"10.48550/arXiv.2305.14387","CorpusId":258865545},"title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"},{"paperId":"bcdaf6c98ddbd6809cf6241aa77200d7394db163","externalIds":{"DBLP":"conf/iclr/GouSGSYDC24","ArXiv":"2305.11738","DOI":"10.48550/arXiv.2305.11738","CorpusId":258823123},"title":"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"73207b9fd2dcfeead7fe086cfdb097e4929a7b44","externalIds":{"DBLP":"conf/emnlp/LiQ23","ArXiv":"2305.05181","DOI":"10.18653/v1/2023.emnlp-main.392","CorpusId":258564230},"title":"MoT: Memory-of-Thought Enables ChatGPT to Self-Improve"},{"paperId":"e01515c6138bc525f7aec30fc85f2adf028d4156","externalIds":{"DBLP":"journals/corr/abs-2305-03047","ArXiv":"2305.03047","DOI":"10.48550/arXiv.2305.03047","CorpusId":258479665},"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"},{"paperId":"b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a","externalIds":{"ArXiv":"2305.01210","DBLP":"journals/corr/abs-2305-01210","CorpusId":258437095},"title":"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation"},{"paperId":"08a80cb34d785258c770acecd302ab41ead46eed","externalIds":{"DBLP":"conf/iclr/XuSZG0FTLJ24","ArXiv":"2304.12244","CorpusId":258298159},"title":"WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"},{"paperId":"9e3c493fb09dcd61bb05e8c5659f23327b7b6340","externalIds":{"ArXiv":"2304.05128","DBLP":"journals/corr/abs-2304-05128","DOI":"10.48550/arXiv.2304.05128","CorpusId":258059885},"title":"Teaching Large Language Models to Self-Debug"},{"paperId":"38179848e2d6a3ad373b1793848816111428ac36","externalIds":{"DBLP":"journals/corr/abs-2304-04370","ArXiv":"2304.04370","DOI":"10.48550/arXiv.2304.04370","CorpusId":258049306},"title":"OpenAGI: When LLM Meets Domain Experts"},{"paperId":"3aaf6a2cbad5850ad81ab5c163599cb3d523436f","externalIds":{"DBLP":"journals/corr/abs-2303-17651","ArXiv":"2303.17651","DOI":"10.48550/arXiv.2303.17651","CorpusId":257900871},"title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"9a75e23639bfcc3a51da57a3b682a984d1d8ac0b","externalIds":{"ArXiv":"2303.17491","DBLP":"conf/nips/KimBM23","DOI":"10.48550/arXiv.2303.17491","CorpusId":257834038},"title":"Language Models can Solve Computer Tasks"},{"paperId":"0671fd553dd670a4e820553a974bc48040ba0819","externalIds":{"DBLP":"conf/nips/ShinnCGNY23","ArXiv":"2303.11366","CorpusId":258833055},"title":"Reflexion: language agents with verbal reinforcement learning"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"f2b0017ddd77fa38760a18145e63553105a1a236","externalIds":{"DBLP":"journals/corr/abs-2301-13688","ArXiv":"2301.13688","DOI":"10.48550/arXiv.2301.13688","CorpusId":256415991},"title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","externalIds":{"ACL":"2023.acl-long.806","ArXiv":"2212.09689","DBLP":"conf/acl/HonovichSLS23","DOI":"10.48550/arXiv.2212.09689","CorpusId":254853659},"title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"},{"paperId":"7715ba5e75f5256e1061c7473afe61bb0dbb9065","externalIds":{"ArXiv":"2212.09561","DBLP":"conf/emnlp/WengZX0HLSLZ23","DOI":"10.18653/v1/2023.findings-emnlp.167","CorpusId":258840837},"title":"Large Language Models are Better Reasoners with Self-Verification"},{"paperId":"3936fd3c6187f606c6e4e2e20b196dbc41cc4654","externalIds":{"DBLP":"journals/corr/abs-2212-08073","ArXiv":"2212.08073","DOI":"10.48550/arXiv.2212.08073","CorpusId":254823489},"title":"Constitutional AI: Harmlessness from AI Feedback"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"3fa70115248377c3d1517c9f978791a296fbc1dd","externalIds":{"DBLP":"conf/emnlp/0001GHW00023","ArXiv":"2210.11610","DOI":"10.48550/arXiv.2210.11610","CorpusId":253080328},"title":"Large Language Models Can Self-Improve"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","externalIds":{"DBLP":"conf/iclr/YaoZYDSN023","ArXiv":"2210.03629","CorpusId":252762395},"title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","externalIds":{"DBLP":"journals/corr/abs-2207-10397","ArXiv":"2207.10397","DOI":"10.48550/arXiv.2207.10397","CorpusId":250920542},"title":"CodeT: Code Generation with Generated Tests"},{"paperId":"29acc890e521f7a6415666ab9eb3432c49b4587a","externalIds":{"DBLP":"journals/corr/abs-2206-05802","ArXiv":"2206.05802","DOI":"10.48550/arXiv.2206.05802","CorpusId":249626555},"title":"Self-critiquing models for assisting human evaluators"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","externalIds":{"DBLP":"conf/iclr/0002WSLCNCZ23","ArXiv":"2203.11171","CorpusId":247595263},"title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"54020e5fe48ebb250f27d744e20a63cac2988a84","externalIds":{"DBLP":"conf/icml/WortsmanIGRLMNF22","ArXiv":"2203.05482","DOI":"10.48550/arXiv.2203.05482","CorpusId":247362886},"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","externalIds":{"DBLP":"journals/corr/abs-2202-03286","ACL":"2022.emnlp-main.225","ArXiv":"2202.03286","DOI":"10.18653/v1/2022.emnlp-main.225","CorpusId":246634238},"title":"Red Teaming Language Models with Language Models"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"50544ed8e618368d025a4ed32d604cfc21932c59","externalIds":{"MAG":"3080443832","ArXiv":"2008.10937","DBLP":"journals/tnn/LiuSXZYT23","DOI":"10.1109/TNNLS.2021.3100554","CorpusId":221293236,"PubMed":"34357870"},"title":"A Survey on Evolutionary Neural Architecture Search"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"132d602fd1276521f7b08577f5d4dbec2adc3c03","externalIds":{"MAG":"2980897231","DBLP":"journals/corr/abs-1910-09281","ArXiv":"1910.09281","CorpusId":204801270},"title":"Dealing with Sparse Rewards in Reinforcement Learning"},{"paperId":"38fb1902c6a2ab4f767d4532b28a92473ea737aa","externalIds":{"DBLP":"journals/corr/abs-1712-01815","MAG":"2772709170","ArXiv":"1712.01815","CorpusId":33081038},"title":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"},{"paperId":"2e55ba6c97ce5eb55abd959909403fe8da7e9fe9","externalIds":{"DBLP":"journals/corr/KirkpatrickPRVD16","MAG":"2560647685","ArXiv":"1612.00796","DOI":"10.1073/pnas.1611835114","CorpusId":4704285,"PubMed":"28292907"},"title":"Overcoming catastrophic forgetting in neural networks"},{"paperId":"846aedd869a00c09b40f1f1f35673cb22bc87490","externalIds":{"DBLP":"journals/nature/SilverHMGSDSAPL16","MAG":"2257979135","DOI":"10.1038/nature16961","CorpusId":515925,"PubMed":"26819042"},"title":"Mastering the game of Go with deep neural networks and tree search"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"5c3f49189140335cf92bd9200db51fee8c7d305d","externalIds":{"MAG":"2092433168","DOI":"10.5465/AMJ.2006.22083026","CorpusId":15839045},"title":"The interplay between exploration and exploitation."},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"eb002f44dff9bb33c9d312299bffa1df6a099052","externalIds":{"MAG":"2020009149","DBLP":"journals/ec/BackS93","DOI":"10.1162/evco.1993.1.1.1","CorpusId":5166635},"title":"An Overview of Evolutionary Algorithms for Parameter Optimization"},{"paperId":"4b4279db68b16e20fbc56f9d41980a950191d30a","externalIds":{"DBLP":"books/mit/H1992","MAG":"1659842140","DOI":"10.7551/MITPRESS/1090.001.0001","CorpusId":58781161},"title":"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence"},{"paperId":"5dbaf198821c13051d0603a52f8e0ad187e5aca5","externalIds":{"MAG":"2060501736","DOI":"10.5840/TEACHPHIL19869468","CorpusId":144641997},"title":"Minds, Brains, and Science"},{"paperId":"7ad5ae13992f494369d4abba054dd5ad9fc6bc35","externalIds":{"DBLP":"journals/pieee/Bogumil85","MAG":"2045510465","DOI":"10.1109/PROC.1985.13210","CorpusId":32133811},"title":"The reflective practitioner: How professionals think in action"},{"paperId":"f4b5ed96ca25c0f61778a26d03cdd6c4b946b5ea","externalIds":{"DBLP":"conf/iclr/YangKCPT24","CorpusId":271745706},"title":"RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment"},{"paperId":"51f931afb9437c9512ae0058041e260d6f9ea27c","externalIds":{"DBLP":"conf/acl/PengDZOR0T23","ACL":"2023.acl-short.73","DOI":"10.18653/v1/2023.acl-short.73","CorpusId":259370625},"title":"Token-Level Self-Evolution Training for Sequence-to-Sequence Learning"},{"paperId":"3bf191c152c3eb693f2fe22eda99197516da9db3","externalIds":{"DBLP":"journals/corr/abs-2312-14033","DOI":"10.48550/arXiv.2312.14033","CorpusId":266435671},"title":"T-Eval: Evaluating the Tool Utilization Capability Step by Step"},{"paperId":"e64003447d9d186b96a1e557b97d514997a4b76e","externalIds":{"DBLP":"journals/corr/abs-2303-07992","DOI":"10.48550/arXiv.2303.07992","CorpusId":263878122},"title":"Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions"},{"paperId":"e730164e17975547564a1eaa70cea5884b16c89d","externalIds":{"CorpusId":259937133},"title":"Instruction : Translate the phrase ” Bonne chance ” into English Response : Good Luck"},{"paperId":"92a79acaa912a8769caabf0794800c81f9f53cd3","externalIds":{"DBLP":"journals/corr/abs-2310-01444","DOI":"10.48550/arXiv.2310.01444","CorpusId":278937905},"title":"Adapting LLM Agents Through Communication"},{"paperId":"24df244bf7a6e8c93c5f183d3f62d39c0f773c68","externalIds":{"DBLP":"journals/corr/abs-2310-05910","DOI":"10.48550/arXiv.2310.05910","CorpusId":280526392},"title":"SALMON: Self-Alignment with Principle-Following Reward Models"},{"paperId":"5b5f17f97bd5fe76b7a2741a8c1cdf583d283f32","externalIds":{"DOI":"10.2307/2940970","CorpusId":252001401},"title":"CONSCIOUSNESS EXPLAINED?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"37fcdea7045b42d5e006ef23b91f038d5f02a220","externalIds":{"MAG":"1975818306","DOI":"10.1002/(sici)1097-461x(1998)66:1<107::aid-qua9>3.0.co;2-z","CorpusId":15580905},"title":"The conscious mind: In search of a fundamental theory"},{"paperId":"3af406939750d526bd402363297dc4c40daa1623","externalIds":{"MAG":"1533746441","CorpusId":178327759},"title":"Reflection, turning experience into learning"},{"paperId":"cd0f365fca59f303ce158c36faa7a7f430a5a698","externalIds":{"CorpusId":268553864},"title":"ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training"}]}