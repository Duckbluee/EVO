{"abstract":"Video anomaly detection is challenging due to the lack of abnormal videos and ambiguity of anomaly definition. Context information is important to identify anomalous events and can be reflected from the salient objects and related background. In this paper, we propose a new multi-level attention network consisting of an Object-Guided Attention Module (OGAM) and a Motion-Refined Attention Module (MRAM) to fully exploit context by leveraging both frame-level and object-level semantics. Specifically, OGAM highlights features of salient objects on frame features for future frame prediction. MRAM further leverages global positions and optical flow of objects to improve the prediction loss. By multi-level attention, objects are highlighted in the predicted frame and necessary context information is preserved at both feature level and pixel level to identify anomalies. Experiments demonstrate our method achieves state-of-the-art performance on benchmark datasets including Avenue, UCSD Ped2 and ShanghaiTech, specifically obtaining a frame-level AUC of 92.6% on Avenue."}