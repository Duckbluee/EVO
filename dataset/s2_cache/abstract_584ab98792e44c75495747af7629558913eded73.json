{"abstract":"Unsupervised Domain Adaptation (UDA) for semantic seg-mentation aims to transfer the knowledge learned from the source domain to the target domain. Unlike the source-available UDA setting, Source-Free Domain Adaptation (SFDA) has no access to the source data and rely solely on the well-trained source model for adaptation. Without the source data for reference, SDFA often leads to unstable adaptation and mostly focuses on common semantic classes. In this pa-per, we propose a Distribution Transfer and Adaptive Class-balanced self-training (DTAC) framework to tackle the issues of SFDA for semantic segmentation. First, in the distribution transfer stage, we propose to narrow the domain gap by aligning the implicit feature characteristics of source model with the feature statistics of the target data. Next, in the self-training stage, we propose a multi-class negative learning method with adaptive thresholding to dynamically select per-class pseudo labels for self-supervision. Experimental re-sults on urban scene benchmarks show that DTAC outper-forms other SFDA baselines and even achieves competitive results with source-available UDA methods."}