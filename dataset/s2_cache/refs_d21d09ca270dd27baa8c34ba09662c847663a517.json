{"references":[{"paperId":"0c2f1134c09b878080f8b261a8d181a91e20f1bf","externalIds":{"DBLP":"journals/corr/abs-2501-04513","ArXiv":"2501.04513","DOI":"10.48550/arXiv.2501.04513","CorpusId":275358234},"title":"Improving Image Captioning by Mimicking Human Reformulation Feedback at Inference-time"},{"paperId":"aa23adfdaf808caad3925313f87d9c91fb6887e4","externalIds":{"DBLP":"conf/naacl/BergerP25","ArXiv":"2409.16646","DOI":"10.48550/arXiv.2409.16646","CorpusId":272880722},"title":"Cross-Lingual and Cross-Cultural Variation in Image Descriptions"},{"paperId":"1a4e522e302e6b35e259ffaaff7e10bd1095398b","externalIds":{"ArXiv":"2404.19752","DBLP":"journals/corr/abs-2404-19752","DOI":"10.1109/CVPR52733.2024.01331","CorpusId":269457182},"title":"Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation"},{"paperId":"f0eadee76505b94a504b634472f7fbb7c20eba53","externalIds":{"DBLP":"journals/corr/abs-2404-02904","ArXiv":"2404.02904","ACL":"2024.naacl-short.30","DOI":"10.48550/arXiv.2404.02904","CorpusId":268876096},"title":"ALOHa: A New Measure for Hallucination in Captioning Models"},{"paperId":"ddd64e0b3111a26e91cf7cde8e567cc27da311c8","externalIds":{"DBLP":"conf/aaai/QiZW24","DOI":"10.1609/aaai.v38i5.28251","CorpusId":268692471},"title":"Relational Distant Supervision for Image Captioning without Image-Text Pairs"},{"paperId":"2d98517c2897311f9c1a56c63fbef077297477d4","externalIds":{"DBLP":"conf/aaai/FuSZY24","DOI":"10.1609/aaai.v38i11.29097","CorpusId":268692734},"title":"Noise-Aware Image Captioning with Progressively Exploring Mismatched Words"},{"paperId":"70faf1731707ddb329877031a00d4b262902ba3c","externalIds":{"ArXiv":"2403.03715","DBLP":"conf/cvpr/ZengXZCCW24","DOI":"10.1109/CVPR52733.2024.01337","CorpusId":268253180},"title":"MeaCap: Memory-Augmented Zero-shot Image Captioning"},{"paperId":"f478e096d7802179765f5bc0d4f46da2f82b816d","externalIds":{"DBLP":"journals/corr/abs-2402-19119","ArXiv":"2402.19119","DOI":"10.1609/aaai.v38i2.27843","CorpusId":268063341},"title":"VIXEN: Visual Text Comparison Network for Image Difference Captioning"},{"paperId":"6bcda71e7ba5ed7dbc95076ac7fbcdb8a203ff01","externalIds":{"DBLP":"conf/cvpr/WadaKSS24","ArXiv":"2402.18091","DOI":"10.1109/CVPR52733.2024.01287","CorpusId":268041837},"title":"Polos: Multimodal Metric Learning from Human Feedback for Image Captioning"},{"paperId":"94d45ec965dd7359abae1aa715684a6b52b0b91d","externalIds":{"DBLP":"journals/mta/AlShamaylehAAHKE24","DOI":"10.1007/s11042-024-18307-8","CorpusId":267904504},"title":"A comprehensive literature review on image captioning methods and metrics based on deep learning technique"},{"paperId":"08d669b23413700acefb0f3b2b1512f8a299f4ab","externalIds":{"DBLP":"journals/corr/abs-2401-02347","ArXiv":"2401.02347","DOI":"10.48550/arXiv.2401.02347","CorpusId":266755661},"title":"Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training"},{"paperId":"0833d5b8c46fe7391e7decc3f11fea87e023d710","externalIds":{"DBLP":"journals/corr/abs-2401-02137","ArXiv":"2401.02137","DOI":"10.48550/arXiv.2401.02137","CorpusId":266755820},"title":"SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment"},{"paperId":"2be74ac5974847bd295441bf2067df26a08d6700","externalIds":{"ArXiv":"2312.15162","DBLP":"journals/corr/abs-2312-15162","DOI":"10.48550/arXiv.2312.15162","CorpusId":266550987},"title":"Cycle-Consistency Learning for Captioning and Grounding"},{"paperId":"5c4f8effb87d7db31ff5b53ead15b921d60a103e","externalIds":{"DBLP":"conf/aaai/LiuLM24","ArXiv":"2312.08865","DOI":"10.48550/arXiv.2312.08865","CorpusId":266210321},"title":"Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image Captioning"},{"paperId":"339ec34efdccdf2bf43bb817ef7cab5058bfa2e7","externalIds":{"DBLP":"journals/corr/abs-2312-00869","ArXiv":"2312.00869","DOI":"10.1109/CVPR52733.2024.01273","CorpusId":265609446},"title":"Segment and Caption Anything"},{"paperId":"1c9060b7246bae6f5000ddd3041b54619243e2bf","externalIds":{"ArXiv":"2311.15879","DBLP":"conf/cvpr/LiVSN24","DOI":"10.1109/CVPR52733.2024.01303","CorpusId":265456926},"title":"Evcap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for Open-World Comprehension"},{"paperId":"9c6b14596d9710ca03c7a04c5edc77009c6bd8d6","externalIds":{"ACL":"2023.arabicnlp-1.1","ArXiv":"2311.08844","DBLP":"conf/wanlp/MohamedANIA23","DOI":"10.48550/arXiv.2311.08844","CorpusId":265213033},"title":"Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder"},{"paperId":"03ca692027883a11315d59c9a422e67879548c41","externalIds":{"DBLP":"journals/corr/abs-2311-08223","ArXiv":"2311.08223","DOI":"10.48550/arXiv.2311.08223","CorpusId":265157620},"title":"Improving Image Captioning via Predicting Structured Concepts"},{"paperId":"da4deaf81232d94e2f38a9d23c6b04ae1d79fbfc","externalIds":{"DBLP":"journals/corr/abs-2310-12971","ArXiv":"2310.12971","DOI":"10.48550/arXiv.2310.12971","CorpusId":264305715},"title":"CLAIR: Evaluating Image Captions with Large Language Models"},{"paperId":"00c1ff63468305ea3fa430c2b3aef156d580c4ff","externalIds":{"DBLP":"conf/iccv/HuHYSSL23","DOI":"10.1109/ICCV51070.2023.00277","CorpusId":257637217},"title":"PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3"},{"paperId":"9d8d53857aeadc6c587f3f9689105f45bd3359fd","externalIds":{"ArXiv":"2312.06299","DBLP":"conf/iccv/FanLLHZ23","DOI":"10.1109/ICCV51070.2023.01422","CorpusId":265447309},"title":"RCA-NOC: Relative Contrastive Alignment for Novel Object Captioning"},{"paperId":"72813e372a7748302c631f7da9b367d0cce2554a","externalIds":{"ArXiv":"2309.16283","DBLP":"journals/corr/abs-2309-16283","DOI":"10.1109/ICCV51070.2023.00263","CorpusId":263136677},"title":"Self-supervised Cross-view Representation Reconstruction for Change Captioning"},{"paperId":"2840c2c2325150b43434c1b7b82128d4e9a52e4c","externalIds":{"DOI":"10.1016/s0001-2092(07)61072-2","CorpusId":154199069},"title":"Los Angeles"},{"paperId":"19d39f8393754fc8d93b6f277ab80b49ba8f2a29","externalIds":{"DBLP":"conf/acl/YangLWW0Z23","ArXiv":"2308.13218","ACL":"2023.acl-long.664","DOI":"10.18653/v1/2023.acl-long.664","CorpusId":259370697},"title":"MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning"},{"paperId":"44cfa54407e6403e2c3124957e532c2fa3a994bc","externalIds":{"DBLP":"journals/corr/abs-2308-12383","ArXiv":"2308.12383","DOI":"10.1109/ICCV51070.2023.00282","CorpusId":261100755},"title":"With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning"},{"paperId":"c4fba6625d9fa99d71fa4f3e87b6a83f6e55a455","externalIds":{"DBLP":"conf/naacl/QuTM24","ArXiv":"2308.08325","ACL":"2024.naacl-long.162","DOI":"10.48550/arXiv.2308.08325","CorpusId":260926430},"title":"Visually-Aware Context Modeling for News Image Captioning"},{"paperId":"5e26dddd695b09ee8ffddc458370f43b1aea746e","externalIds":{"ArXiv":"2308.02833","DBLP":"journals/corr/abs-2308-02833","DOI":"10.48550/arXiv.2308.02833","CorpusId":260682886},"title":"A Comprehensive Analysis of Real-World Image Captioning and Scene Identification"},{"paperId":"c8a7474edf6ee5b7be689aa3b664ac04bed7be9d","externalIds":{"ArXiv":"2307.16686","DBLP":"journals/corr/abs-2307-16686","DOI":"10.1109/ICCV51070.2023.01400","CorpusId":260334572},"title":"Guiding image captioning models toward more specific captions"},{"paperId":"94bd1541b3f2f80064ff4881bcc06c76d9e35271","externalIds":{"DBLP":"journals/corr/abs-2307-16525","ArXiv":"2307.16525","DOI":"10.1109/ICCV51070.2023.00291","CorpusId":260334193},"title":"Transferable Decoding with Visual Entities for Zero-Shot Image Captioning"},{"paperId":"bd54aa49b6a16916ba12eedbbee63eb33f6ac391","externalIds":{"DBLP":"journals/corr/abs-2307-11636","ArXiv":"2307.11636","DOI":"10.1109/ICCV51070.2023.01856","CorpusId":260091839},"title":"OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?"},{"paperId":"9e4a0d123bcdf52370dcb6947d0a7fbfca380864","externalIds":{"DBLP":"conf/nips/YueHZJ23","ArXiv":"2306.13460","DOI":"10.48550/arXiv.2306.13460","CorpusId":259244145},"title":"Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation"},{"paperId":"19f59c14b3d79e3203c696128a135d33eb35e468","externalIds":{"DBLP":"conf/acl/OuKF23","ArXiv":"2306.08818","DOI":"10.48550/arXiv.2306.08818","CorpusId":259164492},"title":"Pragmatic Inference with a CLIP Listener for Contrastive Captioning"},{"paperId":"4279a38a098d1d359881b73c6a88a112fe93443a","externalIds":{"DBLP":"conf/nips/Luo0L023","ArXiv":"2306.07279","DOI":"10.48550/arXiv.2306.07279","CorpusId":259137821},"title":"Scalable 3D Captioning with Pretrained Models"},{"paperId":"75b8cf0945e02761526e8d091c11ebc1c3849895","externalIds":{"DBLP":"conf/sustainlp/AnagnostopoulouHS23","ACL":"2023.sustainlp-1.19","ArXiv":"2306.03500","DOI":"10.48550/arXiv.2306.03500","CorpusId":259089019},"title":"Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory"},{"paperId":"4a7b5ffb0ed6ec5055ea00248a36da7eadd269cf","externalIds":{"DBLP":"conf/cvpr/RenMFL0DZO23","DOI":"10.1109/CVPR52729.2023.00281","CorpusId":260871443},"title":"Crossing the Gap: Domain Generalization for Image Captioning"},{"paperId":"f7c6553d47bbe5682377e975058592d5d495ad39","externalIds":{"ACL":"2023.acl-industry.67","ArXiv":"2306.00931","DBLP":"conf/acl/KalaraniBCS23","DOI":"10.48550/arXiv.2306.00931","CorpusId":259000001},"title":"“Let’s not Quote out of Context”: Unified Vision-Language Pretraining for Context Assisted Image Captioning"},{"paperId":"cf851103cdb37162a46ff2b03d77ca68e5dfa33e","externalIds":{"DBLP":"journals/corr/abs-2305-19821","ArXiv":"2305.19821","DOI":"10.48550/arXiv.2305.19821","CorpusId":258987414},"title":"LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting"},{"paperId":"96a7ce9510adc87bdce0b6e8bc541dfad7ae7df7","externalIds":{"ArXiv":"2305.18072","DBLP":"conf/aaai/MaZRZ024","DOI":"10.1609/aaai.v38i5.28203","CorpusId":258960145},"title":"Image Captioning with Multi-Context Synthetic Data"},{"paperId":"f583bdb250eb1d25aea7d074299ee962cab1d008","externalIds":{"DBLP":"journals/corr/abs-2305-16295","ArXiv":"2305.16295","DOI":"10.1109/CVPR52729.2023.01062","CorpusId":258888004},"title":"HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning"},{"paperId":"3510c6405cf848fef24e5c63ce832c9326e0518c","externalIds":{"DBLP":"conf/eacl/AhmadiA24","ArXiv":"2305.14998","ACL":"2024.findings-eacl.14","DOI":"10.48550/arXiv.2305.14998","CorpusId":258865823},"title":"An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics"},{"paperId":"0744783bbefc12b2b1383bed137e8a80061274b7","externalIds":{"ArXiv":"2305.14800","DBLP":"journals/corr/abs-2305-14800","DOI":"10.48550/arXiv.2305.14800","CorpusId":258865250},"title":"Exploring Diverse In-Context Configurations for Image Captioning"},{"paperId":"7751d96028e5a198a0b95257d183801d5cd73633","externalIds":{"DBLP":"conf/emnlp/QiuDWCP23","ArXiv":"2305.14711","DOI":"10.18653/v1/2023.emnlp-main.520","CorpusId":264425897},"title":"Gender Biases in Automatic Evaluation Metrics for Image Captioning"},{"paperId":"b82c1b0512d25307e3c81bb8d9df1607267a7a52","externalIds":{"DBLP":"conf/emnlp/HwangS23","ArXiv":"2305.13703","DOI":"10.48550/arXiv.2305.13703","CorpusId":258841099},"title":"MemeCap: A Dataset for Captioning and Interpreting Memes"},{"paperId":"6a117b92a9d31b604bfad6e875077888a28fd794","externalIds":{"DBLP":"conf/acl/Wu00C23","ACL":"2023.acl-long.146","ArXiv":"2305.12260","DOI":"10.48550/arXiv.2305.12260","CorpusId":258833472},"title":"Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment"},{"paperId":"69b6b6f82eeaca06eddb632c92db09ba0d5c16b9","externalIds":{"ArXiv":"2305.11768","ACL":"2023.acl-long.442","DBLP":"journals/corr/abs-2305-11768","DOI":"10.48550/arXiv.2305.11768","CorpusId":258823353},"title":"Generating Visual Spatial Description via Holistic 3D Scene Understanding"},{"paperId":"7bc16a36a8623e7f7d53bee9fb765fc89614c080","externalIds":{"DBLP":"conf/acl/HuCZJ23","ACL":"2023.acl-long.178","ArXiv":"2305.06002","DOI":"10.48550/arXiv.2305.06002","CorpusId":258588434},"title":"InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation"},{"paperId":"2da74670eab9e2f84caf2dc150aeb73216055ca7","externalIds":{"DBLP":"conf/eacl/LiLQE24","ArXiv":"2305.03610","ACL":"2024.eacl-long.65","CorpusId":258547260},"title":"The Role of Data Curation in Image Captioning"},{"paperId":"a8d559a4f4ac9da3e6bc308df02c5928f20d57d1","externalIds":{"ACL":"2023.acl-long.694","DBLP":"conf/acl/YangPWXYLHHLZ23","ArXiv":"2305.02177","DOI":"10.18653/v1/2023.acl-long.694","CorpusId":258461558},"title":"Transforming Visual Scene Graphs to Image Captions"},{"paperId":"c854cc6c79a84b8e6685cbe893d5045049f5b6cd","externalIds":{"DBLP":"journals/ijon/XuT0ZZ023","DOI":"10.1016/j.neucom.2023.126287","CorpusId":258504649},"title":"Deep image captioning: A review of methods, trends and future challenges"},{"paperId":"1cbf13d558911fb79663473cfe482bafa71d1e77","externalIds":{"DBLP":"journals/corr/abs-2304-06602","ArXiv":"2304.06602","DOI":"10.1109/CVPR52729.2023.01042","CorpusId":258107962},"title":"A-CAP: Anticipation Captioning with Commonsense Knowledge"},{"paperId":"8786848846cb246c8f00c4a81a0c23bdc03bb8ce","externalIds":{"DBLP":"conf/aaai/Abdelrahman0LE24","ArXiv":"2304.04874","DOI":"10.48550/arXiv.2304.04874","CorpusId":258060211},"title":"ImageCaptioner2: Image Captioner for Image Captioning Bias Amplification Assessment"},{"paperId":"73120e069de74bb234986307a49b191395af8749","externalIds":{"DBLP":"journals/corr/abs-2304-03693","ArXiv":"2304.03693","DOI":"10.1109/CVPR52729.2023.01458","CorpusId":258040970},"title":"Model-Agnostic Gender Debiased Image Captioning"},{"paperId":"09b2b77111900880585072d82ab272c9222ac9a1","externalIds":{"ArXiv":"2304.01662","DBLP":"conf/cvpr/DessiBGRFB23","DOI":"10.1109/CVPR52729.2023.00670","CorpusId":257921928},"title":"Cross-Domain Image Captioning with Discriminative Finetuning"},{"paperId":"187552e7a052e32ccb8d9acca4c57655ae73dcb7","externalIds":{"DBLP":"conf/cvpr/SartoBCBC23","ArXiv":"2303.12112","DOI":"10.1109/CVPR52729.2023.00668","CorpusId":257663410},"title":"Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation"},{"paperId":"0883be76599f590cc20084d85256a22ad2bceb88","externalIds":{"ArXiv":"2303.08389","DBLP":"conf/emnlp/KimHY0BJ23","DOI":"10.48550/arXiv.2303.08389","CorpusId":257532371},"title":"PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning"},{"paperId":"a20e7e5e4338644d24145e71a9b89100af87e5d0","externalIds":{"DBLP":"journals/corr/abs-2303-03032","ArXiv":"2303.03032","DOI":"10.48550/arXiv.2303.03032","CorpusId":257365203},"title":"DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training"},{"paperId":"6e31cedc02984a564107d15716071c86894bedf0","externalIds":{"DBLP":"conf/cvpr/ZengZLWCW23","ArXiv":"2303.02437","DOI":"10.1109/CVPR52729.2023.02247","CorpusId":257365573},"title":"ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing"},{"paperId":"6a4901e3bc255dd9c14d1e0d9e76e46cdd0f0ac0","externalIds":{"ACL":"2023.eacl-main.266","DBLP":"journals/corr/abs-2302-08268","ArXiv":"2302.08268","DOI":"10.48550/arXiv.2302.08268","CorpusId":256901128},"title":"Retrieval-augmented Image Captioning"},{"paperId":"717d4cc5188e06791ef2043045e6e570ae764091","externalIds":{"DBLP":"conf/emnlp/YangPLKNHFYLLS023","ArXiv":"2302.04858","DOI":"10.48550/arXiv.2302.04858","CorpusId":256697441},"title":"Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning"},{"paperId":"0d53981f3bc7329016ca28c2793cd58b44d38ada","externalIds":{"DBLP":"conf/emnlp/ChanMVRC23","ArXiv":"2302.01328","DOI":"10.48550/arXiv.2302.01328","CorpusId":256503776},"title":"IC3: Image Captioning by Committee Consensus"},{"paperId":"ce032d151caeffeafeab355e69454d89c3c0b6a9","externalIds":{"DBLP":"conf/eacl/ZhouL23","ACL":"2023.findings-eacl.169","ArXiv":"2301.11367","DOI":"10.48550/arXiv.2301.11367","CorpusId":256358690},"title":"Style-Aware Contrastive Learning for Multi-Style Image Captioning"},{"paperId":"92d3f7cea95bba8cb905454324c3eeb84d2b6e58","externalIds":{"ArXiv":"2301.02508","DBLP":"journals/corr/abs-2301-02508","DOI":"10.1109/CVPR52729.2023.01070","CorpusId":255522451},"title":"End-to-End 3D Dense Captioning with Vote2Cap-DETR"},{"paperId":"945bcae64483ae95a5a404f2c15fd85d36af469b","externalIds":{"ArXiv":"2301.00805","DBLP":"conf/iccv/WuLDLCTL23","DOI":"10.1109/ICCV51070.2023.02005","CorpusId":255372843},"title":"Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation"},{"paperId":"ed64edb957f1f2b86d94ba5e6193774bab057cdb","externalIds":{"DBLP":"conf/iccv/KangMLR23","ArXiv":"2212.13563","DOI":"10.1109/ICCV51070.2023.00275","CorpusId":255186162},"title":"Noise-aware Learning from Web-crawled Image-Text Data for Image Captioning"},{"paperId":"d6332c9a4607ce9823123e6407e40ab6ac33ac08","externalIds":{"DBLP":"journals/corr/abs-2212-08985","ArXiv":"2212.08985","DOI":"10.48550/arXiv.2212.08985","CorpusId":254853824},"title":"Efficient Image Captioning for Edge Devices"},{"paperId":"1c5dc727d624da1fd0474570775ad2da6385eef6","externalIds":{"ACL":"2022.emnlp-main.516","DBLP":"conf/emnlp/ZhangSAZS022","ArXiv":"2212.07075","DOI":"10.48550/arXiv.2212.07075","CorpusId":254636533},"title":"Cross-Modal Similarity-Based Curriculum Learning for Image Captioning"},{"paperId":"a6a59c9e4cd446d0d04f76587699e3e8ab5197c2","externalIds":{"DBLP":"journals/corr/abs-2212-03099","ArXiv":"2212.03099","DOI":"10.1109/CVPR52729.2023.02237","CorpusId":254275438},"title":"Semantic-Conditional Diffusion Networks for Image Captioning*"},{"paperId":"f66aeec98816c3a52685e570a04fa8f2bd53dfb4","externalIds":{"DBLP":"conf/aaai/WangXWJL23","ArXiv":"2212.01803","DOI":"10.48550/arXiv.2212.01803","CorpusId":254247022},"title":"Controllable Image Captioning via Prompting"},{"paperId":"8abaaa14c7b2f089c081bf031cc4b144a60a6759","externalIds":{"ArXiv":"2212.00836","DBLP":"conf/iccv/ChenHCNC23","DOI":"10.1109/ICCV51070.2023.01660","CorpusId":254220973},"title":"UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding"},{"paperId":"7ec49b2cb2cd31541de39d9cac82120ffc3455b9","externalIds":{"DBLP":"conf/emnlp/ZhouLRY22","ArXiv":"2212.00843","DOI":"10.48550/arXiv.2212.00843","CorpusId":254220860},"title":"Focus! Relevant and Sufficient Context Selection for News Image Captioning"},{"paperId":"d13f52cbff1416d11986f996fa68ee9767844c60","externalIds":{"DBLP":"journals/corr/abs-2211-16769","ArXiv":"2211.16769","DOI":"10.48550/arXiv.2211.16769","CorpusId":254096325},"title":"Uncertainty-Aware Image Captioning"},{"paperId":"15d6ef576fb07bb5fc07fef6f63708e440396dd9","externalIds":{"DBLP":"journals/corr/abs-2211-15378","ArXiv":"2211.15378","DOI":"10.48550/arXiv.2211.15378","CorpusId":254044062},"title":"Aesthetically Relevant Image Captioning"},{"paperId":"c9731012ce41943eba34a88cf250e9ae07cf345e","externalIds":{"ArXiv":"2211.04971","DBLP":"journals/corr/abs-2211-04971","ACL":"2022.umios-1.6","DOI":"10.48550/arXiv.2211.04971","CorpusId":253420317},"title":"Understanding Cross-modal Interactions in V&L Models that Generate Scene Descriptions"},{"paperId":"40187d7c30b666e3c45337eb36ff86871d8347a7","externalIds":{"DBLP":"conf/emnlp/NukraiMG22","ArXiv":"2211.00575","DOI":"10.48448/n7sq-p557","CorpusId":253244258},"title":"Text-Only Training for Image Captioning using Noise-Injected CLIP"},{"paperId":"1cc512fc89651dc4e9a1998447a6693d90b73ee0","externalIds":{"ACL":"2022.emnlp-main.716","DBLP":"journals/corr/abs-2210-15028","ArXiv":"2210.15028","DOI":"10.48550/arXiv.2210.15028","CorpusId":253157676},"title":"FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning"},{"paperId":"b7bf8fb8de6359b3b3273502f62f49d8df42911e","externalIds":{"ACL":"2022.emnlp-main.753","DBLP":"conf/emnlp/DeutschDR22","ArXiv":"2210.12563","DOI":"10.48550/arXiv.2210.12563","CorpusId":253098066},"title":"On the Limitations of Reference-Free Evaluations of Generated Text"},{"paperId":"0b574244f2ecea75a536106789f08d3c3c2590e0","externalIds":{"DBLP":"journals/corr/abs-2210-11109","ArXiv":"2210.11109","ACL":"2022.emnlp-main.93","DOI":"10.48550/arXiv.2210.11109","CorpusId":253018728},"title":"Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation"},{"paperId":"49d13f1dcaa2eac0bdb572127d5b6c89b1f9451e","externalIds":{"DBLP":"conf/cvpr/Ramos0EK23","ArXiv":"2209.15323","DOI":"10.1109/CVPR52729.2023.00278","CorpusId":252668790},"title":"Smallcap: Lightweight Image Captioning Prompted with Retrieval Augmentation"},{"paperId":"8c15556f2292b7db8d87050cb1286db699c0a22c","externalIds":{"DBLP":"journals/corr/abs-2209-12343","ArXiv":"2209.12343","DOI":"10.48550/arXiv.2209.12343","CorpusId":252531901},"title":"Paraphrasing Is All You Need for Novel Object Captioning"},{"paperId":"c7405b595f266e02f5cac24a11d83b7e341662f6","externalIds":{"DBLP":"conf/aaai/NguyenBMGK23","ArXiv":"2209.10474","DOI":"10.48550/arXiv.2209.10474","CorpusId":252407476},"title":"Show, Interpret and Tell: Entity-aware Contextualised Image Captioning in Wikipedia"},{"paperId":"5097a26224f78e3696e741f27a459e016f344d69","externalIds":{"DBLP":"journals/corr/abs-2207-09666","ArXiv":"2207.09666","DOI":"10.48550/arXiv.2207.09666","CorpusId":250698884},"title":"GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features"},{"paperId":"fc8bec01cc8963226af94786f70bd3e71a7fad4a","externalIds":{"ArXiv":"2207.01733","DBLP":"journals/spic/GonzalezChavezRMR24","DOI":"10.48550/arXiv.2207.01733","CorpusId":250279760},"title":"Are metrics measuring what they should? An evaluation of image captioning task metrics"},{"paperId":"977f49f96ec7b532c29576e468adbd140c502810","externalIds":{"DBLP":"conf/aaai/Fei22","DOI":"10.1609/aaai.v36i1.19940","CorpusId":250288254},"title":"Attention-Aligned Transformer for Image Captioning"},{"paperId":"ee01303666c29ef85bd92341f11577b189f5b7ee","externalIds":{"ArXiv":"2206.01843","DBLP":"journals/corr/abs-2206-01843","DOI":"10.48550/arXiv.2206.01843","CorpusId":249395345},"title":"Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning"},{"paperId":"afc6c4f3e387e3136a4b46cdfc48c0ce3d23cb55","externalIds":{"DBLP":"conf/cvpr/FeiYW022","DOI":"10.1109/CVPR52688.2022.01190","CorpusId":249916633},"title":"DeeCap: Dynamic Early Exiting for Efficient Image Captioning"},{"paperId":"520c775aa18759a1740521371d6f36dfcf966e95","externalIds":{"DBLP":"journals/corr/abs-2206-06930","ArXiv":"2206.06930","DOI":"10.1109/CVPR52688.2022.01746","CorpusId":249642656},"title":"Comprehending and Ordering Semantics for Image Captioning"},{"paperId":"f09cab749bff0a1a7796b4a2188a05063d93b38e","externalIds":{"DBLP":"conf/cvpr/MavroudiV22","DOI":"10.1109/CVPR52688.2022.01510","CorpusId":249916829},"title":"Weakly-Supervised Generation and Grounding of Visual Descriptions with Conditional Generative Models"},{"paperId":"098dc7f2ea862d5109f638a9f25e817f9667d595","externalIds":{"DBLP":"conf/cvpr/WuZSZCGSJ22","DOI":"10.1109/CVPR52688.2022.01749","CorpusId":250563752},"title":"DIFNet: Boosting Visual Information Flow for Image Captioning"},{"paperId":"cb01ca278cac5fd924f2180ff4dff8be34d14083","externalIds":{"ACL":"2022.aacl-short.5","DBLP":"conf/ijcnlp/GuoWL22","ArXiv":"2206.00629","DOI":"10.48550/arXiv.2206.00629","CorpusId":249240554},"title":"CLIP4IDC: CLIP for Image Difference Captioning"},{"paperId":"554a2dc57b1f43057737b1cab2dc70319afb3ac3","externalIds":{"DBLP":"conf/cvpr/LiuWY0YS022","DOI":"10.1109/CVPR52688.2022.01751","CorpusId":250520562},"title":"Show, Deconfound and Tell: Image Captioning with Causal Inference"},{"paperId":"fade0ef67bcad3369e83348111a73c0f9578786f","externalIds":{"DBLP":"conf/cvpr/CaiZZSX22","DOI":"10.1109/CVPR52688.2022.01597","CorpusId":250980730},"title":"3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds"},{"paperId":"6f69730e5ebed45d59d092ddd44d1da24047d00c","externalIds":{"ArXiv":"2205.13115","DBLP":"conf/naacl/00010KDBB22","DOI":"10.48550/arXiv.2205.13115","CorpusId":249097750},"title":"Fine-grained Image Captioning with CLIP Reward"},{"paperId":"a8260077135246476a0b0601495ef08e56c21a50","externalIds":{"DBLP":"journals/corr/abs-2205-12522","ACL":"2022.emnlp-main.45","ArXiv":"2205.12522","DOI":"10.48550/arXiv.2205.12522","CorpusId":249062751},"title":"Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset"},{"paperId":"2b8228373c0e6f103f78dc932390a9880f34278c","externalIds":{"ACL":"2022.iwslt-1.7","DBLP":"conf/iwslt/XuBCBY22","ArXiv":"2205.06522","DOI":"10.48550/arXiv.2205.06522","CorpusId":248780003},"title":"Joint Generation of Captions and Subtitles with Dual Decoding"},{"paperId":"dd39954b8d8ca93ab1274de31819025669e0fa55","externalIds":{"DBLP":"conf/cvpr/KuoK22","ArXiv":"2205.04363","DOI":"10.1109/CVPR52688.2022.01744","CorpusId":248572429},"title":"Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning"},{"paperId":"140f83a8774b1b847aa0aa6a5b0ac63fce27bb00","externalIds":{"DBLP":"journals/corr/abs-2204-12974","ACL":"2022.emnlp-main.226","ArXiv":"2204.12974","DOI":"10.48550/arXiv.2204.12974","CorpusId":248405744},"title":"CapOnImage: Context-driven Dense-Captioning on Image"},{"paperId":"9f00ef42f656ab829cf10a4cb467839b7242ec1e","externalIds":{"DBLP":"journals/corr/abs-2204-07660","ArXiv":"2204.07660","DOI":"10.1109/CVPR52688.2022.02058","CorpusId":248227685},"title":"It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection"},{"paperId":"efbc6f6d983a189831106aaa8f3f4bd3b4ad8b15","externalIds":{"DBLP":"conf/eccv/WangGYLFS22","ArXiv":"2204.00486","DOI":"10.1007/978-3-031-19833-5_41","CorpusId":250699030},"title":"GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval"},{"paperId":"23903b4c42fdbea0b7b35e3157b48d8dfd18e1a5","externalIds":{"DBLP":"conf/aaai/WangXS22","ArXiv":"2203.15350","DOI":"10.48550/arXiv.2203.15350","CorpusId":247778466},"title":"End-to-End Transformer Based Model for Image Captioning"},{"paperId":"72a6b51c492aa9333b857477ff19f76c37053aa9","externalIds":{"DBLP":"journals/corr/abs-2203-15395","ArXiv":"2203.15395","DOI":"10.1109/CVPR52688.2022.01309","CorpusId":247778463},"title":"Quantifying Societal Bias Amplification in Image Captioning"},{"paperId":"d1bf1095ec5334bdf09e9a80679411416c0d4db3","externalIds":{"ArXiv":"2203.14499","DBLP":"conf/cvpr/VoCSN22","DOI":"10.1109/CVPR52688.2022.01747","CorpusId":247763130},"title":"NOC-REK: Novel Object Captioning with Retrieved Vocabulary from External Knowledge"},{"paperId":"0119bd115c4aa627a537cda0512e4a62e3c67bdd","externalIds":{"DBLP":"journals/csur/SharmaM23","DOI":"10.1145/3492865","CorpusId":247470101},"title":"A Comprehensive Report on Machine Learning-based Early Detection of Alzheimer's Disease using Multi-modal Neuroimaging Data"},{"paperId":"a006979969bb11aaa4e06fb3845919538808a7c6","externalIds":{"DBLP":"conf/eccv/RutaGAMKBSJFFLC22","ArXiv":"2203.05321","DOI":"10.48550/arXiv.2203.05321","CorpusId":247362532},"title":"StyleBabel: Artistic Style Tagging and Captioning"},{"paperId":"07546f1f0b35012a5d8ca850580438f306f3bdd3","externalIds":{"ArXiv":"2203.05203","DBLP":"conf/eccv/JiaoCJCMJ22","DOI":"10.48550/arXiv.2203.05203","CorpusId":247362768},"title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes"},{"paperId":"a9861b0c4fc76c96493e67f9398cb17e70a19e1a","externalIds":{"ArXiv":"2203.00843","DBLP":"conf/cvpr/YuanYLGLCL22","DOI":"10.1109/CVPR52688.2022.00837","CorpusId":247218430},"title":"X -Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning"},{"paperId":"4993b87d3e26983c37bf1d05b07c8179d81ad196","externalIds":{"DBLP":"conf/aaai/YaoWJ22","ArXiv":"2202.04298","DOI":"10.1609/aaai.v36i3.20218","CorpusId":246680391},"title":"Image Difference Captioning with Pre-training and Contrastive Learning"},{"paperId":"a37bf9d65a4d1ebf95c79b3ab973bb7a8019ac3e","externalIds":{"DBLP":"journals/corr/abs-2201-12944","ArXiv":"2201.12944","DOI":"10.1145/3617592","CorpusId":246430542},"title":"Deep Learning Approaches on Image Captioning: A Review"},{"paperId":"1e91743216e8b9a0a7a2908634e7412084d3fc0f","externalIds":{"DBLP":"conf/aaai/ZhangS0ZC00Z22","ArXiv":"2112.06558","DOI":"10.1609/aaai.v36i3.20243","CorpusId":245124461},"title":"MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-based Image Captioning"},{"paperId":"45348358505da4158afb98e0e18ee4e384d8d798","externalIds":{"DBLP":"conf/cvpr/FangW0LGWY022","ArXiv":"2112.05230","DOI":"10.1109/CVPR52688.2022.01748","CorpusId":245117789},"title":"Injecting Semantic Concepts into End-to-End Image Captioning"},{"paperId":"17bc31c1231691d8ef9d4200852bebe9abbb72cb","externalIds":{"ArXiv":"2112.00969","DBLP":"conf/eccv/MengYCSL22","DOI":"10.1007/978-3-031-20059-5_13","CorpusId":244798910},"title":"Object-Centric Unsupervised Image Captioning"},{"paperId":"3ea60cbce6c9065661d207fccf021c5d58a83f01","externalIds":{"DBLP":"conf/cvpr/0006GWY0LW22","ArXiv":"2111.12233","DOI":"10.1109/CVPR52688.2022.01745","CorpusId":244527510},"title":"Scaling Up Vision-Language Pretraining for Image Captioning"},{"paperId":"59600d21fc9d5e71bde2e9deeb23620f8bf15c47","externalIds":{"ArXiv":"2111.08940","DBLP":"conf/naacl/KasaiSDMBCS22","ACL":"2022.naacl-main.254","DOI":"10.18653/v1/2022.naacl-main.254","CorpusId":244270388},"title":"Transparent Human Evaluation for Image Captioning"},{"paperId":"f8b9d8d187413d758d7849e96288838b5c1b75cf","externalIds":{"ACL":"2021.emnlp-main.735","DBLP":"conf/emnlp/TuLYG021","ArXiv":"2110.10328","DOI":"10.18653/v1/2021.emnlp-main.735","CorpusId":239049518},"title":"Rˆ3Net:Relation-embedded Representation Reconstruction Network for Change Captioning"},{"paperId":"807f377de905eda62e4cd2f0797153a59296adbb","externalIds":{"DBLP":"conf/iccv/ShiLW21","DOI":"10.1109/ICCV48922.2021.00219","CorpusId":244430079},"title":"Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning"},{"paperId":"3a8637c0d93bab1f651003a895335b8a71149c0c","externalIds":{"DBLP":"conf/iccv/KimKLPK21","DOI":"10.1109/ICCV48922.2021.00210","CorpusId":245999341},"title":"Viewpoint-Agnostic Change Captioning with Cycle Consistency"},{"paperId":"5156785557b186bc7a1860ebebd8a14c1665c7e6","externalIds":{"ACL":"2021.wnut-1.39","DBLP":"journals/corr/abs-2109-13701","ArXiv":"2109.13701","DOI":"10.18653/v1/2021.wnut-1.39","CorpusId":238198699},"title":"CIDEr-R: Robust Consensus-based Image Description Evaluation"},{"paperId":"dd83137f3bfd14f6bf0f714e4fa08a20c06bbbd0","externalIds":{"DBLP":"conf/emnlp/InanSKSSA21","ArXiv":"2109.05281","DOI":"10.18653/v1/2021.findings-emnlp.291","CorpusId":237491865},"title":"COSMic: A Coherence-Aware Generation Metric for Image Descriptions"},{"paperId":"299983121dec88d4cc8e4ea2aa06514787d8d878","externalIds":{"DBLP":"conf/aaai/FengLTAMHG22","ArXiv":"2109.03892","DOI":"10.1609/aaai.v36i10.21306","CorpusId":237452278},"title":"Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models"},{"paperId":"9b10c49575c5aa07b5076d2f5b27c67b68b09c9b","externalIds":{"ArXiv":"2109.02865","ACL":"2021.emnlp-main.419","DBLP":"conf/emnlp/YangKTJ21","DOI":"10.18653/v1/2021.emnlp-main.419","CorpusId":237431500},"title":"Journalistic Guidelines Aware News Image Captioning"},{"paperId":"f5a76442659066434b1bdf480cf11f4f549411ab","externalIds":{"DBLP":"journals/corr/abs-2108-12560","ArXiv":"2108.12560","DOI":"10.18653/v1/2021.findings-emnlp.395","CorpusId":237353055},"title":"QACE: Asking Questions to Evaluate an Image Caption"},{"paperId":"e4c95438e8668f19e6d00fa794aad67c60b364ef","externalIds":{"ArXiv":"2108.10568","DBLP":"journals/corr/abs-2108-10568","DOI":"10.1109/ICCV48922.2021.00220","CorpusId":237278308},"title":"Auto-Parsing Network for Image Captioning and Visual Question Answering"},{"paperId":"1c83f3f9789df43bf937ae2618721e2da83dcc06","externalIds":{"DBLP":"journals/pami/StefaniniCBCFC23","ArXiv":"2107.06912","DOI":"10.1109/TPAMI.2022.3148210","CorpusId":244772950,"PubMed":"35130142"},"title":"From Show to Tell: A Survey on Deep Learning-Based Image Captioning"},{"paperId":"389b98518980f218cdc0869fd852428686eef6dd","externalIds":{"DBLP":"journals/corr/abs-2106-14019","ACL":"2021.acl-short.29","ArXiv":"2106.14019","DOI":"10.18653/v1/2021.acl-short.29","CorpusId":235658821},"title":"UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning"},{"paperId":"558f811c61c35f8fbe0d6ab3f4195c6725784573","externalIds":{"ArXiv":"2106.08503","DBLP":"conf/iccv/ZhaoWR21","DOI":"10.1109/ICCV48922.2021.01456","CorpusId":235446842},"title":"Understanding and Evaluating Racial Biases in Image Captioning"},{"paperId":"81c774bf206d518ffbefafc9acfe670ffe2d1377","externalIds":{"ACL":"2021.acl-long.175","DBLP":"journals/corr/abs-2106-01444","ArXiv":"2106.01444","DOI":"10.18653/v1/2021.acl-long.175","CorpusId":235313922},"title":"SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis"},{"paperId":"afa76fedf8701e057b2bf7a228bf41980ac2d1c9","externalIds":{"DBLP":"conf/cvpr/ZhangSLJZWHJ21","DOI":"10.1109/CVPR46437.2021.01521","CorpusId":235702714},"title":"RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words"},{"paperId":"a5878295f400df1cba04398bdaf4f0b2e5c71af8","externalIds":{"MAG":"3170032221","ACL":"2021.maiworkshop-1.6","DOI":"10.18653/V1/2021.MAIWORKSHOP-1.6","CorpusId":235097492},"title":"Validity-Based Sampling and Smoothing Methods for Multiple Reference Image Captioning"},{"paperId":"e736814d45b6721bfc204b6ee86bd29c15e51f46","externalIds":{"ACL":"2021.alvr-1.3","MAG":"3172418174","DOI":"10.18653/V1/2021.ALVR-1.3","CorpusId":235097567},"title":"Leveraging Partial Dependency Trees to Control Image Captions"},{"paperId":"c1d41e75bd795e21984452d276af4721fa2beb21","externalIds":{"DBLP":"conf/cvpr/WangYWWC21","DOI":"10.1109/CVPR46437.2021.01383","CorpusId":235681308},"title":"FAIEr: Fidelity and Adequacy Ensured Image Caption Evaluation"},{"paperId":"0d806dcbb1c14bea8139f21290358c483f208b18","externalIds":{"DBLP":"conf/cvpr/WangTYBL21","DOI":"10.1109/CVPR46437.2021.00136","CorpusId":235703415},"title":"Improving OCR-based Image Captioning by Incorporating Geometrical Relationship"},{"paperId":"8888c82729866b4fd9692461f86ddace104cebc1","externalIds":{"DBLP":"conf/cvpr/Hosseinzadeh021","DOI":"10.1109/CVPR46437.2021.00275","CorpusId":235719338},"title":"Image Change Captioning by Learning from an Auxiliary Task"},{"paperId":"7d1d75e7830a34642508763e3b538a701ee11958","externalIds":{"DBLP":"conf/aaai/ZhangSTXYZ21","ArXiv":"2112.00974","DOI":"10.1609/aaai.v35i4.16452","CorpusId":235306605},"title":"Consensus Graph Representation Learning for Better Grounded Image Captioning"},{"paperId":"eed6293e9d99332dae29dac714ea38eec354311b","externalIds":{"DBLP":"conf/aaai/YangYH21","DOI":"10.1609/aaai.v35i4.16423","CorpusId":235306563},"title":"Object Relation Attention for Image Paragraph Captioning"},{"paperId":"7ba268c6d5489dd3b3c08e3642f1385c6235118e","externalIds":{"DBLP":"conf/aaai/Hu0LZGW021","DOI":"10.1609/aaai.v35i2.16249","CorpusId":235306184},"title":"VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning"},{"paperId":"ffef9f033a6e7383747e674ca7afacda4188d65e","externalIds":{"DBLP":"conf/aaai/Fei21","DOI":"10.1609/aaai.v35i2.16219","CorpusId":235306635},"title":"Partially Non-Autoregressive Image Captioning"},{"paperId":"d2c10cd2403a7847d4380b68dee2cb29c81f532b","externalIds":{"DBLP":"conf/aaai/Fei21a","DOI":"10.1609/aaai.v35i2.16220","CorpusId":235306391},"title":"Memory-Augmented Image Captioning"},{"paperId":"9fa2903b98c9dc5597bc1c05fa6c2733d03ae5bc","externalIds":{"ACL":"2021.naacl-srw.8","DBLP":"journals/corr/abs-2105-08106","ArXiv":"2105.08106","DOI":"10.18653/v1/2021.naacl-srw.8","CorpusId":234762922},"title":"Multi-Modal Image Captioning for the Visually Impaired"},{"paperId":"c27ad8bcef5123c1f6be3561f9173cce03d1c2ef","externalIds":{"MAG":"3157643567","DBLP":"conf/eacl/HondaUHWM21","ACL":"2021.eacl-main.323","ArXiv":"2104.13872","DOI":"10.18653/v1/2021.eacl-main.323","CorpusId":233189606},"title":"Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning"},{"paperId":"17e695d7b00600e0fd6599e1d7703d9f76e796e8","externalIds":{"DBLP":"conf/cvpr/XuNTLD021","ArXiv":"2105.03236","DOI":"10.1109/CVPR46437.2021.01245","CorpusId":234096092},"title":"Towards Accurate Text-based Image Captioning with Content Diversity Exploration"},{"paperId":"38b0567e83386ddc294d6c81b541deacbd8e3c2a","externalIds":{"DBLP":"conf/emnlp/HesselHFBC21","ACL":"2021.emnlp-main.595","ArXiv":"2104.08718","DOI":"10.18653/v1/2021.emnlp-main.595","CorpusId":233296711},"title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning"},{"paperId":"898e66efc50279dd01dc70eb40959683e962250c","externalIds":{"ArXiv":"2104.08376","DBLP":"conf/emnlp/KreissFGP22","ACL":"2022.emnlp-main.308","DOI":"10.18653/v1/2022.emnlp-main.308","CorpusId":253224427},"title":"Concadia: Towards Image-Based Text Generation with a Purpose"},{"paperId":"647120ebe1dbcb9f96aefe4e86cc2809cb351be4","externalIds":{"ArXiv":"2104.01703","DBLP":"conf/aaai/KimZBB21","DOI":"10.1609/aaai.v35i14.17555","CorpusId":233025259},"title":"FixMyPose: Pose Correctional Captioning and Retrieval"},{"paperId":"ada35e2c099fbde9d07a279311f4abe698341cd8","externalIds":{"DBLP":"conf/cvpr/0016J0021","ArXiv":"2103.12204","DOI":"10.1109/CVPR46437.2021.01657","CorpusId":232320659},"title":"Human-like Controllable Image Captioning with Verb-specific Semantic Roles"},{"paperId":"616e0ed02ca024a8c1d4b86167f7486ea92a13d9","externalIds":{"DBLP":"conf/cvpr/ChenGY0E22","ArXiv":"2102.10407","DOI":"10.1109/CVPR52688.2022.01750","CorpusId":235351128},"title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"},{"paperId":"0e5e50179453179fa51bb0793665f2fee48558fa","externalIds":{"ArXiv":"2102.04990","DBLP":"conf/iccv/NguyenTDGN21","DOI":"10.1109/ICCV48922.2021.00144","CorpusId":237108374},"title":"In Defense of Scene Graphs for Image Captioning"},{"paperId":"10161db52bfa53bdab84ae97b47cef2f22119131","externalIds":{"ACL":"2021.eacl-main.48","ArXiv":"2101.11911","DBLP":"conf/eacl/BugliarelloE21","MAG":"3155703583","DOI":"10.18653/v1/2021.eacl-main.48","CorpusId":231719581},"title":"The Role of Syntactic Planning in Compositional Image Captioning"},{"paperId":"cd919b85d465c6cd91f6b2579c597ca5077e7543","externalIds":{"ACL":"2021.eacl-main.104","ArXiv":"2101.09865","DBLP":"journals/corr/abs-2101-09865","DOI":"10.18653/v1/2021.eacl-main.104","CorpusId":231698660},"title":"ECOL-R: Encouraging Copying in Novel Object Captioning with Reinforcement Learning"},{"paperId":"ae7e5a4de962ca4face3bb52b36dfd09db5451d8","externalIds":{"DBLP":"conf/aaai/LuoJSCWHLJ21","ArXiv":"2101.06462","DOI":"10.1609/aaai.v35i3.16328","CorpusId":231632752},"title":"Dual-Level Collaborative Transformer for Image Captioning"},{"paperId":"7217b5d8d0fb753532026cc36b0aaa056960c6f8","externalIds":{"MAG":"3113377113","ArXiv":"2012.07061","DBLP":"journals/corr/abs-2012-07061","DOI":"10.1609/aaai.v35i2.16258","CorpusId":229152503},"title":"Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network"},{"paperId":"4ce5fd3eaea0422c2971111e4d7fe941b64a3c39","externalIds":{"MAG":"3111709576","ArXiv":"2012.05545","DBLP":"journals/corr/abs-2012-05545","DOI":"10.1609/aaai.v35i3.16361","CorpusId":228083558},"title":"Image Captioning with Context-Aware Auxiliary Guidance"},{"paperId":"8deceb13cb3afcfbaab06a2c655f1935445635fe","externalIds":{"DBLP":"conf/cvpr/YangLW0FWZ0L21","MAG":"3112156821","ArXiv":"2012.04638","DOI":"10.1109/CVPR46437.2021.00864","CorpusId":227736593},"title":"TAP: Text-Aware Pre-training for Text-VQA and Text-Caption"},{"paperId":"29d9ae446167342a9a2c0bfad197916ef38e97dc","externalIds":{"ArXiv":"2012.02339","DBLP":"journals/corr/abs-2012-02339","ACL":"2021.conll-1.14","MAG":"3112993007","DOI":"10.18653/v1/2021.conll-1.14","CorpusId":227305870},"title":"Understanding Guided Image Captioning Performance across Domains"},{"paperId":"7a4ba78d377eea9650e5e399a0878e30bd22f648","externalIds":{"DBLP":"conf/cvpr/ChenGNC21","MAG":"3111353235","ArXiv":"2012.02206","DOI":"10.1109/CVPR46437.2021.00321","CorpusId":227305513},"title":"Scan2Cap: Context-aware Dense Captioning in RGB-D Scans"},{"paperId":"7d805e35f17dab7382748130a2ca1bda629cdceb","externalIds":{"DBLP":"conf/nips/Mahajan020","MAG":"3097062010","ArXiv":"2011.00966","CorpusId":226227108},"title":"Diverse Image Captioning with Context-Object Split Latent Spaces"},{"paperId":"306ec4956aa2bb4e29a0b5c8b52d1c0e6007a32b","externalIds":{"MAG":"3099577714","DBLP":"conf/emnlp/TakmazPBF20","ArXiv":"2011.04592","ACL":"2020.emnlp-main.377","DOI":"10.18653/v1/2020.emnlp-main.377","CorpusId":226262261},"title":"Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze"},{"paperId":"2be4e374800a0db69695eb4c558a6653dd258fcd","externalIds":{"MAG":"3098358988","DBLP":"conf/eval4nlp/LeeYDKBJ20","ACL":"2020.eval4nlp-1.4","DOI":"10.18653/v1/2020.eval4nlp-1.4","CorpusId":226283802},"title":"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT"},{"paperId":"28ffdee5f50398202f236fd088fbc0c624b9f9ce","externalIds":{"DBLP":"conf/emnlp/NguyenPHN20","MAG":"3105437121","ArXiv":"2011.08543","ACL":"2020.findings-emnlp.411","DOI":"10.18653/v1/2020.findings-emnlp.411","CorpusId":226283580},"title":"Structural and Functional Decomposition for Personality Image Captioning in a Communication Game"},{"paperId":"60b508a92f73f7e7f14380b45f75d5f0df9548fe","externalIds":{"ACL":"2021.emnlp-main.542","ArXiv":"2010.03743","DBLP":"conf/emnlp/LiuWWO21","DOI":"10.18653/v1/2021.emnlp-main.542","CorpusId":237500283},"title":"Visual News: Benchmark and Challenges in News Image Captioning"},{"paperId":"b41882903384ef849688a325d747fdaad8ecee82","externalIds":{"DBLP":"conf/eccv/ShiYGJ020","ArXiv":"2009.14352","MAG":"3091269500","DOI":"10.1007/978-3-030-58568-6_34","CorpusId":222066860},"title":"Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning"},{"paperId":"e5e76001ef03fcad810330763332d90005c7f527","externalIds":{"MAG":"3088548005","ArXiv":"2009.12313","ACL":"2020.aacl-main.50","DBLP":"conf/ijcnlp/MilewskiMC20","DOI":"10.18653/v1/2020.aacl-main.50","CorpusId":221949339},"title":"Are Scene Graphs Good Enough to Improve Image Captioning?"},{"paperId":"81be56a5783552d5b32463b392ff0499dd86a5ab","externalIds":{"MAG":"3110157234","ArXiv":"2009.03949","DBLP":"conf/eccv/WangFNR20","DOI":"10.1007/978-3-030-58571-6_37","CorpusId":221554607},"title":"Towards Unique and Informative Captioning of Images"},{"paperId":"eb5f7e1244bde88aa45559a3ac9b5274ba57b78a","externalIds":{"DBLP":"journals/corr/abs-2008-02693","ArXiv":"2008.02693","MAG":"3107492437","DOI":"10.1007/978-3-030-58601-0_1","CorpusId":221006339},"title":"Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards"},{"paperId":"66ed8795eb6de5d2a6b204baac9378d6d28136cc","externalIds":{"MAG":"3043933971","ArXiv":"2007.11731","DBLP":"conf/eccv/ZhongWC0L20","DOI":"10.1007/978-3-030-58568-6_13","CorpusId":220713307},"title":"Comprehensive Image Captioning via Scene Graph Decomposition"},{"paperId":"2320f853059c29ce7e70409fa559074d727da5a2","externalIds":{"DBLP":"conf/eccv/DengDTW20","ArXiv":"2007.09580","MAG":"3043320985","DOI":"10.1007/978-3-030-58601-0_42","CorpusId":220646486},"title":"Length-Controllable Image Captioning"},{"paperId":"b6473852e19ebb31161b2f62d53912b431231fa5","externalIds":{"MAG":"3042227549","DBLP":"journals/corr/abs-2007-06877","ArXiv":"2007.06877","DOI":"10.1007/978-3-030-58452-8_22","CorpusId":220514258},"title":"Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets"},{"paperId":"1c12bd0779d4cc5148cb8b9f82e9b1469866785c","externalIds":{"DBLP":"conf/nips/ChiaroTB020","ArXiv":"2007.06271","MAG":"3041184466","CorpusId":220496733},"title":"RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning"},{"paperId":"3b85f64b0a7c8eabd11476fa870b14b4ec696166","externalIds":{"DBLP":"conf/acl/ParkKYPC20","ACL":"2020.acl-srw.14","MAG":"3037607347","DOI":"10.18653/v1/2020.acl-srw.14","CorpusId":220058510},"title":"Feature Difference Makes Sense: A medical image captioning model exploiting feature difference and tag information"},{"paperId":"0b4d5b7cef06b66182db80803f783d077e3637b6","externalIds":{"DBLP":"conf/acl/YiDH20","ACL":"2020.acl-main.93","MAG":"3034417909","DOI":"10.18653/v1/2020.acl-main.93","CorpusId":220045419},"title":"Improving Image Captioning Evaluation by Considering Inter References Variance"},{"paperId":"7e0f91e51ee372939c96714c7919dde6dc756849","externalIds":{"MAG":"3034733309","DBLP":"journals/corr/abs-2006-11807","ArXiv":"2006.11807","ACL":"2020.acl-main.664","DOI":"10.18653/v1/2020.acl-main.664","CorpusId":219965869},"title":"Improving Image Captioning with Better Use of Caption"},{"paperId":"474952c4ceeec59d2677c60e92ebbf6d34140b2d","externalIds":{"DBLP":"conf/acl/AlikhaniSLSS20","MAG":"3035532688","ACL":"2020.acl-main.583","ArXiv":"2005.00908","DOI":"10.18653/v1/2020.acl-main.583","CorpusId":220047809},"title":"Cross-modal Coherence Modeling for Caption Generation"},{"paperId":"c858aeb711e0f9e0b9752d887f965096481cc37d","externalIds":{"MAG":"3021989929","ArXiv":"2004.14667","ACL":"2020.evalnlgeval-1.4","DBLP":"journals/corr/abs-2004-14667","CorpusId":216869168},"title":"NUBIA: NeUral Based Interchangeability Assessor for Text Generation"},{"paperId":"464d2c88185abdb6c17727000f247b39eddf3de4","externalIds":{"MAG":"3034689697","DBLP":"conf/cvpr/TranMX20","ArXiv":"2004.08070","DOI":"10.1109/CVPR42600.2020.01305","CorpusId":215814392},"title":"Transform and Tell: Entity-Aware News Image Captioning"},{"paperId":"4df184d6a74f1ffd84b644735c9afb5060552770","externalIds":{"MAG":"2996984511","DBLP":"conf/aaai/HouWZQJL20","DOI":"10.1609/AAAI.V34I07.6731","CorpusId":213104909},"title":"Joint Commonsense and Relation Reasoning for Image and Video Captioning"},{"paperId":"12fba4aca633fd7aff247b669be3697cb7ab2e0d","externalIds":{"MAG":"2998159097","DBLP":"conf/aaai/LiuWXZXSY20","DOI":"10.1609/AAAI.V34I07.6826","CorpusId":213379814},"title":"Interactive Dual Generative Adversarial Networks for Image Captioning"},{"paperId":"076b02b481a41f1e07b8a2bdbe0ac8d946f9872e","externalIds":{"MAG":"2998409107","DBLP":"conf/aaai/ZhangYLZ20","DOI":"10.1609/AAAI.V34I05.6503","CorpusId":213585423},"title":"Learning Long- and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption"},{"paperId":"9421fa26257e6a8d59bb874cf3b376c6d4c4118b","externalIds":{"DBLP":"conf/aaai/ZhaoWZ20","MAG":"2997248215","DOI":"10.1609/AAAI.V34I07.6998","CorpusId":213205959},"title":"MemCap: Memorizing Style Knowledge for Image Captioning"},{"paperId":"35da4164fd013e15404a58c955c09fd9c87f30c0","externalIds":{"MAG":"2998116350","DBLP":"conf/aaai/CaoHW0FJX20","DOI":"10.1609/AAAI.V34I07.6620","CorpusId":213033236},"title":"Feature Deformation Meta-Networks in Image Captioning of Novel Objects"},{"paperId":"88c86523d500d636f453647385ddaa04085b5f1b","externalIds":{"DBLP":"conf/emnlp/NieCP20","ArXiv":"2004.14451","MAG":"3099883243","ACL":"2020.findings-emnlp.173","DOI":"10.18653/v1/2020.findings-emnlp.173","CorpusId":216868075},"title":"Pragmatic Issue-Sensitive Image Captioning"},{"paperId":"c936e70cc1de52c5ad0ed5ec7a219bd25a46902c","externalIds":{"MAG":"3035323998","ArXiv":"2004.00390","DBLP":"journals/corr/abs-2004-00390","DOI":"10.1109/cvpr42600.2020.00483","CorpusId":214743250},"title":"More Grounded Image Captioning by Distilling Image-Text Matching Model"},{"paperId":"4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0","externalIds":{"MAG":"3014966943","DBLP":"conf/cvpr/PanYLM20","ArXiv":"2003.14080","DOI":"10.1109/cvpr42600.2020.01098","CorpusId":214727638},"title":"X-Linear Attention Networks for Image Captioning"},{"paperId":"33eadd4e666a894306a22ba0839c5e0cef77280e","externalIds":{"ArXiv":"2003.12462","MAG":"3106859150","DBLP":"journals/corr/abs-2003-12462","DOI":"10.1007/978-3-030-58536-5_44","CorpusId":214693197},"title":"TextCaps: a Dataset for Image Captioning with Reading Comprehension"},{"paperId":"833560cd68a3e3d1be1bc650756dd6c679798551","externalIds":{"MAG":"3035160838","DBLP":"journals/corr/abs-2003-08897","ArXiv":"2003.08897","DOI":"10.1109/cvpr42600.2020.01034","CorpusId":213176464},"title":"Normalized and Geometry-Aware Self-Attention Network for Image Captioning"},{"paperId":"96485bda4f4118da249cc8a898230281ac8040a7","externalIds":{"ArXiv":"2003.03749","MAG":"3010356384","DBLP":"conf/cvpr/ChenJ20","DOI":"10.1109/CVPR42600.2020.01090","CorpusId":212633783},"title":"Better Captioning With Sequence-Level Exploration"},{"paperId":"e9419436682726232e1b37a04c53bba919b12025","externalIds":{"DBLP":"conf/cvpr/SammaniM20","MAG":"3034316193","ArXiv":"2003.03107","DOI":"10.1109/cvpr42600.2020.00486","CorpusId":212628625},"title":"Show, Edit and Tell: A Framework for Editing Image Captions"},{"paperId":"b4916e339caef2d2a98e633e1f0b2144e2b0c9e2","externalIds":{"ArXiv":"2003.00387","MAG":"3008585839","DBLP":"conf/cvpr/ChenJWW20","DOI":"10.1109/cvpr42600.2020.00998","CorpusId":211678371},"title":"Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs"},{"paperId":"8b2c80788f789d4ce7849c13943fa920d9e3c95f","externalIds":{"DBLP":"journals/corr/abs-2002-08565","MAG":"3007456797","ArXiv":"2002.08565","DOI":"10.1007/978-3-030-58520-4_25","CorpusId":211204968},"title":"Captioning Images Taken by People Who Are Blind"},{"paperId":"85079d64d6fdd0ba5318fda119d152f2d2946391","externalIds":{"DBLP":"journals/corr/abs-2001-05876","MAG":"2998988444","ArXiv":"2001.05876","DOI":"10.1609/AAAI.V34I07.6898","CorpusId":210702035},"title":"Show, Recall, and Tell: Image Captioning with Recall Mechanism"},{"paperId":"fc9c52f55ffe0e860b1bb4222fe86cce60c05551","externalIds":{"DBLP":"journals/corr/abs-1912-08226","MAG":"2995753487","ArXiv":"1912.08226","DOI":"10.1109/cvpr42600.2020.01059","CorpusId":219635470},"title":"Meshed-Memory Transformer for Image Captioning"},{"paperId":"02d31c9de118777103de26d784008d52dcce68d7","externalIds":{"DOI":"10.1007/978-3-642-41714-6_72394","CorpusId":142007213},"title":"Grounding"},{"paperId":"05cc1fee1c0109c5859bcae9b6111c81825c45c1","externalIds":{"DOI":"10.1080/13602365.2019.1714340","CorpusId":219643295},"title":"Captions"},{"paperId":"93447c939657029b6305053599f51e78ba8a4c3d","externalIds":{"MAG":"2989377923","DBLP":"conf/emnlp/WangBC19","ACL":"D19-6405","DOI":"10.18653/v1/D19-6405","CorpusId":207914496},"title":"On the Role of Scene Graphs in Image Captioning"},{"paperId":"6fb03dcb0d57eb74d3395e598ae1712e3e926ec2","externalIds":{"MAG":"2987118624","DBLP":"conf/iccv/Vered0AC19","DOI":"10.1109/ICCV.2019.00899","CorpusId":208000044},"title":"Joint Optimization for Cooperative Image Captioning"},{"paperId":"7c4530882cfcef1d2b4aa2996f494dfac626b5d9","externalIds":{"MAG":"2990818246","DBLP":"conf/iccv/LiZLY19","DOI":"10.1109/ICCV.2019.00902","CorpusId":207982683},"title":"Entangled Transformer for Image Captioning"},{"paperId":"b774d6cd89cb27e62f03f183b89b7cacc412e131","externalIds":{"DBLP":"conf/iccv/LiuT0G19","MAG":"2989489923","DOI":"10.1109/ICCV.2019.00434","CorpusId":204964783},"title":"Generating Diverse and Descriptive Image Captions Using Visual Paraphrases"},{"paperId":"4b5231149f566fc8a78797b6fb448f9bca416380","externalIds":{"MAG":"2982151481","DBLP":"conf/iccv/GeYZZ019","ArXiv":"1910.06475","DOI":"10.1109/ICCV.2019.00184","CorpusId":204575831},"title":"Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style"},{"paperId":"6648b4db5f12c30941ea78c695e77aded19672bb","externalIds":{"MAG":"2997591391","ArXiv":"1909.11059","DBLP":"journals/corr/abs-1909-11059","DOI":"10.1609/AAAI.V34I07.7005","CorpusId":202734445},"title":"Unified Vision-Language Pre-Training for Image Captioning and VQA"},{"paperId":"fcf74c4a3042eca2994aac122f1ed045be098902","externalIds":{"MAG":"2973642807","ArXiv":"1909.09060","DBLP":"journals/corr/abs-1909-09060","CorpusId":202677610},"title":"Adaptively Aligned Image Captioning via Adaptive Attention Time"},{"paperId":"32c9a0acee8d236c553395052c29a6d853d8ea2d","externalIds":{"MAG":"2986729057","DBLP":"conf/conll/NikolausALAE19","ACL":"K19-1009","ArXiv":"1909.04402","DOI":"10.18653/v1/K19-1009","CorpusId":202542872},"title":"Compositional Generalization in Image Captioning"},{"paperId":"05106b86ec45914d1136719d311078182d437872","externalIds":{"MAG":"2983141445","ArXiv":"1909.03918","DBLP":"conf/iccv/YaoPL019","DOI":"10.1109/ICCV.2019.00271","CorpusId":202540514},"title":"Hierarchy Parsing for Image Captioning"},{"paperId":"94c6e5ccd67be60a5ced11d0a5c59e0ab0f749d4","externalIds":{"DBLP":"journals/corr/abs-1909-02217","MAG":"2971866238","ArXiv":"1909.02217","ACL":"D19-1156","DOI":"10.18653/v1/D19-1156","CorpusId":202541461},"title":"REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning"},{"paperId":"0ed9efc8bc8b5c8af907479cbf0c8a184530ffbc","externalIds":{"DBLP":"journals/corr/abs-1909-02201","MAG":"3103211586","ArXiv":"1909.02201","ACL":"D19-1208","DOI":"10.18653/v1/D19-1208","CorpusId":202537756},"title":"Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach"},{"paperId":"ce01073130ff984eb43cbf43f6bdcbe4d5a09df9","externalIds":{"DBLP":"journals/corr/abs-1909-02050","MAG":"2972303997","ArXiv":"1909.02050","ACL":"D19-1220","DOI":"10.18653/v1/D19-1220","CorpusId":202537622},"title":"TIGEr: Text-to-Image Grounding for Image Caption Evaluation"},{"paperId":"3b6e6822eabe2f64192a1965c23e38043866319c","externalIds":{"MAG":"2972206754","ACL":"D19-1155","ArXiv":"1909.02097","DBLP":"journals/corr/abs-1909-02097","DOI":"10.18653/v1/D19-1155","CorpusId":202537727},"title":"Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering"},{"paperId":"0db903dd28a3be3e57f40033c16cce574231f78e","externalIds":{"MAG":"2970754407","ArXiv":"1908.11824","DBLP":"journals/corr/abs-1908-11824","DOI":"10.1109/ICCV.2019.00898","CorpusId":201698351},"title":"Reflective Decoding Network for Image Captioning"},{"paperId":"a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be","externalIds":{"DBLP":"conf/iccv/Laina0N19","MAG":"2969295463","ArXiv":"1908.09317","DOI":"10.1109/ICCV.2019.00751","CorpusId":201666330},"title":"Towards Unsupervised Image Captioning With Shared Multimodal Embeddings"},{"paperId":"49d46b0245475067bb7192d9bb1538701ae1c014","externalIds":{"ArXiv":"1908.08529","MAG":"2988793532","DBLP":"conf/iccv/AnejaABS19","DOI":"10.1109/ICCV.2019.00436","CorpusId":201327983},"title":"Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning"},{"paperId":"4c163d4942117179d3e97182e1b280027d7d60a9","externalIds":{"DBLP":"conf/iccv/HuangWCW19","MAG":"2968549119","ArXiv":"1908.06954","DOI":"10.1109/ICCV.2019.00473","CorpusId":201070367},"title":"Attention on Attention for Image Captioning"},{"paperId":"88bb4ce649cbdda6396cef96049afcd67a98ca12","externalIds":{"DBLP":"conf/aaai/LiCL19","MAG":"2903928149","DOI":"10.1609/AAAI.V33I01.33018626","CorpusId":69881897},"title":"Meta Learning for Image Captioning"},{"paperId":"e9ea5e8e018fa2d1f508aa0310469ba11a61345a","externalIds":{"MAG":"2903617461","DBLP":"conf/aaai/0002JH19","DOI":"10.1609/AAAI.V33I01.33018650","CorpusId":57427597},"title":"Learning Object Context for Dense Captioning"},{"paperId":"0e69fc5911249910407db61442410808c65b93b9","externalIds":{"MAG":"2905288264","DBLP":"conf/aaai/Song0QC19","DOI":"10.1609/AAAI.V33I01.33018885","CorpusId":69525548},"title":"Connecting Language to Images: A Progressive Attention-Guided Network for Simultaneous Image Captioning and Language Grounding"},{"paperId":"b312f921c937cf9b11cc3b7005613576f5607fcf","externalIds":{"DBLP":"conf/aaai/WangC019","MAG":"2904551248","DOI":"10.1609/AAAI.V33I01.33018957","CorpusId":69715157},"title":"Hierarchical Attention Network for Image Captioning"},{"paperId":"49c0796d3cbc811c6e27470afdf191ba382cfae5","externalIds":{"DBLP":"conf/aaai/GaoFSLXS19","MAG":"2965846473","DOI":"10.1609/aaai.v33i01.33018320","CorpusId":198190077},"title":"Deliberate Attention Networks for Image Captioning"},{"paperId":"aaf5e3afd61d0df6c483ca32faf8e7a9198b1557","externalIds":{"MAG":"2949792119","DBLP":"conf/acl/FanWWH19","ACL":"P19-1652","DOI":"10.18653/v1/P19-1652","CorpusId":196206846},"title":"Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning"},{"paperId":"fbcea7715f62395fd5a83423d57556e8acca9a62","externalIds":{"ArXiv":"1907.09340","DBLP":"conf/acl/MadhyasthaWS19","MAG":"2963696820","ACL":"P19-1654","DOI":"10.18653/v1/P19-1654","CorpusId":196206571},"title":"VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions"},{"paperId":"68490e9e0bca2f0b1ae2ca636effcd8fc63d2008","externalIds":{"DBLP":"journals/corr/abs-1906-08876","ACL":"P19-1650","ArXiv":"1906.08876","MAG":"2952003359","DOI":"10.18653/v1/P19-1650","CorpusId":195317077},"title":"Informative Image Captioning with External Sources of Information"},{"paperId":"b499228aa74b59be32711c3926e44de208d6b636","externalIds":{"MAG":"2971310675","DBLP":"conf/nips/HerdadeKBS19","ArXiv":"1906.05963","CorpusId":189898359},"title":"Image Captioning: Transforming Objects into Words"},{"paperId":"f692df9ca116884860d580902fa642370bd1be5d","externalIds":{"MAG":"2969557970","DBLP":"conf/eccv/MaKAVRK20","DOI":"10.1007/978-3-030-58523-5_21","CorpusId":221093733},"title":"Learning to Generate Grounded Visual Captions Without Localization Supervision"},{"paperId":"4fef1313fd4948fed09dee318e2e231216c4fb3b","externalIds":{"MAG":"2955956881","DBLP":"conf/cvpr/GuoLYLL19","DOI":"10.1109/CVPR.2019.00433","CorpusId":198117779},"title":"MSCap: Multi-Style Image Captioning With Unpaired Stylized Text"},{"paperId":"e0c0ac3bb66203c32be81193fabeee44c3585582","externalIds":{"MAG":"2965697393","DBLP":"conf/cvpr/QinDZL19","DOI":"10.1109/CVPR.2019.00856","CorpusId":201143583},"title":"Look Back and Predict Forward in Image Captioning"},{"paperId":"92ccf5a39c63cb5e1639be518e6db2e357acd58e","externalIds":{"MAG":"2945222541","DOI":"10.3390/APP9102024","CorpusId":181562553},"title":"A Systematic Literature Review on Image Captioning"},{"paperId":"6dc67482ee0530e9ff535775891481ed9fd5f6ad","externalIds":{"ArXiv":"1904.11251","DBLP":"conf/cvpr/LiYPCM19","MAG":"2941236977","DOI":"10.1109/CVPR.2019.01278","CorpusId":131779161},"title":"Pointing Novel Objects in Image Captioning"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"45f9856d418527d23dc7c89197627fa1f3b215f9","externalIds":{"ArXiv":"1904.08608","DBLP":"journals/corr/abs-1904-08608","MAG":"2938690944","DOI":"10.1109/ICCV.2019.00435","CorpusId":121347508},"title":"Learning to Collocate Neural Modules for Image Captioning"},{"paperId":"b6b3d6a37e7e77f5d5c763a4abeade256324268c","externalIds":{"MAG":"2935791917","DBLP":"conf/cvpr/GaoWWM019","ArXiv":"1904.06861","DOI":"10.1109/CVPR.2019.00646","CorpusId":119304359},"title":"Self-Critical N-Step Training for Image Captioning"},{"paperId":"eba62fe8050e475ffe533b9f70db538074d8d0d1","externalIds":{"MAG":"2927788011","DBLP":"conf/cvpr/YinSLYWS19","ArXiv":"1904.01410","DOI":"10.1109/CVPR.2019.00640","CorpusId":91184088},"title":"Context and Attribute Grounded Dense Captioning"},{"paperId":"908c6b1577a1f5309ae183daf2e24363039f22a8","externalIds":{"MAG":"2982573303","DBLP":"conf/cvpr/BitenGRK19","ArXiv":"1904.01475","DOI":"10.1109/CVPR.2019.01275","CorpusId":91184120},"title":"Good News, Everyone! Context Driven Entity-Aware Captioning for News Images"},{"paperId":"5e5c35db1643b3220202c6576eca064f1543866a","externalIds":{"DBLP":"journals/corr/abs-1903-12020","MAG":"2923793772","ArXiv":"1903.12020","DOI":"10.1109/CVPR.2019.00432","CorpusId":85543308},"title":"Describing Like Humans: On Diversity in Image Captioning"},{"paperId":"f4c60c3ee4904d61ff84c9d4c15c9aecdcf04cdc","externalIds":{"ArXiv":"1903.10658","DBLP":"conf/iccv/GuJCZYW19","MAG":"2924200781","DOI":"10.1109/ICCV.2019.01042","CorpusId":85517761},"title":"Unpaired Image Captioning via Scene Graph Alignments"},{"paperId":"79de42d6ca8d1bf2952c46eb74e6e0561f979257","externalIds":{"ArXiv":"1903.05942","MAG":"2976658881","DBLP":"conf/cvpr/0003COK19","DOI":"10.1109/CVPR.2019.00643","CorpusId":76665989},"title":"Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning"},{"paperId":"e4a91b8c1259e7744784d922f250dd77ea951e9f","externalIds":{"MAG":"2965129775","DBLP":"conf/iccv/HeTBP19","DOI":"10.1109/ICCV.2019.00862","CorpusId":199441964},"title":"Human Attention in Image Captioning: Dataset and Analysis"},{"paperId":"5abe916562fad8306e3f4e571f83015047f0be1d","externalIds":{"MAG":"2938232066","DBLP":"conf/iccv/ParkDR19","DOI":"10.1109/ICCV.2019.00472","CorpusId":119189647},"title":"Robust Change Captioning"},{"paperId":"f6feb1af1809dfd872d868dfcc13021cc42f496c","externalIds":{"MAG":"2902243109","DBLP":"journals/corr/abs-1812-02378","ArXiv":"1812.02378","DOI":"10.1109/CVPR.2019.01094","CorpusId":54460890},"title":"Auto-Encoding Scene Graphs for Image Captioning"},{"paperId":"b4cb0a7617212eb40c537f4053d571faa4b8227c","externalIds":{"MAG":"2996319563","DBLP":"conf/iccv/ShenKF19","ArXiv":"1812.00235","DOI":"10.1109/ICCV.2019.01049","CorpusId":84843010},"title":"Learning to Caption Images Through a Lifetime by Asking Questions"},{"paperId":"580fd9a601314ea32dc85ec98267b411dd3465cf","externalIds":{"MAG":"2950985651","DBLP":"journals/corr/abs-1811-10787","ArXiv":"1811.10787","DOI":"10.1109/CVPR.2019.00425","CorpusId":53771922},"title":"Unsupervised Image Captioning"},{"paperId":"8e59cf8c3becbedced0089028a1cddac8b19b251","externalIds":{"DBLP":"conf/cvpr/CorniaBC19","MAG":"2901988662","ArXiv":"1811.10652","DOI":"10.1109/CVPR.2019.00850","CorpusId":53778132},"title":"Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions"},{"paperId":"23e943809c131c50dc90c1d308373febc60b9029","externalIds":{"MAG":"2901291904","DBLP":"journals/corr/abs-1811-07662","ArXiv":"1811.07662","DOI":"10.1109/CVPR.2019.00859","CorpusId":53720830},"title":"Intention Oriented Image Captions With Guiding Objects"},{"paperId":"de6ab8cd9d402c976082b707b1207c3ad49ae204","externalIds":{"MAG":"2893913486","ACL":"W18-5455","DBLP":"conf/emnlp/MadhyasthaWS18","DOI":"10.18653/v1/W18-5455","CorpusId":52198665},"title":"End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space"},{"paperId":"c677000c9078fdff8622be15a37db7d4945f36c2","externalIds":{"MAG":"2982260276","DBLP":"journals/corr/abs-1810-10665","ArXiv":"1810.10665","DOI":"10.1109/CVPR.2019.01280","CorpusId":53022581},"title":"Engaging Image Captioning via Personality"},{"paperId":"345a222fef6f5c1415056319ae7e87a369940d3f","externalIds":{"MAG":"2950316689","DBLP":"journals/corr/abs-1810-09630","ArXiv":"1810.09630","CorpusId":53036732},"title":"A Neural Compositional Paradigm for Image Captioning"},{"paperId":"7e27d44e3fac723ccb703e0a83b22711bd42efe8","externalIds":{"MAG":"2896348597","ArXiv":"1810.04020","DBLP":"journals/csur/HossainSSL19","DOI":"10.1145/3295748","CorpusId":52947736},"title":"A Comprehensive Survey of Deep Learning for Image Captioning"},{"paperId":"a78199a2cc678818489087454fe1150db4870196","externalIds":{"MAG":"2953137234","DBLP":"journals/corr/abs-1809-06227","ArXiv":"1809.06227","ACL":"D18-1083","DOI":"10.18653/v1/D18-1083","CorpusId":52290620},"title":"Improving Reinforcement Learning Based Image Captioning with Natural Language Prior"},{"paperId":"5e79138fd656ebea59eb008fcb97c97be7be8007","externalIds":{"DBLP":"journals/corr/abs-1809-06214","MAG":"2890231609","ArXiv":"1809.06214","DOI":"10.1609/AAAI.V33I01.33018151","CorpusId":52283611},"title":"Unsupervised Stylish Image Description Generation via Domain Layer Norm"},{"paperId":"5cab3ce511ec8345d16a28c00094a2800b3919ce","externalIds":{"MAG":"2949362007","DBLP":"conf/eccv/ChenZ18","ArXiv":"1904.00767","DOI":"10.1007/978-3-030-01252-6_5","CorpusId":52955413},"title":"Boosted Attention: Leveraging Human Attention for Image Captioning"},{"paperId":"0000fcfd467a19cf0e59169c2f07d730a0f3a8b9","externalIds":{"MAG":"2890531016","ArXiv":"1809.07041","DBLP":"conf/eccv/YaoPLM18","DOI":"10.1007/978-3-030-01264-9_42","CorpusId":52304560},"title":"Exploring Visual Relationship for Image Captioning"},{"paperId":"fa9189749a4c95c45ec7d98db49e5f736c51760e","externalIds":{"MAG":"2895708641","DBLP":"conf/eccv/SharifWBS18","DOI":"10.1007/978-3-030-01237-3_3","CorpusId":52956670},"title":"NNEval: Neural Network Based Evaluation Metric for Image Captioning"},{"paperId":"6610cecb59fdd108c8fcb73cac3562fa10d5e845","externalIds":{"MAG":"2889720786","DBLP":"conf/bmvc/MadhyasthaWS18","ArXiv":"1809.04144","CorpusId":52935881},"title":"End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space"},{"paperId":"4921243268c81d0d6db99053a9d004852225a622","externalIds":{"MAG":"2962735233","DBLP":"conf/emnlp/RohrbachHBDS18","ArXiv":"1809.02156","ACL":"D18-1437","DOI":"10.18653/v1/D18-1437","CorpusId":52176506},"title":"Object Hallucination in Image Captioning"},{"paperId":"091433bc8791bb66797b519811834a8a53af622d","externalIds":{"ACL":"D18-1013","DBLP":"conf/emnlp/LiuRLW018","MAG":"2888321701","ArXiv":"1808.08732","DOI":"10.18653/v1/D18-1013","CorpusId":52097135},"title":"simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions"},{"paperId":"04cd9168dcf1a0d2cf01db95a1af53d0900bc346","externalIds":{"DBLP":"conf/eccv/JiangMJLZ18","ArXiv":"1807.09986","MAG":"2949597956","DOI":"10.1007/978-3-030-01216-8_31","CorpusId":50786845},"title":"Recurrent Fusion Network for Image Captioning"},{"paperId":"94e3b75a6732b5918c4c2b87d127a9216ff07efc","externalIds":{"MAG":"2953087599","DBLP":"journals/corr/abs-1807-09958","ArXiv":"1807.09958","DOI":"10.1007/978-3-030-01228-1_18","CorpusId":50785149},"title":"Rethinking the Form of Latent States in Image Captioning"},{"paperId":"abc7998326cc4fc3c9c0c3a9ede8ae2538439966","externalIds":{"MAG":"2952640344","ArXiv":"1807.03871","DBLP":"conf/eccv/ChenZYFWJL18","DOI":"10.1007/978-3-030-01249-6_32","CorpusId":49667950},"title":"\"Factual\" or \"Emotional\": Stylized Image Captioning with Adaptive Learning and Attention"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"ceabd7ff28ce2d501511da998252aeb938adc98b","externalIds":{"MAG":"2963049308","DBLP":"journals/corr/abs-1806-06004","ArXiv":"1806.06004","CorpusId":49274791},"title":"Partially-Supervised Image Captioning"},{"paperId":"f79c03f977d1c9acb71d87301272682422b0b14f","externalIds":{"DBLP":"journals/vc/LiuXW19","MAG":"2805954242","DOI":"10.1007/s00371-018-1566-y","CorpusId":47012886},"title":"A survey on deep neural network-based image captioning"},{"paperId":"6d3d61ef9b5ff6d41badbc3d40ea23acbbc9c3fe","externalIds":{"MAG":"2951859929","DBLP":"journals/corr/abs-1806-06422","ArXiv":"1806.06422","DOI":"10.1109/CVPR.2018.00608","CorpusId":46908873},"title":"Learning to Evaluate Image Captioning"},{"paperId":"2115fe369b3a6b859c6992ba023d5c11b1689801","externalIds":{"DBLP":"conf/cvpr/ChenJSWS18","MAG":"2798959609","DOI":"10.1109/CVPR.2018.00146","CorpusId":52830169},"title":"GroupCap: Group-Based Image Captioning with Structured Relevance and Diversity Constraints"},{"paperId":"e7fe886600399448f3282c8da8fd98ab7e50eae3","externalIds":{"MAG":"2901433466","ArXiv":"1805.12589","DBLP":"conf/cvpr/DeshpandeAWSF19","DOI":"10.1109/CVPR.2019.01095","CorpusId":53739686},"title":"Fast, Diverse and Accurate Image Captioning Guided by Part-Of-Speech"},{"paperId":"a484b7eda0e5389ae62ab1549f27594050a60f71","externalIds":{"MAG":"2962968665","DBLP":"journals/corr/abs-1805-08170","ArXiv":"1805.08170","CorpusId":29154127},"title":"Turbo Learning for Captionbot and Drawingbot"},{"paperId":"0daa3a4118e00b9f63b2d014a16ff1bc3ca9ff7e","externalIds":{"MAG":"2951876174","DBLP":"journals/corr/abs-1805-07112","ArXiv":"1805.07112","DOI":"10.1609/AAAI.V33I01.33018142","CorpusId":29152002},"title":"Improving Image Captioning with Conditional Generative Adversarial Nets"},{"paperId":"beeebd2af0d8f130dcf234231de4569d584cb7fd","externalIds":{"ArXiv":"1805.07030","MAG":"2952201055","DBLP":"conf/cvpr/MathewsXH18","DOI":"10.1109/CVPR.2018.00896","CorpusId":29161017},"title":"SemStyle: Learning to Generate Stylised Image Captions Using Unaligned Text"},{"paperId":"24fdb40c354599ee33a25530c3fa7b9ebc75e840","externalIds":{"DBLP":"conf/cvpr/DogninMMRS19","MAG":"2967223102","DOI":"10.1109/CVPR.2019.01071","CorpusId":174801318},"title":"Adversarial Semantic Alignment for Improved Image Captions"},{"paperId":"1e8feffa2280e41ceb864b940869c5408db89285","externalIds":{"MAG":"2798690958","ArXiv":"1804.07889","DBLP":"conf/emnlp/LuWHJC18","ACL":"D18-1435","DOI":"10.18653/v1/D18-1435","CorpusId":5083989},"title":"Entity-aware Image Caption Generation"},{"paperId":"3dc2c3be0796f65154d2106ed4442889c84546df","externalIds":{"MAG":"2952957242","DBLP":"conf/aaai/JiangMCZL18","ArXiv":"1804.00887","DOI":"10.1609/aaai.v32i1.12283","CorpusId":4569354},"title":"Learning to Guide Decoding for Image Captioning"},{"paperId":"025f852b227766c3a5dc914ded6f6c0ae137c617","externalIds":{"MAG":"2797753589","ACL":"N18-2070","ArXiv":"1804.05417","DBLP":"conf/naacl/Cohn-GordonGP18","DOI":"10.18653/v1/N18-2070","CorpusId":4899062},"title":"Pragmatically Informative Image Captioning with Character-Level Inference"},{"paperId":"85e2b2c35b916b1ee4926c155065d01b21c80c60","externalIds":{"DBLP":"journals/corr/abs-1803-11439","MAG":"2964253311","ArXiv":"1803.11439","DOI":"10.1109/CVPR.2018.00834","CorpusId":4717181},"title":"Regularizing RNNs for Caption Generation by Reconstructing the Past with the Present"},{"paperId":"b0c5dc3fa19a2bc97606ccb6f55226b913984395","externalIds":{"DBLP":"journals/corr/abs-1807-00517","MAG":"2810512327","ArXiv":"1803.09797","DOI":"10.1007/978-3-030-01219-9_47","CorpusId":4384334},"title":"Women also Snowboard: Overcoming Bias in Captioning Models"},{"paperId":"caab1c1d53718315f54bc4df42eb9a727fa18483","externalIds":{"MAG":"2963170456","ArXiv":"1803.08314","DBLP":"conf/eccv/LiuLSCW18","DOI":"10.1007/978-3-030-01267-0_21","CorpusId":4100657},"title":"Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data"},{"paperId":"05544876b7bc58b59b9ec5a8ee09e3ce9b4791ce","externalIds":{"MAG":"2950814174","DBLP":"conf/eccv/GuJCW18","ArXiv":"1803.05526","DOI":"10.1007/978-3-030-01246-5_31","CorpusId":3919314},"title":"Unpaired Image Captioning by Language Pivoting"},{"paperId":"7c1802d8d43dfe783650a03f03d41609fa5ae91e","externalIds":{"ArXiv":"1803.04376","MAG":"2790600496","DBLP":"conf/cvpr/LuoPCS18","DOI":"10.1109/CVPR.2018.00728","CorpusId":3875506},"title":"Discriminability Objective for Training Descriptive Captions"},{"paperId":"7d3dd33950f4a1be56eb88c0791263b3e3a6deee","externalIds":{"MAG":"2963270032","ACL":"N18-1198","DBLP":"conf/naacl/WangMS18","ArXiv":"1805.00314","DOI":"10.18653/v1/N18-1198","CorpusId":21387831},"title":"Object Counts! Bringing Explicit Detections Back into Image Captioning"},{"paperId":"86be5c90c4128ec59b1c320a16996bb5de68624e","externalIds":{"MAG":"2952320212","DBLP":"conf/sigir/ZhuLZGZWY18","ArXiv":"1802.01886","DOI":"10.1145/3209978.3210080","CorpusId":3636178},"title":"Texygen: A Benchmarking Platform for Text Generation Models"},{"paperId":"60af58a2435fe758fe9a172f2009efbb89584f58","externalIds":{"DBLP":"conf/aaai/ChenDZH18","MAG":"2788527657","DOI":"10.1609/aaai.v32i1.12263","CorpusId":3585731},"title":"Temporal-Difference Learning With Sampling Baseline for Image Captioning"},{"paperId":"96e53b9c4e7ce52b32f090ceb3b069786559a2b7","externalIds":{"MAG":"2951280437","ACL":"P18-1241","DBLP":"conf/acl/HsiehYCZC18","DOI":"10.18653/v1/P18-1241","CorpusId":44220219},"title":"Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning"},{"paperId":"9fb5e3db385588f671b11cfc8bf18efb90ee7b19","externalIds":{"DBLP":"conf/cvpr/AnejaDS18","ArXiv":"1711.09151","MAG":"2952949531","DOI":"10.1109/CVPR.2018.00583","CorpusId":35543299},"title":"Convolutional Image Captioning"},{"paperId":"82247c9e74ddebb4dce65560ee69620579358f2d","externalIds":{"DBLP":"journals/corr/abs-1711-07068","MAG":"2951876942","ArXiv":"1711.07068","CorpusId":314973},"title":"Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space"},{"paperId":"83d66c1f808962536a68418587b691f30221c5a1","externalIds":{"MAG":"2963360726","ArXiv":"1710.02534","DBLP":"conf/nips/DaiL17","CorpusId":7900381},"title":"Contrastive Learning for Image Captioning"},{"paperId":"7f14e73dade94b8b1f276dcd91257aa7de5f19d7","externalIds":{"MAG":"2951855380","ArXiv":"1709.03376","DBLP":"journals/corr/abs-1709-03376","DOI":"10.1609/aaai.v32i1.12266","CorpusId":3826624},"title":"Stack-Captioning: Coarse-to-Fine Learning for Image Captioning"},{"paperId":"135bafc83e9a73c88e759f98a28edfdb5c02f81d","externalIds":{"DBLP":"journals/corr/ZhaoWYOC17","MAG":"2951685111","ArXiv":"1707.09457","ACL":"D17-1323","DOI":"10.18653/v1/D17-1323","CorpusId":1389483},"title":"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","externalIds":{"MAG":"2745461083","DBLP":"conf/cvpr/00010BT0GZ18","ArXiv":"1707.07998","DOI":"10.1109/CVPR.2018.00636","CorpusId":3753452},"title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"561ed7e47524fb3218e6a38f41cd877a9c33d3b9","externalIds":{"MAG":"2625940279","DBLP":"conf/cvpr/GanGHGD17","DOI":"10.1109/CVPR.2017.108","CorpusId":23414983},"title":"StyleNet: Generating Attractive Visual Captions with Styles"},{"paperId":"10480a42957a8e08e4c543185e135d7c254583a5","externalIds":{"MAG":"2952220025","DBLP":"conf/cvpr/YaoPLM17","ArXiv":"1708.05271","DOI":"10.1109/CVPR.2017.559","CorpusId":6153535},"title":"Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects"},{"paperId":"1cee733ee31e245dac4655a870fd9226163a52b5","externalIds":{"ArXiv":"1705.08759","MAG":"2619010992","DBLP":"conf/cvpr/SunLB17","DOI":"10.1109/CVPR.2017.763","CorpusId":7864745},"title":"Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-in-the-Blank Image Captioning"},{"paperId":"2d9830134f3ad0f66f75331ba73fe416bd76320f","externalIds":{"MAG":"2952247729","ArXiv":"1704.08224","DBLP":"journals/corr/ChandrasekaranP17","ACL":"N18-2121","DOI":"10.18653/v1/N18-2121","CorpusId":30046385},"title":"Punny Captions: Witty Wordplay in Image Descriptions"},{"paperId":"584225123ea64c2ea4bffe2ef595ffa4bba15ef7","externalIds":{"ArXiv":"1704.07434","DBLP":"conf/iccv/TavakoliSBL17","MAG":"2962781144","DOI":"10.1109/ICCV.2017.272","CorpusId":19158922},"title":"Paying Attention to Descriptions Generated by Image Captioning Models"},{"paperId":"e4e74ca18d9e6638ffe6b695b3610d4e85cb81df","externalIds":{"MAG":"2607768201","DBLP":"journals/corr/WangLSCC17","ArXiv":"1704.06972","DOI":"10.1109/CVPR.2017.780","CorpusId":764110},"title":"Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition"},{"paperId":"9d842c2fb5a0c54f15bc4e45c7c93bdb8b0c44de","externalIds":{"DBLP":"conf/cvpr/ParkKK17","ArXiv":"1704.06485","MAG":"2953002754","DOI":"10.1109/CVPR.2017.681","CorpusId":17618504},"title":"Attend to You: Personalized Image Captioning with Context Sequence Memory Networks"},{"paperId":"c689f73f8ea65c6e81c628f2b37feae09b29e46b","externalIds":{"DBLP":"journals/corr/RenWZLL17","MAG":"2607151106","ArXiv":"1704.03899","DOI":"10.1109/CVPR.2017.128","CorpusId":2899486},"title":"Deep Reinforcement Learning-Based Image Captioning with Embedding Reward"},{"paperId":"db2fecc8b1bd175d39687eb471360707a5fddb03","externalIds":{"ArXiv":"1704.01518","DBLP":"journals/corr/RohrbachRTOS17","MAG":"2605585413","DOI":"10.1109/CVPR.2017.447","CorpusId":1212193},"title":"Generating Descriptions with Grounded and Co-referenced People"},{"paperId":"1c0a6854b793ca8ad281513c184318b73d4868c4","externalIds":{"MAG":"2950672580","ArXiv":"1703.10476","DBLP":"conf/iccv/ShettyRHFS17","DOI":"10.1109/ICCV.2017.445","CorpusId":264100409},"title":"Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training"},{"paperId":"24dc571a49d3431e8cb1f1008f86d5dd5b7a1613","externalIds":{"MAG":"2962968835","ArXiv":"1703.06029","DBLP":"journals/corr/DaiLUF17","DOI":"10.1109/ICCV.2017.323","CorpusId":665667},"title":"Towards Diverse and Natural Image Descriptions via a Conditional GAN"},{"paperId":"910eb7a8c3f175a9b71680f70303751726bebd30","externalIds":{"MAG":"2604522653","DBLP":"conf/aaai/ChenDZCLH17","DOI":"10.1609/aaai.v31i1.11198","CorpusId":29163413},"title":"Reference Based LSTM for Image Captioning"},{"paperId":"7d1a7dae43b630d61d19d6cf139824380f2cf42f","externalIds":{"DBLP":"conf/aaai/LiTDZT17","MAG":"2604729005","DOI":"10.1609/aaai.v31i1.11236","CorpusId":29170844},"title":"Image Caption with Global-Local Attention"},{"paperId":"e782437503f2a24fd1a836a434da395bf15c88c2","externalIds":{"DBLP":"journals/corr/VedantamBMPC17","ArXiv":"1701.02870","MAG":"2574790321","DOI":"10.1109/CVPR.2017.120","CorpusId":2808607},"title":"Context-Aware Captions from Context-Agnostic Supervision"},{"paperId":"f75e7a025bf0eaba3de4efb53d6c2b0e4b3669fd","externalIds":{"DBLP":"conf/eacl/KilickayaEIE17","MAG":"2564590796","ACL":"E17-1019","ArXiv":"1612.07600","DOI":"10.18653/V1/E17-1019","CorpusId":10281325},"title":"Re-evaluating Automatic Metrics for Image Captioning"},{"paperId":"d85704f4814e9fa5ff0b68b1e5cad9e6527d0bbf","externalIds":{"MAG":"2951916837","ArXiv":"1612.07086","DBLP":"conf/iccv/GuWCC17","DOI":"10.1109/ICCV.2017.138","CorpusId":3928398},"title":"An Empirical Study of Language CNN for Image Captioning"},{"paperId":"afbff808f4a4c6eafcce3858451b9b1a508ecba3","externalIds":{"MAG":"2950317891","DBLP":"journals/corr/MunCH16","ArXiv":"1612.03557","DOI":"10.1609/aaai.v31i1.11237","CorpusId":8090024},"title":"Text-Guided Attention Model for Image Captioning"},{"paperId":"9f4d7d622d1f7319cc511bfef661cd973e881a4c","externalIds":{"DBLP":"journals/corr/LuXPS16","MAG":"2952469094","ArXiv":"1612.01887","DOI":"10.1109/CVPR.2017.345","CorpusId":18347865},"title":"Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning"},{"paperId":"6d86f0e22fed5e065ecf54b273d540b2430f014d","externalIds":{"MAG":"2949906242","DBLP":"conf/iccv/PedersoliLSV17","ArXiv":"1612.01033","DOI":"10.1109/ICCV.2017.140","CorpusId":15143565},"title":"Areas of Attention for Image Captioning"},{"paperId":"086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c","externalIds":{"ArXiv":"1612.00576","MAG":"2951654105","DBLP":"conf/emnlp/AndersonFJG17","ACL":"D17-1098","DOI":"10.18653/v1/D17-1098","CorpusId":9662636},"title":"Guided Open Vocabulary Image Captioning with Constrained Beam Search"},{"paperId":"6c8353697cdbb98dfba4f493875778c4286d3e3a","externalIds":{"DBLP":"conf/cvpr/RennieMMRG17","MAG":"2963084599","ArXiv":"1612.00563","DOI":"10.1109/CVPR.2017.131","CorpusId":206594923},"title":"Self-Critical Sequence Training for Image Captioning"},{"paperId":"163a474747fd63ab62ae586711fa5e5a2ac91bd8","externalIds":{"MAG":"2558533273","DBLP":"journals/corr/LiuZYG016","ArXiv":"1612.00370","DOI":"10.1109/ICCV.2017.100","CorpusId":3873857},"title":"Improved Image Captioning via Policy Gradient optimization of SPIDEr"},{"paperId":"778ce81457383bd5e3fdb11b145ded202ebb4970","externalIds":{"DBLP":"journals/corr/GanGHPTGCD16","MAG":"2951225148","ArXiv":"1611.08002","DOI":"10.1109/CVPR.2017.127","CorpusId":11045175},"title":"Semantic Compositional Networks for Visual Captioning"},{"paperId":"21fa67345e49642b8ebb22a59c4b2799a56e996f","externalIds":{"DBLP":"conf/cvpr/YangTYL17","MAG":"2950371778","ArXiv":"1611.06949","DOI":"10.1109/CVPR.2017.214","CorpusId":1189091},"title":"Dense Captioning with Joint Inference and Visual Context"},{"paperId":"88513e738a95840de05a62f0e43d30a67b3c542e","externalIds":{"MAG":"2550553598","ArXiv":"1611.05594","DBLP":"conf/cvpr/ChenZXNSLC17","DOI":"10.1109/CVPR.2017.667","CorpusId":206596371},"title":"SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning"},{"paperId":"5785466bc14529e94e54baa4ed051f7037f3b1d3","externalIds":{"MAG":"2552161745","DBLP":"conf/iccv/YaoPLQM17","ArXiv":"1611.01646","DOI":"10.1109/ICCV.2017.524","CorpusId":1868294},"title":"Boosting Image Captioning with Attributes"},{"paperId":"f4c5d13a8e9e80edcd4f69f0eab0b4434364c6dd","externalIds":{"DBLP":"journals/corr/PuGHYLSC16","MAG":"2527569769","ArXiv":"1609.08976","CorpusId":2665144},"title":"Variational Autoencoder for Deep Learning of Images, Labels and Captions"},{"paperId":"f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f","externalIds":{"DBLP":"journals/corr/AndersonFJG16","MAG":"2950201573","ArXiv":"1607.08822","DOI":"10.1007/978-3-319-46454-1_24","CorpusId":11933981},"title":"SPICE: Semantic Propositional Image Caption Evaluation"},{"paperId":"b9aa3bafa9e8e21bb92908ae23b468fa248239b3","externalIds":{"MAG":"2951503114","ArXiv":"1606.07770","DBLP":"conf/cvpr/VenugopalanHRMD17","DOI":"10.1109/CVPR.2017.130","CorpusId":8457705},"title":"Captioning Images with Diverse Objects"},{"paperId":"ae9850ce1ba187dc5f9e5ab0da381d8a551c1fc0","externalIds":{"MAG":"2410323755","DBLP":"conf/aaai/LiuMSY17","ArXiv":"1605.09553","DOI":"10.1609/aaai.v31i1.11197","CorpusId":14223891},"title":"Attention Correctness in Neural Image Captioning"},{"paperId":"61d2dda8d96a10a714636475c7589bd149bda053","externalIds":{"MAG":"2544271936","DBLP":"conf/nips/YangYWCS16","CorpusId":17369385},"title":"Review Networks for Caption Generation"},{"paperId":"bf55591e09b58ea9ce8d66110d6d3000ee804bdd","externalIds":{"MAG":"2302086703","ArXiv":"1603.03925","DBLP":"journals/corr/YouJWFL16","DOI":"10.1109/CVPR.2016.503","CorpusId":3120635},"title":"Image Captioning with Semantic Attention"},{"paperId":"1c252bc2434f55ac4e0f31f4a055748b444712b3","externalIds":{"DBLP":"journals/corr/BernardiCEEEIKM16","ArXiv":"1601.03896","MAG":"2951674897","DOI":"10.1613/jair.4900","CorpusId":1770055},"title":"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures"},{"paperId":"ced9f7178f8032d3408fcba493c02eb48e8a8636","externalIds":{"DBLP":"conf/iccv/YuPBB15","MAG":"2202226326","DOI":"10.1109/ICCV.2015.283","CorpusId":7142199},"title":"Visual Madlibs: Fill in the Blank Description Generation and Question Answering"},{"paperId":"3f6ed8a79b814893bbe4902634416096ac4e8ed6","externalIds":{"MAG":"2197223256","DBLP":"conf/iccv/UshikuYMH15","DOI":"10.1109/ICCV.2015.306","CorpusId":15943605},"title":"Common Subspace for Model and Similarity: Phrase Learning for Caption Generation from Images"},{"paperId":"c3640aae13e344ad70a926510221dada626a44de","externalIds":{"MAG":"2220981600","DBLP":"conf/iccv/JiaGFT15","DOI":"10.1109/ICCV.2015.277","CorpusId":7502269},"title":"Guiding the Long-Short Term Memory Model for Image Caption Generation"},{"paperId":"d7ce5665a72c0b607f484c1b448875f02ddfac3b","externalIds":{"DBLP":"journals/corr/JohnsonKL15","MAG":"2254252455","ArXiv":"1511.07571","DOI":"10.1109/CVPR.2016.494","CorpusId":14521054},"title":"DenseCap: Fully Convolutional Localization Networks for Dense Captioning"},{"paperId":"e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17","externalIds":{"MAG":"2952155606","DBLP":"conf/cvpr/HendricksVRMSD16","ArXiv":"1511.05284","DOI":"10.1109/CVPR.2016.8","CorpusId":6918332},"title":"Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data"},{"paperId":"4078c37c39dc5c7c65a5494651ba6dd443cf9269","externalIds":{"MAG":"2267310606","CorpusId":41558717},"title":"Empirical performance upper bounds for image and video captioning"},{"paperId":"dabd645ec406793c1effb908e75f588ebe02f484","externalIds":{"ArXiv":"1511.03292","DBLP":"journals/corr/AdityaYBFA15","MAG":"2226480163","CorpusId":123321},"title":"From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge"},{"paperId":"79b0da2080e9a8bab8ff1ec1364fa58d253a6676","externalIds":{"DOI":"10.1007/BF02824813","CorpusId":206566667},"title":"GENeration"},{"paperId":"7da9c26ea68a31d119e8222d1a5c33ef136ebed8","externalIds":{"MAG":"2963062932","DBLP":"journals/corr/MathewsXH15","ArXiv":"1510.01431","DOI":"10.1609/aaai.v30i1.10475","CorpusId":2875390},"title":"SentiCap: Generating Image Descriptions with Sentiments"},{"paperId":"66021a920001bc3e6258bffe7076d647614147b7","externalIds":{"MAG":"658020064","DBLP":"conf/icml/KusnerSKW15","CorpusId":14674248},"title":"From Word Embeddings To Document Distances"},{"paperId":"12aee52ad6c2b15d4006611651400baa9d01dee9","externalIds":{"ACL":"P15-2018","MAG":"2250842428","DBLP":"conf/acl/YagciogluEEC15","DOI":"10.3115/v1/P15-2018","CorpusId":18207738},"title":"A Distributed Representation Based Query Expansion Approach for Image Captioning"},{"paperId":"a72b8bbd039989db39769da836cdb287737deb92","externalIds":{"MAG":"1895989618","DBLP":"conf/cvpr/ChenZ15","DOI":"10.1109/CVPR.2015.7298856","CorpusId":6785090},"title":"Mind's eye: A recurrent visual representation for image caption generation"},{"paperId":"f142c849ffef66f7520aff4e0b40ac964ccb8cc1","externalIds":{"ArXiv":"1505.01809","ACL":"P15-2017","MAG":"2105103432","DBLP":"conf/acl/DevlinCFGDHZM15","DOI":"10.3115/v1/P15-2017","CorpusId":7988167},"title":"Language Models for Image Captioning: The Quirks and What Works"},{"paperId":"123b9de009865472c660192f8072493a48352dc2","externalIds":{"DBLP":"conf/icml/LebretPC15","MAG":"1828348983","ArXiv":"1502.03671","CorpusId":12064358},"title":"Phrase-based Image Captioning"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","externalIds":{"MAG":"2950178297","DBLP":"conf/icml/XuBKCCSZB15","ArXiv":"1502.03044","CorpusId":1055111},"title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745","externalIds":{"ArXiv":"1412.6632","MAG":"1811254738","DBLP":"journals/corr/MaoXYWY14a","CorpusId":3509328},"title":"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)"},{"paperId":"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","externalIds":{"ArXiv":"1412.2306","DBLP":"journals/corr/KarpathyF14","MAG":"1905882502","DOI":"10.1109/CVPR.2015.7298932","CorpusId":8517067},"title":"Deep visual-semantic alignments for generating image descriptions"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b","externalIds":{"MAG":"1931639407","ArXiv":"1411.4952","DBLP":"journals/corr/FangGISDDGHMPZZ14","DOI":"10.1109/CVPR.2015.7298754","CorpusId":9254582},"title":"From captions to visual concepts and back"},{"paperId":"f01fc808592ea7c473a69a6e7484040a435f36d9","externalIds":{"DBLP":"journals/pami/DonahueHRVGSD17","MAG":"2951183276","ArXiv":"1411.4389","DOI":"10.1109/CVPR.2015.7298878","CorpusId":5736847,"PubMed":"27608449"},"title":"Long-term recurrent convolutional networks for visual recognition and description"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","externalIds":{"MAG":"1895577753","DBLP":"journals/corr/VinyalsTBE14","ArXiv":"1411.4555","DOI":"10.1109/CVPR.2015.7298935","CorpusId":1169492},"title":"Show and tell: A neural image caption generator"},{"paperId":"59927ded86ab4f7253fc32efb351e5a13e746ead","externalIds":{"ACL":"Q14-1028","MAG":"2296385829","DBLP":"journals/tacl/KuznetsovaOBC14","DOI":"10.1162/tacl_a_00188","CorpusId":13344783},"title":"TreeTalk: Composition and Compression of Trees for Image Descriptions"},{"paperId":"88dfc5148bdd92669bbbd5c64d1ac324207bf831","externalIds":{"MAG":"2251583212","DBLP":"conf/starsem/YatskarGVZ14","ACL":"S14-1015","DOI":"10.3115/v1/S14-1015","CorpusId":11958702},"title":"See No Evil, Say No Evil: Description Generation from Densely Labeled Images"},{"paperId":"a8e0239269eb035513b0e8c061577355c7d30560","externalIds":{"ACL":"W14-1602","MAG":"2251955524","DBLP":"conf/conll/MasonC14","DOI":"10.3115/v1/W14-1602","CorpusId":16430728},"title":"Domain-Specific Image Captioning"},{"paperId":"88e652a681ffa132d0438b0d0c322766dc8eac91","externalIds":{"ACL":"P14-2097","DBLP":"conf/acl/MasonC14","MAG":"2131179926","DOI":"10.3115/v1/P14-2097","CorpusId":12665230},"title":"Nonparametric Method for Data-driven Image Captioning"},{"paperId":"52f86811b57034ba5c0478b37cab101d9a84024a","externalIds":{"ACL":"P14-2074","MAG":"2128856065","DBLP":"conf/acl/ElliottK14","DOI":"10.3115/v1/P14-2074","CorpusId":10297558},"title":"Comparing Automatic Evaluation Measures for Image Description"},{"paperId":"3f6a4556769e819242d669d073b895f1e45a706f","externalIds":{"DBLP":"conf/emnlp/ElliottK13","MAG":"2143449221","ACL":"D13-1128","DOI":"10.18653/v1/d13-1128","CorpusId":10282227},"title":"Image Description using Visual Dependency Representations"},{"paperId":"9814df8bd00ba999c4d1e305a7e9bca579dc7c75","externalIds":{"MAG":"68733909","DBLP":"journals/jair/HodoshYH13","DOI":"10.1613/jair.3994","CorpusId":928608},"title":"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics"},{"paperId":"6b56454a84dcc4de62aea4334c414dc979c42a31","externalIds":{"MAG":"1963620879","DBLP":"journals/ejis/NickersonVM13","DOI":"10.1057/ejis.2012.26","CorpusId":34927311},"title":"A method for taxonomy development and its application in information systems"},{"paperId":"2a0d0f6c5a69b264710df0230696f47c5918e2f2","externalIds":{"DBLP":"conf/acl/KuznetsovaOBBC12","ACL":"P12-1038","MAG":"2149172860","CorpusId":10315654},"title":"Collective Generation of Natural Image Descriptions"},{"paperId":"355de7460120ddc1150d9ce3756f9848983f7ff4","externalIds":{"MAG":"8316075","DBLP":"conf/eacl/MitchellDGYSHMBBD12","ACL":"E12-1076","CorpusId":2972357},"title":"Midge: Generating Image Descriptions From Computer Vision Detections"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","externalIds":{"DBLP":"conf/nips/OrdonezKB11","MAG":"2109586012","CorpusId":14579301},"title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"fbdbe747c6aa8b35b981d21e475ff1506a1bae66","externalIds":{"ACL":"W11-0326","MAG":"1687846465","DBLP":"conf/conll/LiKBBC11","CorpusId":10702193},"title":"Composing Simple Image Descriptions using Web-scale N-grams"},{"paperId":"c8b7e13a5d0c13dfde17c16f9cad2d50b442dba1","externalIds":{"MAG":"2126384256","DBLP":"conf/acl/FengL10","ACL":"P10-1126","CorpusId":18650536},"title":"How Many Words Is a Picture Worth? Automatic Caption Generation for News Images"},{"paperId":"e8dbc756ea246f599250c09e3efd9bba9909a842","externalIds":{"DBLP":"conf/acl/AkerG10","MAG":"2114841702","ACL":"P10-1127","CorpusId":5223711},"title":"Generating Image Descriptions Using Dependency Relational Patterns"},{"paperId":"bf60322f83714523e2d7c1d39983151fe9db7146","externalIds":{"DBLP":"conf/naacl/RashtchianYHH10","ACL":"W10-0721","MAG":"2119775030","CorpusId":5583509},"title":"Collecting Image Annotations Using Amazon’s Mechanical Turk"},{"paperId":"c365683ce624f3fc79450ec9232893f85ca31080","externalIds":{"DBLP":"conf/wmt/GimenezM07a","ACL":"W07-0738","MAG":"2028176545","DOI":"10.3115/1626355.1626393","CorpusId":9988433},"title":"Linguistic Features for Automatic Evaluation of Heterogenous MT Systems"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"3af95af0d1534b24df7ea4d94cbedba0b9663e37","externalIds":{"MAG":"2078861931","DOI":"10.3115/1289189.1289273","CorpusId":14067706},"title":"Automatic evaluation of machine translation quality using n-gram co-occurrence statistics"},{"paperId":"195bbeaeff80dfd60ea29715695372d3052e7c9e","externalIds":{"MAG":"1995689975","DOI":"10.1117/12.373443","CorpusId":7113662},"title":"Conceptual framework for indexing visual information at multiple levels"},{"paperId":"c769286434affcba7153281c54f8ae952a3dea63","externalIds":{"MAG":"1964209572","DOI":"10.1145/323409.328679","CorpusId":44292826},"title":"Language"},{"paperId":"6777b44c162b289fd5d0423316cb89ae05c5acb8","externalIds":{"MAG":"2142468277","DOI":"10.1300/J104V06N03_04","CorpusId":61570695},"title":"Analyzing the Subject of a Picture: A Theoretical Approach"},{"paperId":"32391912140f2454a30c51ff6384e8b8f2b09992","externalIds":{"MAG":"2963476168","DOI":"10.1177/003591577406700924","CorpusId":706492,"PubMed":"4431798"},"title":"Short Papers"},{"paperId":"c02fcb1ad7c7fdafcafc2f35934a855f3fbe8e5d","externalIds":{"DBLP":"conf/naacl/TanakaUGMH24","DOI":"10.18653/v1/2024.findings-naacl.152","CorpusId":271520253},"title":"Content-Specific Humorous Image Captioning Using Incongruity Resolution Chain-of-Thought"},{"paperId":"0807581ea24d6d646e5f90de0afb89812a6dadb7","externalIds":{"ACL":"2024.trustnlp-1.19","DOI":"10.18653/v1/2024.trustnlp-1.19","CorpusId":270765561},"title":"Masking Latent Gender Knowledge for Debiasing Image Captioning"},{"paperId":"80997695ca57c552cbe0020f5517f4bf6631a56c","externalIds":{"DBLP":"conf/naacl/RamosBME24","DOI":"10.18653/v1/2024.findings-naacl.225","CorpusId":271520543},"title":"PAELLA: Parameter-Efficient Lightweight Language-Agnostic Captioning Model"},{"paperId":"4e3b9a80a5322333d80557af9af71b7c2f1f56d1","externalIds":{"DBLP":"conf/acl/Zhang023c","DOI":"10.18653/v1/2023.findings-acl.818","CorpusId":259858771},"title":"Exploring the Impact of Vision Features in News Image Captioning"},{"paperId":"c979783eb02eb60e55c181e37e79b4c4810d4b89","externalIds":{"DBLP":"conf/acl/Li0LWSH23","ACL":"2023.acl-demo.3","DOI":"10.18653/v1/2023.acl-demo.3","CorpusId":259370591},"title":"LAVIS: A One-stop Library for Language-Vision Intelligence"},{"paperId":"22080a3a84ffaee1c548dceac5d1e037764cbad1","externalIds":{"ACL":"2023.repl4nlp-1.22","DBLP":"conf/rep4nlp/BielawskiV23","DOI":"10.18653/v1/2023.repl4nlp-1.22","CorpusId":259833849},"title":"CLIP-based image captioning via unsupervised cycle-consistency in the latent space"},{"paperId":"d922c317fbad1912b0b512d3f3d42e02d2d56287","externalIds":{"DBLP":"conf/icml/Zheng023","CorpusId":260957229},"title":"Evidential Interactive Learning for Medical Image Captioning"},{"paperId":"54410d3f4d069b290c2acaedb92fb1ac4b511713","externalIds":{"DOI":"10.18653/v1/2023.findings-emnlp.463","CorpusId":266166977},"title":"Query-based Image Captioning from Multi-context 360cdegree Images Images"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","externalIds":{"DBLP":"conf/acl/PantazopoulosSE22","ACL":"2022.acl-srw.11","DOI":"10.18653/v1/2022.acl-srw.11","CorpusId":248780552},"title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"9ed2efebdf373dadf45fdfbdde13dcd2c9910f5e","externalIds":{"ACL":"2022.emnlp-main.480","DBLP":"conf/emnlp/NishinoMTOSKT22","DOI":"10.18653/v1/2022.emnlp-main.480","CorpusId":256460887},"title":"Factual Accuracy is not Enough: Planning Consistent Description Order for Radiology Report Generation"},{"paperId":"3f65c7277488ffb888f20b7b3246dfabc3b73de1","externalIds":{"DBLP":"conf/acl/YanJL0D020","ACL":"2021.acl-long.157","DOI":"10.18653/v1/2021.acl-long.157","CorpusId":236460139},"title":"Control Image Captioning Spatially and Temporally"},{"paperId":"bce036f82d266c9781d2cbf64efc190679d4769e","externalIds":{"ACL":"2021.findings-acl.6","DBLP":"conf/acl/TuYLLGYY21","DOI":"10.18653/v1/2021.findings-acl.6","CorpusId":236477908},"title":"Semantic Relation-aware Difference Representation Learning for Change Captioning"},{"paperId":"6f40741579bfed3d3ae5c637a32ea52a2c3e5690","externalIds":{"ACL":"2021.lantern-1.3","CorpusId":233365234},"title":"Exploiting Image–Text Synergy for Contextual Image Captioning"},{"paperId":"c5738ecde7e9c5fdc9e36af84bfb4db5c3ff73a7","externalIds":{"DBLP":"conf/emnlp/0001WZJ21","DOI":"10.18653/v1/2021.findings-emnlp.162","CorpusId":244119575},"title":"Language Resource Efficient Learning for Captioning"},{"paperId":"6d466de7180776023a539371fad3d521eb5ff791","externalIds":{"DBLP":"conf/emnlp/ShiLMMLZ21","DOI":"10.18653/v1/2021.findings-emnlp.171","CorpusId":244119615},"title":"Retrieval, Analogy, and Composition: A framework for Compositional Generalization in Image Captioning"},{"paperId":"4d2a4f3ff1e27afba7190f777038aacb072fc43e","externalIds":{"DOI":"10.1088/1757-899X/1116/1/012184","CorpusId":235292921},"title":"A Survey on Image Captioning datasets and Evaluation Metrics"},{"paperId":"e761a0d40cc7d6efddd0123d11045eba72c3d760","externalIds":{"DBLP":"conf/acl/ShiLZ20","ACL":"2021.acl-short.36","DOI":"10.18653/v1/2021.acl-short.36","CorpusId":236460007},"title":"Enhancing Descriptive Image Captioning with Natural Language Inference"},{"paperId":"8b55402ffee2734bfc7d5d7595500916e1ef04e8","externalIds":{"MAG":"2904565150","DBLP":"conf/iccv/AgrawalAD0CJ0BP19","ArXiv":"1812.08658","DOI":"10.1109/ICCV.2019.00904","CorpusId":56517630},"title":"nocaps: novel object captioning at scale"},{"paperId":"2eca0afa23ee2b9b0999c445b8dba44ad3039bd1","externalIds":{"DBLP":"conf/nips/ChenJJSZGWHW19","MAG":"2970626077","CorpusId":202770245},"title":"Variational Structured Semantic Inference for Diverse Image Captioning"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"8bee8d14fd7356f9257e84eeb35f6610830bcf68","externalIds":{"DOI":"10.4135/9781483380810.n104","CorpusId":239370389},"title":"Captioning"},{"paperId":"b8298cf0056af5afa3185181ddd5f6bb03181696","externalIds":{"MAG":"2890781596","DBLP":"conf/emnlp/Melas-KyriaziRH18","ACL":"D18-1084","DOI":"10.18653/v1/D18-1084","CorpusId":53083043},"title":"Training for Diversity in Image Paragraph Captioning"},{"paperId":"d0fd9b23e4e9b65c44a0ab722c4a69a6ed38cc3d","externalIds":{"DBLP":"conf/naacl/ChenKWC15","MAG":"2293523832","ACL":"N15-1053","DOI":"10.3115/v1/N15-1053","CorpusId":2007128},"title":"Déjà Image-Captions: A Corpus of Expressive Descriptions in Repetition"},{"paperId":"9002c8d2c2210387dd0c694c151c4ab1dde42b4b","externalIds":{"DBLP":"conf/acl/KuznetsovaOBBC13","MAG":"2144715671","ACL":"P13-2138","CorpusId":1875219},"title":"Generalizing Image Captions for Image-Text Parallel Corpus"},{"paperId":"d10fae1e0246c642695cfe0514def71590aba99a","externalIds":{"DOI":"10.1016/j.ajic.2010.10.003","CorpusId":9442564,"PubMed":"21093690"},"title":"The present."},{"paperId":"3d07b5087e53c6f7c228b3c7e769494527be228e","externalIds":{"DBLP":"conf/amta/SnoverDSMM06","ACL":"2006.amta-papers.25","CorpusId":263887736},"title":"A Study of Translation Edit Rate with Targeted Human Annotation"},{"paperId":"13167f9cd8c7906ca808b01d28dca6dd951da8a5","externalIds":{"CorpusId":15641339},"title":"of the Association for Computational Linguistics"}]}