{"references":[{"paperId":"20acbd0eac8e8bfdb05ea7deff23e30e2ab8f15e","externalIds":{"DBLP":"journals/corr/abs-2407-17211","ArXiv":"2407.17211","DOI":"10.48550/arXiv.2407.17211","CorpusId":271404290},"title":"Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles"},{"paperId":"126061d85cf25e37b74f2a4eb84578c3fae609c5","externalIds":{"DBLP":"journals/tiv/JiangCCLRYYFWC25","ArXiv":"2407.14239","DOI":"10.1109/TIV.2024.3488793","CorpusId":271310037},"title":"KoMA: Knowledge-Driven Multi-Agent Framework for Autonomous Driving With Large Language Models"},{"paperId":"0021b60bde8d4f0999794344dfc2365f3357e095","externalIds":{"ArXiv":"2406.14556","DBLP":"journals/corr/abs-2406-14556","DOI":"10.48550/arXiv.2406.14556","CorpusId":270620342},"title":"Asynchronous Large Language Model Enhanced Planner for Autonomous Driving"},{"paperId":"efbba49eb8faa538d7ddfce93680379e20861b2f","externalIds":{"DBLP":"journals/corr/abs-2406-10667","ArXiv":"2406.10667","DOI":"10.48550/arXiv.2406.10667","CorpusId":270559382},"title":"UniZero: Generalized and Efficient Planning with Scalable Latent World Models"},{"paperId":"bc2ac51104e3a477938aa664e20527a7178fd418","externalIds":{"DBLP":"conf/nips/JiaYLZY24","ArXiv":"2406.03877","DOI":"10.48550/arXiv.2406.03877","CorpusId":270286090},"title":"Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving"},{"paperId":"dee60b5cf8a58697cace48a6ec52367954d0bbca","externalIds":{"ArXiv":"2406.01587","DBLP":"journals/corr/abs-2406-01587","DOI":"10.48550/arXiv.2406.01587","CorpusId":270214282},"title":"PlanAgent: A Multi-modal Large Language Agent for Closed-loop Vehicle Motion Planning"},{"paperId":"893fc269a46c2552cec5e076db99eb724362e9c0","externalIds":{"DBLP":"conf/ivs/KongBFW24","ArXiv":"2406.05651","DOI":"10.1109/IV55156.2024.10588403","CorpusId":270371283},"title":"A Superalignment Framework in Autonomous Driving with Large Language Models"},{"paperId":"a3326a57275d3f36794ec71bf2722a06904bb785","externalIds":{"DBLP":"conf/cvpr/ZhangXL24","ArXiv":"2405.14062","DOI":"10.1109/CVPR52733.2024.01464","CorpusId":269982316},"title":"ChatScene: Knowledge-Enabled Safety-Critical Scenario Generation for Autonomous Vehicles"},{"paperId":"dcd357113a9ff1d1e8c66d1385173cef85f01617","externalIds":{"DBLP":"journals/corr/abs-2405-05956","ArXiv":"2405.05956","DOI":"10.1109/LRA.2025.3608656","CorpusId":269635668},"title":"Probing Multimodal LLMs as World Models for Driving"},{"paperId":"7ef10b9b5bbce61573f647774004f95221efe665","externalIds":{"DBLP":"journals/corr/abs-2404-06345","ArXiv":"2404.06345","DOI":"10.48550/arXiv.2404.06345","CorpusId":269009845},"title":"AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning"},{"paperId":"cbd2d7977f92dad8db6c4d87c5ec5e5a49619f8c","externalIds":{"ArXiv":"2403.20116","DBLP":"conf/iros/PaulGCSK24","DOI":"10.1109/IROS58592.2024.10801870","CorpusId":268793772},"title":"LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving"},{"paperId":"0eac68cf7c11f059fd872e75117121658c86f149","externalIds":{"DBLP":"journals/corr/abs-2403-19838","ArXiv":"2403.19838","DOI":"10.48550/arXiv.2403.19838","CorpusId":268793702},"title":"Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving"},{"paperId":"77a9c310df0d7896d297da90fc4a1131819c341e","externalIds":{"ArXiv":"2403.18344","DBLP":"journals/corr/abs-2403-18344","DOI":"10.1016/j.commtr.2025.100170","CorpusId":268723960},"title":"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models"},{"paperId":"7a4b015ca47cdf4582b094961119cb5652712779","externalIds":{"DBLP":"conf/cvpr/LiangSSGZ0C24","ArXiv":"2403.17373","DOI":"10.1109/CVPR52733.2024.01392","CorpusId":268691449},"title":"AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving"},{"paperId":"a8cff74a8d6ac98ecd3fc757b6afb33154a0c949","externalIds":{"DBLP":"journals/corr/abs-2403-16289","ArXiv":"2403.16289","DOI":"10.1109/RE59067.2024.00029","CorpusId":268681074},"title":"Engineering Safety Requirements for Autonomous Driving with Large Language Models"},{"paperId":"f3c834227c4c1a4461483abfdd3b693bfe506128","externalIds":{"DBLP":"journals/corr/abs-2403-13331","ArXiv":"2403.13331","DOI":"10.48550/arXiv.2403.13331","CorpusId":268537036},"title":"AMP: Autoregressive Motion Prediction Revisited with Next Token Prediction for Autonomous Driving"},{"paperId":"a70f267ce46eb8b46e9482e9bacb00e64b03c6ea","externalIds":{"DBLP":"journals/corr/abs-2403-09630","ArXiv":"2403.09630","DOI":"10.1109/CVPR52733.2024.01389","CorpusId":268385457},"title":"Generalized Predictive Model for Autonomous Driving"},{"paperId":"7f96bb27a8fca35b1f7d02ee319a64be04114809","externalIds":{"ArXiv":"2403.08337","DBLP":"journals/corr/abs-2403-08337","DOI":"10.48550/arXiv.2403.08337","CorpusId":268379562},"title":"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments"},{"paperId":"93ec628fec45d1c7d856b7e751f9232f35b1e1a3","externalIds":{"ArXiv":"2403.06845","DBLP":"journals/corr/abs-2403-06845","DOI":"10.48550/arXiv.2403.06845","CorpusId":268363690},"title":"DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation"},{"paperId":"d64e270e92d20cb071c2f188f523cd63b28ecd5a","externalIds":{"ArXiv":"2403.04593","DBLP":"journals/corr/abs-2403-04593","DOI":"10.48550/arXiv.2403.04593","CorpusId":268264428},"title":"Embodied Understanding of Driving Scenarios"},{"paperId":"4f1a59bec54c2bf66cd40b9b9cae486260c4862d","externalIds":{"DBLP":"journals/corr/abs-2402-16720","ArXiv":"2402.16720","DOI":"10.48550/arXiv.2402.16720","CorpusId":268032582},"title":"Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)"},{"paperId":"5dc1611ce19a1f6a8562da7455a5370fde148d1b","externalIds":{"DBLP":"journals/corr/abs-2402-13602","ArXiv":"2402.13602","DOI":"10.1109/ICCMA63715.2024.10843921","CorpusId":267770624},"title":"Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving"},{"paperId":"758c2dc290c037a6f211ec503beee70abe2d1197","externalIds":{"DBLP":"conf/corl/TianGLLWZZJLZ24","ArXiv":"2402.12289","DOI":"10.48550/arXiv.2402.12289","CorpusId":267750682},"title":"DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models"},{"paperId":"7f254928af6718c80224199e9cd915e7372ac54d","externalIds":{"ArXiv":"2402.10828","DBLP":"conf/rss/YuanSOZ0KG24","DOI":"10.48550/arXiv.2402.10828","CorpusId":267740546},"title":"RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model"},{"paperId":"8f0c0066680a19580e0373eecfd6f50e62d4efb9","externalIds":{"DBLP":"conf/cvpr/Li0MIVL024","ArXiv":"2402.05932","DOI":"10.1109/CVPR52733.2024.01416","CorpusId":267547859},"title":"Driving Everywhere with Large Language Model Policy Adaptation"},{"paperId":"cabdac367c6c1b00a0cc6217c3814b18fac74ade","externalIds":{"ArXiv":"2402.05746","DBLP":"conf/cvpr/WeiWLXLZCW24","DOI":"10.1109/CVPR52733.2024.01428","CorpusId":267548034},"title":"Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents"},{"paperId":"e84fe33362ef545667b51929482c5180576f5937","externalIds":{"DBLP":"journals/corr/abs-2402-01246","ArXiv":"2402.01246","DOI":"10.1109/IV55156.2024.10588848","CorpusId":267406399},"title":"LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving"},{"paperId":"e8ee8dc3a806495c6084389323ca09aca3238836","externalIds":{"ArXiv":"2401.10314","CorpusId":267061180},"title":"LangProp: A code optimization framework using Large Language Models applied to driving"},{"paperId":"4a88eb58cb3d1f72da7ac501a28f2e9d831336c4","externalIds":{"ArXiv":"2401.05577","DBLP":"conf/cvpr/PanYNMAVR24","DOI":"10.1109/CVPR52733.2024.01398","CorpusId":266933301},"title":"VLP: Vision Language Planning for Autonomous Driving"},{"paperId":"2e6b8c6c48e10ae9a4f438149fc0749ffced19a3","externalIds":{"ArXiv":"2401.03641","DBLP":"journals/corr/abs-2401-03641","DOI":"10.48550/arXiv.2401.03641","CorpusId":266844639},"title":"DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in Autonomous Driving"},{"paperId":"cd49101103f73d88a4a3b368898066f03984c339","externalIds":{"ArXiv":"2401.00988","DBLP":"conf/cvpr/DingHXLZ024","DOI":"10.1109/CVPR52733.2024.01297","CorpusId":266725320},"title":"Holistic Autonomous Driving Understanding by Bird'View Injected Multi-Modal Large Models"},{"paperId":"237b0cf9d78f4a52274b868656ad011f599aeb26","externalIds":{"DBLP":"journals/corr/abs-2401-00125","ArXiv":"2401.00125","DOI":"10.48550/arXiv.2401.00125","CorpusId":266693892},"title":"LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning"},{"paperId":"3c8cc9a5ee373d51e0bf71621b6eb6901c762e8f","externalIds":{"ArXiv":"2312.14150","DBLP":"conf/eccv/SimaRCCZXBLGL24","DOI":"10.48550/arXiv.2312.14150","CorpusId":266435584},"title":"DriveLM: Driving with Graph Visual Question Answering"},{"paperId":"0ede8cf8ffad002de16f67133e5833351341c96e","externalIds":{"DBLP":"journals/corr/abs-2312-13156","ArXiv":"2312.13156","DOI":"10.48550/arXiv.2312.13156","CorpusId":266375057},"title":"AccidentGPT: Accident Analysis and Prevention from V2X Environmental Perception with Multi-modal Large Model"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","externalIds":{"ArXiv":"2312.09245","DBLP":"journals/corr/abs-2312-09245","DOI":"10.1007/s44267-025-00095-w","CorpusId":266210476},"title":"DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving"},{"paperId":"cc7da8d0c5edd9785e80737fa9cb49a1da5fbe1e","externalIds":{"DBLP":"journals/corr/abs-2312-06351","ArXiv":"2312.06351","DOI":"10.48550/arXiv.2312.06351","CorpusId":266162537},"title":"Evaluation of Large Language Models for Decision Making in Autonomous Driving"},{"paperId":"10578bc0bdb3ebf9232931dd4961f55ba470caad","externalIds":{"DBLP":"journals/corr/abs-2312-04372","ArXiv":"2312.04372","DOI":"10.1109/CVPR52733.2024.01434","CorpusId":266055179},"title":"LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs"},{"paperId":"98de0a73fc32e04b58d76579aef964cf686b25da","externalIds":{"ArXiv":"2312.03661","DBLP":"conf/eccv/NiePWCHXZ24","DOI":"10.48550/arXiv.2312.03661","CorpusId":265688025},"title":"Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving"},{"paperId":"c95c4fb96868d6512c32988632a7b101a42c455d","externalIds":{"DBLP":"conf/eccv/MaCSPX24","ArXiv":"2312.00438","DOI":"10.48550/arXiv.2312.00438","CorpusId":265551475},"title":"Dolphins: Multimodal Language Model for Driving"},{"paperId":"3de1d214dc79ecd0b34faf6fb0520dd7d2988c97","externalIds":{"DBLP":"journals/tiv/WangZLWLH23","DOI":"10.1109/TIV.2023.3325300","CorpusId":264314440},"title":"ChatGPT as Your Vehicle Co-Pilot: An Initial Attempt"},{"paperId":"db2aae1bd6e71e7c23716d0eb50d12aa72a0f0f2","externalIds":{"DBLP":"journals/corr/abs-2311-17918","ArXiv":"2311.17918","DOI":"10.1109/CVPR52733.2024.01397","CorpusId":265498831},"title":"Driving Into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving"},{"paperId":"c579ab910bd0ef8d6e06fc1b3557c16068af4fe5","externalIds":{"ArXiv":"2312.00812","DBLP":"journals/corr/abs-2312-00812","DOI":"10.48550/arXiv.2312.00812","CorpusId":265609613},"title":"Empowering Autonomous Driving with Large Language Models: A Safety Perspective"},{"paperId":"357e182a38219625dd37cba526befe5f8429aa4b","externalIds":{"ArXiv":"2311.10813","DBLP":"journals/corr/abs-2311-10813","DOI":"10.48550/arXiv.2311.10813","CorpusId":265294541},"title":"A Language Agent for Autonomous Driving"},{"paperId":"0333cf45841955d19de5096d5fd0ee55fdcd15ad","externalIds":{"ArXiv":"2311.08206","DBLP":"journals/corr/abs-2311-08206","DOI":"10.1109/WACVW60836.2024.00108","CorpusId":265157758},"title":"Human-Centric Autonomous Systems With LLMs for User Command Reasoning"},{"paperId":"c85845796afd368853cb9ac51c308f2f9356bab1","externalIds":{"DBLP":"journals/corr/abs-2311-05332","ArXiv":"2311.05332","DOI":"10.48550/arXiv.2311.05332","CorpusId":265067382},"title":"On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving"},{"paperId":"10158879cdb64ce7d3f7bb5572c4617ea808602e","externalIds":{"ArXiv":"2310.08034","DBLP":"journals/corr/abs-2310-08034","DOI":"10.1109/MITS.2024.3381793","CorpusId":263908840},"title":"Receive, Reason, and React: Drive as You Say, With Large Language Models in Autonomous Vehicles"},{"paperId":"06134e5af64e96a5a0339431933d13c62f898b6f","externalIds":{"ArXiv":"2310.08348","DBLP":"journals/corr/abs-2310-08348","DOI":"10.48550/arXiv.2310.08348","CorpusId":263909384},"title":"LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios"},{"paperId":"c465e2c612c9bb15f3efcb554e35052b6374d012","externalIds":{"DBLP":"journals/corr/abs-2310-07771","ArXiv":"2310.07771","DOI":"10.48550/arXiv.2310.07771","CorpusId":263909293},"title":"DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model"},{"paperId":"712ac6589e5eefe480e80f16050810a28a4e3f50","externalIds":{"DBLP":"conf/iclr/0001CXHLY024","ArXiv":"2310.02601","DOI":"10.48550/arXiv.2310.02601","CorpusId":263620520},"title":"MagicDrive: Street View Generation with Diverse 3D Geometry Control"},{"paperId":"19933dd9e03058e686ef412262eef7696cce3e8f","externalIds":{"ArXiv":"2310.03026","DBLP":"journals/corr/abs-2310-03026","DOI":"10.48550/arXiv.2310.03026","CorpusId":263620279},"title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving"},{"paperId":"f01ff5acf9e086030c01beda6f433f99013ebbd4","externalIds":{"DBLP":"conf/icra/ChenSHKWBMS24","ArXiv":"2310.01957","DOI":"10.1109/ICRA57147.2024.10611018","CorpusId":263608168},"title":"Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"},{"paperId":"ccd6f8b6544f112de632e49bfbe592a0a654537d","externalIds":{"ArXiv":"2310.01412","DBLP":"journals/ral/XuZXZGWLZ24","DOI":"10.1109/LRA.2024.3440097","CorpusId":263605524},"title":"DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model"},{"paperId":"958ed4830ae80a189ecb9b93ab75a6ce2e3926fc","externalIds":{"DBLP":"journals/corr/abs-2310-01415","ArXiv":"2310.01415","DOI":"10.48550/arXiv.2310.01415","CorpusId":263605637},"title":"GPT-Driver: Learning to Drive with GPT"},{"paperId":"54814744b42b06c855c97b23de1366e0bcbe775a","externalIds":{"ArXiv":"2309.17421","DBLP":"journals/corr/abs-2309-17421","DOI":"10.48550/arXiv.2309.17421","CorpusId":263310951},"title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)"},{"paperId":"98478ac589e5b40a20630ff54bb4eec4ab4c5f6b","externalIds":{"ArXiv":"2309.17080","DBLP":"journals/corr/abs-2309-17080","DOI":"10.48550/arXiv.2309.17080","CorpusId":263310665},"title":"GAIA-1: A Generative World Model for Autonomous Driving"},{"paperId":"3cbfe152220de84ecf8059fa50c47587a3134c86","externalIds":{"DBLP":"conf/iclr/WenF0C0CDS0024","ArXiv":"2309.16292","DOI":"10.48550/arXiv.2309.16292","CorpusId":263136146},"title":"DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models"},{"paperId":"482665786ce1956fb9ea4b694d2d8e8cf92276fa","externalIds":{"ArXiv":"2309.10228","DBLP":"conf/wacv/CuiMCYW24","DOI":"10.1109/WACVW60836.2024.00101","CorpusId":262054629},"title":"Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles"},{"paperId":"0ef8bbccbfab7e6d9c8bd09bcadfe9c5bbbff512","externalIds":{"ArXiv":"2309.09777","DBLP":"journals/corr/abs-2309-09777","DOI":"10.48550/arXiv.2309.09777","CorpusId":262044078},"title":"DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving"},{"paperId":"8aab972b0c3a3d581536b0d74339794809dc1a64","externalIds":{"ArXiv":"2309.06719","DBLP":"journals/corr/abs-2309-06719","DOI":"10.48550/arXiv.2309.06719","CorpusId":261705716},"title":"TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models"},{"paperId":"e14b245c2ed6d46bf2f8661fcd61b74f34d60ad7","externalIds":{"DBLP":"conf/wacv/SachdevaACR0KCD24","ArXiv":"2309.06597","DOI":"10.1109/WACV57701.2024.00734","CorpusId":261706176},"title":"Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning"},{"paperId":"7679dc8534cb1dd65c63c50b38f56386228d32d1","externalIds":{"ArXiv":"2309.05282","DBLP":"journals/corr/abs-2309-05282","DOI":"10.48550/arXiv.2309.05282","CorpusId":261681760},"title":"Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving"},{"paperId":"bd8a7cc93e1515b2b644edc8f17baac7dd4df34f","externalIds":{"DBLP":"journals/corr/abs-2308-01661","ArXiv":"2308.01661","DOI":"10.48550/arXiv.2308.01661","CorpusId":260438574},"title":"BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout"},{"paperId":"d9fc4a4179ba964a6c5f69e7be7deffc14a434e4","externalIds":{"ArXiv":"2308.00398","DBLP":"conf/iccv/JiaGCYLL23","DOI":"10.1109/ICCV51070.2023.00731","CorpusId":260351212},"title":"DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving"},{"paperId":"ed1c04bd73d3b3c25d13d67c3d96f6c54ab3f35d","externalIds":{"DBLP":"conf/itsc/LiuHQWS23","ArXiv":"2307.16118","DOI":"10.1109/ITSC57777.2023.10421993","CorpusId":260334534},"title":"MTD-GPT: A Multi-Task Decision-Making GPT Model for Autonomous Driving at Unsignalized Intersections"},{"paperId":"38939304bb760473141c2aca0305e44fbe04e6e8","externalIds":{"ArXiv":"2307.15818","DBLP":"conf/corl/ZitkovichYXXXXW23","DOI":"10.48550/arXiv.2307.15818","CorpusId":260293142},"title":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"},{"paperId":"eff0c08f39460098a1b42d4ad6b58e571876b3dc","externalIds":{"ArXiv":"2307.14591","DBLP":"journals/corr/abs-2307-14591","DOI":"10.48550/arXiv.2307.14591","CorpusId":260202836},"title":"The detection and rectification for identity-switch based on unfalsified control"},{"paperId":"eae18cf748729ab740dbb8c11a28ea377dc41db9","externalIds":{"ArXiv":"2307.11769","DBLP":"journals/corr/abs-2307-11769","DOI":"10.1109/ITSC57777.2023.10422308","CorpusId":260125133},"title":"Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain"},{"paperId":"11bca2cafe89e14dc733504f97e2489de697ceab","externalIds":{"ArXiv":"2307.07162","DBLP":"conf/wacv/FuLWDCSQ24","DOI":"10.1109/WACVW60836.2024.00102","CorpusId":259924488},"title":"Drive Like a Human: Rethinking Autonomous Driving with Large Language Models"},{"paperId":"23be5ad6c40d3b59835edef7fe0b4a25315a77b3","externalIds":{"ArXiv":"2307.03073","DBLP":"journals/corr/abs-2307-03073","DOI":"10.1109/IROS58592.2024.10801660","CorpusId":259360864},"title":"Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning"},{"paperId":"318128fa82a15888a5db28341c5c23d1147271f3","externalIds":{"DBLP":"journals/pami/ChenWCJGL24","ArXiv":"2306.16927","DOI":"10.1109/TPAMI.2024.3435937","CorpusId":259287283,"PubMed":"39078757"},"title":"End-to-End Autonomous Driving: Challenges and Frontiers"},{"paperId":"34847169f8ec239292abdefc094b60569b4ef5bd","externalIds":{"DBLP":"journals/corr/abs-2306-07962","ArXiv":"2306.07962","DOI":"10.48550/arXiv.2306.07962","CorpusId":259145009},"title":"Parting with Misconceptions about Learning-based Vehicle Motion Planning"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","externalIds":{"DBLP":"journals/corr/abs-2306-07207","ArXiv":"2306.07207","DOI":"10.1145/3796716","CorpusId":259138706},"title":"Valley: Video Assistant with Large Language Model Enhanced Ability"},{"paperId":"9120da58459f3fca99f94ca343467653f5275a40","externalIds":{"DBLP":"journals/corr/abs-2306-06344","ArXiv":"2306.06344","DOI":"10.48550/arXiv.2306.06344","CorpusId":259137395},"title":"Language-Guided Traffic Simulation via Scene-Level Diffusion"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"ee156428803c5bd6e7372f6b27d74bcf88390db3","externalIds":{"ArXiv":"2305.14836","DBLP":"conf/aaai/QianCZJJ24","DOI":"10.48550/arXiv.2305.14836","CorpusId":258866014},"title":"NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario"},{"paperId":"dcb051ce8825ae82bde44f43938c070d18394f1f","externalIds":{"ArXiv":"2305.06242","DBLP":"journals/corr/abs-2305-06242","DOI":"10.1109/CVPR52729.2023.02105","CorpusId":258587978},"title":"Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving"},{"paperId":"c748a4dcb7b67f5b9465a08bdfa4eab8851be6e7","externalIds":{"ArXiv":"2304.05277","CorpusId":258059866},"title":"Graph-based Topology Reasoning for Driving Scenes"},{"paperId":"923a03032014a12c4e8b26511c0394e1b915fe74","externalIds":{"DBLP":"conf/iccv/KhachatryanMTHW23","ArXiv":"2303.13439","DOI":"10.1109/ICCV51070.2023.01462","CorpusId":257687280},"title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"},{"paperId":"c5b2243baf88a00db2d4e4f9edb33cde08eb153f","externalIds":{"ArXiv":"2303.11403","DBLP":"journals/corr/abs-2303-11403","DOI":"10.1109/ICCV51070.2023.02016","CorpusId":257636922},"title":"eP-ALM: Efficient Perceptual Augmentation of Language Models"},{"paperId":"26c6090b7e7ba4513f82aa28d41360c60770c618","externalIds":{"ArXiv":"2303.08320","DBLP":"journals/corr/abs-2303-08320","DOI":"10.1109/CVPR52729.2023.00984","CorpusId":257532642},"title":"VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"6584892e7d0f0055ee9adabf03f36f2fa74319e5","externalIds":{"DBLP":"journals/corr/abs-2301-06267","ArXiv":"2301.06267","DOI":"10.1109/CVPR52729.2023.01852","CorpusId":255942320},"title":"Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models"},{"paperId":"46f4ed274096a8fa561a768ac9b19f192dc4602a","externalIds":{"DBLP":"journals/corr/abs-2301-04634","ArXiv":"2301.04634","DOI":"10.1109/LRA.2024.3368234","CorpusId":255595883},"title":"Street-View Image Generation From a Bird's-Eye View Layout"},{"paperId":"aa5645b4896acb72aa4893d174af765d962aa708","externalIds":{"DBLP":"conf/iclr/WuCLJY023","ArXiv":"2301.01006","CorpusId":257532315},"title":"Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling"},{"paperId":"fdd7d5b0f6b8641c356e170fd264cd11f70ba657","externalIds":{"DBLP":"conf/cvpr/HuYCLSZCDLWLJLD23","ArXiv":"2212.10156","DOI":"10.1109/CVPR52729.2023.01712","CorpusId":257687420},"title":"Planning-oriented Autonomous Driving"},{"paperId":"247fbf266e70e196d983f53e7b4990b9548b0fc5","externalIds":{"DBLP":"journals/corr/abs-2210-17366","ArXiv":"2210.17366","DOI":"10.1109/ICRA48891.2023.10161463","CorpusId":253237771},"title":"Guided Conditional Diffusion for Controllable Traffic Simulation"},{"paperId":"d532ade62543bfc52a4827f90fa4128dbfffb12f","externalIds":{"ArXiv":"2209.13508","DBLP":"journals/corr/abs-2209-13508","DOI":"10.48550/arXiv.2209.13508","CorpusId":252545317},"title":"Motion Transformer with Global Intention Localization and Local Movement Refinement"},{"paperId":"eb85c73e8bcfb66bcd5dde49e43c179e2c561ffe","externalIds":{"ArXiv":"2209.10767","DBLP":"journals/corr/abs-2209-10767","DOI":"10.1109/WACV56688.2023.00110","CorpusId":252439240},"title":"DRAMA: Joint Risk Localization and Captioning in Driving"},{"paperId":"77ae985b6c43c56ae64077d4839d89cbea444da1","externalIds":{"DBLP":"journals/pami/LiSDWLWZLYDTXXCLLGJLSLQ24","ArXiv":"2209.05324","DOI":"10.1109/TPAMI.2023.3333838","CorpusId":252199219,"PubMed":"37976193"},"title":"Delving Into the Devils of Birdâ€™s-Eye-View Perception: A Review, Evaluation and Recipe"},{"paperId":"1b91e3aea4b247b51f40b061de48532de4bc09ba","externalIds":{"ArXiv":"2209.04796","DBLP":"journals/corr/abs-2209-04796","DOI":"10.48550/arXiv.2209.04796","CorpusId":252200170},"title":"Multiple Object Tracking in Recent Times: A Literature Review"},{"paperId":"40717baa413da976571bf82410ea5542b0be86db","externalIds":{"DBLP":"conf/nips/WuJCYLQ22","ArXiv":"2206.08129","DOI":"10.48550/arXiv.2206.08129","CorpusId":249712386},"title":"Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline"},{"paperId":"c539f6ab5818bde96f61298856cb0c38f6268369","externalIds":{"DBLP":"conf/cvpr/Parmar0Z22","DOI":"10.1109/CVPR52688.2022.01112","CorpusId":249879327},"title":"On Aliased Resizing and Surprising Subtleties in GAN Evaluation"},{"paperId":"1a8ac422f4e594155af1721837009264ce14fe32","externalIds":{"DBLP":"journals/pami/ChittaPJYRG23","ArXiv":"2205.15997","DOI":"10.1109/TPAMI.2022.3200245","CorpusId":249209900,"PubMed":"35984797"},"title":"TransFuser: Imitation With Transformer-Based Sensor Fusion for Autonomous Driving"},{"paperId":"4ac46b5bbd5728b51279d07c1730b509943a982e","externalIds":{"ArXiv":"2205.13542","DBLP":"journals/corr/abs-2205-13542","DOI":"10.1109/ICRA48891.2023.10160968","CorpusId":249097415},"title":"BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation"},{"paperId":"24bc17895733e7d3bf0632c2b1004a07c0a6d951","externalIds":{"DBLP":"journals/corr/abs-2205-09753","ArXiv":"2205.09753","DOI":"10.1109/TPAMI.2023.3298301","CorpusId":248965065,"PubMed":"37486847"},"title":"HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"a824c6e214dd0118f70af8bb05d67d94a858d076","externalIds":{"DBLP":"conf/eccv/LiWLXSLQD22","ArXiv":"2203.17270","DOI":"10.48550/arXiv.2203.17270","CorpusId":247839336},"title":"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers"},{"paperId":"09a8102a46ab60d92632b70aba015a29093151ec","externalIds":{"DBLP":"journals/corr/abs-2203-11934","ArXiv":"2203.11934","DOI":"10.1109/CVPR52688.2022.01671","CorpusId":247596695},"title":"Learning from All Vehicles"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"6296aa7cab06eaf058f7291040b320b5a83c0091","externalIds":{"DBLP":"journals/corr/abs-2203-00667","ArXiv":"2203.00667","DOI":"10.1109/ICCCNT56998.2023.10306417","CorpusId":1033682},"title":"Generative Adversarial Networks"},{"paperId":"658a017302d29e4acf4ca789cb5d9f27983717ff","externalIds":{"DBLP":"conf/cvpr/ChengMSKG22","ArXiv":"2112.01527","DOI":"10.1109/CVPR52688.2022.00135","CorpusId":244799297},"title":"Masked-attention Mask Transformer for Universal Image Segmentation"},{"paperId":"07669f4c3a853545fd4ba9057178f4c380ae3910","externalIds":{"ArXiv":"2109.03805","DBLP":"journals/ral/FongMHZCBV22","DOI":"10.1109/lra.2022.3148457","CorpusId":237442193},"title":"Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking"},{"paperId":"dbe41b7715078917c2f65dfda316c34d47afe697","externalIds":{"DBLP":"journals/corr/abs-2109-01827","ArXiv":"2109.01827","DOI":"10.1109/icra46639.2022.9812253","CorpusId":237263205},"title":"GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"6460249d2eb60d9d801908d9aacf0cfb7064174e","externalIds":{"DBLP":"journals/corr/abs-2107-08142","ArXiv":"2107.08142","CorpusId":236087684},"title":"Autonomy 2.0: Why is self-driving always 5 years away?"},{"paperId":"9c7b437a12182fb5b8945994bd6192eab9eef55e","externalIds":{"DBLP":"conf/corl/DeoWB21","ArXiv":"2106.15004","CorpusId":235669892},"title":"Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals"},{"paperId":"4ddbaccfd305091f5fb45ec76f857fc4343e108a","externalIds":{"DOI":"10.1016/j.jvir.2021.06.009","CorpusId":235660984,"PubMed":"34175432"},"title":"Follow the instructions."},{"paperId":"b6d83dfbd8b748006cb05882674e82054e5bcc32","externalIds":{"DBLP":"journals/corr/abs-2105-03247","ArXiv":"2105.03247","DOI":"10.1007/978-3-031-19812-0_38","CorpusId":234096063},"title":"MOTR: End-to-End Multiple-Object Tracking with TRansformer"},{"paperId":"b07fb2dbaeda171c5f1d821f492dd0cb8bd4e668","externalIds":{"DBLP":"journals/corr/abs-2104-10133","ArXiv":"2104.10133","DOI":"10.1109/ICCV48922.2021.00957","CorpusId":233307215},"title":"Large Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open Motion Dataset"},{"paperId":"1840e87d1720d1e00fedb6d29130dd80569b5d24","externalIds":{"DBLP":"journals/tase/HoferBHGMGAFGLL21","DOI":"10.1109/TASE.2021.3064065","CorpusId":233303198},"title":"Sim2Real in Robotics and Automation: Applications and Challenges"},{"paperId":"42d21108b026d814d8243242459ea8d283c4d70d","externalIds":{"DBLP":"conf/cvpr/XuHL21","ArXiv":"2103.15538","DOI":"10.1109/CVPR46437.2021.00975","CorpusId":232404763},"title":"SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2ce9702e8a5dea0aec98ed3a8f70d92454fb032a","externalIds":{"DBLP":"journals/corr/abs-2011-02403","MAG":"3095341535","ArXiv":"2011.02403","DOI":"10.1109/LRA.2021.3062309","CorpusId":226246272},"title":"IDE-Net: Interactive Driving Event and Pattern Extraction From Human Data"},{"paperId":"70bc865301a87cba2ddd892f6a5ee88368c1b271","externalIds":{"MAG":"3109791956","ArXiv":"2008.05930","DBLP":"journals/corr/abs-2008-05930","DOI":"10.1007/978-3-030-58592-1_25","CorpusId":221112947},"title":"Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable Semantic Representations"},{"paperId":"22d40963e633e1b4af4a9fefda68e1b8dc96ba63","externalIds":{"MAG":"3036067687","DBLP":"journals/corr/abs-2006-11275","ArXiv":"2006.11275","DOI":"10.1109/CVPR46437.2021.01161","CorpusId":219956621},"title":"Center-based 3D Object Detection and Tracking"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"4ef52feb1997b1a71f1ca4d49f72a5ce4d43a8b0","externalIds":{"ArXiv":"2005.14711","MAG":"3034782805","DBLP":"journals/corr/abs-2005-14711","DOI":"10.1109/cvpr42600.2020.01157","CorpusId":219124090},"title":"PnPNet: End-to-End Perception and Prediction With Tracking in the Loop"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"5895cdf583a12d25d92b79d89a3f1e9fe006b179","externalIds":{"MAG":"2265055174","DOI":"10.7551/mitpress/10692.001.0001","CorpusId":265038863},"title":"Decomposed"},{"paperId":"ecf2a5496c765c8a0133c45952e82e3756961a11","externalIds":{"DBLP":"journals/corr/abs-1909-10838","MAG":"2970603850","ArXiv":"1909.10838","ACL":"D19-1215","DOI":"10.18653/v1/D19-1215","CorpusId":202734592},"title":"Talk2Car: Taking Control of Your Self-Driving Car"},{"paperId":"dee86cf4ddab965169297960b55a4a2c0aee58d5","externalIds":{"MAG":"2986684110","DBLP":"conf/cvpr/KimMCTC19","ArXiv":"1911.06978","DOI":"10.1109/CVPR.2019.01084","CorpusId":204241631},"title":"Grounding Human-To-Vehicle Advice for Self-Driving Vehicles"},{"paperId":"9e475a514f54665478aac6038c262e5a6bac5e64","externalIds":{"DBLP":"journals/corr/abs-1903-11027","ArXiv":"1903.11027","MAG":"3035574168","DOI":"10.1109/cvpr42600.2020.01164","CorpusId":85517967},"title":"nuScenes: A Multimodal Dataset for Autonomous Driving"},{"paperId":"889c81b4d7b7ed43a3f69f880ea60b0572e02e27","externalIds":{"ArXiv":"1902.09630","MAG":"2916043515","DBLP":"conf/cvpr/RezatofighiTGS019","DOI":"10.1109/CVPR.2019.00075","CorpusId":67855581},"title":"Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression"},{"paperId":"b59233aab8364186603967bc12d88af48cc0992d","externalIds":{"DBLP":"journals/corr/abs-1812-01717","ArXiv":"1812.01717","MAG":"2902437806","CorpusId":54458806},"title":"Towards Accurate Generative Models of Video: A New Metric & Challenges"},{"paperId":"e2a7ca0d20c3c8cdc72742d0f1abe9734f4dba93","externalIds":{"DOI":"10.1201/9781315144948-10","CorpusId":240025634},"title":"Tracking"},{"paperId":"9d0907770cd4619aa6a36139a859e8f09bc9f0ef","externalIds":{"MAG":"2885138528","ArXiv":"1807.11546","DBLP":"conf/eccv/KimRDCA18","DOI":"10.1007/978-3-030-01216-8_35","CorpusId":51887402},"title":"Textual Explanations for Self-Driving Vehicles"},{"paperId":"53f3e676a1e7adc2b727a992a819352435365104","externalIds":{"MAG":"2951897135","DBLP":"journals/corr/abs-1806-01896","ArXiv":"1806.01896","DOI":"10.1145/3274005.3274032","CorpusId":46949537},"title":"Performance Evaluation of Deep Learning Networks for Semantic Segmentation of Traffic Stereo-Pair Images"},{"paperId":"9bdc71eacf7440bab8d2852f229da537498c9546","externalIds":{"DBLP":"journals/corr/abs-2012-12395","ArXiv":"2012.12395","MAG":"2798930779","DOI":"10.1109/CVPR.2018.00376","CorpusId":49366092},"title":"Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net"},{"paperId":"a20f3dfc9142b48b924e68ee22ba259a0d621bb2","externalIds":{"DBLP":"journals/corr/ShahDLK17","MAG":"2952869693","ArXiv":"1705.05065","DOI":"10.1007/978-3-319-67361-5_40","CorpusId":20999239},"title":"AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles"},{"paperId":"f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f","externalIds":{"DBLP":"journals/corr/AndersonFJG16","MAG":"2950201573","ArXiv":"1607.08822","DOI":"10.1007/978-3-319-46454-1_24","CorpusId":11933981},"title":"SPICE: Semantic Propositional Image Caption Evaluation"},{"paperId":"f35559b2497a885bd3dad039ad5212bb48a0e60f","externalIds":{"ArXiv":"1607.03476","MAG":"2951785524","DBLP":"conf/accv/HendersonF16","DOI":"10.1007/978-3-319-54193-8_13","CorpusId":347907},"title":"End-to-End Training of Object Class Detectors for Mean Average Precision"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"0f899b92b7fb03b609fee887e4b6f3b633eaf30d","externalIds":{"MAG":"299440670","ArXiv":"1505.05770","DBLP":"journals/corr/RezendeM15","CorpusId":12554042},"title":"Variational Inference with Normalizing Flows"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"b75ee166a4dfb335ccab604148ed0b02a6812113","externalIds":{"MAG":"3030336169","DOI":"10.1021/cen-09225-bus1","CorpusId":194251638},"title":"End To End"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"2258e01865367018ed6f4262c880df85b94959f8","externalIds":{"DBLP":"journals/ejivp/BernardinS08","MAG":"2124781496","DOI":"10.1155/2008/246309","CorpusId":13567980},"title":"Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"b58a7707f1027ea9d161a4ae7a16d0e6c205e64d","externalIds":{"DOI":"10.1080/136688000200032489","CorpusId":218567425},"title":"James"},{"paperId":"5d3b5d2c6b1775b575b484296cd7db67fbbfdbe4","externalIds":{"ArXiv":"cond-mat/0002177","MAG":"1965455100","DOI":"10.1103/PhysRevE.62.1805","CorpusId":1100293,"PubMed":"11088643"},"title":"Congested traffic states in empirical observations and microscopic simulations"},{"paperId":"c769286434affcba7153281c54f8ae952a3dea63","externalIds":{"MAG":"1964209572","DOI":"10.1145/323409.328679","CorpusId":44292826},"title":"Language"},{"paperId":"4aa95dc3682d664b333a868ce350d1567abc47cd","externalIds":{"DOI":"10.1017/s0305741000011243","CorpusId":210923373,"PubMed":"31983695"},"title":"Contributors"},{"paperId":"bcf2bd95a6f60dd2998b57c26873d31461011e8d","externalIds":{"DBLP":"journals/corr/abs-2406-01309","DOI":"10.48550/arXiv.2406.01309","CorpusId":270211241},"title":"REvolve: Reward Evolution with Large Language Models for Autonomous Driving"},{"paperId":"50f66327a45813b442824a5796bdc1a608a3f6fc","externalIds":{"DBLP":"conf/eccv/LiZY24","DOI":"10.1007/978-3-031-73229-4_27","CorpusId":273706421},"title":"DrivingDiffusion: Layout-Guided Multi-view Driving Scenarios Video Generation with Latent Diffusion Model"},{"paperId":"40fbc19197da3e414aa6e460c5e39789cf248100","externalIds":{"DBLP":"journals/corr/abs-2405-01533","DOI":"10.48550/arXiv.2405.01533","CorpusId":277824043},"title":"OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning"},{"paperId":"825298ea1933f01f1cfbff0c5e9bca4c60fdb5cd","externalIds":{"DBLP":"journals/corr/abs-2309-13193","DOI":"10.48550/arXiv.2309.13193","CorpusId":262460657},"title":"SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model"},{"paperId":"6692fc513f8b649fef164704422647c55bd744eb","externalIds":{"DBLP":"journals/corr/abs-2311-14786","DOI":"10.48550/arXiv.2311.14786","CorpusId":265456670},"title":"GPT-4V Takes the Wheel: Evaluating Promise and Challenges for Pedestrian Behavior Prediction"},{"paperId":"e9f389bc1ccf32bc0ed3c0d5925c9837dfbf9bd5","externalIds":{"DBLP":"journals/corr/abs-2312-09397","DOI":"10.48550/arXiv.2312.09397","CorpusId":273501718},"title":"Large Language Models for Autonomous Driving: Real-World Experiments"},{"paperId":"8f5f7ae022a075dda077f8a774b836fca9d09bfe","externalIds":{"DBLP":"conf/iclr/ZhangZYCZYLZNL23","CorpusId":259298519},"title":"GoBigger: A Scalable Platform for Cooperative-Competitive Multi-Agent Interactive Simulation"},{"paperId":"95ea8e08cfa9212aa2e0855c082323d53f3d9e02","externalIds":{"DBLP":"conf/corl/JiaCWZYL022","CorpusId":257432752},"title":"Towards Capturing the Temporal Dynamics for Trajectory Prediction: a Coarse-to-Fine Approach"},{"paperId":"5dc430988e6818826aac6004c3c8ae0232311fc2","externalIds":{"CorpusId":265038263},"title":"Video Question"},{"paperId":"8297426d6925f6976dfd4249c37c46f10fd90fc9","externalIds":{"DBLP":"conf/corl/JiaSZTZ21","CorpusId":246037656},"title":"Multi-Agent Trajectory Prediction by Combining Egocentric and Allocentric Views"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"}]}