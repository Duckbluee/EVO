{"abstract":"Semantic communication is considered as a promising enabler for saving gradually exhausted spectrum resources due to its innovative idea of aggressively extracting critical information. Although the learnable semantic-aware codec brings exciting compression gains, its data-driven kernel endogenously incurs conflicts between data utilization and privacy protection. Namely, the data utility loss hurts transmission reliability while the need toward knowledge sharing brings privacy disclosure concerns. In this article, we first analyze the key challenges of semantic communication derived from its high dependence on sample richness and diversity. To strike an efficient trade-off between data utilization and privacy protection, we propose a joint online inference and offline training framework for semantic communication, which covers privacy-protected codec training and personalized codec deployment. Specifically, a novel distributed learning strategy is developed for codec training, where model splitting between clients and servers makes gradient leakage attacks computationally prohibitive and partial model aggregation provides customized data utilization. For personalized codec deployment, a model pre-training and distillation scheme is designed by exploiting modality-aware semantic associations. Lastly, an experimental case study demonstrates the superiority of our proposed strategy in balancing data utilization and privacy protection in semantic communications."}