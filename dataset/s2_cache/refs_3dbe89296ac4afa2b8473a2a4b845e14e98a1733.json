{"references":[{"paperId":"fc41ff83f506476e7be31a6c079b4b57c28d1efe","externalIds":{"DBLP":"journals/tsmc/YangZSCWL23","DOI":"10.1109/TSMC.2023.3248324","CorpusId":257477888},"title":"Watch and Act: Learning Robotic Manipulation From Visual Demonstration"},{"paperId":"2af40b47e58ce167280d0fd97a9be6acd5388fcf","externalIds":{"DOI":"10.1109/JSEN.2023.3256704","CorpusId":257620903},"title":"A Safe Driving Decision-Making Methodology Based on Cascade Imitation Learning Network for Automated Commercial Vehicles"},{"paperId":"212bfd361896b0a6735caa48a3a28b7c45857d54","externalIds":{"DBLP":"conf/icra/PaolilloGS23","DOI":"10.1109/ICRA48891.2023.10160935","CorpusId":259338500},"title":"Dynamical System-based Imitation Learning for Visual Servoing using the Large Projection Formulation"},{"paperId":"8922fccadca8044e11425739922981561865db2b","externalIds":{"DBLP":"journals/corr/abs-2305-03365","ArXiv":"2305.03365","DOI":"10.48550/arXiv.2305.03365","CorpusId":258546960},"title":"Repairing Deep Neural Networks Based on Behavior Imitation"},{"paperId":"74b05bba46db21e589a2cc0f916f81069b0368ef","externalIds":{"DBLP":"journals/corr/abs-2305-00955","ArXiv":"2305.00955","DOI":"10.48550/arXiv.2305.00955","CorpusId":258426970},"title":"Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"},{"paperId":"7e287a03b8d4073ec05537eb100dee91ec292195","externalIds":{"ArXiv":"2301.06387","DBLP":"conf/atal/LouGZ0H023","DOI":"10.48550/arXiv.2301.06387","CorpusId":255941619},"title":"PECAN: Leveraging Policy Ensemble for Context-Aware Zero-Shot Human-AI Coordination"},{"paperId":"9ffc8f7b3fbd5e609f609b1c20206129f22b4eb7","externalIds":{"DBLP":"journals/corr/abs-2212-05711","ArXiv":"2212.05711","DOI":"10.48550/arXiv.2212.05711","CorpusId":254183982},"title":"CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning"},{"paperId":"4c09d6969f451d288d8a188aa7e48a2af38d1911","externalIds":{"DBLP":"journals/corr/abs-2210-09598","ArXiv":"2210.09598","DOI":"10.48550/arXiv.2210.09598","CorpusId":252968311},"title":"Planning for Sample Efficient Imitation Learning"},{"paperId":"c3b849ac59741497ca70e9ceabb9367b50b22f42","externalIds":{"DBLP":"journals/corr/abs-2208-00088","ArXiv":"2208.00088","DOI":"10.48550/arXiv.2208.00088","CorpusId":251223497},"title":"Improved Policy Optimization for Online Imitation Learning"},{"paperId":"32c9b3859086d15184989454eb878638659e64c6","externalIds":{"ArXiv":"2206.08853","DBLP":"conf/nips/FanWJMYZTHZA22","DOI":"10.48550/arXiv.2206.08853","CorpusId":249848263},"title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge"},{"paperId":"4c0f15750769e7f96e3f5ff3446e2829f4c0ed2d","externalIds":{"DBLP":"journals/corr/abs-2204-11446","ArXiv":"2204.11446","DOI":"10.48550/arXiv.2204.11446","CorpusId":248377437},"title":"Imitation Learning from Observations under Transition Model Disparity"},{"paperId":"88a74a1d669c390ce78cbc7dc43bd4dd0397adf4","externalIds":{"DBLP":"conf/hri/ShihES22","ArXiv":"2201.01448","DOI":"10.1109/HRI53351.2022.9889671","CorpusId":245704713},"title":"Conditional Imitation Learning for Multi-Agent Games"},{"paperId":"86ff08680aa9eff4f751ead8a27e818e688d4cb1","externalIds":{"ArXiv":"2108.01069","DBLP":"conf/iros/ShangR21","DOI":"10.1109/IROS51168.2021.9636363","CorpusId":236772575},"title":"Self-Supervised Disentangled Representation Learning for Third-Person Imitation Learning"},{"paperId":"41fb9393955d6e3d401c4efa59be100a408e7935","externalIds":{"ArXiv":"2106.13687","CorpusId":245335407},"title":"panda-gym: Open-source goal-conditioned environments for robotic learning"},{"paperId":"3f82ad2581b8905f8a3b5359e37c97a560cf636e","externalIds":{"ArXiv":"2105.10037","DBLP":"conf/icml/RaychaudhuriPBR21","CorpusId":235125527},"title":"Cross-domain Imitation from Observations"},{"paperId":"330bd0ae3926e294f728f29dc5510ad51f81d427","externalIds":{"ArXiv":"2104.00163","DBLP":"conf/iros/TorabiWS21","MAG":"3143756065","DOI":"10.1109/IROS51168.2021.9636169","CorpusId":232478875},"title":"DEALIO: Data-Efficient Adversarial Learning for Imitation from Observation"},{"paperId":"e99dd160c931425f485ada0d1bf13ab2d33f9685","externalIds":{"DBLP":"conf/nips/ZhuLDZ20","MAG":"3102359080","ArXiv":"2102.13185","CorpusId":227276236},"title":"Off-Policy Imitation Learning from Observations"},{"paperId":"3f8e2cdd7a73ea0193f43ee3b69a717baded5310","externalIds":{"ArXiv":"2102.10769","DBLP":"conf/nips/KidambiCS21","CorpusId":235436359},"title":"MobILE: Model-Based Imitation Learning From Observation Alone"},{"paperId":"3002d4f76f36bd78bf9e2118fbc830648cc5ea08","externalIds":{"DBLP":"journals/air/DuD21","MAG":"3106649810","DOI":"10.1007/s10462-020-09938-y","CorpusId":229459548},"title":"A survey on multi-agent deep reinforcement learning: from the perspective of challenges and applications"},{"paperId":"3ac9a2fede1e8f50e47577eabd07d567d923925f","externalIds":{"DBLP":"conf/iros/PulverECHAR21","MAG":"3097323743","ArXiv":"2011.00509","DOI":"10.1109/IROS51168.2021.9636862","CorpusId":226226533},"title":"PILOT: Efficient Planning by Imitation Learning and Optimisation for Safe Autonomous Driving"},{"paperId":"4aaae90048280b90271cccae49d1e23e5ed3cc3a","externalIds":{"MAG":"3091852309","ArXiv":"2010.05359","DBLP":"journals/corr/abs-2010-05359","DOI":"10.1109/ICRA48506.2021.9561306","CorpusId":222290833},"title":"Inverse Dynamics vs. Forward Dynamics in Direct Transcription Formulations for Trajectory Optimization"},{"paperId":"adb871f048f5c7de72d2517b741ce998a61cfa2a","externalIds":{"MAG":"3048412490","ArXiv":"2008.05660","DBLP":"journals/corr/abs-2008-05660","DOI":"10.5244/c.34.167","CorpusId":221112576},"title":"Imitating Unknown Policies via Exploration"},{"paperId":"82c26810b86a76454757770c14da5f772aa7751b","externalIds":{"MAG":"3042058594","ArXiv":"2007.02753","DBLP":"conf/iros/LucchiZMP20","DOI":"10.1109/IROS45743.2020.9340956","CorpusId":220364830},"title":"robo-gym – An Open Source Toolkit for Distributed Deep Reinforcement Learning on Real and Simulated Robots"},{"paperId":"9c546735ed31bdbef9c418d7cf95b28a638d3cb8","externalIds":{"ArXiv":"2006.15061","MAG":"3035432676","DBLP":"journals/corr/abs-2006-15061","CorpusId":220128012},"title":"Intrinsic Reward Driven Imitation Learning via Generative Model"},{"paperId":"8ba600c169f0d2422625822223976bce562eabe1","externalIds":{"MAG":"3036619998","DBLP":"journals/simpa/Tunyasuvunakool20","ArXiv":"2006.12983","DOI":"10.1016/j.simpa.2020.100022","CorpusId":219980295},"title":"dm_control: Software and Tasks for Continuous Control"},{"paperId":"1cc12fe047e4d54577c11c6bc8acebffa07a26b5","externalIds":{"DBLP":"conf/ijcnn/MonteiroGGMB20","MAG":"3090121858","ArXiv":"2004.13529","DOI":"10.1109/IJCNN48605.2020.9207672","CorpusId":216562819},"title":"Augmented Behavioral Cloning from Observation"},{"paperId":"ca5045c9d9e0bf2e95f6694dff657e28ffcd4f07","externalIds":{"ArXiv":"1912.12294","DBLP":"journals/corr/abs-1912-12294","MAG":"2996998055","CorpusId":204780964},"title":"Learning by Cheating"},{"paperId":"18d026ec5d0eebd17ee2c762da89540c0b3d7bde","externalIds":{"ArXiv":"1911.02685","MAG":"3041133507","DBLP":"journals/pieee/ZhuangQDXZZXH21","DOI":"10.1109/JPROC.2020.3004555","CorpusId":207847753},"title":"A Comprehensive Survey on Transfer Learning"},{"paperId":"f7bb5d5ffe9e908b400dfbf8455580b303e04f2c","externalIds":{"ACL":"D19-1266","MAG":"2971038186","DBLP":"conf/emnlp/LiC19","DOI":"10.18653/v1/D19-1266","CorpusId":202767523},"title":"DIVINE: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning"},{"paperId":"eb41a0cffa84d3d6035e6f5f420806ddc962b1e6","externalIds":{"ArXiv":"1910.05789","MAG":"2980061931","DBLP":"journals/corr/abs-1910-05789","CorpusId":202770731},"title":"On the Utility of Learning about Humans for Human-AI Coordination"},{"paperId":"5116e926f11838a1d1dcd681b3269eec9e2d5b16","externalIds":{"MAG":"2971073116","DBLP":"journals/corr/abs-1908-10835","ACL":"D19-1619","ArXiv":"1908.10835","DOI":"10.18653/v1/D19-1619","CorpusId":201660357},"title":"An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation"},{"paperId":"fff2bcc0348dffdb7661640d51738d4716a6304c","externalIds":{"DBLP":"journals/corr/abs-1906-07372","MAG":"2949673982","ArXiv":"1906.07372","DOI":"10.1109/LRA.2020.3010750","CorpusId":189999150},"title":"RIDM: Reinforced Inverse Dynamics Modeling for Learning from a Single Observed Demonstration"},{"paperId":"cbcbdb44d9d4ad7bb6bf4e9104653aa7623a17c5","externalIds":{"DBLP":"journals/corr/abs-1905-13566","ArXiv":"1905.13566","MAG":"2964460729","DOI":"10.24963/ijcai.2019/882","CorpusId":173188327},"title":"Recent Advances in Imitation Learning from Observation"},{"paperId":"6e81e3b82c875063dca2aee551aa5d50cc3c69cc","externalIds":{"ArXiv":"1905.10948","DBLP":"conf/icml/0002VBB19","MAG":"2949696506","CorpusId":166228643},"title":"Provably Efficient Imitation Learning from Observation Alone"},{"paperId":"1706cc1c2433275fc326967da8318790378da850","externalIds":{"ArXiv":"1905.09335","MAG":"2945581894","DBLP":"conf/ijcai/TorabiWS19","DOI":"10.24963/ijcai.2019/497","CorpusId":162184018},"title":"Imitation Learning from Video by Leveraging Proprioception"},{"paperId":"15704ce8121737e3eb109a210d4b72e2fe1a0ad7","externalIds":{"ArXiv":"1903.04110","MAG":"2904145923","DBLP":"conf/aaai/GuoCYTC19","DOI":"10.1609/AAAI.V33I01.33013739","CorpusId":59000414},"title":"Hybrid Reinforcement Learning with Expert State Sequences"},{"paperId":"99a7df93a2e16bd7ac3349d52cc34417cda7909d","externalIds":{"MAG":"2919872398","DBLP":"journals/corr/abs-1903-01973","ArXiv":"1903.01973","CorpusId":67877011},"title":"Learning Latent Plans from Play"},{"paperId":"6a505dbfb89cf05344457bf85b2e8307af5c4ad0","externalIds":{"MAG":"2913781869","DBLP":"journals/corr/abs-1902-00506","ArXiv":"1902.00506","DOI":"10.1016/j.artint.2019.103216","CorpusId":59553476},"title":"The Hanabi Challenge: A New Frontier for AI Research"},{"paperId":"ef2bc452812d6005ab0a66af6c3f97b6b0ba837e","externalIds":{"MAG":"2903181768","ArXiv":"1812.02341","DBLP":"conf/icml/CobbeKHKS19","CorpusId":54448010},"title":"Quantifying Generalization in Reinforcement Learning"},{"paperId":"b1914c912dea62703856d89fe3724675a6139b71","externalIds":{"MAG":"2903709398","DBLP":"conf/itsc/LopezBBEFHLRWW18","DOI":"10.1109/ITSC.2018.8569938","CorpusId":54462985},"title":"Microscopic Traffic Simulation using SUMO"},{"paperId":"8dc8f3e0127adc6985d4695e9b69d04717b2fde8","externalIds":{"MAG":"2891612330","ArXiv":"1810.03292","DBLP":"journals/corr/abs-1810-03292","CorpusId":52938797},"title":"Sanity Checks for Saliency Maps"},{"paperId":"4b59846404c085f5c9523c69cd790537613a3df5","externalIds":{"MAG":"2964209830","DBLP":"conf/iclr/PengKTAL19","ArXiv":"1810.00821","CorpusId":52895409},"title":"Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow"},{"paperId":"1f1f8330cddf1f4bf7bd73478223e5c02b69a1ff","externalIds":{"MAG":"2884247313","DBLP":"journals/corr/abs-1807-06158","ArXiv":"1807.06158","CorpusId":49863329},"title":"Generative Adversarial Imitation from Observation"},{"paperId":"705bbc4dcd475f9230863771da6596e1f677a92d","externalIds":{"MAG":"2962715211","DBLP":"journals/corr/abs-1805-11592","ArXiv":"1805.11592","CorpusId":44061126},"title":"Playing hard exploration games by watching YouTube"},{"paperId":"35da1cd669ad5492a6358ea53aea95de28d39ded","externalIds":{"DBLP":"conf/ijcai/TorabiWS18","MAG":"2952876086","ArXiv":"1805.01954","DOI":"10.24963/ijcai.2018/687","CorpusId":23206414},"title":"Behavioral Cloning from Observation"},{"paperId":"395ea8a62d84c8dd85a8dadfc3043cf2228e38c5","externalIds":{"ArXiv":"1805.07914","DBLP":"journals/corr/abs-1805-07914","MAG":"2804020409","CorpusId":29156793},"title":"Imitating Latent Policies from Observation"},{"paperId":"31d1b5a681f0353f7751fbd1a367acbec9ef47fa","externalIds":{"MAG":"2796897898","DOI":"10.1101/298430","CorpusId":90992624},"title":"Evaluation of UMAP as an alternative to t-SNE for single-cell data"},{"paperId":"9217c4aa5a95d3209d2071e6889c8dd4b7d9309e","externalIds":{"MAG":"2908470496","DBLP":"conf/iclr/PathakMLACSSMED18","ArXiv":"1804.08606","DOI":"10.1109/CVPRW.2018.00278","CorpusId":5037032},"title":"Zero-Shot Visual Imitation"},{"paperId":"a9a3ed69c94a3e1c08ef1f833d9199f57736238b","externalIds":{"MAG":"2781585732","ArXiv":"1801.00690","DBLP":"journals/corr/abs-1801-00690","CorpusId":6315299},"title":"DeepMind Control Suite"},{"paperId":"62e39c6dbcead1fe4d29017914591d929aa6ac4c","externalIds":{"MAG":"2766047647","ArXiv":"1711.00867","DBLP":"series/lncs/KindermansHAASDEK19","DOI":"10.1007/978-3-030-28954-6_14","CorpusId":28562869},"title":"The (Un)reliability of saliency methods"},{"paperId":"ebf0615fc4d98cf1dbe527c79146ce1e50dce9af","externalIds":{"MAG":"2767621168","DBLP":"conf/corl/DosovitskiyRCLK17","ArXiv":"1711.03938","CorpusId":5550767},"title":"CARLA: An Open Urban Driving Simulator"},{"paperId":"b864f89eaa91120e04e8c62eb0b36568ab4244a8","externalIds":{"DBLP":"journals/corr/abs-1710-04615","MAG":"2761060327","ArXiv":"1710.04615","DOI":"10.1109/ICRA.2018.8461249","CorpusId":3720790},"title":"Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation"},{"paperId":"3b290ffa1f4f8226e326f00984acecdfbe9e28bf","externalIds":{"MAG":"2754879180","DBLP":"journals/jair/MachadoBTVHB18","ArXiv":"1709.06009","DOI":"10.1613/jair.5699","CorpusId":530289},"title":"Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents"},{"paperId":"498238a3bd5fd322fc3ce1572e33bbe3853a356f","externalIds":{"ArXiv":"1708.05866","DBLP":"journals/corr/abs-1708-05866","MAG":"3100789280","DOI":"10.1109/MSP.2017.2743240","CorpusId":4884302},"title":"Deep Reinforcement Learning: A Brief Survey"},{"paperId":"814b36fedfe7f7ce10eaa72bddb3c7ea7a663f37","externalIds":{"MAG":"2740210681","DBLP":"conf/icml/BaramACM17","CorpusId":19562686},"title":"End-to-End Differentiable Adversarial Imitation Learning"},{"paperId":"77fa0239b9b074e7b62ca3798b8abf6fa3823f80","externalIds":{"ArXiv":"1707.03374","MAG":"2735025040","DBLP":"journals/corr/LiuGAL17","DOI":"10.1109/ICRA.2018.8462901","CorpusId":2866526},"title":"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation"},{"paperId":"cddb1f7f9f004396a2efef285caf29d7780a8e21","externalIds":{"MAG":"2735089625","DBLP":"journals/corr/WangMRWFH17","ArXiv":"1707.02747","CorpusId":10324627},"title":"Robust Imitation of Diverse Behaviors"},{"paperId":"a002e71561c90767240672f357b7d9e6d4d95186","externalIds":{"DBLP":"journals/corr/MontavonSM17","MAG":"2657631929","ArXiv":"1706.07979","DOI":"10.1016/j.dsp.2017.10.011","CorpusId":207170725},"title":"Methods for interpreting and understanding deep neural networks"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"7c3ece1ba41c415d7e81cfa5ca33a8de66efd434","externalIds":{"DBLP":"conf/nips/LoweWTHAM17","MAG":"2623431351","ArXiv":"1706.02275","CorpusId":26419660},"title":"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"},{"paperId":"ca8b268ff7fc1b139db712225e0ebd565d7c5714","externalIds":{"MAG":"3088428855","ACL":"E17-5003","CorpusId":219303199},"title":"Imitation learning for structured prediction in natural language processing"},{"paperId":"4135004c75a361c91311314fc588d229a7107526","externalIds":{"MAG":"2963328631","DBLP":"conf/nips/LiSE17","CorpusId":4474295},"title":"InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations"},{"paperId":"5c57bb5630835a05eb1c3d0df3e12d6180d75de2","externalIds":{"DBLP":"journals/corr/DuanASHSSAZ17","MAG":"2951639441","ArXiv":"1703.07326","CorpusId":8270841},"title":"One-Shot Imitation Learning"},{"paperId":"5d2f5c2dc11c18c0d45203e2b980fe375a56d774","externalIds":{"ArXiv":"1703.04908","MAG":"2602275733","DBLP":"journals/corr/MordatchA17","DOI":"10.1609/aaai.v32i1.11492","CorpusId":13548281},"title":"Emergence of Grounded Compositional Language in Multi-Agent Populations"},{"paperId":"67a462fd5c8f20846df0d1a8cfedac70d5678a50","externalIds":{"DBLP":"journals/corr/NairCAIAML17","MAG":"2962871243","ArXiv":"1703.02018","DOI":"10.1109/ICRA.2017.7989247","CorpusId":17868859},"title":"Combining self-supervised learning and imitation for vision-based rope manipulation"},{"paperId":"2e1a1b9c2e8feeb31c6855292859bf94101e8382","externalIds":{"ArXiv":"1703.01703","DBLP":"journals/corr/StadieAS17","MAG":"2591957724","CorpusId":14724343},"title":"Third-Person Imitation Learning"},{"paperId":"5999079be4ed40b65b0957c442230bf2e61b9dae","externalIds":{"MAG":"2542768043","DOI":"10.23915/DISTILL.00002","CorpusId":155770513},"title":"How to Use t-SNE Effectively"},{"paperId":"8e0540ea3c3cc8cd009b2006d96c4b3ac2a84e52","externalIds":{"MAG":"2512051764","ArXiv":"1608.00627","DBLP":"journals/corr/DaftryBH16","DOI":"10.1007/978-3-319-50115-4_1","CorpusId":10634567},"title":"Learning Transferable Policies for Monocular Reactive MAV Control"},{"paperId":"bd6b6291c3c14551cf9f2aa0e04e2e33c86b800e","externalIds":{"DBLP":"conf/ijcai/JohnsonHHB16","MAG":"2480004914","CorpusId":9953039},"title":"The Malmo Platform for Artificial Intelligence Experimentation"},{"paperId":"4ab53de69372ec2cd2d90c126b6a100165dc8ed1","externalIds":{"DBLP":"journals/corr/HoE16","ArXiv":"1606.03476","MAG":"2949080919","CorpusId":16153365},"title":"Generative Adversarial Imitation Learning"},{"paperId":"a473f545318325ba23b7a6b477485d29777ba873","externalIds":{"MAG":"2362143032","ArXiv":"1605.02097","DBLP":"journals/corr/KempkaWRTJ16","DOI":"10.1109/CIG.2016.7860433","CorpusId":430714},"title":"ViZDoom: A Doom-based AI research platform for visual reinforcement learning"},{"paperId":"bfdf3431ff5182b585b3b0c11fa8cbfc13a6bde4","externalIds":{"MAG":"2951564032","ArXiv":"1604.03605","DBLP":"journals/pami/BylinskiiJOTD19","DOI":"10.1109/TPAMI.2018.2815601","CorpusId":13927710,"PubMed":"29993800"},"title":"What Do Different Evaluation Metrics Tell Us About Saliency Models?"},{"paperId":"04162cb8cfaa0f7e37586823ff4ad0bff09ed21d","externalIds":{"MAG":"2963590100","DBLP":"journals/corr/FinnLA16","ArXiv":"1603.00448","CorpusId":8121626},"title":"Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization"},{"paperId":"d316c82c12cf4c45f9e85211ef3d1fa62497bff8","externalIds":{"MAG":"1191599655","DBLP":"journals/corr/SchulmanMLJA15","ArXiv":"1506.02438","CorpusId":3075448},"title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","externalIds":{"ArXiv":"1409.0575","DBLP":"journals/corr/RussakovskyDSKSMHKKBBF14","MAG":"2546241758","DOI":"10.1007/s11263-015-0816-y","CorpusId":2930547},"title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"6fb1c2e081d060857b1181d6f4e052e54a8878cc","externalIds":{"MAG":"2092430508","DBLP":"conf/cec/LeeLZL14","DOI":"10.1109/CEC.2014.6900246","CorpusId":1172957},"title":"Learning a Super Mario controller from examples of human play"},{"paperId":"616b246e332573af1f4859aa91440280774c183a","externalIds":{"DBLP":"journals/ijcv/EveringhamEGWWZ15","MAG":"2037227137","DOI":"10.1007/s11263-014-0733-5","CorpusId":207252270},"title":"The Pascal Visual Object Classes Challenge: A Retrospective"},{"paperId":"0961e2650b3a62a1d198a046bef5f0700ab8c08f","externalIds":{"MAG":"1975675278","DBLP":"conf/ease/Wohlin14","DOI":"10.1145/2601248.2601268","CorpusId":3348787},"title":"Guidelines for snowballing in systematic literature studies and a replication in software engineering"},{"paperId":"f0d2fd5c3d7cb7cecc02f51c2c5da2efaec3a0a6","externalIds":{"DBLP":"journals/entcom/OrtegaSTY13","MAG":"2136065562","DOI":"10.1016/J.ENTCOM.2012.10.001","CorpusId":7731527},"title":"Imitating human playing styles in Super Mario Bros"},{"paperId":"2cb3f18be535757bd0aecdf6118e09aea4a8142c","externalIds":{"DBLP":"journals/evi/DAmbrosioS13","MAG":"2057010516","DOI":"10.1007/s12065-012-0086-3","CorpusId":18800251},"title":"Scalable multiagent learning through indirect encoding of policy geometry"},{"paperId":"b354ee518bfc1ac0d8ac447eece9edb69e92eae1","externalIds":{"MAG":"2158782408","DBLP":"conf/iros/TodorovET12","DOI":"10.1109/IROS.2012.6386109","CorpusId":5230692},"title":"MuJoCo: A physics engine for model-based control"},{"paperId":"71b552b2e058d5a6a760ba203f10f13be759edd3","externalIds":{"DBLP":"conf/iros/TassaET12","MAG":"2087617385","DOI":"10.1109/IROS.2012.6386025","CorpusId":6652724},"title":"Synthesis and stabilization of complex behaviors through online trajectory optimization"},{"paperId":"f813bcf55de3ebfd925afa37df81c1ec2da47568","externalIds":{"DBLP":"conf/robio/RazaHW12","MAG":"2047425368","DOI":"10.1109/ROBIO.2012.6491170","CorpusId":16022839},"title":"Teaching coordinated strategies to soccer robots via imitation"},{"paperId":"f82e4ff4f003581330338aaae71f60316e58dd26","externalIds":{"ArXiv":"1207.4708","DBLP":"journals/jair/BellemareNVB13","MAG":"2150468603","DOI":"10.1613/jair.3912","CorpusId":1552061},"title":"The Arcade Learning Environment: An Evaluation Platform for General Agents"},{"paperId":"9ad97d4258288d9f5fe75ff782decdcbb5a8f4d2","externalIds":{"MAG":"2294316001","DBLP":"conf/rss/ErezTT11","DOI":"10.15607/RSS.2011.VII.010","CorpusId":12424685},"title":"Infinite-Horizon Model Predictive Control for Periodic Tasks with Contacts"},{"paperId":"79ab3c49903ec8cb339437ccf5cf998607fc313e","externalIds":{"MAG":"2962957031","DBLP":"journals/jmlr/RossGB11","ArXiv":"1011.0686","CorpusId":103456},"title":"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"},{"paperId":"a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","externalIds":{"DBLP":"journals/tkde/PanY10","MAG":"2165698076","DOI":"10.1109/TKDE.2009.191","CorpusId":740063},"title":"A Survey on Transfer Learning"},{"paperId":"dae1287cad7580ee7035419a6b7fca9805ea173e","externalIds":{"MAG":"2095865381","DBLP":"conf/humanoids/ChernovaV08","DOI":"10.1109/ICHR.2008.4755982","CorpusId":14597248},"title":"Teaching collaborative multi-robot tasks through demonstration"},{"paperId":"c8221c054459e37edbf313668523d667fe5c1536","externalIds":{"DBLP":"conf/aaai/ZiebartMBD08","MAG":"2098774185","CorpusId":336219},"title":"Maximum Entropy Inverse Reinforcement Learning"},{"paperId":"4aece8df7bd59e2fbfedbf5729bba41abc56d870","externalIds":{"DBLP":"journals/tsmc/BusoniuBS08","MAG":"2099618002","DOI":"10.1109/TSMCC.2007.913919","CorpusId":206794869},"title":"A Comprehensive Survey of Multiagent Reinforcement Learning"},{"paperId":"0b6ed6ba5ea4c3b262496739c17a119ea7870d29","externalIds":{"MAG":"2130726249","DOI":"10.1098/RSTB.2002.1258","CorpusId":15250963,"PubMed":"12689379"},"title":"Computational approaches to motor learning by imitation."},{"paperId":"d7853249d888f3e7c4eaebe549db9fe526d02134","externalIds":{"DBLP":"journals/aim/KitanoAKNOM97","MAG":"1580572138","DOI":"10.1609/aimag.v18i1.1276","CorpusId":17560789},"title":"RoboCup: A Challenge Problem for AI"},{"paperId":"7ca8ac34767d6e6cb389eeebcdabc4225b39edfe","externalIds":{"DBLP":"conf/nips/Sutton95","MAG":"2124175081","CorpusId":10253791},"title":"Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding"},{"paperId":"8a7acaf6469c06ae5876d92f013184db5897bb13","externalIds":{"DBLP":"journals/tsmc/BartoSA83","MAG":"1583833196","DOI":"10.1109/TSMC.1983.6313077","CorpusId":1522994},"title":"Neuronlike adaptive elements that can solve difficult learning control problems"},{"paperId":"70987dd4fb41109c723bcff127818870662aa108","externalIds":{"DBLP":"journals/corr/abs-2404-19456","DOI":"10.48550/arXiv.2404.19456","CorpusId":277742754},"title":"Imitation Learning: A Survey of Learning Methods, Environments and Metrics"},{"paperId":"452d8c734a2ed6726f2e693bde1519ce0bc5a44b","externalIds":{"DBLP":"phd/dnb/Memmesheimer23","CorpusId":258240697},"title":"On the recognition of human activities and the evaluation of its imitation by robotic systems"},{"paperId":"1adc4b4c1393e58af7f0419675aff136bf83cbec","externalIds":{"DBLP":"conf/bracis/GavenskiMMB22","DOI":"10.1007/978-3-031-21689-3_32","CorpusId":253762675},"title":"How Resilient Are Imitation Learning Methods to Sub-optimal Experts?"},{"paperId":"fb8ad585a3f151a7d142a031bc6c37a8d39dbed7","externalIds":{"DBLP":"conf/ismis/HolmbergHH22","DOI":"10.1007/978-3-031-16564-1_17","CorpusId":252546499},"title":"More Sanity Checks for Saliency Maps"},{"paperId":"3e7044375e70afe3c314c59a5439b242aec08712","externalIds":{"DBLP":"journals/corr/abs-2106-12177","CorpusId":235606253},"title":"Imitation Learning: Progress, Taxonomies and Opportunities"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3","externalIds":{"CorpusId":18420840},"title":"ALVINN, an autonomous land vehicle in a neural network"},{"paperId":"1c46943103bd7b7a2c7be86859995a4144d1938b","externalIds":{"MAG":"2187089797","CorpusId":5855042},"title":"Visualizing Data using t-SNE"},{"paperId":"b9c4d931665ec87c16fcd44cae8fdaec1215e81e","externalIds":{"MAG":"2727840223","CorpusId":16920486},"title":"TORCS, The Open Racing Car Simulator"},{"paperId":"23729ae655799ed2baa40ad77a2d4344453754f6","externalIds":{"DBLP":"books/lib/RussellN03","CorpusId":262339885},"title":"Artificial intelligence - a modern approach, 2nd Edition"},{"paperId":"c5794b0366e6940281866ef8a84fe285a8d513e3","externalIds":{"MAG":"49038156","DBLP":"phd/hal/Coulom02","CorpusId":262936446},"title":"Reinforcement Learning Using Neural Networks, with Applications to Motor Control. (Apprentissage par renforcement utilisant des réseaux de neurones, avec des applications au contrôle moteur)"},{"paperId":"1f4731d5133cb96ab30e08bf39dffa874aebf487","externalIds":{"DBLP":"conf/mi/BainS95","MAG":"2174803659","CorpusId":10738655},"title":"A Framework for Behavioural Cloning"},{"paperId":"874b3a63422eeaf24c14435ee6091ed48247bff3","externalIds":{"MAG":"1600437712","CorpusId":60851166},"title":"Efficient memory-based learning for robot control"}]}