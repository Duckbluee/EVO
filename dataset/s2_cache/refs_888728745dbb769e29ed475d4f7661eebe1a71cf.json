{"references":[{"paperId":"1146d40d3d01427a008a20530269667b8989750c","externalIds":{"ArXiv":"2311.15296","DBLP":"conf/acl/LiangSNLXTWHPWD24","DOI":"10.18653/v1/2024.acl-long.288","CorpusId":265456112},"title":"UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation"},{"paperId":"be177300487b6d0f25e6cade9a31900454b13281","externalIds":{"DBLP":"journals/corr/abs-2310-03214","ArXiv":"2310.03214","DOI":"10.48550/arXiv.2310.03214","CorpusId":263672149},"title":"FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation"},{"paperId":"ff3a22641d21e9725efb5e79f22094300b689ab7","externalIds":{"ArXiv":"2310.02174","DBLP":"journals/corr/abs-2310-02174","DOI":"10.48550/arXiv.2310.02174","CorpusId":263608964},"title":"Ask Again, Then Fail: Large Language Models' Vacillations in Judgement"},{"paperId":"27f165d57fa2c7020c87a6d0ab694c400d7d4493","externalIds":{"ACL":"2024.lrec-main.285","ArXiv":"2309.11737","DBLP":"conf/coling/HouZC24","DOI":"10.48550/arXiv.2309.11737","CorpusId":262084258},"title":"Choice-75: A Dataset on Decision Branching in Script Learning"},{"paperId":"84a36e19f9394f22b34f79756fa9628a795e02ea","externalIds":{"DBLP":"conf/iclr/ZhengC0LZW00LXG24","ArXiv":"2309.11998","DOI":"10.48550/arXiv.2309.11998","CorpusId":262084217},"title":"LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"},{"paperId":"77b1f1c6d1658d120456b9046667cf009ceb39ce","externalIds":{"ArXiv":"2309.12284","DBLP":"journals/corr/abs-2309-12284","CorpusId":262084051},"title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"},{"paperId":"12b233752c7097ea6525622bed238ae2d2193c5a","externalIds":{"DBLP":"conf/iclr/00020LCYPJ24","ArXiv":"2309.10691","DOI":"10.48550/arXiv.2309.10691","CorpusId":262053695},"title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback"},{"paperId":"b549adab842e48b037d8aca84336296f84a9fa81","externalIds":{"DBLP":"conf/aaai/HeZHCXHZLX24","ArXiv":"2309.09150","DOI":"10.48550/arXiv.2309.09150","CorpusId":262043773},"title":"Can Large Language Models Understand Real-World Complex Instructions?"},{"paperId":"3803d1f291e162bdaa4678a2c5a2bbcf63c050f4","externalIds":{"ArXiv":"2309.07915","DBLP":"conf/iclr/ZhaoCSMA0LWHC24","DOI":"10.48550/arXiv.2309.07915","CorpusId":261823391},"title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning"},{"paperId":"9b9a4fa3ed510fc6eb1bf831979235f3d9f8b556","externalIds":{"ArXiv":"2309.07045","DBLP":"conf/acl/ZhangLWSHL0L0H24","DOI":"10.48550/arXiv.2309.07045","CorpusId":261706197},"title":"SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions"},{"paperId":"6450356c5431eccfee22268fb7183cfa138bafee","externalIds":{"DBLP":"journals/aiedu/Winter24","DOI":"10.1007/s40593-023-00372-z","CorpusId":261851724},"title":"Can ChatGPT Pass High School Exams on English Language Comprehension?"},{"paperId":"71bc0c97c20fffce796a355b16bd202987260029","externalIds":{"ArXiv":"2309.05922","DBLP":"journals/corr/abs-2309-05922","DOI":"10.48550/arXiv.2309.05922","CorpusId":261696947},"title":"A Survey of Hallucination in Large Foundation Models"},{"paperId":"d00735241af700d21762d2f3ca00d920241a15a4","externalIds":{"DBLP":"journals/corr/abs-2309-01219","ArXiv":"2309.01219","DOI":"10.1162/coli.a.16","CorpusId":261530162},"title":"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"},{"paperId":"5df24ed6fdf10d1e92885687abce7bd5e56f3f85","externalIds":{"ArXiv":"2308.08833","DBLP":"conf/naacl/WangCS0CXCJLWW024","ACL":"2024.naacl-long.343","DOI":"10.48550/arXiv.2308.08833","CorpusId":261030527},"title":"CMB: A Comprehensive Medical Benchmark in Chinese"},{"paperId":"ebbffe5db352a10fde868843b8d5787b87843f09","externalIds":{"ArXiv":"2308.03656","DBLP":"journals/corr/abs-2308-03656","DOI":"10.48550/arXiv.2308.03656","CorpusId":260682960},"title":"Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","externalIds":{"ArXiv":"2308.02490","DBLP":"journals/corr/abs-2308-02490","DOI":"10.48550/arXiv.2308.02490","CorpusId":260611572},"title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"3705919b880f4f8dc37483a704e14dd078cb9ac4","externalIds":{"ArXiv":"2308.01862","DBLP":"journals/corr/abs-2308-01862","DOI":"10.48550/arXiv.2308.01862","CorpusId":260438863},"title":"Wider and Deeper LLM Networks are Fairer LLM Evaluators"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","externalIds":{"DBLP":"journals/corr/abs-2307-16789","ArXiv":"2307.16789","DOI":"10.48550/arXiv.2307.16789","CorpusId":260334759},"title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"4309d572a37d655779f9dce6a2c98c66334132de","externalIds":{"DBLP":"journals/corr/abs-2307-16125","ArXiv":"2307.16125","DOI":"10.48550/arXiv.2307.16125","CorpusId":260334888},"title":"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"},{"paperId":"c29dbfbc17fa190b787a2662d49f08a38c8bd166","externalIds":{"DBLP":"journals/corr/abs-2307-13692","ArXiv":"2307.13692","DOI":"10.48550/arXiv.2307.13692","CorpusId":260155126},"title":"ARB: Advanced Reasoning Benchmark for Large Language Models"},{"paperId":"bc0549a5f07474c18987c219ecf367fb73a1b79c","externalIds":{"DBLP":"journals/corr/abs-2307-09705","ArXiv":"2307.09705","DOI":"10.48550/arXiv.2307.09705","CorpusId":259983087},"title":"CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility"},{"paperId":"b37b1dc72b1882858f5120f2cd6883134089a6ed","externalIds":{"ArXiv":"2307.06281","DBLP":"journals/corr/abs-2307-06281","DOI":"10.48550/arXiv.2307.06281","CorpusId":259837088},"title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"92930ed3560ea6c86d53cf52158bc793b089054d","externalIds":{"ArXiv":"2307.04657","DBLP":"conf/nips/JiLDPZB0SW023","DOI":"10.48550/arXiv.2307.04657","CorpusId":259501579},"title":"BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"},{"paperId":"a35f1315e91513ff0bec0c488fe175214fd9636c","externalIds":{"DBLP":"journals/corr/abs-2307-02046","ArXiv":"2307.02046","DOI":"10.1109/TKDE.2024.3392335","CorpusId":259342486},"title":"Recommender Systems in the Era of Large Language Models (LLMs)"},{"paperId":"f8e99be4f9a01761fab74bade2c3c18de9fc686b","externalIds":{"DBLP":"journals/corr/abs-2307-02477","ArXiv":"2307.02477","ACL":"2024.naacl-long.102","DOI":"10.48550/arXiv.2307.02477","CorpusId":259341893},"title":"Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks"},{"paperId":"7b1a2a807752a975ae9124e40394add5d3a9f984","externalIds":{"ArXiv":"2307.01135","DBLP":"journals/corr/abs-2307-01135","DOI":"10.48550/arXiv.2307.01135","CorpusId":259317110},"title":"ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience"},{"paperId":"942130a875ccfe55a4c60c27c636f693e25cb13d","externalIds":{"DBLP":"journals/corr/abs-2307-00184","ArXiv":"2307.00184","DOI":"10.48550/arXiv.2307.00184","CorpusId":259317218},"title":"Personality Traits in Large Language Models"},{"paperId":"3c996ace0fc6c54dbd9035d92d731bd9a57f8c6d","externalIds":{"DBLP":"conf/icalt/DaiLJLTGC23","DOI":"10.1109/ICALT58122.2023.00100","CorpusId":263231102},"title":"Can Large Language Models Provide Feedback to Students? A Case Study on ChatGPT"},{"paperId":"0405a2267077b1a03afbfcef285c8499daf46bab","externalIds":{"DBLP":"journals/corr/abs-2307-00112","ArXiv":"2307.00112","DOI":"10.48550/arXiv.2307.00112","CorpusId":259316954},"title":"Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education"},{"paperId":"5efec343015f9329c5cd56e2259f68f03c2ef8b5","externalIds":{"DBLP":"journals/corr/abs-2306-16636","ArXiv":"2306.16636","DOI":"10.48550/arXiv.2306.16636","CorpusId":259287423},"title":"CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?"},{"paperId":"36e99d8a60c4a8e7fdd78dd0bd5cbf8dd2da816b","externalIds":{"DBLP":"conf/ijcai/LiLGB23","ArXiv":"2306.15261","DOI":"10.24963/ijcai.2023/749","CorpusId":259261931},"title":"A Survey on Out-of-Distribution Evaluation of Neural NLP Models"},{"paperId":"5448c4305d918427e742627503b4999d0cdc6bbc","externalIds":{"DOI":"10.1038/s44159-023-00211-x","CorpusId":259713140},"title":"Baby steps in evaluating the capacities of large language models"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"dc2b36fb20490c0d540e01e74efd868d2e17faa3","externalIds":{"ArXiv":"2306.13651","DBLP":"journals/corr/abs-2306-13651","DOI":"10.48550/arXiv.2306.13651","CorpusId":259243643},"title":"Bring Your Own Data! Self-Supervised Evaluation for Large Language Models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","externalIds":{"ArXiv":"2306.13394","DBLP":"journals/corr/abs-2306-13394","DOI":"10.48550/arXiv.2306.13394","CorpusId":259243928},"title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"9d81ec931b85d6c6cf3453126670cd7a30a689e7","externalIds":{"ArXiv":"2306.11507","DBLP":"journals/corr/abs-2306-11507","DOI":"10.48550/arXiv.2306.11507","CorpusId":259202452},"title":"TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models"},{"paperId":"a6d3794c23626060781da0f1ff2bcdf7457b6c43","externalIds":{"DBLP":"conf/nips/WangCPXKZXXDSTA23","ArXiv":"2306.11698","DOI":"10.48550/arXiv.2306.11698","CorpusId":259202782},"title":"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models"},{"paperId":"20e0e0cab7a9c6d332171493cd345fd65443f1a3","externalIds":{"ArXiv":"2306.10512","DBLP":"conf/icml/Zhuang0PKZH0C25","CorpusId":259204023},"title":"Position: AI Evaluation Should Learn from How We Test Humans"},{"paperId":"a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9","externalIds":{"DBLP":"journals/corr/abs-2306-09265","ArXiv":"2306.09265","DOI":"10.1109/TPAMI.2024.3507000","CorpusId":259165040,"PubMed":"40030308"},"title":"LVLM-EHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models"},{"paperId":"bb9a44c94a89dbe00f0061d05c70a45064ff6ea6","externalIds":{"DBLP":"journals/corr/abs-2306-09212","ArXiv":"2306.09212","DOI":"10.48550/arXiv.2306.09212","CorpusId":259164635},"title":"CMMLU: Measuring massive multitask language understanding in Chinese"},{"paperId":"3e826e52754d0876611c8cf2fa7a781a701c39e6","externalIds":{"ArXiv":"2306.09296","DBLP":"journals/corr/abs-2306-09296","DOI":"10.48550/arXiv.2306.09296","CorpusId":259165244},"title":"KoLA: Carefully Benchmarking World Knowledge of Large Language Models"},{"paperId":"a65cbc88c5cb7af1103c060e48f63f95ad403b3c","externalIds":{"ArXiv":"2306.08997","DBLP":"journals/corr/abs-2306-08997","CorpusId":259165520},"title":"Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models"},{"paperId":"9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6","externalIds":{"ArXiv":"2306.08302","DBLP":"journals/corr/abs-2306-08302","DOI":"10.1109/TKDE.2024.3352100","CorpusId":259165563},"title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap"},{"paperId":"fe50667e1bea4c6f63909b90986231240818c1d6","externalIds":{"DBLP":"journals/corr/abs-2306-07799","ArXiv":"2306.07799","ACL":"2023.acl-srw.1","DOI":"10.48550/arXiv.2306.07799","CorpusId":259145296},"title":"ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer"},{"paperId":"a6a0963fcf21ed47a2616ca3980f8f4f21e6d5ad","externalIds":{"PubMedCentral":"10894689","ArXiv":"2306.07075","DBLP":"journals/corr/abs-2306-07075","DOI":"10.1098/rsta.2023.0159","CorpusId":259138525,"PubMed":"38403061"},"title":"Large language models as tax attorneys: a case study in legal capabilities emergence"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","externalIds":{"ArXiv":"2306.06687","DBLP":"conf/nips/YinWCSLLH0S0SO23","DOI":"10.48550/arXiv.2306.06687","CorpusId":259138958},"title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"d8e924cd982adef0944634044c8ed2e5c0e65c4c","externalIds":{"ArXiv":"2306.06331","DBLP":"journals/corr/abs-2306-06331","DOI":"10.48550/arXiv.2306.06331","CorpusId":259137620},"title":"Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination"},{"paperId":"eda08c6f5919f39979acf0b3bc52e903063b5ba4","externalIds":{"ArXiv":"2306.05783","DBLP":"conf/aaai/GuZYZWZJXLWHXHL24","DOI":"10.48550/arXiv.2306.05783","CorpusId":259129613},"title":"Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"87bd28003ca39c7ce5a2d7ca6a03446b2e482a4f","externalIds":{"DBLP":"conf/icer/Hellas0SKKS22","ArXiv":"2306.05715","DOI":"10.1145/3568813.3600139","CorpusId":259129577},"title":"Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests"},{"paperId":"5e096f65139e789fd3aa41de7e11bc9c04da79d5","externalIds":{"ArXiv":"2306.06264","DBLP":"conf/icmla/Pezeshkpour23","DOI":"10.1109/ICMLA58977.2023.00122","CorpusId":259138387},"title":"Measuring and Modifying Factual Knowledge in Large Language Models"},{"paperId":"ccd94602e3acecf999d0c9ba62b1a8bc02e9f696","externalIds":{"ArXiv":"2306.05087","DBLP":"journals/corr/abs-2306-05087","DOI":"10.48550/arXiv.2306.05087","CorpusId":259108266},"title":"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization"},{"paperId":"89689059d0cdcb52d7fbb6007ab953db22936a90","externalIds":{"DBLP":"conf/nips/ZhangAGCB23","ArXiv":"2306.05179","DOI":"10.48550/arXiv.2306.05179","CorpusId":259108959},"title":"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models"},{"paperId":"59cd274a64c37d5d02ff986ca88878abbadacfb1","externalIds":{"ArXiv":"2306.04926","DBLP":"journals/corr/abs-2306-04926","DOI":"10.48550/arXiv.2306.04926","CorpusId":259108337},"title":"covLLM: Large Language Models for COVID-19 Biomedical Literature"},{"paperId":"a2ce9963f1f072d578b1a1f1b995fec75e8c2247","externalIds":{"ArXiv":"2306.04528","DBLP":"conf/lamps/Zhu0ZW0WY000024","DOI":"10.1145/3689217.3690621","CorpusId":259095572},"title":"PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts"},{"paperId":"e47e63781c0e7a2c0504b9381b76b5d01b62c53d","externalIds":{"ACL":"2024.scalellm-1.4","ArXiv":"2306.04757","DBLP":"journals/corr/abs-2306-04757","DOI":"10.48550/arXiv.2306.04757","CorpusId":259108199},"title":"InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models"},{"paperId":"378a545c3a1cf6c4aada8f9ee8820c0d8008220a","externalIds":{"ArXiv":"2306.04181","DBLP":"conf/nips/BaiY0LHWYZXLZLH23","DOI":"10.48550/arXiv.2306.04181","CorpusId":259095491},"title":"Benchmarking Foundation Models with Language-Model-as-an-Examiner"},{"paperId":"1a55d16c14587edda62dc9c9ff09e0b531dd169c","externalIds":{"ArXiv":"2306.04618","DBLP":"journals/corr/abs-2306-04618","DOI":"10.48550/arXiv.2306.04618","CorpusId":259096157},"title":"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations"},{"paperId":"d962b6772dab0ce2573370e72a477665dfe5ab08","externalIds":{"ArXiv":"2306.04563","ACL":"2023.wassa-1.29","DBLP":"journals/corr/abs-2306-04563","DOI":"10.48550/arXiv.2306.04563","CorpusId":259095915},"title":"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models"},{"paperId":"ea7e6df5b48f36dd13c838fd56744aae6189ee8b","externalIds":{"ACL":"2023.bionlp-1.30","ArXiv":"2306.04504","DBLP":"journals/corr/abs-2306-04504","DOI":"10.48550/arXiv.2306.04504","CorpusId":259096053},"title":"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers"},{"paperId":"a4a7bb906df135107008e940aabfa42b43f621fd","externalIds":{"DBLP":"journals/corr/abs-2306-04610","ArXiv":"2306.04610","DOI":"10.48550/arXiv.2306.04610","CorpusId":259095527},"title":"The Two Word Test: A Semantic Benchmark for Large Language Models"},{"paperId":"db7fb022d11b4f72d9a2f514db1bba5f9f48eea8","externalIds":{"ArXiv":"2306.02549","DBLP":"journals/corr/abs-2306-02549","DOI":"10.48550/arXiv.2306.02549","CorpusId":259076294},"title":"Evaluation of AI Chatbots for Patient-Specific EHR Questions"},{"paperId":"661e64593fca437e41d4b90bcbc440ba76d988d2","externalIds":{"ArXiv":"2306.02864","DBLP":"conf/icdar/PenaMFSOPCC23","DOI":"10.1007/978-3-031-41498-5_2","CorpusId":259076261},"title":"Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs"},{"paperId":"8ea21903605f2671b1d0dc64f9a1779151d30659","externalIds":{"DBLP":"journals/corr/abs-2306-03090","ACL":"2023.bea-1.53","ArXiv":"2306.03090","DOI":"10.48550/arXiv.2306.03090","CorpusId":259075346},"title":"Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","externalIds":{"ArXiv":"2306.02408","DBLP":"journals/corr/abs-2306-02408","DOI":"10.48550/arXiv.2306.02408","CorpusId":259075484},"title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"c7dbd6c80ae941daf1de81439b0d1f992da130ab","externalIds":{"DBLP":"journals/corr/abs-2306-01499","ArXiv":"2306.01499","DOI":"10.48550/arXiv.2306.01499","CorpusId":259064252},"title":"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today"},{"paperId":"f178afd3afe6970ff9ed172e8ef5b1946d0c3ba8","externalIds":{"ArXiv":"2306.01337","CorpusId":259063798},"title":"MathChat: Converse to Tackle Challenging Math Problems with LLM Agents"},{"paperId":"88b9a3e5882e5dc6dc56d3476948d1c5be67d798","externalIds":{"ArXiv":"2306.01694","DBLP":"journals/corr/abs-2306-01694","DOI":"10.48550/arXiv.2306.01694","CorpusId":259063728},"title":"Evaluating Language Models for Mathematics through Interactions"},{"paperId":"8835de81a3b18971eb7495ca32f71c10d7b8e367","externalIds":{"DBLP":"journals/corr/abs-2306-01248","ArXiv":"2306.01248","DOI":"10.48550/arXiv.2306.01248","CorpusId":259064225},"title":"How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?"},{"paperId":"38d64919ba526868a850a0e5f6239d4c474b7e7e","externalIds":{"DBLP":"journals/corr/abs-2305-17926","ArXiv":"2305.17926","DOI":"10.48550/arXiv.2305.17926","CorpusId":258960339},"title":"Large Language Models are not Fair Evaluators"},{"paperId":"d3060876d9ad4e4e50e1c88a8c04186df00f24e2","externalIds":{"DBLP":"conf/acl/LaskarBRBJH23","ArXiv":"2305.18486","DOI":"10.48550/arXiv.2305.18486","CorpusId":258967462},"title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets"},{"paperId":"75e69720189a4e4cf252827e8c8e364bf18c0cad","externalIds":{"DOI":"10.1038/s41431-023-01396-8","CorpusId":258959776,"PubMed":"37246194"},"title":"Analysis of large-language model versus human performance for genetics questions"},{"paperId":"20d7965c0b282a0cd7f990e435d0f6bc9535bbc6","externalIds":{"ArXiv":"2305.18365","DBLP":"conf/nips/GuoGNLGCW023","CorpusId":258967365},"title":"What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks"},{"paperId":"49f8a40aea0e945bd8b15019a3e4b1bb1c9279ea","externalIds":{"DBLP":"journals/corr/abs-2305-16837","ArXiv":"2305.16837","DOI":"10.48550/arXiv.2305.16837","CorpusId":258947110},"title":"ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks"},{"paperId":"ea75117f34b168a20f2a4309ac2eb685ca6b1436","externalIds":{"ArXiv":"2305.17306","DBLP":"journals/corr/abs-2305-17306","DOI":"10.48550/arXiv.2305.17306","CorpusId":258959433},"title":"Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","externalIds":{"ArXiv":"2305.16934","DBLP":"conf/nips/ZhaoPDYLCL23","DOI":"10.48550/arXiv.2305.16934","CorpusId":258947177},"title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"ce913026f693101e54d3ab9152e107034d81fce1","externalIds":{"DBLP":"journals/tmlr/LiangBLTSYZNWKN23","DOI":"10.1111/nyas.15007","CorpusId":253553585,"PubMed":"37230490"},"title":"Holistic Evaluation of Language Models"},{"paperId":"d61ca71f64133f12d0534f1be5d2b7a0af3d2803","externalIds":{"ArXiv":"2305.16151","DBLP":"journals/corr/abs-2305-16151","DOI":"10.48550/arXiv.2305.16151","CorpusId":258887822},"title":"Understanding the Capabilities of Large Language Models for Automated Planning"},{"paperId":"dedfe929d182cc3537a9ed765d589b4735ce062a","externalIds":{"DBLP":"conf/nips/ValmeekamMSK23","ArXiv":"2305.15771","DOI":"10.48550/arXiv.2305.15771","CorpusId":260440590},"title":"On the Planning Abilities of Large Language Models - A Critical Investigation"},{"paperId":"c589ddc6c6fb07189af7c1212f6eb15c5ff72cde","externalIds":{"DBLP":"journals/corr/abs-2305-15005","ArXiv":"2305.15005","DOI":"10.48550/arXiv.2305.15005","CorpusId":258866189},"title":"Sentiment Analysis in the Era of Large Language Models: A Reality Check"},{"paperId":"2cf1f6c723006f258599fd9f000bb616ae83387a","externalIds":{"DBLP":"conf/emnlp/AroraSM23","ArXiv":"2305.15074","DOI":"10.48550/arXiv.2305.15074","CorpusId":258866000},"title":"Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models"},{"paperId":"cf3522700d89af9dabfbad44c509a0fed2bde517","externalIds":{"PubMedCentral":"10244637","DBLP":"journals/frai/OrruPCG23","DOI":"10.3389/frai.2023.1199350","CorpusId":258846153,"PubMed":"37293238"},"title":"Human-like problem-solving abilities in large language models using ChatGPT"},{"paperId":"d723193c0453223803ffb926354cd6d2dee32b06","externalIds":{"ArXiv":"2305.15268","DBLP":"journals/corr/abs-2305-15268","DOI":"10.48550/arXiv.2305.15268","CorpusId":258866165},"title":"EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models"},{"paperId":"c1592c211f8b7791a55afd7162249c723b87c237","externalIds":{"DBLP":"journals/corr/abs-2305-14938","ArXiv":"2305.14938","DOI":"10.18653/v1/2023.emnlp-main.699","CorpusId":258865939},"title":"Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark"},{"paperId":"fb0cd3f58d405b2d1511d6f6066affeb7d5a4902","externalIds":{"DBLP":"journals/corr/abs-2305-14693","ArXiv":"2305.14693","DOI":"10.48550/arXiv.2305.14693","CorpusId":258866061},"title":"Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs"},{"paperId":"c58325547156a70cb27c148e5b57738ca9ce79aa","externalIds":{"DBLP":"conf/nips/SaparovPPJKK023","ArXiv":"2305.15269","DOI":"10.48550/arXiv.2305.15269","CorpusId":258865898},"title":"Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples"},{"paperId":"ab4ce5dda7ad4d9032995c9c049a89d65723c6aa","externalIds":{"ArXiv":"2305.14975","DBLP":"conf/emnlp/TianMZSRYFM23","DOI":"10.48550/arXiv.2305.14975","CorpusId":258865733},"title":"Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback"},{"paperId":"bd5deadc58ee45b5e004378ba1d54a96bc947b4a","externalIds":{"ArXiv":"2305.14251","DBLP":"conf/emnlp/MinKLLYKIZH23","DOI":"10.48550/arXiv.2305.14251","CorpusId":258841470},"title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"},{"paperId":"4f480bae3196dbbc27ab383bce33478ea963f9b3","externalIds":{"ArXiv":"2305.13711","DBLP":"journals/corr/abs-2305-13711","ACL":"2023.nlp4convai-1.5","DOI":"10.48550/arXiv.2305.13711","CorpusId":258841681},"title":"LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models"},{"paperId":"cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa","externalIds":{"DBLP":"journals/corr/abs-2305-14387","ArXiv":"2305.14387","DOI":"10.48550/arXiv.2305.14387","CorpusId":258865545},"title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"},{"paperId":"c18f2239a4bd8cc68db9a013416167357f5e1353","externalIds":{"ArXiv":"2305.12474","DBLP":"journals/corr/abs-2305-12474","DOI":"10.48550/arXiv.2305.12474","CorpusId":258833562},"title":"Evaluating the Performance of Large Language Models on GAOKAO Benchmark"},{"paperId":"c4affe78f824f8dc604201ad40cd00c381297d7c","externalIds":{"ArXiv":"2305.11700","DBLP":"journals/corr/abs-2305-11700","DOI":"10.1145/3746252.3761429","CorpusId":258822859},"title":"Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights"},{"paperId":"d0c69c309fbf1233b6351cd57484557c16f28427","externalIds":{"DBLP":"conf/emnlp/0003WMD0LXW23","ArXiv":"2305.11792","DOI":"10.18653/v1/2023.findings-emnlp.806","CorpusId":264146343},"title":"Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs"},{"paperId":"d2d16333a4b0dc7e3463b280b9945e5ee6c53396","externalIds":{"DBLP":"conf/emnlp/GekhmanHAES23","ArXiv":"2305.11171","DOI":"10.48550/arXiv.2305.11171","CorpusId":258762340},"title":"TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models"},{"paperId":"839cc546b58968e2a8cb968337fb2e3a279e2b00","externalIds":{"DBLP":"journals/corr/abs-2305-11262","ACL":"2023.acl-long.757","ArXiv":"2305.11262","DOI":"10.48550/arXiv.2305.11262","CorpusId":258823380},"title":"CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models"},{"paperId":"25f729d7773614846b412db3c6c2a3aab41ec409","externalIds":{"ArXiv":"2305.10263","DBLP":"journals/corr/abs-2305-10263","DOI":"10.48550/arXiv.2305.10263","CorpusId":258740849},"title":"M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models"},{"paperId":"206400aba5f12f734cdd2e4ab48ef6014ea60773","externalIds":{"DBLP":"journals/corr/abs-2305-10355","ArXiv":"2305.10355","DOI":"10.48550/arXiv.2305.10355","CorpusId":258740697},"title":"Evaluating Object Hallucination in Large Vision-Language Models"},{"paperId":"e0f27336698c84709bd60b6b7f4ce588cbae66bf","externalIds":{"DBLP":"journals/corr/abs-2305-09645","ArXiv":"2305.09645","DOI":"10.48550/arXiv.2305.09645","CorpusId":258714753},"title":"StructGPT: A General Framework for Large Language Model to Reason over Structured Data"},{"paperId":"236c7dafea3df7ecffb5f18ec780d12f2f27d4b0","externalIds":{"DBLP":"conf/nips/HuangBZZZSLLZLF23","ArXiv":"2305.08322","DOI":"10.48550/arXiv.2305.08322","CorpusId":258685666},"title":"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"},{"paperId":"2e92b3699668f920a8d692535622ebeaa53315e2","externalIds":{"ArXiv":"2305.07609","DBLP":"conf/recsys/ZhangBZWF023","DOI":"10.1145/3604915.3608860","CorpusId":258676079},"title":"Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation"},{"paperId":"450b5490cc653478c272be50aa986798df828a20","externalIds":{"DBLP":"journals/corr/abs-2305-02182","ArXiv":"2305.02182","DOI":"10.1145/3604915.3610646","CorpusId":258461170},"title":"Uncovering ChatGPT’s Capabilities in Recommender Systems"},{"paperId":"ffabde35e2437db93871801e7d733f411911813f","externalIds":{"ArXiv":"2305.01181","DBLP":"conf/coling/LyuD0DWLAWW24","ACL":"2024.lrec-main.120","CorpusId":258436851},"title":"A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models"},{"paperId":"b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a","externalIds":{"ArXiv":"2305.01210","DBLP":"journals/corr/abs-2305-01210","CorpusId":258437095},"title":"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation"},{"paperId":"e9c0379226083298d4f24c2e0b4585b4ee93ad94","externalIds":{"DOI":"10.1016/j.fertnstert.2023.05.151","CorpusId":258825860,"PubMed":"37217092"},"title":"The promise and peril of using a large language model to obtain clinical information: ChatGPT performs strongly as a fertility counseling tool with limitations."},{"paperId":"bac7a2da1080cb2df79cc27df21b01958faa91b8","externalIds":{"PubMedCentral":"10172028","DOI":"10.4174/astr.2023.104.5.269","CorpusId":258518128,"PubMed":"37179699"},"title":"ChatGPT goes to the operating room: evaluating GPT-4 performance and its potential in surgical education and training in the era of large language models"},{"paperId":"990c299b9b99bcf0504f8235401d75368e9792c7","externalIds":{"PubMedCentral":"10234918","DOI":"10.1007/s11695-023-06603-5","CorpusId":258376846,"PubMed":"37106269"},"title":"Assessing the Accuracy of Responses by the Language Model ChatGPT to Questions Regarding Bariatric Surgery"},{"paperId":"459c82205d2a27a8542bba7a4d478a8a23be2f5d","externalIds":{"ArXiv":"2304.09542","DBLP":"conf/emnlp/0001YMWRCYR23","DOI":"10.48550/arXiv.2304.09542","CorpusId":258212638},"title":"Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent"},{"paperId":"428057300776bc5a6ed215b351b8d517d484ef1e","externalIds":{"PubMedCentral":"10375390","DOI":"10.2196/48305","CorpusId":259842568,"PubMed":"37440293"},"title":"Variability in Large Language Models’ Responses to Medical Licensing and Certification Examinations. Comment on “How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment”"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","externalIds":{"DBLP":"journals/corr/abs-2304-08354","ArXiv":"2304.08354","DOI":"10.1145/3704435","CorpusId":258179336},"title":"Tool Learning with Foundation Models"},{"paperId":"6e4635f8632f34e934a055264bd3b1ce44595f9b","externalIds":{"DBLP":"journals/corr/abs-2304-07849","ArXiv":"2304.07849","DOI":"10.48550/arXiv.2304.07849","CorpusId":258179346},"title":"ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human"},{"paperId":"d26c55bee1ac6856a20862b0f7b4ff38fa39af50","externalIds":{"DBLP":"journals/corr/abs-2304-07619","ArXiv":"2304.07619","DOI":"10.2139/ssrn.4412788","CorpusId":258071542},"title":"Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models"},{"paperId":"17606dbe67df42d973015fdd35f2807b0cafc15b","externalIds":{"ArXiv":"2304.07333","DBLP":"journals/corr/abs-2304-07333","DOI":"10.48550/arXiv.2304.07333","CorpusId":258180220},"title":"The Self-Perception and Political Biases of ChatGPT"},{"paperId":"68c834c19cd126bbd6d25a3572d7205cfed76271","externalIds":{"DBLP":"journals/corr/abs-2304-06364","ArXiv":"2304.06364","DOI":"10.48550/arXiv.2304.06364","CorpusId":258108259},"title":"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"},{"paperId":"93752cae0d4ecd2d09d6660feb3c1860af973f18","externalIds":{"DBLP":"journals/coling/ZiemsHSCZY24","ACL":"2024.cl-1.8","ArXiv":"2305.03514","DOI":"10.1162/coli_a_00502","CorpusId":258547324},"title":"Can Large Language Models Transform Computational Social Science?"},{"paperId":"dfbfa21a93c3164ae8a033398c8de42b03b1b84d","externalIds":{"DBLP":"journals/corr/abs-2304-05613","ArXiv":"2304.05613","DOI":"10.48550/arXiv.2304.05613","CorpusId":258079179},"title":"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning"},{"paperId":"281a7a99c16ce8f53bfbfb7aeb460dbd28648d28","externalIds":{"DBLP":"conf/emnlp/DeshpandeMRKN23","ArXiv":"2304.05335","DOI":"10.48550/arXiv.2304.05335","CorpusId":258060002},"title":"Toxicity in ChatGPT: Analyzing Persona-assigned Language Models"},{"paperId":"1aeb3239735e28c7318af096044e48d919ea500b","externalIds":{"DBLP":"journals/corr/abs-2304-04339","ArXiv":"2304.04339","DOI":"10.48550/arXiv.2304.04339","CorpusId":258048703},"title":"Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study"},{"paperId":"85cc48276c69924d3e92ddb38facb7d92be9a4a6","externalIds":{"DBLP":"journals/corr/abs-2304-03439","ArXiv":"2304.03439","DOI":"10.48550/arXiv.2304.03439","CorpusId":258041354},"title":"Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4"},{"paperId":"16d83e930a4dab2d49f5d276838ddce79df3f787","externalIds":{"DBLP":"journals/firstmonday/Ferrara23a","ArXiv":"2304.03738","DOI":"10.5210/fm.v28i11.13346","CorpusId":258041203},"title":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models"},{"paperId":"68850153b0210615c86f9a72624f34e2913bcddf","externalIds":{"DBLP":"journals/corr/abs-2304-02210","ArXiv":"2304.02210","DOI":"10.18653/v1/2023.emnlp-main.1036","CorpusId":257952312},"title":"Document-Level Machine Translation with Large Language Models"},{"paperId":"fdaacabb69ca054d1d9acf2f1409c083672adc4a","externalIds":{"DBLP":"journals/ijcv/WangYWHCYXXZ24","ArXiv":"2304.01457","DOI":"10.1007/s11263-023-01868-w","CorpusId":257921626},"title":"Exploring Vision-Language Models for Imbalanced Learning"},{"paperId":"8b7b0791bed2853fdaef2184b79e53dd55c26708","externalIds":{"ArXiv":"2304.00228","DBLP":"conf/websci/YangM25","DOI":"10.1145/3717867.3717903","CorpusId":257913006},"title":"Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models"},{"paperId":"a98862ffe4c18634a67a3df8a965a35e5e0d7ec8","externalIds":{"DOI":"10.1016/j.lindif.2023.102274","CorpusId":257445349},"title":"ChatGPT for good? On opportunities and challenges of large language models for education"},{"paperId":"9ec42d155e2014e86ab49adcf76fd40a41a867ea","externalIds":{"ArXiv":"2304.01938","PubMedCentral":"10388568","DBLP":"journals/corr/abs-2304-01938","DOI":"10.3389/fonc.2023.1219326","CorpusId":257921233,"PubMed":"37529688"},"title":"Evaluating large language models on a highly-specialized topic, radiation oncology physics"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"ca94c924d8a3b77a2bd5b16ffc03b8723bce9c1f","externalIds":{"DBLP":"journals/corr/abs-2303-17466","ACL":"2023.c3nlp-1.7","ArXiv":"2303.17466","DOI":"10.48550/arXiv.2303.17466","CorpusId":257833897},"title":"Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study"},{"paperId":"4d7571441f507f39133209e8afa7ad088da2199c","externalIds":{"ACL":"2024.lrec-main.276","DBLP":"conf/coling/BianH0L0HJD24","ArXiv":"2303.16421","CorpusId":257804619},"title":"ChatGPT Is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models"},{"paperId":"11628f656257e75e46447ac21cdaa86c4b340a0a","externalIds":{"DBLP":"journals/corr/abs-2303-13835","ArXiv":"2303.13835","DOI":"10.1145/3539618.3591932","CorpusId":257756994},"title":"Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited"},{"paperId":"8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c","externalIds":{"ArXiv":"2303.12712","DBLP":"journals/corr/abs-2303-12712","CorpusId":257663729},"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"62ad7ea9467bbcdbfe325b9ee561cab3908e4583","externalIds":{"ArXiv":"2303.12528","DBLP":"conf/emnlp/AhujaDHORJNGSAB23","DOI":"10.18653/v1/2023.emnlp-main.258","CorpusId":257663467},"title":"MEGA: Multilingual Evaluation of Generative AI"},{"paperId":"99bd07e888476904c6dd77ca154fd48629ac6dce","externalIds":{"ArXiv":"2304.02015","DBLP":"journals/corr/abs-2304-02015","DOI":"10.48550/arXiv.2304.02015","CorpusId":257952500},"title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"7c1707db9aafd209aa93db3251e7ebd593d55876","externalIds":{"DBLP":"conf/emnlp/ManakulLG23","ArXiv":"2303.08896","DOI":"10.48550/arXiv.2303.08896","CorpusId":257557820},"title":"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models"},{"paperId":"23d01461a54505705649c1f5378f81dcd524d46c","externalIds":{"DBLP":"journals/jcisd/NascimentoP23","DOI":"10.1021/acs.jcim.3c00285","CorpusId":257581728,"PubMed":"36926868"},"title":"Do Large Language Models Understand Chemistry? A Conversation with ChatGPT"},{"paperId":"6c9263117c9276b4d373f32491dff0f92564f9e2","externalIds":{"PubMedCentral":"10192466","DBLP":"journals/vciba/0003TZPNM0W23","ArXiv":"2303.09038","DOI":"10.1186/s42492-023-00136-5","CorpusId":258745593,"PubMed":"37198498"},"title":"Translating radiology reports into plain language using ChatGPT and GPT-4 with prompt learning: results, limitations, and potential"},{"paperId":"aa2fa431ce1d5a8d56d138e3330d3df381d36e3a","externalIds":{"DBLP":"journals/corr/abs-2303-07142","ArXiv":"2303.07142","DOI":"10.48550/arXiv.2303.07142","CorpusId":257496827},"title":"Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification"},{"paperId":"acc7a277c6a9cb05b8bf4c3c25c7369aa9a42555","externalIds":{"PubMedCentral":"10011374","DOI":"10.1038/s41598-023-31412-2","CorpusId":257485880,"PubMed":"36914821"},"title":"Evaluating the use of large language model in identifying top research questions in gastroenterology"},{"paperId":"8221f1597000543432b7021ca79dbc51a7a63f9c","externalIds":{"DBLP":"journals/corr/abs-2303-04048","ArXiv":"2303.04048","ACL":"2023.newsum-1.1","DOI":"10.48550/arXiv.2303.04048","CorpusId":257378627},"title":"Is ChatGPT a Good NLG Evaluator? A Preliminary Study"},{"paperId":"6c7ba2af4b3e472bd8a5717367b88dcdd4abbd31","externalIds":{"DBLP":"journals/jms/CascellaMBB23","PubMedCentral":"9985086","DOI":"10.1007/s10916-023-01925-4","CorpusId":257312905,"PubMed":"36869927"},"title":"Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios"},{"paperId":"67655143fdd9dafe018870e1915149111c8c81b6","externalIds":{"PubMedCentral":"10002821","DOI":"10.21203/rs.3.rs-2566942/v1","CorpusId":257437276,"PubMed":"36909565"},"title":"Assessing the Accuracy and Reliability of AI-Generated Medical Responses: An Evaluation of the Chat-GPT Model"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","externalIds":{"ArXiv":"2302.14045","DBLP":"conf/nips/Huang0WHSML0MPL23","DOI":"10.48550/arXiv.2302.14045","CorpusId":257219775},"title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"aad8b1ca56aef4a7512789102ce0cc3fc8b064e4","externalIds":{"DBLP":"conf/eacl/MargatinaWVJBB23","ArXiv":"2302.12297","ACL":"2023.eacl-main.211","DOI":"10.48550/arXiv.2302.12297","CorpusId":257206033},"title":"Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views"},{"paperId":"5c7353fac22a8fdc43fc2f5c006b5d6902c47e75","externalIds":{"DBLP":"journals/debu/0001HH0ZWY0HGJ024","ArXiv":"2302.12095","DOI":"10.48550/arXiv.2302.12095","CorpusId":257102461},"title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective"},{"paperId":"08b85bce712168998004ee80ce4e475390413c74","externalIds":{"ArXiv":"2302.11382","DBLP":"journals/corr/abs-2302-11382","CorpusId":257079092},"title":"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT"},{"paperId":"21647053c34d191d910668c2f18140346bf0f255","externalIds":{"PubMedCentral":"10163403","DOI":"10.2196/46599","CorpusId":258096420,"PubMed":"37083633"},"title":"Trialling a Large Language Model (ChatGPT) in General Practice With the Applied Knowledge Test: Observational Study Demonstrating Opportunities and Limitations in Primary Care"},{"paperId":"85996f9fc312777f487dd51bf9e96bb3704c2fb7","externalIds":{"DBLP":"journals/corr/abs-2302-06706","ArXiv":"2302.06706","DOI":"10.48550/arXiv.2302.06706","CorpusId":256846992},"title":"On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)"},{"paperId":"09239dac5b1cded9414c946333eaf619dca9aaa7","externalIds":{"DBLP":"journals/corr/abs-2303-02155","ArXiv":"2303.02155","DOI":"10.1145/3583131.3590351","CorpusId":257364995},"title":"ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","externalIds":{"DBLP":"journals/corr/abs-2302-04761","ArXiv":"2302.04761","DOI":"10.48550/arXiv.2302.04761","CorpusId":256697342},"title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"bf8491bef353df126e2306ad2fe4b898697b906a","externalIds":{"ArXiv":"2302.04023","DBLP":"conf/ijcnlp/BangCLDSWLJYCDXF23","ACL":"2023.ijcnlp-main.45","DOI":"10.18653/v1/2023.ijcnlp-main.45","CorpusId":256662612},"title":"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity"},{"paperId":"cd4d112f3f9120d0715f22a9de2ce4720822368c","externalIds":{"PubMedCentral":"9947764","DOI":"10.2196/45312","CorpusId":256663603,"PubMed":"36753318"},"title":"How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment"},{"paperId":"873a581320d928249609d3c07229d5af182a379c","externalIds":{"DBLP":"conf/emnlp/QinZ0CYY23","ArXiv":"2302.06476","DOI":"10.18653/v1/2023.emnlp-main.85","CorpusId":256827430},"title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"},{"paperId":"3f2cb353c7528efafb847309ab1e1e95245740a4","externalIds":{"DBLP":"journals/corr/abs-2301-13867","ArXiv":"2301.13867","CorpusId":256415984},"title":"Mathematical Capabilities of ChatGPT"},{"paperId":"1c91b23d78944f7f237cb512029c2165972ae9d5","externalIds":{"ACL":"2023.eacl-main.77","DBLP":"journals/corr/abs-2301-12868","ArXiv":"2301.12868","DOI":"10.48550/arXiv.2301.12868","CorpusId":256389762},"title":"On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex"},{"paperId":"b03d3df5f7d641669b74881703cfb67874838ec3","externalIds":{"DBLP":"journals/corr/abs-2301-12307","ACL":"2023.ijcnlp-main.4","ArXiv":"2301.12307","DOI":"10.48550/arXiv.2301.12307","CorpusId":256389726},"title":"MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization"},{"paperId":"680c72c29b518398d9c45b5995a160583ea8e090","externalIds":{"DOI":"10.1101/2023.01.27.23285115","CorpusId":256325373,"PubMed":"36789422"},"title":"Analysis of large-language model versus human performance for genetics questions"},{"paperId":"edc9bf11c4810a77f00ccb96130ff67ee578391e","externalIds":{"DBLP":"journals/corr/abs-2301-11596","PubMedCentral":"10409727","ArXiv":"2301.11596","DOI":"10.1038/s41597-023-02433-3","CorpusId":256358591,"PubMed":"37553439"},"title":"ThoughtSource: A central hub for large language model reasoning data"},{"paperId":"b6f8cffc5da51581aec71d919d010d55e5ac068a","externalIds":{"DBLP":"journals/corr/abs-2301-01768","ArXiv":"2301.01768","DOI":"10.48550/arXiv.2301.01768","CorpusId":255440573},"title":"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation"},{"paperId":"bff499d51b002fd0b1aa05ba151a4a515e5bf36f","externalIds":{"DBLP":"journals/corr/abs-2307-09042","ArXiv":"2307.09042","DOI":"10.1177/18344909231213958","CorpusId":259951557},"title":"Emotional intelligence of Large Language Models"},{"paperId":"6052486bc9144dc1730c12bf35323af3792a1fd0","externalIds":{"ArXiv":"2212.13138","DBLP":"journals/corr/abs-2212-13138","PubMedCentral":"10396962","DOI":"10.1038/s41586-023-06291-2","CorpusId":255124952,"PubMed":"37438534"},"title":"Large language models encode clinical knowledge"},{"paperId":"cf1f26e7cbed3958b3c2870656568c299fece6e3","externalIds":{"PubMedCentral":"9931230","DOI":"10.1371/journal.pdig.0000198","CorpusId":254876189,"PubMed":"36812645"},"title":"Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models"},{"paperId":"841f5c091ed8491d9fd50cf124de7c67d500bdb8","externalIds":{"ArXiv":"2212.02774","DBLP":"journals/corr/abs-2212-02774","DOI":"10.1109/ICCV51070.2023.00370","CorpusId":254274920},"title":"Adaptive Testing of Computer Vision Models"},{"paperId":"d15091e73f7295ba8c0bdbabe0b7188307c96039","externalIds":{"DBLP":"journals/corr/abs-2211-08073","ArXiv":"2211.08073","DOI":"10.48550/arXiv.2211.08073","CorpusId":253523094},"title":"GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective"},{"paperId":"4610ffb1b016acaa82a2065ffd1a3adbae1ce722","externalIds":{"DBLP":"journals/corr/abs-2211-01910","ArXiv":"2211.01910","DOI":"10.48550/arXiv.2211.01910","CorpusId":253265328},"title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"041f5dbfcd07a3369ac44a6b902ee4b145eccf2b","externalIds":{"ArXiv":"2210.07197","ACL":"2022.emnlp-main.131","DBLP":"journals/corr/abs-2210-07197","DOI":"10.18653/v1/2022.emnlp-main.131","CorpusId":252873117},"title":"Towards a Unified Multi-Dimensional Evaluator for Text Generation"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","externalIds":{"DBLP":"journals/corr/abs-2210-02414","ArXiv":"2210.02414","DOI":"10.48550/arXiv.2210.02414","CorpusId":252715691},"title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"bf72cfa87a5c55e430abf6d2a3d9b66eb9e1a717","externalIds":{"ArXiv":"2209.12106","ACL":"2023.acl-srw.40","DBLP":"journals/corr/abs-2209-12106","DOI":"10.48550/arXiv.2209.12106","CorpusId":252531526},"title":"Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity"},{"paperId":"d697b440dd0e65a05fe027e4c0ea85f62dcba033","externalIds":{"DBLP":"journals/patterns/LievinHMW24","ArXiv":"2207.08143","PubMedCentral":"10935498","DOI":"10.1016/j.patter.2024.100943","CorpusId":250627547,"PubMed":"38487804"},"title":"Can large language models reason about medical questions?"},{"paperId":"142ebbf4760145f591166bde2564ac70c001e927","externalIds":{"ArXiv":"2207.05221","DBLP":"journals/corr/abs-2207-05221","DOI":"10.48550/arXiv.2207.05221","CorpusId":250451161},"title":"Language Models (Mostly) Know What They Know"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","externalIds":{"ArXiv":"2206.04615","DBLP":"journals/corr/abs-2206-04615","CorpusId":263625818},"title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"c28e95a06dfcf13fc65a1cac83722f53e34f12a5","externalIds":{"DBLP":"journals/corr/abs-2205-12615","ArXiv":"2205.12615","DOI":"10.48550/arXiv.2205.12615","CorpusId":249063032},"title":"Autoformalization with Large Language Models"},{"paperId":"354bf043179e3e9f05df73e3f04517e53c326d1f","externalIds":{"ArXiv":"2205.12255","DBLP":"journals/corr/abs-2205-12255","DOI":"10.48550/arXiv.2205.12255","CorpusId":249017698},"title":"TALM: Tool Augmented Language Models"},{"paperId":"1b0cf7a2e2178e244c7805dc1971de5ab0b8466a","externalIds":{"DBLP":"conf/nips/TchangoGWMG22","ArXiv":"2205.09148","CorpusId":249707831},"title":"DDXPlus: A New Dataset For Automatic Medical Diagnosis"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"1bcde55995a957b3e8a595d536b816cb8989cf1d","externalIds":{"DBLP":"journals/corr/abs-2205-00445","ArXiv":"2205.00445","DOI":"10.48550/arXiv.2205.00445","CorpusId":248496374},"title":"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"79a7b2969c9ab6e1cd67db7ca93430445aafedaf","externalIds":{"DBLP":"conf/acl/ThrushTGBRKRMWK22","ACL":"2022.acl-demo.17","ArXiv":"2204.01906","DOI":"10.48550/arXiv.2204.01906","CorpusId":247958140},"title":"Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"7b3d26bd1d65ed5937c76043b5cd058260d8469f","externalIds":{"DBLP":"conf/icml/ParisiRP022","ArXiv":"2203.03580","DOI":"10.48550/arXiv.2203.03580","CorpusId":247292805},"title":"The Unsurprising Effectiveness of Pre-Trained Vision Models for Control"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","externalIds":{"ArXiv":"2201.11990","DBLP":"journals/corr/abs-2201-11990","CorpusId":246411325},"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","externalIds":{"DBLP":"journals/corr/abs-2201-08239","ArXiv":"2201.08239","CorpusId":246063428},"title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","externalIds":{"ArXiv":"2112.00861","DBLP":"journals/corr/abs-2112-00861","CorpusId":244799619},"title":"A General Language Assistant as a Laboratory for Alignment"},{"paperId":"9b54a69217cc01e7ce0fc4b6d6407cea7e532c78","externalIds":{"DBLP":"journals/corr/abs-2111-08181","ArXiv":"2111.08181","ACL":"2022.dadc-1.8","DOI":"10.18653/v1/2022.dadc-1.8","CorpusId":244129853},"title":"Adversarially Constructed Evaluation Sets Are More Challenging, but May Not Be Fair"},{"paperId":"8436897e713c2242d6291df9a6a33c1544d4dd39","externalIds":{"DBLP":"journals/corr/abs-2111-02840","ArXiv":"2111.02840","CorpusId":242757097},"title":"Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models"},{"paperId":"7d5c661fa9a4255ee087e861f820564ea2e2bd6b","externalIds":{"DBLP":"journals/corr/abs-2110-08193","ArXiv":"2110.08193","ACL":"2022.findings-acl.165","DOI":"10.18653/v1/2022.findings-acl.165","CorpusId":239010011},"title":"BBQ: A hand-built bias benchmark for question answering"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","externalIds":{"DBLP":"journals/corr/abs-2109-07958","ACL":"2022.acl-long.229","ArXiv":"2109.07958","DOI":"10.18653/v1/2022.acl-long.229","CorpusId":237532606},"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"cb390170928a159485da44c7366366c1b5872a01","externalIds":{"DOI":"10.4324/9781003237242-5","CorpusId":239430204},"title":"Fluency"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","externalIds":{"DBLP":"conf/emnlp/0034WJH21","ACL":"2021.emnlp-main.685","ArXiv":"2109.00859","DOI":"10.18653/v1/2021.emnlp-main.685","CorpusId":237386541},"title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a52983978b68b1a278d9eafcddc94b979b345e04","externalIds":{"DBLP":"journals/iahe/JanssonHSE21","MAG":"3172010169","DOI":"10.1016/J.IHEDUC.2021.100817","CorpusId":236278033},"title":"Online question and answer sessions: How students support their own and other students' processes of inquiry in a text-based learning environment"},{"paperId":"d25bb256e5b69f769a429750217b0d9ec1cf4d86","externalIds":{"DBLP":"journals/corr/abs-2106-06052","ArXiv":"2106.06052","CorpusId":235399978},"title":"Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","externalIds":{"ArXiv":"2105.09938","DBLP":"conf/nips/HendrycksBKMAGB21","CorpusId":234790100},"title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"bfb0854d1f84124167deec177949276d801beaf6","externalIds":{"ACL":"2021.acl-long.330","DBLP":"conf/acl/ShengCNP20","ArXiv":"2105.04054","DOI":"10.18653/v1/2021.acl-long.330","CorpusId":234337004},"title":"Societal Biases in Language Generation: Progress and Challenges"},{"paperId":"807600ef43073cd9c59d4208ee710e90cf14efa8","externalIds":{"DBLP":"journals/corr/abs-2104-08663","ArXiv":"2104.08663","CorpusId":233296016},"title":"BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models"},{"paperId":"77a096d80eb4dd4ccd103d1660c5a5498f7d026b","externalIds":{"ACL":"2021.naacl-main.324","MAG":"3171654528","DBLP":"journals/corr/abs-2104-14337","ArXiv":"2104.14337","DOI":"10.18653/V1/2021.NAACL-MAIN.324","CorpusId":233444226},"title":"Dynabench: Rethinking Benchmarking in NLP"},{"paperId":"6a1b25f7a67395ad1e676027322913acbb0a0635","externalIds":{"ArXiv":"2103.06268","DBLP":"conf/nips/HendrycksBCB21","CorpusId":232170369},"title":"CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","externalIds":{"DBLP":"conf/nips/HendrycksBKABTS21","ArXiv":"2103.03874","CorpusId":232134851},"title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"93884d89dfc8c3886f642018227a43fb7b58044f","externalIds":{"DBLP":"conf/ijcai/0001LLOQ21","ArXiv":"2103.03097","DOI":"10.1109/TKDE.2022.3178128","CorpusId":232110832},"title":"Generalizing to Unseen Domains: A Survey on Domain Generalization"},{"paperId":"ce3b364b7e6358940ce97d8d5887a65e5024ca21","externalIds":{"DBLP":"conf/fat/DhamalaSKKPCG21","ArXiv":"2101.11718","DOI":"10.1145/3442188.3445924","CorpusId":231719337},"title":"BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","externalIds":{"ArXiv":"2012.15723","DBLP":"conf/acl/GaoFC20","ACL":"2021.acl-long.295","DOI":"10.18653/v1/2021.acl-long.295","CorpusId":229923710},"title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"b103e87c7727134927d3ffb06934a95c10c02fc0","externalIds":{"MAG":"3095319910","DBLP":"journals/mima/FloridiC20","DOI":"10.1007/s11023-020-09548-1","CorpusId":228954221},"title":"GPT-3: Its Nature, Scope, Limits, and Consequences"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","externalIds":{"MAG":"3088599783","ACL":"2020.findings-emnlp.301","DBLP":"journals/corr/abs-2009-11462","ArXiv":"2009.11462","DOI":"10.18653/v1/2020.findings-emnlp.301","CorpusId":221878771},"title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"65906e6027246ae9e4ecd18d6e019a24505c842e","externalIds":{"ArXiv":"2008.02275","MAG":"3047185145","DBLP":"journals/corr/abs-2008-02275","CorpusId":220968818},"title":"Aligning AI With Shared Human Values"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"33ec7eb2168e37e3007d1059aa96b9a63254b4da","externalIds":{"MAG":"3035507081","DBLP":"conf/ijcai/RibeiroWG021","ACL":"2020.acl-main.442","ArXiv":"2005.04118","DOI":"10.18653/v1/2020.acl-main.442","CorpusId":218551201},"title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"},{"paperId":"55ea32647d1a58c029e5dfa1b6b6f886542ec171","externalIds":{"DOI":"10.1002/9781119569817.ch11","CorpusId":242564576,"PubMed":"26943080"},"title":"Learners."},{"paperId":"fc168513421fc328151e0e5788b80415f54f2c14","externalIds":{"DOI":"10.1201/9780429341830-29","CorpusId":243685752},"title":"Cross validation"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","externalIds":{"MAG":"3034850762","ACL":"2020.acl-main.441","DBLP":"conf/acl/NieWDBWK20","ArXiv":"1910.14599","DOI":"10.18653/v1/2020.acl-main.441","CorpusId":207756753},"title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","externalIds":{"MAG":"2982399380","ACL":"2020.acl-main.703","DBLP":"journals/corr/abs-1910-13461","ArXiv":"1910.13461","DOI":"10.18653/v1/2020.acl-main.703","CorpusId":204960716},"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"d1aa325a5adefeba786d4c50a8ca9a8df9598f32","externalIds":{"DBLP":"conf/inlg/LeeGMWK19","MAG":"2992347006","ACL":"W19-8643","DOI":"10.18653/v1/W19-8643","CorpusId":209333890},"title":"Best practices for the human evaluation of automatically generated text"},{"paperId":"7a15950dc71079285a4eaf195de5aadd87c41b40","externalIds":{"MAG":"2973379954","DBLP":"journals/corr/abs-1909-08593","ArXiv":"1909.08593","CorpusId":202660943},"title":"Fine-Tuning Language Models from Human Preferences"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","externalIds":{"ACL":"Q19-1026","MAG":"2912924812","DBLP":"journals/tacl/KwiatkowskiPRCP19","DOI":"10.1162/tacl_a_00276","CorpusId":86611921},"title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","externalIds":{"MAG":"2943552823","ArXiv":"1905.00537","DBLP":"conf/nips/WangPNSMHLB19","CorpusId":143424870},"title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"d2ca23d17ace237c2cc6c4f0df8fdd7e0ed4c439","externalIds":{"MAG":"2753704268","ArXiv":"1708.08559","DBLP":"conf/icse/TianPJR18","DOI":"10.1145/3180155.3180220","CorpusId":4055261},"title":"DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars"},{"paperId":"0d441ab58a1027cb64084ad065cfea5e15b8e74c","externalIds":{"ArXiv":"1707.06875","DBLP":"journals/corr/NovikovaDCR17","MAG":"2963672599","ACL":"D17-1238","DOI":"10.18653/v1/D17-1238","CorpusId":1929239},"title":"Why We Need New Evaluation Metrics for NLG"},{"paperId":"d65ce2b8300541414bfe51d03906fca72e93523c","externalIds":{"MAG":"2950953798","ArXiv":"1706.04599","DBLP":"journals/corr/GuoPSW17","CorpusId":28671436},"title":"On Calibration of Modern Neural Networks"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","externalIds":{"MAG":"2612431505","ArXiv":"1705.03551","ACL":"P17-1147","DBLP":"journals/corr/JoshiCWZ17","DOI":"10.18653/v1/P17-1147","CorpusId":26501419},"title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"2ed7cc027367295b1a7d7cd49406acfa5c580138","externalIds":{"MAG":"2963613748","ArXiv":"1705.08500","DBLP":"journals/corr/GeifmanE17","CorpusId":491127},"title":"Selective Classification for Deep Neural Networks"},{"paperId":"d42b11ce90c9c69a20ed015b73dc33e0e4100a7b","externalIds":{"DBLP":"journals/corr/HardtPS16","ArXiv":"1610.02413","MAG":"2530395818","CorpusId":7567061},"title":"Equality of Opportunity in Supervised Learning"},{"paperId":"79b0da2080e9a8bab8ff1ec1364fa58d253a6676","externalIds":{"DOI":"10.1007/BF02824813","CorpusId":206566667},"title":"GENeration"},{"paperId":"4732a77c3753c0a5f31c62ce2e38407d6a945ebf","externalIds":{"DBLP":"journals/pr/Wong15","MAG":"1996020380","DOI":"10.1016/j.patcog.2015.03.009","CorpusId":5319234},"title":"Performance evaluation of classification algorithms by k-fold and leave-one-out cross validation"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"37c3303d173c055592ef923235837e1cbc6bd986","externalIds":{"MAG":"2162670686","DBLP":"conf/icml/ZemelWSPD13","CorpusId":490669},"title":"Learning Fair Representations"},{"paperId":"a81893fbdbaf2cead51f6886408c9c792e00d362","externalIds":{"MAG":"2234474721","DOI":"10.1016/B978-0-12-407236-7.00002-4","CorpusId":2570757},"title":"Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism"},{"paperId":"acdad79bab2eccb4b25227c9f8fc10c9bd2d21e3","externalIds":{"MAG":"2099905795","DOI":"10.1177/0894318412457070","CorpusId":44549431,"PubMed":"23087340"},"title":"Transparency"},{"paperId":"cab104cbea93bb3c0dfac8beacf0ba70d179a554","externalIds":{"DBLP":"journals/sac/Fushiki11","MAG":"2085281262","DOI":"10.1007/s11222-009-9153-8","CorpusId":21933061},"title":"Estimation of prediction error by using K-fold cross-validation"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"09c435e3ab1e1c114058ccc5ed96a31b17226c94","externalIds":{"MAG":"2215880389","DOI":"10.1021/cen-09210-ad07","CorpusId":203958841},"title":"Performance"},{"paperId":"7779da70a70d920a191162877233cdc7d274e721","externalIds":{"DBLP":"conf/selmas/SchelfthoutHB05a","MAG":"2018009444","DOI":"10.1145/1082983.1082973","CorpusId":12323373},"title":"Views"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"9dc1748099dd4321d42fb84bc7ee1f71e7814459","externalIds":{"DBLP":"journals/talip/GaoL04","MAG":"2073147434","DOI":"10.1145/1034780.1034781","CorpusId":322920},"title":"Introduction to the special issue on statistical language modeling"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"4fa8b59c1d794468c44528c5f13f273f2687aa17","externalIds":{"DOI":"10.3917/psca.013.0007","CorpusId":272822419},"title":"Richard"},{"paperId":"c894d05ebaf8eff1dce2a23b54819a92ef160d84","externalIds":{"MAG":"2152594618","DOI":"10.1037/1082-989X.2.4.329","CorpusId":45639063},"title":"Validity problems comparing values across cultures and possible solutions."},{"paperId":"52b7bf3ba59b31f362aa07f957f1543a29a4279e","externalIds":{"MAG":"2119821739","DOI":"10.1023/A:1022627411411","CorpusId":52874011},"title":"Support-Vector Networks"},{"paperId":"8c70a0a39a686bf80b76cb1b77f9eef156f6432d","externalIds":{"DBLP":"conf/ijcai/Kohavi95","MAG":"1680392829","CorpusId":2702042},"title":"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection"},{"paperId":"3de5d40b60742e3dfa86b19e7f660962298492af","externalIds":{"MAG":"2121227244","ACL":"J92-4003","DBLP":"journals/coling/BrownPdLM92","CorpusId":10986188},"title":"Class-Based n-gram Models of Natural Language"},{"paperId":"76cb5a926a789e1f28035fcc7fa01d5900df6d0e","externalIds":{"DBLP":"journals/tnn/Gallant90","MAG":"2129727551","DOI":"10.1109/72.80230","CorpusId":31763947,"PubMed":"18282835"},"title":"Perceptron-based learning algorithms"},{"paperId":"a75f8705c9443996e44f751ec9be8ebcda4aa39a","externalIds":{"MAG":"212582735","CorpusId":141281067,"PubMed":"5780638"},"title":"What is intelligence?"},{"paperId":"2d5673caa9e6af3a7b82a43f19ee920992db07ad","externalIds":{"DBLP":"journals/x/Turing50","MAG":"2145482038","DOI":"10.1093/MIND/LIX.236.433","CorpusId":14636783},"title":"Computing Machinery and Intelligence"},{"paperId":"323a9ef2c01e62df48239bb6c7bc85bdc5e3fbde","externalIds":{"DOI":"10.4169/math.mag.89.1.60","CorpusId":218541090},"title":"Solutions"},{"paperId":"c7f0c31bd260ccafd6995350f30707b3cf03ce9e","externalIds":{"DBLP":"journals/corr/abs-2306-01590","DOI":"10.48550/arXiv.2306.01590","CorpusId":263887216},"title":"An Evaluation of Log Parsing with ChatGPT"},{"paperId":"70916fbeb446ab7dc811ab74b193365d789bf1eb","externalIds":{"DBLP":"journals/corr/abs-2305-11792","DOI":"10.48550/arXiv.2305.11792","CorpusId":258822931},"title":"Chain-of-thought prompting for responding to in-depth dialogue questions with LLM"},{"paperId":"6298fb744cb31c0ca3db96833edd57a0c974424b","externalIds":{"DBLP":"journals/corr/abs-2304-08244","DOI":"10.48550/arXiv.2304.08244","CorpusId":276344352},"title":"API-Bank: A Benchmark for Tool-Augmented LLMs"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","externalIds":{"ArXiv":"2303.17580","DBLP":"journals/corr/abs-2303-17580","DOI":"10.48550/arXiv.2303.17580","CorpusId":257833781},"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"df239785e6d26a45e9c8e06551cfecba92d1ecad","externalIds":{"DBLP":"journals/corr/abs-2301-12867","DOI":"10.48550/arXiv.2301.12867","CorpusId":256390238},"title":"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis"},{"paperId":"b78aa1fbe406020f42c193b2fad8d635123b9d2e","externalIds":{"DBLP":"journals/corr/abs-2305-12421","DOI":"10.48550/arXiv.2305.12421","CorpusId":258833523},"title":"Evaluating Open Question Answering Evaluation"},{"paperId":"73f31601114e037f71b9645c5932856484be3dbb","externalIds":{"DBLP":"journals/corr/abs-2306-04308","DOI":"10.48550/arXiv.2306.04308","CorpusId":259095544},"title":"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results"},{"paperId":"7fc09c81b43e25834a763b421c1a8b3868ecd6ab","externalIds":{"DBLP":"journals/corr/abs-2304-00723","CorpusId":257913780},"title":"Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study"},{"paperId":"58ba42ffc34dd24d6a77fe58c8973b3533c369cd","externalIds":{"DBLP":"journals/corr/abs-2306-09841","DOI":"10.48550/arXiv.2306.09841","CorpusId":259188006},"title":"Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views"},{"paperId":"643f3e79ca5a7effee96973a21af50a9dddeaf10","externalIds":{"DBLP":"journals/corr/abs-2305-18365","DOI":"10.48550/arXiv.2305.18365","CorpusId":263875917},"title":"What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks"},{"paperId":"f9aaeccdf56e71d6af498a192fdb1b39c76a45a1","externalIds":{"DBLP":"journals/corr/abs-2306-07622","DOI":"10.48550/arXiv.2306.07622","CorpusId":263828908},"title":"Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models - and Disappeared in GPT-4"},{"paperId":"814671c172531f344e2558631ae50925eeb6b097","externalIds":{"DBLP":"journals/corr/abs-2303-12057","DOI":"10.48550/arXiv.2303.12057","CorpusId":262906910},"title":"Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting"},{"paperId":"6c5a1079d9705c0ee022cef77207daa20ce2cde5","externalIds":{"DBLP":"journals/corr/abs-2304-01852","DOI":"10.48550/arXiv.2304.01852","CorpusId":263893278},"title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models"},{"paperId":"436d9e6249f239652150453e72e8f4b5311ca61f","externalIds":{"DBLP":"conf/aied/Olney23","CorpusId":262070493},"title":"Generating Multiple Choice Questions from a Textbook: LLMs Match Human Performance on Most Metrics"},{"paperId":"be7cb8f79bc018e57467168fc0c7f8ad59bba04f","externalIds":{"ACL":"2022.acl-long.230","DBLP":"conf/acl/RibeiroL22","DOI":"10.18653/v1/2022.acl-long.230","CorpusId":248779886},"title":"Adaptive Testing and Debugging of NLP Models"},{"paperId":"c69f9a5185b4c29525bedb2dcc79d20b42c14cc6","externalIds":{"DBLP":"conf/acl-dialdoc/HonovichAHTKCSS22","ACL":"2022.naacl-main.287","DOI":"10.48550/arXiv.2204.04991","CorpusId":247694170},"title":"TRUE: Re-evaluating Factual Consistency Evaluation"},{"paperId":"4f8e75d10cc9bcbd20e6aefea972b6d419a688cc","externalIds":{"DBLP":"conf/emnlp/2021f","DOI":"10.18653/v1/2021.findings-emnlp","CorpusId":244119161},"title":"Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"69426234b03624c01254679ac3385b1c30dc2f0f","externalIds":{"DOI":"10.1007/978-3-662-53120-4_6575","CorpusId":239322149},"title":"Accuracy"},{"paperId":"c63f428911e9ea11a7d582e5f3f817197a6cbfe8","externalIds":{"DOI":"10.4324/9780203772294-10","CorpusId":260437028},"title":"Artiﬁcial Intelligence"},{"paperId":"cf6cdcecbc2317a520fa431c411ff1c8de8031dd","externalIds":{"DOI":"10.1016/b978-0-323-50951-0.00074-8","CorpusId":239382941},"title":"Safety"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"de4b9bc12ddb6b9f6090c032ef5c6290bd64ef36","externalIds":{"DBLP":"conf/bnaic/2017","MAG":"2895834617","DOI":"10.1007/978-3-319-76892-2","CorpusId":3538256},"title":"Artificial Intelligence"},{"paperId":"4f8d648c52edf74e41b0996128aa536e13cc7e82","externalIds":{"DBLP":"journals/ijsc/HaoZM16","DOI":"10.1142/S1793351X16500045","CorpusId":1779661},"title":"Deep Learning"},{"paperId":"5e537c4d988d55f74d0bd5bb5015208977fc52e6","externalIds":{"CorpusId":126210996},"title":"FWDselect : Variable selection algorithm in regression models"},{"paperId":"b4fc91e543ec868658cde6170f1e59c33292e595","externalIds":{"DBLP":"conf/interspeech/KombrinkMKB11","MAG":"2293185259","DOI":"10.21437/Interspeech.2011-720","CorpusId":39718},"title":"Recurrent Neural Network Based Language Modeling in Meeting Recognition"},{"paperId":"b05bad9f21c00d2fedaebcb9c7653feebc024066","externalIds":{"MAG":"1041531548","DOI":"10.1007/978-3-642-20917-8_8","CorpusId":166740685},"title":"Opportunities and Risks"},{"paperId":"a70e48c119742cb69b1cdbd62e58a8a8d0d28a8e","externalIds":{"MAG":"1763968285","DBLP":"conf/eacl/BelzR06","ACL":"E06-1040","CorpusId":10438447},"title":"Comparing Automatic and Human Evaluation of NLG Systems"},{"paperId":"94a833fd36f2ad1989d4323b078a0dc6a691a1fa","externalIds":{"DBLP":"journals/jasis/Meadow85","DOI":"10.1002/asi.4630360516","CorpusId":218624022},"title":"Relevance?"}]}