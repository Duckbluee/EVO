{"references":[{"paperId":"e794be8c290de840281bc6d3ce4738998765050a","externalIds":{"MAG":"2954342481","PubMedCentral":"6761873","DOI":"10.1039/c9sc02572c","CorpusId":240283508,"PubMed":"31588327"},"title":"Bio-inspired synthesis of xishacorenes A, B, and C, and a new congener from fuscol"},{"paperId":"88c307c51594c6d802080a0780d0d654e2e2891f","externalIds":{"MAG":"2964138343","DBLP":"journals/cviu/WuTWSDH17","ArXiv":"1607.05910","DOI":"10.1016/j.cviu.2017.05.001","CorpusId":11746788},"title":"Visual question answering: A survey of methods and datasets"},{"paperId":"c41eb895616e453dcba1a70c9b942c5063cc656c","externalIds":{"MAG":"2468907370","ArXiv":"1606.09375","DBLP":"conf/nips/DefferrardBV16","CorpusId":3016223},"title":"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"},{"paperId":"121a9a160f1f2819a01edbe522024b58dbfee798","externalIds":{"DBLP":"conf/icmcs/SaitoSUH17","MAG":"2964023275","ArXiv":"1606.06108","DOI":"10.1109/ICME.2017.8019436","CorpusId":5805515},"title":"DualNet: Domain-invariant network for visual question answering"},{"paperId":"12f7de07f9b00315418e381b2bd797d21f12b419","externalIds":{"DBLP":"conf/emnlp/FukuiPYRDR16","ACL":"D16-1044","ArXiv":"1606.01847","MAG":"2412400526","DOI":"10.18653/v1/D16-1044","CorpusId":2840197},"title":"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"},{"paperId":"1afb710a5b35a2352a6495e4bf6eef66808daf1b","externalIds":{"DBLP":"journals/corr/KimLKHKHZ16","MAG":"2963917086","ArXiv":"1606.01455","CorpusId":1657806},"title":"Multimodal Residual Learning for Visual QA"},{"paperId":"fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b","externalIds":{"MAG":"2963668159","ArXiv":"1606.00061","DBLP":"journals/corr/LuYBP16","CorpusId":868693},"title":"Hierarchical Question-Image Co-Attention for Visual Question Answering"},{"paperId":"f96898d15a1bf1fa8925b1280d0e07a7a8e72194","externalIds":{"ArXiv":"1603.01417","MAG":"2963579811","DBLP":"journals/corr/XiongMS16","CorpusId":14294589},"title":"Dynamic Memory Networks for Visual and Textual Question Answering"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"98ea4abc9bf0e30eb020db2075c9c8a039a848a3","externalIds":{"MAG":"2963143606","DBLP":"conf/naacl/AndreasRDK16","ACL":"N16-1181","ArXiv":"1601.01705","DOI":"10.18653/v1/N16-1181","CorpusId":3130692},"title":"Learning to Compose Neural Networks for Question Answering"},{"paperId":"175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22","externalIds":{"MAG":"2963191264","ArXiv":"1511.07394","DBLP":"conf/cvpr/ShihSH16","DOI":"10.1109/CVPR.2016.499","CorpusId":11923637},"title":"Where to Look: Focus Regions for Visual Question Answering"},{"paperId":"d01379ebb53c66a4ccf5f4959d904dcf9e161e41","externalIds":{"MAG":"2951876403","DBLP":"journals/corr/VinyalsBK15","ArXiv":"1511.06391","CorpusId":260429228},"title":"Order Matters: Sequence to sequence for sets"},{"paperId":"3d1382fa43c31e594ed2d84dda9984b1db047b0e","externalIds":{"ArXiv":"1511.05676","MAG":"2266930373","DBLP":"journals/corr/JiangWPL15","CorpusId":18540179},"title":"Compositional Memory for Visual Question Answering"},{"paperId":"b196bc11ad516c8e6ff96f83acfc443fd7161730","externalIds":{"MAG":"2174492417","ArXiv":"1511.05960","DBLP":"journals/corr/ChenWCGXN15","CorpusId":16566944},"title":"ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering"},{"paperId":"492f57ee9ceb61fb5a47ad7aebfec1121887a175","externalIds":{"MAG":"2244807774","ArXiv":"1511.05493","DBLP":"journals/corr/LiTBZ15","CorpusId":8393918},"title":"Gated Graph Sequence Neural Networks"},{"paperId":"44e1ee7a63a01a76371d7070c132361a5ddd54a0","externalIds":{"MAG":"2949318071","DBLP":"journals/corr/JainZSS15","ArXiv":"1511.05298","DOI":"10.1109/CVPR.2016.573","CorpusId":563473},"title":"Structural-RNN: Deep Learning on Spatio-Temporal Graphs"},{"paperId":"1cf6bc0866226c1f8e282463adc8b75d92fba9bb","externalIds":{"DBLP":"conf/eccv/XuS16","MAG":"2963656855","ArXiv":"1511.05234","DOI":"10.1007/978-3-319-46478-7_28","CorpusId":10363459},"title":"Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering"},{"paperId":"5fa973b8d284145bf0ced9acf2913a74674260f6","externalIds":{"MAG":"2273038706","ArXiv":"1511.05099","DBLP":"conf/cvpr/ZhangGSBP16","DOI":"10.1109/CVPR.2016.542","CorpusId":6733279},"title":"Yin and Yang: Balancing and Answering Binary Visual Questions"},{"paperId":"def584565d05d6a8ba94de6621adab9e301d375d","externalIds":{"MAG":"2962749469","DBLP":"journals/corr/ZhuGBF15","ArXiv":"1511.03416","DOI":"10.1109/CVPR.2016.540","CorpusId":5714907},"title":"Visual7W: Grounded Question Answering in Images"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","externalIds":{"MAG":"2416885651","DBLP":"conf/cvpr/AndreasRDK16","ArXiv":"1511.02799","DOI":"10.1109/CVPR.2016.12","CorpusId":5276660},"title":"Neural Module Networks"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","externalIds":{"MAG":"2950318722","DBLP":"conf/cvpr/YangHGDS16","ArXiv":"1511.02274","DOI":"10.1109/CVPR.2016.10","CorpusId":8849206},"title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"5d1bfeed240709725c78bc72ea40e55410b373dc","externalIds":{"DBLP":"journals/corr/DuvenaudMAGHAA15","MAG":"2173027866","ArXiv":"1509.09292","CorpusId":1690180},"title":"Convolutional Networks on Graphs for Learning Molecular Fingerprints"},{"paperId":"df4f851e3c37017822a683b1356c6c390b5b5487","externalIds":{"MAG":"2396147015","DBLP":"journals/corr/RenKZ15","CorpusId":78798},"title":"Image Question Answering: A Visual Semantic Embedding Model and a New Dataset"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","externalIds":{"DBLP":"journals/corr/AntolALMBZP15","MAG":"1933349210","ArXiv":"1505.00468","DOI":"10.1007/s11263-016-0966-6","CorpusId":3180429},"title":"VQA: Visual Question Answering"},{"paperId":"ac64fb7e6d2ddf236332ec9f371fe85d308c114d","externalIds":{"MAG":"2951619830","DBLP":"journals/corr/MalinowskiF14","ArXiv":"1410.0210","CorpusId":3158329},"title":"A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","externalIds":{"DBLP":"conf/emnlp/PenningtonSM14","ACL":"D14-1162","MAG":"2250539671","DOI":"10.3115/v1/D14-1162","CorpusId":1957433},"title":"GloVe: Global Vectors for Word Representation"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","externalIds":{"MAG":"2950635152","DBLP":"conf/emnlp/ChoMGBBSB14","ACL":"D14-1179","ArXiv":"1406.1078","DOI":"10.3115/v1/D14-1179","CorpusId":5590763},"title":"Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation"},{"paperId":"8729441d734782c3ed532a7d2d9611b438c0a09a","externalIds":{"ArXiv":"1212.5701","MAG":"6908809","DBLP":"journals/corr/abs-1212-5701","CorpusId":7365802},"title":"ADADELTA: An Adaptive Learning Rate Method"},{"paperId":"ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","externalIds":{"MAG":"2606251538","DBLP":"journals/jmlr/GlorotB10","CorpusId":5575601},"title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":"f66821598f4db7a6a2f54a6a4ae43e391649f4c1","externalIds":{"DBLP":"conf/coling/MarneffeM08a","MAG":"2094061585","ACL":"W08-1301","DOI":"10.3115/1608858.1608859","CorpusId":3542573},"title":"The Stanford Typed Dependencies Representation"}]}