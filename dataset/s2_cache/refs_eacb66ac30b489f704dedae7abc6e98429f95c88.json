{"references":[{"paperId":"ef5a327d048b525ae2182e59e844711d66ddc7ac","externalIds":{"ArXiv":"2505.20074","DBLP":"conf/ijcai/WangYWSGSLF25","DOI":"10.48550/arXiv.2505.20074","CorpusId":278911900},"title":"An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks"},{"paperId":"d00862331c4eba2e35b902ba2ca340716745d94b","externalIds":{"ArXiv":"2503.19176","DBLP":"journals/corr/abs-2503-19176","DOI":"10.48550/arXiv.2503.19176","CorpusId":277313502},"title":"SoK: How Robust is Audio Watermarking in Generative AI models?"},{"paperId":"bbd5d7577c5c2c6d0ec69b679690cc370e7032c6","externalIds":{"DBLP":"conf/iclr/Lin0LL25","ArXiv":"2502.03052","DOI":"10.48550/arXiv.2502.03052","CorpusId":276116367},"title":"Understanding and Enhancing the Transferability of Jailbreaking Attacks"},{"paperId":"5d51b8f382a57e4eb97cf43c8de9b1deb6f06ac8","externalIds":{"DBLP":"conf/aaai/LiZLJ25","ArXiv":"2412.11735","DOI":"10.48550/arXiv.2412.11735","CorpusId":274777139},"title":"Transferable Adversarial Face Attack with Text Controlled Attribute"},{"paperId":"fd283d1bf172aa98e1efe484c83aa75386803b9f","externalIds":{"DBLP":"journals/csur/ZhangPLYCW25","DOI":"10.1145/3704725","CorpusId":274110847},"title":"Backdoor Attacks and Defenses Targeting Multi-Domain AI Models: A Comprehensive Review"},{"paperId":"e61a616ef46444f1943763daa004af2079b98b5b","externalIds":{"DBLP":"journals/csur/BaiHYLWX25","ArXiv":"2412.06157","DOI":"10.1145/3704633","CorpusId":274085608},"title":"Membership Inference Attacks and Defenses in Federated Learning: A Survey"},{"paperId":"434f6046e82ed0167b1476ec66e4c0e6b35d13fa","externalIds":{"PubMedCentral":"11526138","DOI":"10.1038/s41467-024-53147-y","CorpusId":273705183,"PubMed":"39477923"},"title":"A large-scale examination of inductive biases shaping high-level visual representation in brains and machines"},{"paperId":"b2f1349db7956d3dce0ea1d9e9ef69e85640c5bf","externalIds":{"DBLP":"journals/corr/abs-2409-01696","ArXiv":"2409.01696","DOI":"10.48550/arXiv.2409.01696","CorpusId":272367849},"title":"On the Vulnerability of Skip Connections to Model Inversion Attacks"},{"paperId":"9fa2a77d1deff6419f797900049396e39ffefcd8","externalIds":{"ArXiv":"2407.13863","DBLP":"journals/corr/abs-2407-13863","DOI":"10.48550/arXiv.2407.13863","CorpusId":271310420},"title":"A Closer Look at GAN Priors: Exploiting Intermediate Features for Enhanced Model Inversion Attacks"},{"paperId":"5608070833bf65ef04437816a7ec31fd942ff62d","externalIds":{"DBLP":"conf/ijcnn/ZengGJ24","ArXiv":"2407.13700","DOI":"10.1109/IJCNN60899.2024.10650118","CorpusId":271270871},"title":"Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift"},{"paperId":"ca7c9007635a599e3307d2ba40002da24425366f","externalIds":{"DBLP":"conf/cvpr/YangBG0LX24","DOI":"10.1109/CVPR52733.2024.02306","CorpusId":272722102},"title":"Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transfomers"},{"paperId":"bd425a702cfd898c37657bbb73909fa50714a887","externalIds":{"DBLP":"journals/nn/ZhangH24a","DOI":"10.1016/j.neunet.2024.106490","CorpusId":270782552,"PubMed":"38968777"},"title":"Aligning the domains in cross domain model inversion attack"},{"paperId":"19c2259d208a30fddd252810d6dcbe4f13420f1a","externalIds":{"DBLP":"journals/corr/abs-2405-17984","ArXiv":"2405.17984","DOI":"10.1145/3637528.3671956","CorpusId":270067542},"title":"Cross-Context Backdoor Attacks against Graph Prompt Learning"},{"paperId":"a42df2cc7f85e133f2ea38b2bbd2cb2b21a0f904","externalIds":{"DBLP":"conf/sp/WangDZQLFWL24","DOI":"10.1109/SP54263.2024.00102","CorpusId":272434222},"title":"Transferable Multimodal Attack on Vision-Language Pre-training Models"},{"paperId":"85f707934e1630695fbfbdf1934a21760917416d","externalIds":{"DBLP":"conf/host/LiNWE24","DOI":"10.1109/HOST55342.2024.10545382","CorpusId":270315405},"title":"TinyPower: Side-Channel Attacks with Tiny Neural Networks"},{"paperId":"88aa2fdbb750aea65629a64e99a9c55e776fb091","externalIds":{"DBLP":"journals/corr/abs-2410-02182","ArXiv":"2410.02182","DOI":"10.1109/TIP.2024.3378918","CorpusId":268722376,"PubMed":"38530729"},"title":"BadCM: Invisible Backdoor Attack Against Cross-Modal Learning"},{"paperId":"0ba18c68371252a028c80d8abb7d85f15af43a24","externalIds":{"DBLP":"conf/uss/Zhu0CZ023","ArXiv":"2401.00148","DOI":"10.48550/arXiv.2401.00148","CorpusId":266582934},"title":"TPatch: A Triggered Physical Adversarial Patch"},{"paperId":"351a2d50b4aff8e9754dc7074dd589b10a7465d4","externalIds":{"DBLP":"journals/corr/abs-2312-17617","ArXiv":"2312.17617","DOI":"10.1007/s11704-024-40555-y","CorpusId":266690657},"title":"Large language models for generative information extraction: a survey"},{"paperId":"3a62a6ebf84f15052d877072002b5e43595b4f2b","externalIds":{"DBLP":"journals/csur/ChaalanPKGZ24","DOI":"10.1145/3627536","CorpusId":264518326},"title":"The Path to Defence: A Roadmap to Characterising Data Poisoning Attacks on Victim Models"},{"paperId":"d542c12c76f398bc441a89d0b365d125804b7d1c","externalIds":{"DBLP":"conf/mm/ZhangTLLY23","DOI":"10.1145/3581783.3612475","CorpusId":264492756},"title":"Cross-modal and Cross-medium Adversarial Attack for Audio"},{"paperId":"840ca3b602cbc2efecadcdfdc4b75100b1b9d69f","externalIds":{"DBLP":"conf/mm/0027WCWJ23","DOI":"10.1145/3581783.3612110","CorpusId":264492775},"title":"GCMA: Generative Cross-Modal Transferable Adversarial Attacks from Images to Videos"},{"paperId":"8f48071cf937ba6c44c4b18ea0df05f3f3444cf5","externalIds":{"DBLP":"conf/emnlp/LvDLZH23","ArXiv":"2310.14265","DOI":"10.48550/arXiv.2310.14265","CorpusId":264426450},"title":"CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability"},{"paperId":"e30525cfd243b29c7c3e3831e4a92d0a35ec7258","externalIds":{"DBLP":"journals/tcsv/WangZZZH23","DOI":"10.1109/TCSVT.2023.3263054","CorpusId":257898874},"title":"Targeted Adversarial Attack Against Deep Cross-Modal Hashing Retrieval"},{"paperId":"e594cef29d90e82f681afa6bb7b0aa4e8da2ceae","externalIds":{"DBLP":"journals/csur/HuYYLHZDY24","DOI":"10.1145/3620667","CorpusId":262015056},"title":"Defenses to Membership Inference Attacks: A Survey"},{"paperId":"e562c8ee825ed4e1b3ec4037f6e9e1194d355f07","externalIds":{"DBLP":"conf/iclr/BeethamKMS23","ArXiv":"2309.10058","DOI":"10.48550/arXiv.2309.10058","CorpusId":259861536},"title":"Dual Student Networks for Data-Free Model Stealing"},{"paperId":"39d224210a5ced93a1c21c82083329f0ac53917e","externalIds":{"ArXiv":"2309.06960","DBLP":"conf/raid/Guo0WCY023","DOI":"10.1145/3607199.3607240","CorpusId":261705699},"title":"PhantomSound: Black-Box, Query-Efficient Audio Adversarial Attack via Split-Second Phoneme Injection"},{"paperId":"aa83437007d3fd6b79f11180f0c5b640d0c48cb3","externalIds":{"ArXiv":"2309.06055","DBLP":"journals/corr/abs-2309-06055","DOI":"10.1109/TNNLS.2025.3540303","CorpusId":261697173,"PubMed":"40031656"},"title":"Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review"},{"paperId":"e7987503524dfbcf118c4ccdd303cc798bb90b47","externalIds":{"DBLP":"journals/tkde/ZhangZLCWKYD24","ArXiv":"2308.16375","DOI":"10.1109/TKDE.2024.3454328","CorpusId":261397004},"title":"A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications"},{"paperId":"41cbea251c120f20bc0a9d92c5f8e896ecd9693e","externalIds":{"DBLP":"journals/network/YangXGLLY24","ArXiv":"2308.14367","DOI":"10.1109/MNET.2024.3367788","CorpusId":261244059},"title":"A Comprehensive Overview of Backdoor Attacks in Large Language Models Within Communication Networks"},{"paperId":"2fa77343b8995dfa59f0a263e0ca3a8261c4461e","externalIds":{"ArXiv":"2308.06015","DBLP":"conf/iccv/LiuZZQD23","DOI":"10.1109/ICCV51070.2023.00409","CorpusId":260866179},"title":"Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation"},{"paperId":"2fd52a3544dc0e3a480d12af01bb978c4d1a59fc","externalIds":{"DBLP":"conf/csr2/GencOT23","DOI":"10.1109/CSR57506.2023.10224959","CorpusId":261308416},"title":"A Taxonomic Survey of Model Extraction Attacks"},{"paperId":"47030369e97cc44d4b2e3cf1be85da0fd134904a","externalIds":{"DBLP":"journals/corr/abs-2307-15043","ArXiv":"2307.15043","CorpusId":260202961},"title":"Universal and Transferable Adversarial Attacks on Aligned Language Models"},{"paperId":"b86cdc12d63b8beadfaca1aa96cb5542d320f749","externalIds":{"DBLP":"journals/csur/YanLY25","ArXiv":"2307.13643","DOI":"10.1145/3701985","CorpusId":260154986},"title":"Backdoor Attacks against Voice Recognition Systems: A Survey"},{"paperId":"107a04026fee14c7c3cca3f5e305b90d22e4f145","externalIds":{"ArXiv":"2307.12280","DBLP":"journals/corr/abs-2307-12280","DOI":"10.1109/ICCV51070.2023.00401","CorpusId":260125919},"title":"Downstream-agnostic Adversarial Examples"},{"paperId":"2195bd3cdc14b8d37f1ec39816b046d5560b49da","externalIds":{"ArXiv":"2307.07859","DBLP":"journals/corr/abs-2307-07859","DOI":"10.1109/ICCV51070.2023.00410","CorpusId":259937136},"title":"Unified Adversarial Patch for Cross-modal Attacks in the Physical World"},{"paperId":"7be3256b94667b233b2c0ac343c423c03d2e6fe8","externalIds":{"DBLP":"journals/corr/abs-2307-09579","ArXiv":"2307.09579","DOI":"10.1145/3607199.3607237","CorpusId":259983012},"title":"Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots"},{"paperId":"81cd9575100643a3463465ec19e90ee78e122f93","externalIds":{"DBLP":"conf/csfw/Dibbo23","DOI":"10.1109/CSF57540.2023.00027","CorpusId":261319874},"title":"SoK: Model Inversion Attack Landscape: Taxonomy, Challenges, and Future Roadmap"},{"paperId":"85f99102eff38ab5eb56c9128dcf8ad582e23f91","externalIds":{"DBLP":"journals/corr/abs-2306-15363","ArXiv":"2306.15363","DOI":"10.1145/3607199.3607227","CorpusId":259262445},"title":"Your Attack Is Too DUMB: Formalizing Attacker Scenarios for Adversarial Transferability"},{"paperId":"8724579d3f126e753a0451d98ff57b165f722e72","externalIds":{"ArXiv":"2306.15447","DBLP":"journals/corr/abs-2306-15447","DOI":"10.48550/arXiv.2306.15447","CorpusId":259262181},"title":"Are aligned neural networks adversarially aligned?"},{"paperId":"5206aa1f39cdbc90245bab8761375f9159d913a0","externalIds":{"DBLP":"conf/aaai/LiW0ZL23","DOI":"10.1609/aaai.v37i2.25239","CorpusId":259695053},"title":"CDTA: A Cross-Domain Transfer-Based Attack with Contrastive Learning"},{"paperId":"af77300381156898024ea3a10f6d00cc31b8ee86","externalIds":{"DBLP":"conf/aaai/WangGW23","DOI":"10.1609/aaai.v37i2.25362","CorpusId":259762338},"title":"Global-Local Characteristic Excited Cross-Modal Attacks from Images to Videos"},{"paperId":"4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded","externalIds":{"DOI":"10.1155/2023/8691095","CorpusId":259400222},"title":"Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT"},{"paperId":"31d76bc7724ad3b3d44a5bbffcfdef09b11f0565","externalIds":{"DBLP":"conf/cvpr/Jiang0X023","DOI":"10.1109/CVPR52729.2023.00786","CorpusId":260068548},"title":"Color Backdoor: A Robust Poisoning Attack in Color Space"},{"paperId":"fdb8f4ab2e32e6dfd66e2bc422ecf22862fac088","externalIds":{"DBLP":"journals/pacmmod/WangICWNY23","DOI":"10.1145/3588956","CorpusId":259077170},"title":"Graph Learning for Interactive Threat Detection in Heterogeneous Smart Home Rule Data"},{"paperId":"98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2","externalIds":{"DBLP":"journals/corr/abs-2305-17826","ArXiv":"2305.17826","ACL":"2023.acl-long.867","DOI":"10.48550/arXiv.2305.17826","CorpusId":258960289},"title":"NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models"},{"paperId":"4c035a4f8779194a5ade5eb911e8dad0d8540652","externalIds":{"DBLP":"conf/infocom/WangYPWZ23","DOI":"10.1109/INFOCOM53939.2023.10228985","CorpusId":261382368},"title":"FacER: Contrastive Attention based Expression Recognition via Smartphone Earpiece Speaker"},{"paperId":"b14617abc61ca8c3d02f1d44eddcd11365ce61e9","externalIds":{"DBLP":"journals/corr/abs-2305-08192","ArXiv":"2305.08192","DOI":"10.1109/TPAMI.2024.3480519","CorpusId":258686793,"PubMed":"39405140"},"title":"Diffusion Models for Imperceptible and Transferable Adversarial Attack"},{"paperId":"c1d24ab606d6487e6daeed5c9e85dc650804754f","externalIds":{"ArXiv":"2305.05736","DBLP":"conf/wisec/WangGWCY23","DOI":"10.1145/3558482.3590189","CorpusId":258587820},"title":"VSMask: Defending Against Voice Synthesis Attack via Real-Time Predictive Perturbation"},{"paperId":"aa64af3efe45c77b1d62e2aaf34ad98675da5636","externalIds":{"DBLP":"journals/corr/abs-2304-11579","ArXiv":"2304.11579","DOI":"10.1109/CVPR52729.2023.00789","CorpusId":258297932},"title":"StyLess: Boosting the Transferability of Adversarial Examples"},{"paperId":"d73db78d1dc5fc8b3e0950fd4c4bc36758a452fe","externalIds":{"DBLP":"journals/corr/abs-2303-15940","ArXiv":"2303.15940","DOI":"10.1109/ICASSP49357.2023.10096873","CorpusId":257771521},"title":"Transaudio: Towards the Transferable Adversarial Audio Attack Via Learning Contextualized Perturbations"},{"paperId":"d822aafc2c53eb6c79ebe9a27c21ed0c30cec8c3","externalIds":{"DBLP":"journals/eaai/NguyenNNPDW24","ArXiv":"2303.02213","DOI":"10.48550/arXiv.2303.02213","CorpusId":257364914},"title":"Backdoor Attacks and Defenses in Federated Learning: Survey, Challenges and Future Research Directions"},{"paperId":"3599a236f285af48782fc30b1341d13ec7320735","externalIds":{"ArXiv":"2302.09419","DBLP":"journals/corr/abs-2302-09419","DOI":"10.48550/arXiv.2302.09419","CorpusId":257039063},"title":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"},{"paperId":"22ca556563dc24023a0d987328bef0add64131a8","externalIds":{"DBLP":"journals/corr/abs-2302-06801","ArXiv":"2302.06801","DOI":"10.48550/arXiv.2302.06801","CorpusId":256846917},"title":"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions"},{"paperId":"c54baca4c262c2336568e65c05ca7c12f8035607","externalIds":{"DBLP":"journals/ijon/ChangGYX23","DOI":"10.1016/j.neucom.2023.01.071","CorpusId":256575104},"title":"TextGuise: Adaptive adversarial example attacks on text classification model"},{"paperId":"cb5b71a622aff47014d4f28a958679629a8b6363","externalIds":{"ArXiv":"2301.10226","DBLP":"conf/icml/KirchenbauerGWK23","DOI":"10.48550/arXiv.2301.10226","CorpusId":256194179},"title":"A Watermark for Large Language Models"},{"paperId":"7a49c2c9ada7826df4cbbf2b48b43c4a78106077","externalIds":{"ArXiv":"2301.01197","DBLP":"conf/ndss/LiuL0S023","DOI":"10.48550/arXiv.2301.01197","CorpusId":255393915},"title":"Backdoor Attacks Against Dataset Distillation"},{"paperId":"8578cb028088e563061210eb4140772bdec989b2","externalIds":{"DBLP":"conf/icpads/ZeLCJX22","DOI":"10.1109/ICPADS56603.2022.00033","CorpusId":257785203},"title":"UltraBD: Backdoor Attack against Automatic Speaker Verification Systems via Adversarial Ultrasound"},{"paperId":"64a72c020871cd38a569cf7d2c3a35563b8c53ba","externalIds":{"DBLP":"journals/tecs/ChenK23","DOI":"10.1145/3574159","CorpusId":254247706},"title":"Tutorial: Toward Robust Deep Learning against Poisoning Attacks"},{"paperId":"7d65017ebd4a60b1390674cf8eee97c57dbe333e","externalIds":{"DBLP":"journals/compsec/ZhuZZRJ23","DOI":"10.1016/j.cose.2022.103057","CorpusId":254519744},"title":"LIGAA: Generative adversarial attack method based on low-frequency information"},{"paperId":"9459bdac15a6b0912ba0ada977e795538a879a9c","externalIds":{"DBLP":"conf/qrs/ShengHLC22","ArXiv":"2211.11958","DOI":"10.1109/QRS57517.2022.00086","CorpusId":253761016},"title":"A Survey on Backdoor Attack and Defense in Natural Language Processing"},{"paperId":"a509228a76dba1fde30a031913071876da7a6718","externalIds":{"DBLP":"journals/corr/abs-2211-03117","ArXiv":"2211.03117","DOI":"10.1109/ICASSP49357.2023.10096332","CorpusId":253384141},"title":"Going in Style: Audio Backdoors Through Stylistic Transformations"},{"paperId":"a9cb57e24bbc946abb3a64d37c7127f68cee7ac9","externalIds":{"DBLP":"conf/sensys/ChenCLYCWBL022","DOI":"10.1145/3560905.3568518","CorpusId":256195245},"title":"Push the Limit of Adversarial Example Attack on Speaker Recognition in Physical Domain"},{"paperId":"a279d7d05ce4cf849879e6948c34613b2dade858","externalIds":{"DBLP":"conf/sp/PuSARKBJV23","ArXiv":"2210.09421","DOI":"10.1109/SP46215.2023.10179387","CorpusId":252182306},"title":"Deepfake Text Detection: Limitations and Opportunities"},{"paperId":"538f0c8379f9e091d4769ec3bdff6eb45130554f","externalIds":{"ArXiv":"2210.05968","DBLP":"conf/nips/QinFL0Z0W22","DOI":"10.48550/arXiv.2210.05968","CorpusId":252846342},"title":"Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation"},{"paperId":"a4c748225360d89a6b03e1f277daddb64f429bcc","externalIds":{"DBLP":"conf/mm/LiangLLLBC22","DOI":"10.1145/3503161.3548416","CorpusId":252782673},"title":"Imitated Detectors: Stealing Knowledge of Black-box Object Detectors"},{"paperId":"df41628e0b60ba96d13fd716d7407a655982b53f","externalIds":{"DBLP":"journals/ijis/ZhangTLCLZ22","DOI":"10.1002/int.22932","CorpusId":251815039},"title":"Boosting cross‚Äêtask adversarial attack with random blur"},{"paperId":"d03f4ca6facd18b30ab4c6034350d430bca0bc33","externalIds":{"DBLP":"journals/corr/abs-2209-06506","ArXiv":"2209.06506","DOI":"10.1145/3548606.3560683","CorpusId":252222521},"title":"Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models"},{"paperId":"74292b0b41e955bcc65ba0215eb010ff2af07025","externalIds":{"DBLP":"journals/cm/GongCWWL22","DOI":"10.1109/MCOM.004.2100867","CorpusId":249449872},"title":"Private Data Inference Attacks against Cloud: Model, Technologies, and Research Directions"},{"paperId":"3e6e6a157fcf23e3808cdcf3b8323a256e0de2da","externalIds":{"ArXiv":"2208.03610","DBLP":"journals/corr/abs-2208-03610","DOI":"10.48550/arXiv.2208.03610","CorpusId":251402368},"title":"Blackbox Attacks via Surrogate Ensemble Search"},{"paperId":"0301917931b8e8858e7c1864e3bb73b4dc0590d2","externalIds":{"DBLP":"journals/csur/TianCLY23","DOI":"10.1145/3551636","CorpusId":251161366},"title":"A Comprehensive Survey on Poisoning Attacks and Countermeasures in Machine Learning"},{"paperId":"43dbb920f0e523cce8bbd8d35a3a1369a44e8a53","externalIds":{"DBLP":"conf/eccv/JingMYZSWT22","ArXiv":"2207.11681","DOI":"10.48550/arXiv.2207.11681","CorpusId":251040580},"title":"Learning Graph Neural Networks for Image Style Transfer"},{"paperId":"8680c075abfb7832ff1321b5f409f2a5e57570f2","externalIds":{"DBLP":"journals/corr/abs-2207-05382","ArXiv":"2207.05382","DOI":"10.48550/arXiv.2207.05382","CorpusId":250451607},"title":"Frequency Domain Model Augmentation for Adversarial Attack"},{"paperId":"35efa06e8c55a209677bcb48a6790b654d8b322f","externalIds":{"DBLP":"conf/eccv/ChengLCTCLZ22","ArXiv":"2207.04718","DOI":"10.48550/arXiv.2207.04718","CorpusId":250426022},"title":"Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches"},{"paperId":"e8ffe9828e0fd115566eba03098628dd41d52c1b","externalIds":{"DBLP":"journals/csi/ZhangCXCZX23","DOI":"10.1016/j.csi.2022.103672","CorpusId":251087899},"title":"A survey on privacy inference attacks and defenses in cloud-based Deep Neural Network"},{"paperId":"6fb706d286e6d1332ca9ea135ca93576695ffd2b","externalIds":{"DBLP":"conf/dsc/FanYLQX22","DOI":"10.1109/DSC55868.2022.00014","CorpusId":252626653},"title":"A Survey on Data Poisoning Attacks and Defenses"},{"paperId":"6ca266fc9a80646e821cbaa5dac3085ead9c7650","externalIds":{"ArXiv":"2207.00278","DBLP":"conf/mm/HuZZZZH022","DOI":"10.1145/3503161.3548272","CorpusId":250243701},"title":"BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label"},{"paperId":"1935668faf67ed68c6320d45144785ce4256fdf0","externalIds":{"DBLP":"conf/aaai/LiYWYH22","DOI":"10.1609/aaai.v36i2.20023","CorpusId":250301985},"title":"Learning Universal Adversarial Perturbation by Adversarial Example"},{"paperId":"d405b58a8f465d5ba2e91f9541e09760904c11a8","externalIds":{"ArXiv":"2206.08451","DBLP":"journals/csur/OliynykMR23","DOI":"10.1145/3595292","CorpusId":249848185},"title":"I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences"},{"paperId":"ef10e6a5ef05f4a76012669ca73d278e6df4d709","externalIds":{"ArXiv":"2206.07284","DBLP":"conf/ijcai/ZhangGWXT22","DOI":"10.48550/arXiv.2206.07284","CorpusId":249674534},"title":"A Survey on Gradient Inversion: Attacks, Defenses and Future Directions"},{"paperId":"6aa8d38e271bea8f962a386e08bc41c2e86e6e66","externalIds":{"DBLP":"journals/iotj/ZhouLLYSW22","DOI":"10.1109/jiot.2021.3130434","CorpusId":244654962},"title":"Hierarchical Adversarial Attacks Against Graph-Neural-Network-Based IoT Network Intrusion Detection System"},{"paperId":"f9a9675af906aba3f43d6fbb62175de83ade20f6","externalIds":{"DBLP":"conf/dac/CaoZGLY22","DOI":"10.1145/3489517.3530517","CorpusId":251744316},"title":"AL-PA: Cross-Device Profiled Side-Channel Attack using Adversarial Learning"},{"paperId":"df415323353b9344154b966949b70e3bd7c4cad9","externalIds":{"DBLP":"conf/cvpr/00810X0DZ022","DOI":"10.1109/CVPR52688.2022.01469","CorpusId":249951086},"title":"Towards Efficient Data Free Blackbox Adversarial Attack"},{"paperId":"75d9be5a85ca9fabe2ee41812ecfee6886b05670","externalIds":{"DBLP":"conf/cvpr/Sun0LPH22","DOI":"10.1109/CVPR52688.2022.01492","CorpusId":249947411},"title":"Exploring Effective Data for Surrogate Training Towards Black-box Attack"},{"paperId":"2d17efe0fa6ed96197896fcdcbe25d6fd2073038","externalIds":{"DBLP":"journals/csur/WangMWHQR23","DOI":"10.1145/3538707","CorpusId":249019685},"title":"Threats to Training: A Survey of Poisoning Attacks and Defenses on Machine Learning Systems"},{"paperId":"8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e","externalIds":{"ArXiv":"2205.07890","DBLP":"journals/corr/abs-2205-07890","DOI":"10.48550/arXiv.2205.07890","CorpusId":248834354},"title":"On the Difficulty of Defending Self-Supervised Learning against Model Extraction"},{"paperId":"5a123d014b77652aefff24e6e26c7f95d43f409a","externalIds":{"DBLP":"conf/uss/0005JG22","ArXiv":"2205.06401","DOI":"10.48550/arXiv.2205.06401","CorpusId":248798448},"title":"PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning"},{"paperId":"ae18eae54e78ee27d0ca65f69f92b1763532af3b","externalIds":{"DBLP":"journals/trets/GiechaskielTS23","DOI":"10.1145/3534972","CorpusId":275695634},"title":"Cross-VM Covert- and Side-Channel Attacks in Cloud FPGAs"},{"paperId":"d9141ca7fba548be09150e92b394a6b85ec9f6c4","externalIds":{"ArXiv":"2205.01992","DBLP":"journals/corr/abs-2205-01992","DOI":"10.1145/3585385","CorpusId":248512562},"title":"Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning"},{"paperId":"43cb6af19b504db2bb027c6ddca60f514d68afbd","externalIds":{"DBLP":"conf/infocom/ChenLBR22","DOI":"10.1109/INFOCOM48880.2022.9796934","CorpusId":249908113},"title":"PhoneyTalker: An Out-of-the-Box Toolkit for Adversarial Example Attack on Speaker Recognition"},{"paperId":"35ade8553de7259a5e8105bd20a160f045f9d112","externalIds":{"DBLP":"conf/cvpr/SanyalAB22","ArXiv":"2204.11022","DOI":"10.1109/CVPR52688.2022.01485","CorpusId":248377016},"title":"Towards Data-Free Model Stealing in a Hard Label Setting"},{"paperId":"b51eae3cc63fb57c690b27a9044b5df0ca680af8","externalIds":{"ArXiv":"2204.04063","DBLP":"conf/sp/MaoFWJ0LZLB022","DOI":"10.48550/arXiv.2204.04063","CorpusId":248069564},"title":"Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings"},{"paperId":"038ece8c19e11cefa1cb205fb2f88cbce836df61","externalIds":{"DBLP":"journals/ijon/QiuLZH22","DOI":"10.1016/j.neucom.2022.04.020","CorpusId":248044063},"title":"Adversarial attack and defense technologies in natural language processing: A survey"},{"paperId":"411b07870690a9492aec0331e07ede019f3d6814","externalIds":{"DBLP":"journals/corr/abs-2204-00008","ArXiv":"2204.00008","DOI":"10.1109/CVPR52688.2022.01457","CorpusId":247922429},"title":"Improving Adversarial Transferability via Neuron Attribution-based Attacks"},{"paperId":"edb8fee740880d720a5185dab74833e773db63b2","externalIds":{"DBLP":"journals/corr/abs-2203-16000","ArXiv":"2203.16000","DOI":"10.1109/SP46215.2023.10179383","CorpusId":247793092},"title":"StyleFool: Fooling Video Classification Systems via Style Transfer"},{"paperId":"1b308b8bb51fc6a1257a71a0d06bb9389bfb3192","externalIds":{"DBLP":"journals/corr/abs-2203-03121","ArXiv":"2203.03121","DOI":"10.1109/CVPR52688.2022.01459","CorpusId":247291928},"title":"Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer"},{"paperId":"e8f37427638bdf6892cc059b36570475bebdb62d","externalIds":{"DBLP":"journals/compsec/KwonL22","DOI":"10.1016/j.cose.2022.102695","CorpusId":247543344},"title":"Ensemble transfer attack targeting text classification systems"},{"paperId":"c4cefa5eeea76c203acd7ed137130a169c91b0e5","externalIds":{"ArXiv":"2202.10276","DBLP":"journals/corr/abs-2202-10276","CorpusId":247011610},"title":"Poisoning Attacks and Defenses on Artificial Intelligence: A Survey"},{"paperId":"1ff0c1a353880976d0375d5bc45602d47332c563","externalIds":{"ArXiv":"2202.05470","DBLP":"conf/sp/YangCCPTPCW23","DOI":"10.1109/SP46215.2023.10179347","CorpusId":246822474},"title":"Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers"},{"paperId":"384b58539d1e2fd7f1937d130d7e95ff0cc14a6a","externalIds":{"ArXiv":"2202.02585","DBLP":"conf/ndss/WangGY22","DOI":"10.14722/ndss.2022.24254","CorpusId":246634594},"title":"GhostTalk: Interactive Attack on Smartphone Voice System Through Power Line"},{"paperId":"a6cb46a2d7549d82abe893602b9a22b406859ebb","externalIds":{"DBLP":"conf/ccs/CongHZ22","ArXiv":"2201.11692","DOI":"10.1145/3548606.3559355","CorpusId":246294509},"title":"SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders"},{"paperId":"7da6c9273a14eb8681824d0c3ee84e05366c5627","externalIds":{"DBLP":"journals/corr/abs-2201-07513","ArXiv":"2201.07513","DOI":"10.1109/CVPR52729.2023.01571","CorpusId":246035632},"title":"Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders"},{"paperId":"d15fca16dae6f7ef2f52433be6b3acd59e9b9102","externalIds":{"DBLP":"conf/ccs/LiuJ0G22","ArXiv":"2201.05889","DOI":"10.1145/3548606.3560586","CorpusId":250698869},"title":"StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning"},{"paperId":"dd56f4d2e723a9a63101f7516abb76c6f503a541","externalIds":{"DOI":"10.36227/techrxiv.18254723.v1","CorpusId":245973810},"title":"A Survey of Image Gradient Inversion Against Federated Learning"},{"paperId":"7751b6cfbfefc1a6b76a3d421c37ad3b891a4817","externalIds":{"DBLP":"journals/corr/abs-2201-00672","ArXiv":"2201.00672","DOI":"10.1007/s10489-023-04575-8","CorpusId":245650212},"title":"Compression-resistant backdoor attack against deep neural networks"},{"paperId":"40cfec74524cf766f6c0030bc1f1736041a128e2","externalIds":{"ArXiv":"2112.06011","DBLP":"journals/corr/abs-2112-06011","MAG":"3106050153","DOI":"10.1007/978-3-030-58542-6_34","CorpusId":227034740},"title":"Improving the Transferability of Adversarial Examples with Resized-Diverse-Inputs, Diversity-Ensemble and Region Fitting"},{"paperId":"11b981f8c34c848ebf67ccdfcebbd87e45ee2d8e","externalIds":{"ArXiv":"2112.06658","DBLP":"journals/corr/abs-2112-06658","DOI":"10.1609/aaai.v36i1.19936","CorpusId":245124298},"title":"Learning to Learn Transferable Attack"},{"paperId":"91325f7c76edfcde496c9682676118c7f2a44422","externalIds":{"DBLP":"journals/corr/abs-2112-05379","ArXiv":"2112.05379","DOI":"10.1109/CVPR52688.2022.01464","CorpusId":245117841},"title":"Cross-Modal Transferable Adversarial Attacks from Images to Videos"},{"paperId":"54d7ae7cfee56b0e19fd42c45d365f760a41794d","externalIds":{"ArXiv":"2112.03570","DBLP":"conf/sp/CarliniCN0TT22","DOI":"10.1109/sp46214.2022.9833649","CorpusId":244920593},"title":"Membership Inference Attacks From First Principles"},{"paperId":"310af28c4686b25dde6a745cb918fce83959c81a","externalIds":{"DBLP":"conf/dac/YuSPJ21","DOI":"10.1109/dac18074.2021.9586100","CorpusId":234352716},"title":"Cross-Device Profiled Side-Channel Attacks using Meta-Transfer Learning"},{"paperId":"a04633398182ecec1a475c75352a18ec4d80d5f9","externalIds":{"ArXiv":"2111.08429","DBLP":"journals/corr/abs-2111-08429","DOI":"10.1109/ojsp.2022.3190213","CorpusId":244130385},"title":"An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences"},{"paperId":"342f6ae2ebfc0ddddf15e8ba910eab6f2e06bf83","externalIds":{"DBLP":"conf/emnlp/QiCZLLS21","ACL":"2021.emnlp-main.374","ArXiv":"2110.07139","DOI":"10.18653/v1/2021.emnlp-main.374","CorpusId":238857078},"title":"Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer"},{"paperId":"eff7cf1aa34483ca5d282e90983468896c9e5ffa","externalIds":{"DBLP":"journals/corr/abs-2110-04488","ArXiv":"2110.04488","DOI":"10.1109/TNSM.2022.3164354","CorpusId":238583055},"title":"Demystifying the Transferability of Adversarial Attacks in Computer Networks"},{"paperId":"76f759c5fb7c9b5cb0e34b319f53d5f9b9a94d3f","externalIds":{"DBLP":"conf/uss/ZhouYSS22","ArXiv":"2110.03154","CorpusId":238419626},"title":"DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle Avoidance in Autonomous Systems"},{"paperId":"6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786","externalIds":{"DBLP":"conf/iclr/ChenMSG0LF22","ArXiv":"2110.02467","CorpusId":238407985},"title":"BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models"},{"paperId":"16ab6586adebc68d8165003d117f6a7a052798fd","externalIds":{"DBLP":"conf/iccv/ZhangBKK21","DOI":"10.1109/ICCV48922.2021.00777","CorpusId":244114200},"title":"Data-free Universal Adversarial Perturbation and Black-box Attack"},{"paperId":"7791352f7f08cd6f621fdcfb34668cebaca84137","externalIds":{"DBLP":"journals/corr/abs-2109-07986","ArXiv":"2109.07986","DOI":"10.1145/3548606.3560566","CorpusId":237532614},"title":"Harnessing Perceptual Adversarial Patches for Crowd Counting"},{"paperId":"dcf115bd311988638bf4791bf6f88b1aeed0322d","externalIds":{"ArXiv":"2108.11023","DBLP":"journals/corr/abs-2108-11023","DOI":"10.1145/3460120.3484749","CorpusId":237290083},"title":"EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning"},{"paperId":"c50256fb118918412646f678bbcb3f6b5f3c9355","externalIds":{"DBLP":"journals/tches/CaoZLG21","DOI":"10.46586/tches.v2021.i4.27-56","CorpusId":237397870},"title":"Cross-Device Profiled Side-Channel Attack with Unsupervised Domain Adaptation"},{"paperId":"83ff3789f54577c13dda82b2e0a07cdbbf498c97","externalIds":{"DBLP":"conf/sp/JiaLG22","ArXiv":"2108.00352","DOI":"10.1109/sp46214.2022.9833644","CorpusId":236772198},"title":"BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning"},{"paperId":"b18580c6746a47603955074931d4687e268781ea","externalIds":{"ArXiv":"2107.14185","DBLP":"conf/iccv/WangGZLQ021","DOI":"10.1109/ICCV48922.2021.00754","CorpusId":236493523},"title":"Feature Importance-aware Transferable Adversarial Attacks"},{"paperId":"837c93685be51e07170e18e4a37c0804567d9aab","externalIds":{"MAG":"3192770481","DBLP":"journals/dcan/0011C0ML22","DOI":"10.1016/j.dcan.2021.07.009","CorpusId":238808387},"title":"Poisoning attacks and countermeasures in intelligent networks: Status quo and prospects"},{"paperId":"cbc1e8bbfe98f94c0d13d111b824cf603b62712c","externalIds":{"DBLP":"conf/sp/BoucherS0P22","ArXiv":"2106.09898","DOI":"10.1109/sp46214.2022.9833641","CorpusId":235485405},"title":"Bad Characters: Imperceptible NLP Attacks"},{"paperId":"052de58bf06815511ddf57883ec4d966dc80931e","externalIds":{"DBLP":"journals/corr/abs-2106-03614","ArXiv":"2106.03614","DOI":"10.1109/TPAMI.2024.3365699","CorpusId":235358209,"PubMed":"38349823"},"title":"Adversarial Attack and Defense in Deep Ranking"},{"paperId":"9e5dd10d0706cec7d16e03f45f8a347c194d5888","externalIds":{"DBLP":"conf/cvpr/WuSLK21","DOI":"10.1109/CVPR46437.2021.00891","CorpusId":241599204},"title":"Improving the Transferability of Adversarial Samples with Adversarial Transformations"},{"paperId":"aabe743b8b05f3d1a91d2421779d75f010649e72","externalIds":{"DBLP":"conf/cvpr/XiaoW21","DOI":"10.1109/CVPR46437.2021.00197","CorpusId":235719574},"title":"You See What I Want You to See: Exploring Targeted Black-Box Transferability Attack for Hash-based Image Retrieval Systems"},{"paperId":"3f3a248bde1c0c3173d1e1324c39dfc3ee7a0d3b","externalIds":{"DBLP":"journals/tai/ChenHTH24","ArXiv":"2105.15010","DOI":"10.1109/TAI.2023.3257276","CorpusId":257496319},"title":"Query Attack by Multi-Identity Surrogates"},{"paperId":"2e73516bb02f8ce1caa05e1984ace7ce14d1727b","externalIds":{"DBLP":"journals/csur/RosenbergSER21","DOI":"10.1145/3453158","CorpusId":235770456},"title":"Adversarial Machine Learning Attacks and Defense Methods in the Cyber Security Domain"},{"paperId":"016988fadfe52d5d017621ed1190f93333f91b8b","externalIds":{"DBLP":"conf/icassp/ZhangZL0CZH21","ArXiv":"2105.09022","DOI":"10.1109/ICASSP39728.2021.9413467","CorpusId":234778100},"title":"Attack on Practical Speaker Verification System Using Universal Adversarial Perturbations"},{"paperId":"3a8034426fd3fbe74c4c9e4517d69de706065efe","externalIds":{"DBLP":"conf/aaai/ChenFZZLLQ21","DOI":"10.1609/aaai.v35i2.16195","CorpusId":235306147},"title":"A Unified Multi-Scenario Attacking Network for Visual Object Tracking"},{"paperId":"78dedb15ffbbb2fed5cda9ce1e59cfb1b11bbea3","externalIds":{"ArXiv":"2105.06300","DBLP":"journals/corr/abs-2105-06300","CorpusId":234482894},"title":"Privacy Inference Attacks and Defenses in Cloud-based Deep Neural Network: A Survey"},{"paperId":"5179302656cf46cfbcc9cce2b70d4b0c758f7d7c","externalIds":{"DBLP":"conf/eccv/WangLLWWHJ22","ArXiv":"2105.00623","DOI":"10.1007/978-3-031-20065-6_12","CorpusId":233481094},"title":"Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack"},{"paperId":"dbb3278218029144fb034241c2d7604b6d8ed867","externalIds":{"ArXiv":"2106.09249","DBLP":"journals/corr/abs-2106-09249","DOI":"10.1109/SP40001.2021.00076","CorpusId":235457977},"title":"Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks"},{"paperId":"6ec73b69051cc4204da9a7a30b32e07b3b454286","externalIds":{"DBLP":"conf/sp/JiCZWYXF21","DOI":"10.1109/SP40001.2021.00091","CorpusId":235601506},"title":"Poltergeist: Acoustic Adversarial Machine Learning against Cameras and Computer Vision"},{"paperId":"7301d58eb8a750e44b7da5ab0266b134d1651237","externalIds":{"DBLP":"conf/cvpr/Wang021","ArXiv":"2103.15571","DOI":"10.1109/CVPR46437.2021.00196","CorpusId":232404127},"title":"Enhancing the Transferability of Adversarial Attacks through Variance Tuning"},{"paperId":"8c263746e9fa6d2b98345a46a4a1e3be9ebe3651","externalIds":{"MAG":"3136737068","ArXiv":"2103.09448","DBLP":"journals/corr/abs-2103-09448","DOI":"10.1109/IROS51168.2021.9636638","CorpusId":232257743},"title":"Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection"},{"paperId":"ac40415570e7d51efb237c5e4b4dbe4e93919409","externalIds":{"DBLP":"journals/corr/abs-2103-07853","ArXiv":"2103.07853","DOI":"10.1145/3523273","CorpusId":232233426},"title":"Membership Inference Attacks on Machine Learning: A Survey"},{"paperId":"b2d5359cef34b43b5d01971e5f9bc51c9f08896a","externalIds":{"DBLP":"journals/csur/LiCL21","DOI":"10.1145/3436729","CorpusId":244129235},"title":"Deep AI Enabled Ubiquitous Wireless Sensing"},{"paperId":"c514fe110102ae7243b9443b089cee3edaf007bf","externalIds":{"DBLP":"conf/ijcai/ZhangBLKWK21","ArXiv":"2103.01498","DOI":"10.24963/ijcai.2021/635","CorpusId":232092965},"title":"A Survey On Universal Adversarial Attack"},{"paperId":"2d862044d74662fd5428a05ee220f71b659db1ed","externalIds":{"DBLP":"journals/corr/abs-2103-01050","ArXiv":"2103.01050","DOI":"10.1109/CVPR46437.2021.00846","CorpusId":232092784},"title":"Dual Attention Suppression Attack: Generate Adversarial Camouflage in Physical World"},{"paperId":"a06a8ca70096e567e5cf1e433cc99ac1d519c4d0","externalIds":{"DBLP":"journals/caaitrit/ChakrabortyADCM21","MAG":"3135970545","DOI":"10.1049/CIT2.12028","CorpusId":233783448},"title":"A survey on adversarial attacks and defences"},{"paperId":"b5cc8f77bfb9feb057cba5136e9fa39b089a4ab0","externalIds":{"DBLP":"journals/corr/abs-2102-04140","ArXiv":"2102.04140","DOI":"10.1145/3460120.3484571","CorpusId":231846491},"title":"Quantifying and Mitigating Privacy Risks of Contrastive Learning"},{"paperId":"558f7fa8c8e909e7aa67b20d41ba5896b68dcf54","externalIds":{"DBLP":"journals/corr/abs-2102-00436","ArXiv":"2102.00436","DOI":"10.1109/ICCV48922.2021.01585","CorpusId":231741167},"title":"Admix: Enhancing the Transferability of Adversarial Attacks"},{"paperId":"68cede47122a128eb084c64877df187a4b068826","externalIds":{"DBLP":"conf/elinfocom/WonCJBB21","DOI":"10.1109/ICEIC51217.2021.9369754","CorpusId":232237088},"title":"Time to Leak: Cross-Device Timing Attack On Edge Deep Learning Accelerator"},{"paperId":"9d81282187337a4770b3cd2e842dadba329dfdfd","externalIds":{"DBLP":"journals/corr/abs-2101-11517","ArXiv":"2101.11517","DOI":"10.1109/TPAMI.2021.3132674","CorpusId":231718655,"PubMed":"34871167"},"title":"Investigating Bi-Level Optimization for Learning and Vision From a Unified Perspective: A Survey and Beyond"},{"paperId":"b4d93484790a68c575c4acbeac0cd99a58f46ebe","externalIds":{"ArXiv":"2101.08717","DBLP":"journals/pr/SilvaBBSO21","DOI":"10.1016/j.patcog.2021.107830","CorpusId":231662437},"title":"Copycat CNN: Are Random Non-Labeled Data Enough to Steal Knowledge from Black-box Models?"},{"paperId":"e019b8845413df345dcf5f6adb1d92cfd5cc4376","externalIds":{"DBLP":"journals/corr/abs-2101-08154","ArXiv":"2101.08154","DOI":"10.1609/aaai.v35i4.16477","CorpusId":231648020},"title":"Fooling thermal infrared pedestrian detectors in real world using small bulbs"},{"paperId":"de84aa4e7c33e7b062f31aaef368e196d291224e","externalIds":{"DBLP":"journals/corr/abs-2101-06855","ArXiv":"2101.06855","DOI":"10.1109/tnse.2021.3127557","CorpusId":231632159},"title":"GraphAttacker: A General Multi-Task Graph Attack Framework"},{"paperId":"d6b69662c905081e521d0dd0ec4ed073c9ff20f9","externalIds":{"DBLP":"journals/corr/abs-2101-06784","ArXiv":"2101.06784","CorpusId":231632532},"title":"Exploring Adversarial Robustness of Multi-Sensor Perception Systems in Self Driving"},{"paperId":"5b8090dd6420cc35d68c45514e94ce0c3cb3c665","externalIds":{"DBLP":"conf/infocom/HuangLWJZ21","ArXiv":"2101.00848","DOI":"10.1109/INFOCOM42981.2021.9488798","CorpusId":230438799},"title":"Towards Cross-Modal Forgery Detection and Localization on Live Surveillance Videos"},{"paperId":"cdcca029ee6afe2e9cec6fabb9cecff6c529d0d1","externalIds":{"DBLP":"conf/cvpr/ZolfiKES21","ArXiv":"2012.12528","DOI":"10.1109/CVPR46437.2021.01498","CorpusId":229363390},"title":"The Translucent Patch: A Physical and Universal Attack on Object Detectors"},{"paperId":"2cd35c6e53580d2adf5f693b05d5caa19844c19c","externalIds":{"ArXiv":"2012.11212","DBLP":"journals/corr/abs-2012-11212","DOI":"10.1609/aaai.v35i2.16201","CorpusId":229339767},"title":"Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification"},{"paperId":"10d05886c555258fcb2ba68d280c7cf5287a7a02","externalIds":{"DBLP":"journals/corr/abs-2012-11207","ArXiv":"2012.11207","CorpusId":229340618},"title":"On Success and Simplicity: A Second Look at Transferable Targeted Attacks"},{"paperId":"113fba4c88e2eb74f49544a161ef70e9745e969a","externalIds":{"DBLP":"journals/corr/abs-2012-10544","ArXiv":"2012.10544","DOI":"10.1109/TPAMI.2022.3162397","CorpusId":229934464,"PubMed":"35333711"},"title":"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"},{"paperId":"35d5395b90a69e9994cb208b91776df6233ea3e3","externalIds":{"ArXiv":"2012.07006","DBLP":"conf/asiaccs/0001ZGZQT21","MAG":"3111491915","DOI":"10.1145/3433210.3453108","CorpusId":229156177},"title":"DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation"},{"paperId":"d4a679c685de885ab98b7c1f2378e26a255de490","externalIds":{"DBLP":"journals/cm/GongWCYJ20","DOI":"10.1109/MCOM.001.2000196","CorpusId":228731605},"title":"Model Extraction Attacks and Defenses on Cloud-Based Machine Learning Models"},{"paperId":"1b053963c516b1bbd51c8a7a549e860edc35d89d","externalIds":{"MAG":"3102377752","DBLP":"journals/jetc/DanialDGGRS22","ArXiv":"2011.06139","DOI":"10.1145/3465380","CorpusId":226306899},"title":"EM-X-DL: Efficient Cross-device Deep Learning Side-channel Attack With Noisy EM Signatures"},{"paperId":"936623c631066a7e9c5b5f7ef8a9a5a9f1d73728","externalIds":{"MAG":"3109104051","DBLP":"conf/ccs/NassiMNBDE20","DOI":"10.1145/3372297.3423359","CorpusId":223654355},"title":"Phantom of the ADAS: Securing Advanced Driver-Assistance Systems from Split-Second Phantom Attacks"},{"paperId":"f70fe40f85e20eb1736e65a97f87eb340dcf8408","externalIds":{"DBLP":"conf/ccs/LiW00020","MAG":"3109668151","DOI":"10.1145/3372297.3423348","CorpusId":222318234},"title":"AdvPulse: Universal, Synchronization-free, and Targeted Audio Adversarial Attacks via Subsecond Perturbations"},{"paperId":"82132f7abf204b13cc171980781f76a0d627a970","externalIds":{"MAG":"3095939721","DBLP":"journals/tse/HeMCHH22","DOI":"10.1109/tse.2020.3034721","CorpusId":225093829},"title":"Towards Security Threats of Deep Learning Systems: A Survey"},{"paperId":"914d8143f747d4025afed64e72ab3a1735423822","externalIds":{"MAG":"3096513071","DBLP":"conf/interspeech/WangD0Q020","DOI":"10.21437/interspeech.2020-1255","CorpusId":226200350},"title":"Dual-Adversarial Domain Adaptation for Generalized Replay Attack Detection"},{"paperId":"ccad27088b9098de4eaca8dc449b18766db4b3ab","externalIds":{"MAG":"3100727892","ArXiv":"2010.05700","DBLP":"conf/emnlp/KrishnaWI20","ACL":"2020.emnlp-main.55","DOI":"10.18653/v1/2020.emnlp-main.55","CorpusId":222291619},"title":"Reformulating Unsupervised Style Transfer as Paraphrase Generation"},{"paperId":"42cf02f9e21aef51152631f2da48cb40b4855afd","externalIds":{"DBLP":"journals/corr/abs-2010-03288","MAG":"3091776972","ArXiv":"2010.03288","DOI":"10.1007/978-3-030-69538-5_18","CorpusId":222177032},"title":"Double Targeted Universal Adversarial Perturbations"},{"paperId":"f8492a321d66c381637b693a24af994af41b3cdf","externalIds":{"DBLP":"journals/corr/abs-2009-07888","MAG":"3085267010","ArXiv":"2009.07888","DOI":"10.1109/TPAMI.2023.3292075","CorpusId":221761694,"PubMed":"37402188"},"title":"Transfer Learning in Deep Reinforcement Learning: A Survey"},{"paperId":"f41647658a240098e3f3885430f9d52341bb203d","externalIds":{"ArXiv":"2009.06701","DBLP":"conf/uss/Sato0WJLC21","CorpusId":235422737},"title":"Dirty Road Can Attack: Security of Deep Learning based Automated Lane Centering under Physical-World Attack"},{"paperId":"9d06601c711920851d7975295d042da0928a892b","externalIds":{"DBLP":"journals/corr/abs-2009-05796","MAG":"3086842820","ArXiv":"2009.05796","DOI":"10.1007/978-3-030-68238-5_33","CorpusId":221655304},"title":"Revisiting the Threat Space for Vision-based Keystroke Inference Attacks"},{"paperId":"78db1529bd67ef885fe550ab3ed7f965067e8928","externalIds":{"DBLP":"journals/corr/abs-2009-03015","MAG":"3083360291","ArXiv":"2009.03015","DOI":"10.1109/SP40001.2021.00083","CorpusId":221516138},"title":"Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding"},{"paperId":"a14d6a842ac94cc561f39a5cab6ec133862d5f42","externalIds":{"DBLP":"journals/corr/abs-2007-14321","ArXiv":"2007.14321","MAG":"3046102592","CorpusId":220831381},"title":"Label-Only Membership Inference Attacks"},{"paperId":"69dd1b9e8391430a667214a9ca6c0bc94560deb2","externalIds":{"DBLP":"journals/corr/abs-2007-10760","MAG":"3044223678","ArXiv":"2007.10760","CorpusId":220665637},"title":"Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review"},{"paperId":"faa48bc9d554505bb542c03a99bbf52ca2b599e3","externalIds":{"DBLP":"journals/corr/abs-2007-08745","ArXiv":"2007.08745","MAG":"3042368254","DOI":"10.1109/TNNLS.2022.3182979","CorpusId":220633116,"PubMed":"35731760"},"title":"Backdoor Learning: A Survey"},{"paperId":"00366d7fe89a87a13c711121637782a04edf50be","externalIds":{"MAG":"3043638540","DBLP":"journals/corr/abs-2007-07646","ArXiv":"2007.07646","DOI":"10.1145/3624010","CorpusId":220525609},"title":"A Survey of Privacy Attacks in Machine Learning"},{"paperId":"b4196762025fb50eb3fdf521702d70f94e270b40","externalIds":{"MAG":"3043075157","ArXiv":"2007.06765","DBLP":"conf/eccv/GaoZSLS20","DOI":"10.1007/978-3-030-58604-1_19","CorpusId":220514392},"title":"Patch-wise Attack for Fooling Deep Neural Network"},{"paperId":"23811906b2fc98b2591f1d9f69d57814d5151108","externalIds":{"MAG":"3042075786","DBLP":"conf/uss/LovisottoTSSM21","ArXiv":"2007.04137","CorpusId":220403405},"title":"SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations"},{"paperId":"9d5434885b76ed7cd83f00cb81c929d087775e9a","externalIds":{"ArXiv":"2007.02343","DBLP":"conf/eccv/LiuM0020","MAG":"3039176595","DOI":"10.1007/978-3-030-58607-2_11","CorpusId":220363879},"title":"Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks"},{"paperId":"37cd9ce53b9ac7ec1fbe63b88af34ef5a1a64ac2","externalIds":{"MAG":"3047561893","DBLP":"conf/infocom/ZhangMYXFZ20","DOI":"10.1109/infocom41043.2020.9155483","CorpusId":221091827},"title":"Voiceprint Mimicry Attack Towards Speaker Verification System in Smart Home"},{"paperId":"130bd985dde3db51be96581a4d19085888287700","externalIds":{"DBLP":"conf/icml/LiangZWYKL21","ArXiv":"2006.14512","CorpusId":235422144},"title":"Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability"},{"paperId":"0ee6b61136c6b95f4efdd51d263126cf3b838559","externalIds":{"DBLP":"journals/corr/abs-2006-11946","ArXiv":"2006.11946","MAG":"3048968121","CorpusId":207908613},"title":"Light Commands: Laser-Based Audio Injection Attacks on Voice-Controllable Systems"},{"paperId":"2e5bcdf9c9f6dbd9a1f2e0e1a748da8081b79e87","externalIds":{"DBLP":"conf/nips/0001DM20","MAG":"3093525224","ArXiv":"2006.05057","CorpusId":225086593},"title":"Towards More Practical Adversarial Attacks on Graph Neural Networks"},{"paperId":"261641979088d2aab86fafe867a1af5be52592dc","externalIds":{"MAG":"3034372982","DBLP":"conf/cvpr/WangLLW20","DOI":"10.1109/cvpr42600.2020.01058","CorpusId":219634198},"title":"Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification"},{"paperId":"060c6c8312cd9277c97379e82db7c36b9321cce2","externalIds":{"DBLP":"conf/cvpr/LiDLYGH20","MAG":"3035569111","DOI":"10.1109/cvpr42600.2020.00072","CorpusId":219963354},"title":"Towards Transferable Targeted Attack"},{"paperId":"d5ffa58133940646d1339c2610cb35f27442e0d3","externalIds":{"DBLP":"conf/cvpr/Kariyappa0Q21","MAG":"3023663521","ArXiv":"2005.03161","DOI":"10.1109/CVPR46437.2021.01360","CorpusId":218538003},"title":"MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation"},{"paperId":"89b7cbab521812d20ab0aab508cc14844aeb96af","externalIds":{"DBLP":"journals/corr/abs-2004-04692","ArXiv":"2004.04692","MAG":"3015678314","CorpusId":215548043},"title":"Rethinking the Trigger of Backdoor Attack"},{"paperId":"692b40d55f9ef96ddd7ed351f78f570371e698bc","externalIds":{"ArXiv":"2004.03428","MAG":"3015977761","DBLP":"journals/corr/abs-2004-03428","DOI":"10.1109/ICME46284.2020.9102886","CorpusId":215238467},"title":"Universal Adversarial Perturbations Generative Network For Speaker Recognition"},{"paperId":"2b87bd8f5d5d6db21103f0cf7a79e3ca85fb1218","externalIds":{"MAG":"3014566885","ArXiv":"2004.01959","DBLP":"journals/corr/abs-2004-01959","DOI":"10.1109/cvpr42600.2020.00671","CorpusId":214802592},"title":"Cross-Domain Face Presentation Attack Detection via Multi-Domain Disentangled Representation Learning"},{"paperId":"530e299050651dbf827d56f83808de6c7c0b1a92","externalIds":{"MAG":"3034530016","DBLP":"conf/cvpr/ZhouWLLZ20","ArXiv":"2003.12703","DOI":"10.1109/cvpr42600.2020.00031","CorpusId":214714048},"title":"DaST: Data-Free Substitute Training for Adversarial Attacks"},{"paperId":"d8f19df22974cf63e990355175dbc6fc92a3f9fe","externalIds":{"MAG":"3009810649","DBLP":"journals/corr/abs-2003-02301","ArXiv":"2003.02301","DOI":"10.1109/ICASSP40776.2020.9053747","CorpusId":212414744},"title":"Real-Time, Universal, and Robust Adversarial Attacks Against Speaker Recognition Systems"},{"paperId":"346c5e5d28c45ad3460277699bf8e244cd33c2cf","externalIds":{"MAG":"3007913141","ArXiv":"2003.00653","DBLP":"journals/corr/abs-2003-00653","CorpusId":211678150},"title":"Adversarial Attacks and Defenses on Graphs: A Review and Empirical Study"},{"paperId":"5e40edeb996ac1b5d019a47fe43adfbeb8bba654","externalIds":{"DBLP":"journals/sigkdd/JinLXWJAT20","MAG":"3111218397","DOI":"10.1145/3447556.3447566","CorpusId":229171471},"title":"Adversarial Attacks and Defenses on Graphs"},{"paperId":"00b9f57012aece5bf4fecc87cce3b3df714690f7","externalIds":{"MAG":"3013170989","DBLP":"conf/isqed/0001M0ZJXS20","DOI":"10.1109/ISQED48828.2020.9137011","CorpusId":211478391},"title":"A Survey on Neural Trojans"},{"paperId":"2fb43a4c5cab8215d510fc585ca81fb5ee8a3abb","externalIds":{"MAG":"3006076803","DBLP":"journals/corr/abs-2002-05990","ArXiv":"2002.05990","CorpusId":211126665},"title":"Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets"},{"paperId":"92c6a2f7e178337fb99c695ae856b9377e8e72a5","externalIds":{"ArXiv":"2002.00123","MAG":"3004223849","DBLP":"journals/corr/abs-2002-00123","CorpusId":211010933},"title":"Model Extraction Attacks against Recurrent Neural Networks"},{"paperId":"ec21d03bdfdc5b4cd107b2f58a95e2615a248830","externalIds":{"DBLP":"conf/raid/ManLG20","MAG":"3092419617","CorpusId":219979041},"title":"GhostImage: Remote Perception Attacks against Camera-based Image Classification Systems"},{"paperId":"781c4027e9efcee90899dcc53d8740b2cc28631f","externalIds":{"ArXiv":"2001.03274","MAG":"2999321020","DBLP":"journals/corr/abs-2001-03274","DOI":"10.1109/TSC.2020.3000900","CorpusId":210157166},"title":"Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models"},{"paperId":"2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca","externalIds":{"MAG":"2989885118","DBLP":"conf/acsac/HeZL19","DOI":"10.1145/3359789.3359824","CorpusId":208277767},"title":"Model inversion attacks against collaborative inference"},{"paperId":"babf3916f3734fceeccf7aecb8da2353d92451e8","externalIds":{"MAG":"3035616549","ArXiv":"1911.07135","DBLP":"journals/corr/abs-1911-07135","DOI":"10.1109/cvpr42600.2020.00033","CorpusId":208139345},"title":"The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks"},{"paperId":"9584881e2f7aeadaebd4743bc876d1b492f3ab94","externalIds":{"DBLP":"conf/ccs/ZhaoZLSZ019","MAG":"2984260944","DOI":"10.1145/3319535.3354259","CorpusId":207947087},"title":"Seeing isn't Believing: Towards More Robust Adversarial Attack Against Real World Object Detectors"},{"paperId":"dbeb4e92905d8ae5dcfc2e024378d17d67df1198","externalIds":{"MAG":"2986013765","DBLP":"conf/ccs/LiuLTMAZ19","DOI":"10.1145/3319535.3363216","CorpusId":204746801},"title":"ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation"},{"paperId":"1e56e32dc0b9a001ea96e96029a958e12d859105","externalIds":{"ArXiv":"1911.01840","MAG":"2985489290","DBLP":"conf/sp/ChenCFDZSL21","DOI":"10.1109/SP40001.2021.00004","CorpusId":207869852},"title":"Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems"},{"paperId":"a7afa7815a40cc881459cbcfd26324ebe471553a","externalIds":{"DBLP":"conf/cikm/ChenL19","MAG":"2985888257","DOI":"10.1145/3357384.3358116","CorpusId":207759028},"title":"Data Poisoning Attacks on Cross-domain Recommendation"},{"paperId":"06fa914208bff3b5579ced913241868142daaeff","externalIds":{"ArXiv":"1910.14667","MAG":"2982663445","DBLP":"conf/eccv/WuLDG20","DOI":"10.1007/978-3-030-58548-8_1","CorpusId":207757900},"title":"Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors"},{"paperId":"ac713aebdcc06f15f8ea61e1140bb360341fdf27","externalIds":{"MAG":"2994896922","DBLP":"journals/corr/abs-1910-12366","ArXiv":"1910.12366","CorpusId":204907203},"title":"Thieves on Sesame Street! Model Extraction of BERT-based APIs"},{"paperId":"2f61d7afed8cf669d56e25b41be7c79cb3a4d8ee","externalIds":{"MAG":"2979711137","ArXiv":"1910.05262","DBLP":"journals/corr/abs-1910-05262","CorpusId":204401895},"title":"Hear \"No Evil\", See \"Kenansville\": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems"},{"paperId":"a1416c09d459c4b4a2edb9ffd6910f7ac7dc46eb","externalIds":{"MAG":"3013925760","DBLP":"journals/corr/abs-1909-08526","ArXiv":"1909.08526","DOI":"10.1007/978-3-030-33432-1_2","CorpusId":202660921},"title":"Defending against Machine Learning based Inference Attacks via Adversarial Examples: Opportunities and Challenges"},{"paperId":"6ad5f1d88534715051c6aba7436d60bdf65337e8","externalIds":{"ArXiv":"1909.08072","DBLP":"journals/ijautcomp/XuMLDLTJ20","MAG":"3013520104","DOI":"10.1007/s11633-019-1211-x","CorpusId":202660800},"title":"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review"},{"paperId":"72a8fd18652d55aa2c9e99bc629233fcfb6fe61a","externalIds":{"MAG":"2972432827","DBLP":"conf/interspeech/WangD0Q019","DOI":"10.21437/interspeech.2019-2120","CorpusId":202735648},"title":"Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training"},{"paperId":"4b0a779731c4dbca1d23eca00de8c8d43426bd5b","externalIds":{"MAG":"3028815699","DBLP":"conf/cvpr/HuangGZXYZL20","ArXiv":"1909.04326","DOI":"10.1109/CVPR42600.2020.00080","CorpusId":219099302},"title":"Universal Physical Camouflage Attacks on Object Detectors"},{"paperId":"52a222d38a8640499010d470d5589a81882bc425","externalIds":{"MAG":"3010489274","DBLP":"conf/uss/JagielskiCBKP20","ArXiv":"1909.01838","CorpusId":211858541},"title":"High Accuracy and High Fidelity Extraction of Neural Networks"},{"paperId":"3caf34532597683c980134579b156cd0d7db2f40","externalIds":{"ArXiv":"1908.07125","DBLP":"conf/emnlp/WallaceFKGS19","MAG":"2970290563","ACL":"D19-1221","DOI":"10.18653/v1/D19-1221","CorpusId":201698258},"title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"bcb53cf0f6cc9b63aeae120af452a15c93307477","externalIds":{"MAG":"2969875089","ArXiv":"1908.07021","DBLP":"journals/corr/abs-1908-07021","DOI":"10.1016/J.AIM.2020.107239","CorpusId":201103837},"title":"A synthetic approach to Markov kernels, conditional independence, and theorems on sufficient statistics"},{"paperId":"fb0568dcb546bbb4d7d1ed71ce395f4b66691003","externalIds":{"MAG":"2976752987","ArXiv":"1908.06281","DBLP":"conf/iclr/LinS00H20","CorpusId":202750057},"title":"Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks"},{"paperId":"f468e88c9b46147327c16f910e9695775d64f9d0","externalIds":{"ArXiv":"1907.02674","MAG":"2954035222","DBLP":"journals/tvlsi/GolderDDGSR19","DOI":"10.1109/TVLSI.2019.2926324","CorpusId":195820450},"title":"Practical Approaches Toward Deep-Learning-Based Cross-Device Power Side-Channel Attack"},{"paperId":"73e613c35671a2c9445a066208fcc492a0187b60","externalIds":{"MAG":"2945476062","DBLP":"conf/dac/DasGDGRS19","DOI":"10.1145/3316781.3317934","CorpusId":163164577},"title":"X-DeepSCA: Cross-Device Deep Learning Side Channel Attack*"},{"paperId":"bc1138738f24c4a23d865d7786fc4c8229e4662a","externalIds":{"MAG":"2990289029","ArXiv":"1905.11736","DBLP":"journals/corr/abs-1905-11736","CorpusId":167217657},"title":"Cross-Domain Transferability of Adversarial Perturbations"},{"paperId":"6be44364db3a46ab5fcf8172910650b210cc5c39","externalIds":{"MAG":"2982109374","ArXiv":"1904.05181","DBLP":"conf/mm/JiangMC0J19","DOI":"10.1145/3343031.3351088","CorpusId":131774147},"title":"Black-box Adversarial Attacks on Video Recognition Models"},{"paperId":"44d43bfbd23d55b1e7c4c4fd91fe101c0eaf1a06","externalIds":{"DBLP":"journals/corr/abs-1904-02884","ArXiv":"1904.02884","MAG":"2969542116","DOI":"10.1109/CVPR.2019.00444","CorpusId":102350868},"title":"Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks"},{"paperId":"652107ea8161f607e3bdabc89199e9ff2fdfd015","externalIds":{"MAG":"2940009958","CorpusId":260428188},"title":"Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey"},{"paperId":"48640ef269ffeb3e19c961de2eeca027a30841d8","externalIds":{"DBLP":"journals/corr/abs-1812-10528","ArXiv":"1812.10528","MAG":"2908442265","DOI":"10.1109/TKDE.2022.3201243","CorpusId":57189241},"title":"Adversarial Attack and Defense on Graph Data: A Survey"},{"paperId":"67e856f76905f2269089cede51c9a6a7c4fd2f8c","externalIds":{"MAG":"2997583194","DBLP":"journals/corr/abs-1812-03413","ArXiv":"1812.03413","DOI":"10.1609/AAAI.V34I07.6810","CorpusId":54462231},"title":"Learning Transferable Adversarial Examples via Ghost Networks"},{"paperId":"089c6224cfbcf5c18b63564eb65001c7c42a7acf","externalIds":{"MAG":"2905209730","ArXiv":"1812.02766","DBLP":"journals/corr/abs-1812-02766","DOI":"10.1109/CVPR.2019.00509","CorpusId":54457412},"title":"Knockoff Nets: Stealing Functionality of Black-Box Models"},{"paperId":"f84f88fa0a52c21bb764b87746c8dfbb427525c8","externalIds":{"MAG":"2965198951","DBLP":"conf/ijcai/WeiLCC19","ArXiv":"1811.12641","DOI":"10.24963/ijcai.2019/134","CorpusId":54434451},"title":"Transferable Adversarial Attacks for Image and Video Object Detection"},{"paperId":"e380347727234a8bd7ed86ea0dc6832acf94f6b4","externalIds":{"MAG":"2973057252","DBLP":"conf/interspeech/KhareAM19","DOI":"10.21437/interspeech.2019-2420","CorpusId":195791621},"title":"Adversarial Black-Box Attacks on Automatic Speech Recognition Systems Using Multi-Objective Evolutionary Optimization"},{"paperId":"9cdf005d7225ffba0949ebb4bcc419522d82edd3","externalIds":{"MAG":"2945042590","DBLP":"conf/ipccc/WangZYL18","DOI":"10.1109/PCCC.2018.8710834","CorpusId":155107619},"title":"Socialite: Social Activity Mining and Friend Auto-labeling"},{"paperId":"30ee0f59c6d41cc3f8a00a1d5fdea1a61fa09ff9","externalIds":{"MAG":"2964285471","DBLP":"conf/nips/TaoMLZ18","ArXiv":"1810.11580","CorpusId":53094696},"title":"Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples"},{"paperId":"1a83564d61aebde360c0be4834cf6eb4c472c1bd","externalIds":{"MAG":"2954014647","DBLP":"conf/cvpr/XiaoYLDL19","DOI":"10.1109/CVPR.2019.00706","CorpusId":195767667},"title":"MeshAdv: Adversarial Meshes for Visual Recognition"},{"paperId":"efdb28e0107a3c6142ff5b01e4fc5a974b5ea762","externalIds":{"DBLP":"conf/milcom/JhaJJJ18","MAG":"2906965747","DOI":"10.1109/MILCOM.2018.8599691","CorpusId":57376324},"title":"Detecting Adversarial Examples Using Data Manifolds"},{"paperId":"869fdb53a40290a3941fd6ab808835e9b5184d62","externalIds":{"MAG":"2893554781","DBLP":"journals/corr/abs-1810-00069","ArXiv":"1810.00069","CorpusId":52901134},"title":"Adversarial Attacks and Defences: A Survey"},{"paperId":"1879d6b29eee6efab8f6217a7a6f47ec04f25b3e","externalIds":{"ArXiv":"1809.04790","DBLP":"journals/tnn/0002L20","MAG":"2891828758","DOI":"10.1109/TNNLS.2019.2933524","CorpusId":52269752,"PubMed":"31722487"},"title":"Adversarial Examples: Opportunities and Challenges"},{"paperId":"8bac2716cd208cb8041650a001ab72ba81b559cd","externalIds":{"DBLP":"conf/uss/DemontisMPJBONR19","MAG":"2913770005","CorpusId":128088823},"title":"Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks"},{"paperId":"9912aef790770b029968c6c52f8bae4ff9cfc859","externalIds":{"MAG":"2885039223","DBLP":"conf/eccv/MopuriUB18","ArXiv":"1808.01153","DOI":"10.1007/978-3-030-01240-3_2","CorpusId":51921466},"title":"Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions"},{"paperId":"c67855ff840c9f8d256486158c63a242d912e41e","externalIds":{"MAG":"2964175514","DBLP":"conf/woot/SongEEF0RTPK18","ArXiv":"1807.07769","CorpusId":49904930},"title":"Physical Adversarial Examples for Object Detectors"},{"paperId":"cc7c62516fbbf5d872d874bd598c8ab8ba903808","externalIds":{"MAG":"2884771968","DBLP":"conf/mm/WangFCYHY18","ArXiv":"1807.07258","DOI":"10.1145/3240508.3240512","CorpusId":49883347},"title":"Visual Domain Adaptation with Manifold Embedded Distribution Alignment"},{"paperId":"1d033b30f38642e4b6dd146bb8b464bfb58aad96","externalIds":{"MAG":"2883725317","ArXiv":"1807.05520","DBLP":"journals/corr/abs-1807-05520","DOI":"10.1007/978-3-030-01264-9_9","CorpusId":263891125},"title":"Deep Clustering for Unsupervised Learning of Visual Features"},{"paperId":"7f77058976e2fe75e98280371962c43d98c98321","externalIds":{"MAG":"2950317500","DBLP":"journals/corr/abs-1806-02371","ArXiv":"1806.02371","CorpusId":46968933},"title":"Adversarial Attack on Graph Structured Data"},{"paperId":"f0c5991dbb130fa6b5de011cf7a04f6ed815ef68","externalIds":{"MAG":"2798302089","DBLP":"conf/cvpr/EykholtEF0RXPKS18","DOI":"10.1109/CVPR.2018.00175","CorpusId":29162614},"title":"Robust Physical-World Attacks on Deep Learning Visual Classification"},{"paperId":"6c44f8e62d824bcda4f291c679a5518bbd4225f6","externalIds":{"ArXiv":"1805.07984","MAG":"2803831897","DBLP":"conf/ijcai/ZugnerAG19","DOI":"10.1145/3219819.3220078","CorpusId":29169801},"title":"Adversarial Attacks on Neural Networks for Graph Data"},{"paperId":"9e1549e9225502f86a94c26f65dfee1a5683bb30","externalIds":{"MAG":"2803853585","ArXiv":"1805.07820","DBLP":"conf/sp/TaoriKCV19","DOI":"10.1109/SPW.2019.00016","CorpusId":29154090},"title":"Targeted Adversarial Examples for Black Box Audio Systems"},{"paperId":"0f885fd46064d271d4404cf9bb3d758e1a6f8d55","externalIds":{"MAG":"2799269579","DBLP":"conf/eccv/MahajanGRHPLBM18","ArXiv":"1805.00932","DOI":"10.1007/978-3-030-01216-8_12","CorpusId":13751202},"title":"Exploring the Limits of Weakly Supervised Pretraining"},{"paperId":"2b110fce160468eb179b6c43ea27e098757a56dd","externalIds":{"MAG":"2963126845","DBLP":"journals/corr/abs-1804-06059","ACL":"N18-1170","ArXiv":"1804.06059","DOI":"10.18653/v1/N18-1170","CorpusId":4956100},"title":"Adversarial Example Generation with Syntactically Controlled Paraphrase Networks"},{"paperId":"f78a911f516625d6b7b76a9a33c1eb14613341c4","externalIds":{"MAG":"2962847335","ArXiv":"1803.06978","DBLP":"conf/cvpr/XieZZBWRY19","DOI":"10.1109/CVPR.2019.00284","CorpusId":3972825},"title":"Improving Transferability of Adversarial Examples With Input Diversity"},{"paperId":"8a2045c0eadec75b2cf5b5b3353e01b7973bda67","externalIds":{"MAG":"2922108301","DBLP":"journals/corr/abs-1803-06975","ArXiv":"1803.06975","CorpusId":3971778},"title":"When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks"},{"paperId":"49d1709ee2f7167f455f10c0a6c6e0a569d08c18","externalIds":{"DBLP":"journals/tifs/LiLCWHK18","MAG":"2790392345","DOI":"10.1109/TIFS.2018.2801312","CorpusId":4624345},"title":"Unsupervised Domain Adaptation for Face Anti-Spoofing"},{"paperId":"57144dcc8db1a678fab2331f350e8f7f298f1dbd","externalIds":{"DBLP":"conf/icassp/KreukACK18","MAG":"2783047542","ArXiv":"1801.03339","DOI":"10.1109/ICASSP.2018.8462693","CorpusId":3354671},"title":"Fooling End-To-End Speaker Verification With Adversarial Examples"},{"paperId":"b514949ad8344071c0f342f182390d2d88bcc26d","externalIds":{"MAG":"2962700793","DBLP":"journals/corr/abs-1801-00553","ArXiv":"1801.00553","DOI":"10.1109/ACCESS.2018.2807385","CorpusId":3536399},"title":"Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"},{"paperId":"e8da4ff1519011ed018202bb96dee4b611f5d842","externalIds":{"DBLP":"conf/cvpr/PoursaeedKGB18","MAG":"2963855547","ArXiv":"1712.02328","DOI":"10.1109/CVPR.2018.00465","CorpusId":4670982},"title":"Generative Adversarial Perturbations"},{"paperId":"8e37a3b227b68953f8067215828dc8b8714cb21b","externalIds":{"DBLP":"conf/cvpr/DongLPS0HL18","MAG":"2774644650","DOI":"10.1109/CVPR.2018.00957","CorpusId":4119221},"title":"Boosting Adversarial Attacks with Momentum"},{"paperId":"dc777eb44207a3349c6bd0656687ffd376101610","externalIds":{"MAG":"2754808688","DBLP":"journals/corr/abs-1709-03582","ArXiv":"1709.03582","DOI":"10.1109/CVPR.2018.00893","CorpusId":6904814},"title":"Art of Singular Vectors and Universal Adversarial Perturbations"},{"paperId":"4e7ee598342276ea02b87e064538300e1796d09d","externalIds":{"DBLP":"journals/corr/abs-1708-09537","MAG":"2751902866","ArXiv":"1708.09537","DOI":"10.1145/3133956.3134052","CorpusId":2419970},"title":"DolphinAttack: Inaudible Voice Commands"},{"paperId":"66e1dff1be7ad92104ef33a031b3392517a19b58","externalIds":{"DBLP":"conf/interspeech/KinnunenSDTEYL17","MAG":"2745896134","DOI":"10.21437/Interspeech.2017-1111","CorpusId":5719998},"title":"The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection"},{"paperId":"a2b4c396dc1064fb90bb5455525733733c761a7f","externalIds":{"DBLP":"conf/interspeech/LavrentyevaNMKK17","MAG":"2747024632","DOI":"10.21437/Interspeech.2017-360","CorpusId":7750612},"title":"Audio Replay Attack Detection with Deep Learning Frameworks"},{"paperId":"07e9c1d9a6927d9003bc69d6e3535f1917a25062","externalIds":{"MAG":"2741594031","DBLP":"conf/ijcai/ZhaoAGZ17","DOI":"10.24963/ijcai.2017/551","CorpusId":27887563},"title":"Efficient Label Contamination Attacks Against Black-Box Learning Models"},{"paperId":"36a3eed52ff0a694aa73ce6a0d592cb440ed3d31","externalIds":{"DBLP":"journals/corr/EvtimovEFKLPRS17","MAG":"2741933435","CorpusId":31567351},"title":"Robust Physical-World Attacks on Machine Learning Models"},{"paperId":"8dce99e33c6fceb3e79023f5894fdbe733c91e92","externalIds":{"MAG":"2963557656","DBLP":"conf/icml/AthalyeEIK18","CorpusId":2645819},"title":"Synthesizing Robust Adversarial Examples"},{"paperId":"5dd660b3e464e0f9f9a1af62e81f6f17fd4e2f66","externalIds":{"ArXiv":"1707.03184","DBLP":"journals/corr/KumarM17","MAG":"2736106054","CorpusId":10869869},"title":"A Survey on Resilient Machine Learning"},{"paperId":"21f360cce06ffdb3e7a8a56f7499cc79ff86405a","externalIds":{"MAG":"2738229973","DBLP":"conf/bmvc/MopuriGR17","ArXiv":"1707.05572","DOI":"10.5244/C.31.30","CorpusId":38305772},"title":"Fast Feature Fool: A data independent approach to universal adversarial perturbations"},{"paperId":"63a010c69f00e65c946a68b546bbd42cbed03564","externalIds":{"MAG":"2618043096","ArXiv":"1705.09064","DBLP":"journals/corr/MengC17","DOI":"10.1145/3133956.3134057","CorpusId":3583538},"title":"MagNet: A Two-Pronged Defense against Adversarial Examples"},{"paperId":"136dee73f203df2f4831994bf4f0c0a4ad2e764e","externalIds":{"MAG":"2952396876","DBLP":"journals/corr/TramerKPBM17","ArXiv":"1705.07204","CorpusId":21946795},"title":"Ensemble Adversarial Training: Attacks and Defenses"},{"paperId":"2ac11a0b84580e88bb8c6e5edb38c92fe1456864","externalIds":{"MAG":"2623427976","DOI":"10.1109/THS.2017.7943475","CorpusId":43625028},"title":"How to steal a machine learning classifier with deep learning"},{"paperId":"be0ef77fb0345c5851bb5d297f3ed84ae3c581ee","externalIds":{"MAG":"2940630528","ArXiv":"1703.06868","DBLP":"conf/iccv/HuangB17","DOI":"10.1109/ICCV.2017.167","CorpusId":6576859},"title":"Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization"},{"paperId":"b663e16f3b466278c0ee24f1780935755c6dd436","externalIds":{"DBLP":"journals/corr/HosseiniKZP17","MAG":"2594590228","ArXiv":"1702.08138","CorpusId":15418780},"title":"Deceiving Google's Perspective API Built for Detecting Toxic Comments"},{"paperId":"7dfbbc101cf9e7359a336df25dab90d66e249255","externalIds":{"MAG":"2561975083","ArXiv":"1612.07767","DBLP":"journals/corr/LiL16e","DOI":"10.1109/ICCV.2017.615","CorpusId":7733308},"title":"Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics"},{"paperId":"f4d3d851986b7849058e7fc9ef1c643633b0b89d","externalIds":{"MAG":"3100675173","DBLP":"journals/comsur/SpreitzerMKM18","ArXiv":"1611.03748","DOI":"10.1109/COMST.2017.2779824","CorpusId":206578562},"title":"Systematic Classification of Side-Channel Attacks: A Case Study for Mobile Devices"},{"paperId":"99e5a8c10cf92749d4a7c2949691c3a6046e499a","externalIds":{"ArXiv":"1611.02770","DBLP":"conf/iclr/LiuCLS17","MAG":"2570685808","CorpusId":17707860},"title":"Delving into Transferable Adversarial Examples and Black-box Attacks"},{"paperId":"16aa01ca0834a924c25faad5d8bfef3fd1acfcfe","externalIds":{"DBLP":"conf/cvpr/Moosavi-Dezfooli17","MAG":"2543927648","ArXiv":"1610.08401","DOI":"10.1109/CVPR.2017.17","CorpusId":11558223},"title":"Universal Adversarial Perturbations"},{"paperId":"f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d","externalIds":{"ArXiv":"1610.05820","DBLP":"journals/corr/ShokriSS16","MAG":"2535690855","DOI":"10.1109/SP.2017.41","CorpusId":10488675},"title":"Membership Inference Attacks Against Machine Learning Models"},{"paperId":"efbd381493bb9636f489b965a2034d529cd56bcd","externalIds":{"ArXiv":"1609.07843","MAG":"2525332836","DBLP":"journals/corr/MerityXBS16","CorpusId":16299141},"title":"Pointer Sentinel Mixture Models"},{"paperId":"df40ce107a71b770c9d0354b78fdd8989da80d2f","externalIds":{"DBLP":"conf/sp/Carlini017","MAG":"2951755642","ArXiv":"1608.04644","DOI":"10.1109/SP.2017.49","CorpusId":2893830},"title":"Towards Evaluating the Robustness of Neural Networks"},{"paperId":"8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e","externalIds":{"MAG":"2461943168","ArXiv":"1609.02943","DBLP":"conf/uss/TramerZJRR16","CorpusId":2984526},"title":"Stealing Machine Learning Models via Prediction APIs"},{"paperId":"b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b","externalIds":{"DBLP":"conf/iclr/KurakinGB17a","MAG":"2460937040","ArXiv":"1607.02533","DOI":"10.1201/9781351251389-8","CorpusId":1257772},"title":"Adversarial examples in the physical world"},{"paperId":"7568d13a82f7afa4be79f09c295940e48ec6db89","externalIds":{"MAG":"2475287302","DBLP":"conf/cvpr/GatysEB16","DOI":"10.1109/CVPR.2016.265","CorpusId":206593710},"title":"Image Style Transfer Using Convolutional Neural Networks"},{"paperId":"78aa018ee7d52360e15d103390ea1cdb3a0beb41","externalIds":{"DBLP":"journals/corr/PapernotMG16","ArXiv":"1605.07277","MAG":"2408141691","CorpusId":17362994},"title":"Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples"},{"paperId":"53b047e503f4c24602f376a774d653f7ed56c024","externalIds":{"ArXiv":"1602.02697","MAG":"2603766943","DBLP":"conf/ccs/PapernotMGJCS17","DOI":"10.1145/3052973.3053009","CorpusId":1090603},"title":"Practical Black-Box Attacks against Machine Learning"},{"paperId":"6adf016e7531c91100d3cf4a74f5d4c87b26b528","externalIds":{"MAG":"2174868984","DBLP":"journals/corr/PapernotMWJS15","ArXiv":"1511.04508","DOI":"10.1109/SP.2016.41","CorpusId":2672720},"title":"Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks"},{"paperId":"d1b9a3b11e6c9571a1553556f82b605b2b4baec3","externalIds":{"DBLP":"conf/ccs/FredriksonJR15","MAG":"2051267297","DOI":"10.1145/2810103.2813677","CorpusId":207229839},"title":"Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures"},{"paperId":"bee044c8e8903fb67523c1f8c105ab4718600cdb","externalIds":{"MAG":"2952181735","DBLP":"journals/corr/GoodfellowSS14","ArXiv":"1412.6572","CorpusId":6706414},"title":"Explaining and Harnessing Adversarial Examples"},{"paperId":"d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad","externalIds":{"ArXiv":"1312.6199","MAG":"2964153729","DBLP":"journals/corr/SzegedyZSBEGF13","CorpusId":604334},"title":"Intriguing properties of neural networks"},{"paperId":"688b5c92faf0275484f51c7f35be28d9a42724d2","externalIds":{"MAG":"2086815162","DBLP":"journals/jce/MontminyBTL13","DOI":"10.1007/s13389-012-0038-y","CorpusId":18343838},"title":"Improving cross-device attacks using zero-mean unit-variance normalization"},{"paperId":"a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","externalIds":{"DBLP":"journals/tkde/PanY10","MAG":"2165698076","DOI":"10.1109/TKDE.2009.191","CorpusId":740063},"title":"A Survey on Transfer Learning"},{"paperId":"cf1db9d4b45752edb368855224e0572d41c6a169","externalIds":{"MAG":"2034742893","DOI":"10.1016/0378-2166(87)90099-3","CorpusId":58053800},"title":"Generating Natural Language Under Pragmatic Constraints"},{"paperId":"9e7bfb0c2185d86cd98cf4abea05065c8081a24e","externalIds":{"DBLP":"journals/pr/LuLLCV25","DOI":"10.1016/j.patcog.2025.111652","CorpusId":277685263},"title":"Cross-task and time-aware adversarial attack framework for perception of autonomous driving"},{"paperId":"95ab040c47adfcb6cd1fe856f489c44273d324b0","externalIds":{"DBLP":"journals/tifs/HuLFW24","DOI":"10.1109/TIFS.2024.3402167","CorpusId":269855138},"title":"Toward Transferable Attack via Adversarial Diffusion in Face Recognition"},{"paperId":"18ab1f02016c0a062d893df2511fcacddfcf38cd","externalIds":{"DBLP":"journals/tifs/XuFZZ24","DOI":"10.1109/TIFS.2024.3380248","CorpusId":268716108},"title":"A Unified Optimization Framework for Feature-Based Transferable Attacks"},{"paperId":"cf897eb4d8c2a42c12177211fc983ab38ce8b93d","externalIds":{"DBLP":"journals/ojcomps/LiZWS23","DOI":"10.1109/OJCS.2023.3267221","CorpusId":258157415},"title":"Backdoor Attacks to Deep Learning Models and Countermeasures: A Survey"},{"paperId":"31d01efe53253c1204321a9ace3539a961b9023e","externalIds":{"DBLP":"conf/uss/Jiang00XL023","CorpusId":260777897},"title":"GlitchHiker: Uncovering Vulnerabilities of Image Signal Transmission with IEMI"},{"paperId":"ca16d6ce2e0b1c5ba1eec27dfb1a25a026d042dc","externalIds":{"DBLP":"journals/access/XiaCYM23","DOI":"10.1109/ACCESS.2023.3238823","CorpusId":256218299},"title":"Poisoning Attacks in Federated Learning: A Survey"},{"paperId":"6f6f9f1632907ca5e8e8cb644fb5929ccf7debd6","externalIds":{"DBLP":"conf/uss/TaoA0S023","CorpusId":266691382},"title":"Hard-label Black-box Universal Adversarial Patch Attack"},{"paperId":"62fbdb848e52defa9ab1dafca687250c75d56e72","externalIds":{"DBLP":"journals/comsur/WangSLYNHP23","DOI":"10.1109/COMST.2023.3319492","CorpusId":263206990},"title":"Adversarial Attacks and Defenses in Machine Learning-Empowered Communication Systems and Networks: A Contemporary Survey"},{"paperId":"8d666f200b1c59121aef0c9552037fc5486ec929","externalIds":{"DBLP":"conf/uss/Yang0SSZ023","CorpusId":266160399},"title":"Towards a General Video-based Keystroke Inference Attack"},{"paperId":"07354de7d79313bee393c5873b06c2f4fbcd5c04","externalIds":{"DBLP":"journals/corr/abs-2307-12872","DOI":"10.48550/arXiv.2307.12872","CorpusId":283070547},"title":"Data-free Black-box Attack based on Diffusion Model"},{"paperId":"84a33d6966cbb2cf8f5192087b286122e806a242","externalIds":{"DBLP":"conf/uss/PanZSZ022","CorpusId":252972254},"title":"Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation"},{"paperId":"b2f9b3b41879571c69d9fd8c0fb3642160515c7e","externalIds":{"MAG":"3097631606","DBLP":"journals/ijon/KavianiS21","DOI":"10.1016/j.neucom.2020.07.133","CorpusId":228823392},"title":"Defense against neural trojan attacks: A survey"},{"paperId":"668abd8f1643cde74d2bbe83909837943bdcc1be","externalIds":{"DBLP":"conf/aces/AhmedK21","DOI":"10.1007/978-981-16-8059-5_36","CorpusId":269387154},"title":"Threats on Machine Learning Technique by Data Poisoning Attack: A Survey"},{"paperId":"7a688c1968c4475a330b5704cf27c29dc0e796d4","externalIds":{"DBLP":"conf/ndss/YanLZGZ20","MAG":"3007913795","DOI":"10.14722/ndss.2020.24068","CorpusId":211268709},"title":"SurfingAttack: Interactive Hidden Attack on Voice Assistants Using Ultrasonic Guided Waves"},{"paperId":"bde997305a544f472516bc36ec213fdc48dce8db","externalIds":{"MAG":"3048605852","DBLP":"conf/uss/AhmedKHKOK20","CorpusId":215536246},"title":"Void: A fast and light voice liveness detection system"},{"paperId":"a39767bc5080dd184b6e488a1fe86790c26f0393","externalIds":{"MAG":"3109309187","DBLP":"conf/acml/WuZ20","CorpusId":227179997},"title":"Towards Understanding and Improving the Transferability of Adversarial Examples in Deep Neural Networks"},{"paperId":"858aac30b7cd863c2b3eec1b6885c79856141519","externalIds":{"MAG":"3048796438","DBLP":"conf/uss/ChenYZ0Z0020","CorpusId":211543770},"title":"Devil's Whisper: A General Approach for Physical Adversarial Attacks against Commercial Black-box Speech Recognition Devices"},{"paperId":"269c1bf9e9bd49e022453a2a72508fb57b7fc0db","externalIds":{"DBLP":"conf/uss/XiaoCS0019","MAG":"2965628707","CorpusId":199525454},"title":"Seeing is Not Believing: Camouflage Attacks on Image Scaling Algorithms"}]}