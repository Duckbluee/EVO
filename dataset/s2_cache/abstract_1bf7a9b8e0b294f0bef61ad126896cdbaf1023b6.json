{"abstract":"The increasing complexity of deep neural networks poses significant barriers to democratizing AI to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution that enables device-server co-training through model splitting. However, although system optimization substantially influences the performance of SFL, the problem remains largely uncharted. In this paper, we first provide a unified convergence analysis of SFL, which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on its learning performance, laying a theoretical foundation for this field. Based on this convergence bound, we introduce AdaptSFL, an adaptive SFL framework to accelerate SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls MS and client-side MA to balance communication-computing latency and training convergence. Extensive simulations across various datasets validate that our proposed AdaptSFL framework takes considerably less time to achieve target accuracy than existing benchmarks."}