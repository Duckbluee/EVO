{"references":[{"paperId":"1991666b99d1f9781aab32af504168748c117557","externalIds":{"ArXiv":"2402.12030","DBLP":"journals/corr/abs-2402-12030","DOI":"10.48550/arXiv.2402.12030","CorpusId":267750227},"title":"Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs"},{"paperId":"602f041b7ba9f1bc0bd30d9f26c8227026e812be","externalIds":{"ArXiv":"2402.11890","DBLP":"conf/acl/Zhong00L0T24","DOI":"10.48550/arXiv.2402.11890","CorpusId":267750520},"title":"Revisiting Knowledge Distillation for Autoregressive Language Models"},{"paperId":"8c95d8c8e34bf4cd778384fa49feaef54514cb9e","externalIds":{"DBLP":"conf/acl/LiCCZ24","ArXiv":"2402.10614","DOI":"10.48550/arXiv.2402.10614","CorpusId":267740270},"title":"Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements"},{"paperId":"ee85c7c666135f4aae32336968f09584029b6a35","externalIds":{"DBLP":"conf/acl/LiCCHGZ24","ArXiv":"2402.10110","DOI":"10.48550/arXiv.2402.10110","CorpusId":267682220},"title":"Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning"},{"paperId":"4fe4f0f9d39d708a6c3d7b8dfbfa2616cd376e1e","externalIds":{"DBLP":"journals/corr/abs-2402-06457","ArXiv":"2402.06457","DOI":"10.48550/arXiv.2402.06457","CorpusId":267617275},"title":"V-STaR: Training Verifiers for Self-Taught Reasoners"},{"paperId":"4c2d8df556589ff4fbb5ee68c1f45bff3786624f","externalIds":{"DBLP":"conf/nips/Ji0LHZPQD024","ArXiv":"2402.02416","DOI":"10.52202/079017-2884","CorpusId":267412276},"title":"Aligner: Efficient Alignment by Learning to Correct"},{"paperId":"bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e","externalIds":{"ArXiv":"2402.01364","DBLP":"journals/corr/abs-2402-01364","DOI":"10.48550/arXiv.2402.01364","CorpusId":267406164},"title":"Continual Learning for Large Language Models: A Survey"},{"paperId":"e9aec062906c7fb16e540dc9fb7ed2cbcf129407","externalIds":{"DBLP":"journals/corr/abs-2402-00530","ArXiv":"2402.00530","DOI":"10.48550/arXiv.2402.00530","CorpusId":267365346},"title":"Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning"},{"paperId":"fd5331eb998ce21acb272a3d71ad88cca245e52f","externalIds":{"DBLP":"journals/corr/abs-2401-14367","ArXiv":"2401.14367","DOI":"10.48550/arXiv.2401.14367","CorpusId":267211959},"title":"Genie: Achieving Human Parity in Content-Grounded Datasets Generation"},{"paperId":"993611f5d9f1585c861c9806f8df84d2cdbb19d0","externalIds":{"DBLP":"conf/icml/ZhaoHC24","ArXiv":"2401.12200","DOI":"10.48550/arXiv.2401.12200","CorpusId":267069084},"title":"APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference"},{"paperId":"6d9d552af11f333b56158b4c4a3ccc236820eca1","externalIds":{"ArXiv":"2401.12292","DBLP":"journals/corr/abs-2401-12292","DOI":"10.48550/arXiv.2401.12292","CorpusId":267094842},"title":"GRATH: Gradual Self-Truthifying for Large Language Models"},{"paperId":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","externalIds":{"ArXiv":"2401.10727","DBLP":"journals/corr/abs-2401-10727","DOI":"10.1109/WACV61041.2025.00650","CorpusId":267060838},"title":"MLLM-Tool: A Multimodal Large Language Model for Tool Agent Learning"},{"paperId":"8c1243e089621d09025e1e51e8e01cb2cb20eabf","externalIds":{"ArXiv":"2401.10491","DBLP":"conf/iclr/WanH0QB024","DOI":"10.48550/arXiv.2401.10491","CorpusId":267061245},"title":"Knowledge Fusion of Large Language Models"},{"paperId":"04d64be16fb402f28348faffef484bd419c8bd8f","externalIds":{"ArXiv":"2401.10020","DBLP":"journals/corr/abs-2401-10020","DOI":"10.48550/arXiv.2401.10020","CorpusId":267035293},"title":"Self-Rewarding Language Models"},{"paperId":"09d4808857397bc858c6cb6fc46989a9776819c0","externalIds":{"ArXiv":"2401.07950","DBLP":"conf/nips/ZhangHZDYWYD024","DOI":"10.52202/079017-0046","CorpusId":266999634},"title":"SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models"},{"paperId":"ff61aef2fef3a235bfaa123158a990c4f5f27d1a","externalIds":{"ACL":"2024.emnlp-main.929","DBLP":"journals/corr/abs-2401-07324","ArXiv":"2401.07324","DOI":"10.48550/arXiv.2401.07324","CorpusId":266999372},"title":"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent"},{"paperId":"51c159c838c06f756bdbf7940aa3c1fb1168356e","externalIds":{"ArXiv":"2401.07013","CorpusId":266998882},"title":"Knowledge Distillation of Black-Box Large Language Models"},{"paperId":"76030473be6a8495125d6c2ab58446617048d36f","externalIds":{"ArXiv":"2401.06477","DBLP":"journals/corr/abs-2401-06477","DOI":"10.48550/arXiv.2401.06477","CorpusId":266977233},"title":"Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation"},{"paperId":"bf21281f3faba32b275012bc90b10e7e988b8867","externalIds":{"DBLP":"conf/naacl/YuanSCTSRLY25","ArXiv":"2401.06201","DOI":"10.48550/arXiv.2401.06201","CorpusId":266977201},"title":"EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction"},{"paperId":"59084df7203c6be33838ba3e3854eb9bda053ed2","externalIds":{"DBLP":"journals/corr/abs-2401-06081","ArXiv":"2401.06081","DOI":"10.48550/arXiv.2401.06081","CorpusId":266933254},"title":"Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint"},{"paperId":"c064c79e3026f81e5043cd5b0f4264b4d43336e6","externalIds":{"ArXiv":"2401.06199","DBLP":"journals/corr/abs-2401-06199","DOI":"10.1101/2023.07.05.547496","CorpusId":259502990},"title":"xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein"},{"paperId":"fb4dc0178e5d7347b1615c48caf05347b6e5eb48","externalIds":{"DBLP":"journals/corr/abs-2401-05561","ArXiv":"2401.05561","DOI":"10.48550/arXiv.2401.05561","CorpusId":266933236},"title":"TrustLLM: Trustworthiness in Large Language Models"},{"paperId":"f5b077e01f6e3d91f58cb4ed7158fa61eec5a1f8","externalIds":{"ArXiv":"2401.05268","DBLP":"conf/acl/Qiao0FLZJLC24","DOI":"10.18653/v1/2024.acl-long.165","CorpusId":266902590},"title":"AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning"},{"paperId":"60f47874beb5f00364f8621410d1d34c37d11007","externalIds":{"DBLP":"journals/corr/abs-2401-01916","ArXiv":"2401.01916","DOI":"10.3847/2515-5172/ad1abe","CorpusId":266755857},"title":"AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets"},{"paperId":"04a340b15945c70e642227bf249639b171beb3f8","externalIds":{"DBLP":"journals/corr/abs-2401-01600","ArXiv":"2401.01600","DOI":"10.48550/arXiv.2401.01600","CorpusId":266741610},"title":"PLLaMa: An Open-source Large Language Model for Plant Science"},{"paperId":"ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5","externalIds":{"DBLP":"conf/icml/ChenDYJG24","ArXiv":"2401.01335","DOI":"10.48550/arXiv.2401.01335","CorpusId":266725672},"title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"},{"paperId":"6d408c2c56a73cec8b21c450b6588c6d98026bc3","externalIds":{"DBLP":"journals/corr/abs-2401-00434","ArXiv":"2401.00434","DOI":"10.48550/arXiv.2401.00434","CorpusId":266693296},"title":"GeoGalactica: A Scientific Large Language Model in Geoscience"},{"paperId":"59c436bbdcc524e1c8a07d1d24fa3ef122d7d9fa","externalIds":{"ArXiv":"2312.17055","DBLP":"conf/acl/QinXJ0HD0J25","DOI":"10.18653/v1/2025.acl-long.1573","CorpusId":266573858},"title":"Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning"},{"paperId":"41113411e1748a34bb80f12c761b7af1ed6dbb90","externalIds":{"ArXiv":"2312.15685","DBLP":"conf/iclr/0131Z00H24","DOI":"10.48550/arXiv.2312.15685","CorpusId":266551413},"title":"What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning"},{"paperId":"718eeca371e1f533614f8b530a40cfdfcab9c330","externalIds":{"DBLP":"journals/corr/abs-2312-15692","ArXiv":"2312.15692","DOI":"10.48550/arXiv.2312.15692","CorpusId":266551187},"title":"Instruction Fusion: Advancing Prompt Evolution through Hybridization"},{"paperId":"3713112311efbcf785de17fa86e5bf42e4360f77","externalIds":{"ArXiv":"2312.11370","DBLP":"conf/iclr/GaoPZYZ0HHXLK25","DOI":"10.48550/arXiv.2312.11370","CorpusId":266359733},"title":"G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model"},{"paperId":"1bd9466f0bb10d29a16f614943ec7823e13cb210","externalIds":{"DBLP":"journals/corr/abs-2312-10730","ArXiv":"2312.10730","DOI":"10.48550/arXiv.2312.10730","CorpusId":266359672},"title":"Mixed Distillation Helps Smaller Language Model Better Reasoning"},{"paperId":"2b14d9e190022e388476ebb24eb1a84349ca0de4","externalIds":{"DBLP":"journals/corr/abs-2312-10665","ArXiv":"2312.10665","DOI":"10.48550/arXiv.2312.10665","CorpusId":266348439},"title":"Silkie: Preference Distillation for Large Visual Language Models"},{"paperId":"7a31971b0af439dec6fc484cca20df57f440b644","externalIds":{"DBLP":"journals/corr/abs-2312-10302","ArXiv":"2312.10302","DOI":"10.48550/arXiv.2312.10302","CorpusId":266348323},"title":"One Shot Learning as Instruction Data Prospector for Large Language Models"},{"paperId":"6b97aa78bcdb88548c44e7e1671c0ed37ed37976","externalIds":{"ArXiv":"2312.09390","DBLP":"conf/icml/BurnsIKBGACEJLS24","DOI":"10.48550/arXiv.2312.09390","CorpusId":266312608},"title":"Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"},{"paperId":"6cfbbf7604adda1df65932e3c4d157770a2df000","externalIds":{"DBLP":"journals/corr/abs-2312-07000","ArXiv":"2312.07000","DOI":"10.48550/arXiv.2312.07000","CorpusId":266174420},"title":"Alignment for Honesty"},{"paperId":"48362b169a235ca650918c489c8cea4c597da645","externalIds":{"DBLP":"journals/tmlr/SinghCAAPGLH0XP24","ArXiv":"2312.06585","DOI":"10.48550/arXiv.2312.06585","CorpusId":266163375},"title":"Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"},{"paperId":"5c17fa02a4a4c0655b1873cf3e34fffbc1e7d601","externalIds":{"ArXiv":"2312.04837","DBLP":"journals/corr/abs-2312-04837","DOI":"10.48550/arXiv.2312.04837","CorpusId":266149843},"title":"Localized Symbolic Knowledge Distillation for Visual Commonsense Models"},{"paperId":"5851121df5ce46be5faea265c868ec0beabfce96","externalIds":{"DBLP":"journals/corr/abs-2312-03863","ArXiv":"2312.03863","DOI":"10.48550/arXiv.2312.03863","CorpusId":266044196},"title":"Efficient Large Language Models: A Survey"},{"paperId":"a2b150e02306038389f5df683428f5a4659a468e","externalIds":{"DBLP":"conf/iclr/Lou0XSAX0024","ArXiv":"2312.02436","DOI":"10.48550/arXiv.2312.02436","CorpusId":265659379},"title":"MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following"},{"paperId":"621610b527e0f4ee391d9d8f98a4a0a214e757e8","externalIds":{"ArXiv":"2312.03755","DBLP":"journals/corr/abs-2312-03755","DOI":"10.48550/arXiv.2312.03755","CorpusId":266055146},"title":"Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data and Large-Language Models"},{"paperId":"425f1edd88fe3539c40ddd93c3e07c95de67ba00","externalIds":{"ArXiv":"2311.16714","DBLP":"journals/corr/abs-2311-16714","DOI":"10.1109/CVPR52733.2024.02482","CorpusId":265466250},"title":"Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld"},{"paperId":"2b3554a8fea6f123fc04bd3e120f2293f227e1b2","externalIds":{"ArXiv":"2311.16208","DBLP":"journals/corr/abs-2311-16208","DOI":"10.48550/arXiv.2311.16208","CorpusId":265466509},"title":"InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery"},{"paperId":"e3f7ad05b1652c6ada78cffbe405bceb723bc70c","externalIds":{"ArXiv":"2311.15653","DBLP":"journals/corr/abs-2311-15653","DOI":"10.48550/arXiv.2311.15653","CorpusId":265457248},"title":"MoDS: Model-oriented Data Selection for Instruction Tuning"},{"paperId":"321160254c3b059ec7aed0bcbd7d7c6d1be17372","externalIds":{"ArXiv":"2312.03718","DBLP":"journals/corr/abs-2312-03718","DOI":"10.48550/arXiv.2312.03718","CorpusId":266054920},"title":"Large Language Models in Law: A Survey"},{"paperId":"e1b7f13051bec4a713c6226c177b63599d49e3e9","externalIds":{"ArXiv":"2311.14904","DBLP":"journals/corr/abs-2311-14904","DOI":"10.48550/arXiv.2311.14904","CorpusId":265456227},"title":"LLM-Assisted Code Cleaning For Training Accurate Code Generators"},{"paperId":"33eed6ea6805b59617fed7c41ef5825b5e4d1621","externalIds":{"DBLP":"journals/corr/abs-2311-11315","ArXiv":"2311.11315","DOI":"10.48550/arXiv.2311.11315","CorpusId":265294410},"title":"TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems"},{"paperId":"288e64e8adb23d81e291a2cb51e3a56b315023b7","externalIds":{"DBLP":"conf/naacl/YuGW24","ArXiv":"2311.09724","DOI":"10.18653/v1/2024.findings-naacl.55","CorpusId":265221057},"title":"OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning"},{"paperId":"2a86d281bef364e2ea2d4fc61fde46ca25b955f1","externalIds":{"ArXiv":"2311.09774","DBLP":"journals/corr/abs-2311-09774","DOI":"10.48550/arXiv.2311.09774","CorpusId":265221365},"title":"HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs"},{"paperId":"907cc13d907b2dfc39f6bb9e0c64cc69e71baefa","externalIds":{"DBLP":"journals/corr/abs-2311-09214","ArXiv":"2311.09214","ACL":"2024.naacl-long.376","DOI":"10.48550/arXiv.2311.09214","CorpusId":265212690},"title":"Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models"},{"paperId":"9c4980946bfa544785fa4e961dc40a60bc3d8fb9","externalIds":{"ArXiv":"2311.08213","DBLP":"journals/corr/abs-2311-08213","DOI":"10.48550/arXiv.2311.08213","CorpusId":265158100},"title":"Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models"},{"paperId":"619184447595337a9fe3dca72c4e951e7ab7467c","externalIds":{"DBLP":"journals/corr/abs-2311-07574","ArXiv":"2311.07574","DOI":"10.48550/arXiv.2311.07574","CorpusId":265150580},"title":"To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning"},{"paperId":"a8568bac56c24b5d25e373a117f947171d5f97be","externalIds":{"DBLP":"journals/corr/abs-2311-06503","ArXiv":"2311.06503","DOI":"10.48550/arXiv.2311.06503","CorpusId":265150374},"title":"Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering"},{"paperId":"0a29191d66a129709980cbe3c937aa9e98707dc8","externalIds":{"DBLP":"conf/sc/YinDWS23","DOI":"10.1145/3581784.3613215","CorpusId":264591559},"title":"FORGE: Pre-Training Open Foundation Models for Science"},{"paperId":"e51f20efb872d0ba99a8b501259948bbb2f6963f","externalIds":{"DBLP":"conf/iclr/GuoZT0W24","ArXiv":"2311.04072","DOI":"10.48550/arXiv.2311.04072","CorpusId":265043685},"title":"Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment"},{"paperId":"15d3a18d5ec49b13af446782aaa4dfbccdd9ec7a","externalIds":{"ArXiv":"2311.02805","DBLP":"journals/corr/abs-2311-02805","DOI":"10.48550/arXiv.2311.02805","CorpusId":265034089},"title":"Tailoring Self-Rationalizers with Multi-Reward Distillation"},{"paperId":"0a27dde07d28ca8d92ed46cecc71585e1c9693f2","externalIds":{"DBLP":"journals/corr/abs-2311-02303","ArXiv":"2311.02303","DOI":"10.48550/arXiv.2311.02303","CorpusId":265033510},"title":"MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"},{"paperId":"751855a18fceb0c613d7c0366824560d2077eb14","externalIds":{"ArXiv":"2311.01555","DBLP":"journals/corr/abs-2311-01555","DOI":"10.48550/arXiv.2311.01555","CorpusId":265019321},"title":"Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers"},{"paperId":"5aa3b1009955ce2c8f896e0d5e94e06155ef1e43","externalIds":{"ArXiv":"2311.00423","DBLP":"journals/corr/abs-2311-00423","DOI":"10.1145/3616855.3635853","CorpusId":264832979},"title":"LLMRec: Large Language Models with Graph Augmentation for Recommendation"},{"paperId":"c559a8c6bab06be554025be3d1014070c35eb850","externalIds":{"DBLP":"journals/corr/abs-2310-17903","ArXiv":"2310.17903","DOI":"10.1145/3748647","CorpusId":264555289},"title":"Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey"},{"paperId":"9c0102443a1b5adc0c2235fab23a80bf8122ce72","externalIds":{"DBLP":"journals/corr/abs-2310-16271","ArXiv":"2310.16271","DOI":"10.48550/arXiv.2310.16271","CorpusId":264487383},"title":"CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment"},{"paperId":"cdcf3f36866ef1e16eba26d57c2324362247ba84","externalIds":{"DBLP":"journals/corr/abs-2310-16944","ArXiv":"2310.16944","DOI":"10.48550/arXiv.2310.16944","CorpusId":264490502},"title":"Zephyr: Direct Distillation of LM Alignment"},{"paperId":"4829b73a47be18f73e9e8d90f3c23c8f84d0fccb","externalIds":{"ArXiv":"2310.15950","DBLP":"conf/www/RenWXSCWY024","DOI":"10.1145/3589334.3645458","CorpusId":264439548},"title":"Representation Learning with Large Language Models for Recommendation"},{"paperId":"15a2682ba1b479dea284062dd097a9a349a2eceb","externalIds":{"ArXiv":"2310.14558","DBLP":"journals/corr/abs-2310-14558","DOI":"10.48550/arXiv.2310.14558","CorpusId":264426685},"title":"AlpaCare: Instruction-tuned Large Language Models for Medical Application"},{"paperId":"75a85d74433d03a78a07bb95f6f261323a79eb80","externalIds":{"DBLP":"journals/corr/abs-2310-14510","ArXiv":"2310.14510","DOI":"10.48550/arXiv.2310.14510","CorpusId":264426357},"title":"CITB: A Benchmark for Continual Instruction Tuning"},{"paperId":"ce2272a439ba89f1ea0a822c8d19078732d75e5d","externalIds":{"DBLP":"journals/corr/abs-2310-14151","ArXiv":"2310.14151","DOI":"10.48550/arXiv.2310.14151","CorpusId":264426196},"title":"PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain"},{"paperId":"ba136326385bbd69680534c0312c7ba12f0c155e","externalIds":{"DBLP":"conf/cikm/ZhangPTS23","DOI":"10.1145/3583780.3615993","CorpusId":264350291},"title":"Unleashing the Power of Large Language Models for Legal Applications"},{"paperId":"525d4aee811dcfdfd11afe7d0ae9204f03c8a74e","externalIds":{"ArXiv":"2310.14029","DBLP":"journals/corr/abs-2310-14029","DOI":"10.48550/arXiv.2310.14029","CorpusId":264426172},"title":"LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions"},{"paperId":"938d1028b3c3cd4e3d34eed20b622bdc33453f6e","externalIds":{"ArXiv":"2310.13596","DBLP":"journals/corr/abs-2310-13596","DOI":"10.48550/arXiv.2310.13596","CorpusId":264405801},"title":"MarineGPT: Unlocking Secrets of Ocean to the Public"},{"paperId":"46fe9ce789408b8a50fb4259e6bf0cc5855f4ed5","externalIds":{"ArXiv":"2310.12823","DBLP":"conf/acl/ZengLLWLD024","DOI":"10.48550/arXiv.2310.12823","CorpusId":264306101},"title":"AgentTuning: Enabling Generalized Agent Abilities for LLMs"},{"paperId":"6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc","externalIds":{"DBLP":"journals/corr/abs-2310-12931","ArXiv":"2310.12931","DOI":"10.48550/arXiv.2310.12931","CorpusId":264306288},"title":"Eureka: Human-Level Reward Design via Coding Large Language Models"},{"paperId":"ddbd8fe782ac98e9c64dd98710687a962195dd9b","externalIds":{"DBLP":"conf/iclr/AsaiWWSH24","ArXiv":"2310.11511","CorpusId":264288947},"title":"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"},{"paperId":"b16c7d45183b9d595ab64301be019741b1528860","externalIds":{"DBLP":"conf/iclr/AzerbayevSPSMJD24","ArXiv":"2310.10631","DOI":"10.48550/arXiv.2310.10631","CorpusId":264172303},"title":"Llemma: An Open Language Model For Mathematics"},{"paperId":"1e98a9532d4e1fcf947d5b215e2cfabbf6cc41e0","externalIds":{"ArXiv":"2310.08582","DBLP":"journals/corr/abs-2310-08582","DOI":"10.48550/arXiv.2310.08582","CorpusId":263909090},"title":"Tree-Planner: Efficient Close-loop Task Planning with Large Language Models"},{"paperId":"d65c8066ab553d4d7ed70c567f50af2d5d875a82","externalIds":{"ArXiv":"2310.07838","DBLP":"journals/corr/abs-2310-07838","DOI":"10.48550/arXiv.2310.07838","CorpusId":263908983},"title":"Towards the Fundamental Limits of Knowledge Transfer over Finite Domains"},{"paperId":"fafb8bb0cf43681749dbb0ddd3325d46d529198d","externalIds":{"DBLP":"journals/corr/abs-2310-07652","ArXiv":"2310.07652","DOI":"10.48550/arXiv.2310.07652","CorpusId":263834585},"title":"LLM4Vis: Explainable Visualization Recommendation using ChatGPT"},{"paperId":"5160224f7daf64fd490ed6d517bef316e383a311","externalIds":{"ArXiv":"2310.07328","DBLP":"journals/corr/abs-2310-07328","DOI":"10.48550/arXiv.2310.07328","CorpusId":263834601},"title":"An Empirical Study of Instruction-tuning Large Language Models in Chinese"},{"paperId":"5001630bcc65e8e0e621b19625629a2689724743","externalIds":{"DBLP":"conf/iclr/LiSYF0024","ArXiv":"2310.05470","DOI":"10.48550/arXiv.2310.05470","CorpusId":263829791},"title":"Generative Judge for Evaluating Alignment"},{"paperId":"67daf8c4fe1958d20ebdf95c2a36dd490c73836f","externalIds":{"DBLP":"journals/corr/abs-2310-05915","ArXiv":"2310.05915","DOI":"10.48550/arXiv.2310.05915","CorpusId":263829338},"title":"FireAct: Toward Language Agent Fine-tuning"},{"paperId":"d0e1a78bd77c9b6179bc773f0b7bada8df3ce39f","externalIds":{"DBLP":"conf/iui/0002SS025","ArXiv":"2310.04869","DOI":"10.1145/3708359.3712129","CorpusId":263830178},"title":"ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations"},{"paperId":"bd09391fbd124dc0c0a6be5d0ab2eb5d9c43fbac","externalIds":{"DBLP":"journals/corr/abs-2310-04793","ArXiv":"2310.04793","DOI":"10.48550/arXiv.2310.04793","CorpusId":263829590},"title":"FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"d6354e91d8dcf73bff50097b76a81de874f7bd7a","externalIds":{"ArXiv":"2310.02031","DBLP":"journals/corr/abs-2310-02031","DOI":"10.48550/arXiv.2310.02031","CorpusId":263608392},"title":"OceanGPT: A Large Language Model for Ocean Science Tasks"},{"paperId":"d238a9770d24d0725656ef6cf4789afebf2126e7","externalIds":{"ArXiv":"2310.00752","DBLP":"journals/tmlr/JiangLZHLC24","DOI":"10.48550/arXiv.2310.00752","CorpusId":263334281},"title":"TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks"},{"paperId":"b272513916b45c8517d289d7abee4a53e6832187","externalIds":{"DBLP":"conf/iclr/GouSGSYHDC24","ArXiv":"2309.17452","DOI":"10.48550/arXiv.2309.17452","CorpusId":263310365},"title":"ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"},{"paperId":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","externalIds":{"ArXiv":"2309.17428","DBLP":"journals/corr/abs-2309-17428","DOI":"10.48550/arXiv.2309.17428","CorpusId":263310662},"title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"},{"paperId":"ba03ca8faa9f01cd9d26b80f08d421376f70de22","externalIds":{"ArXiv":"2309.15088","DBLP":"journals/corr/abs-2309-15088","DOI":"10.48550/arXiv.2309.15088","CorpusId":262825475},"title":"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models"},{"paperId":"29f032fc875576b5c3c6b1c2d76af8639bacfb88","externalIds":{"ArXiv":"2309.11235","DBLP":"journals/corr/abs-2309-11235","DOI":"10.48550/arXiv.2309.11235","CorpusId":262064307},"title":"OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"},{"paperId":"6806ecad90a778aaa7f6a3cd3a539582d823066c","externalIds":{"DBLP":"journals/corr/abs-2309-11325","ArXiv":"2309.11325","DOI":"10.48550/arXiv.2309.11325","CorpusId":262064568},"title":"DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services"},{"paperId":"6988596f88276920a4e555cbe624e1431bc8a9f7","externalIds":{"DBLP":"conf/iclr/KothaSR24","ArXiv":"2309.10105","DOI":"10.48550/arXiv.2309.10105","CorpusId":262054014},"title":"Understanding Catastrophic Forgetting in Language Models via Implicit Inference"},{"paperId":"aa638d5ffb01a84bed00d4c890da3cfcb386a8fa","externalIds":{"ArXiv":"2309.09530","CorpusId":262044959},"title":"Adapting Large Language Models to Domains via Reading Comprehension"},{"paperId":"62b4e06f5249d22e4a153ec4a2dc934c6a014372","externalIds":{"DBLP":"conf/iclr/Guo0LYCBPHCZSZZ24","ArXiv":"2309.09298","DOI":"10.48550/arXiv.2309.09298","CorpusId":262043747},"title":"OWL: A Large Language Model for IT Operations"},{"paperId":"844bc3b26b5c63ec3b251ae634c194dcfb41a7d2","externalIds":{"DBLP":"journals/corr/abs-2309-13064","ArXiv":"2309.13064","DOI":"10.48550/arXiv.2309.13064","CorpusId":262459267},"title":"InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning"},{"paperId":"3647f94be510f73dea1b0ee2dcef968cfcac9fe3","externalIds":{"DBLP":"journals/corr/abs-2309-06917","ArXiv":"2309.06917","DOI":"10.48550/arXiv.2309.06917","CorpusId":261705938},"title":"Continual Learning with Dirichlet Generative-based Rehearsal"},{"paperId":"4b508ba98a180f27fd93b702d7044adad91620eb","externalIds":{"DBLP":"conf/emnlp/XuT0XXLLM24","ArXiv":"2309.06275","ACL":"2024.emnlp-main.871","DOI":"10.18653/v1/2024.emnlp-main.871","CorpusId":261696483},"title":"Re-Reading Improves Reasoning in Large Language Models"},{"paperId":"e33a538e80d1877782df26e1493f5adc661ceec4","externalIds":{"DBLP":"journals/corr/abs-2309-06126","ArXiv":"2309.06126","ACL":"2023.wiesp-1.7","DOI":"10.48550/arXiv.2309.06126","CorpusId":261696577},"title":"AstroLLaMA: Towards Specialized Foundation Models in Astronomy"},{"paperId":"8f4ae6552e49d229eaf374156f9197e980fa7df8","externalIds":{"ArXiv":"2309.06089","DBLP":"journals/corr/abs-2309-06089","DOI":"10.48550/arXiv.2309.06089","CorpusId":261696642},"title":"Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","externalIds":{"DBLP":"conf/iclr/YueQZFH00C24","ArXiv":"2309.05653","DOI":"10.48550/arXiv.2309.05653","CorpusId":261696697},"title":"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"},{"paperId":"e26888285436bc7998e5c95102a9beb60144be5e","externalIds":{"DBLP":"journals/corr/abs-2309-05463","ArXiv":"2309.05463","DOI":"10.48550/arXiv.2309.05463","CorpusId":261696657},"title":"Textbooks Are All You Need II: phi-1.5 technical report"},{"paperId":"74b4b993babe99bc5f5c589c27fef0f1baba606b","externalIds":{"DBLP":"journals/corr/abs-2309-02144","ArXiv":"2309.02144","DOI":"10.48550/arXiv.2309.02144","CorpusId":261558535},"title":"Making Large Language Models Better Reasoners with Alignment"},{"paperId":"600ff4c4ae9fc506c86673c5ecce4fa90803e987","externalIds":{"DBLP":"conf/icml/0001PMMFLBHCRP24","ArXiv":"2309.00267","CorpusId":261493811},"title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","externalIds":{"DBLP":"conf/eccv/XuWWCPL24","ArXiv":"2308.16911","DOI":"10.48550/arXiv.2308.16911","CorpusId":261397321},"title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"401510c31725ea9277a4d2e049682578a0134e1c","externalIds":{"DBLP":"journals/corr/abs-2308-14731","ArXiv":"2308.14731","DOI":"10.1007/s10515-024-00421-4","CorpusId":261276325},"title":"Distilled GPT for source code summarization"},{"paperId":"4c5b4a8e31d3119c1e3b5753693ff283c9717218","externalIds":{"DBLP":"journals/corr/abs-2308-14346","ArXiv":"2308.14346","DOI":"10.48550/arXiv.2308.14346","CorpusId":261243110},"title":"DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation"},{"paperId":"3b36d16985286b03e06e8404a7be49a9713d37b9","externalIds":{"DBLP":"journals/corr/abs-2308-14034","ArXiv":"2308.14034","DOI":"10.48550/arXiv.2308.14034","CorpusId":261243312},"title":"Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum"},{"paperId":"0b0debb710366cdff461938c80763eace1651af6","externalIds":{"DBLP":"journals/corr/abs-2308-12950","ArXiv":"2308.12950","DOI":"10.48550/arXiv.2308.12950","CorpusId":261100919},"title":"Code Llama: Open Foundation Models for Code"},{"paperId":"e3052ebca5eeae6a8a73e44517903d39746f5f3a","externalIds":{"ArXiv":"2308.12032","ACL":"2024.naacl-long.421","DBLP":"journals/corr/abs-2308-12032","DOI":"10.18653/v1/2024.naacl-long.421","CorpusId":261076515},"title":"From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"},{"paperId":"f8b90d640158f61c4553518a8554a73b540e07e7","externalIds":{"ArXiv":"2308.12014","DBLP":"journals/corr/abs-2308-12014","DOI":"10.48550/arXiv.2308.12014","CorpusId":261076203},"title":"From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models"},{"paperId":"da96ec9c32d63292e506ba8f8ea8e838df998c02","externalIds":{"DBLP":"journals/corr/abs-2308-10253","ArXiv":"2308.10253","DOI":"10.48550/arXiv.2308.10253","CorpusId":261049617},"title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","externalIds":{"DBLP":"journals/corr/abs-2308-09583","ArXiv":"2308.09583","DOI":"10.48550/arXiv.2308.09583","CorpusId":261030818},"title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"e3fd89a7f6b28973cfc68bfc51caebd8fb93f0bc","externalIds":{"DBLP":"journals/corr/abs-2308-09442","ArXiv":"2308.09442","DOI":"10.48550/arXiv.2308.09442","CorpusId":261030404},"title":"BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine"},{"paperId":"838cd69a0b6c9c244a6eebb0f4742c0625132de6","externalIds":{"DBLP":"journals/corr/abs-2308-08747","ArXiv":"2308.08747","DOI":"10.1109/TASLPRO.2025.3606231","CorpusId":261031244},"title":"An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-Tuning"},{"paperId":"5df24ed6fdf10d1e92885687abce7bd5e56f3f85","externalIds":{"ArXiv":"2308.08833","DBLP":"conf/naacl/WangCS0CXCJLWW024","ACL":"2024.naacl-long.343","DOI":"10.48550/arXiv.2308.08833","CorpusId":261030527},"title":"CMB: A Comprehensive Medical Benchmark in Chinese"},{"paperId":"182c7b40ff7560a5545764814338f55a2098e441","externalIds":{"ArXiv":"2308.08998","DBLP":"journals/corr/abs-2308-08998","CorpusId":261031028},"title":"Reinforced Self-Training (ReST) for Language Modeling"},{"paperId":"7ac38c3398f2696754bec69f296468e7a8237a64","externalIds":{"ArXiv":"2308.09723","DBLP":"journals/corr/abs-2308-09723","DOI":"10.48550/arXiv.2308.09723","CorpusId":261049460},"title":"FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"},{"paperId":"d58c87575a4e00da93be0d63af568ac10532aab4","externalIds":{"DBLP":"journals/corr/abs-2308-06744","ArXiv":"2308.06744","DOI":"10.48550/arXiv.2308.06744","CorpusId":260886785},"title":"Token-Scaled Logit Distillation for Ternary Weight Generative Language Models"},{"paperId":"f2ba9e7d9624bd94a786ea5e3161a9425a21a475","externalIds":{"DBLP":"conf/iclr/LiYZSLZWL24","ArXiv":"2308.06259","DOI":"10.48550/arXiv.2308.06259","CorpusId":260866107},"title":"Self-Alignment with Instruction Backtranslation"},{"paperId":"c74a13b251b6af6dfce49eeb128b1c0e2ddf955d","externalIds":{"DBLP":"conf/coling/Zhao0HYLHZL24","ACL":"2024.lrec-main.1460","ArXiv":"2308.05696","DOI":"10.48550/arXiv.2308.05696","CorpusId":260775760},"title":"Tree-Instruct: A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment"},{"paperId":"be1a943609db984917964aa05372be88cb1da886","externalIds":{"ArXiv":"2308.05361","DBLP":"journals/corr/abs-2308-05361","DOI":"10.48550/arXiv.2308.05361","CorpusId":260775975},"title":"WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine"},{"paperId":"7142e920b6b9355d9cbacc9450818f912eca138e","externalIds":{"ArXiv":"2308.05374","DBLP":"journals/corr/abs-2308-05374","DOI":"10.48550/arXiv.2308.05374","CorpusId":260775522},"title":"Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment"},{"paperId":"c2f9006993d9d84d48eb894aab3ba60f946d0e15","externalIds":{"ArXiv":"2308.02773","DBLP":"journals/corr/abs-2308-02773","DOI":"10.48550/arXiv.2308.02773","CorpusId":260681803},"title":"EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education"},{"paperId":"1dede9d21db0be1c58208e1f970e57aac4fc45f8","externalIds":{"ArXiv":"2308.02019","DBLP":"journals/corr/abs-2308-02019","DOI":"10.48550/arXiv.2308.02019","CorpusId":260611172},"title":"Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","externalIds":{"DBLP":"journals/corr/abs-2307-16789","ArXiv":"2307.16789","DOI":"10.48550/arXiv.2307.16789","CorpusId":260334759},"title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"bf9e20852c04ea5aa28b5a092f7fe8354878bf92","externalIds":{"DBLP":"conf/acl/Wen0DM23","ArXiv":"2307.15190","ACL":"2023.acl-long.605","DOI":"10.48550/arXiv.2307.15190","CorpusId":259370700},"title":"f-Divergence Minimization for Sequence-Level Knowledge Distillation"},{"paperId":"af6d0ba799213cbbcbfceb1fb9b78d2858486308","externalIds":{"DBLP":"journals/corr/abs-2307-14535","ArXiv":"2307.14535","DOI":"10.48550/arXiv.2307.14535","CorpusId":260203080},"title":"Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition"},{"paperId":"6d8fb47bf1022a9169875905aec106d8534e3052","externalIds":{"ArXiv":"2307.14192","DBLP":"journals/corr/abs-2307-14192","DOI":"10.48550/arXiv.2307.14192","CorpusId":260164746},"title":"Unveiling Security, Privacy, and Ethical Concerns of ChatGPT"},{"paperId":"de7e5fee8cf03bd485b1104d3e40e8ab45d76c0a","externalIds":{"ArXiv":"2307.14367","DBLP":"conf/aaai/AbdineCBV24","DOI":"10.1609/aaai.v38i10.28948","CorpusId":260203284},"title":"Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","externalIds":{"DBLP":"conf/ijcai/ZhaoYGYWZSPDH024","ArXiv":"2307.09474","DOI":"10.48550/arXiv.2307.09474","CorpusId":259951197},"title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"c92516b47444b2b18108b8a6e5785d263173789d","externalIds":{"DBLP":"conf/sigir/FerrarettoLLN23","DOI":"10.1145/3539618.3592067","CorpusId":259949767},"title":"ExaRanker: Synthetic Explanations Improve Neural Rankers"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"d44031f253668c61ac6d68b95bbe9cac57730d51","externalIds":{"ArXiv":"2307.08303","DBLP":"journals/kbs/PengWWF25","DOI":"10.48550/arXiv.2307.08303","CorpusId":259937100},"title":"Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models"},{"paperId":"0547b1b984b4051375e4371030731736ea7c44fc","externalIds":{"DBLP":"journals/corr/abs-2307-05779","ArXiv":"2307.05779","DOI":"10.48550/arXiv.2307.05779","CorpusId":259837429},"title":"Neural Machine Translation Data Generation and Augmentation using ChatGPT"},{"paperId":"d62c4d00b277e948956b6610ce2644e88fe1577b","externalIds":{"DBLP":"journals/cacm/Cerf23c","ArXiv":"2307.05782","DOI":"10.1007/978-981-96-6259-3","CorpusId":259837466,"PubMed":"38320147"},"title":"Large Language Models"},{"paperId":"92930ed3560ea6c86d53cf52158bc793b089054d","externalIds":{"ArXiv":"2307.04657","DBLP":"conf/nips/JiLDPZB0SW023","DOI":"10.48550/arXiv.2307.04657","CorpusId":259501579},"title":"BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"},{"paperId":"451a3f03aca4aa87b93981364842137417549e58","externalIds":{"DBLP":"journals/corr/abs-2307-04087","ArXiv":"2307.04087","DOI":"10.48550/arXiv.2307.04087","CorpusId":259501644},"title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","externalIds":{"ArXiv":"2307.03601","DBLP":"conf/eccv/ZhangSCXSZLCL24","DOI":"10.48550/arXiv.2307.03601","CorpusId":259375716},"title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e","externalIds":{"DBLP":"conf/aaai/00010LYHLW24","ArXiv":"2306.17492","DOI":"10.48550/arXiv.2306.17492","CorpusId":259308873},"title":"Preference Ranking Optimization for Human Alignment"},{"paperId":"2d3bc530d8f1ed36932a70bc362ea94d988adec9","externalIds":{"ArXiv":"2306.17563","DBLP":"journals/corr/abs-2306-17563","DOI":"10.48550/arXiv.2306.17563","CorpusId":259309299},"title":"Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting"},{"paperId":"44bdd340aa7d54c3afb1831ffe6b6a8035b41200","externalIds":{"DBLP":"conf/acl-clinicalnlp/WangYMO0023","ArXiv":"2306.16931","ACL":"2023.clinicalnlp-1.49","DOI":"10.48550/arXiv.2306.16931","CorpusId":259287344},"title":"UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?"},{"paperId":"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","externalIds":{"DBLP":"journals/corr/abs-2306-17107","ArXiv":"2306.17107","DOI":"10.48550/arXiv.2306.17107","CorpusId":259287523},"title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"},{"paperId":"78c488e2d84bd193a40006b1fceb03e3845b81d4","externalIds":{"ArXiv":"2306.15895","DBLP":"journals/corr/abs-2306-15895","DOI":"10.48550/arXiv.2306.15895","CorpusId":259275123},"title":"Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias"},{"paperId":"228aee5393e7a11e018bbef940fea1c2816b6ec4","externalIds":{"ArXiv":"2306.16092","CorpusId":259274889},"title":"Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model"},{"paperId":"e2a58fd18961c3941102989e3a3d0d27c615e015","externalIds":{"ArXiv":"2306.15195","DBLP":"journals/corr/abs-2306-15195","DOI":"10.48550/arXiv.2306.15195","CorpusId":259262082},"title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"7d87fbdfbf5038a4e0ff09801b6d3b8a2e0c613a","externalIds":{"DBLP":"conf/nips/ManikandanJK23","ArXiv":"2306.14101","DOI":"10.48550/arXiv.2306.14101","CorpusId":259252065},"title":"Language models are weak learners"},{"paperId":"7a6a298efb965ce9a351a3212f6f536e94dbbb03","externalIds":{"ArXiv":"2306.14050","DBLP":"conf/acl/LiHYRC023","ACL":"2023.acl-long.150","DOI":"10.48550/arXiv.2306.14050","CorpusId":259251773},"title":"Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step"},{"paperId":"a57ef1f5c3af185af79751855b8033b7fc6d89b3","externalIds":{"ArXiv":"2306.13649","DBLP":"conf/iclr/AgarwalVZSGGB24","CorpusId":263610088},"title":"On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes"},{"paperId":"2922768fd451ecdb45f48c1a83eb57f54a91221b","externalIds":{"DBLP":"journals/corr/abs-2306-11644","ArXiv":"2306.11644","CorpusId":259203998},"title":"Textbooks Are All You Need"},{"paperId":"bc8428e270a5474cabfaff578d44955f757ccacd","externalIds":{"ArXiv":"2306.11222","DBLP":"journals/corr/abs-2306-11222","DOI":"10.48550/arXiv.2306.11222","CorpusId":259203385},"title":"LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation"},{"paperId":"c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4","externalIds":{"ArXiv":"2306.10933","DBLP":"conf/recsys/XiLLCZZCT0024","DOI":"10.1145/3640457.3688104","CorpusId":259202547},"title":"Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","externalIds":{"ArXiv":"2306.09093","DBLP":"journals/corr/abs-2306-09093","DOI":"10.48550/arXiv.2306.09093","CorpusId":259165461},"title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"454c8fef2957aa2fb13eb2c7a454393a2ee83805","externalIds":{"DBLP":"journals/corr/abs-2306-08568","ArXiv":"2306.08568","CorpusId":259164815},"title":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","externalIds":{"DBLP":"journals/corr/abs-2306-07207","ArXiv":"2306.07207","DOI":"10.1145/3796716","CorpusId":259138706},"title":"Valley: Video Assistant with Large Language Model Enhanced Ability"},{"paperId":"ed30969f0e4811473144ffe83c1baa6d54f02202","externalIds":{"ArXiv":"2306.06624","CorpusId":261241602},"title":"RestGPT: Connecting Large Language Models with Real-World RESTful APIs"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"32f541216112de78037d8e0f95ddc152eb6f05fa","externalIds":{"DBLP":"conf/wsdm/DengZHCSXF0WZLH24","ArXiv":"2306.05064","DOI":"10.1145/3616855.3635772","CorpusId":259108887},"title":"K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization"},{"paperId":"ccd94602e3acecf999d0c9ba62b1a8bc02e9f696","externalIds":{"ArXiv":"2306.05087","DBLP":"journals/corr/abs-2306-05087","DOI":"10.48550/arXiv.2306.05087","CorpusId":259108266},"title":"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization"},{"paperId":"455866ca838f356b53a7e3e5b344834f9e93dbbc","externalIds":{"ArXiv":"2306.05301","DBLP":"journals/corr/abs-2306-05301","DOI":"10.48550/arXiv.2306.05301","CorpusId":259108190},"title":"ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases"},{"paperId":"109929be7890ef982fb3b6be0d78609cfab1ea13","externalIds":{"DBLP":"journals/corr/abs-2306-05443","ArXiv":"2306.05443","DOI":"10.48550/arXiv.2306.05443","CorpusId":259129602},"title":"PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","externalIds":{"ArXiv":"2306.05425","DBLP":"journals/corr/abs-2306-05425","DOI":"10.48550/arXiv.2306.05425","CorpusId":259108295},"title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"0244aeb7c6927e2fb0c2e668687e160a00737dbe","externalIds":{"ArXiv":"2306.02707","DBLP":"journals/corr/abs-2306-02707","DOI":"10.48550/arXiv.2306.02707","CorpusId":259075316},"title":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4"},{"paperId":"3566e1245bfc90096fe0cdb8b18674da6519c8d6","externalIds":{"DBLP":"conf/recsys/MysoreMZ23","ArXiv":"2306.02250","DOI":"10.1145/3604915.3608829","CorpusId":259076066},"title":"Large Language Model Augmented Narrative Driven Recommendations"},{"paperId":"fd8730a7e7efbfc3c7cd94f6534fb4eb11718c2c","externalIds":{"ACL":"2023.acl-long.344","ArXiv":"2306.00186","DBLP":"conf/acl/RoitFSACDGGHKMG23","DOI":"10.48550/arXiv.2306.00186","CorpusId":258999861},"title":"Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","externalIds":{"ArXiv":"2305.18752","DBLP":"conf/nips/YangSLZGLS23","DOI":"10.48550/arXiv.2305.18752","CorpusId":258967184},"title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"714740e38dbb0642475ff3eae5681ae0a4103670","externalIds":{"ArXiv":"2306.00020","DBLP":"journals/corr/abs-2306-00020","DOI":"10.48550/arXiv.2306.00020","CorpusId":258999680},"title":"GPT4GEO: How a Language Model Sees the World's Geography"},{"paperId":"6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2","externalIds":{"DBLP":"conf/acl/LiuO0CSMSKC24","ArXiv":"2305.17888","DOI":"10.48550/arXiv.2305.17888","CorpusId":258959117},"title":"LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"b463f5dd8ed6871ddeac03754c1c2d99547b08fe","externalIds":{"ArXiv":"2305.18403","DBLP":"conf/acl/Zhang0SYOYZ24","DOI":"10.18653/v1/2024.findings-acl.178","CorpusId":258967906},"title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"},{"paperId":"8eb3f847e29745b5df685b313c798993b6eb1bc4","externalIds":{"ACL":"2023.acl-long.474","DBLP":"journals/corr/abs-2305-17804","ArXiv":"2305.17804","DOI":"10.48550/arXiv.2305.17804","CorpusId":258960506},"title":"Targeted Data Generation: Finding and Fixing Model Weaknesses"},{"paperId":"119a3ed0898499fce0ce6af6958d566d82390ba5","externalIds":{"ArXiv":"2306.13089","DBLP":"conf/nips/ZhaoLMXFDKL23","DOI":"10.1101/2023.05.30.542904","CorpusId":259077070},"title":"GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning"},{"paperId":"ebf3a59aacdd9982283d7f41229ee2a93800d6ef","externalIds":{"DBLP":"conf/nips/KangLBKH23","ArXiv":"2305.18395","DOI":"10.48550/arXiv.2305.18395","CorpusId":258967252},"title":"Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks"},{"paperId":"ad4b365630f1c13d74d78f0f5d8cee87ef356d41","externalIds":{"DBLP":"conf/nips/MalladiGNDL0A23","ArXiv":"2305.17333","DOI":"10.48550/arXiv.2305.17333","CorpusId":258959274},"title":"Fine-Tuning Language Models with Just Forward Passes"},{"paperId":"a36658b26ea4ccb58f85d8a578f6ec6767446095","externalIds":{"ArXiv":"2305.16635","DBLP":"conf/naacl/JungWJBLFS024","ACL":"2024.naacl-long.250","DOI":"10.18653/v1/2024.naacl-long.250","CorpusId":258947505},"title":"Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model"},{"paperId":"13fd4277388cc2a9da75e8b772e5efcf6ebe2d32","externalIds":{"ArXiv":"2305.16960","DBLP":"conf/iclr/LiuYJZYV24","CorpusId":264590778},"title":"Training Socially Aligned Language Models on Simulated Social Interactions"},{"paperId":"32dcd0887537cece54e214f531d2c384470b023f","externalIds":{"DBLP":"journals/corr/abs-2305-17126","ArXiv":"2305.17126","DOI":"10.48550/arXiv.2305.17126","CorpusId":258947222},"title":"Large Language Models as Tool Makers"},{"paperId":"5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35","externalIds":{"ArXiv":"2305.15717","DBLP":"journals/corr/abs-2305-15717","DOI":"10.48550/arXiv.2305.15717","CorpusId":258887629},"title":"The False Promise of Imitating Proprietary LLMs"},{"paperId":"ce913026f693101e54d3ab9152e107034d81fce1","externalIds":{"DBLP":"journals/tmlr/LiangBLTSYZNWKN23","DOI":"10.1111/nyas.15007","CorpusId":253553585,"PubMed":"37230490"},"title":"Holistic Evaluation of Language Models"},{"paperId":"dedfe929d182cc3537a9ed765d589b4735ce062a","externalIds":{"DBLP":"conf/nips/ValmeekamMSK23","ArXiv":"2305.15771","DOI":"10.48550/arXiv.2305.15771","CorpusId":260440590},"title":"On the Planning Abilities of Large Language Models - A Critical Investigation"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","externalIds":{"ArXiv":"2305.16103","DBLP":"journals/corr/abs-2305-16103","DOI":"10.48550/arXiv.2305.16103","CorpusId":258887944},"title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"7d8905a1fd288068f12c8347caeabefd36d0dd6c","externalIds":{"DBLP":"journals/corr/abs-2305-15334","ArXiv":"2305.15334","DOI":"10.52202/079017-4020","CorpusId":258865184},"title":"Gorilla: Large Language Model Connected with Massive APIs"},{"paperId":"a7977870b58e716cd93f571a3b75a610167a75bd","externalIds":{"DBLP":"conf/emnlp/LuoZCGKWMG23","ArXiv":"2305.15225","DOI":"10.48550/arXiv.2305.15225","CorpusId":258865283},"title":"SAIL: Search-Augmented Instruction Learning"},{"paperId":"4fd05237af737c58c60e4ca8a745f85013681604","externalIds":{"ArXiv":"2305.15062","CorpusId":258865862},"title":"Lawyer LLaMA Technical Report"},{"paperId":"5dbffedcabe3fa43060ebbe2b1789500edfd871f","externalIds":{"DBLP":"conf/emnlp/HaoGMHWWH23","ArXiv":"2305.14992","DOI":"10.48550/arXiv.2305.14992","CorpusId":258865812},"title":"Reasoning with Language Model is Planning with World Model"},{"paperId":"763eb8d43e2f8a5d9da26269a4985efd1c099a5b","externalIds":{"DBLP":"journals/corr/abs-2305-14688","ArXiv":"2305.14688","DOI":"10.48550/arXiv.2305.14688","CorpusId":258865458},"title":"ExpertPrompting: Instructing Large Language Models to be Distinguished Experts"},{"paperId":"5459cab5dcf3c65c6b4f63b3d9f1e376f722bbcb","externalIds":{"DBLP":"journals/corr/abs-2305-15075","ArXiv":"2305.15075","DOI":"10.48550/arXiv.2305.15075","CorpusId":258865566},"title":"HuatuoGPT, towards Taming Language Model to Be a Doctor"},{"paperId":"c226a4acb42912054d498bcf771023b0ba2da001","externalIds":{"DBLP":"conf/iclr/PangWLC0Z024","ArXiv":"2305.14483","DOI":"10.48550/arXiv.2305.14483","CorpusId":258865735},"title":"Language Model Self-improvement by Reinforcement Learning Contemplation"},{"paperId":"a122863d239643453195424c04067e89406246e1","externalIds":{"DBLP":"conf/emnlp/DingCXQHL0Z23","ArXiv":"2305.14233","DOI":"10.48550/arXiv.2305.14233","CorpusId":258840897},"title":"Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"},{"paperId":"f743287be3ced6757de7ecb26d03815b22cd737b","externalIds":{"ArXiv":"2305.14283","DBLP":"journals/corr/abs-2305-14283","DOI":"10.48550/arXiv.2305.14283","CorpusId":258841283},"title":"Query Rewriting for Retrieval-Augmented Large Language Models"},{"paperId":"2ad8183c72a90511383a32ccaeea313eb85f4085","externalIds":{"DBLP":"journals/corr/abs-2305-14167","ArXiv":"2305.14167","DOI":"10.48550/arXiv.2305.14167","CorpusId":258841764},"title":"DetGPT: Detect What You Need via Reasoning"},{"paperId":"32ac52069e562d4f900afee70bdca63f53461481","externalIds":{"ArXiv":"2305.14314","DBLP":"conf/nips/DettmersPHZ23","DOI":"10.48550/arXiv.2305.14314","CorpusId":258841328},"title":"QLoRA: Efficient Finetuning of Quantized LLMs"},{"paperId":"5b8f0460d408a8688d9ee0cba127c779d3291d99","externalIds":{"ArXiv":"2305.13735","DBLP":"journals/corr/abs-2305-13735","DOI":"10.48550/arXiv.2305.13735","CorpusId":258841835},"title":"Aligning Large Language Models through Synthetic Feedback"},{"paperId":"a10843d1349fff8d2a7d9722f800802187fef67f","externalIds":{"DBLP":"conf/nips/KimLKPYKL23","ArXiv":"2305.14152","DOI":"10.48550/arXiv.2305.14152","CorpusId":258841104},"title":"Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"},{"paperId":"c329ddddc53b9df84cf174d49a37c0bae585ea7a","externalIds":{"DBLP":"conf/emnlp/00020ZWA023","ArXiv":"2305.12962","DOI":"10.48550/arXiv.2305.12962","CorpusId":258832881},"title":"Distilling ChatGPT for Explainable Automated Student Answer Assessment"},{"paperId":"9591e76f296afe9589a42e899266e1c6a355d9ba","externalIds":{"DBLP":"conf/emnlp/XuXILWZ023","ArXiv":"2305.13083","DOI":"10.48550/arXiv.2305.13083","CorpusId":258833086},"title":"InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT"},{"paperId":"e9b3e82b1c9eb4136df28e94f24cd823431be93b","externalIds":{"DBLP":"journals/corr/abs-2305-12281","ArXiv":"2305.12281","DOI":"10.48550/arXiv.2305.12281","CorpusId":258833488},"title":"Lifelong Language Pretraining with Distribution-Specialized Experts"},{"paperId":"6783b17fe4328f48403f57009a73f784de09f645","externalIds":{"DBLP":"journals/corr/abs-2305-12002","ArXiv":"2305.12002","DOI":"10.1145/3583780.3615285","CorpusId":258833440},"title":"XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters"},{"paperId":"017010b941d902a467f6d329ae5e74fd67e67912","externalIds":{"DBLP":"journals/corr/abs-2305-11627","ArXiv":"2305.11627","DOI":"10.48550/arXiv.2305.11627","CorpusId":258823276},"title":"LLM-Pruner: On the Structural Pruning of Large Language Models"},{"paperId":"c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c","externalIds":{"DBLP":"journals/corr/abs-2305-11554","ArXiv":"2305.11554","DOI":"10.48550/arXiv.2305.11554","CorpusId":258823133},"title":"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings"},{"paperId":"546d0624adfc6e18fb87d8cc77e7705bb9ea7445","externalIds":{"ArXiv":"2305.11206","DBLP":"conf/nips/ZhouLX0SMMEYYZG23","CorpusId":258822910},"title":"LIMA: Less Is More for Alignment"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"ea72fb2a0d340f9d14fbcf300cd5f5fbbe1050bb","externalIds":{"DBLP":"journals/corr/abs-2305-09617","ArXiv":"2305.09617","DOI":"10.48550/arXiv.2305.09617","CorpusId":258715226},"title":"Towards Expert-Level Medical Question Answering with Large Language Models"},{"paperId":"6a6cbcc596758dd214120a1d51528ce55daa333d","externalIds":{"DBLP":"journals/corr/abs-2305-07804","ArXiv":"2305.07804","DOI":"10.48550/arXiv.2305.07804","CorpusId":258685548},"title":"Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation"},{"paperId":"0383e049e98c9eedbc61be728d4ef037300bbedf","externalIds":{"DBLP":"journals/corr/abs-2305-07001","ArXiv":"2305.07001","DOI":"10.1145/3708882","CorpusId":258615776},"title":"Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach"},{"paperId":"ef0679f8b3114c339bdf5a0c202403a08d160a88","externalIds":{"DBLP":"conf/wsdm/LiuCS024","ArXiv":"2305.06566","DOI":"10.1145/3616855.3635845","CorpusId":258615357},"title":"ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models"},{"paperId":"c3ed333a37a6d9a0fcf1dad3106a114f66a45b99","externalIds":{"DBLP":"journals/corr/abs-2305-06849","ArXiv":"2305.06849","ACL":"2023.acl-long.499","DOI":"10.48550/arXiv.2305.06849","CorpusId":258615343},"title":"WebCPM: Interactive Web Search for Chinese Long-form Question Answering"},{"paperId":"e01515c6138bc525f7aec30fc85f2adf028d4156","externalIds":{"DBLP":"journals/corr/abs-2305-03047","ArXiv":"2305.03047","DOI":"10.48550/arXiv.2305.03047","CorpusId":258479665},"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"},{"paperId":"450b5490cc653478c272be50aa986798df828a20","externalIds":{"DBLP":"journals/corr/abs-2305-02182","ArXiv":"2305.02182","DOI":"10.1145/3604915.3610646","CorpusId":258461170},"title":"Uncovering ChatGPT’s Capabilities in Recommender Systems"},{"paperId":"aad167be3c902388ea625da4117fcae4325b8b7d","externalIds":{"ArXiv":"2305.02301","DBLP":"journals/corr/abs-2305-02301","DOI":"10.48550/arXiv.2305.02301","CorpusId":258461606},"title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"},{"paperId":"8be0ec99f80710887e3a8e6bac5fba51a8fd7186","externalIds":{"DBLP":"journals/corr/abs-2305-02156","ArXiv":"2305.02156","DOI":"10.48550/arXiv.2305.02156","CorpusId":258461030},"title":"Zero-Shot Listwise Document Reranking with a Large Language Model"},{"paperId":"389ec3e8902a5dcfcde1adec735854e93f845937","externalIds":{"DBLP":"journals/corr/abs-2304-14402","ACL":"2024.eacl-long.57","ArXiv":"2304.14402","DOI":"10.48550/arXiv.2304.14402","CorpusId":258352678},"title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"},{"paperId":"04ee9597be4d6d2457214334e495e591000b5542","externalIds":{"ArXiv":"2304.14454","CorpusId":258417843},"title":"PMC-LLaMA: Towards Building Open-source Language Models for Medicine"},{"paperId":"718989761d0dc0f97727470f0dc23de7ea48c26d","externalIds":{"ArXiv":"2304.14233","DBLP":"journals/corr/abs-2304-14233","DOI":"10.48550/arXiv.2304.14233","CorpusId":258352285},"title":"Large Language Models are Strong Zero-Shot Retriever"},{"paperId":"08a80cb34d785258c770acecd302ab41ead46eed","externalIds":{"DBLP":"conf/iclr/XuSZG0FTLJ24","ArXiv":"2304.12244","CorpusId":258298159},"title":"WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"},{"paperId":"003ef1cd670d01af05afa0d3c72d72228f494432","externalIds":{"ArXiv":"2304.11477","DBLP":"journals/corr/abs-2304-11477","DOI":"10.48550/arXiv.2304.11477","CorpusId":258298051},"title":"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"},{"paperId":"459c82205d2a27a8542bba7a4d478a8a23be2f5d","externalIds":{"ArXiv":"2304.09542","DBLP":"conf/emnlp/0001YMWRCYR23","DOI":"10.48550/arXiv.2304.09542","CorpusId":258212638},"title":"Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"ae736662f64d56f3ab1894fbd9c45f8f37251843","externalIds":{"ArXiv":"2304.07327","DBLP":"conf/nips/KopfKRATSBNSNES23","DOI":"10.48550/arXiv.2304.07327","CorpusId":258179434},"title":"OpenAssistant Conversations - Democratizing Large Language Model Alignment"},{"paperId":"90e41626b8c78600da70c4350c67c3a10525cb37","externalIds":{"ArXiv":"2304.08247","CorpusId":258180068},"title":"MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data"},{"paperId":"302ee27524a717ddc21f332ca634b9211c6ec6aa","externalIds":{"ArXiv":"2304.06975","DBLP":"journals/corr/abs-2304-06975","DOI":"10.48550/arXiv.2304.06975","CorpusId":258170497},"title":"HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"},{"paperId":"748698bd4387afd08594e0dc8150c2afa210d9ae","externalIds":{"DBLP":"conf/nips/YuanYTWHH23","ArXiv":"2304.05302","DOI":"10.48550/arXiv.2304.05302","CorpusId":258059818},"title":"RRHF: Rank Responses to Align Language Models with Human Feedback without tears"},{"paperId":"0d502a1e300336ae628f5c8b99ee4d3766c8f60b","externalIds":{"DBLP":"journals/corr/abs-2304-11116","ArXiv":"2304.11116","DOI":"10.48550/arXiv.2304.11116","CorpusId":258291494},"title":"Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"},{"paperId":"6159549f986c63e160a678feef2130a2a4b93feb","externalIds":{"DBLP":"journals/corr/abs-2304-03516","ArXiv":"2304.03516","DOI":"10.48550/arXiv.2304.03516","CorpusId":258041206},"title":"Generative Recommendation: Towards Next-generation Recommender Paradigm"},{"paperId":"9e8cb8c91a0acb6e661b58ad724aa758490f2bea","externalIds":{"ArXiv":"2304.03277","DBLP":"journals/corr/abs-2304-03277","CorpusId":257985497},"title":"Instruction Tuning with GPT-4"},{"paperId":"e2e0b39eee7b922fe1d364720ba3965b814fd9ea","externalIds":{"ArXiv":"2304.02213","DBLP":"journals/corr/abs-2304-02213","DOI":"10.48550/arXiv.2304.02213","CorpusId":267913092},"title":"Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT"},{"paperId":"bdb68c5e2369633b20e733774ac66eb4600c34d1","externalIds":{"DBLP":"journals/corr/abs-2304-01933","ArXiv":"2304.01933","DOI":"10.48550/arXiv.2304.01933","CorpusId":257921386},"title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"},{"paperId":"bce55193d9a887ad00774a9134df08cd521a85ae","externalIds":{"DBLP":"journals/corr/abs-2304-01097","ArXiv":"2304.01097","DOI":"10.48550/arXiv.2304.01097","CorpusId":257912795},"title":"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"3aaf6a2cbad5850ad81ab5c163599cb3d523436f","externalIds":{"DBLP":"journals/corr/abs-2303-17651","ArXiv":"2303.17651","DOI":"10.48550/arXiv.2303.17651","CorpusId":257900871},"title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","externalIds":{"DBLP":"journals/corr/abs-2303-17564","ArXiv":"2303.17564","CorpusId":257833842},"title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"70da4fb798a86cbe8cad96c27ced0415885bbd9d","externalIds":{"DBLP":"conf/naacl/HeLGJZLJYDC24","ArXiv":"2303.16854","DOI":"10.48550/arXiv.2303.16854","CorpusId":257805087},"title":"AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","externalIds":{"DBLP":"journals/corr/abs-2303-16434","ArXiv":"2303.16434","DOI":"10.48550/arXiv.2303.16434","CorpusId":257804802},"title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"c11810fa8887b678facea62da4607c4898360308","externalIds":{"DBLP":"journals/corr/abs-2303-16755","ArXiv":"2303.16755","DOI":"10.48550/arXiv.2303.16755","CorpusId":257805110},"title":"Training Language Models with Language Feedback at Scale"},{"paperId":"a9e155fda1d97baa2b8712f580cc61887cc64e9b","externalIds":{"PubMedCentral":"10372638","DBLP":"journals/corr/abs-2303-15056","ArXiv":"2303.15056","DOI":"10.1073/pnas.2305016120","CorpusId":257766307,"PubMed":"37463210"},"title":"ChatGPT outperforms crowd workers for text-annotation tasks"},{"paperId":"8fc90497d9043fdf35e71302b7c2e79bb907144f","externalIds":{"DBLP":"journals/corr/abs-2303-14742","ArXiv":"2303.14742","DOI":"10.48550/arXiv.2303.14742","CorpusId":257766844},"title":"Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases"},{"paperId":"4a7f6c4e71e20311ade4e76e8d0945d499c31fcd","externalIds":{"PubMedCentral":"10364849","ArXiv":"2303.14070","DOI":"10.7759/cureus.40895","CorpusId":259252045,"PubMed":"37492832"},"title":"ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge"},{"paperId":"70630751848624f77825f2c060f11f99270a2aab","externalIds":{"DBLP":"conf/cvpr/HuLLGV23","ArXiv":"2303.12696","DOI":"10.1109/CVPR52729.2023.01141","CorpusId":257663474},"title":"Dense Network Expansion for Class Incremental Learning"},{"paperId":"9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","externalIds":{"ArXiv":"2303.09136","DBLP":"journals/corr/abs-2303-09136","DOI":"10.48550/arXiv.2303.09136","CorpusId":257557504},"title":"A Short Survey of Viewing Large Language Models in Legal Aspect"},{"paperId":"bdf7bf9e81a6c12e22323d0402885b2ba62f623e","externalIds":{"DBLP":"journals/corr/abs-2303-04360","ArXiv":"2303.04360","DOI":"10.48550/arXiv.2303.04360","CorpusId":257405132},"title":"Does Synthetic Data Generation of LLMs Help Clinical Text Mining?"},{"paperId":"44b0d2e884efa5344e50424dbe2edf616981f201","externalIds":{"ArXiv":"2303.00807","DBLP":"journals/corr/abs-2303-00807","DOI":"10.48550/arXiv.2303.00807","CorpusId":257279774},"title":"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers"},{"paperId":"8df67942e29cba92bb5913b62d1d2df7371842d9","externalIds":{"ArXiv":"2302.13007","DBLP":"journals/tbd/DaiLLHCWZXZLLLZCSLSLL25","DOI":"10.1109/TBDATA.2025.3536934","CorpusId":257631936},"title":"AugGPT: Leveraging ChatGPT for Text Data Augmentation"},{"paperId":"aafae4730b1add0b3e243e011db9ac87428f83cd","externalIds":{"ArXiv":"2302.09432","DBLP":"journals/corr/abs-2302-09432","DOI":"10.48550/arXiv.2302.09432","CorpusId":257038067},"title":"BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark"},{"paperId":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","externalIds":{"ArXiv":"2302.07842","DBLP":"journals/tmlr/MialonDLNPRRSDC23","CorpusId":256868474},"title":"Augmented Language Models: a Survey"},{"paperId":"89e184d2bc830af568e439db9476caa0c047e11a","externalIds":{"ArXiv":"2302.06692","DBLP":"journals/corr/abs-2302-06692","DOI":"10.48550/arXiv.2302.06692","CorpusId":256846700},"title":"Guiding Pretraining in Reinforcement Learning with Large Language Models"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","externalIds":{"DBLP":"journals/corr/abs-2302-04761","ArXiv":"2302.04761","DOI":"10.48550/arXiv.2302.04761","CorpusId":256697342},"title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"c589a3420ba335a05c248f525ea3c6e90215e42b","externalIds":{"DBLP":"journals/corr/abs-2302-03735","ArXiv":"2302.03735","DOI":"10.1162/tacl_a_00619","CorpusId":256662721},"title":"Pre-train, Prompt, and Recommendation: A Comprehensive Survey of Language Modeling Paradigm Adaptations in Recommender Systems"},{"paperId":"ccb1ccc4deacc4fb18000f0e1ce24329548963ae","externalIds":{"ArXiv":"2302.01560","DBLP":"journals/corr/abs-2302-01560","DOI":"10.48550/arXiv.2302.01560","CorpusId":256598146},"title":"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents"},{"paperId":"f2b0017ddd77fa38760a18145e63553105a1a236","externalIds":{"DBLP":"journals/corr/abs-2301-13688","ArXiv":"2301.13688","DOI":"10.48550/arXiv.2301.13688","CorpusId":256415991},"title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"},{"paperId":"fbd49b25bdab98c171af49962a41139c73dacbde","externalIds":{"DBLP":"conf/icml/FuPOSK23","ArXiv":"2301.12726","DOI":"10.48550/arXiv.2301.12726","CorpusId":256390607},"title":"Specializing Smaller Language Models towards Multi-Step Reasoning"},{"paperId":"c1c663d8a7d78342d8eabb6ca144e5761b6a2443","externalIds":{"ArXiv":"2301.12507","DBLP":"conf/icml/SumersMAF023","DOI":"10.48550/arXiv.2301.12507","CorpusId":256389594},"title":"Distilling Internet-Scale Vision-Language Models into Embodied Agents"},{"paperId":"76427fe94e4564fd5df2177bb259d93527fddca5","externalIds":{"DBLP":"journals/corr/abs-2301-01820","ArXiv":"2301.01820","DOI":"10.48550/arXiv.2301.01820","CorpusId":255440689},"title":"InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval"},{"paperId":"909ad57ce8caa6b390a65ae09db352d27d8f3996","externalIds":{"DBLP":"journals/corr/abs-2301-00774","ArXiv":"2301.00774","DOI":"10.48550/arXiv.2301.00774","CorpusId":255372747},"title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"},{"paperId":"5784a10804c122d09349ff4e215cd12d8b72b6aa","externalIds":{"ArXiv":"2212.10720","ACL":"2023.acl-long.123","DBLP":"conf/acl/0012ZMWLCW0H23","DOI":"10.18653/v1/2023.acl-long.123","CorpusId":258947011},"title":"MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions"},{"paperId":"929ecaf5e6f8f706c736e64222e06a552aea0934","externalIds":{"DBLP":"conf/acl/TaoL0XGJJ24","ArXiv":"2212.10192","DOI":"10.48550/arXiv.2212.10192","CorpusId":254877418},"title":"Adam: Dense Retrieval Distillation with Adaptive Dark Examples"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","externalIds":{"ACL":"2023.acl-long.830","DBLP":"conf/acl/HoSY23","ArXiv":"2212.10071","DOI":"10.48550/arXiv.2212.10071","CorpusId":254877399},"title":"Large Language Models Are Reasoning Teachers"},{"paperId":"70b98d90767345b15e0569082c0e4ac661279b5d","externalIds":{"ArXiv":"2212.10450","DBLP":"conf/acl/DingQLCLJB23","ACL":"2023.acl-long.626","DOI":"10.48550/arXiv.2212.10450","CorpusId":254877171},"title":"Is GPT-3 a Good Data Annotator?"},{"paperId":"f9ad1fffa1cc76fd5db3ff758c0839492c5147c4","externalIds":{"DBLP":"journals/corr/abs-2212-10670","ArXiv":"2212.10670","DOI":"10.48550/arXiv.2212.10670","CorpusId":254926556},"title":"In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models"},{"paperId":"126a4776ff8315fd506766cb8f3c722cf746ad9e","externalIds":{"DBLP":"journals/corr/abs-2212-08410","ACL":"2023.acl-short.151","ArXiv":"2212.08410","DOI":"10.48550/arXiv.2212.08410","CorpusId":254823156},"title":"Teaching Small Language Models to Reason"},{"paperId":"3936fd3c6187f606c6e4e2e20b196dbc41cc4654","externalIds":{"DBLP":"journals/corr/abs-2212-08073","ArXiv":"2212.08073","DOI":"10.48550/arXiv.2212.08073","CorpusId":254823489},"title":"Constitutional AI: Harmlessness from AI Feedback"},{"paperId":"8ee45aeb7c97e3346cc62f216f673b91277ac718","externalIds":{"DBLP":"conf/iccv/SongSWCW023","ArXiv":"2212.04088","DOI":"10.1109/ICCV51070.2023.00280","CorpusId":254408960},"title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"},{"paperId":"2c994fadbb84fb960d8306ee138dbeef41a5b323","externalIds":{"ArXiv":"2211.10438","DBLP":"conf/icml/XiaoLSWDH23","DOI":"10.48550/arXiv.2211.10438","CorpusId":253708271},"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","externalIds":{"DBLP":"journals/corr/abs-2211-09085","ArXiv":"2211.09085","CorpusId":253553203},"title":"Galactica: A Large Language Model for Science"},{"paperId":"65bad077608a3c2ed8eac242e993aa40aa8c13e9","externalIds":{"DBLP":"journals/corr/abs-2210-15718","ArXiv":"2210.15718","DOI":"10.48550/arXiv.2210.15718","CorpusId":253224416},"title":"QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation"},{"paperId":"3fa70115248377c3d1517c9f978791a296fbc1dd","externalIds":{"DBLP":"conf/emnlp/0001GHW00023","ArXiv":"2210.11610","DOI":"10.48550/arXiv.2210.11610","CorpusId":253080328},"title":"Large Language Models Can Self-Improve"},{"paperId":"7d29a84a589aa5655e5d3fed8d725ea472816599","externalIds":{"ArXiv":"2210.06726","DBLP":"journals/corr/abs-2210-06726","DOI":"10.48550/arXiv.2210.06726","CorpusId":252873123},"title":"Explanations from Large Language Models Make Small Reasoners Better"},{"paperId":"33be243ac9dd8723e6267dea45fd6a6172d4f6a5","externalIds":{"ArXiv":"2210.01351","DBLP":"journals/corr/abs-2210-01351","DOI":"10.48550/arXiv.2210.01351","CorpusId":252693152},"title":"Less is More: Task-aware Layer-wise Distillation for Language Model Compression"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","externalIds":{"DBLP":"journals/corr/abs-2209-14375","ArXiv":"2209.14375","DOI":"10.48550/arXiv.2209.14375","CorpusId":252596089},"title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"e86009d9f9b1cdf083a48d087552bc4153784451","externalIds":{"DBLP":"journals/corr/abs-2209-11755","ArXiv":"2209.11755","DOI":"10.48550/arXiv.2209.11755","CorpusId":252519173},"title":"Promptagator: Few-shot Dense Retrieval From 8 Examples"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","externalIds":{"DBLP":"journals/corr/abs-2209-11302","ArXiv":"2209.11302","DOI":"10.1109/ICRA48891.2023.10161317","CorpusId":252519594},"title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"59b71e2a248d67a2692bc7e35faa504ee2dbc98d","externalIds":{"ArXiv":"2209.00465","DBLP":"journals/corr/abs-2209-00465","DOI":"10.1609/aaai.v37i11.26549","CorpusId":251979509},"title":"On Grounded Planning for Embodied Tasks with Language Models"},{"paperId":"a938ff4539b09a785a66669844f1a35f76169218","externalIds":{"DBLP":"journals/corr/abs-2208-11663","ArXiv":"2208.11663","DOI":"10.48550/arXiv.2208.11663","CorpusId":251765117},"title":"PEER: A Collaborative Language Model"},{"paperId":"30a7390ec0103684eba9fb6bde1983d706fb57b3","externalIds":{"DBLP":"journals/corr/abs-2208-11580","ArXiv":"2208.11580","DOI":"10.48550/arXiv.2208.11580","CorpusId":251765570},"title":"Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","externalIds":{"DBLP":"journals/corr/abs-2208-05051","ArXiv":"2208.05051","ACL":"2023.acl-long.516","DOI":"10.48550/arXiv.2208.05051","CorpusId":251467816},"title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"624b2ea0ba57ce67a4524e029ac11f748a0782fa","externalIds":{"ArXiv":"2206.10658","DBLP":"journals/tacl/SachanLYZPZ23","ACL":"2023.tacl-1.35","DOI":"10.1162/tacl_a_00564","CorpusId":249926985},"title":"Questions Are All You Need to Train a Dense Passage Retriever"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"29acc890e521f7a6415666ab9eb3432c49b4587a","externalIds":{"DBLP":"journals/corr/abs-2206-05802","ArXiv":"2206.05802","DOI":"10.48550/arXiv.2206.05802","CorpusId":249626555},"title":"Self-critiquing models for assisting human evaluators"},{"paperId":"e03609f2587f690867e7ea0bedaf0db25282c548","externalIds":{"DBLP":"conf/nips/YaoAZWLH22","ArXiv":"2206.01861","DOI":"10.48550/arXiv.2206.01861","CorpusId":249395624},"title":"ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"},{"paperId":"6f7e03e4ccd26c762090e25dc5d2eb1e1f8c641d","externalIds":{"ArXiv":"2205.12679","DBLP":"conf/iclr/GaoPLXY0ZLLK23","CorpusId":257219618},"title":"Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning"},{"paperId":"354bf043179e3e9f05df73e3f04517e53c326d1f","externalIds":{"ArXiv":"2205.12255","DBLP":"journals/corr/abs-2205-12255","DOI":"10.48550/arXiv.2205.12255","CorpusId":249017698},"title":"TALM: Tool Augmented Language Models"},{"paperId":"eb4d54651c4f610749caf2bf401af3ce28ddc439","externalIds":{"ArXiv":"2210.17451","DBLP":"conf/emnlp/WangAM00AG22","ACL":"2022.emnlp-main.388","DOI":"10.48550/arXiv.2210.17451","CorpusId":253153886},"title":"AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","externalIds":{"ArXiv":"2205.10625","DBLP":"conf/iclr/ZhouSHWS0SCBLC23","DOI":"10.48550/arXiv.2205.10625","CorpusId":248986239},"title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"cbf3bf8f541f5b446c59c8deacbcc18527768c75","externalIds":{"ArXiv":"2205.08084","DBLP":"journals/corr/abs-2205-08084","DOI":"10.48550/arXiv.2205.08084","CorpusId":248834570},"title":"M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems"},{"paperId":"7cdaa08890895e1ad92afb5fad429690ad7b1dac","externalIds":{"DBLP":"conf/nips/LiuTMMHBR22","ArXiv":"2205.05638","DOI":"10.48550/arXiv.2205.05638","CorpusId":248693283},"title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"df434c1289f3c7243b585cb9982afac3c5bf0439","externalIds":{"DBLP":"conf/naacl/ZuoZLHZC22","ACL":"2022.naacl-main.116","ArXiv":"2204.07675","DOI":"10.48550/arXiv.2204.07675","CorpusId":248227273},"title":"MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation"},{"paperId":"5f8f992d84d5f8b5e8806dad6076bcc4dca11a34","externalIds":{"ArXiv":"2204.07496","DBLP":"conf/emnlp/SachanLJAYPZ22","ACL":"2022.emnlp-main.249","DOI":"10.48550/arXiv.2204.07496","CorpusId":248218489},"title":"Improving Passage Retrieval with Zero-Shot Question Generation"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"e4f82c0a13cae6739239ae0c25a554b6daff35af","externalIds":{"ArXiv":"2203.10705","DBLP":"journals/corr/abs-2203-10705","ACL":"2022.acl-long.331","DOI":"10.48550/arXiv.2203.10705","CorpusId":247593909},"title":"Compression of Generative Pre-trained Language Models via Quantization"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"2145fcceeb69385e108bf1796d52f974854d4c0b","externalIds":{"ArXiv":"2202.07922","ACL":"2022.emnlp-main.801","DBLP":"journals/corr/abs-2202-07922","DOI":"10.18653/v1/2022.emnlp-main.801","CorpusId":246867045},"title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation"},{"paperId":"4e36db22808c1d677438137b10979a9279fb6c1f","externalIds":{"ArXiv":"2202.05144","DBLP":"journals/corr/abs-2202-05144","CorpusId":246705967},"title":"InPars: Data Augmentation for Information Retrieval using Large Language Models"},{"paperId":"23c265ba884b92ecbd9d18641078d964697e4590","externalIds":{"ArXiv":"2202.04538","DBLP":"journals/corr/abs-2202-04538","CorpusId":246680398},"title":"Generating Training Data with Language Models: Towards Zero-Shot Language Understanding"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","externalIds":{"DBLP":"journals/corr/abs-2201-07207","ArXiv":"2201.07207","CorpusId":246035276},"title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","externalIds":{"DBLP":"journals/corr/abs-2112-09332","ArXiv":"2112.09332","CorpusId":245329531},"title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"b526c3c450d9810ae8b037b4a87bf2a22ac48b38","externalIds":{"ArXiv":"2112.08654","DBLP":"journals/corr/abs-2112-08654","DOI":"10.1109/CVPR52688.2022.00024","CorpusId":245218925},"title":"Learning to Prompt for Continual Learning"},{"paperId":"3af37400f1f9a4f4f211c4a472e18963edc2b34f","externalIds":{"DBLP":"conf/aaai/QiuZLLPGZ22","ArXiv":"2112.06346","DOI":"10.1609/aaai.v36i10.21368","CorpusId":245123993},"title":"ValueNet: A New Dataset for Human Value Driven Dialogue System"},{"paperId":"fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf","externalIds":{"ArXiv":"2112.04359","DBLP":"journals/corr/abs-2112-04359","CorpusId":244954639},"title":"Ethical and social risks of harm from Language Models"},{"paperId":"3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","externalIds":{"ArXiv":"2112.00861","DBLP":"journals/corr/abs-2112-00861","CorpusId":244799619},"title":"A General Language Assistant as a Laboratory for Alignment"},{"paperId":"521ccc898395a2818fced22b4cf371b0e5121f94","externalIds":{"ACL":"2022.naacl-main.341","DBLP":"journals/corr/abs-2110-07178","ArXiv":"2110.07178","DOI":"10.18653/v1/2022.naacl-main.341","CorpusId":238857304},"title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"},{"paperId":"a6fdb277d0a4b09899f802bda3359f5c2021a156","externalIds":{"ArXiv":"2109.10862","DBLP":"journals/corr/abs-2109-10862","CorpusId":237593001},"title":"Recursively Summarizing Books with Human Feedback"},{"paperId":"c2a79e2a65b721d4de5f6d4806323174b9f8f393","externalIds":{"DBLP":"journals/corr/abs-2109-09193","ArXiv":"2109.09193","CorpusId":237572306},"title":"Towards Zero-Label Language Learning"},{"paperId":"4e263b4cd6998bff2501dd143e685f413179b12d","externalIds":{"DBLP":"conf/emnlp/WangLXZZ21","ArXiv":"2108.13487","DOI":"10.18653/v1/2021.findings-emnlp.354","CorpusId":237363383},"title":"Want To Reduce Labeling Cost? GPT-3 Can Help"},{"paperId":"d624bc273821c871f899d8256a34be40c09fc3cd","externalIds":{"DBLP":"journals/corr/abs-2106-10328","ArXiv":"2106.10328","CorpusId":235489789},"title":"Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"1cf50a2e906dc89463d7eab827de9a3c371e7c53","externalIds":{"DBLP":"journals/tacl/HeNKH022","ArXiv":"2106.06168","DOI":"10.1162/tacl_a_00492","CorpusId":245007179},"title":"Generate, Annotate, and Learn: NLP with Synthetic Text"},{"paperId":"63d8426ba1f51a8525dd19fd8ec92934ec71aea5","externalIds":{"ACL":"2021.findings-acl.84","ArXiv":"2105.03075","DBLP":"journals/corr/abs-2105-03075","DOI":"10.18653/v1/2021.findings-acl.84","CorpusId":234093015},"title":"A Survey of Data Augmentation Approaches for NLP"},{"paperId":"b769b629c8de35b16735214251d6b4e99cb55762","externalIds":{"ArXiv":"2104.07540","DBLP":"journals/corr/abs-2104-07540","ACL":"2021.emnlp-main.555","DOI":"10.18653/v1/2021.emnlp-main.555","CorpusId":233241169},"title":"Generating Datasets with Pretrained Language Models"},{"paperId":"3a14e36108aca336e37b28d89a6f4c529f73954c","externalIds":{"DBLP":"conf/aaai/GangalFAMH22","ArXiv":"2104.06669","DOI":"10.1609/aaai.v36i10.21309","CorpusId":233231442},"title":"NAREOR: The Narrative Reordering Problem"},{"paperId":"255e6239bcc51047d020d41ce0179c1270f3c22f","externalIds":{"DBLP":"conf/iclr/Allen-ZhuL23","ArXiv":"2012.09816","MAG":"3113303810","CorpusId":229297687},"title":"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning"},{"paperId":"e8f6eb89897c5880a99748f23c3d3763346eea45","externalIds":{"DBLP":"journals/corr/abs-2011-00593","MAG":"3095273266","ArXiv":"2011.00593","CorpusId":226226888},"title":"MixKD: Towards Efficient Distillation of Large-scale Language Models"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","externalIds":{"MAG":"3082115681","DBLP":"journals/corr/abs-2009-01325","ArXiv":"2009.01325","CorpusId":221665105},"title":"Learning to summarize from human feedback"},{"paperId":"9baab08fbe37369856688b2abe5b3c90cce1682c","externalIds":{"DBLP":"journals/tkdd/GuptaA22","ArXiv":"2008.05221","MAG":"3048823912","DOI":"10.1145/3487045","CorpusId":221112343},"title":"Compression of Deep Learning Models for Text: A Survey"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","externalIds":{"MAG":"3138154797","ArXiv":"2006.05525","DBLP":"journals/ijcv/GouYMT21","DOI":"10.1007/s11263-021-01453-z","CorpusId":219559263},"title":"Knowledge Distillation: A Survey"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"2573af4e13d9a5dddb257d22cd38a600528d9a8b","externalIds":{"DBLP":"journals/corr/abs-2004-02984","ArXiv":"2004.02984","ACL":"2020.acl-main.195","MAG":"3034457371","DOI":"10.18653/v1/2020.acl-main.195","CorpusId":215238853},"title":"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"},{"paperId":"1c332cfa211400fc6f56983fb01a6692046116dd","externalIds":{"DBLP":"conf/nips/HouHSJCL20","MAG":"3101731278","ArXiv":"2004.04037","CorpusId":215415863},"title":"DynaBERT: Dynamic BERT with Adaptive Width and Depth"},{"paperId":"22e510c1f0fffed225c49dc5e5f57a9d80f0d61f","externalIds":{"MAG":"2993608592","DBLP":"journals/corr/abs-1912-02145","ArXiv":"1912.02145","ACL":"D19-5829","DOI":"10.18653/v1/D19-5829","CorpusId":207982228},"title":"An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering"},{"paperId":"703b96f65fb349fe2a2deb896ae07e6eb552b391","externalIds":{"ArXiv":"1911.12011","DBLP":"journals/corr/abs-1911-12011","MAG":"2998733856","DOI":"10.1609/AAAI.V34I05.6519","CorpusId":208310122},"title":"JEC-QA: A Legal-Domain Question Answering Dataset"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","externalIds":{"DBLP":"journals/corr/abs-1910-01108","ArXiv":"1910.01108","MAG":"2978017171","CorpusId":203626972},"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"0cbf97173391b0430140117027edcaf1a37968c7","externalIds":{"MAG":"3105966348","ACL":"2020.findings-emnlp.372","DBLP":"conf/emnlp/JiaoYSJCL0L20","ArXiv":"1909.10351","DOI":"10.18653/v1/2020.findings-emnlp.372","CorpusId":202719327},"title":"TinyBERT: Distilling BERT for Natural Language Understanding"},{"paperId":"d388760ce74792bb764ccf77ea8620ca77406e56","externalIds":{"MAG":"2959353218","DBLP":"conf/ictir/BruchWBN19","DOI":"10.1145/3341981.3344221","CorpusId":199134224},"title":"An Analysis of the Softmax Cross Entropy Loss for Learning-to-Rank with Binary Relevance"},{"paperId":"7a15950dc71079285a4eaf195de5aadd87c41b40","externalIds":{"MAG":"2973379954","DBLP":"journals/corr/abs-1909-08593","ArXiv":"1909.08593","CorpusId":202660943},"title":"Fine-Tuning Language Models from Human Preferences"},{"paperId":"80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","externalIds":{"DBLP":"conf/emnlp/SunCGL19","MAG":"2969515962","ACL":"D19-1441","ArXiv":"1908.09355","DOI":"10.18653/v1/D19-1441","CorpusId":201670719},"title":"Patient Knowledge Distillation for BERT Model Compression"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"f4327b978dec52f16b089c222c43543f8ecf4717","externalIds":{"DOI":"10.5860/choice.189890","CorpusId":240757309},"title":"arXiv"},{"paperId":"ec12fedbfc758abd882ad291635771cd3224453b","externalIds":{"ArXiv":"1903.04566","MAG":"2965043358","DBLP":"conf/ijcai/RostamiKP19","DOI":"10.24963/ijcai.2019/463","CorpusId":75135149},"title":"Complementary Learning for Overcoming Catastrophic Forgetting Using Experience Replay"},{"paperId":"d9ff7a9344dd5d6653bd7a02bfd704422bb29951","externalIds":{"MAG":"2970586779","DBLP":"conf/nips/RolnickASLW19","ArXiv":"1811.11682","CorpusId":53860287},"title":"Experience Replay for Continual Learning"},{"paperId":"c992df5341a6ec7b7984d437f244d346216612e8","externalIds":{"DBLP":"conf/cikm/WangLGBN18","MAG":"2898073868","DOI":"10.1145/3269206.3271784","CorpusId":53033881},"title":"The LambdaLoss Framework for Ranking Metric Optimization"},{"paperId":"d5bb3faa48b83469da1a01ef267886e71f4a931a","externalIds":{"MAG":"2950262738","DBLP":"conf/eccv/MallyaDL18","DOI":"10.1007/978-3-030-01225-0_5","CorpusId":3977226},"title":"Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights"},{"paperId":"c342c71cb23199f112d0bc644fcce56a7306bf94","externalIds":{"MAG":"2774918944","DBLP":"conf/iclr/SenerS18","CorpusId":3383786},"title":"Active Learning for Convolutional Neural Networks: A Core-Set Approach"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"0a9a3380090d994e867026388f64a85481cdb700","externalIds":{"MAG":"2949995560","DBLP":"journals/corr/LeeKHZ17","ArXiv":"1703.08475","CorpusId":5160005},"title":"Overcoming Catastrophic Forgetting by Incremental Moment Matching"},{"paperId":"2e55ba6c97ce5eb55abd959909403fe8da7e9fe9","externalIds":{"DBLP":"journals/corr/KirkpatrickPRVD16","MAG":"2560647685","ArXiv":"1612.00796","DOI":"10.1073/pnas.1611835114","CorpusId":4704285,"PubMed":"28292907"},"title":"Overcoming catastrophic forgetting in neural networks"},{"paperId":"57a10537978600fd33dcdd48922c791609a4851a","externalIds":{"ACL":"D16-1139","DBLP":"conf/emnlp/KimR16","ArXiv":"1606.07947","MAG":"2463507112","DOI":"10.18653/v1/D16-1139","CorpusId":8451212},"title":"Sequence-Level Knowledge Distillation"},{"paperId":"642d0f49b7826adcf986616f4af77e736229990f","externalIds":{"MAG":"2119144962","DBLP":"journals/corr/HanMD15","ArXiv":"1510.00149","CorpusId":2134321},"title":"Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"},{"paperId":"8b6a9c14c549191ab2339345b7cfbec5049fcb8a","externalIds":{"MAG":"2518143623","DBLP":"journals/tit/SasonV16","ArXiv":"1508.00335","DOI":"10.1109/TIT.2016.2603151","CorpusId":7855260},"title":"$f$ -Divergence Inequalities"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","externalIds":{"ArXiv":"1505.04870","DBLP":"conf/iccv/PlummerWCCHL15","MAG":"2568262903","DOI":"10.1007/s11263-016-0965-7","CorpusId":6941275},"title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"6ff0ab1e9064dba97bb8e5ae0b0f1110b5565e06","externalIds":{"DBLP":"journals/siamsc/Oseledets11","MAG":"1993482030","DOI":"10.1137/090752286","CorpusId":207059098},"title":"Tensor-Train Decomposition"},{"paperId":"0b274c13464879ca6d03299ed8f1be4fc9830f4c","externalIds":{"PubMedCentral":"7139439","DOI":"10.1007/978-3-540-89702-6_9","CorpusId":219574650},"title":"Singapore"},{"paperId":"63aaf12163fe9735dfe9a69114937c4fa34f303a","externalIds":{"MAG":"2143331230","DBLP":"conf/icml/BurgesSRLDHH05","DOI":"10.1145/1102351.1102363","CorpusId":11168734},"title":"Learning to rank using gradient descent"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"4b8df079495cbec21ae90d60ab84e8dd813ca7e6","externalIds":{"DBLP":"journals/corr/abs-2401-07103","DOI":"10.48550/arXiv.2401.07103","CorpusId":266999586},"title":"Leveraging Large Language Models for NLG Evaluation: A Survey"},{"paperId":"f4b5ed96ca25c0f61778a26d03cdd6c4b946b5ea","externalIds":{"DBLP":"conf/iclr/YangKCPT24","CorpusId":271745706},"title":"RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment"},{"paperId":"460609e217fd59eaa34f5e11a820661f8ec8d7b6","externalIds":{"DBLP":"conf/emnlp/XuWPSFWL23","DOI":"10.48550/arXiv.2305.14282","CorpusId":258841553},"title":"INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback"},{"paperId":"6f184a1bc9ec3934702d30d960f05fcd471a4423","externalIds":{"DBLP":"conf/emnlp/LeeSENMFRA23","DOI":"10.18653/v1/2023.findings-emnlp.836","CorpusId":266166549},"title":"Ensemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs"},{"paperId":"6713f623e0c7ebc1c94c58a1c0a650e9a204182b","externalIds":{"DBLP":"journals/corr/abs-2312-02120","DOI":"10.48550/arXiv.2312.02120","CorpusId":265609970},"title":"Magicoder: Source Code Is All You Need"},{"paperId":"7a29f47f6509011fe5b19462abf6607867b68373","externalIds":{"CorpusId":263218031},"title":"GPT-4V(ision) System Card"},{"paperId":"362cae35fb65710af8230a4ee8e1aad1f275a4e7","externalIds":{"DBLP":"journals/corr/abs-2312-14187","DOI":"10.48550/arXiv.2312.14187","CorpusId":266521384},"title":"WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation"},{"paperId":"209afcf7480c6b9ebefce8c8daa54a3ccefecd79","externalIds":{"CorpusId":257405222},"title":"AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation"},{"paperId":"cd2d1a0f73ba8c40f882a386cd367899785fb877","externalIds":{"DBLP":"journals/corr/abs-2304-14454","DOI":"10.48550/arXiv.2304.14454","CorpusId":263888272},"title":"PMC-LLaMA: Further Finetuning LLaMA on Medical Papers"},{"paperId":"24acac7461c8a9dd8e4d17927fdb5cb160487584","externalIds":{"DBLP":"conf/eacl/YangCV23","ACL":"2023.findings-eacl.144","DOI":"10.18653/v1/2023.findings-eacl.144","CorpusId":258378178},"title":"Data Augmentation for Radiology Report Simplification"},{"paperId":"4c8cc2383cec93bd9ea0758692f01b98a035215b","externalIds":{"DBLP":"journals/corr/abs-2310-01377","DOI":"10.48550/arXiv.2310.01377","CorpusId":263605623},"title":"UltraFeedback: Boosting Language Models with High-quality Feedback"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","externalIds":{"ArXiv":"2303.17580","DBLP":"journals/corr/abs-2303-17580","DOI":"10.48550/arXiv.2303.17580","CorpusId":257833781},"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"7ca954844bc1dd405bc43445b1c990e42d865095","externalIds":{"DBLP":"journals/corr/abs-2303-17760","DOI":"10.48550/arXiv.2303.17760","CorpusId":257900712},"title":"CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society"},{"paperId":"ac4ffaab10f6b6ad83e79ca5691f338abf5cff82","externalIds":{"DBLP":"journals/corr/abs-2307-06290","DOI":"10.48550/arXiv.2307.06290","CorpusId":259837472},"title":"Instruction Mining: High-Quality Instruction Data Selection for Large Language Models"},{"paperId":"3424b252c1eea8e3b4b9926d21feb35f4280b0ab","externalIds":{"DBLP":"conf/emnlp/ChenSHJ23","DOI":"10.18653/v1/2023.emnlp-main.417","CorpusId":266163971},"title":"Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"},{"paperId":"af67be0fff8d087a0d8554b6e8998ab12409bbda","externalIds":{"DBLP":"journals/corr/abs-2307-00526","DOI":"10.48550/arXiv.2307.00526","CorpusId":259316802},"title":"TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"},{"paperId":"6c79dc9d5ea8ef204e0543fd69a5e9eb80bbf612","externalIds":{"DBLP":"conf/iclr/CaiT0XGL0J23","CorpusId":259298675},"title":"HypeR: Multitask Hyper-Prompted Training Enables Large-Scale Retrieval Generalization"},{"paperId":"b73ea8adfef3cdc63167e3a7f1f2be3f4ef4219d","externalIds":{"DBLP":"journals/corr/abs-2305-07402","DOI":"10.48550/arXiv.2305.07402","CorpusId":266180782},"title":"Knowledge Refinement via Interaction Between Search Engines and Large Language Models"},{"paperId":"7dd3b54233a71c532a15adc6faa7284af7c02f15","externalIds":{"DBLP":"journals/corr/abs-2311-05657","DOI":"10.48550/arXiv.2311.05657","CorpusId":270823634},"title":"Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs"},{"paperId":"e730164e17975547564a1eaa70cea5884b16c89d","externalIds":{"CorpusId":259937133},"title":"Instruction : Translate the phrase ” Bonne chance ” into English Response : Good Luck"},{"paperId":"b8b45b14df9029562b8995c6ab7fd90a8810f312","externalIds":{"DBLP":"conf/nips/DettmersLBZ22","CorpusId":258509304},"title":"GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"},{"paperId":"74b20a8a3832612df9d5cb79f9b4a7515e5ef0bc","externalIds":{"DBLP":"conf/acl/KieselAHCW022","ACL":"2022.acl-long.306","DOI":"10.18653/v1/2022.acl-long.306","CorpusId":248094722},"title":"Identifying the Human Values behind Arguments"},{"paperId":"2a83a92b08e0f3873d07162c73c67e533321112e","externalIds":{"DBLP":"conf/naacl/LiuZFV22","DOI":"10.18653/v1/2022.findings-naacl.18","CorpusId":250562745},"title":"Aligning Generative Language Models with Human Values"},{"paperId":"ec936b808e0fab9281c050ad4010cddec92c8cbe","externalIds":{"ACL":"2022.acl-short.8","DBLP":"conf/acl/LiuJFTDY022","DOI":"10.18653/v1/2022.acl-short.8","CorpusId":248780177},"title":"P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"6cbb968ef5d81f33134c7022b12241d1bbef47ff","externalIds":{"DBLP":"conf/iclr/2021","CorpusId":235614232},"title":"9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021"},{"paperId":"94a81307056161cc26ebd3e11078902e04bc3f4a","externalIds":{"DOI":"10.1007/978-981-13-8464-6_4","CorpusId":4307401},"title":"Optimization"},{"paperId":"09c8425f95a1cb488589a750542352a48a5e88e3","externalIds":{"CorpusId":265123408},"title":"OF FINANCE"}]}