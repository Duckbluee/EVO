{"references":[{"paperId":"f3d1ef20ff4f143a0fab00c1b8f5aa3cfec770b9","externalIds":{"DBLP":"journals/ijon/KimILLK25","DOI":"10.1016/j.neucom.2024.129167","CorpusId":274765609},"title":"RobustMixGen: Data augmentation for enhancing robustness of visual-language models in the presence of distribution shift"},{"paperId":"56a4fb8bf5bac348e2efd5f8628d52a409102100","externalIds":{"DBLP":"conf/emnlp/SuLWC24","ArXiv":"2403.01216","DOI":"10.48550/arXiv.2403.01216","CorpusId":268230867},"title":"API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access"},{"paperId":"1524b7ff78755a3445d22400a8d6c75ba8c0cd65","externalIds":{"DBLP":"journals/corr/abs-2401-12915","ArXiv":"2401.12915","DOI":"10.48550/arXiv.2401.12915","CorpusId":267094801},"title":"Red Teaming Visual Language Models"},{"paperId":"2b14d9e190022e388476ebb24eb1a84349ca0de4","externalIds":{"DBLP":"journals/corr/abs-2312-10665","ArXiv":"2312.10665","DOI":"10.48550/arXiv.2312.10665","CorpusId":266348439},"title":"Silkie: Preference Distillation for Large Visual Language Models"},{"paperId":"8e106c5992a491e74dbad73d60c8b2ebe4660549","externalIds":{"DBLP":"journals/corr/abs-2312-07424","ArXiv":"2312.07424","DOI":"10.48550/arXiv.2312.07424","CorpusId":266174413},"title":"How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation"},{"paperId":"9bfeb4baec1640ea4413354ab693eed083bd0d9f","externalIds":{"ArXiv":"2312.07550","DBLP":"journals/corr/abs-2312-07550","DOI":"10.48550/arXiv.2312.07550","CorpusId":266191550},"title":"Understanding (Un)Intended Memorization in Text-to-Image Generative Models"},{"paperId":"0415ec332d455a831e8e0c766970e7f34603d9fd","externalIds":{"DBLP":"journals/corr/abs-2311-01723","ArXiv":"2311.01723","DOI":"10.48550/arXiv.2311.01723","CorpusId":265019206},"title":"Towards Calibrated Robust Fine-Tuning of Vision-Language Models"},{"paperId":"52910cb1ae2f3a2950392100f3cf64769014fd07","externalIds":{"DBLP":"journals/corr/abs-2311-00453","ArXiv":"2311.00453","DOI":"10.48550/arXiv.2311.00453","CorpusId":264832890},"title":"CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection"},{"paperId":"553b25f1e371ae6bd7126af54206444043ee7da3","externalIds":{"DBLP":"conf/iclr/ZhouP0H024","ArXiv":"2310.18961","DOI":"10.48550/arXiv.2310.18961","CorpusId":264590476},"title":"AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection"},{"paperId":"bd1818d345acb805ba7f0b9643852a52ccd765f9","externalIds":{"DBLP":"journals/corr/abs-2310-19070","ArXiv":"2310.19070","DOI":"10.48550/arXiv.2310.19070","CorpusId":264817610},"title":"Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection"},{"paperId":"87ee351f7c668a145c511fcfff23f6248fb5dc93","externalIds":{"ArXiv":"2310.17373","DBLP":"journals/tois/ChenCNZ25","DOI":"10.1145/3744240","CorpusId":264490663},"title":"Causality-Inspired Fair Representation Learning for Multimodal Recommendation"},{"paperId":"0e1a7f453976d7aa74eed46686c943bc6e630b56","externalIds":{"DBLP":"journals/corr/abs-2310-09926","ArXiv":"2310.09926","DOI":"10.48550/arXiv.2310.09926","CorpusId":264145761},"title":"Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data"},{"paperId":"02aabccb51ff000ad460deca629d2148f5e39ab6","externalIds":{"ArXiv":"2310.04971","DBLP":"conf/iclr/XueJNM24","DOI":"10.48550/arXiv.2310.04971","CorpusId":263831684},"title":"Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift"},{"paperId":"961adb2787708bf530816c3498d90379783d0b97","externalIds":{"ArXiv":"2310.04929","DBLP":"journals/corr/abs-2310-04929","DOI":"10.48550/arXiv.2310.04929","CorpusId":263830361},"title":"DISCOVER: Making Vision Networks Interpretable via Competition and Dissection"},{"paperId":"408772a7e41d7ac4cd60d861bcf351c328794250","externalIds":{"DBLP":"journals/cviu/ZanellaLMPWR24","ArXiv":"2310.02835","DOI":"10.48550/arXiv.2310.02835","CorpusId":263611937},"title":"Delving into CLIP latent space for Video Anomaly Recognition"},{"paperId":"2403c8e72a90d9c778970fc0812ecdcc58800c5d","externalIds":{"ArXiv":"2310.02224","DBLP":"journals/corr/abs-2310-02224","DOI":"10.48550/arXiv.2310.02224","CorpusId":263608643},"title":"Can Language Models be Instructed to Protect Personal Information?"},{"paperId":"4d08d652a80050d3682a626ca0fa388534a160b4","externalIds":{"DBLP":"conf/gis/Rao0MJ23","ArXiv":"2309.17319","DOI":"10.1145/3589132.3625611","CorpusId":263310422},"title":"Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models (Vision Paper)"},{"paperId":"0aee8500fd2bb33c69ee3b05a20a75fbfc5bb3aa","externalIds":{"DBLP":"journals/corr/abs-2310-00108","ArXiv":"2310.00108","DOI":"10.1109/ICCV51070.2023.00449","CorpusId":263334258},"title":"Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study"},{"paperId":"c23b765ec30c91aa0819c7e8523ffa585142ae3a","externalIds":{"DBLP":"conf/cikm/Ni0LFL0ZY25","ArXiv":"2309.15379","DOI":"10.1145/3746252.3761655","CorpusId":263151653},"title":"A Content-Driven Micro-Video Recommendation Dataset at Scale"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"502bcc31dc76d6796f1cbc0b7514dc0177b09713","externalIds":{"DBLP":"journals/corr/abs-2309-10283","ArXiv":"2309.10283","DOI":"10.1109/TKDE.2024.3382726","CorpusId":262053728},"title":"FRAMU: Attention-Based Machine Unlearning Using Federated Reinforcement Learning"},{"paperId":"9d409bc84d006d6506c3894fa2e1455bad9d08de","externalIds":{"ArXiv":"2308.13513","DBLP":"conf/cikm/ZhaoH023","DOI":"10.1145/3583780.3615104","CorpusId":261214697},"title":"Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs"},{"paperId":"a58a1eb932372f70039dbfb0b49af84de855d18e","externalIds":{"DBLP":"conf/aaai/WuZPZY0Z24","ArXiv":"2308.11681","DOI":"10.48550/arXiv.2308.11681","CorpusId":261076368},"title":"VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection"},{"paperId":"5690e35b8beab92a80055fe2530c29c24e495379","externalIds":{"ArXiv":"2308.10741","DBLP":"conf/iccvw/Schlarmann023","DOI":"10.1109/ICCVW60793.2023.00395","CorpusId":261048835},"title":"On the Adversarial Robustness of Multi-Modal Foundation Models"},{"paperId":"ea1710f4d34950560663210781d11b87225c5d55","externalIds":{"DBLP":"journals/corr/abs-2308-07026","ArXiv":"2308.07026","DOI":"10.1145/3581783.3612454","CorpusId":260887071},"title":"AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning"},{"paperId":"142e934dd5d6c53f877c30243d436255e3a0dde7","externalIds":{"ArXiv":"2306.13213","DBLP":"conf/aaai/QiHP0WM24","DOI":"10.1609/aaai.v38i19.30150","CorpusId":259244034},"title":"Visual Adversarial Examples Jailbreak Aligned Large Language Models"},{"paperId":"378406966c3edb7d31a9637093fdee07d81cddf9","externalIds":{"ArXiv":"2306.09486","DBLP":"journals/corr/abs-2306-09486","DOI":"10.1145/3580305.3599825","CorpusId":259187495},"title":"FedMultimodal: A Benchmark for Multimodal Federated Learning"},{"paperId":"b3b726863c8245ebd8ffea7bd859f0bb12f57f02","externalIds":{"ArXiv":"2306.08173","DBLP":"journals/corr/abs-2306-08173","DOI":"10.48550/arXiv.2306.08173","CorpusId":259165143},"title":"Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training"},{"paperId":"fcb2afd8744a37b45b35aac1cef16d4e1178f6e2","externalIds":{"ArXiv":"2306.05731","DBLP":"journals/corr/abs-2306-05731","DOI":"10.1109/ACCESS.2024.3467062","CorpusId":259129555},"title":"Multimodal Explainable Artificial Intelligence: A Comprehensive Review of Methodological Advances and Future Research Directions"},{"paperId":"9514393d1d596c2b63b96d296b1da1d4d0a6a469","externalIds":{"ArXiv":"2306.02050","DBLP":"conf/icml/ZhangWZHFZP23","DOI":"10.48550/arXiv.2306.02050","CorpusId":259075995},"title":"Provable Dynamic Fusion for Low-Quality Multimodal Data"},{"paperId":"5c2dad6db34617187893675252d804526798c07e","externalIds":{"DBLP":"conf/icml/MaZZWFZH23","ArXiv":"2306.01265","DOI":"10.48550/arXiv.2306.01265","CorpusId":259064254},"title":"Calibrating Multimodal Learning"},{"paperId":"ec4398c262169eb7c8bcec0ad9916dd898a9a5d1","externalIds":{"DBLP":"conf/nips/SomepalliSGGG23","ArXiv":"2305.20086","DOI":"10.48550/arXiv.2305.20086","CorpusId":258987384},"title":"Understanding and Mitigating Copying in Diffusion Models"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"ada3a458eb760f0702861d53b424dd4df42c985e","externalIds":{"DBLP":"journals/corr/abs-2304-03646","ArXiv":"2304.03646","DOI":"10.1145/3583780.3614875","CorpusId":258041323},"title":"Fairness through Aleatoric Uncertainty"},{"paperId":"d9b95937934d7291b7c253b28b6c9aaee033c91d","externalIds":{"DBLP":"journals/corr/abs-2303-17591","ArXiv":"2303.17591","DOI":"10.1109/CVPRW63382.2024.00182","CorpusId":257833863},"title":"Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models"},{"paperId":"aa207668318fec38d60b79f407fb64982e46fce9","externalIds":{"ArXiv":"2303.14814","DBLP":"journals/corr/abs-2303-14814","DOI":"10.1109/CVPR52729.2023.01878","CorpusId":257766897},"title":"WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation"},{"paperId":"a20d42fe6450919645f8fad8eca757f748b9a878","externalIds":{"DBLP":"conf/cvpr/0020PZYWW23","ArXiv":"2303.00601","DOI":"10.1109/CVPR52729.2023.00776","CorpusId":257255313},"title":"Multimodal Industrial Anomaly Detection via Hybrid Fusion"},{"paperId":"eb07d132c5f3b0686e68c64bb554cf306a967bed","externalIds":{"DBLP":"conf/iclr/YuLWXL23","ArXiv":"2302.08888","DOI":"10.48550/arXiv.2302.08888","CorpusId":257019892},"title":"Multimodal Federated Learning via Contrastive Representation Ensemble"},{"paperId":"7a63b216572f21a3d7e05007a1b27329d9e0ef46","externalIds":{"DBLP":"conf/iclr/Liu0SZ0SW23","ArXiv":"2212.14453","DOI":"10.48550/arXiv.2212.14453","CorpusId":255340832},"title":"Learning Multimodal Data Augmentation in Feature Space"},{"paperId":"a91332878b9f47c9eede80f4d0c4d15f7089439e","externalIds":{"ArXiv":"2210.14556","DBLP":"journals/corr/abs-2210-14556","DOI":"10.18653/v1/2022.findings-emnlp.36","CorpusId":253116746},"title":"Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis"},{"paperId":"59027b574cd02905cbdcb1fd8feefb878cc49c8d","externalIds":{"DBLP":"journals/tist/NguyenHRNLYN25","ArXiv":"2209.02299","DOI":"10.1145/3749987","CorpusId":252089272},"title":"A Survey of Machine Unlearning"},{"paperId":"8a778cd7d136a57b20c0864e643d4afe2bc83ffc","externalIds":{"ArXiv":"2206.09391","DBLP":"journals/corr/abs-2206-09391","DOI":"10.1145/3503161.3547801","CorpusId":249888984},"title":"Towards Adversarial Attack on Vision-Language Pre-training Models"},{"paperId":"1f3a5c1f38b25aec818442a2cc896a0845b2849c","externalIds":{"DBLP":"conf/cvpr/HanYHZY22","DOI":"10.1109/CVPR52688.2022.02005","CorpusId":250278803},"title":"Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification"},{"paperId":"699e2d99c1506ac63988b00e34e08e59b6640735","externalIds":{"ArXiv":"2205.08383","DBLP":"journals/corr/abs-2205-08383","DOI":"10.13140/RG.2.2.14341.01769","CorpusId":248834463},"title":"Bias and Fairness on Multimodal Emotion Detection Algorithms"},{"paperId":"0b73a37c06f3d79a1cb5fd61e3556676634b9d2f","externalIds":{"ArXiv":"2205.01397","DBLP":"conf/icml/FangIWWSDS22","DOI":"10.48550/arXiv.2205.01397","CorpusId":248505704},"title":"Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)"},{"paperId":"834b5b5b25e99186f900a7eb1c8d641caf024fcb","externalIds":{"DBLP":"conf/cvpr/0002R0T022","ArXiv":"2204.05454","DOI":"10.1109/CVPR52688.2022.01764","CorpusId":248118952},"title":"Are Multimodal Transformers Robust to Missing Modality?"},{"paperId":"86a8bb518dd62dd9bab3f549307426e89a257f1d","externalIds":{"ArXiv":"2204.04588","DBLP":"conf/cvpr/AndonianCH22","DOI":"10.1109/CVPR52688.2022.01594","CorpusId":248085000},"title":"Robust Cross-Modal Representation Learning with Progressive Self-Distillation"},{"paperId":"0567131ec1f839240179927ceb61f51bdd173055","externalIds":{"ArXiv":"2203.14960","DBLP":"journals/corr/abs-2203-14960","DOI":"10.48550/arXiv.2203.14960","CorpusId":247779010},"title":"Domino: Discovering Systematic Errors with Cross-Modal Embeddings"},{"paperId":"54020e5fe48ebb250f27d744e20a63cac2988a84","externalIds":{"DBLP":"conf/icml/WortsmanIGRLMNF22","ArXiv":"2203.05482","DOI":"10.48550/arXiv.2203.05482","CorpusId":247362886},"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","externalIds":{"ArXiv":"2203.02155","DBLP":"journals/corr/abs-2203-02155","CorpusId":246426909},"title":"Training language models to follow instructions with human feedback"},{"paperId":"0fc5cd83fcc493a764b6b8ab8496a40f46d130d8","externalIds":{"DBLP":"journals/corr/abs-2203-02013","ArXiv":"2203.02013","DOI":"10.1145/3514094.3534148","CorpusId":247244569},"title":"DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations"},{"paperId":"119b019943727e74135671205eab19b0add15ecb","externalIds":{"DBLP":"journals/ijon/XiongYQX22","DOI":"10.1016/j.neucom.2022.01.063","CorpusId":247191659},"title":"A unified framework for multi-modal federated learning"},{"paperId":"55ba9652caea8e5c12408085fcb32b0eb7862c71","externalIds":{"DBLP":"conf/icumt/JezekJBDS21","DOI":"10.1109/ICUMT54235.2021.9631567","CorpusId":245147304},"title":"Deep learning-based defect detection of metal parts: evaluating current methods in complex conditions"},{"paperId":"65280dab94a6617cc87a5878210dc2c4fdd1ecb1","externalIds":{"DBLP":"conf/icmi/BoothHSTWD21","DOI":"10.1145/3462244.3479897","CorpusId":238992528},"title":"Bias and Fairness in Multimodal Machine Learning: A Case Study of Automated Video Interviews"},{"paperId":"05b2b28ebd8bcf0de4fe1cea9d096f20bbd3ab5f","externalIds":{"DBLP":"conf/fat/dEondWL22","ArXiv":"2107.00758","DOI":"10.1145/3531146.3533240","CorpusId":235727396},"title":"The Spotlight: A General Method for Discovering Systematic Errors in Deep Learning Models"},{"paperId":"0ff3585e15f8799f191c53daf1cb9ccd330b169a","externalIds":{"DBLP":"conf/acl/WangLW22a","ACL":"2022.findings-acl.211","ArXiv":"2106.06683","DOI":"10.18653/v1/2022.findings-acl.211","CorpusId":235422358},"title":"Assessing Multilingual Fairness in Pre-trained Multimodal Representations"},{"paperId":"cbf62054f25002675284b643e7f17deab8853106","externalIds":{"ArXiv":"2206.12714","DBLP":"journals/corr/abs-2206-12714","DOI":"10.1109/CVPR46437.2021.00335","CorpusId":235656727},"title":"Defending Multimodal Fusion Models against Single-Source Adversaries"},{"paperId":"736b7adac030b616a2a5556762b82b0ff1946d8d","externalIds":{"ArXiv":"2403.03563","DBLP":"conf/icra/YooLZ21","DOI":"10.1109/ICRA48506.2021.9561586","CorpusId":239040944},"title":"Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots"},{"paperId":"6bda005a727969356a012e177cd600e7fdf80c96","externalIds":{"DBLP":"journals/access/JoshiWK21","ArXiv":"2105.07878","DOI":"10.1109/ACCESS.2021.3070212","CorpusId":233376164},"title":"A Review on Explainability in Multimodal Deep Neural Nets"},{"paperId":"913d43b69fd4ddee2ff64d3a1e6402d3787e2b7e","externalIds":{"DBLP":"journals/corr/abs-2104-10036","ArXiv":"2104.10036","DOI":"10.1109/ISIE45552.2021.9576231","CorpusId":233307063},"title":"VT-ADL: A Vision Transformer Network for Image Anomaly Detection and Localization"},{"paperId":"7eb24c2109f75c614cc7aa4c1cac8b643c05e70c","externalIds":{"ArXiv":"2104.06064","DBLP":"journals/cii/BozicTS21","DOI":"10.1016/j.compind.2021.103459","CorpusId":233219867},"title":"Mixed supervision for surface-defect detection: from weakly to fully supervised learning"},{"paperId":"771ba492e3d93bfd422c05778346c88f35d6cd8a","externalIds":{"ArXiv":"2104.02000","DBLP":"conf/cvpr/TianX21","DOI":"10.1109/CVPR46437.2021.00555","CorpusId":233025045},"title":"Can audio-visual integration strengthen robustness under multimodal attacks?"},{"paperId":"0f016f28e3b1cdb97ed50acc5b3c36026e083c91","externalIds":{"DBLP":"journals/jair/ChengV021","ArXiv":"2101.02032","DOI":"10.1613/jair.1.12814","CorpusId":230770023},"title":"Socially Responsible AI Algorithms: Issues, Purposes, and Challenges"},{"paperId":"2c340d7bc21aa6a9f44b466b7f74ac9150dfcb41","externalIds":{"ArXiv":"2012.08673","MAG":"3113067643","DBLP":"journals/corr/abs-2012-08673","CorpusId":229220286},"title":"A Closer Look at the Robustness of Vision-and-Language Pre-trained Models"},{"paperId":"4be17fd8f8105f282a0c18670c473cb1459cd3b1","externalIds":{"ArXiv":"2012.08637","DBLP":"journals/corr/abs-2012-08637","MAG":"3110913339","CorpusId":229220208},"title":"Multi-Modal Anomaly Detection for Unstructured and Uncertain Environments"},{"paperId":"eccfae5f09387461a3b9aae3838cc3d326b68816","externalIds":{"DBLP":"conf/nips/KrishnanT20","ArXiv":"2012.07923","MAG":"3104448368","CorpusId":227275177},"title":"Improving model calibration with accuracy versus uncertainty optimization"},{"paperId":"ba212de1258aa800f147046ef91f5e0eede3292c","externalIds":{"DBLP":"conf/icmi/YanHS20","MAG":"3094268584","DOI":"10.1145/3382507.3418889","CorpusId":224817491},"title":"Mitigating Biases in Multimodal Personality Assessment"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","externalIds":{"MAG":"3082115681","DBLP":"journals/corr/abs-2009-01325","ArXiv":"2009.01325","CorpusId":221665105},"title":"Learning to summarize from human feedback"},{"paperId":"8f28873be3601c5a2736996eba543cf51950a381","externalIds":{"ArXiv":"2007.04687","DBLP":"conf/eccv/Wu0SSSWY20","MAG":"3089682612","DOI":"10.1007/978-3-030-58577-8_20","CorpusId":220424732},"title":"Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision"},{"paperId":"4cd91c51098783ec972f6a0ab430cacdd634a5b2","externalIds":{"MAG":"3040636077","DOI":"10.2307/1268172","CorpusId":27862354},"title":"A Mathematical Theory of Evidence"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","externalIds":{"DBLP":"journals/corr/abs-2006-06195","MAG":"3102995547","ArXiv":"2006.06195","CorpusId":219573512},"title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"e924bfb95435153185f8d89e77f5a3534e2a29bd","externalIds":{"MAG":"3039690871","ArXiv":"2004.07173","DBLP":"conf/cvpr/PenaSMF20","DOI":"10.1109/CVPRW50498.2020.00022","CorpusId":215768624},"title":"Bias in Multimodal AI: Testbed for Fair Automatic Recruitment"},{"paperId":"c12dda55bdff2f4cebf0a274331de8d117c2b7aa","externalIds":{"MAG":"3007960301","DBLP":"conf/nips/YehKALPR20","ArXiv":"1910.07969","CorpusId":213097139},"title":"On Completeness-aware Concept-Based Explanations in Deep Neural Networks"},{"paperId":"e7308644562065bc1b1690d382d61cf3c8725262","externalIds":{"ArXiv":"1909.13550","MAG":"2976356773","DBLP":"journals/corr/abs-1909-13550","CorpusId":203593350},"title":"Well-calibrated Model Uncertainty with Temperature Scaling for Dropout Variational Inference"},{"paperId":"0090023afc66cd2741568599057f4e82b566137c","externalIds":{"ArXiv":"1908.09635","MAG":"2969896603","DBLP":"journals/csur/MehrabiMSLG21","DOI":"10.1145/3457607","CorpusId":201666566},"title":"A Survey on Bias and Fairness in Machine Learning"},{"paperId":"3aa681914a7da79f7d7293f51a058eefe61c8bb7","externalIds":{"MAG":"2948982773","DBLP":"conf/cvpr/BergmannFSS19","DOI":"10.1109/CVPR.2019.00982","CorpusId":189857704},"title":"MVTec AD â€” A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection"},{"paperId":"463b46e0447b7ca34e1bbe5f4711b64d16e7d76b","externalIds":{"MAG":"2907419877","DBLP":"conf/wsdm/ChengLSHL19","DOI":"10.1145/3289600.3291037","CorpusId":59261205},"title":"XBully: Cyberbullying Detection within a Multi-Modal Context"},{"paperId":"8eca936f462712f3d67ebe564292857144f704f1","externalIds":{"MAG":"2948027124","DBLP":"conf/iccv/SubedarKLTH19","DOI":"10.1109/ICCV.2019.00640","CorpusId":182952366},"title":"Uncertainty-Aware Audiovisual Activity Recognition Using Deep Bayesian Variational Inference"},{"paperId":"a62b0b8ff07bdbcf4fd1c8449acac1f24d8434c4","externalIds":{"MAG":"2888407265","DBLP":"journals/vc/HuangQY20","DOI":"10.1007/s00371-018-1588-5","CorpusId":52066826},"title":"Surface defect saliency of magnetic tile"},{"paperId":"06ba3492e3a9a2e98df2c81b91ec94787e3f97fb","externalIds":{"MAG":"2790777763","ArXiv":"1803.07464","DBLP":"conf/eccv/LiTJCL18","DOI":"10.1007/978-3-030-01234-2_34","CorpusId":4052735},"title":"VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions"},{"paperId":"96ed8ce9ef9fc475db9e02c79f984dc110409b62","externalIds":{"DBLP":"conf/cvpr/SultaniCS18","MAG":"2951723061","ArXiv":"1801.04264","DOI":"10.1109/CVPR.2018.00678","CorpusId":1610415},"title":"Real-World Anomaly Detection in Surveillance Videos"},{"paperId":"8a6acba7fb2aad1299fcf35701417e063d410ed4","externalIds":{"ArXiv":"1712.09867","MAG":"2777556841","DBLP":"conf/cvpr/LiuLLG18","DOI":"10.1109/CVPR.2018.00684","CorpusId":3865699},"title":"Future Frame Prediction for Anomaly Detection - A New Baseline"},{"paperId":"79cfb51a51fc093f66aac8e858afe2e14d4a1f20","externalIds":{"MAG":"2950100464","DBLP":"journals/corr/abs-1708-02002","DOI":"10.1109/ICCV.2017.324","CorpusId":47252984},"title":"Focal Loss for Dense Object Detection"},{"paperId":"d65ce2b8300541414bfe51d03906fca72e93523c","externalIds":{"MAG":"2950953798","ArXiv":"1706.04599","DBLP":"journals/corr/GuoPSW17","CorpusId":28671436},"title":"On Calibration of Modern Neural Networks"},{"paperId":"6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91","externalIds":{"ArXiv":"1705.09406","DBLP":"journals/pami/BaltrusaitisAM19","MAG":"2951127645","DOI":"10.1109/TPAMI.2018.2798607","CorpusId":10137425,"PubMed":"29994351"},"title":"Multimodal Machine Learning: A Survey and Taxonomy"},{"paperId":"50004c086ffd6a201a4b782281aaa930fbfe6ecf","externalIds":{"MAG":"2432481613","DBLP":"journals/corr/MilletariNA16","ArXiv":"1606.04797","DOI":"10.1109/3DV.2016.79","CorpusId":206429151},"title":"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","externalIds":{"PubMedCentral":"4878278","MAG":"2396881363","DOI":"10.1038/sdata.2016.35","CorpusId":33285731,"PubMed":"27219127"},"title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"5cf0d213f3253cd46673d955209f8463db73cc51","externalIds":{"DBLP":"journals/lre/BussoBLKMKCLN08","MAG":"2146334809","DOI":"10.1007/S10579-008-9076-6","CorpusId":11820063},"title":"IEMOCAP: interactive emotional dyadic motion capture database"},{"paperId":"fec18520144b8707a5e4cbbad00e5b63caabfbf3","externalIds":{"MAG":"2952259152","DBLP":"journals/corr/abs-0706-3188","ArXiv":"0706.3188","DOI":"10.5555/1390681.1390693","CorpusId":795794},"title":"A tutorial on conformal prediction"},{"paperId":"c5503905b38605849f417e2f87e918b2124420e7","externalIds":{"MAG":"2158941266","DOI":"10.1175/1520-0477(2002)083<0683:CBESCF>2.3.CO;2","CorpusId":122754640},"title":"CONFIDENCE BUILDERS Evaluating Seasonal Climate Forecasts from User Perspectives"},{"paperId":"2cb9d167e9ddc9d48b931520248bdc1b7e818703","externalIds":{"DBLP":"conf/icml/McKinzieSCYST23","CorpusId":260815045},"title":"Robustness in Multimodal Learning under Train-Test Modality Mismatch"},{"paperId":"19e97c83e5637a751f6945344dc19ba658b67468","externalIds":{"DBLP":"journals/corr/abs-2212-08044","DOI":"10.48550/arXiv.2212.08044","CorpusId":269033716},"title":"Are Multimodal Models Robust to Image and Text Perturbations?"}]}