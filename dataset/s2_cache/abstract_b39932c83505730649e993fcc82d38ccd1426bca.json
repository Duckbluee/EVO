{"abstract":"In this letter, we present a novel federated semantic learning (FedSem) framework to collaboratively train the semantic-channel encoders of multiple devices with the coordination of a base station-based semantic-channel decoder, which is more practical for task-oriented communications rather than performing the vanilla distributed learning merely at the userâ€™s side. Particularly, one of the most potential tasks, i.e., constructing semantic knowledge graph (SKG), is considered due to its powerful semantic structured representation and cross-modal comprehension capacity. In this case, the proposed FedSem can make full use of the distributed data and computing resources while protecting user privacy to directly parse images into SKGs. Furthermore, the information bottleneck theory is leveraged to drive the loss design by formalizing a rate-distortion tradeoff, which eliminates the redundancies of semantic features while maintaining the task-relevant information. Compared to the baselines, simulation results demonstrate that the proposed FedSem achieves better rate-distortion performance and convergence under different number of users and channel conditions."}