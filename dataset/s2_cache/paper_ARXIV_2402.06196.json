{"paperId":"a1f76db91c0debcf93ae9889736bce8470902113","externalIds":{"DBLP":"journals/corr/abs-2402-06196","ArXiv":"2402.06196","DOI":"10.48550/arXiv.2402.06196","CorpusId":267617032},"title":"Large Language Models: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2402.06196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2164604","name":"Shervin Minaee"},{"authorId":"2273053608","name":"Tomáš Mikolov"},{"authorId":"1620486779","name":"Narjes Nikzad"},{"authorId":"89845455","name":"M. Chenaghlu"},{"authorId":"2166511","name":"R. Socher"},{"authorId":"2283771739","name":"Xavier Amatriain"},{"authorId":"2257313530","name":"Jianfeng Gao"}],"abstract":"Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions."}