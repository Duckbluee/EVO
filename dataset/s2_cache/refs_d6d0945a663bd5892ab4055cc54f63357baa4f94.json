{"references":[{"paperId":"86a01be1028a4789c9f16bf4d0ed74ff7d351b68","externalIds":{"DBLP":"journals/tvcg/XuXZXH25","DOI":"10.1109/TVCG.2024.3397712","CorpusId":269625321,"PubMed":"38713571"},"title":"DreamAnime: Learning Style-Identity Textual Disentanglement for Anime and Beyond"},{"paperId":"ec45c3f0f88c8ce1deb5baa71c2c0e14ad64d249","externalIds":{"DBLP":"conf/eccv/LinPLLXNZR24","ArXiv":"2404.01291","DOI":"10.48550/arXiv.2404.01291","CorpusId":268857167},"title":"Evaluating Text-to-Visual Generation with Image-to-Text Generation"},{"paperId":"e9a3ecd05c34824aa9d94997a21b5222431baf48","externalIds":{"ArXiv":"2402.15021","DBLP":"journals/corr/abs-2402-15021","DOI":"10.48550/arXiv.2402.15021","CorpusId":267897719},"title":"CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models"},{"paperId":"bb102dbec6b31fca6e1b7bfe2f77f3a317762e80","externalIds":{"ArXiv":"2402.11572","DBLP":"journals/corr/abs-2402-11572","DOI":"10.48550/arXiv.2402.11572","CorpusId":267751012},"title":"Cobra Effect in Reference-Free Image Captioning Metrics"},{"paperId":"69c1595f53677bf6b0babcc663f57a36e918cba2","externalIds":{"DBLP":"journals/corr/abs-2401-15559","ArXiv":"2401.15559","DOI":"10.1145/3613904.3642165","CorpusId":267312299},"title":"IntentTuner: An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image Generative Models"},{"paperId":"55a305a44829a9dbdcca9f4e06a68ca5c319e12e","externalIds":{"DBLP":"journals/corr/abs-2312-14867","ArXiv":"2312.14867","DOI":"10.48550/arXiv.2312.14867","CorpusId":266521161},"title":"VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation"},{"paperId":"9a5dda25a3f2dce144b7fef305820e537401050e","externalIds":{"ArXiv":"2312.10240","DBLP":"conf/cvpr/LiangHLLKCSPYYK24","DOI":"10.1109/CVPR52733.2024.01835","CorpusId":266348349},"title":"Rich Human Feedback for Text-to-Image Generation"},{"paperId":"a17fe25540a96782cd1f24d7be512f7516359a7f","externalIds":{"DBLP":"conf/cvpr/UrbanekBAWSR24","ArXiv":"2312.08578","DOI":"10.1109/CVPR52733.2024.02521","CorpusId":266209761},"title":"A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions"},{"paperId":"5220687c42520855db240b67415a20e09c137004","externalIds":{"ArXiv":"2312.03766","DBLP":"journals/corr/abs-2312-03766","DOI":"10.48550/arXiv.2312.03766","CorpusId":266052781},"title":"Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment"},{"paperId":"d963cf2a3ac3bdb70e06df10218eb9bb7c85ad3d","externalIds":{"ArXiv":"2311.12919","DBLP":"journals/corr/abs-2311-12919","DOI":"10.48550/arXiv.2311.12919","CorpusId":265352128},"title":"SPOT! Revisiting Video-Language Models for Event Understanding"},{"paperId":"299a77bf4050c9686d35b905b96d8902734845c5","externalIds":{"DBLP":"conf/nips/LeeYMMPGZNTBKPL23","ArXiv":"2311.04287","DOI":"10.48550/arXiv.2311.04287","CorpusId":265051037},"title":"Holistic Evaluation of Text-To-Image Models"},{"paperId":"2c38ab72d91a27c62d774689924e8d53e821fe06","externalIds":{"ArXiv":"2310.09247","DBLP":"journals/corr/abs-2310-09247","DOI":"10.48550/arXiv.2310.09247","CorpusId":264127903},"title":"Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy"},{"paperId":"6487ec82f6d8082a5b402a5416ea03009acb1679","externalIds":{"DBLP":"journals/cgf/PoYGABBCDHKLLMNOTWW24","ArXiv":"2310.07204","DOI":"10.1111/cgf.15063","CorpusId":263835355},"title":"State of the Art on Diffusion Models for Visual Computing"},{"paperId":"174df7d2cc8d49f219d8899c7d75bc929f3a57c7","externalIds":{"ArXiv":"2310.03739","DBLP":"journals/corr/abs-2310-03739","DOI":"10.48550/arXiv.2310.03739","CorpusId":263671961},"title":"Aligning Text-to-Image Diffusion Models with Reward Backpropagation"},{"paperId":"7e7a36d15a80b07f473dcc40a21f12484b8c8e90","externalIds":{"DBLP":"journals/corr/abs-2310-05590","ArXiv":"2310.05590","DOI":"10.1109/ICCV51070.2023.00697","CorpusId":263830850},"title":"Perceptual Artifacts Localization for Image Synthesis Tasks"},{"paperId":"c4a349353c8053d9bedfd68110fbcab5f358cff6","externalIds":{"DBLP":"conf/bmvc/LiK0K23","ArXiv":"2307.10864","DOI":"10.48550/arXiv.2307.10864","CorpusId":259991537},"title":"Divide & Bind Your Attention for Improved Generative Semantic Nursing"},{"paperId":"afe4ee7ba225d8965cd96056b3e1e76a31451a75","externalIds":{"ArXiv":"2307.09036","DBLP":"journals/tvcg/FengWWWLZWC24","DOI":"10.1109/TVCG.2023.3327168","CorpusId":259950893,"PubMed":"37878445"},"title":"PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation"},{"paperId":"eb3f3e42332a2aec2798752b30f93f93b647be8b","externalIds":{"DBLP":"conf/mm/BettiSBBCS23","ArXiv":"2307.09416","DOI":"10.1145/3581783.3612706","CorpusId":259950866},"title":"Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation"},{"paperId":"f7d57f223154965e6e5584d3a51561aaea7ca13b","externalIds":{"DBLP":"journals/corr/abs-2307-05134","ArXiv":"2307.05134","DOI":"10.1109/WACV57701.2024.00287","CorpusId":259766704},"title":"TIAM - A Metric for Evaluating Alignment in Text-to-Image Generation"},{"paperId":"650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1","externalIds":{"DBLP":"journals/corr/abs-2307-04749","ArXiv":"2307.04749","DOI":"10.48550/arXiv.2307.04749","CorpusId":259501554},"title":"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback"},{"paperId":"dedbac319177d04ce63fe00d2fec24bdaab90d6d","externalIds":{"DBLP":"journals/corr/abs-2306-14610","ArXiv":"2306.14610","DOI":"10.48550/arXiv.2306.14610","CorpusId":259251493},"title":"SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality"},{"paperId":"d8008ace077d72ccd277be795c720e9381623c10","externalIds":{"DBLP":"journals/corr/abs-2306-09341","ArXiv":"2306.09341","DOI":"10.48550/arXiv.2306.09341","CorpusId":259171771},"title":"Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis"},{"paperId":"de374dc9bb0b443ef399fc36587aa1e192447466","externalIds":{"ArXiv":"2306.09344","DBLP":"journals/corr/abs-2306-09344","DOI":"10.48550/arXiv.2306.09344","CorpusId":259171761},"title":"DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data"},{"paperId":"52b10ae66d025e99fbb602935e155f97f4f0696f","externalIds":{"DBLP":"journals/corr/abs-2306-00943","ArXiv":"2306.00943","DOI":"10.1109/TVCG.2024.3365804","CorpusId":258999372,"PubMed":"38354074"},"title":"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance"},{"paperId":"46ac5734e8e5122d3c34df1b37cd68481bd4b9f2","externalIds":{"DBLP":"conf/emnlp/SinghZWWXDC23","ArXiv":"2305.13812","DOI":"10.48550/arXiv.2305.13812","CorpusId":258840965},"title":"Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality"},{"paperId":"d8c78221e4366d6a72a6b3e41e35b706cc45c01d","externalIds":{"DBLP":"journals/corr/abs-2305-13301","ArXiv":"2305.13301","DOI":"10.48550/arXiv.2305.13301","CorpusId":258833251},"title":"Training Diffusion Models with Reinforcement Learning"},{"paperId":"972501b057e2b84d6ce6506f70bcac697bab7872","externalIds":{"DBLP":"conf/nips/LuYLWW23","ArXiv":"2305.11116","DOI":"10.48550/arXiv.2305.11116","CorpusId":258762865},"title":"LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation"},{"paperId":"b1ef91c5541f88297c7551e1adf15fcee7987197","externalIds":{"DBLP":"conf/nips/YaromBCAHLOS23","ArXiv":"2305.10400","DOI":"10.48550/arXiv.2305.10400","CorpusId":258740893},"title":"What You See is What You Read? Improving Text-Image Alignment Evaluation"},{"paperId":"231a434f8fac0b01cbc05890b283f4d9da4cb100","externalIds":{"DBLP":"journals/corr/abs-2305-09972","ArXiv":"2305.09972","DOI":"10.48550/arXiv.2305.09972","CorpusId":258741093},"title":"Real-Time Flying Object Detection with YOLOv8"},{"paperId":"d23b11cc2c87017914c7b4c95262da01810aa743","externalIds":{"ArXiv":"2305.03689","DBLP":"conf/nips/RayRDPKS23","CorpusId":258546995},"title":"Cola: A Benchmark for Compositional Text-to-image Retrieval"},{"paperId":"cf694df964caa156ec306b45d3a3127533cb458f","externalIds":{"ArXiv":"2305.01569","DBLP":"journals/corr/abs-2305-01569","DOI":"10.48550/arXiv.2305.01569","CorpusId":258437096},"title":"Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation"},{"paperId":"3ab661db57d924f4ff1706e05ac807873ca00e0a","externalIds":{"DBLP":"journals/corr/abs-2304-06767","ArXiv":"2304.06767","DOI":"10.48550/arXiv.2304.06767","CorpusId":258170300},"title":"RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"},{"paperId":"1b2355c3c674b26a977768a91a164384ad51bbb1","externalIds":{"ArXiv":"2304.05977","DBLP":"conf/nips/XuLWTLDTD23","DOI":"10.48550/arXiv.2304.05977","CorpusId":258079316},"title":"ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation"},{"paperId":"6a195df2f7611aa75d5734b2efb32a408d2f8348","externalIds":{"DBLP":"conf/wacv/ChenLV24","ArXiv":"2304.03373","DOI":"10.1109/WACV57701.2024.00526","CorpusId":258041377},"title":"Training-Free Layout Control with Cross-Attention Guidance"},{"paperId":"9550076d9930dd3533ab9276126f1a9265fad7b4","externalIds":{"ArXiv":"2303.14420","DBLP":"conf/iccv/WuSZZL23","DOI":"10.1109/ICCV51070.2023.00200","CorpusId":257767281},"title":"Human Preference Score: Better Aligning Text-to-image Models with Human Preference"},{"paperId":"0cbb518c364067200476a51e5ce7476a4f582770","externalIds":{"DBLP":"journals/corr/abs-2303-13873","ArXiv":"2303.13873","DOI":"10.1109/ICCV51070.2023.02033","CorpusId":257757213},"title":"Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation"},{"paperId":"1f02ba1c6fae779ec3d003340e72eaf82351cfb9","externalIds":{"DBLP":"conf/iccv/HuLKWOKS23","ArXiv":"2303.11897","DOI":"10.1109/ICCV51070.2023.01866","CorpusId":257636562},"title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"35ccd924de9e8483bdcf144cbf2edf09be157b7e","externalIds":{"ArXiv":"2303.07909","DBLP":"journals/corr/abs-2303-07909","DOI":"10.48550/arXiv.2303.07909","CorpusId":257505012},"title":"Text-to-image Diffusion Models in Generative AI: A Survey"},{"paperId":"1e4b6567ff66de6cdcbe58c863538241e2f62b45","externalIds":{"ArXiv":"2302.12192","DBLP":"journals/corr/abs-2302-12192","DOI":"10.48550/arXiv.2302.12192","CorpusId":257102772},"title":"Aligning Text-to-Image Models using Human Feedback"},{"paperId":"c3c7464acb90049c5f520b0732dc7435ba3690bd","externalIds":{"DBLP":"journals/tog/CheferAVWC23","ArXiv":"2301.13826","DOI":"10.1145/3592116","CorpusId":256416326},"title":"Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"39e7cdc762118540c3b942a21535ac1caa5c341d","externalIds":{"PubMedCentral":"10245047","ArXiv":"2301.01902","DBLP":"journals/corr/abs-2301-01902","DOI":"10.1016/j.heliyon.2023.e16757","CorpusId":258957226,"PubMed":"37292268"},"title":"What’s in a text-to-image prompt? The potential of stable diffusion in visual arts education"},{"paperId":"4bf77d64b860ed0cd84a63aecd92a3cb295b88ee","externalIds":{"ArXiv":"2212.10015","DBLP":"journals/corr/abs-2212-10015","DOI":"10.48550/arXiv.2212.10015","CorpusId":254877055},"title":"Benchmarking Spatial Relationships in Text-to-Image Generation"},{"paperId":"26590b0c0e22b8c06c31ad51eda4fbab00a85e80","externalIds":{"DBLP":"conf/cvpr/MaHGGGK23","ArXiv":"2212.07796","DOI":"10.1109/CVPR52729.2023.01050","CorpusId":254685851},"title":"@ CREPE: Can Vision-Language Foundation Models Reason Compositionally?"},{"paperId":"f1c39410893794ee3643efa85be1816964aa85ea","externalIds":{"DBLP":"conf/cvpr/0001SMPNPOLFSB023","ArXiv":"2212.06909","DOI":"10.1109/CVPR52729.2023.01761","CorpusId":254636532},"title":"Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting"},{"paperId":"25de00096c45121a06668bc501f91adec5d0aff9","externalIds":{"ArXiv":"2212.05032","DBLP":"conf/iclr/FengHFJANBWW23","DOI":"10.48550/arXiv.2212.05032","CorpusId":254535649},"title":"Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis"},{"paperId":"33b1dbf1ad913047b919cd090907ffc4199b4178","externalIds":{"DBLP":"journals/corr/abs-2212-00280","ArXiv":"2212.00280","DOI":"10.48550/arXiv.2212.00280","CorpusId":254125445},"title":"GRiT: A Generative Region-to-text Transformer for Object Understanding"},{"paperId":"bdf4af8311637c681904e71cf50f96fd0026f578","externalIds":{"DBLP":"journals/corr/abs-2211-10440","ArXiv":"2211.10440","DOI":"10.1109/CVPR52729.2023.00037","CorpusId":253708074},"title":"Magic3D: High-Resolution Text-to-3D Content Creation"},{"paperId":"793939b83e10903f58d8edbb7534963df627a1fe","externalIds":{"DBLP":"journals/corr/abs-2211-07600","ArXiv":"2211.07600","DOI":"10.1109/CVPR52729.2023.01218","CorpusId":253510536},"title":"Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures"},{"paperId":"0231f2aed9a96cb516242fb57f2cb63f5651c4d8","externalIds":{"DBLP":"journals/corr/abs-2211-05105","ArXiv":"2211.05105","DOI":"10.1109/CVPR52729.2023.02157","CorpusId":253420366},"title":"Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","externalIds":{"DBLP":"conf/nips/SchuhmannBVGWCC22","ArXiv":"2210.08402","DOI":"10.48550/arXiv.2210.08402","CorpusId":252917726},"title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"10667c1ae4b49808772b5a377c5b52196701267f","externalIds":{"DBLP":"conf/iclr/Yuksekgonul0KJ023","ArXiv":"2210.01936","DOI":"10.48550/arXiv.2210.01936","CorpusId":252734947},"title":"When and why vision-language models behave like bags-of-words, and what to do about it?"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","externalIds":{"ArXiv":"2209.06794","DBLP":"journals/corr/abs-2209-06794","DOI":"10.48550/arXiv.2209.06794","CorpusId":252222320},"title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"efa1647594b236361610a20d507127f0586a379b","externalIds":{"DBLP":"journals/corr/abs-2209-04747","ArXiv":"2209.04747","DOI":"10.1109/TPAMI.2023.3261988","CorpusId":252199918,"PubMed":"37030794"},"title":"Diffusion Models in Vision: A Survey"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","externalIds":{"DBLP":"journals/corr/abs-2209-00796","ArXiv":"2209.00796","DOI":"10.1145/3626235","CorpusId":252070859},"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"5b19bf6c3f4b25cac96362c98b930cf4b37f6744","externalIds":{"ArXiv":"2208.12242","DBLP":"conf/cvpr/RuizLJPRA23","DOI":"10.1109/CVPR52729.2023.02155","CorpusId":251800180},"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"paperId":"03ae89e796b1a8b566ae1554fab65c8c88b3a55f","externalIds":{"ArXiv":"2207.12396","DBLP":"journals/corr/abs-2207-12396","DOI":"10.48550/arXiv.2207.12396","CorpusId":251040466},"title":"Exploring CLIP for Assessing the Look and Feel of Images"},{"paperId":"6b3e980b57663b335f1ea0535c074cb8f38bcab8","externalIds":{"DBLP":"journals/cgf/HartwigSOVHR22","DOI":"10.1111/cgf.14613","CorpusId":250121964},"title":"Learning Human Viewpoint Preferences from Sparsely Annotated Models"},{"paperId":"3ff7153fd6bd47d08084c7f50f8fd70026c126e7","externalIds":{"ArXiv":"2206.01714","DBLP":"conf/eccv/LiuLDTT22","DOI":"10.48550/arXiv.2206.01714","CorpusId":249375227},"title":"Compositional Visual Generation with Composable Diffusion Models"},{"paperId":"31f040eeae5674d5bc3cb3c376ff0aa44c5023c1","externalIds":{"DBLP":"conf/nips/KimKLYL22","ArXiv":"2205.13445","DOI":"10.48550/arXiv.2205.13445","CorpusId":249097749},"title":"Mutual Information Divergence: A Unified Metric for Multimodal Generative Models"},{"paperId":"f5c165b6317896a65151050201c737536fa17c31","externalIds":{"ArXiv":"2205.12005","DBLP":"conf/emnlp/LiXTWYBYCXCZHHZ22","ACL":"2022.emnlp-main.488","DOI":"10.48550/arXiv.2205.12005","CorpusId":249018141},"title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"9dae204dad41633188022002a04c8aa67c79a4e1","externalIds":{"DBLP":"journals/corr/abs-2205-06230","ArXiv":"2205.06230","DOI":"10.48550/arXiv.2205.06230","CorpusId":248721818},"title":"Simple Open-Vocabulary Object Detection with Vision Transformers"},{"paperId":"1b72a2842c535502f639e6082562723717582046","externalIds":{"ArXiv":"2204.13807","DBLP":"journals/corr/abs-2204-13807","DOI":"10.48550/arXiv.2204.13807","CorpusId":248476147},"title":"A very preliminary analysis of DALL-E 2"},{"paperId":"c435ecd0321dcec1f25e458bf930311f9e1d04b6","externalIds":{"ArXiv":"2204.03162","DBLP":"journals/corr/abs-2204-03162","DOI":"10.1109/CVPR52688.2022.00517","CorpusId":248006414},"title":"Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","externalIds":{"DBLP":"conf/nips/HoSGC0F22","ArXiv":"2204.03458","DOI":"10.48550/arXiv.2204.03458","CorpusId":248006185},"title":"Video Diffusion Models"},{"paperId":"92a2c42e9ad5c1bce57efc347f130667e261e7bf","externalIds":{"ArXiv":"2203.15799","DBLP":"conf/cvpr/LiM0X22","DOI":"10.1109/CVPR52688.2022.01766","CorpusId":247778704},"title":"StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis"},{"paperId":"212732c649d84382f4e74ca047b13f3c835591d7","externalIds":{"ArXiv":"2202.04053","DBLP":"conf/iccv/0001ZB23","DOI":"10.1109/ICCV51070.2023.00283","CorpusId":253510037},"title":"DALL-EVAL: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"d9973264638feb5c1adbbd729ba405eef8a23820","externalIds":{"ArXiv":"2112.01398","DBLP":"conf/eccv/DinhNH22","DOI":"10.1007/978-3-031-20059-5_34","CorpusId":250644532},"title":"TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation"},{"paperId":"b8638fc7561a0ea315952de19af2c2636e9d06b0","externalIds":{"DBLP":"conf/kdd/BaiLRK21","ArXiv":"2106.03062","DOI":"10.1145/3447548.3467198","CorpusId":235358253},"title":"On Training Sample Memorization: Lessons from Benchmarking Generative Modeling with a Large-scale Competition"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","externalIds":{"DBLP":"conf/cvpr/ZhangLHY0WCG21","DOI":"10.1109/CVPR46437.2021.00553","CorpusId":235692795},"title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"38b0567e83386ddc294d6c81b541deacbd8e3c2a","externalIds":{"DBLP":"conf/emnlp/HesselHFBC21","ACL":"2021.emnlp-main.595","ArXiv":"2104.08718","DOI":"10.18653/v1/2021.emnlp-main.595","CorpusId":233296711},"title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning"},{"paperId":"3fac34f504390cab973068cbc4cde3b279a99a93","externalIds":{"DBLP":"conf/iciai/Luo21","DOI":"10.1145/3461353.3461388","CorpusId":237412418},"title":"A Survey on Multimodal Deep Learning for Image Synthesis: Applications, methods, datasets, evaluation metrics, and results comparison"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"a87bb9e70d1127b6a9c0e721e48c0b58c997c70e","externalIds":{"ArXiv":"2102.10772","DBLP":"conf/iccv/HuS21","DOI":"10.1109/ICCV48922.2021.00147","CorpusId":237204499},"title":"UniT: Multimodal Multitask Learning with a Unified Transformer"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","externalIds":{"ArXiv":"2102.02779","DBLP":"journals/corr/abs-2102-02779","CorpusId":231802355},"title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","externalIds":{"ArXiv":"2102.03334","DBLP":"journals/corr/abs-2102-03334","CorpusId":231839613},"title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"2be4e374800a0db69695eb4c558a6653dd258fcd","externalIds":{"MAG":"3098358988","DBLP":"conf/eval4nlp/LeeYDKBJ20","ACL":"2020.eval4nlp-1.4","DOI":"10.18653/v1/2020.eval4nlp-1.4","CorpusId":226283802},"title":"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"5b92d901f7242d40071f060841f3509d04e5b2eb","externalIds":{"MAG":"3096601784","DBLP":"conf/eccv/LiangPL20","DOI":"10.1007/978-3-030-58548-8_29","CorpusId":226079201},"title":"CPGAN: Content-Parsing Generative Adversarial Networks for Text-to-Image Synthesis"},{"paperId":"3e86f5a0e2a97894de1cf1f1587799ac79bad0f2","externalIds":{"DBLP":"journals/corr/abs-2006-06666","ArXiv":"2006.06666","MAG":"3034588855","DOI":"10.1109/CVPR46437.2021.01101","CorpusId":219573658},"title":"VirTex: Learning Visual Representations from Textual Annotations"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","externalIds":{"DBLP":"journals/corr/abs-2006-06195","MAG":"3102995547","ArXiv":"2006.06195","CorpusId":219573512},"title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","externalIds":{"DBLP":"conf/emnlp/KhashabiMKSTCH20","MAG":"3099655892","ArXiv":"2005.00700","ACL":"2020.findings-emnlp.171","DOI":"10.18653/v1/2020.findings-emnlp.171","CorpusId":218487109},"title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"5db9878fa2fb61c05b1d3a41cdca400e098699fe","externalIds":{"ArXiv":"2003.08932","MAG":"3011418067","DBLP":"conf/eccv/GuBCW20","DOI":"10.1007/978-3-030-58621-8_22","CorpusId":213175539},"title":"GIQA: Generated Image Quality Assessment"},{"paperId":"14fdc18d9c164e5b0d6d946b3238c04e81921358","externalIds":{"MAG":"3035574324","DBLP":"conf/cvpr/KarrasLAHLA20","ArXiv":"1912.04958","DOI":"10.1109/cvpr42600.2020.00813","CorpusId":209202273},"title":"Analyzing and Improving the Image Quality of StyleGAN"},{"paperId":"ac32fd11fda311146f3d8d1a8abea96356a8f78e","externalIds":{"MAG":"2983674809","DBLP":"conf/cikm/JenkinsFWL19","DOI":"10.1145/3357384.3358001","CorpusId":207758338},"title":"Unsupervised Representation Learning of Spatial Data via Multimodal Embedding"},{"paperId":"b338cb632eee50743aa62956038b9ff157e8cc38","externalIds":{"DBLP":"journals/corr/abs-1910-13321","MAG":"2982450728","ArXiv":"1910.13321","DOI":"10.1109/TPAMI.2020.3021209","CorpusId":204949374,"PubMed":"32877332"},"title":"Semantic Object Accuracy for Generative Text-to-Image Synthesis"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","externalIds":{"DBLP":"conf/eccv/ChenLYK0G0020","MAG":"3090449556","DOI":"10.1007/978-3-030-58577-8_7","CorpusId":216080982},"title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"26500af6bb97bf2dab1cc14dfb3b8b08fef67940","externalIds":{"MAG":"2988823324","DBLP":"journals/corr/abs-1909-02701","ArXiv":"1909.02701","DOI":"10.1109/ICCV.2019.00475","CorpusId":202234815},"title":"Visual Semantic Reasoning for Image-Text Matching"},{"paperId":"ce01073130ff984eb43cbf43f6bdcbe4d5a09df9","externalIds":{"DBLP":"journals/corr/abs-1909-02050","MAG":"2972303997","ArXiv":"1909.02050","ACL":"D19-1220","DOI":"10.18653/v1/D19-1220","CorpusId":202537622},"title":"TIGEr: Text-to-Image Grounding for Image Caption Evaluation"},{"paperId":"4aa6298b606941a282d735fa3143da293199d2ca","externalIds":{"ArXiv":"1908.08530","MAG":"2995460200","DBLP":"conf/iclr/SuZCLLWD20","CorpusId":201317624},"title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","externalIds":{"MAG":"2969862959","DBLP":"conf/emnlp/TanB19","ACL":"D19-1514","ArXiv":"1908.07490","DOI":"10.18653/v1/D19-1514","CorpusId":201103729},"title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","externalIds":{"MAG":"2966715458","DBLP":"journals/corr/abs-1908-02265","ArXiv":"1908.02265","CorpusId":199453025},"title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"fbcea7715f62395fd5a83423d57556e8acca9a62","externalIds":{"ArXiv":"1907.09340","DBLP":"conf/acl/MadhyasthaWS19","MAG":"2963696820","ACL":"P19-1654","DOI":"10.18653/v1/P19-1654","CorpusId":196206571},"title":"VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions"},{"paperId":"ca42e4d7021d4e563bbeae7db35c1ce09fe38bfa","externalIds":{"DBLP":"conf/nips/RavuriV19","MAG":"2945027741","ArXiv":"1905.10887","CorpusId":166228599},"title":"Classification Accuracy Score for Conditional Generative Models"},{"paperId":"295065d942abca0711300b2b4c39829551060578","externalIds":{"MAG":"2936695845","ArXiv":"1904.09675","DBLP":"journals/corr/abs-1904-09675","CorpusId":127986044},"title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"d383dd8ced85d7898d8b1546c514a34fb626ea16","externalIds":{"DBLP":"journals/corr/abs-1904-06991","MAG":"2971128425","ArXiv":"1904.06991","CorpusId":118648975},"title":"Improved Precision and Recall Metric for Assessing Generative Models"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","externalIds":{"MAG":"2958882215","ArXiv":"1811.10830","DBLP":"conf/cvpr/ZellersBFC19","DOI":"10.1109/CVPR.2019.00688","CorpusId":53734356},"title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"0955252cd57db8503a2ed9e56f195fa44b1bc0d4","externalIds":{"MAG":"2901479108","ArXiv":"1811.10582","DBLP":"journals/corr/abs-1811-10582","CorpusId":53787909},"title":"Visual Entailment Task for Visually-Grounded Language Learning"},{"paperId":"22aab110058ebbd198edb1f1e7b4f69fb13c0613","externalIds":{"ArXiv":"1809.11096","MAG":"2893749619","DBLP":"conf/iclr/BrockDS19","CorpusId":52889459},"title":"Large Scale GAN Training for High Fidelity Natural Image Synthesis"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"35ebe95db7ab148e25904604d3b06a9412f6b4a4","externalIds":{"DBLP":"conf/acl/KirosCH18","ACL":"P18-1085","MAG":"2798881773","DOI":"10.18653/v1/P18-1085","CorpusId":48357847},"title":"Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search"},{"paperId":"6d3d61ef9b5ff6d41badbc3d40ea23acbbc9c3fe","externalIds":{"MAG":"2951859929","DBLP":"journals/corr/abs-1806-06422","ArXiv":"1806.06422","DOI":"10.1109/CVPR.2018.00608","CorpusId":46908873},"title":"Learning to Evaluate Image Captioning"},{"paperId":"1ea9f643171115e4a89e77c9a770c593f0794712","externalIds":{"MAG":"2951984970","DBLP":"journals/corr/abs-1806-00035","ArXiv":"1806.00035","CorpusId":44104089},"title":"Assessing Generative Models via Precision and Recall"},{"paperId":"da4e3270085fe5f59d9e89c072345c6600e7eb9a","externalIds":{"DBLP":"journals/corr/abs-1801-01973","MAG":"2782980316","ArXiv":"1801.01973","CorpusId":38384342},"title":"A Note on the Inception Score"},{"paperId":"722514cf193ea8b301475de9da5a0061f2e47bdd","externalIds":{"MAG":"2772330423","DOI":"10.23919/TST.2017.8195348","CorpusId":196150698},"title":"A survey of image synthesis and editing with generative adversarial networks"},{"paperId":"8b35c00edfa4edfd7a99d816e671023d2c000d55","externalIds":{"MAG":"2771088323","ArXiv":"1711.10485","DBLP":"conf/cvpr/XuZHZGH018","DOI":"10.1109/CVPR.2018.00143","CorpusId":8858625},"title":"AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","externalIds":{"MAG":"2963981733","DBLP":"conf/nips/HeuselRUNH17","CorpusId":326772},"title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"024d30897e0a2b036bc122163a954b7f1a1d0679","externalIds":{"MAG":"2585630030","ArXiv":"1612.02136","DBLP":"journals/corr/CheLJBL16","CorpusId":13002849},"title":"Mode Regularized Generative Adversarial Networks"},{"paperId":"cc18cb42289fd570a06896b5543b085ebabee57b","externalIds":{"ArXiv":"1611.08321","MAG":"2551915941","DBLP":"journals/corr/MaoXJY16","CorpusId":9461243},"title":"Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images"},{"paperId":"cfec1e2e670bc5de1a0621f66eaed7abb56387f3","externalIds":{"MAG":"2599043313","ArXiv":"1610.06545","DBLP":"conf/iclr/Lopez-PazO17","CorpusId":51758422},"title":"Revisiting Classifier Two-Sample Tests"},{"paperId":"f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f","externalIds":{"DBLP":"journals/corr/AndersonFJG16","MAG":"2950201573","ArXiv":"1607.08822","DOI":"10.1007/978-3-319-46454-1_24","CorpusId":11933981},"title":"SPICE: Semantic Propositional Image Caption Evaluation"},{"paperId":"571b0750085ae3d939525e62af510ee2cee9d5ea","externalIds":{"DBLP":"conf/nips/SalimansGZCRCC16","ArXiv":"1606.03498","MAG":"2949938177","CorpusId":1687220},"title":"Improved Techniques for Training GANs"},{"paperId":"154c22ca5eef149aedc8a986fa684ca1fd14e7dc","externalIds":{"MAG":"3037858973","DBLP":"journals/ijcv/RohrbachTRTPLCS17","ArXiv":"1605.03705","DOI":"10.1007/s11263-016-0987-1","CorpusId":18217052},"title":"Movie Description"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","externalIds":{"MAG":"2183341477","DBLP":"conf/cvpr/SzegedyVISW16","ArXiv":"1512.00567","DOI":"10.1109/CVPR.2016.308","CorpusId":206593880},"title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","externalIds":{"ArXiv":"1505.04870","DBLP":"conf/iccv/PlummerWCCHL15","MAG":"2568262903","DOI":"10.1007/s11263-016-0965-7","CorpusId":6941275},"title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","externalIds":{"DBLP":"journals/corr/AntolALMBZP15","MAG":"1933349210","ArXiv":"1505.00468","DOI":"10.1007/s11263-016-0966-6","CorpusId":3180429},"title":"VQA: Visual Question Answering"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"44040913380206991b1991daf1192942e038fe31","externalIds":{"ACL":"Q14-1006","DBLP":"journals/tacl/YoungLHH14","MAG":"2185175083","DOI":"10.1162/tacl_a_00166","CorpusId":3104920},"title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"051830b0ea58d1568f19ec3297e301d9789c9a76","externalIds":{"MAG":"1996418862","DBLP":"conf/cvpr/ZitnickP13","DOI":"10.1109/CVPR.2013.387","CorpusId":10554419},"title":"Bringing Semantics into Focus Using Visual Abstraction"},{"paperId":"9814df8bd00ba999c4d1e305a7e9bca579dc7c75","externalIds":{"MAG":"68733909","DBLP":"journals/jair/HodoshYH13","DOI":"10.1613/jair.3994","CorpusId":928608},"title":"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","externalIds":{"DBLP":"conf/nips/OrdonezKB11","MAG":"2109586012","CorpusId":14579301},"title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"bf60322f83714523e2d7c1d39983151fe9db7146","externalIds":{"DBLP":"conf/naacl/RashtchianYHH10","ACL":"W10-0721","MAG":"2119775030","CorpusId":5583509},"title":"Collecting Image Annotations Using Amazon’s Mechanical Turk"},{"paperId":"9bca4d7b932e0854c3325f1578cfd17341dd8ea8","externalIds":{"MAG":"2950536412","DBLP":"journals/corr/abs-0805-2368","ArXiv":"0805.2368","DOI":"10.7551/mitpress/7503.003.0069","CorpusId":1993257},"title":"A Kernel Method for the Two-Sample-Problem"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"16408af0fea79a138537cab1e4c0966ac10c8c01","externalIds":{"MAG":"2056716515","DOI":"10.1016/S0016-0032(96)00063-4","CorpusId":120842983},"title":"THE JENSEN-SHANNON DIVERGENCE"},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","externalIds":{"MAG":"2081580037","DBLP":"conf/naacl/Miller92","ACL":"H92-1116","DOI":"10.1145/219717.219748","CorpusId":1671874},"title":"WordNet: A Lexical Database for English"},{"paperId":"87d76615c649357d76c6adb5d317ced97851a0fc","externalIds":{"DBLP":"conf/nips/ParkALDR21","CorpusId":244906179},"title":"Benchmark for Compositional Text-to-Image Synthesis"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8b55402ffee2734bfc7d5d7595500916e1ef04e8","externalIds":{"MAG":"2904565150","DBLP":"conf/iccv/AgrawalAD0CJ0BP19","ArXiv":"1812.08658","DOI":"10.1109/ICCV.2019.00904","CorpusId":56517630},"title":"nocaps: novel object captioning at scale"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"}]}