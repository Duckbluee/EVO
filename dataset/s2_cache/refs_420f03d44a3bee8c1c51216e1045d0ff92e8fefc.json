{"references":[{"paperId":"ed8ac4ff13d32a291bbe74f3e5a138800bba47fd","externalIds":{"DBLP":"conf/cvpr/WangBDBPLAMSSW23","DOI":"10.1109/CVPR52729.2023.01838","CorpusId":260068316},"title":"Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks"},{"paperId":"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","externalIds":{"ArXiv":"2305.11172","DBLP":"journals/corr/abs-2305-11172","DOI":"10.48550/arXiv.2305.11172","CorpusId":258762390},"title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"},{"paperId":"03755613d50e1958a97bfaad2efb976f786fbb70","externalIds":{"DBLP":"journals/pami/LiuCHGZWT25","ArXiv":"2304.08345","DOI":"10.1109/TPAMI.2024.3479776","CorpusId":258179576,"PubMed":"39418158"},"title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"8165c92e8794cc197b5f9909487b79d1bcf2c0b2","externalIds":{"DBLP":"conf/cvpr/WangZSY23","ArXiv":"2212.09737","DOI":"10.1109/CVPR52729.2023.02226","CorpusId":254854456},"title":"Position-Guided Text Prompt for Vision-Language Pre-Training"},{"paperId":"47aece805c34fc9b29e0b9bcd0966703e5c128fd","externalIds":{"DOI":"10.1109/RAAI56146.2022.10092972","CorpusId":220786665},"title":"Reward Shaping Study for Sub-Goal Based Robotic Tasks"},{"paperId":"19ddeaf042fa00814a459e12d03f15801551e423","externalIds":{"DBLP":"journals/corr/abs-2211-15516","ArXiv":"2211.15516","DOI":"10.48550/arXiv.2211.15516","CorpusId":254043830},"title":"DQ-DETR: Dual Query Detection Transformer for Phrase Extraction and Grounding"},{"paperId":"ecfd114269f36420277015549dc13a57133158f1","externalIds":{"DBLP":"journals/corr/abs-2211-12402","ArXiv":"2211.12402","DOI":"10.1109/TPAMI.2023.3339661","CorpusId":260441709,"PubMed":"38090826"},"title":"X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq1-3339661.gif\"/></alternatives></inline-formula>-VLM: All-in-One Pre-Trained Model for Vision-Language Tasks"},{"paperId":"50502d0de333902d90ec0b68ce7339281281eeb9","externalIds":{"ArXiv":"2209.09019","DBLP":"journals/corr/abs-2209-09019","DOI":"10.48550/arXiv.2209.09019","CorpusId":252367367},"title":"LAVIS: A Library for Language-Vision Intelligence"},{"paperId":"aae2f4d5d2907153edef9e0af110d46930f814c2","externalIds":{"DBLP":"journals/corr/abs-2209-07526","ArXiv":"2209.07526","DOI":"10.48550/arXiv.2209.07526","CorpusId":252283985},"title":"OmniVL: One Foundation Model for Image-Language and Video-Language Tasks"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","externalIds":{"ArXiv":"2209.06794","DBLP":"journals/corr/abs-2209-06794","DOI":"10.48550/arXiv.2209.06794","CorpusId":252222320},"title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"599be9043ef3571f65758cf36e184c9dc1781baf","externalIds":{"DBLP":"journals/corr/abs-2208-06366","ArXiv":"2208.06366","DOI":"10.48550/arXiv.2208.06366","CorpusId":251554649},"title":"BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers"},{"paperId":"4c2668b3ae22fa592716480ec56012775b139f52","externalIds":{"DBLP":"journals/corr/abs-2206-08657","ArXiv":"2206.08657","DOI":"10.48550/arXiv.2206.08657","CorpusId":249848120},"title":"Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","externalIds":{"DBLP":"journals/corr/abs-2205-14100","ArXiv":"2205.14100","DOI":"10.48550/arXiv.2205.14100","CorpusId":249152323},"title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"f5c165b6317896a65151050201c737536fa17c31","externalIds":{"ArXiv":"2205.12005","DBLP":"conf/emnlp/LiXTWYBYCXCZHHZ22","ACL":"2022.emnlp-main.488","DOI":"10.48550/arXiv.2205.12005","CorpusId":249018141},"title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","externalIds":{"DBLP":"journals/corr/abs-2205-01917","ArXiv":"2205.01917","DOI":"10.48550/arXiv.2205.01917","CorpusId":248512473},"title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"5e76879aaea118b532fb24a50b721076d4c6ae93","externalIds":{"DBLP":"journals/corr/abs-2204-03610","ArXiv":"2204.03610","DOI":"10.1109/CVPR52688.2022.01857","CorpusId":248006101},"title":"Unified Contrastive Learning in Image-Text-Label Space"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","externalIds":{"DBLP":"conf/icml/WangYMLBLMZZY22","ArXiv":"2202.03052","CorpusId":246634906},"title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","externalIds":{"DBLP":"conf/cvpr/SinghHGCGRK22","ArXiv":"2112.04482","DOI":"10.1109/CVPR52688.2022.01519","CorpusId":244954250},"title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"3ea60cbce6c9065661d207fccf021c5d58a83f01","externalIds":{"DBLP":"conf/cvpr/0006GWY0LW22","ArXiv":"2111.12233","DOI":"10.1109/CVPR52688.2022.01745","CorpusId":244527510},"title":"Scaling Up Vision-Language Pretraining for Image Captioning"},{"paperId":"22312f763328cf540791de8c2449ea1e7436f476","externalIds":{"ArXiv":"2111.12085","DBLP":"conf/eccv/YangGW000LW22","DOI":"10.1007/978-3-031-20059-5_30","CorpusId":251105279},"title":"UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","externalIds":{"ArXiv":"2111.11432","DBLP":"journals/corr/abs-2111-11432","CorpusId":244477674},"title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"197d5867a45a2988f4dd159063cdfbfe90164962","externalIds":{"DBLP":"conf/cvpr/ZhaiWMSK0B22","ArXiv":"2111.07991","DOI":"10.1109/CVPR52688.2022.01759","CorpusId":244117175},"title":"LiT: Zero-Shot Transfer with Locked-image text Tuning"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","externalIds":{"DBLP":"journals/corr/abs-2111-02358","ArXiv":"2111.02358","CorpusId":241035439},"title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","externalIds":{"DBLP":"conf/cvpr/DouXGWWWZZYP0022","ArXiv":"2111.02387","DOI":"10.1109/CVPR52688.2022.01763","CorpusId":241033425},"title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"de0efc73850606f0c44eefe06854c9c23000f15c","externalIds":{"ArXiv":"2109.10504","DBLP":"conf/naacl/LiuWTL0D22","DOI":"10.18653/v1/2022.findings-naacl.119","CorpusId":237593048},"title":"KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","externalIds":{"DBLP":"journals/corr/abs-2108-10904","ArXiv":"2108.10904","CorpusId":237291550},"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","externalIds":{"DBLP":"journals/corr/abs-2107-07651","ArXiv":"2107.07651","CorpusId":236034189},"title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"800cfb3d23115cdcd4d114234b65bbdf2080f798","externalIds":{"DBLP":"conf/cvpr/DongBCZYYCG22","ArXiv":"2107.00652","DOI":"10.1109/CVPR52688.2022.01181","CorpusId":235694312},"title":"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","externalIds":{"DBLP":"conf/iclr/Bao0PW22","ArXiv":"2106.08254","CorpusId":235436185},"title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","externalIds":{"DBLP":"conf/nips/DaiLLT21","ArXiv":"2106.04803","CorpusId":235376986},"title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"9dcaf5ab101ba551ac334f3ede177a444e154643","externalIds":{"DBLP":"journals/corr/abs-2106-03089","ArXiv":"2106.03089","CorpusId":235358563},"title":"Referring Transformer: A One-step Approach to Multi-task Visual Grounding"},{"paperId":"fea02a76f504f6dfefd2497220da913c5274f5ab","externalIds":{"ArXiv":"2106.01804","DBLP":"conf/acl/XuYLBHXH20","ACL":"2021.acl-long.42","DOI":"10.18653/v1/2021.acl-long.42","CorpusId":235313626},"title":"E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","externalIds":{"DBLP":"conf/cvpr/ZhangLHY0WCG21","DOI":"10.1109/CVPR46437.2021.00553","CorpusId":235692795},"title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"7ba9c013988eaff5cd186d73704af329d027872d","externalIds":{"DBLP":"journals/corr/abs-2104-12763","ArXiv":"2104.12763","DOI":"10.1109/ICCV48922.2021.00180","CorpusId":233393962},"title":"MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"},{"paperId":"2fa4938001b18f464c62aa38a5a469bb92569d57","externalIds":{"ArXiv":"2104.03135","DBLP":"journals/corr/abs-2104-03135","DOI":"10.1109/CVPR46437.2021.01278","CorpusId":233169113},"title":"Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","externalIds":{"DBLP":"conf/cvpr/ChangpinyoSDS21","ArXiv":"2102.08981","DOI":"10.1109/CVPR46437.2021.00356","CorpusId":231951742},"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","externalIds":{"ArXiv":"2102.05918","DBLP":"conf/icml/JiaYXCPPLSLD21","CorpusId":231879586},"title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","externalIds":{"ArXiv":"2102.02779","DBLP":"journals/corr/abs-2102-02779","CorpusId":231802355},"title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","externalIds":{"ArXiv":"2102.03334","DBLP":"journals/corr/abs-2102-03334","CorpusId":231839613},"title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"5e5fbc41106db9acaaf3a365801051e477f0e984","externalIds":{"ArXiv":"2012.15409","DBLP":"journals/corr/abs-2012-15409","ACL":"2021.acl-long.202","DOI":"10.18653/v1/2021.acl-long.202","CorpusId":229924402},"title":"UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"},{"paperId":"ad7ddcc14984caae308c397f1a589aae75d4ab71","externalIds":{"ArXiv":"2012.12877","DBLP":"journals/corr/abs-2012-12877","CorpusId":229363322},"title":"Training data-efficient image transformers & distillation through attention"},{"paperId":"dec2f6d3215de9aa2d87d358b7933fb21eeb3bc0","externalIds":{"DBLP":"journals/tacl/BugliarelloCOE21","ArXiv":"2011.15124","DOI":"10.1162/tacl_a_00408","CorpusId":227238841},"title":"Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","externalIds":{"DBLP":"journals/corr/abs-2006-06195","MAG":"3102995547","ArXiv":"2006.06195","CorpusId":219573512},"title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","externalIds":{"MAG":"3096609285","DBLP":"conf/eccv/CarionMSUKZ20","ArXiv":"2005.12872","DOI":"10.1007/978-3-030-58452-8_13","CorpusId":218889832},"title":"End-to-End Object Detection with Transformers"},{"paperId":"b5ef0f91663f0cbd6910dec9a890c138f7ec10e0","externalIds":{"DBLP":"journals/corr/abs-2004-06165","MAG":"3091588028","ArXiv":"2004.06165","DOI":"10.1007/978-3-030-58577-8_8","CorpusId":215754208},"title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","externalIds":{"MAG":"3014611590","DBLP":"journals/corr/abs-2004-00849","ArXiv":"2004.00849","CorpusId":214775221},"title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","externalIds":{"DBLP":"journals/corr/abs-1910-01108","ArXiv":"1910.01108","MAG":"2978017171","CorpusId":203626972},"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","externalIds":{"MAG":"2996428491","DBLP":"journals/corr/abs-1909-11942","ArXiv":"1909.11942","CorpusId":202888986},"title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"54416048772b921720f19869ed11c2a360589d03","externalIds":{"MAG":"2975501350","DBLP":"journals/corr/abs-1909-11740","ArXiv":"1909.11740","CorpusId":202889174},"title":"UNITER: Learning UNiversal Image-TExt Representations"},{"paperId":"4aa6298b606941a282d735fa3143da293199d2ca","externalIds":{"ArXiv":"1908.08530","MAG":"2995460200","DBLP":"conf/iclr/SuZCLLWD20","CorpusId":201317624},"title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","externalIds":{"MAG":"2969862959","DBLP":"conf/emnlp/TanB19","ACL":"D19-1514","ArXiv":"1908.07490","DOI":"10.18653/v1/D19-1514","CorpusId":201103729},"title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"2bc1c8bd00bbf7401afcb5460277840fd8bab029","externalIds":{"ArXiv":"1908.06066","DBLP":"journals/corr/abs-1908-06066","MAG":"2998356391","DOI":"10.1609/AAAI.V34I07.6795","CorpusId":201058752},"title":"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","externalIds":{"ArXiv":"1908.03557","MAG":"2968124245","DBLP":"journals/corr/abs-1908-03557","CorpusId":199528533},"title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","externalIds":{"MAG":"2966715458","DBLP":"journals/corr/abs-1908-02265","ArXiv":"1908.02265","CorpusId":199453025},"title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","externalIds":{"DBLP":"journals/corr/abs-1907-10529","ACL":"2020.tacl-1.5","MAG":"2962753370","ArXiv":"1907.10529","DOI":"10.1162/tacl_a_00300","CorpusId":198229624},"title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"e9b13731027418ed38103d1dfc8a70f6881bc684","externalIds":{"MAG":"2903867219","ArXiv":"1812.05252","DBLP":"conf/cvpr/GaoJYLHWL19","DOI":"10.1109/CVPR.2019.00680","CorpusId":54700454},"title":"Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","externalIds":{"MAG":"2958882215","ArXiv":"1811.10830","DBLP":"conf/cvpr/ZellersBFC19","DOI":"10.1109/CVPR.2019.00688","CorpusId":53734356},"title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"cf336d272a30d6ad6141db67faa64deb8791cd61","externalIds":{"MAG":"2963530300","DBLP":"conf/acl/SuhrZZZBA19","ACL":"P19-1644","ArXiv":"1811.00491","DOI":"10.18653/v1/P19-1644","CorpusId":53178856},"title":"A Corpus for Reasoning about Natural Language Grounded in Photographs"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","externalIds":{"MAG":"2885185669","DBLP":"journals/corr/abs-1808-06226","ArXiv":"1808.06226","ACL":"D18-2012","DOI":"10.18653/v1/D18-2012","CorpusId":52051958},"title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"fdce9cbe5c726201575b3c8a8c1af0752f1af53f","externalIds":{"MAG":"2952593550","DBLP":"journals/corr/abs-1801-08186","ArXiv":"1801.08186","DOI":"10.1109/CVPR.2018.00142","CorpusId":3441497},"title":"MAttNet: Modular Attention Network for Referring Expression Comprehension"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","externalIds":{"ArXiv":"1609.08144","MAG":"2525778437","DBLP":"journals/corr/WuSCLNMKCGMKSJL16","CorpusId":3603249},"title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","externalIds":{"DBLP":"conf/acl/SennrichHB16a","ACL":"P16-1162","MAG":"1816313093","ArXiv":"1508.07909","DOI":"10.18653/v1/P16-1162","CorpusId":1114678},"title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","externalIds":{"DBLP":"conf/iccv/ZhuKZSUTF15","MAG":"1566289585","ArXiv":"1506.06724","DOI":"10.1109/ICCV.2015.11","CorpusId":6866988},"title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"f8e79ac0ea341056ef20f2616628b3e964764cfd","externalIds":{"ArXiv":"1506.02640","MAG":"2950099460","DBLP":"journals/corr/RedmonDGF15","DOI":"10.1109/CVPR.2016.91","CorpusId":206594738},"title":"You Only Look Once: Unified, Real-Time Object Detection"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","externalIds":{"ArXiv":"1505.04870","DBLP":"conf/iccv/PlummerWCCHL15","MAG":"2568262903","DOI":"10.1007/s11263-016-0965-7","CorpusId":6941275},"title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","externalIds":{"DBLP":"journals/corr/AntolALMBZP15","MAG":"1933349210","ArXiv":"1505.00468","DOI":"10.1007/s11263-016-0966-6","CorpusId":3180429},"title":"VQA: Visual Question Answering"},{"paperId":"7ffdbc358b63378f07311e883dddacc9faeeaf4b","externalIds":{"ArXiv":"1504.08083","CorpusId":206770307},"title":"Fast R-CNN"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","externalIds":{"ACL":"D14-1086","DBLP":"conf/emnlp/KazemzadehOMB14","MAG":"2251512949","DOI":"10.3115/v1/D14-1086","CorpusId":6308361},"title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","externalIds":{"ArXiv":"1409.0575","DBLP":"journals/corr/RussakovskyDSKSMHKKBBF14","MAG":"2546241758","DOI":"10.1007/s11263-015-0816-y","CorpusId":2930547},"title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","externalIds":{"DBLP":"conf/nips/OrdonezKB11","MAG":"2109586012","CorpusId":14579301},"title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"4106342a079eecadd4a4a1d39eabd4a50e0a7cd1","externalIds":{"MAG":"2987976854","DBLP":"journals/corr/cs-AI-9906002","ArXiv":"cs/9906002","DOI":"10.1016/0167-2789(90)90087-6","CorpusId":3204300},"title":"The Symbol Grounding Problem"},{"paperId":"97da3aaa0dfbd32942ca99c60329d590b1234937","externalIds":{"DBLP":"journals/corr/abs-2206-07699","DOI":"10.48550/arXiv.2206.07699","CorpusId":249674762},"title":"Prefix Language Models are Unified Modal Learners"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","externalIds":{"ArXiv":"2103.14030","DBLP":"conf/iccv/LiuL00W0LG21","DOI":"10.1109/ICCV48922.2021.00986","CorpusId":232352874},"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8b55402ffee2734bfc7d5d7595500916e1ef04e8","externalIds":{"MAG":"2904565150","DBLP":"conf/iccv/AgrawalAD0CJ0BP19","ArXiv":"1812.08658","DOI":"10.1109/ICCV.2019.00904","CorpusId":56517630},"title":"nocaps: novel object captioning at scale"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"}]}