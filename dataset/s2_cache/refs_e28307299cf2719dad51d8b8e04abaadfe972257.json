{"references":[{"paperId":"1112ac74f9299838c8a354f778fbbfd951dfc50c","externalIds":{"DBLP":"journals/ton/JinBZZZL24","DOI":"10.1109/TNET.2024.3355010","CorpusId":267249623},"title":"DistMind: Efficient Resource Disaggregation for Deep Learning Workloads"},{"paperId":"df83494879ea9d6c7e984eba36b2c653ab8636de","externalIds":{"DBLP":"journals/tc/GaoYSZW24","DOI":"10.1109/TC.2024.3371794","CorpusId":268151259},"title":"UniSched: A Unified Scheduler for Deep Learning Training Jobs With Different User Demands"},{"paperId":"b17d73d5b9e78014fec495a7ebb97fd058fc399e","externalIds":{"DBLP":"journals/tpds/LiuWHP24","DOI":"10.1109/TPDS.2024.3390109","CorpusId":269228633},"title":"Sampling-Based Multi-Job Placement for Heterogeneous Deep Learning Clusters"},{"paperId":"88fc3fbe452982507bd4e124644e7d27b9113ccb","externalIds":{"DBLP":"conf/ics/GaoZHGS0024","DOI":"10.1145/3650200.3656598","CorpusId":270227282},"title":"AutoSched: An Adaptive Self-configured Framework for Scheduling Deep Learning Training Workloads"},{"paperId":"2d6bd2c05c05691c8f30eef54e01eab750295f19","externalIds":{"DBLP":"conf/asplos/SunCWF0WC24","DOI":"10.1145/3620666.3651359","CorpusId":269364682},"title":"AdaPipe: Optimizing Pipeline Parallelism with Adaptive Recomputation and Partitioning"},{"paperId":"3ec78834e1295498450fee9521e13d8b36c7c598","externalIds":{"DBLP":"conf/asplos/MoX024","DOI":"10.1145/3620665.3640375","CorpusId":269317086},"title":"Heet: Accelerating Elastic Training in Heterogeneous Deep Learning Clusters"},{"paperId":"423d83256f500fa413da9d9d1af474078754ff05","externalIds":{"DBLP":"conf/eurosys/StratiMK24","DOI":"10.1145/3627703.3629578","CorpusId":265531176},"title":"Orion: Interference-aware, Fine-grained GPU Sharing for ML Applications"},{"paperId":"83858f08aef77ced91428207b8891a82e7087cd2","externalIds":{"DBLP":"conf/eurosys/ShiPWLRHYLC24","DOI":"10.1145/3627703.3650083","CorpusId":269243508},"title":"ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling"},{"paperId":"d83f751e56af189402e3041e82562a342cf67aff","externalIds":{"DBLP":"journals/corr/abs-2404-06114","ArXiv":"2404.06114","DOI":"10.48550/arXiv.2404.06114","CorpusId":269009419},"title":"Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey"},{"paperId":"f1342395afadd4e4cd5086d8f349eb6cdc231cfe","externalIds":{"DBLP":"journals/fgcs/WangCS24","DOI":"10.1016/j.future.2023.10.022","CorpusId":264982931},"title":"GPARS: Graph predictive algorithm for efficient resource scheduling in heterogeneous GPU clusters"},{"paperId":"1ff785b59f218dd430007b8a735fd2e1f461ce04","externalIds":{"DBLP":"journals/tcad/LiuGZZJLLZ24","DOI":"10.1109/TCAD.2023.3314388","CorpusId":261793834},"title":"Ace-Sniper: Cloud–Edge Collaborative Scheduling Framework With DNN Inference Latency Modeling on Heterogeneous Devices"},{"paperId":"883b5a6cbbf3c499c8402204a657abf1e836d310","externalIds":{"DBLP":"journals/csur/YeGHSWLZW24","DOI":"10.1145/3638757","CorpusId":266561946},"title":"Deep Learning Workload Scheduling in GPU Datacenters: A Survey"},{"paperId":"30c493ed26e38786e7ddfbec279fa121566b198f","externalIds":{"DBLP":"journals/tcc/FilippiniAAG24","DOI":"10.1109/TCC.2023.3336540","CorpusId":265434101},"title":"A Stochastic Approach for Scheduling AI Training Jobs in GPU-Based Systems"},{"paperId":"174f881f77cb7d7adfadb956e00193d927ac06f6","externalIds":{"DBLP":"journals/tpds/YuDL23","DOI":"10.1109/TPDS.2023.3323282","CorpusId":263848726},"title":"Communication Optimization Algorithms for Distributed Deep Learning Systems: A Survey"},{"paperId":"9a85cd8a2cc809e740485e1904dd5f6b15fcc371","externalIds":{"DBLP":"conf/sc/LiXYSZRLJLLLQ23","DOI":"10.1145/3581784.3607054","CorpusId":264591554},"title":"EasyScale: Elastic Training with Consistent Accuracy and Improved Utilization on GPUs"},{"paperId":"8fae24ba59f06b5097ed4f8d6e2f2efde144280a","externalIds":{"DBLP":"conf/cloud/ChengCYMBGX23","DOI":"10.1145/3620678.3624661","CorpusId":264907681},"title":"Towards GPU Memory Efficiency for Distributed Training at Scale"},{"paperId":"42ac39327eb25d2e757b55eb5eb180f6a55dd020","externalIds":{"DBLP":"journals/tcc/YangBLYW23","DOI":"10.1109/TCC.2023.3308161","CorpusId":261143453},"title":"On a Meta Learning-Based Scheduler for Deep Learning Clusters"},{"paperId":"e97addc2c9d137ca53a73d41ad59083c1a4cf214","externalIds":{"DBLP":"journals/corr/abs-2309-08125","ArXiv":"2309.08125","DOI":"10.1145/3600006.3613152","CorpusId":262012886},"title":"Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates"},{"paperId":"dda9e433ac572636b57d05c1a1bf529fb4df2973","externalIds":{"DBLP":"conf/icpp/GuZWCG23","ArXiv":"2309.00558","DOI":"10.1145/3605573.3605638","CorpusId":261494131},"title":"FaST-GShare: Enabling Efficient Spatio-Temporal GPU Sharing in Serverless Computing for Deep Learning Inference"},{"paperId":"c095361942179c96cae0c30248fb6ff80b0bc390","externalIds":{"ArXiv":"2308.00852","DBLP":"conf/nsdi/RajasekaranGA24","DOI":"10.48550/arXiv.2308.00852","CorpusId":260378738},"title":"CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters"},{"paperId":"63c1052d76098022b1d3a3811e1bfcaca9077bca","externalIds":{"DBLP":"journals/tc/YangWXWZZ23","DOI":"10.1109/TC.2023.3242200","CorpusId":256596661},"title":"Hydra: Deadline-Aware and Efficiency-Oriented Scheduling for Deep Learning Jobs on Heterogeneous GPUs"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"dc50eeab0e4d04a968f596555f71e943b9f11fec","externalIds":{"DBLP":"conf/IEEEcloud/FerikoglouCTKMS23","DOI":"10.1109/CLOUD60044.2023.00021","CorpusId":262967314},"title":"IRIS: Interference and Resource Aware Predictive Orchestration for ML Inference Serving"},{"paperId":"ac54fee4b4160e6cdf56fa95652a102abacf91d5","externalIds":{"DBLP":"journals/tpds/DuanPWXLWL23","DOI":"10.1109/TPDS.2023.3250462","CorpusId":257276320},"title":"Accelerating Distributed DNN Training via Transport Layer Scheduling"},{"paperId":"5a6e344dfa48e20cec414fb0691698beefa8cd3e","externalIds":{"DBLP":"journals/jsac/CaoBDELPZ23a","DOI":"10.1109/JSAC.2023.3242710","CorpusId":256654957},"title":"Communication-Efficient Distributed Learning: An Overview"},{"paperId":"f8ae23caf5047f21acd178496da29d71a10602bd","externalIds":{"DBLP":"journals/tcc/MaoSZCGL23","DOI":"10.1109/TCC.2022.3194128","CorpusId":251142102},"title":"Elastic Resource Management for Deep Learning Applications in a Container Cluster"},{"paperId":"8425a5d93af72796e074876c7ea4876325d3b92a","externalIds":{"ArXiv":"2303.13803","DBLP":"journals/corr/abs-2303-13803","DOI":"10.48550/arXiv.2303.13803","CorpusId":257757330},"title":"MuxFlow: Efficient and Safe GPU Sharing in Large-Scale Production Deep Learning Clusters"},{"paperId":"f8e95d07e78369d3b792753734a58d1c14dd2f24","externalIds":{"PubMedCentral":"10033450","DOI":"10.1038/s41591-023-02225-7","CorpusId":257309821,"PubMed":"36864252"},"title":"A deep-learning algorithm to classify skin lesions from mpox virus infection"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"3822cb0a089a66f2ad88e7960fcae6ebdc4e4427","externalIds":{"ArXiv":"2302.12445","DBLP":"conf/icdcs/ZhangSCWLL23","DOI":"10.1109/ICDCS57875.2023.00054","CorpusId":259164359},"title":"DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining"},{"paperId":"ffdd341df1e84caca4d717959af2a82d009aa14c","externalIds":{"DBLP":"journals/tvt/SaikiaBSL23","DOI":"10.1109/TVT.2022.3211652","CorpusId":252728720},"title":"Signal Detection in GSM-Based In-Band Full-Duplex Communication Using DNN"},{"paperId":"5278b81db686b4d36143941bff1c683bea963a63","externalIds":{"DBLP":"journals/corr/abs-2301-11913","ArXiv":"2301.11913","DOI":"10.48550/arXiv.2301.11913","CorpusId":251542095},"title":"SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient"},{"paperId":"8f268cd103bd55e76e010e916412f0da5bc34508","externalIds":{"DOI":"10.56726/irjmets32980","CorpusId":255874721},"title":"A STUDY ON GENETIC ALGORITHM AND ITS APPLICATIONS"},{"paperId":"637c212a287c394e50710ccd1f2f8e45677fe793","externalIds":{"ArXiv":"2212.09668","DBLP":"journals/corr/abs-2212-09668","DOI":"10.1109/MWC.006.2200494","CorpusId":254854349},"title":"Task-Oriented Communications for NextG: End-to-end Deep Learning and AI Security Aspects"},{"paperId":"22bc6d8c6fe96758ccf7099fcf482fce5525889f","externalIds":{"DBLP":"conf/cloud/GaoS0022","DOI":"10.1145/3542929.3563460","CorpusId":253385756},"title":"Titan: a scheduler for foundation model fine-tuning workloads"},{"paperId":"d54ebb7b83e81013dc75e12b31fd9e29e587cf4c","externalIds":{"DBLP":"journals/corr/abs-2211-01713","ArXiv":"2211.01713","DOI":"10.1109/TPDS.2022.3232715","CorpusId":253264995},"title":"iGniter: Interference-Aware GPU Resource Provisioning for Predictable DNN Inference in the Cloud"},{"paperId":"e2f20d95928dcaa2293a0970e431adbf3d1635e0","externalIds":{"MAG":"3106918569","DBLP":"journals/tcc/LiCLQXZ22","DOI":"10.1109/TCC.2020.3040312","CorpusId":229606876},"title":"Efficient Online Scheduling for Coflow-Aware Machine Learning Clusters"},{"paperId":"8798b3a01c29fe0ce45a271bedd934787343dfb5","externalIds":{"DOI":"10.1111/1911-3846.12832","CorpusId":252666016},"title":"FinBERT\n : A Large Language Model for Extracting Information from Financial Text†"},{"paperId":"baa467a4dccf87bc7e2c5a4ea6fd5e401d962d39","externalIds":{"DBLP":"conf/cluster/LiuLLDGL22","DOI":"10.1109/CLUSTER51413.2022.00042","CorpusId":252999300},"title":"AutoPipe: A Fast Pipeline Parallelism Approach with Balanced Partitioning and Micro-batch Slicing"},{"paperId":"e66384bfc81d0c9357ce5e692a1b42a30bbe48e1","externalIds":{"DBLP":"conf/icpp/LiuYD22","DOI":"10.1145/3545008.3545027","CorpusId":255775678},"title":"Adaptive and Efficient GPU Time Sharing for Hyperparameter Tuning in Cloud"},{"paperId":"ff749d42b2822b6124f404c19c8bf5fd904f8cd9","externalIds":{"DBLP":"conf/dac/LiuGZCL22","DOI":"10.1145/3489517.3530474","CorpusId":251744185},"title":"Sniper: Cloud-Edge Collaborative Inference Scheduling with Neural Network Similarity Modeling"},{"paperId":"6b117a8dcaa161562b0a69afbb9811e11afb5b3e","externalIds":{"DBLP":"journals/corr/abs-2206-01288","ArXiv":"2206.01288","DOI":"10.48550/arXiv.2206.01288","CorpusId":249375466},"title":"Decentralized Training of Foundation Models in Heterogeneous Environments"},{"paperId":"b2bc82192fdbb58d0d244768addac5fa93c9962e","externalIds":{"DBLP":"journals/tnsm/HeCZSLYG22","DOI":"10.1109/tnsm.2021.3132361","CorpusId":244886208},"title":"Beamer: Stage-Aware Coflow Scheduling to Accelerate Hyper-Parameter Tuning in Deep Learning Clusters"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"533beb3cc4b99f05ac2f3baae90e1bab07fd93a2","externalIds":{"DBLP":"conf/eurosys/OhLKS22","DOI":"10.1145/3492321.3519563","CorpusId":247765527},"title":"Out-of-order backprop: an effective scheduling technique for deep learning"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","externalIds":{"ArXiv":"2203.13474","DBLP":"conf/iclr/NijkampPHTWZSX23","CorpusId":252668917},"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"15822ae7035dec57b3126dac84386a415cb4d59e","externalIds":{"ArXiv":"2202.07896","DBLP":"journals/corr/abs-2202-07896","DOI":"10.1145/3552326.3587445","CorpusId":258508802},"title":"Lyra: Elastic Scheduling for Deep Learning Clusters"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","externalIds":{"ArXiv":"2201.11990","DBLP":"journals/corr/abs-2201-11990","CorpusId":246411325},"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","externalIds":{"DBLP":"journals/corr/abs-2201-08239","ArXiv":"2201.08239","CorpusId":246063428},"title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"d1e3f04fb2c6a84c35d1ee3be6cb5fbd2b029043","externalIds":{"DBLP":"journals/corr/abs-2201-01684","ArXiv":"2201.01684","DOI":"10.1109/TPDS.2021.3137867","CorpusId":245457305},"title":"Dynamic GPU Energy Optimization for Machine Learning Training Workloads"},{"paperId":"8b67d9535efd60194d37f6c76b67d53bd8015be5","externalIds":{"DBLP":"journals/tpds/YeungBYFHG22","DOI":"10.1109/TPDS.2021.3079202","CorpusId":235692248},"title":"Horus: Interference-Aware and Prediction-Based Scheduling in Deep Learning Systems"},{"paperId":"157bddb85786b980c962e649bed33d8ebbf7b4a1","externalIds":{"DBLP":"journals/corr/abs-2111-06061","ArXiv":"2111.06061","DOI":"10.1109/TKDE.2022.3178211","CorpusId":243985820},"title":"Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI"},{"paperId":"e581a00152e3764d56277a409bc2c46f6092c484","externalIds":{"DBLP":"conf/cloud/GaoYS0021","DOI":"10.1145/3472883.3486978","CorpusId":239890151},"title":"Chronus: A Novel Deadline-aware Scheduler for Deep Learning Training Jobs"},{"paperId":"5b8f5a218ceecf164886135741a7416553dce7e2","externalIds":{"DBLP":"conf/cloud/WangYYWLSHZ21","DOI":"10.1145/3472883.3486987","CorpusId":239890145},"title":"Morphling: Fast, Near-Optimal Auto-Configuration for Cloud-Native Model Serving"},{"paperId":"ac35dffd21c16b02e140a36726b3a21d266cab0f","externalIds":{"DBLP":"conf/sc/Hu0Y0021","ArXiv":"2109.01313","DOI":"10.1145/3458817.3476223","CorpusId":237417338},"title":"Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters"},{"paperId":"d9548bd87381749c8822d34abc786de8c7e305f7","externalIds":{"DBLP":"conf/icpp/ZhangQSCX21","DOI":"10.1145/3472456.3472467","CorpusId":238359072},"title":"Prophet: Speeding up Distributed DNN Training with Predictable Communication Scheduling"},{"paperId":"10f3ca78e194552427ebe9173b19d1b910469e27","externalIds":{"DBLP":"conf/sc/0002H21","ArXiv":"2107.06925","DOI":"10.1145/3458817.3476145","CorpusId":235898937},"title":"Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"},{"paperId":"cc4294064134d32a075048a43be698ce6832cfab","externalIds":{"MAG":"3175487073","DBLP":"conf/aaai/Say21","DOI":"10.1609/aaai.v35i6.16635","CorpusId":235306075},"title":"A Unified Framework for Planning with Learned Neural Network Transition Models"},{"paperId":"546264399f3914e87f1ad6aaf2f1a132aba6073c","externalIds":{"DBLP":"conf/infocom/YuWJL21","ArXiv":"2105.13855","DOI":"10.1109/INFOCOM42981.2021.9488916","CorpusId":231978223},"title":"A Sum-of-Ratios Multi-Dimensional-Knapsack Decomposition for DNN Resource Scheduling"},{"paperId":"f938cffd498ffb81ee9d66b4cd473e82c2e12c72","externalIds":{"DBLP":"journals/corr/abs-2104-14362","ArXiv":"2104.14362","DOI":"10.1007/s10115-022-01664-x","CorpusId":233444166},"title":"From distributed machine learning to federated learning: a survey"},{"paperId":"a3893072ac8aaaa01cbd754a59fabc4d6c1adb2c","externalIds":{"DOI":"10.1073/pnas.2024789118","CorpusId":233371133,"PubMed":"33888586"},"title":"Communication-efficient federated learning"},{"paperId":"774591fdd988eaaff3917e7c5171d044b0843e63","externalIds":{"ArXiv":"2104.04473","DBLP":"conf/sc/NarayananSCLPKV21","DOI":"10.1145/3458817.3476209","CorpusId":236635565},"title":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"},{"paperId":"344953cc0367ccd91fa7f6bd5497d2b4e3b6d3bf","externalIds":{"DBLP":"conf/hpca/LiaoTXLZYH21","DOI":"10.1109/HPCA51647.2021.00071","CorpusId":233376329},"title":"Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing : Industry Track Paper"},{"paperId":"6d0c42fb3fe2160708b8afe4d130521e46fb902c","externalIds":{"MAG":"3102606411","DBLP":"journals/jpdc/OuyangDXX21","DOI":"10.1016/j.jpdc.2020.11.005","CorpusId":227143064},"title":"Communication optimization strategies for distributed deep neural network training: A survey"},{"paperId":"af9806c22e8b78d9d995c5f8f85cf7bf367a873a","externalIds":{"DBLP":"conf/conext/0002LS20","MAG":"3108033633","DOI":"10.1145/3386367.3432588","CorpusId":227154870},"title":"Job scheduling for large-scale machine learning clusters"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"9d64a1dc36c851a334bd60e0721588af443adb63","externalIds":{"DBLP":"conf/cloud/DhakalKR20","MAG":"3097411828","DOI":"10.1145/3419111.3421284","CorpusId":222296326},"title":"GSLICE: controlled spatial sharing of GPUs for a scalable inference platform"},{"paperId":"ff56d18c46b6aa3c4fc03927ee0402ae0410522c","externalIds":{"DBLP":"journals/corr/abs-2010-04116","MAG":"3092309115","ArXiv":"2010.04116","CorpusId":222208763},"title":"Interlocking Backpropagation: Improving depthwise model-parallelism"},{"paperId":"cc7c2b1e6eb6475f066982ba21a4cbb0e75a1acf","externalIds":{"DBLP":"conf/IEEEcloud/KangYYY20","DOI":"10.1109/CLOUD49709.2020.00014","CorpusId":229357897},"title":"TensorExpress: In-Network Communication Scheduling for Distributed Deep Learning"},{"paperId":"6de9eb6cb68185dd311be3093375527e5b56330d","externalIds":{"MAG":"3087041179","DBLP":"journals/tcyb/VuCNNHD22","DOI":"10.1109/TCYB.2020.3013416","CorpusId":221798943,"PubMed":"32946404"},"title":"Learning Latent Representation for IoT Anomaly Detection"},{"paperId":"2adcdc9e9e81147499e1372f992d25ac6265fb29","externalIds":{"DBLP":"conf/osdi/QiaoCSNH0GX21","MAG":"3080451848","ArXiv":"2008.12260","CorpusId":221340551},"title":"Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning"},{"paperId":"fd2a2caefd0c986701aa676d14839c723e570edf","externalIds":{"DBLP":"conf/icpp/SultanaCXY20","MAG":"3047783725","DOI":"10.1145/3404397.3404415","CorpusId":221070168},"title":"E-LAS: Design and Analysis of Completion-Time Agnostic Scheduling for Distributed Deep Learning Cluster"},{"paperId":"23d69b7b5eb1c7c2423d213c8ccbdad96c329918","externalIds":{"MAG":"3047002910","DBLP":"conf/infocom/WangLG20","DOI":"10.1109/infocom41043.2020.9155282","CorpusId":221087223},"title":"Geryon: Accelerating Distributed CNN Training by Network-Level Flow Scheduling"},{"paperId":"31f8c61248d6994c6f53942436756356cbefc332","externalIds":{"MAG":"3047371394","DBLP":"conf/infocom/LiHZLT20","DOI":"10.1109/infocom41043.2020.9155267","CorpusId":219965190},"title":"Automating Cloud Deployment for Deep Learning Inference of Real-time Online Services"},{"paperId":"9d9dbb4487aca2b62ca3659446d7010ac65aa642","externalIds":{"MAG":"3045098739","DBLP":"conf/usenix/ParkYYNLCNC20","ArXiv":"2005.14038","CorpusId":218971732},"title":"HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"ee81584f7e426b891765fe122655fe466b00a072","externalIds":{"DBLP":"journals/tifs/FengHLWC20","MAG":"3032769455","DOI":"10.1109/TIFS.2020.2997134","CorpusId":219877325},"title":"SecureNLP: A System for Multi-Party Privacy-Preserving Natural Language Processing"},{"paperId":"7c8351ac6e11327a12cb7a0c24debd3a4ce81b7d","externalIds":{"MAG":"3012536640","DBLP":"journals/jnca/ZhouHLYS20","DOI":"10.1016/j.jnca.2020.102590","CorpusId":215865690},"title":"JPAS: Job-progress-aware flow scheduling for deep learning clusters"},{"paperId":"b6e8d81bc70af67a71d288563f5eb0daf67efc3b","externalIds":{"DBLP":"journals/corr/abs-2004-00363","ArXiv":"2004.00363","MAG":"3014216009","DOI":"10.1109/GLOBECOM42002.2020.9348191","CorpusId":214743008},"title":"DNN-based Localization from Channel Estimates: Feature Design and Experimental Results"},{"paperId":"a137be632a6248c31f564e076d7587d2206b4f2d","externalIds":{"ArXiv":"2003.06307","CorpusId":261494342},"title":"Communication-Efficient Distributed Deep Learning: A Comprehensive Survey"},{"paperId":"47a8355e76c3675c481f135cce3a5911c74aeac3","externalIds":{"MAG":"3007279920","DBLP":"journals/comsur/ShiYJZL20","ArXiv":"2002.09668","DOI":"10.1109/COMST.2020.3007787","CorpusId":211258847},"title":"Communication-Efficient Edge AI: Algorithms and Systems"},{"paperId":"67b6fa1b19113619a70eb443f056ffdabaa92ca7","externalIds":{"DBLP":"journals/tpds/ChenQWFYZL20","MAG":"2964667342","DOI":"10.1109/TPDS.2019.2931558","CorpusId":201142972},"title":"Deep Learning Research and Development Platform: Characterizing and Scheduling with QoS Guarantees on GPU Clusters"},{"paperId":"63582943e96bacbfed81f70b1ee3b91543ed01f6","externalIds":{"DBLP":"journals/tpds/ShiCL21","MAG":"2996209028","ArXiv":"1912.09268","DOI":"10.1109/TPDS.2021.3052862","CorpusId":209414722},"title":"MG-WFBP: Merging Gradients Wisely for Efficient Communication in Distributed Deep Learning"},{"paperId":"3ea5bde61362d0dfcbe2751da8dd380adc9f7945","externalIds":{"DBLP":"conf/sc/QinZZYZ019","MAG":"2986864338","DOI":"10.1145/3295500.3356164","CorpusId":207953474},"title":"Swift Machine Learning Model Serving Scheduling: A Region Based Reinforcement Learning Approach"},{"paperId":"3fd7c9ba742dd2b435afa75217847e5087e2f2a8","externalIds":{"DBLP":"conf/sosp/NarayananHPSDGG19","MAG":"2969388332","DOI":"10.1145/3341301.3359646","CorpusId":202488191},"title":"PipeDream: generalized pipeline parallelism for DNN training"},{"paperId":"8b9b2f1ec32040c30d2c296ac1688a6627a8fc12","externalIds":{"DBLP":"conf/sosp/ShenCJZKPKS19","MAG":"2982157693","DOI":"10.1145/3341301.3359658","CorpusId":204812163},"title":"Nexus: a GPU cluster engine for accelerating DNN-based video analysis"},{"paperId":"b213b1a2a80e4d1fb0daeabbdd7196ee4b745fae","externalIds":{"MAG":"2982674764","DBLP":"conf/icccs/JahaniLCAA019","DOI":"10.1109/CCCS.2019.8888151","CorpusId":207833426},"title":"Optimizing on-demand GPUs in the Cloud for Deep Learning Applications Training"},{"paperId":"2734952fe7a88de7cfb16be2dba3cb655c87022d","externalIds":{"DBLP":"journals/corr/abs-1909-06526","MAG":"2972392561","ArXiv":"1909.06526","DOI":"10.1145/3361525.3361538","CorpusId":202577606},"title":"FfDL: A Flexible Multi-tenant Deep Learning Platform"},{"paperId":"44ea486d6d9d33ea654ba880404c99086e603d7d","externalIds":{"MAG":"2977714483","DBLP":"conf/hpcc/TangWLWH19","DOI":"10.1109/HPCC/SmartCity/DSS.2019.00334","CorpusId":203655977},"title":"Nanily: A QoS-Aware Scheduling for DNN Inference Workload in Clouds"},{"paperId":"928cd808aba140ec298508df87c5579811ff2f41","externalIds":{"DBLP":"journals/pieee/ZhouCLZLZ19","MAG":"2950865323","ArXiv":"1905.10083","DOI":"10.1109/JPROC.2019.2918951","CorpusId":165163986},"title":"Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing"},{"paperId":"2ec9681a021ffed564ac364def44e9fbcedacd48","externalIds":{"MAG":"2919897868","DBLP":"conf/infocom/BaoPW19","DOI":"10.1109/INFOCOM.2019.8737460","CorpusId":86466941},"title":"Deep Learning-based Job Placement in Distributed Machine Learning Clusters"},{"paperId":"6b39bea0ae1720dcbc1ed19ffa697114c4d356c4","externalIds":{"MAG":"3004495293","ArXiv":"1903.11314","DBLP":"journals/corr/abs-1903-11314","DOI":"10.1145/3363554","CorpusId":219875251},"title":"Scalable Deep Learning on Distributed Infrastructures"},{"paperId":"1fb744bacb54c3ab4c1884e0c6da2e9e3cc666e4","externalIds":{"DBLP":"journals/access/AmeriniLC19","MAG":"2921916843","DOI":"10.1109/ACCESS.2019.2903876","CorpusId":88486572},"title":"Social Network Identification Through Image Classification With CNN"},{"paperId":"1f5ce22d0bef55ee414fbcaadc2d66a396761979","externalIds":{"MAG":"2912234349","DBLP":"journals/corr/abs-1902-04610","ArXiv":"1902.04610","CorpusId":61153642},"title":"Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications"},{"paperId":"54ddd67944520f249b906ba4e817188686eae94d","externalIds":{"MAG":"2953494469","ArXiv":"1901.05758","DBLP":"conf/usenix/JeonVPQXY19","CorpusId":58014231},"title":"Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads"},{"paperId":"d79a26226393f687ddbc375e32055b40b8ad8d38","externalIds":{"MAG":"2991040477","DBLP":"conf/nips/HuangCBFCCLNLWC19","CorpusId":53670168},"title":"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"},{"paperId":"2582c54766c387f7ee80a21b91a7f5bae262896a","externalIds":{"DBLP":"journals/corr/abs-1810-10161","MAG":"2896827527","ArXiv":"1810.10161","DOI":"10.1109/MCOM.2019.1800155","CorpusId":53085984},"title":"Deep Learning with Long Short-Term Memory for Time Series Prediction"},{"paperId":"0606676f16d581fa453f6b7b8a14fc7c4af8d025","externalIds":{"DBLP":"conf/osdi/XiaoBRSKHPPZZYZ18","MAG":"2899071864","CorpusId":52987896},"title":"Gandiva: Introspective Cluster Scheduling for Deep Learning"},{"paperId":"da8a949f9c9f1df3a38f12c2cac97b789c705465","externalIds":{"MAG":"2885012953","ArXiv":"1807.11023","DBLP":"journals/comsur/Al-GaradiMADAG20","DOI":"10.1109/COMST.2020.2988293","CorpusId":51877788},"title":"A Survey of Machine and Deep Learning Methods for Internet of Things (IoT) Security"},{"paperId":"93a06eb066fe58ed7d036e46e4cee53483e16bb8","externalIds":{"DBLP":"conf/eurosys/PengBCWG18","MAG":"2798515322","DOI":"10.1145/3190508.3190517","CorpusId":4949076},"title":"Optimus: an efficient dynamic resource scheduler for deep learning clusters"},{"paperId":"b1fa66caf75d5c77449930beba864378f4c959ce","externalIds":{"ArXiv":"1804.06087","MAG":"2962746093","DBLP":"journals/corr/abs-1804-06087","DOI":"10.14778/3282495.3282499","CorpusId":4898729},"title":"Rafiki: Machine Learning as an Analytics Service System"},{"paperId":"36f13179cdfc13017df535fdee582d58067301f3","externalIds":{"DBLP":"journals/soco/LotfollahiSZS20","MAG":"2963516518","ArXiv":"1709.02656","DOI":"10.1007/s00500-019-04030-2","CorpusId":35187639},"title":"Deep packet: a novel approach for encrypted traffic classification using deep learning"},{"paperId":"7547eb271adbcb860bd47bc2da1bc8a7fce01ebc","externalIds":{"MAG":"3006487403","DBLP":"journals/ccr/ChowdhuryS15","DOI":"10.1145/2785956.2787480","CorpusId":1042322},"title":"Efficient Coflow Scheduling Without Prior Knowledge"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","externalIds":{"MAG":"2964121744","DBLP":"journals/corr/KingmaB14","ArXiv":"1412.6980","CorpusId":6628106},"title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","externalIds":{"MAG":"2133564696","ArXiv":"1409.0473","DBLP":"journals/corr/BahdanauCB14","CorpusId":11212020},"title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"a058935fd019c2367fd32c16cd1ce6983a29aafb","externalIds":{"MAG":"2072566913","DBLP":"conf/kdd/LiZCS14","DOI":"10.1145/2623330.2623612","CorpusId":1809834},"title":"Efficient mini-batch training for stochastic optimization"},{"paperId":"aa7bfd2304201afbb19971ebde87b17e40242e91","externalIds":{"MAG":"104184427","DBLP":"conf/icml/SutskeverMDH13","CorpusId":10940950},"title":"On the importance of initialization and momentum in deep learning"},{"paperId":"413c1142de9d91804d6d11c67ff3fed59c9fc279","externalIds":{"MAG":"2405601855","DBLP":"journals/jmlr/DuchiHS11","DOI":"10.5555/1953048.2021068","CorpusId":538820},"title":"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"},{"paperId":"31d3c657987119aaa1e0b8306c0abbe84bf35222","externalIds":{"DBLP":"journals/comsur/TangYWWCXGG24","DOI":"10.1109/COMST.2023.3329027","CorpusId":264938778},"title":"A Survey on Scheduling Techniques in Computing and Network Convergence"},{"paperId":"f79d5cd3aab5b3bfa1ac4abbc4d12c8f0309db2e","externalIds":{"DBLP":"journals/corr/abs-2401-09290","DOI":"10.48550/arXiv.2401.09290","CorpusId":267028455},"title":"G-Safe: Safe GPU Sharing in Multi-Tenant Environments"},{"paperId":"6597b53ea7f7c1909870c9be6ed54695ec36d3b6","externalIds":{"DBLP":"conf/osdi/Hu0ZC00023","CorpusId":259858879},"title":"Hydro: Surrogate-Based Hyperparameter Tuning Service in Datacenters"},{"paperId":"9674364f0f53801b3f2ad6efcc11b32578ca7366","externalIds":{"DBLP":"conf/nsdi/WuZBL023","CorpusId":258559210},"title":"Transparent GPU Sharing in Container Clouds for Deep Learning Workloads"},{"paperId":"aa6ddad0a84eaa004e49142981d05c5f36cc585e","externalIds":{"DBLP":"conf/sc/LiuCZ023","ArXiv":"2308.15762","DOI":"10.1145/3581784.3607073","CorpusId":261339639},"title":"Hanayo: Harnessing Wave-Like Pipeline Parallelism for Enhanced Large Model Training Efficiency"},{"paperId":"8f4e626c2783b0fada28d206c3584ed6674ba53f","externalIds":{"DBLP":"conf/usenix/WengYY0TYZ23","CorpusId":259859102},"title":"Beware of Fragmentation: Scheduling GPU-Sharing Workloads with Fragmentation Gradient Descent"},{"paperId":"de036947d8cfc1b6164d55b2470fd8020f086c4c","externalIds":{"DBLP":"conf/usenix/ChoiLKPKH22","CorpusId":252819800},"title":"Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing"},{"paperId":"100e224fd1c43538caa34d535725dc3b539fa9bb","externalIds":{"DBLP":"journals/tpds/GuCLDCZCH22","DOI":"10.1109/TPDS.2021.3138825","CorpusId":245558327},"title":"Liquid: Intelligent Resource Estimation and Network-Efficient Scheduling for Deep Learning Jobs on Distributed GPU Clusters"},{"paperId":"ca1402619a80c140c650961d899319c2928744f0","externalIds":{"DBLP":"conf/nips/TarnawskiNP21","CorpusId":244711821},"title":"Piper: Multidimensional Planner for DNN Parallelization"},{"paperId":"4043ad5c730d4d2f09d71aa563ff8fce7b834e35","externalIds":{"DBLP":"conf/mlsys/YuLC21","CorpusId":234502137},"title":"Fluid: Resource-aware Hyperparameter Tuning Engine"},{"paperId":"920241246c82176173d442fd43214b2762be6e56","externalIds":{"DBLP":"conf/nsdi/HwangKKSP21","CorpusId":232096362},"title":"Elastic Resource Sharing for Distributed Deep Learning"},{"paperId":"775a0d2671905999474cef4a76b397b266741868","externalIds":{"MAG":"3182301081","DBLP":"conf/usenix/LimAXKJ21","CorpusId":236992562},"title":"Zico: Efficient GPU Memory Sharing for Concurrent DNN Training"},{"paperId":"021f6b1e8c328a4ce0e90dbd0d74e11e90aa4224","externalIds":{"DBLP":"conf/usenix/Romero0YK21","CorpusId":236922757},"title":"INFaaS: Automated Model-less Inference Serving"},{"paperId":"96acd6b1c1528d3f50f71083c88d84d619b5b8b8","externalIds":{"DBLP":"conf/osdi/BaiZZJ20","MAG":"3096484587","CorpusId":227177645},"title":"PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications"},{"paperId":"57bc3e71a889013981b191b6431bec3dc45eba9f","externalIds":{"MAG":"3096956001","DBLP":"conf/osdi/XiaoRLZHLFLJ20","CorpusId":229197535},"title":"AntMan: Dynamic Scaling on GPU Clusters for Deep Learning"},{"paperId":"8d2d560bf1c4c6930d2d1411e48b642bd5b81179","externalIds":{"DBLP":"conf/nsdi/GuCSZJQLG19","MAG":"2919594608","CorpusId":73725044},"title":"Tiresias: A GPU Cluster Manager for Distributed Deep Learning"},{"paperId":"146ac1952faf171a1c198b2023200be1d1fcd14a","externalIds":{"CorpusId":249059077},"title":"This paper is included in the Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation."},{"paperId":"f4f9ff1fb8fc987e0bbcbf1ae60546a09a74788d","externalIds":{"CorpusId":257102392},"title":"AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving"}]}