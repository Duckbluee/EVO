{"references":[{"paperId":"c6e3d1b8ce67a96025fe4526a11ba66ef747e33a","externalIds":{"ArXiv":"2509.16633","DBLP":"journals/corr/abs-2509-16633","DOI":"10.48550/arXiv.2509.16633","CorpusId":281420717},"title":"When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs"},{"paperId":"09a3de12b28d09ee2b2237b44eedc468af0ab6c9","externalIds":{"DBLP":"journals/corr/abs-2508-17334","ArXiv":"2508.17334","DOI":"10.48550/arXiv.2508.17334","CorpusId":280711862},"title":"Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs"},{"paperId":"f52c85e8d54409c4e16dd58a35e13281a212274c","externalIds":{"ArXiv":"2507.20046","DBLP":"conf/acl/GhoshGRB025","DOI":"10.18653/v1/2025.acl-long.1003","CorpusId":280016988},"title":"Infogen: Generating Complex Statistical Infographics from Documents"},{"paperId":"911d2e079708529df12a92e920a6bb6e4186f55f","externalIds":{"ArXiv":"2502.09457","DOI":"10.18653/v1/2025.findings-emnlp.474","CorpusId":276317681},"title":"A Survey of Multilingual Reasoning in Language Models"},{"paperId":"54ea1ebd8a17ee9a88f4ca07ae820359215929f5","externalIds":{"PubMedCentral":"11785991","DOI":"10.1038/s41467-024-55631-x","CorpusId":276077942,"PubMed":"39890777"},"title":"Prompt injection attacks on vision language models in oncology"},{"paperId":"d2d7cb6ba2123816eb5849a01cdaeaa1a200f896","externalIds":{"DBLP":"conf/emnlp/Penamakuri024","ArXiv":"2410.19144","ACL":"2024.emnlp-main.1151","DOI":"10.18653/v1/2024.emnlp-main.1151","CorpusId":273638132},"title":"Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant"},{"paperId":"be75e2fceed00e1f98bde055ec07131fb2c1d6a2","externalIds":{"DBLP":"conf/acl/JhaJMC0B24","ArXiv":"2406.05344","DOI":"10.48550/arXiv.2406.05344","CorpusId":270371211},"title":"MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention"},{"paperId":"990062c5ce04f6539d3ad4f3f24d12cd63b21706","externalIds":{"DBLP":"conf/wacv/Rasheed0S0CABFK25","ArXiv":"2402.14818","DOI":"10.1109/WACV61041.2025.00177","CorpusId":267782854},"title":"Palo: A Polyglot Large Multimodal Model for 5B People"},{"paperId":"cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3","externalIds":{"DBLP":"journals/corr/abs-2401-15947","ArXiv":"2401.15947","DOI":"10.48550/arXiv.2401.15947","CorpusId":267311517},"title":"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","externalIds":{"DBLP":"journals/corr/abs-2401-13601","ArXiv":"2401.13601","DOI":"10.48550/arXiv.2401.13601","CorpusId":267199815},"title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"b1e7a92aa19801a959e0fb4472fa4e7fb45cb854","externalIds":{"DBLP":"journals/corr/abs-2401-09899","ArXiv":"2401.09899","ACL":"2024.eacl-long.56","DOI":"10.48550/arXiv.2401.09899","CorpusId":267034638},"title":"Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations"},{"paperId":"ece33ee67d74c29cd2a83c505e5bf0b818f9c2a1","externalIds":{"DBLP":"journals/corr/abs-2401-02330","ArXiv":"2401.02330","DOI":"10.1145/3688863.3689575","CorpusId":266755915},"title":"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model"},{"paperId":"0725debf3183589626823dbb64107bba8ed22448","externalIds":{"ArXiv":"2401.01596","DBLP":"journals/corr/abs-2401-01596","DOI":"10.48550/arXiv.2401.01596","CorpusId":266741554},"title":"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries"},{"paperId":"98ab627dd147db88b5e5cfa9a74f1bd8da110021","externalIds":{"DBLP":"journals/corr/abs-2312-16862","ArXiv":"2312.16862","DOI":"10.48550/arXiv.2312.16862","CorpusId":266572996},"title":"TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones"},{"paperId":"0c4f46e4dcae5527018e6432fb60cfe8c3354e97","externalIds":{"DBLP":"journals/corr/abs-2312-14125","ArXiv":"2312.14125","DOI":"10.48550/arXiv.2312.14125","CorpusId":266435847},"title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation"},{"paperId":"4b1b5e219fb41a7413599c3b2ca6a7fdf045d1a5","externalIds":{"DBLP":"conf/cvpr/SunCZZYWRL0W24","ArXiv":"2312.13286","DOI":"10.1109/CVPR52733.2024.01365","CorpusId":266374640},"title":"Generative Multimodal Models are In-Context Learners"},{"paperId":"36cbea6919cf01f3e5488b160726b01f422b4b93","externalIds":{"DBLP":"conf/aaai/GhoshAJ0CS24","ArXiv":"2312.11541","DOI":"10.48550/arXiv.2312.11541","CorpusId":266362506},"title":"CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare"},{"paperId":"2141ed804636a1cf339d606cd03fd3b3e9582133","externalIds":{"DBLP":"conf/cvpr/LinYP0SH24","ArXiv":"2312.07533","DOI":"10.1109/CVPR52733.2024.02520","CorpusId":266174746},"title":"VILA: On Pre-training for Visual Language Models"},{"paperId":"d23f08611ea1e64f691ecddee1f7f48c8015eea6","externalIds":{"ArXiv":"2312.05278","CorpusId":266162357},"title":"Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects"},{"paperId":"d198a5a1a0c6e31bd0ad70658c8c2a74b8753aed","externalIds":{"ArXiv":"2312.03818","DBLP":"conf/cvpr/0002FWZZKXL024","DOI":"10.1109/CVPR52733.2024.01237","CorpusId":266055413},"title":"Alpha-CLIP: A CLIP Model Focusing on Wherever you Want"},{"paperId":"78582ad19779a69d97b797a3c6eb2397f99398b6","externalIds":{"DBLP":"conf/cvpr/TangYKLZB24","ArXiv":"2311.18775","DOI":"10.1109/CVPR52733.2024.02589","CorpusId":265506621},"title":"CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation"},{"paperId":"e263e08a20080a2543d0ca29d3d63c4717a8beb6","externalIds":{"DBLP":"journals/corr/abs-2311-18799","ArXiv":"2311.18799","DOI":"10.48550/arXiv.2311.18799","CorpusId":265506093},"title":"X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning"},{"paperId":"486c2df78cbb770a90a55f7fa3fe19102fba2c24","externalIds":{"ArXiv":"2311.17043","DBLP":"journals/corr/abs-2311-17043","DOI":"10.48550/arXiv.2311.17043","CorpusId":265466723},"title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"},{"paperId":"be974844cd1e5a441fcfebcff62f72e48af46f63","externalIds":{"ArXiv":"2311.15766","DBLP":"journals/corr/abs-2311-15766","DOI":"10.48550/arXiv.2311.15766","CorpusId":265456592},"title":"Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","externalIds":{"DBLP":"journals/corr/abs-2311-13165","ArXiv":"2311.13165","DOI":"10.1109/BigData59044.2023.10386743","CorpusId":265351653},"title":"Multimodal Large Language Models: A Survey"},{"paperId":"391eaeb1092c2b145ff0e5a2fa61637a42921fce","externalIds":{"DBLP":"conf/cvpr/ChenSCJD24","ArXiv":"2311.10081","DOI":"10.1109/CVPR52733.2024.01350","CorpusId":265221232},"title":"DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"},{"paperId":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","externalIds":{"ArXiv":"2311.07575","DBLP":"journals/corr/abs-2311-07575","DOI":"10.48550/arXiv.2311.07575","CorpusId":265150267},"title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","externalIds":{"DBLP":"conf/eccv/LiuCLZLRZYSZZGL24","ArXiv":"2311.05437","DOI":"10.48550/arXiv.2311.05437","CorpusId":265067489},"title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"a4e7199e725b34ae5ddd574057f60ebb1a2011b7","externalIds":{"ArXiv":"2311.05698","DBLP":"conf/cvpr/PiergiovanniNKR24","DOI":"10.1109/CVPR52733.2024.02531","CorpusId":265129010},"title":"Mirasol3B: A Multimodal Autoregressive Model for Time-Aligned and Contextual Modalities"},{"paperId":"2313afae52d98e569da2dedbf14daf9efc74e7cf","externalIds":{"DBLP":"journals/corr/abs-2311-03079","ArXiv":"2311.03079","DOI":"10.48550/arXiv.2311.03079","CorpusId":265034288},"title":"CogVLM: Visual Expert for Pretrained Language Models"},{"paperId":"055e3aa78a542c6cec016169f3011c2bf4e46932","externalIds":{"DBLP":"conf/iclr/LiCHCCSG24","ArXiv":"2311.03354","DOI":"10.48550/arXiv.2311.03354","CorpusId":265043790},"title":"CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding"},{"paperId":"1ddbd08ad8cf22a5c66c4242194c4286328533bf","externalIds":{"ArXiv":"2310.09478","DBLP":"journals/corr/abs-2310-09478","DOI":"10.48550/arXiv.2310.09478","CorpusId":264146906},"title":"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"},{"paperId":"458111ac5a0f73bb35a2acf55298268be25ccfa2","externalIds":{"ArXiv":"2310.07704","DBLP":"journals/corr/abs-2310-07704","DOI":"10.48550/arXiv.2310.07704","CorpusId":263834718},"title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","externalIds":{"DBLP":"journals/corr/abs-2310-02239","ArXiv":"2310.02239","DOI":"10.48550/arXiv.2310.02239","CorpusId":263608981},"title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"c74c8988fde14e56dbd94829761e4ee067bacd4e","externalIds":{"DBLP":"conf/iccv/QianWDQL023","DOI":"10.1109/ICCV51070.2023.00276","CorpusId":267026685},"title":"Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering"},{"paperId":"1cc39f7691af0bc1061dda7897a3f62f098573d6","externalIds":{"DBLP":"journals/corr/abs-2309-16671","ArXiv":"2309.16671","DOI":"10.48550/arXiv.2309.16671","CorpusId":263131626},"title":"Demystifying CLIP Data"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","externalIds":{"DBLP":"journals/corr/abs-2309-05519","ArXiv":"2309.05519","DOI":"10.48550/arXiv.2309.05519","CorpusId":261696650},"title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"5bdaadb84db0cbf72aaebda9f55f4288b63c6e9b","externalIds":{"DBLP":"journals/corr/abs-2309-00236","ArXiv":"2309.00236","DOI":"10.48550/arXiv.2309.00236","CorpusId":261494235},"title":"Image Hijacks: Adversarial Images can Control Generative Models at Runtime"},{"paperId":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","externalIds":{"ArXiv":"2308.12966","CorpusId":261101015},"title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"},{"paperId":"30cc95639cffca4ffa8c0eafbc502636c0c88fa5","externalIds":{"DBLP":"journals/corr/abs-2308-09936","ArXiv":"2308.09936","DOI":"10.48550/arXiv.2308.09936","CorpusId":261049015},"title":"BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","externalIds":{"DBLP":"journals/corr/abs-2308-06595","ArXiv":"2308.06595","DOI":"10.48550/arXiv.2308.06595","CorpusId":260887670},"title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","externalIds":{"ArXiv":"2308.02490","DBLP":"journals/corr/abs-2308-02490","DOI":"10.48550/arXiv.2308.02490","CorpusId":260611572},"title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","externalIds":{"ArXiv":"2307.15189","DBLP":"conf/ml4h/MoorHWYDLZRR23","DOI":"10.48550/arXiv.2307.15189","CorpusId":260316059},"title":"Med-Flamingo: a Multimodal Medical Few-shot Learner"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","externalIds":{"DBLP":"conf/ijcai/ZhaoYGYWZSPDH024","ArXiv":"2307.09474","DOI":"10.48550/arXiv.2307.09474","CorpusId":259951197},"title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","externalIds":{"DBLP":"journals/corr/abs-2307-08581","ArXiv":"2307.08581","DOI":"10.48550/arXiv.2307.08581","CorpusId":259937702},"title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"e2a58fd18961c3941102989e3a3d0d27c615e015","externalIds":{"ArXiv":"2306.15195","DBLP":"journals/corr/abs-2306-15195","DOI":"10.48550/arXiv.2306.15195","CorpusId":259262082},"title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"3b6179c293df29e31d31cea46476f104ab6950f2","externalIds":{"DBLP":"conf/iclr/Peng00HHMYW24","ArXiv":"2306.14824","DOI":"10.48550/arXiv.2306.14824","CorpusId":259262263},"title":"Kosmos-2: Grounding Multimodal Large Language Models to the World"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","externalIds":{"ArXiv":"2306.13394","DBLP":"journals/corr/abs-2306-13394","DOI":"10.48550/arXiv.2306.13394","CorpusId":259243928},"title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"948e8cfae92c2004f2dd5c9316f5972f8baaea21","externalIds":{"DBLP":"journals/corr/abs-2306-16527","ArXiv":"2306.16527","DOI":"10.48550/arXiv.2306.16527","CorpusId":259287020},"title":"OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"},{"paperId":"597ec7f0789ff61c0856ff9e1ff9a85de9654ce7","externalIds":{"DBLP":"conf/ijcnn/JainMJS23","DOI":"10.1109/IJCNN54540.2023.10191363","CorpusId":260387825},"title":"Generative Models vs Discriminative Models: Which Performs Better in Detecting Cyberbullying in Memes?"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","externalIds":{"ArXiv":"2306.09093","DBLP":"journals/corr/abs-2306-09093","DOI":"10.48550/arXiv.2306.09093","CorpusId":259165461},"title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","externalIds":{"ArXiv":"2306.05424","DBLP":"journals/corr/abs-2306-05424","DOI":"10.48550/arXiv.2306.05424","CorpusId":259108333},"title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8","externalIds":{"DBLP":"journals/corr/abs-2306-04387","ArXiv":"2306.04387","DOI":"10.48550/arXiv.2306.04387","CorpusId":259095896},"title":"M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","externalIds":{"DBLP":"journals/corr/abs-2306-02858","ArXiv":"2306.02858","DOI":"10.48550/arXiv.2306.02858","CorpusId":259075356},"title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"ccd9c3c90c3777bf9f5125e9463c0b4c558c696b","externalIds":{"DBLP":"conf/cvpr/ZhangZX23","DOI":"10.1109/CVPR52729.2023.01831","CorpusId":260053000},"title":"VQACL: A Novel Visual Question Answering Continual Learning Setting"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","externalIds":{"ArXiv":"2305.18752","DBLP":"conf/nips/YangSLZGLS23","DOI":"10.48550/arXiv.2305.18752","CorpusId":258967184},"title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"d49f234e7e935617bd6499ce9f82337f60b035a3","externalIds":{"ArXiv":"2305.15727","DBLP":"conf/cvpr/FanPW0XW22","DOI":"10.1109/CVPRW63382.2024.00773","CorpusId":258887814},"title":"POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","externalIds":{"DBLP":"journals/corr/abs-2305-16355","ArXiv":"2305.16355","ACL":"2023.tllm-1.2","DOI":"10.48550/arXiv.2305.16355","CorpusId":258947721},"title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","externalIds":{"ArXiv":"2305.16103","DBLP":"journals/corr/abs-2305-16103","DOI":"10.48550/arXiv.2305.16103","CorpusId":258887944},"title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","externalIds":{"ArXiv":"2305.15328","DBLP":"journals/corr/abs-2305-15328","DOI":"10.48550/arXiv.2305.15328","CorpusId":258865230},"title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","externalIds":{"DBLP":"journals/corr/abs-2305-15023","ArXiv":"2305.15023","DOI":"10.48550/arXiv.2305.15023","CorpusId":258865326},"title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"9f411fda2ad5b141a3115f707bcf5ee865b3fb94","externalIds":{"DBLP":"conf/nips/TangYZ0B23","ArXiv":"2305.11846","DOI":"10.48550/arXiv.2305.11846","CorpusId":258822817},"title":"Any-to-Any Generation via Composable Diffusion"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","externalIds":{"ArXiv":"2305.06500","DBLP":"journals/corr/abs-2305-06500","DOI":"10.48550/arXiv.2305.06500","CorpusId":258615266},"title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","externalIds":{"DBLP":"journals/corr/abs-2305-05665","ArXiv":"2305.05665","DOI":"10.1109/CVPR52729.2023.01457","CorpusId":258564264},"title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","externalIds":{"DBLP":"journals/corr/abs-2304-14178","ArXiv":"2304.14178","DOI":"10.48550/arXiv.2304.14178","CorpusId":258352455},"title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"03755613d50e1958a97bfaad2efb976f786fbb70","externalIds":{"DBLP":"journals/pami/LiuCHGZWT25","ArXiv":"2304.08345","DOI":"10.1109/TPAMI.2024.3479776","CorpusId":258179576,"PubMed":"39418158"},"title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","externalIds":{"ArXiv":"2303.11381","DBLP":"journals/corr/abs-2303-11381","DOI":"10.48550/arXiv.2303.11381","CorpusId":257637012},"title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","externalIds":{"ArXiv":"2303.03378","DBLP":"journals/corr/abs-2303-03378","DOI":"10.48550/arXiv.2303.03378","CorpusId":257364842},"title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","externalIds":{"ArXiv":"2302.14045","DBLP":"conf/nips/Huang0WHSML0MPL23","DOI":"10.48550/arXiv.2302.14045","CorpusId":257219775},"title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"746bb45433f6b24d3ae64d6cd51c4e9d00a0ffa7","externalIds":{"DBLP":"journals/corr/abs-2302-10035","ArXiv":"2302.10035","DOI":"10.1007/s11633-022-1410-8","CorpusId":257038341},"title":"Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"},{"paperId":"64caaab51d8339f1b99874d3bddb79debbe661ca","externalIds":{"DBLP":"journals/corr/abs-2302-00402","ArXiv":"2302.00402","DOI":"10.48550/arXiv.2302.00402","CorpusId":256459873},"title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","externalIds":{"DBLP":"journals/corr/abs-2301-12597","ArXiv":"2301.12597","DOI":"10.48550/arXiv.2301.12597","CorpusId":256390509},"title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"74e7edf7c6436bb4992ca9c9df9476c9ccf31919","externalIds":{"ArXiv":"2301.05065","DBLP":"journals/corr/abs-2301-05065","DOI":"10.48550/arXiv.2301.05065","CorpusId":255749525},"title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks"},{"paperId":"0c0300f53c01ae609c97395c98de4c9d85d92876","externalIds":{"DBLP":"conf/acl/XuSH23","ACL":"2023.acl-long.641","ArXiv":"2212.10773","DOI":"10.48550/arXiv.2212.10773","CorpusId":254926784},"title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"},{"paperId":"70feb009bc1e8b1cb8dff64bf9fd67789636438b","externalIds":{"DBLP":"conf/cvpr/Guo0LT0TH23","ArXiv":"2212.10846","DOI":"10.1109/CVPR52729.2023.01046","CorpusId":261081574},"title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models"},{"paperId":"f5c853861fcde704a7100e24e963c5262e625229","externalIds":{"ArXiv":"2212.04979","CorpusId":254535696},"title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","externalIds":{"DBLP":"journals/corr/abs-2210-08773","ArXiv":"2210.08773","DOI":"10.48550/arXiv.2210.08773","CorpusId":252917791},"title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","externalIds":{"ArXiv":"2209.06794","DBLP":"journals/corr/abs-2209-06794","DOI":"10.48550/arXiv.2209.06794","CorpusId":252222320},"title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","externalIds":{"ArXiv":"2208.10442","DBLP":"journals/corr/abs-2208-10442","DOI":"10.48550/arXiv.2208.10442","CorpusId":251719655},"title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"4990f7542f0600e0501a7e7a931b32eb7cb804d5","externalIds":{"DBLP":"journals/corr/abs-2203-12602","ArXiv":"2203.12602","DOI":"10.48550/arXiv.2203.12602","CorpusId":247619234},"title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"},{"paperId":"b611c501269224702d1a9942c8600a31ec66ab28","externalIds":{"ArXiv":"2203.10244","DBLP":"conf/acl/MasryLTJH22","ACL":"2022.findings-acl.177","DOI":"10.48550/arXiv.2203.10244","CorpusId":247593713},"title":"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"},{"paperId":"212732c649d84382f4e74ca047b13f3c835591d7","externalIds":{"ArXiv":"2202.04053","DBLP":"conf/iccv/0001ZB23","DOI":"10.1109/ICCV51070.2023.00283","CorpusId":253510037},"title":"DALL-EVAL: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","externalIds":{"ArXiv":"2201.07520","DBLP":"journals/corr/abs-2201-07520","CorpusId":246035820},"title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","externalIds":{"DBLP":"journals/corr/abs-2112-03857","ArXiv":"2112.03857","DOI":"10.1109/CVPR52688.2022.01069","CorpusId":244920947},"title":"Grounded Language-Image Pre-training"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","externalIds":{"DBLP":"journals/corr/abs-2111-02358","ArXiv":"2111.02358","CorpusId":241035439},"title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"32d59ab951be74be351f9777da2cbc71bb68c3c1","externalIds":{"DBLP":"conf/acl/Jin0SC022","ArXiv":"2110.08484","ACL":"2022.acl-long.197","DOI":"10.18653/v1/2022.acl-long.197","CorpusId":239016231},"title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models"},{"paperId":"821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0","externalIds":{"ArXiv":"2109.14084","DBLP":"journals/corr/abs-2109-14084","ACL":"2021.emnlp-main.544","DOI":"10.18653/v1/2021.emnlp-main.544","CorpusId":238215257},"title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","externalIds":{"DBLP":"conf/aaai/YangGW0L0W22","ArXiv":"2109.05014","DOI":"10.1609/aaai.v36i3.20215","CorpusId":237485500},"title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","externalIds":{"DBLP":"journals/corr/abs-2108-10904","ArXiv":"2108.10904","CorpusId":237291550},"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","externalIds":{"ArXiv":"2106.13884","DBLP":"journals/corr/abs-2106-13884","CorpusId":235658331},"title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"9c7aad6a82d2aac1395164ed3147fa4407a85acd","externalIds":{"ArXiv":"2105.14424","DBLP":"journals/corr/abs-2105-14424","DOI":"10.1109/ICPR56361.2022.9956687","CorpusId":235254799},"title":"Gaze Estimation using Transformer"},{"paperId":"bac87bdb1cabc35fafb8176a234d332ebcc02864","externalIds":{"DBLP":"conf/iccv/BainNVZ21","ArXiv":"2104.00650","DOI":"10.1109/ICCV48922.2021.00175","CorpusId":232478955},"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","externalIds":{"ArXiv":"2102.12092","DBLP":"conf/icml/RameshPGGVRCS21","MAG":"3170016573","CorpusId":232035663},"title":"Zero-Shot Text-to-Image Generation"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c","externalIds":{"DBLP":"conf/cvpr/Gurari0SGLGLB18","MAG":"2788643321","ArXiv":"1802.08218","DOI":"10.1109/CVPR.2018.00380","CorpusId":3831582},"title":"VizWiz Grand Challenge: Answering Visual Questions from Blind People"},{"paperId":"e10a5e0baf2aa87d804795af071808a9377cc80a","externalIds":{"MAG":"2784025607","DBLP":"conf/aaai/ZhouXC18","ArXiv":"1703.09788","DOI":"10.1609/aaai.v32i1.12342","CorpusId":19713015},"title":"Towards Automatic Learning of Procedures From Web Instructional Videos"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","externalIds":{"ArXiv":"1505.04870","DBLP":"conf/iccv/PlummerWCCHL15","MAG":"2568262903","DOI":"10.1007/s11263-016-0965-7","CorpusId":6941275},"title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"e17db9e4975bd4d3c078f7294272e1af3c5fc932","externalIds":{"DBLP":"conf/acl/GhoshTT0SS24","DOI":"10.18653/v1/2024.acl-long.708","CorpusId":271923579},"title":"From Sights to Insights: Towards Summarization of Multimodal Clinical Documents"},{"paperId":"8f624066aca95f3ac172b9f7aeaadb8e7e947fb8","externalIds":{"DBLP":"conf/emnlp/GhoshA00RS24","DOI":"10.18653/v1/2024.findings-emnlp.675","CorpusId":274060599},"title":"HealthAlignSumm : Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues"},{"paperId":"7562e25b666cba841b1dd5cf6e700978922beb04","externalIds":{"DBLP":"journals/corr/abs-2304-10691","DOI":"10.48550/arXiv.2304.10691","CorpusId":258291755},"title":"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","externalIds":{"DBLP":"journals/corr/abs-2308-12966","DOI":"10.48550/arXiv.2308.12966","CorpusId":263875678},"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"1c199e3d50349153c0b6200006b02fbe66c27acd","externalIds":{"CorpusId":268877781},"title":"X 2 -VLM: All-In-One Pre-trained Model For Vision-Language Tasks"},{"paperId":"608b4fed95d2a134c1c283ec36b917ee4fe14127","externalIds":{"DBLP":"journals/corr/abs-2212-04979","DOI":"10.48550/arXiv.2212.04979","CorpusId":263889656},"title":"Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners"},{"paperId":"6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce","externalIds":{"MAG":"2525491769","DOI":"10.1201/b18055-8","CorpusId":208945385},"title":"google,我,萨娜"}]}