{"references":[{"paperId":"817632c42e735911e14b89e851ceaf54ba2ad25f","externalIds":{"ArXiv":"2409.02060","DBLP":"journals/corr/abs-2409-02060","DOI":"10.48550/arXiv.2409.02060","CorpusId":272366674},"title":"OLMoE: Open Mixture-of-Experts Language Models"},{"paperId":"ebe6b817ba8a44d0481598575904231c8324932e","externalIds":{"ArXiv":"2407.04153","DBLP":"journals/corr/abs-2407-04153","DOI":"10.48550/arXiv.2407.04153","CorpusId":271038610},"title":"Mixture of A Million Experts"},{"paperId":"05830547cfd19b734777b8546f4d606fd79ebd2b","externalIds":{"DBLP":"journals/corr/abs-2406-16554","ArXiv":"2406.16554","ACL":"2024.emnlp-main.890","DOI":"10.48550/arXiv.2406.16554","CorpusId":270703371},"title":"LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training"},{"paperId":"8970a42d5de27dcf0833f3f1387de586b6d85a3e","externalIds":{"DBLP":"journals/corr/abs-2406-13233","ArXiv":"2406.13233","DOI":"10.48550/arXiv.2406.13233","CorpusId":270619842},"title":"AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models"},{"paperId":"a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8","externalIds":{"DBLP":"journals/corr/abs-2406-10260","ArXiv":"2406.10260","DOI":"10.48550/arXiv.2406.10260","CorpusId":270560556},"title":"Flextron: Many-in-One Flexible Large Language Model"},{"paperId":"2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731","externalIds":{"ArXiv":"2406.06563","DBLP":"journals/corr/abs-2406-06563","DOI":"10.48550/arXiv.2406.06563","CorpusId":270379962},"title":"Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models"},{"paperId":"c8b18682965ff9dccc0130dab3d679f78cefa617","externalIds":{"DBLP":"journals/corr/abs-2406-00515","ArXiv":"2406.00515","DOI":"10.1145/3747588","CorpusId":270214176},"title":"A Survey on Large Language Models for Code Generation"},{"paperId":"773597e8acb8c9ee2a4603e0bd40bb74b7fba871","externalIds":{"DBLP":"journals/tpds/ZhangXWYHZC24","DOI":"10.1109/TPDS.2024.3385639","CorpusId":269024443},"title":"MPMoE: Memory Efficient MoE for Pre-Trained Models With Adaptive Pipeline Parallelism"},{"paperId":"697b1b17c9c27b43b8ee9a504317f1a32bb2a28c","externalIds":{"ArXiv":"2405.17976","DBLP":"journals/corr/abs-2405-17976","DOI":"10.48550/arXiv.2405.17976","CorpusId":270067526},"title":"Yuan 2.0-M32: Mixture of Experts with Attention Router"},{"paperId":"7f6f34090d17d0523b91c0d3cd34fe9f66ebb189","externalIds":{"DBLP":"journals/corr/abs-2405-14297","ArXiv":"2405.14297","DOI":"10.48550/arXiv.2405.14297","CorpusId":269982525},"title":"Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models"},{"paperId":"de1611be67993d8a3db174cc1e815130b3eae3dc","externalIds":{"DBLP":"journals/corr/abs-2405-14488","ArXiv":"2405.14488","DOI":"10.48550/arXiv.2405.14488","CorpusId":269983704},"title":"MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability"},{"paperId":"d9c05ef795a82e35280ddc04f9a0c203114f546e","externalIds":{"DBLP":"conf/nips/Shi0ZWWL0YM24","ArXiv":"2405.14507","DOI":"10.48550/arXiv.2405.14507","CorpusId":269983593},"title":"Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast"},{"paperId":"6a419c51c6ed97599009b6b11ffd8a98d5e0fc50","externalIds":{"ArXiv":"2405.13053","DBLP":"journals/corr/abs-2405-13053","DOI":"10.48550/arXiv.2405.13053","CorpusId":269982348},"title":"MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models"},{"paperId":"57ed7527c45f6b685eeefc65abe083dd350cd579","externalIds":{"DBLP":"journals/corr/abs-2405-11273","ArXiv":"2405.11273","DOI":"10.1109/TPAMI.2025.3532688","CorpusId":269921303,"PubMed":"40031848"},"title":"Uni-MoE: Scaling Unified Multimodal LLMs With Mixture of Experts"},{"paperId":"53a803388e83ae89261624099d7be4287ace67cb","externalIds":{"ArXiv":"2405.04434","DBLP":"journals/corr/abs-2405-04434","DOI":"10.48550/arXiv.2405.04434","CorpusId":269613809},"title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"},{"paperId":"4b879f069d023e03bf537309a99bdaeb39916ea5","externalIds":{"ArXiv":"2405.03133","DBLP":"journals/corr/abs-2405-03133","DOI":"10.48550/arXiv.2405.03133","CorpusId":268891288},"title":"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training"},{"paperId":"146075b3b59aba98edf97a7e4c8303e0e6e560c6","externalIds":{"ArXiv":"2404.19429","DBLP":"conf/mlsys/JiangT000024","DOI":"10.48550/arXiv.2404.19429","CorpusId":269456940},"title":"Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping"},{"paperId":"c497b1de58f9a53338ea3b7507cdc4d0056f48a1","externalIds":{"ArXiv":"2404.18465","DBLP":"journals/corr/abs-2404-18465","DOI":"10.1145/3626772.3657686","CorpusId":269449211},"title":"M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework"},{"paperId":"ebcf108f8bc42140721ff02b6727b0a291362957","externalIds":{"ArXiv":"2404.15159","DBLP":"journals/corr/abs-2404-15159","DOI":"10.48550/arXiv.2404.15159","CorpusId":269302398},"title":"MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts"},{"paperId":"83858f08aef77ced91428207b8891a82e7087cd2","externalIds":{"DBLP":"conf/eurosys/ShiPWLRHYLC24","DOI":"10.1145/3627703.3650083","CorpusId":269243508},"title":"ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling"},{"paperId":"191a82f6b059c0d847d2e390dbb4c8b424388025","externalIds":{"DBLP":"journals/corr/abs-2404-13628","ArXiv":"2404.13628","DOI":"10.48550/arXiv.2404.13628","CorpusId":269293160},"title":"Mixture of LoRA Experts"},{"paperId":"c9ff9fbbe21985b35d6a070b67f22a0e065c4328","externalIds":{"DBLP":"journals/corr/abs-2404-07413","ArXiv":"2404.07413","DOI":"10.48550/arXiv.2404.07413","CorpusId":269042695},"title":"JetMoE: Reaching Llama2 Performance with 0.1M Dollars"},{"paperId":"c67c4c81beed122d7f94580d8816a6dc68867ec4","externalIds":{"ArXiv":"2404.05567","DBLP":"journals/corr/abs-2404-05567","DOI":"10.48550/arXiv.2404.05567","CorpusId":269005191},"title":"Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models"},{"paperId":"3940c3071e343e00d0a1d8c129854eee9430e3fb","externalIds":{"DBLP":"journals/corr/abs-2404-05019","ArXiv":"2404.05019","DOI":"10.48550/arXiv.2404.05019","CorpusId":269004450},"title":"Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts"},{"paperId":"8b57632b19df9ebd3e57e3adbef7fc6ec93bc506","externalIds":{"ArXiv":"2404.01954","DBLP":"journals/corr/abs-2404-01954","DOI":"10.48550/arXiv.2404.01954","CorpusId":268856712},"title":"HyperCLOVA X Technical Report"},{"paperId":"a1f23f04421cdc62e80fe9f04c1ce60f4a6af9f0","externalIds":{"DBLP":"journals/corr/abs-2404-02258","ArXiv":"2404.02258","DOI":"10.48550/arXiv.2404.02258","CorpusId":268876220},"title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"},{"paperId":"cbaf689fd9ea9bc939510019d90535d6249b3367","externalIds":{"DBLP":"journals/corr/abs-2403-19887","ArXiv":"2403.19887","DOI":"10.48550/arXiv.2403.19887","CorpusId":268793596},"title":"Jamba: A Hybrid Transformer-Mamba Language Model"},{"paperId":"916b4926cda574dc3f9486bb9994b6f2788dd800","externalIds":{"DBLP":"journals/corr/abs-2403-14608","ArXiv":"2403.14608","DOI":"10.48550/arXiv.2403.14608","CorpusId":268553763},"title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey"},{"paperId":"6675bcf6dc97c87da7afda223938ec7e51ecc3b2","externalIds":{"ArXiv":"2403.09611","DBLP":"journals/corr/abs-2403-09611","CorpusId":268384865},"title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"},{"paperId":"d8599a04f738a42f5d7a2c9bc598c2c7b74669c5","externalIds":{"ArXiv":"2403.08245","DBLP":"journals/corr/abs-2403-08245","DOI":"10.48550/arXiv.2403.08245","CorpusId":268379441},"title":"Scattered Mixture-of-Experts Implementation"},{"paperId":"07894aeadab9158fdb97647c4792816ede1b60b9","externalIds":{"ArXiv":"2403.07816","DBLP":"journals/corr/abs-2403-07816","DOI":"10.48550/arXiv.2403.07816","CorpusId":268363969},"title":"Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"},{"paperId":"af6aa336c25ead669da0df560376a32314e08006","externalIds":{"ArXiv":"2402.12851","DBLP":"journals/corr/abs-2402-12851","DOI":"10.48550/arXiv.2402.12851","CorpusId":267759827},"title":"MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models"},{"paperId":"022083382f23b3abcd4d93fe07741d7ffdc42f11","externalIds":{"DBLP":"conf/acl/ZhaoQWWHF24","ArXiv":"2402.12656","DOI":"10.18653/v1/2024.acl-long.571","CorpusId":268554647},"title":"HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts"},{"paperId":"94e531c3b139cdc50ab4b7be21a29dba8f87e3c0","externalIds":{"ArXiv":"2402.08562","DBLP":"journals/corr/abs-2402-08562","DOI":"10.48550/arXiv.2402.08562","CorpusId":267636601},"title":"Higher Layers Need More LoRA Experts"},{"paperId":"cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3","externalIds":{"DBLP":"journals/corr/abs-2401-15947","ArXiv":"2401.15947","DOI":"10.48550/arXiv.2401.15947","CorpusId":267311517},"title":"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"},{"paperId":"37ac7683543f0e039197a56e71e752a9ebe5998e","externalIds":{"DBLP":"journals/corr/abs-2402-01739","ArXiv":"2402.01739","DOI":"10.48550/arXiv.2402.01739","CorpusId":267413104},"title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models"},{"paperId":"af9676f9beaeca214bfbf7f2897828820d48abdc","externalIds":{"DBLP":"journals/corr/abs-2401-16160","ArXiv":"2401.16160","DOI":"10.48550/arXiv.2401.16160","CorpusId":267312176},"title":"LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs"},{"paperId":"dab65cdf3dcbe780ca5201d4576e8429e5fa1e07","externalIds":{"ArXiv":"2401.08383","DBLP":"conf/ipps/YaoASSP24","DOI":"10.1109/IPDPS57955.2024.00086","CorpusId":267028699},"title":"Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference"},{"paperId":"16d6e1ed1cf72212f6154644f3aa59d18bc95fda","externalIds":{"ArXiv":"2401.06066","DBLP":"conf/acl/DaiDZXGCLZYWXLH24","DOI":"10.48550/arXiv.2401.06066","CorpusId":266933338},"title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"},{"paperId":"411114f989a3d1083d90afd265103132fee94ebe","externalIds":{"DBLP":"journals/corr/abs-2401-04088","ArXiv":"2401.04088","DOI":"10.48550/arXiv.2401.04088","CorpusId":266844877},"title":"Mixtral of Experts"},{"paperId":"7260442ef9c0448f07ce3803efd49cebaffcebe9","externalIds":{"ArXiv":"2401.02954","DBLP":"journals/corr/abs-2401-02954","CorpusId":266818336},"title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"},{"paperId":"2d4a853affeb0b164fc1134df612aea658f36459","externalIds":{"DBLP":"journals/corr/abs-2312-12379","ArXiv":"2312.12379","DOI":"10.48550/arXiv.2312.12379","CorpusId":266362594},"title":"Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning"},{"paperId":"3a2bb5b4f1f724df9295de5e0e1235cc30769b82","externalIds":{"DBLP":"conf/cvpr/WuHW0S24","ArXiv":"2312.00968","DOI":"10.1109/CVPR52733.2024.01347","CorpusId":265609776},"title":"Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-Rank Experts"},{"paperId":"7bbc7595196a0606a07506c4fb1473e5e87f6082","externalIds":{"ArXiv":"2312.00752","DBLP":"journals/corr/abs-2312-00752","CorpusId":265551773},"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"},{"paperId":"7e88160ab2b3c9224ce0b9a66bec1740728984bf","externalIds":{"ArXiv":"2401.10241","DBLP":"journals/corr/abs-2401-10241","DOI":"10.48550/arXiv.2401.10241","CorpusId":267060979},"title":"Zero Bubble Pipeline Parallelism"},{"paperId":"a88a6112ddd6f2cb171ab61310ffa773b149d773","externalIds":{"DBLP":"conf/wacv/DatMNBB25","ArXiv":"2311.14747","DOI":"10.1109/WACV61041.2025.00115","CorpusId":265457202},"title":"HOPE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts"},{"paperId":"525cd5d5c7d823d0b2ad4eaf086622940d45ed6d","externalIds":{"ArXiv":"2311.09179","DBLP":"journals/corr/abs-2311-09179","DOI":"10.48550/arXiv.2311.09179","CorpusId":265213347},"title":"SiRA: Sparse Mixture of Low Rank Adaptation"},{"paperId":"71c9d2b995c19d43c519eb5ca9504ac790490398","externalIds":{"DBLP":"journals/corr/abs-2310-19341","ArXiv":"2310.19341","DOI":"10.48550/arXiv.2310.19341","CorpusId":264802115},"title":"Skywork: A More Open Bilingual Foundation Model"},{"paperId":"005e19244b9be260a185607c82af9f08449de7e3","externalIds":{"ArXiv":"2310.10908","ACL":"2024.naacl-long.144","DBLP":"conf/naacl/QiuHF24","DOI":"10.18653/v1/2024.naacl-long.144","CorpusId":264172340},"title":"Unlocking Emergent Modularity in Large Language Models"},{"paperId":"1b97570049b72de59c6c90eff9499b0fc8a299ac","externalIds":{"ArXiv":"2310.09832","DBLP":"journals/corr/abs-2310-09832","DOI":"10.48550/arXiv.2310.09832","CorpusId":264145896},"title":"Merging Experts into One: Improving Computational Efficiency of Mixture of Experts"},{"paperId":"bcd84a2b8f9ae40a908f375425f113c82f8dd739","externalIds":{"DBLP":"journals/corr/abs-2310-07096","ArXiv":"2310.07096","DOI":"10.48550/arXiv.2310.07096","CorpusId":263834790},"title":"Sparse Universal Transformer"},{"paperId":"db633c6b1c286c0386f0078d8a2e6224e03a6227","externalIds":{"ArXiv":"2310.06825","DBLP":"journals/corr/abs-2310-06825","DOI":"10.48550/arXiv.2310.06825","CorpusId":263830494},"title":"Mistral 7B"},{"paperId":"953e415f1fd006e968aa13b49cd7523856c0c0fe","externalIds":{"ArXiv":"2310.01542","DBLP":"conf/iclr/0001PS0XY24","DOI":"10.48550/arXiv.2310.01542","CorpusId":263608332},"title":"Fusing Models with Complementary Expertise"},{"paperId":"a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9","externalIds":{"ArXiv":"2309.14509","DBLP":"journals/corr/abs-2309-14509","DOI":"10.48550/arXiv.2309.14509","CorpusId":262826014},"title":"DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"},{"paperId":"83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","externalIds":{"DBLP":"conf/sosp/KwonLZ0ZY0ZS23","ArXiv":"2309.06180","DOI":"10.1145/3600006.3613165","CorpusId":261697361},"title":"Efficient Memory Management for Large Language Model Serving with PagedAttention"},{"paperId":"5aae7d84f8eaa55f3386cee41d94769e7ab01e9d","externalIds":{"DBLP":"journals/corr/abs-2309-05444","ArXiv":"2309.05444","DOI":"10.48550/arXiv.2309.05444","CorpusId":261697072},"title":"Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"},{"paperId":"3ed178316be914658a80e561bf00576577f34389","externalIds":{"DBLP":"conf/isca/HwangWCHTCY24","ArXiv":"2308.12066","DOI":"10.1109/ISCA59077.2024.00078","CorpusId":261076133},"title":"Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"},{"paperId":"041e1cbac95975029bc106eb3142a7192a6cbe40","externalIds":{"DBLP":"journals/corr/abs-2308-10110","ArXiv":"2308.10110","DOI":"10.1109/ICCV51070.2023.00015","CorpusId":261049747},"title":"Robust Mixture-of-Expert Training for Convolutional Neural Networks"},{"paperId":"6f88133bc591cd964667a626a06debad17775757","externalIds":{"ArXiv":"2308.06093","DBLP":"journals/corr/abs-2308-06093","DOI":"10.48550/arXiv.2308.06093","CorpusId":260865986},"title":"Experts Weights Averaging: A New General Training Scheme for Vision Transformers"},{"paperId":"2edccb8fa562ed52cd49ea6fc67ed32db6218247","externalIds":{"DBLP":"journals/corr/abs-2308-00951","ArXiv":"2308.00951","DOI":"10.48550/arXiv.2308.00951","CorpusId":260378993},"title":"From Sparse to Soft Mixtures of Experts"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","externalIds":{"ArXiv":"2307.09288","DBLP":"journals/corr/abs-2307-09288","CorpusId":259950998},"title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"9618aa98729670f74418d2087f5e47ab137856b4","externalIds":{"DBLP":"journals/corr/abs-2306-05406","ACL":"2023.acl-long.280","ArXiv":"2306.05406","DOI":"10.48550/arXiv.2306.05406","CorpusId":259108831},"title":"Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories"},{"paperId":"7e37a2c2575c62f374d7578b4a05e10197c17c7c","externalIds":{"DBLP":"conf/icml/ChowdhuryZW0C23","ArXiv":"2306.04073","DOI":"10.48550/arXiv.2306.04073","CorpusId":259095471},"title":"Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks"},{"paperId":"7a816dd242c4c3f652a448dba54daa53f89a9e4f","externalIds":{"DBLP":"journals/tmlr/MuqeethLR24","ArXiv":"2306.03745","DOI":"10.48550/arXiv.2306.03745","CorpusId":259088823},"title":"Soft Merging of Experts with Adaptive Routing"},{"paperId":"59f87a2464ab1d3c0376ca30d09c9204c89653dd","externalIds":{"DBLP":"conf/cvpr/ChenSDCZLG23","DOI":"10.1109/CVPR52729.2023.01138","CorpusId":254685665},"title":"Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners"},{"paperId":"4b5cbb924f06763a3c785d0ccfb3bc8bd765f4a5","externalIds":{"ArXiv":"2306.00008","DBLP":"conf/icml/ZhouDHPLHSSDLCL23","DOI":"10.48550/arXiv.2306.00008","CorpusId":258999501},"title":"Brainformers: Trading Simplicity for Efficiency"},{"paperId":"024a25b2445ecb3a181c5e2f39fbf8b73a4c1a6f","externalIds":{"ArXiv":"2305.18390","DBLP":"journals/corr/abs-2305-18390","DOI":"10.48550/arXiv.2305.18390","CorpusId":258967629},"title":"Emergent Modularity in Pre-trained Transformers"},{"paperId":"dbfd154190667087ed1cd6c7f75a81858c2f397e","externalIds":{"DBLP":"conf/iclr/ShenHZ0LWCZFCVW24","ArXiv":"2305.14705","CorpusId":259342096},"title":"Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models"},{"paperId":"d7fb8ecc2cf19687d5a56b93c19e0669961e7791","externalIds":{"ArXiv":"2305.14839","DBLP":"conf/acl/LiHYYHL23","ACL":"2023.acl-long.749","DOI":"10.48550/arXiv.2305.14839","CorpusId":258865469},"title":"PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts"},{"paperId":"5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200","externalIds":{"DBLP":"journals/corr/abs-2305-13245","ArXiv":"2305.13245","DOI":"10.48550/arXiv.2305.13245","CorpusId":258833177},"title":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"},{"paperId":"e9b3e82b1c9eb4136df28e94f24cd823431be93b","externalIds":{"DBLP":"journals/corr/abs-2305-12281","ArXiv":"2305.12281","DOI":"10.48550/arXiv.2305.12281","CorpusId":258833488},"title":"Lifelong Language Pretraining with Distribution-Specialized Experts"},{"paperId":"479f22891a50a553ae46ca32e7bc7e195d9293fc","externalIds":{"DBLP":"conf/infocom/ShiPCL23","DOI":"10.1109/INFOCOM53939.2023.10228874","CorpusId":261390644},"title":"PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining"},{"paperId":"c91e748fe13376eb7e3bc02be256421281772c04","externalIds":{"ArXiv":"2305.06942","DBLP":"conf/sc/PunniyamurthyHB24","DOI":"10.1109/SC41406.2024.00094","CorpusId":258615489},"title":"Optimizing Distributed ML Communication with Fused Computation-Collective Operations"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","externalIds":{"ArXiv":"2304.10592","DBLP":"conf/iclr/Zhu0SLE24","DOI":"10.48550/arXiv.2304.10592","CorpusId":258291930},"title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"dbbc5003af690799fa4fe6330fb795311cde106f","externalIds":{"DBLP":"journals/pacmmod/NieMWYXMC023","ArXiv":"2304.03946","DOI":"10.1145/3588964","CorpusId":258048524},"title":"FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"},{"paperId":"6007263dd3d14373be5f84fb6ccb0be3f7fce903","externalIds":{"DBLP":"journals/corr/abs-2303-15647","ArXiv":"2303.15647","CorpusId":257771591},"title":"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning"},{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","externalIds":{"ArXiv":"2303.10845","DBLP":"journals/corr/abs-2303-10845","CorpusId":257666647},"title":"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"9d12916dd46df7a6446cbec0bc4d054f7dafcdab","externalIds":{"DBLP":"conf/emnlp/ShenYLDKH23","ArXiv":"2303.07226","DOI":"10.48550/arXiv.2303.07226","CorpusId":257496100},"title":"Scaling Vision-Language Models with Sparse Mixture of Experts"},{"paperId":"443c1bef6a7dc3db941375ae76451c884ceffb8a","externalIds":{"DBLP":"conf/ics/SinghRARHB23","ArXiv":"2303.06318","DOI":"10.1145/3577193.3593704","CorpusId":258686146},"title":"A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training"},{"paperId":"1462a0e5b7db47301bb0995db56426e1f4a0ac7d","externalIds":{"ArXiv":"2303.01610","DBLP":"journals/corr/abs-2303-01610","DOI":"10.48550/arXiv.2303.01610","CorpusId":257353502},"title":"Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"},{"paperId":"a34384389f74b7b2c31c696b0db0bf813e8bb301","externalIds":{"DBLP":"conf/nips/ChenLWYY22","ArXiv":"2302.09915","DOI":"10.48550/arXiv.2302.09915","CorpusId":257038272},"title":"TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training"},{"paperId":"32fbc1821ef4a6daee1f9e9f140056ff9cda238a","externalIds":{"DBLP":"conf/sosp/ZhengJZHM0YZQYZ23","ArXiv":"2301.10936","DOI":"10.1145/3600006.3613139","CorpusId":256274793},"title":"PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation"},{"paperId":"397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e","externalIds":{"DBLP":"journals/corr/abs-2212-05055","ArXiv":"2212.05055","DOI":"10.48550/arXiv.2212.05055","CorpusId":254535822},"title":"Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"},{"paperId":"43014fc85c4860487336579ec98f509fec1803f7","externalIds":{"DBLP":"conf/mlsys/GaleNYZ23","ArXiv":"2211.15841","DOI":"10.48550/arXiv.2211.15841","CorpusId":254069783},"title":"MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"},{"paperId":"b8d3d97a4107dab0024912467d5dc02ef8e4afc6","externalIds":{"DBLP":"conf/acl/HeDDLYT23","ACL":"2023.acl-long.803","ArXiv":"2211.05528","DOI":"10.18653/v1/2023.acl-long.803","CorpusId":258887710},"title":"PAD-Net: An Efficient Framework for Dynamic Networks"},{"paperId":"e0271cb75087ccfd4a8c3351e0f5189a6de04c03","externalIds":{"ArXiv":"2210.06313","DBLP":"conf/iclr/LiYBLRRYCYGK23","CorpusId":259138847},"title":"The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers"},{"paperId":"3820231d31540ecb05d94c74d959a2f61d3136ea","externalIds":{"ArXiv":"2210.05144","DBLP":"conf/emnlp/ZhangSHZR022","ACL":"2022.emnlp-main.278","DOI":"10.48550/arXiv.2210.05144","CorpusId":252815815},"title":"Mixture of Attention Heads: Selecting Attention Heads Per Token"},{"paperId":"ca086f4c09cf8de705830ac2b70951737fab93ca","externalIds":{"DBLP":"journals/corr/abs-2209-01667","ArXiv":"2209.01667","DOI":"10.48550/arXiv.2209.01667","CorpusId":252089870},"title":"A Review of Sparse Expert Models in Deep Learning"},{"paperId":"8b3a67c7e5289eed160d2acfd04d71cfb552c67d","externalIds":{"ArXiv":"2208.03306","DBLP":"journals/corr/abs-2208-03306","DOI":"10.48550/arXiv.2208.03306","CorpusId":251371375},"title":"Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models"},{"paperId":"33d97417aa0e5c6fc4a58716003b02c09736e782","externalIds":{"ArXiv":"2208.02813","DBLP":"journals/corr/abs-2208-02813","DOI":"10.48550/arXiv.2208.02813","CorpusId":251320183},"title":"Towards Understanding Mixture of Experts in Deep Learning"},{"paperId":"e19b54ad4c1c8af045069e9cac350ffc2ce60e1a","externalIds":{"DBLP":"journals/corr/abs-2207-04672","ArXiv":"2207.04672","DOI":"10.48550/arXiv.2207.04672","CorpusId":250425961},"title":"No Language Left Behind: Scaling Human-Centered Machine Translation"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"bedf0d6e0623ab48349e3d2a493e7fbb79ca5ef5","externalIds":{"DBLP":"conf/nips/ZhuZWWLWD22","ArXiv":"2206.04674","DOI":"10.48550/arXiv.2206.04674","CorpusId":249538647},"title":"Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs"},{"paperId":"2e700ff36108119f5ed19a53bd2eaa22b42ec3d8","externalIds":{"ArXiv":"2206.03382","DBLP":"journals/corr/abs-2206-03382","DOI":"10.48550/arXiv.2206.03382","CorpusId":249431713},"title":"Tutel: Adaptive Mixture-of-Experts at Scale"},{"paperId":"499d3bb3acbc10730dd6582bd9b8f646bf22ccd5","externalIds":{"ArXiv":"2206.02770","DBLP":"conf/nips/MustafaRPJH22","DOI":"10.48550/arXiv.2206.02770","CorpusId":249394802},"title":"Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts"},{"paperId":"0fb8e89e34f00365a56c3e67f7e862fdcf935f2c","externalIds":{"DBLP":"journals/corr/abs-2206-00277","ArXiv":"2206.00277","DOI":"10.48550/arXiv.2206.00277","CorpusId":249240535},"title":"Task-Specific Expert Pruning for Sparse Mixture-of-Experts"},{"paperId":"87c5b281fa43e6f27191b20a8dd694eda1126336","externalIds":{"DBLP":"journals/corr/abs-2205-14135","ArXiv":"2205.14135","CorpusId":249151871},"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"paperId":"1da214f8f265445b5997f5d677452819b334bdfb","externalIds":{"DBLP":"conf/emnlp/YeZR22","ArXiv":"2205.12701","DOI":"10.18653/v1/2022.findings-emnlp.189","CorpusId":253761592},"title":"Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts"},{"paperId":"eb4d54651c4f610749caf2bf401af3ce28ddc439","externalIds":{"ArXiv":"2210.17451","DBLP":"conf/emnlp/WangAM00AG22","ACL":"2022.emnlp-main.388","DOI":"10.48550/arXiv.2210.17451","CorpusId":253153886},"title":"AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"},{"paperId":"68309384f7a8e96d98ede4271e7c04425c23f3f2","externalIds":{"DBLP":"journals/corr/abs-2205-08776","ArXiv":"2205.08776","DOI":"10.1145/3583780.3614773","CorpusId":248863193},"title":"AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation"},{"paperId":"7cdaa08890895e1ad92afb5fad429690ad7b1dac","externalIds":{"DBLP":"conf/nips/LiuTMMHBR22","ArXiv":"2205.05638","DOI":"10.48550/arXiv.2205.05638","CorpusId":248693283},"title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"},{"paperId":"bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96","externalIds":{"ArXiv":"2205.05198","DBLP":"journals/corr/abs-2205-05198","DOI":"10.48550/arXiv.2205.05198","CorpusId":248693351},"title":"Reducing Activation Recomputation in Large Transformer Models"},{"paperId":"ccdae60e7b5f78ac04d0cc7f3b66b2d9b8fd296e","externalIds":{"DBLP":"journals/corr/abs-2204-09636","ArXiv":"2204.09636","DOI":"10.48550/arXiv.2204.09636","CorpusId":248266614},"title":"Residual Mixture of Experts"},{"paperId":"c26bb68806a992bf4fc85b5639e1657a445c4781","externalIds":{"DBLP":"journals/corr/abs-2204-09179","ArXiv":"2204.09179","DOI":"10.48550/arXiv.2204.09179","CorpusId":248266346},"title":"On the Representation Collapse of Sparse Mixture of Experts"},{"paperId":"c9550f0d1940ee1adf1549c9a0d699ef896dbefd","externalIds":{"ACL":"2022.acl-long.489","DBLP":"conf/acl/Dai0MZSCW22","ArXiv":"2204.08396","DOI":"10.48550/arXiv.2204.08396","CorpusId":248227505},"title":"StableMoE: Stable Routing Strategy for Mixture of Experts"},{"paperId":"df434c1289f3c7243b585cb9982afac3c5bf0439","externalIds":{"DBLP":"conf/naacl/ZuoZLHZC22","ACL":"2022.naacl-main.116","ArXiv":"2204.07675","DOI":"10.48550/arXiv.2204.07675","CorpusId":248227273},"title":"MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","externalIds":{"ArXiv":"2204.02311","DBLP":"journals/corr/abs-2204-02311","CorpusId":247951931},"title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","externalIds":{"DBLP":"journals/corr/abs-2203-15556","ArXiv":"2203.15556","CorpusId":247778764},"title":"Training Compute-Optimal Large Language Models"},{"paperId":"408efcc963ab871558e5e145b54ad7ae2aeb11c7","externalIds":{"DBLP":"conf/ppopp/MaHQCWSZWTZLFHG22","DOI":"10.1145/3503221.3508417","CorpusId":247289671},"title":"BaGuaLu: targeting brain scale pretrained models with over 37 million cores"},{"paperId":"54b8bc5be8bbffae333dd73f2cb9d93a492d438e","externalIds":{"DBLP":"journals/corr/abs-2203-14685","ArXiv":"2203.14685","DOI":"10.48550/arXiv.2203.14685","CorpusId":247762354},"title":"HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System"},{"paperId":"0dab58e476f3f0e6f580a295f7c4756c86f1f198","externalIds":{"DBLP":"conf/ppopp/HeZAWLSL22","DOI":"10.1145/3503221.3508418","CorpusId":247765599},"title":"FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models"},{"paperId":"8c62277dada489904a63de4dd87336c27c68fb5e","externalIds":{"DBLP":"journals/corr/abs-2203-06904","ArXiv":"2203.06904","DOI":"10.48550/arXiv.2203.06904","CorpusId":247446969},"title":"Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"},{"paperId":"bbc57e1b3cf90e09b64377f13de455793bc81ad5","externalIds":{"DBLP":"journals/corr/abs-2202-09368","ArXiv":"2202.09368","CorpusId":247011948},"title":"Mixture-of-Experts with Expert Choice Routing"},{"paperId":"1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3","externalIds":{"ArXiv":"2202.08906","CorpusId":248496391},"title":"ST-MoE: Designing Stable and Transferable Sparse Expert Models"},{"paperId":"c2536182c010c41941e8a031071a1880c34cec60","externalIds":{"ArXiv":"2202.01169","DBLP":"journals/corr/abs-2202-01169","CorpusId":246473179},"title":"Unified Scaling Laws for Routed Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","externalIds":{"ArXiv":"2201.11990","DBLP":"journals/corr/abs-2201-11990","CorpusId":246411325},"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"9caa83288602f6c3734c78d8d89bb358b263da24","externalIds":{"ArXiv":"2201.10890","DBLP":"conf/iclr/XueHRLY23","CorpusId":246285330},"title":"One Student Knows All Experts Know: From Sparse to Dense"},{"paperId":"7d1e512888a2fa4e838c12a02ae7fce867d322a8","externalIds":{"DBLP":"conf/icml/RajbhandariLYZA22","ArXiv":"2201.05596","CorpusId":245986500},"title":"DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"},{"paperId":"cf4f4b69b76dc58dc8c0d443ab88ceb461eec719","externalIds":{"ArXiv":"2112.14397","CorpusId":252780015},"title":"EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate"},{"paperId":"fb01415a0decfa3f3d6339930e95028ae1ff4170","externalIds":{"ArXiv":"2112.10684","DBLP":"conf/emnlp/ArtetxeBGMOSLDI22","DOI":"10.18653/v1/2022.emnlp-main.804","CorpusId":245334345},"title":"Efficient Large Scale Language Modeling with Mixtures of Experts"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","externalIds":{"ArXiv":"2112.06905","DBLP":"journals/corr/abs-2112-06905","CorpusId":245124124},"title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"921a958829a80ae7dada07c22ec79c76af8bf902","externalIds":{"DBLP":"journals/ijon/ZhengW22","DOI":"10.1016/j.neucom.2021.11.041","CorpusId":244814987},"title":"A survey of recommender systems with multi-objective optimization"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"547284c6a9c074d7b5cee4049b59fb52ad93df41","externalIds":{"ArXiv":"2110.08246","DBLP":"conf/naacl/DuaBGCLF22","ACL":"2022.naacl-main.244","DOI":"10.18653/v1/2022.naacl-main.244","CorpusId":239009754},"title":"Tricks for Training Sparse Translation Models"},{"paperId":"ad471be93216ddbf8544721d50ee5aed14f07cae","externalIds":{"DBLP":"conf/acl/MaoMHAM0YK22","ACL":"2022.acl-long.433","ArXiv":"2110.07577","DOI":"10.18653/v1/2022.acl-long.433","CorpusId":238857301},"title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning"},{"paperId":"0e23cc159fd2fb34550600d60dd9148c93636183","externalIds":{"DBLP":"journals/corr/abs-2110-04260","ArXiv":"2110.04260","CorpusId":238531645},"title":"Taming Sparsely Activated Transformer with Stochastic Experts"},{"paperId":"561f9f5abb2c0960a886ab6221c821295f0461a1","externalIds":{"DBLP":"conf/acl/ZhangL00S022","ACL":"2022.findings-acl.71","ArXiv":"2110.01786","DOI":"10.18653/v1/2022.findings-acl.71","CorpusId":247958465},"title":"MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"},{"paperId":"8ae292cbd9144acbf4b42b7ead82b079faf33192","externalIds":{"DBLP":"conf/emnlp/KuduguntaHBKLLF21","ArXiv":"2110.03742","DOI":"10.18653/v1/2021.findings-emnlp.304","CorpusId":238531628},"title":"Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference"},{"paperId":"d52977458284acce166bb1291a9d79143aa59070","externalIds":{"DBLP":"journals/corr/abs-2109-10465","ArXiv":"2109.10465","CorpusId":237593026},"title":"Scalable and Efficient MoE Training for Multitask Multilingual Models"},{"paperId":"96ea07447d2f9adefe03852a878517a2a6d45b96","externalIds":{"ArXiv":"2109.01134","DBLP":"journals/ijcv/ZhouYLL22","DOI":"10.1007/s11263-022-01653-1","CorpusId":237386023},"title":"Learning to Prompt for Vision-Language Models"},{"paperId":"917c63f2186119166b3379f5d2816bb1a2f39b09","externalIds":{"ACL":"2022.naacl-main.407","DBLP":"journals/corr/abs-2108-05036","ArXiv":"2108.05036","DOI":"10.18653/v1/2022.naacl-main.407","CorpusId":236976189},"title":"DEMix Layers: Disentangling Domains for Modular Language Modeling"},{"paperId":"ffcd58f453f207d48075627da011f62782334c8f","externalIds":{"DBLP":"journals/corr/abs-2107-11817","ArXiv":"2107.11817","DOI":"10.1609/aaai.v36i8.20858","CorpusId":236428967},"title":"Go Wider Instead of Deeper"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01","externalIds":{"ArXiv":"2106.05974","DBLP":"conf/nips/RiquelmePMNJPKH21","CorpusId":235417196},"title":"Scaling Vision with Sparse Mixture of Experts"},{"paperId":"0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe","externalIds":{"DBLP":"conf/nips/RollerSSW21","ArXiv":"2106.04426","CorpusId":235367626},"title":"Hash Layers For Large Sparse Models"},{"paperId":"36ffa5b1f643f59ccf8396cff9865e5474c8dae7","externalIds":{"DBLP":"journals/corr/abs-2106-03760","ArXiv":"2106.03760","CorpusId":235358484},"title":"DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning"},{"paperId":"16e623059ffccab60f4c35be028a2d4f10933515","externalIds":{"ArXiv":"2105.13120","DBLP":"conf/acl/LiXBL023","ACL":"2023.acl-long.134","DOI":"10.18653/v1/2023.acl-long.134","CorpusId":246017095},"title":"Sequence Parallelism: Long Sequence Training from System Perspective"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","externalIds":{"DBLP":"journals/ijon/SuALPBL24","ArXiv":"2104.09864","DOI":"10.1016/j.neucom.2023.127063","CorpusId":233307138},"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"72dd63d67588a42fc817bbb8d655b397f67425df","externalIds":{"DBLP":"journals/corr/abs-2104-07857","ArXiv":"2104.07857","DOI":"10.1145/3458817.3476205","CorpusId":233289729},"title":"ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"},{"paperId":"774591fdd988eaaff3917e7c5171d044b0843e63","externalIds":{"ArXiv":"2104.04473","DBLP":"conf/sc/NarayananSCLPKV21","DOI":"10.1145/3458817.3476209","CorpusId":236635565},"title":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"},{"paperId":"b15ea460c77a4ee8aa159a30ab0331deedfcf392","externalIds":{"DBLP":"conf/icml/LewisBDGZ21","ArXiv":"2103.16716","CorpusId":232428341},"title":"BASE Layers: Simplifying Training of Large, Sparse Models"},{"paperId":"238eb420c472bfdb1b4d34f9f53abec51f307a6b","externalIds":{"DBLP":"journals/corr/abs-2103-13262","ArXiv":"2103.13262","CorpusId":232335691},"title":"FastMoE: A Fast Mixture-of-Expert Training System"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","externalIds":{"DBLP":"conf/nips/HendrycksBKABTS21","ArXiv":"2103.03874","CorpusId":232134851},"title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"12b71736392209b4292471b7da0aed71ba2aa545","externalIds":{"DBLP":"conf/usenix/0015RARYZ0H21","MAG":"3121562065","ArXiv":"2101.06840","CorpusId":231632857},"title":"ZeRO-Offload: Democratizing Billion-Scale Model Training"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"MAG":"3094502228","ArXiv":"2010.11929","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"687b13c44f849d23c2496996b5da83e706094db9","externalIds":{"DBLP":"journals/corr/abs-2010-11125","MAG":"3093871477","ArXiv":"2010.11125","CorpusId":224814118},"title":"Beyond English-Centric Multilingual Machine Translation"},{"paperId":"0182197c3996d11867ef650b66a2ddf1efa6f631","externalIds":{"DBLP":"conf/recsys/TangLZG20","MAG":"3087931390","DOI":"10.1145/3383313.3412236","CorpusId":221784966},"title":"Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","externalIds":{"MAG":"3040573126","DBLP":"conf/iclr/LepikhinLXCFHKS21","ArXiv":"2006.16668","CorpusId":220265858},"title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","externalIds":{"MAG":"3006439205","DBLP":"journals/corr/abs-2002-05202","ArXiv":"2002.05202","CorpusId":211096588},"title":"GLU Variants Improve Transformer"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"3fd7c9ba742dd2b435afa75217847e5087e2f2a8","externalIds":{"DBLP":"conf/sosp/NarayananHPSDGG19","MAG":"2969388332","DOI":"10.1145/3341301.3359646","CorpusId":202488191},"title":"PipeDream: generalized pipeline parallelism for DNN training"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"10eda4521c032adabaa8e70d6569e17370b29dcd","externalIds":{"ArXiv":"1910.07467","DBLP":"conf/nips/ZhangS19a","MAG":"2981040094","DOI":"10.5167/UZH-177483","CorpusId":113405151},"title":"Root Mean Square Layer Normalization"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","externalIds":{"MAG":"3025935268","DBLP":"conf/sc/RajbhandariRRH20","ArXiv":"1910.02054","DOI":"10.1109/SC41405.2020.00024","CorpusId":269617042},"title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":"6648b4db5f12c30941ea78c695e77aded19672bb","externalIds":{"MAG":"2997591391","ArXiv":"1909.11059","DBLP":"journals/corr/abs-1909-11059","DOI":"10.1609/AAAI.V34I07.7005","CorpusId":202734445},"title":"Unified Vision-Language Pre-Training for Image Captioning and VQA"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","externalIds":{"MAG":"2973727699","ArXiv":"1909.08053","DBLP":"journals/corr/abs-1909-08053","CorpusId":202660670},"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","externalIds":{"MAG":"2966715458","DBLP":"journals/corr/abs-1908-02265","ArXiv":"1908.02265","CorpusId":199453025},"title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"bf442ab269074665a68e4dbbe19e4efc97862541","externalIds":{"MAG":"2970401203","DBLP":"conf/nips/LampleSRDJ19","ArXiv":"1907.05242","CorpusId":195886317},"title":"Large Memory Layers with Product Keys"},{"paperId":"be928f91385999fa90d1e2fe06058f9dbcfd7186","externalIds":{"MAG":"2942190342","ArXiv":"1904.12774","DBLP":"journals/corr/abs-1904-12774","CorpusId":139103965},"title":"Routing Networks and the Challenges of Modular and Compositional Computation"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","externalIds":{"MAG":"2952509486","DBLP":"journals/corr/abs-1904-01038","ArXiv":"1904.01038","ACL":"N19-4009","DOI":"10.18653/v1/N19-4009","CorpusId":91184134},"title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"d79a26226393f687ddbc375e32055b40b8ad8d38","externalIds":{"MAG":"2991040477","DBLP":"conf/nips/HuangCBFCCLNLWC19","CorpusId":53670168},"title":"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"},{"paperId":"2270b8628fd8ca67ae39d277f45bc3c38ac63d5f","externalIds":{"ArXiv":"1811.02084","DBLP":"journals/corr/abs-1811-02084","MAG":"2963351145","CorpusId":53236433},"title":"Mesh-TensorFlow: Deep Learning for Supercomputers"},{"paperId":"af8a8dcb74561d52d904f7bc4afcc747e079b702","externalIds":{"MAG":"2809290718","DBLP":"conf/kdd/MaZYCHC18","DOI":"10.1145/3219819.3220007","CorpusId":50770252},"title":"Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts"},{"paperId":"6428fa299a76ccce775b17ea6f3401ef0cfccf04","externalIds":{"MAG":"2966327190","DBLP":"journals/corr/abs-1806-01531","CorpusId":119184034},"title":"Deep Mixture of Experts via Shallow Embedding"},{"paperId":"59c22bff89a102365814cc61eb06ce3325eda39d","externalIds":{"MAG":"2951336270","ArXiv":"1712.03779","DBLP":"journals/jzusc/YuK18","DOI":"10.1631/FITEE.1700813","CorpusId":1484586},"title":"Artificial intelligence and statistics"},{"paperId":"fc9d5be5e3f14b4c4b145b6c4bd96a9182f39fd2","externalIds":{"ArXiv":"1711.01239","DBLP":"conf/iclr/RosenbaumKR18","MAG":"2767175863","CorpusId":22014305},"title":"Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91","externalIds":{"ArXiv":"1705.09406","DBLP":"journals/pami/BaltrusaitisAM19","MAG":"2951127645","DOI":"10.1109/TPAMI.2018.2798607","CorpusId":10137425,"PubMed":"29994351"},"title":"Multimodal Machine Learning: A Survey and Taxonomy"},{"paperId":"e9c9da57bbf9a968489cb90ec7252319bcab42fb","externalIds":{"DBLP":"conf/cvpr/GrossRS17","MAG":"2949392504","ArXiv":"1704.06363","DOI":"10.1109/CVPR.2017.540","CorpusId":6869636},"title":"Hard Mixtures of Experts for Large Scale Weakly Supervised Vision"},{"paperId":"510e26733aaff585d65701b9f1be7ca9d5afc586","externalIds":{"DBLP":"journals/corr/ShazeerMMDLHD17","MAG":"2952339051","ArXiv":"1701.06538","CorpusId":12462234},"title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"},{"paperId":"2e55ba6c97ce5eb55abd959909403fe8da7e9fe9","externalIds":{"DBLP":"journals/corr/KirkpatrickPRVD16","MAG":"2560647685","ArXiv":"1612.00796","DOI":"10.1073/pnas.1611835114","CorpusId":4704285,"PubMed":"28292907"},"title":"Overcoming catastrophic forgetting in neural networks"},{"paperId":"66b8d34477cf1736f91fd22b27e37ce0b703c86e","externalIds":{"DBLP":"journals/corr/AljundiCT16","ArXiv":"1611.06194","MAG":"2950510923","DOI":"10.1109/CVPR.2017.753","CorpusId":914027},"title":"Expert Gate: Lifelong Learning with a Network of Experts"},{"paperId":"de5e7320729f5d3cbb6709eb6329ec41ace8c95d","externalIds":{"ArXiv":"1606.08415","MAG":"2899663614","CorpusId":125617073},"title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"5a5d48986b855b83a7d9df5005bbd155024ce756","externalIds":{"MAG":"2963518064","DBLP":"conf/icml/AlmahairiBCZLC16","ArXiv":"1511.07838","CorpusId":818973},"title":"Dynamic Capacity Networks"},{"paperId":"fba71eefd060e30f3516fdd46df9a191cd0aaaf7","externalIds":{"MAG":"2179423374","DBLP":"journals/corr/BengioBPP15","ArXiv":"1511.06297","CorpusId":16049527},"title":"Conditional Computation in Neural Networks for faster models"},{"paperId":"f267934e9de60c5badfa9d3f28918e67ae7a2bf4","externalIds":{"MAG":"2097039814","ArXiv":"1506.03478","DBLP":"journals/corr/TheisB15","CorpusId":2865509},"title":"Generative Image Modeling Using Spatial LSTMs"},{"paperId":"e2e81c568ac0aa067e32fbc9ca9396824fa04d66","externalIds":{"MAG":"1911299338","DBLP":"conf/icml/DeisenrothN15","ArXiv":"1502.02843","CorpusId":17686175},"title":"Distributed Gaussian Processes"},{"paperId":"44ddac48353ead135eef4096859956eaa31be2a5","externalIds":{"MAG":"2949231389","DBLP":"journals/corr/EigenRS13","ArXiv":"1312.4314","CorpusId":11492613},"title":"Learning Factored Representations in a Deep Mixture of Experts"},{"paperId":"cf3229e74f912ef365d67d1954441b32ce2573ee","externalIds":{"MAG":"2964072166","ArXiv":"1312.4461","DBLP":"journals/corr/DavisA13","CorpusId":7047554},"title":"Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks"},{"paperId":"62c76ca0b2790c34e85ba1cce09d47be317c7235","externalIds":{"ArXiv":"1308.3432","DBLP":"journals/corr/BengioLC13","MAG":"2242818861","CorpusId":18406556},"title":"Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"},{"paperId":"9a7987ec32aa05b625e63d0deb4affd58e22ce45","externalIds":{"MAG":"2068238590","DBLP":"journals/tnn/YukselWG12","DOI":"10.1109/TNNLS.2012.2200299","CorpusId":9922492,"PubMed":"24807516"},"title":"Twenty Years of Mixture of Experts"},{"paperId":"a78273144520d57e150744cf75206e881e11cc5b","externalIds":{"ArXiv":"2301.04856","MAG":"2184188583","DBLP":"journals/corr/abs-2301-04856","DOI":"10.48550/arXiv.2301.04856","CorpusId":352650},"title":"Multimodal Deep Learning"},{"paperId":"67107f78a84bdb2411053cb54e94fa226eea6d8e","externalIds":{"MAG":"2156387975","DBLP":"journals/jmlr/GlorotBB11","CorpusId":2239473},"title":"Deep Sparse Rectifier Neural Networks"},{"paperId":"81c9cb2c3c08070a80c7b26c288789084f70b43b","externalIds":{"MAG":"2952505044","ArXiv":"math/0703292","DBLP":"journals/jmlr/ShahbabaN09","DOI":"10.5555/1577069.1755846","CorpusId":2044597},"title":"Nonlinear Models Using Dirichlet Process Mixtures"},{"paperId":"16174aa9f7b2f72a078b1301244367f40a754502","externalIds":{"DBLP":"conf/nips/CollobertBB01","MAG":"2125126592","DOI":"10.1162/089976602753633402","CorpusId":529933,"PubMed":"11972909"},"title":"A Parallel Mixture of SVMs for Very Large Scale Problems"},{"paperId":"480d517574a079d3e0159b978cb19b3f014e59a3","externalIds":{"DBLP":"conf/nips/RasmussenG01","MAG":"2098949458","CorpusId":5062147},"title":"Infinite Mixtures of Gaussian Process Experts"},{"paperId":"f6d8a7fc2e2d53923832f9404376512068ca2a57","externalIds":{"MAG":"2025653905","DBLP":"journals/neco/JordanJ94","DOI":"10.1109/IJCNN.1993.716791","CorpusId":67000854},"title":"Hierarchical Mixtures of Experts and the EM Algorithm"},{"paperId":"c8d90974c3f3b40fa05e322df2905fc16204aa56","externalIds":{"DBLP":"journals/neco/JacobsJNH91","MAG":"2150884987","DOI":"10.1162/neco.1991.3.1.79","CorpusId":572361,"PubMed":"31141872"},"title":"Adaptive Mixtures of Local Experts"},{"paperId":"5b6fdb2aea424d02c141da81d62b04d739d62b96","externalIds":{"DBLP":"journals/corr/abs-2412-19437","CorpusId":275118643},"title":"DeepSeek-V3 Technical Report"},{"paperId":"5fffc5f56c67740603c68473dea50ce059cbb78f","externalIds":{"DBLP":"journals/corr/abs-2312-09979","DOI":"10.48550/arXiv.2312.09979","CorpusId":268610117},"title":"LoRAMoE: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment"},{"paperId":"0bd2602df71e89c8961562175c8759e625e99389","externalIds":{"DBLP":"journals/corr/abs-2306-04640","DOI":"10.48550/arXiv.2306.04640","CorpusId":259096191},"title":"ModuleFormer: Learning Modular Large Language Models From Uncurated Data"},{"paperId":"1fb8f2d080e965c833c777f06fccf09dc9856b91","externalIds":{"DBLP":"journals/corr/abs-2310-18339","DOI":"10.48550/arXiv.2310.18339","CorpusId":271065459},"title":"MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications"},{"paperId":"50f2a5b103884c2edb11506445eabff613a79b6e","externalIds":{"DBLP":"conf/emnlp/ChoiKPM023","DOI":"10.18653/v1/2023.emnlp-main.884","CorpusId":266163946},"title":"SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts"},{"paperId":"66129521108ddf3db1377f425cc846ee5aa36a1e","externalIds":{"DBLP":"journals/corr/abs-2310-15961","DOI":"10.48550/arXiv.2310.15961","CorpusId":264439523},"title":"Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation"},{"paperId":"5c6a17850c9ad6bf6dc8992ec598cd932ce42208","externalIds":{"DBLP":"conf/usenix/ZhaiHMZZZ23","CorpusId":259858759},"title":"SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization"},{"paperId":"dc7b27b0a1d891e16f80dd9b8eec0aa8baf95b36","externalIds":{"DBLP":"journals/corr/abs-2308-14352","DOI":"10.48550/arXiv.2308.14352","CorpusId":283081399},"title":"EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models"},{"paperId":"24bd5b7455a07631cbc4023c07eb2f0eef85ea85","externalIds":{"MAG":"3193171560","DBLP":"journals/inffus/UppalBHMPZZ22","DOI":"10.1016/j.inffus.2021.07.009","CorpusId":238639167},"title":"Multimodal research in vision and language: A review of current and emerging trends"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","externalIds":{"DBLP":"conf/acl/LiL20","ACL":"2021.acl-long.353","ArXiv":"2101.00190","DOI":"10.18653/v1/2021.acl-long.353","CorpusId":230433941},"title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","externalIds":{"ArXiv":"2103.14030","DBLP":"conf/iccv/LiuL00W0LG21","DOI":"10.1109/ICCV48922.2021.00986","CorpusId":232352874},"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"91a7d2af2c3294a8e0356e4eeb043636bbb3ca04","externalIds":{"CorpusId":236956947},"title":"M6-T: Exploring Sparse Expert Models and Beyond Anonymous ACL submission"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"67f3725da6b721a0f31461c2a0bd18236efcec0f","externalIds":{"CorpusId":269588500},"title":"A Case Study of Instruction Tuning with Mixture of Parameter-Efficient Experts"},{"paperId":"1d27a56a8133f947a5a0217b00241d26f585f834","externalIds":{"CorpusId":246411250},"title":"This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."}]}