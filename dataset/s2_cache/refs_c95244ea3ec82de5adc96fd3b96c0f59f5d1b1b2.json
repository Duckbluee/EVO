{"references":[{"paperId":"dde0924c125216db5d8bd71dd69cd7b224fd5316","externalIds":{"DBLP":"conf/cvpr/Liang0MTD024","ArXiv":"2312.11598","DOI":"10.1109/CVPR52733.2024.01558","CorpusId":266361979},"title":"SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution"},{"paperId":"bebcb0832cdafb102c75c96ca18207d05034ab0c","externalIds":{"ArXiv":"2312.08533","DBLP":"journals/corr/abs-2312-08533","DOI":"10.48550/arXiv.2312.08533","CorpusId":266209789},"title":"World Models via Policy-Guided Trajectory Diffusion"},{"paperId":"5a1e56d242226495e7036c83d270eb329399623c","externalIds":{"DBLP":"journals/corr/abs-2312-04549","ArXiv":"2312.04549","DOI":"10.48550/arXiv.2312.04549","CorpusId":266055193},"title":"PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play"},{"paperId":"a9c4efebbeda0c38601eee232a07c5c307582140","externalIds":{"DBLP":"conf/icra/KataraXF24","ArXiv":"2310.18308","DOI":"10.1109/ICRA57147.2024.10610566","CorpusId":264555343},"title":"Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models"},{"paperId":"d6bd400073090b88ea535a6166ca9c164b8015b7","externalIds":{"ArXiv":"2310.10639","DBLP":"journals/corr/abs-2310-10639","DOI":"10.48550/arXiv.2310.10639","CorpusId":264172455},"title":"Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models"},{"paperId":"7090d35c316afe869440ede6ad61fdeec85b8bc8","externalIds":{"DBLP":"journals/corr/abs-2401-03360","ArXiv":"2401.03360","DOI":"10.48550/arXiv.2401.03360","CorpusId":261685884},"title":"Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models"},{"paperId":"4e8adade39705cf8683984705bf31f1884a8fff7","externalIds":{"DBLP":"journals/corr/abs-2310-08576","ArXiv":"2310.08576","DOI":"10.48550/arXiv.2310.08576","CorpusId":263908842},"title":"Learning to Act from Actionless Videos through Dense Correspondences"},{"paperId":"25ad5cea6a31cca62dfadfb0cc6f010e646d5647","externalIds":{"DBLP":"journals/corr/abs-2310-06343","ArXiv":"2310.06343","DOI":"10.48550/arXiv.2310.06343","CorpusId":263829075},"title":"Boosting Continuous Control with Consistency Policy"},{"paperId":"d63018e113db0fe3f4c14f8a2d2fac5a90ef609b","externalIds":{"ArXiv":"2310.05333","DBLP":"journals/corr/abs-2310-05333","DOI":"10.48550/arXiv.2310.05333","CorpusId":263830870},"title":"DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning"},{"paperId":"07575d485efe74f9440a36d7b343ac51415ee63e","externalIds":{"ArXiv":"2310.02054","DBLP":"conf/iclr/DongYHNM0HLFH24","DOI":"10.48550/arXiv.2310.02054","CorpusId":263609296},"title":"AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model"},{"paperId":"02aaccf045178a0ad70906941f21b1eabde34227","externalIds":{"DBLP":"conf/iclr/Li24","ArXiv":"2310.00311","DOI":"10.48550/arXiv.2310.00311","CorpusId":263334587},"title":"Efficient Planning with Latent Diffusion"},{"paperId":"bcc5820c7a84f84347bbf1062dcf7330fe2b0870","externalIds":{"DBLP":"conf/iclr/VenkatramanKAD024","ArXiv":"2309.06599","DOI":"10.48550/arXiv.2309.06599","CorpusId":261706069},"title":"Reasoning with Latent Diffusion in Offline Reinforcement Learning"},{"paperId":"6982d8589d3e13d2b21e90ff4bda7d6ba8554ca7","externalIds":{"DBLP":"conf/itsc/ChenCYZ023","ArXiv":"2308.06564","DOI":"10.1109/ITSC57777.2023.10421892","CorpusId":260887031},"title":"EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction"},{"paperId":"8223da35c0fc3143f2903b5bfbfac9e4cce81d0a","externalIds":{"DBLP":"conf/iros/Carvalho0BK023","ArXiv":"2308.01557","DOI":"10.1109/IROS55552.2023.10342382","CorpusId":260191316},"title":"Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models"},{"paperId":"af6d0ba799213cbbcbfceb1fb9b78d2858486308","externalIds":{"DBLP":"journals/corr/abs-2307-14535","ArXiv":"2307.14535","DOI":"10.48550/arXiv.2307.14535","CorpusId":260203080},"title":"Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition"},{"paperId":"2e3ba918a407f5e5d7a4bae88e38e281578c9040","externalIds":{"ArXiv":"2307.07837","DBLP":"conf/nips/GaoHXX23","DOI":"10.48550/arXiv.2307.07837","CorpusId":259937786},"title":"Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?"},{"paperId":"6ec775004e6f4e59dfdfdaf965c64895cbb338d1","externalIds":{"DBLP":"journals/corr/abs-2307-04726","ArXiv":"2307.04726","DOI":"10.1109/LRA.2024.3363530","CorpusId":259501809},"title":"Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning"},{"paperId":"fae53cfc54eed4f2b602cce39d5519782e948d03","externalIds":{"ArXiv":"2307.01849","DBLP":"conf/icra/LiBSR24","DOI":"10.1109/ICRA57147.2024.10610175","CorpusId":259342264},"title":"Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning"},{"paperId":"aeb466d5198024d5770863434f103814d80d7fcb","externalIds":{"ArXiv":"2307.01472","DBLP":"journals/corr/abs-2307-01472","DOI":"10.48550/arXiv.2307.01472","CorpusId":259342283},"title":"Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning"},{"paperId":"a3a5660778ddad38edb4c0940aa922a7f3f92123","externalIds":{"DBLP":"journals/corr/abs-2306-14079","ArXiv":"2306.14079","DOI":"10.48550/arXiv.2306.14079","CorpusId":259252513},"title":"Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching"},{"paperId":"0f10fc57c7a4aae8181feeca69ad232a05ead7cb","externalIds":{"DBLP":"journals/corr/abs-2306-07290","ArXiv":"2306.07290","DOI":"10.48550/arXiv.2306.07290","CorpusId":259144979},"title":"Value function estimation using conditional diffusion models for control"},{"paperId":"7312a2d0ec17241a139b5a8044914e06c12d93cc","externalIds":{"ArXiv":"2306.04875","DBLP":"journals/corr/abs-2306-04875","DOI":"10.48550/arXiv.2306.04875","CorpusId":259108562,"PubMed":"41525589"},"title":"Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning"},{"paperId":"7de381a18b7fc6f716da6fa4534bc302f4f46dbd","externalIds":{"DBLP":"conf/cvpr/JiangCPSZA23","ArXiv":"2306.03083","DOI":"10.1109/CVPR52729.2023.00930","CorpusId":259076079},"title":"MotionDiffuser: Controllable Multi-Agent Motion Prediction Using Diffusion"},{"paperId":"7e2af9a5f665e9b6a95c3065f19aa06b1c012db9","externalIds":{"ArXiv":"2306.01804","DBLP":"journals/corr/abs-2306-01804","DOI":"10.48550/arXiv.2306.01804","CorpusId":259075141},"title":"Extracting Reward Functions from Diffusion Models"},{"paperId":"0b6eea72dcbe3257cac62afb0db9ec28505ede0b","externalIds":{"DBLP":"journals/corr/abs-2306-00148","ArXiv":"2306.00148","DOI":"10.48550/arXiv.2306.00148","CorpusId":259000023},"title":"SafeDiffuser: Safe Planning with Diffusion Probabilistic Models"},{"paperId":"42fa70f18de0fb98381c76f624bd69e3a80d9cdd","externalIds":{"DBLP":"conf/icml/NiHMYZWL23","ArXiv":"2305.19923","DOI":"10.48550/arXiv.2305.19923","CorpusId":258987376},"title":"MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL"},{"paperId":"2aae2dd79ef30022da59d8c33f0b3afa55d9c20d","externalIds":{"DBLP":"conf/nips/Kang0DPY23","ArXiv":"2305.20081","DOI":"10.48550/arXiv.2305.20081","CorpusId":258987691},"title":"Efficient Diffusion Policies for Offline Reinforcement Learning"},{"paperId":"569648a84a6fa3b9e3659c60047d63ed04d5c175","externalIds":{"DBLP":"journals/corr/abs-2305-18738","ArXiv":"2305.18738","DOI":"10.48550/arXiv.2305.18738","CorpusId":258967270},"title":"Generating Behaviorally Diverse Policies with Latent Diffusion Models"},{"paperId":"828e27fd4fcd5e8982032b903950947b12afb6bb","externalIds":{"ArXiv":"2305.18459","DBLP":"conf/nips/HeBXY0WZ023","DOI":"10.48550/arXiv.2305.18459","CorpusId":258968054},"title":"Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning"},{"paperId":"fe9fe9f15f24fbbb19b62bcd9a3418511a699b84","externalIds":{"DBLP":"journals/corr/abs-2305-13122","ArXiv":"2305.13122","DOI":"10.48550/arXiv.2305.13122","CorpusId":258832463},"title":"Policy Representation via Diffusion Probability Model for Reinforcement Learning"},{"paperId":"e90637fd3efe2d208378c361a5c34c94d1f39993","externalIds":{"ArXiv":"2305.12171","DBLP":"journals/corr/abs-2305-12171","DOI":"10.1109/LRA.2023.3330663","CorpusId":258832509},"title":"Diffusion Co-Policy for Synergistic Human-Robot Collaborative Tasks"},{"paperId":"3a09f8a8d64955c0bab002c08579ba3ff567b6c5","externalIds":{"ArXiv":"2304.12824","DBLP":"journals/corr/abs-2304-12824","DOI":"10.48550/arXiv.2304.12824","CorpusId":258309302},"title":"Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning"},{"paperId":"a6f9fb141034a87ff9d627dc8a3ef31d0790c6ed","externalIds":{"ArXiv":"2304.10573","DBLP":"journals/corr/abs-2304-10573","DOI":"10.48550/arXiv.2304.10573","CorpusId":258291616},"title":"IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies"},{"paperId":"1334a47e8f4e4ffd04ff534329d76a5e5cc16f46","externalIds":{"DBLP":"journals/corr/abs-2304-02532","ArXiv":"2304.02532","DOI":"10.48550/arXiv.2304.02532","CorpusId":257952177},"title":"Goal-Conditioned Imitation Learning using Score-based Diffusion Policies"},{"paperId":"731ac2fd7a5586d17cb8959c190158d429821a60","externalIds":{"ArXiv":"2304.01116","DBLP":"conf/iccv/ZhangGPCHLYL23","DOI":"10.1109/ICCV51070.2023.00040","CorpusId":257913363},"title":"ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model"},{"paperId":"90889ff0ce930528e65b1b761bf9bcf9195e5051","externalIds":{"ArXiv":"2303.06614","DBLP":"conf/nips/LuBTP23","DOI":"10.48550/arXiv.2303.06614","CorpusId":257495808},"title":"Synthetic Experience Replay"},{"paperId":"bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2","externalIds":{"DBLP":"journals/ijrr/ChiXFCDBTS25","ArXiv":"2303.04137","DOI":"10.1177/02783649241273668","CorpusId":257378658},"title":"Diffusion policy: Visuomotor policy learning via action diffusion"},{"paperId":"a45120170be3abf48cc872e95e2d2980c8006b96","externalIds":{"DBLP":"conf/rss/YonedaS0SW23","ArXiv":"2302.12244","DOI":"10.48550/arXiv.2302.12244","CorpusId":257102810},"title":"To the Noise and Back: Diffusion for Shared Autonomy"},{"paperId":"e701e4c02a32da186d25b08373ada12d83b73b3d","externalIds":{"DBLP":"conf/rss/YuXTSWBSTMPHIX23","ArXiv":"2302.11550","DOI":"10.48550/arXiv.2302.11550","CorpusId":257079001},"title":"Scaling Robot Learning with Semantically Imagined Experience"},{"paperId":"a513a9a0a99a168c78edac6b53ef10f2e68819cf","externalIds":{"DBLP":"conf/icml/LiuGYC0ZZ23","ArXiv":"2302.07351","DOI":"10.48550/arXiv.2302.07351","CorpusId":256868643},"title":"Constrained Decision Transformer for Offline Safe Reinforcement Learning"},{"paperId":"9a01fc428d195a9c5ea2005dc2943a650d59aa76","externalIds":{"DBLP":"journals/corr/abs-2302-06671","ArXiv":"2302.06671","DOI":"10.48550/arXiv.2302.06671","CorpusId":256846801},"title":"GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"},{"paperId":"0816ee42aa1c84ad929bbfd46dd6d41d8e9b9a19","externalIds":{"DBLP":"journals/corr/abs-2302-01877","ArXiv":"2302.01877","DOI":"10.48550/arXiv.2302.01877","CorpusId":256598386},"title":"AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners"},{"paperId":"da2fe6cd385194b0274d04d04ee72e8caf3854d4","externalIds":{"DBLP":"journals/corr/abs-2302-00111","ArXiv":"2302.00111","DOI":"10.48550/arXiv.2302.00111","CorpusId":256459809},"title":"Learning Universal Policies via Text-Guided Video Generation"},{"paperId":"fcf0cbae0d4986fb33b201f99426723a437d16e7","externalIds":{"DBLP":"journals/corr/abs-2301-12203","ArXiv":"2301.12203","DOI":"10.48550/arXiv.2301.12203","CorpusId":256390497},"title":"SaFormer: A Conditional Sequence Modeling Approach to Offline Safe Reinforcement Learning"},{"paperId":"b43330013a5abcccd366d71f2f66c493c790abc6","externalIds":{"DBLP":"journals/corr/abs-2301-10677","ArXiv":"2301.10677","DOI":"10.48550/arXiv.2301.10677","CorpusId":256231177},"title":"Imitating Human Behaviour with Diffusion Models"},{"paperId":"5af8c7c650e9ec50d91a16be287ce54b16075fe7","externalIds":{"DBLP":"journals/ftml/BeckVLXZFW25","ArXiv":"2301.08028","DOI":"10.1561/2200000080","CorpusId":255999767},"title":"A Tutorial on Meta-Reinforcement Learning"},{"paperId":"638b5c76d96e32f54475a8327a9c68e0167156a9","externalIds":{"DBLP":"journals/corr/abs-2301-03044","ArXiv":"2301.03044","DOI":"10.48550/arXiv.2301.03044","CorpusId":255546304},"title":"A Survey on Transformers in Reinforcement Learning"},{"paperId":"7694f004c67840d7f098b3612d4b3dabd915c116","externalIds":{"DBLP":"journals/corr/abs-2212-04048","ArXiv":"2212.04048","DOI":"10.1109/CVPR52729.2023.01726","CorpusId":254408910},"title":"Executing your Commands via Motion Diffusion in Latent Space"},{"paperId":"dbdfd1623586009305a3e4965bf2c233a46aea5a","externalIds":{"DBLP":"journals/corr/abs-2212-04495","ArXiv":"2212.04495","DOI":"10.1109/CVPR52729.2023.00941","CorpusId":254408793},"title":"MoFusion: A Framework for Denoising-Diffusion-Based Motion Synthesis"},{"paperId":"f19dfc360088922cf1d423c538662aae8d542c28","externalIds":{"DBLP":"journals/corr/abs-2211-15657","ArXiv":"2211.15657","DOI":"10.48550/arXiv.2211.15657","CorpusId":254044710},"title":"Is Conditional Generative Modeling all you need for Decision-Making?"},{"paperId":"75f7e9e2b59fb640ef9d1dff94097175daf46c4d","externalIds":{"ArXiv":"2211.08411","DBLP":"journals/corr/abs-2211-08411","CorpusId":253522998},"title":"Large Language Models Struggle to Learn Long-Tail Knowledge"},{"paperId":"f143659c51896efe0aa59c339e591a7bf4bd5e53","externalIds":{"DBLP":"conf/iclr/ZhangLHW024","ArXiv":"2210.15629","CorpusId":258059908},"title":"Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks"},{"paperId":"4fd4e392fb39124744bdfbb6d71ae2030be5132e","externalIds":{"DBLP":"journals/corr/abs-2210-02438","ArXiv":"2210.02438","DOI":"10.1109/LRA.2023.3272516","CorpusId":252715865},"title":"DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","externalIds":{"DBLP":"journals/corr/abs-2210-02303","ArXiv":"2210.02303","DOI":"10.48550/arXiv.2210.02303","CorpusId":252715883},"title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"d3bc75b2fb8bebef156d425ea46cab5a30904b7c","externalIds":{"DBLP":"journals/corr/abs-2209-15256","ArXiv":"2209.15256","DOI":"10.48550/arXiv.2209.15256","CorpusId":252668538},"title":"S2P: State-conditioned Image Synthesis for Data Augmentation in Offline Reinforcement Learning"},{"paperId":"0f1402c536cc3cbbcb73b06f96289e50a34ca3cf","externalIds":{"DBLP":"conf/iclr/Chen0Y0023","ArXiv":"2209.14548","DOI":"10.48550/arXiv.2209.14548","CorpusId":252596208},"title":"Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling"},{"paperId":"1bbf99b5bfe9869876ac3bdd2999e16b2632c283","externalIds":{"ArXiv":"2208.15001","DBLP":"journals/pami/ZhangCPHGYL24","DOI":"10.1109/TPAMI.2024.3355414","CorpusId":251953565,"PubMed":"38285589"},"title":"MotionDiffuse: Text-Driven Human Motion Generation With Diffusion Model"},{"paperId":"2cbea7615ebecea2c414d8fbad47d5d258a5c3b4","externalIds":{"DBLP":"conf/iclr/WangHZ23","ArXiv":"2208.06193","DOI":"10.48550/arXiv.2208.06193","CorpusId":251554821},"title":"Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning"},{"paperId":"af9f365ed86614c800f082bd8eb14be76072ad16","externalIds":{"DBLP":"journals/corr/abs-2207-12598","ArXiv":"2207.12598","DOI":"10.48550/arXiv.2207.12598","CorpusId":249145348},"title":"Classifier-Free Diffusion Guidance"},{"paperId":"8e9d84a7b2db57adda8d639c6d54c8977ef10761","externalIds":{"DBLP":"conf/corl/ShiLL22","ArXiv":"2207.07560","DOI":"10.48550/arXiv.2207.07560","CorpusId":250607756},"title":"Skill-based Model-based Reinforcement Learning"},{"paperId":"b6b6bc529e665ebf97326d084a71159634ae10a7","externalIds":{"DBLP":"journals/chinaf/LuoXLCZY24","ArXiv":"2206.09328","DOI":"10.1007/s11432-022-3696-5","CorpusId":249889734},"title":"A survey on model-based reinforcement learning"},{"paperId":"7e045a7fe78a6c0de5511980f292c42d1055f396","externalIds":{"DBLP":"journals/corr/abs-2206-04745","ArXiv":"2206.04745","DOI":"10.48550/arXiv.2206.04745","CorpusId":249605389},"title":"Mildly Conservative Q-Learning for Offline Reinforcement Learning"},{"paperId":"4530c25da949bb2185c50663158ef19d52e3c6b5","externalIds":{"DBLP":"conf/nips/0011ZB0L022","ArXiv":"2206.00927","DOI":"10.48550/arXiv.2206.00927","CorpusId":249282317},"title":"DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps"},{"paperId":"51606f776a3b53a08d5a7e726f9ad771d52e2506","externalIds":{"ArXiv":"2205.15241","DBLP":"conf/nips/LeeNYLFGFXJMM22","DOI":"10.48550/arXiv.2205.15241","CorpusId":249192343},"title":"Multi-Game Decision Transformers"},{"paperId":"fe26607ca95c0d1d9005810b4ad12845ee69e9cf","externalIds":{"DBLP":"conf/nips/WenKL000022","ArXiv":"2205.14953","DOI":"10.48550/arXiv.2205.14953","CorpusId":249192420},"title":"Multi-Agent Reinforcement Learning is a Sequence Modeling Problem"},{"paperId":"1386b8a11929cf02da291c56aca353e33bbc22ed","externalIds":{"DBLP":"conf/nips/LiTGLH22","ArXiv":"2205.14217","DOI":"10.48550/arXiv.2205.14217","CorpusId":249192356},"title":"Diffusion-LM Improves Controllable Text Generation"},{"paperId":"3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e","externalIds":{"ArXiv":"2205.09991","DBLP":"journals/corr/abs-2205-09991","DOI":"10.48550/arXiv.2205.09991","CorpusId":248965046},"title":"Planning with Diffusion for Flexible Behavior Synthesis"},{"paperId":"9c5f056c4e7986064722bb522e46e3546be8da51","externalIds":{"ArXiv":"2205.10330","DBLP":"journals/corr/abs-2205-10330","DOI":"10.48550/arXiv.2205.10330","CorpusId":248965265},"title":"A Review of Safe Reinforcement Learning: Methods, Theory and Applications"},{"paperId":"5922f437512158970c417f4413bface021df5f78","externalIds":{"DBLP":"journals/corr/abs-2205-06175","ArXiv":"2205.06175","DOI":"10.48550/arXiv.2205.06175","CorpusId":248722148},"title":"A Generalist Agent"},{"paperId":"23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7","externalIds":{"DBLP":"journals/corr/abs-2204-11828","ArXiv":"2204.11828","DOI":"10.48550/arXiv.2204.11828","CorpusId":248377629},"title":"Skill-based Meta-Reinforcement Learning"},{"paperId":"a225d5d846ba5110232ed5bb32d54ea742b1c2d4","externalIds":{"DBLP":"conf/iclr/SheyninAPSGNT23","ArXiv":"2204.02849","DOI":"10.48550/arXiv.2204.02849","CorpusId":247996596},"title":"KNN-Diffusion: Image Generation via Large-Scale Retrieval"},{"paperId":"c871d2dc802d276608a6734637f8bc9e6da0d837","externalIds":{"DBLP":"journals/corr/abs-2203-02923","ArXiv":"2203.02923","DOI":"10.48550/arXiv.2203.02923","CorpusId":247292764},"title":"GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation"},{"paperId":"fda6c22149d269cb2730e919ed4254a5a5f84ae0","externalIds":{"DBLP":"conf/icml/SootlaCJWMWA22","ArXiv":"2202.06558","CorpusId":246822687},"title":"SAUTE RL: Almost Surely Safe Reinforcement Learning Using State Augmentation"},{"paperId":"1e91fa21b890a8f5d615578f4ddf46c3cb394691","externalIds":{"ArXiv":"2201.09865","DBLP":"journals/corr/abs-2201-09865","DOI":"10.1109/CVPR52688.2022.01117","CorpusId":246240274},"title":"RePaint: Inpainting using Denoising Diffusion Probabilistic Models"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"2db0fbdb68919bbaccdd67a610a4e30c41fc41d6","externalIds":{"ArXiv":"2112.02845","DBLP":"journals/corr/abs-2112-02845","CorpusId":245335360},"title":"Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence Model Tackles All SMAC Tasks"},{"paperId":"b7e531104c95eb67ce6c20d6dc1318e8bf837bf8","externalIds":{"DBLP":"journals/corr/abs-2110-14048","ArXiv":"2110.14048","CorpusId":239998731},"title":"Conflict-Averse Gradient Descent for Multi-task Learning"},{"paperId":"91b32fc0a23f0af53229fceaae9cce43a0406d2e","externalIds":{"DBLP":"journals/corr/abs-2107-03006","ArXiv":"2107.03006","CorpusId":235755106},"title":"Structured Denoising Diffusion Models in Discrete State-Spaces"},{"paperId":"806aac1f7bf886988ea2bce8461f45a09db575b6","externalIds":{"ArXiv":"2106.08417","CorpusId":238419530},"title":"Scene Transformer: A unified architecture for predicting multiple agent trajectories"},{"paperId":"c879b25308026d6538e52b27bcf4fd3cb60855f3","externalIds":{"DBLP":"conf/nips/FujimotoG21","ArXiv":"2106.06860","CorpusId":235422620},"title":"A Minimalist Approach to Offline Reinforcement Learning"},{"paperId":"95f5bafba97beb9b4f8c1fe607f04ec28efab7f9","externalIds":{"DBLP":"journals/corr/abs-2106-03802","ArXiv":"2106.03802","CorpusId":235363972},"title":"Learning to Efficiently Sample from Diffusion Probabilistic Models"},{"paperId":"c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500","externalIds":{"DBLP":"journals/corr/abs-2106-01345","MAG":"3169291081","ArXiv":"2106.01345","CorpusId":235294299},"title":"Decision Transformer: Reinforcement Learning via Sequence Modeling"},{"paperId":"59a92066b7e59f63413f46b3bfe551f42f6c80b4","externalIds":{"DBLP":"conf/ijcai/ZhuLSZC0Y0FY21","ArXiv":"2105.06350","DOI":"10.24963/ijcai.2021/480","CorpusId":234482728},"title":"MapGo: Model-Assisted Policy Optimization for Goal-Oriented Tasks"},{"paperId":"64ea8f180d0682e6c18d1eb688afdb2027c02794","externalIds":{"ArXiv":"2105.05233","DBLP":"journals/corr/abs-2105-05233","CorpusId":234357997},"title":"Diffusion Models Beat GANs on Image Synthesis"},{"paperId":"89cc5d8b9c1579d5a5ba905cfc95c907428a7eb4","externalIds":{"DBLP":"journals/corr/abs-2104-02321","ArXiv":"2104.02321","DOI":"10.21437/Interspeech.2021-36","CorpusId":233033417},"title":"NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling"},{"paperId":"362cc80481b288874af0428107ab31e955dcf09f","externalIds":{"DBLP":"journals/corr/abs-2103-08050","ArXiv":"2103.08050","CorpusId":232233412},"title":"Offline Reinforcement Learning with Fisher Divergence Critic Regularization"},{"paperId":"30256fd41d471e8c6731c732e41ba865321ced7d","externalIds":{"DBLP":"journals/corr/abs-2103-06326","ArXiv":"2103.06326","CorpusId":232185386},"title":"S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning"},{"paperId":"c32fd8ea1b3f2df410410fb18d569dede102c53a","externalIds":{"DBLP":"journals/corr/abs-2103-01458","ArXiv":"2103.01458","DOI":"10.1109/CVPR46437.2021.00286","CorpusId":232092778},"title":"Diffusion Probabilistic Models for 3D Point Cloud Generation"},{"paperId":"8159f3f1bc1155ff56bed7fea38d052c4524108b","externalIds":{"DBLP":"conf/vr/BhattacharyaRBG21","ArXiv":"2101.11101","DOI":"10.1109/VR50410.2021.00037","CorpusId":231718750},"title":"Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents**This work has been supported in part by ARO Grants W911NF1910069 and W911NF1910315, and Intel. Code and additional materials available at: https://gamma.umd.edu/t2g"},{"paperId":"633e2fbfc0b21e959a244100937c5853afca4853","externalIds":{"DBLP":"journals/corr/abs-2011-13456","ArXiv":"2011.13456","MAG":"3110257065","CorpusId":227209335},"title":"Score-Based Generative Modeling through Stochastic Differential Equations"},{"paperId":"014576b866078524286802b1d0e18628520aa886","externalIds":{"ArXiv":"2010.02502","DBLP":"journals/corr/abs-2010-02502","MAG":"3092442149","CorpusId":222140788},"title":"Denoising Diffusion Implicit Models"},{"paperId":"34bf13e58c7226d615afead0c0f679432502940e","externalIds":{"MAG":"3087665158","DBLP":"conf/iclr/KongPHZC21","ArXiv":"2009.09761","CorpusId":221818900},"title":"DiffWave: A Versatile Diffusion Model for Audio Synthesis"},{"paperId":"ea63cca868cc792d4c0d20120f244f866d9aa752","externalIds":{"MAG":"3081310128","DOI":"10.3390/electronics9091363","CorpusId":225277420},"title":"A Survey of Multi-Task Deep Reinforcement Learning"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"28db20a81eec74a50204686c3cf796c42a020d2e","externalIds":{"ArXiv":"2006.04779","DBLP":"journals/corr/abs-2006-04779","MAG":"3102848167","CorpusId":219530894},"title":"Conservative Q-Learning for Offline Reinforcement Learning"},{"paperId":"61b72a01a4a8a5454f13cee7cf94e209f3be7229","externalIds":{"MAG":"3032044946","DBLP":"journals/corr/abs-2005-14171","ArXiv":"2005.14171","DOI":"10.1145/3397271.3401440","CorpusId":218971639},"title":"User Behavior Retrieval for Click-Through Rate Prediction"},{"paperId":"5e7bc93622416f14e6948a500278bfbe58cd3890","externalIds":{"DBLP":"journals/corr/abs-2005-01643","ArXiv":"2005.01643","MAG":"3022566517","CorpusId":218486979},"title":"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"},{"paperId":"744139d65c3bf6da6a6acd384a32d94a06f44f62","externalIds":{"MAG":"3021708257","DBLP":"conf/nips/LaskinLSPAS20","ArXiv":"2004.14990","CorpusId":216868834},"title":"Reinforcement Learning with Augmented Data"},{"paperId":"129983331ca874142a3e8eb2d93d820bdf1f9aca","externalIds":{"MAG":"3127561923","DBLP":"journals/tits/KiranSTMSYP22","ArXiv":"2002.00444","DOI":"10.1109/TITS.2021.3054625","CorpusId":211011033},"title":"Deep Reinforcement Learning for Autonomous Driving: A Survey"},{"paperId":"449c5660d637741f7aa7ff42549c32b43c9968bf","externalIds":{"DBLP":"journals/corr/abs-2001-06782","MAG":"2997359900","ArXiv":"2001.06782","CorpusId":210839011},"title":"Gradient Surgery for Multi-Task Learning"},{"paperId":"7a7a7847041e7b25febb1491d65d842a6c65927e","externalIds":{"MAG":"2992155277","DBLP":"journals/corr/abs-1912-02877","ArXiv":"1912.02877","CorpusId":208857468},"title":"Training Agents using Upside-Down Reinforcement Learning"},{"paperId":"72973e49f453f678eb0b79b5fa5311b158f3909d","externalIds":{"ArXiv":"1912.02875","MAG":"3149148583","DBLP":"journals/corr/abs-1912-02875","CorpusId":208857600},"title":"Reinforcement Learning Upside Down: Don't Predict Rewards - Just Map Them to Actions"},{"paperId":"27d78a26ddb9698b9cefcf6cdeafa4f834466103","externalIds":{"DBLP":"journals/corr/abs-1912-02074","ArXiv":"1912.02074","MAG":"2993185773","CorpusId":208617840},"title":"AlgaeDICE: Policy Gradient from Arbitrary Experience"},{"paperId":"c39fb7a46335c23f7529dd6f9f980462fd38653a","externalIds":{"DBLP":"journals/nature/SchrittwieserAH20","ArXiv":"1911.08265","MAG":"2989847975","DOI":"10.1038/s41586-020-03051-4","CorpusId":208158225,"PubMed":"33361790"},"title":"Mastering Atari, Go, chess and shogi by planning with a learned model"},{"paperId":"9be492858863c8c7c24be1ecb75724de5086bd8e","externalIds":{"DBLP":"journals/corr/abs-1911-11361","ArXiv":"1911.11361","MAG":"2991355586","CorpusId":208291277},"title":"Behavior Regularized Offline Reinforcement Learning"},{"paperId":"5d5e01193e36a77c836fe8ee3ebb13d57520d050","externalIds":{"ArXiv":"1912.11206","MAG":"2998562709","DBLP":"journals/corr/abs-1912-11206","CorpusId":208540318},"title":"Learning to Combat Compounding-Error in Model-Based Reinforcement Learning"},{"paperId":"8c11f3676c33eaf9bfc2801b93d531cf6cbcaa28","externalIds":{"DBLP":"reference/algo/Even-Dar08","ArXiv":"2405.10369","DOI":"10.4249/scholarpedia.1448","CorpusId":2535201},"title":"Reinforcement learning"},{"paperId":"81664382e5db10bc6598db0b8814a8e765d30576","externalIds":{"ArXiv":"1905.07088","DBLP":"journals/corr/abs-1905-07088","MAG":"2964378242","CorpusId":158047026},"title":"Sliced Score Matching: A Scalable Approach to Density and Score Estimation"},{"paperId":"5285cb8faada5de8a92a47622950f6cfd476ac1d","externalIds":{"DBLP":"journals/corr/abs-1812-02900","ArXiv":"1812.02900","MAG":"2904453761","CorpusId":54457299},"title":"Off-Policy Deep Reinforcement Learning without Exploration"},{"paperId":"cb7c479a36520da1caeeec67db10772351a390c6","externalIds":{"MAG":"2949576910","ArXiv":"1805.11074","DBLP":"conf/iclr/TesslerMM19","CorpusId":44095973},"title":"Reward Constrained Policy Optimization"},{"paperId":"811df72e210e20de99719539505da54762a11c6d","externalIds":{"MAG":"2962902376","ArXiv":"1801.01290","DBLP":"journals/corr/abs-1801-01290","CorpusId":28202810},"title":"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"},{"paperId":"498238a3bd5fd322fc3ce1572e33bbe3853a356f","externalIds":{"ArXiv":"1708.05866","DBLP":"journals/corr/abs-1708-05866","MAG":"3100789280","DOI":"10.1109/MSP.2017.2743240","CorpusId":4884302},"title":"Deep Reinforcement Learning: A Brief Survey"},{"paperId":"cce22bf6405042a965a86557684c46a441f2a736","externalIds":{"DBLP":"journals/corr/abs-1708-02596","MAG":"2962872206","ArXiv":"1708.02596","DOI":"10.1109/ICRA.2018.8463189","CorpusId":206853161},"title":"Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"30834ae1497c35d362eea14857d93c28d2d12b57","externalIds":{"MAG":"2620290674","ArXiv":"1706.05064","DBLP":"conf/icml/OhSLK17","CorpusId":11974467},"title":"Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning"},{"paperId":"7a4193d0b042643a8bb9ec262ed7f9d509bdb12e","externalIds":{"MAG":"2952192457","ArXiv":"1705.10528","DBLP":"journals/corr/AchiamHTA17","CorpusId":10647707},"title":"Constrained Policy Optimization"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"5f5dc5b9a2ba710937e2c413b37b053cd673df02","externalIds":{"DBLP":"journals/corr/KingmaW13","MAG":"2951004968","ArXiv":"1312.6114","CorpusId":216078090},"title":"Auto-Encoding Variational Bayes"},{"paperId":"2319a491378867c7049b3da055c5df60e1671158","externalIds":{"DBLP":"journals/corr/MnihKSGAWR13","MAG":"1757796397","ArXiv":"1312.5602","CorpusId":15238391},"title":"Playing Atari with Deep Reinforcement Learning"},{"paperId":"65438e0ba226c1f97bd8a36333ebc3297b1a32fd","externalIds":{"DBLP":"journals/ijrr/KoberBP13","MAG":"1977655452","DOI":"10.1177/0278364913495721","CorpusId":1932843},"title":"Reinforcement learning in robotics: A survey"},{"paperId":"3b0821ff22fdffc95b0caae1f9660773eb54dc52","externalIds":{"ArXiv":"1206.1901","MAG":"621546036","DOI":"10.1201/b10905","CorpusId":1048042},"title":"Handbook of Markov Chain Monte Carlo"},{"paperId":"a200ce9fa37a613348997017bb6b12c4f1780df9","externalIds":{"MAG":"1968512837","DOI":"10.1137/0910062","CorpusId":55374658},"title":"VODE: a variable-coefficient ODE solver"},{"paperId":"df1c1cb4a547b9b207613d81765d4693dde9590b","externalIds":{"MAG":"2144144709","DOI":"10.1016/0771-050X(80)90013-3","CorpusId":122754533},"title":"A family of embedded Runge-Kutta formulae"},{"paperId":"03acfc83fa4d5c0662de6a32eff3c45ce99d7aec","externalIds":{"DOI":"10.1093/bjps/X.37.1","CorpusId":4194818},"title":"SPACE*"},{"paperId":"bd2755fd96e20183f642628f818d7d161d3d0f0b","externalIds":{"DBLP":"conf/icml/LiW0Z23","CorpusId":260859191},"title":"Hierarchical Diffusion for Offline Decision Making"},{"paperId":"c36f3635e090aba84e5e83b904a7697e83730be6","externalIds":{"DBLP":"conf/corl/XianGGKF23","CorpusId":264423725},"title":"ChainedDiffuser: Unifying Trajectory Diffusion and Keypose Prediction for Robotic Manipulation"},{"paperId":"9a21003ba1334e2fa7fc48178f37e02027e91936","externalIds":{"DBLP":"conf/iclr/TaigaAFCB23","CorpusId":259298247},"title":"Investigating Multi-task Pretraining and Generalization in Reinforcement Learning"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"5a6b2b9bc3b51ff187826fc2dc21a967e04125ed","externalIds":{"MAG":"2923762280","CorpusId":108339287},"title":"Model-based reinforcement learning: A survey"},{"paperId":"c0f2c4104ef6e36bb67022001179887e6600d24d","externalIds":{"DBLP":"journals/jmlr/GarciaF15","MAG":"1845972764","DOI":"10.5555/2789272.2886795","CorpusId":2497153},"title":"A comprehensive survey on safe reinforcement learning"},{"paperId":"97efafdb4a3942ab3efba53ded7413199f79c054","externalIds":{"MAG":"2121863487","DBLP":"journals/tnn/SuttonB98","DOI":"10.1109/TNN.1998.712192","CorpusId":60035920},"title":"Reinforcement Learning: An Introduction"}]}