{"paperId":"396ea10d3ab89da41d02693d7165c4b98ecbb5f3","externalIds":{"ArXiv":"2402.11291","DBLP":"journals/corr/abs-2402-11291","ACL":"2024.emnlp-main.646","DOI":"10.18653/v1/2024.emnlp-main.646","CorpusId":267751102},"title":"Puzzle Solving using Reasoning of Large Language Models: A Survey","openAccessPdf":{"url":"http://arxiv.org/pdf/2402.11291","status":"GREEN","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2402.11291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2284693742","name":"Panagiotis Giadikiaroglou"},{"authorId":"2184294391","name":"Maria Lymperaiou"},{"authorId":"2080432906","name":"Giorgos Filandrianos"},{"authorId":"1719165","name":"G. Stamou"}],"abstract":"Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy—dividing puzzles into rule-based and rule-less categories—to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs’ performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs’ puzzle-solving proficiency and contribute to AI’s logical reasoning and creative problem-solving advancements."}