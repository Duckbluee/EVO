{"abstract":"Graphs with abundant attributes are essential in modeling interconnected entities and enhancing predictions across various real-world applications. Traditional Graph Neural Networks (GNNs) often require re-training for different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced new paradigms in natural language processing, their potential for generic graph mining—training a single model to simultaneously handle diverse tasks and datasets—remains under-explored. To this end, our novel framework <inline-formula><tex-math notation=\"LaTeX\">${\\sf MuseGraph}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"sans-serif\">MuseGraph</mml:mi></mml:math><inline-graphic xlink:href=\"yang-ieq1-3603062.gif\"/></alternatives></inline-formula>, seamlessly integrates the strengths of GNNs and LLMs into one foundation model for graph mining across tasks and datasets. This framework first features a compact graph description to encapsulate key graph information within language token limitations. Then, we propose a diverse instruction generation mechanism with Chain-of-Thought (CoT)-based instruction packages to distill the reasoning capabilities from advanced LLMs like GPT-4. Finally, we design a graph-aware instruction tuning strategy to facilitate mutual enhancement across multiple tasks and datasets while preventing catastrophic forgetting of LLMs’ generative abilities. Our experimental results demonstrate significant improvements in five graph tasks and ten datasets, showcasing the potential of our <inline-formula><tex-math notation=\"LaTeX\">${\\sf MuseGraph}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"sans-serif\">MuseGraph</mml:mi></mml:math><inline-graphic xlink:href=\"yang-ieq2-3603062.gif\"/></alternatives></inline-formula> in enhancing the accuracy of graph-oriented downstream tasks while improving the generation abilities of LLMs."}