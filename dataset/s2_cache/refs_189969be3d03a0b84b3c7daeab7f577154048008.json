{"references":[{"paperId":"3c137fb01b613ca7d4919b569abb56491378357e","externalIds":{"DOI":"10.30525/2661-5150/2024-4-3","CorpusId":276143719},"title":"OVERCOMING BARRIERS TO ARTIFICIAL INTELLIGENCE ADOPTION"},{"paperId":"5b48f9b2f26caac836ca6baa2dd806a4d11556b6","externalIds":{"DOI":"10.1109/TTE.2023.3347278","CorpusId":266589623},"title":"An Explainable and Robust Motion Planning and Control Approach for Autonomous Vehicle On-Ramping Merging Task Using Deep Reinforcement Learning"},{"paperId":"8061fc542f45b583e19105c8b775c663893419c4","externalIds":{"DBLP":"journals/corr/abs-2406-17818","ArXiv":"2406.17818","DOI":"10.1145/3637528.3671790","CorpusId":270738170},"title":"Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks"},{"paperId":"a3603df2514a49c0bdad9f3c532fba5fa25d9792","externalIds":{"ArXiv":"2403.07262","DBLP":"conf/nips/Qing0CCZS24","DOI":"10.52202/079017-0915","CorpusId":268363578},"title":"A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective"},{"paperId":"b87d84f46ab66356300f9911a73f8d662fe936f5","externalIds":{"DBLP":"journals/corr/abs-2405-15831","ArXiv":"2405.15831","DOI":"10.1109/TPWRS.2023.3298007","CorpusId":260141545},"title":"Transmission Interface Power Flow Adjustment: A Deep Reinforcement Learning Approach Based on Multi-Task Attribution Map"},{"paperId":"090df35541a06bc9b10ac018704377ed1ea89132","externalIds":{"DBLP":"journals/eswa/LiuZSBGC24","DOI":"10.1016/j.eswa.2023.121070","CorpusId":260996378},"title":"Progressive decision-making framework for power system topology control"},{"paperId":"0c10ffe2630ea66f2644aa0c6cd24feef174f032","externalIds":{"DBLP":"journals/tfs/OuCWL24","DOI":"10.1109/TFUZZ.2023.3295055","CorpusId":259862912},"title":"Fuzzy Centered Explainable Network for Reinforcement Learning"},{"paperId":"66766682c5669964ec6405e0acd9ca228024c5ab","externalIds":{"DBLP":"journals/corr/abs-2312-11118","ArXiv":"2312.11118","DOI":"10.48550/arXiv.2312.11118","CorpusId":266359245},"title":"Explaining Reinforcement Learning Agents Through Counterfactual Action Outcomes"},{"paperId":"9d6ad3fc3d0575152fb755722d8e76ea512b9113","externalIds":{"DBLP":"conf/nips/LeeKC23","ArXiv":"2310.19427","DOI":"10.48550/arXiv.2310.19427","CorpusId":264796693},"title":"Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans"},{"paperId":"9e3079df985f3464bd3a49cce10550ed49fcbf78","externalIds":{"ArXiv":"2310.07747","DBLP":"journals/corr/abs-2310-07747","DOI":"10.48550/arXiv.2310.07747","CorpusId":263909144},"title":"Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples"},{"paperId":"cf39ec7fde9497a5f629980a2becb061037b8af8","externalIds":{"ArXiv":"2309.06097","DBLP":"journals/corr/abs-2309-06097","DOI":"10.48550/arXiv.2309.06097","CorpusId":261696606},"title":"Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning"},{"paperId":"3ccc1fbef008bba91b7f1783388cbc4878190031","externalIds":{"DBLP":"journals/csur/MilaniTVF24","DOI":"10.1145/3616864","CorpusId":261124739},"title":"Explainable Reinforcement Learning: A Survey and Comparative Review"},{"paperId":"bd120c6c642022683b7148bb83113f734ba140bb","externalIds":{"DBLP":"journals/corr/abs-2306-16803","ArXiv":"2306.16803","DOI":"10.48550/arXiv.2306.16803","CorpusId":259287553},"title":"Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis"},{"paperId":"c7a7b63cc3f257f718525a563a6817b99f788d70","externalIds":{"ArXiv":"2306.05810","DBLP":"conf/icml/BeecheySS23","DOI":"10.48550/arXiv.2306.05810","CorpusId":259129482},"title":"Explaining Reinforcement Learning with Shapley Values"},{"paperId":"ac0cd2cfb7b663cc94e9db9d2d9b790ccb644b1a","externalIds":{"ArXiv":"2306.01439","DBLP":"conf/nips/DelfosseSDK23","DOI":"10.48550/arXiv.2306.01439","CorpusId":259063835},"title":"Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction"},{"paperId":"6490bdb932d682ef2c03907800fb8bbc965540f7","externalIds":{"DBLP":"journals/corr/abs-2305-17352","ArXiv":"2305.17352","DOI":"10.48550/arXiv.2305.17352","CorpusId":258960210},"title":"Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?"},{"paperId":"adec78228d27dae140da94e454294a552e22fcd4","externalIds":{"DBLP":"conf/ijcai/BoggessK023","ArXiv":"2305.10378","DOI":"10.48550/arXiv.2305.10378","CorpusId":258741003},"title":"Explainable Multi-Agent Reinforcement Learning for Temporal Queries"},{"paperId":"c3a6432b582d2c3bbffa3c8f69276f9b42939829","externalIds":{"ArXiv":"2305.04073","DBLP":"conf/iclr/DeshmukhDKJATS23","DOI":"10.48550/arXiv.2305.04073","CorpusId":258557232},"title":"Explaining RL Decisions with Trajectories"},{"paperId":"c6773fc39a83dd1512016571f486e7012baaf9ea","externalIds":{"ArXiv":"2305.02749","DBLP":"conf/ijcai/YuRX23","DOI":"10.48550/arXiv.2305.02749","CorpusId":258479970},"title":"Explainable Reinforcement Learning via a Causal World Model"},{"paperId":"70f1e2291235c53e5676512964976d3993f46569","externalIds":{"DBLP":"journals/corr/abs-2304-11632","ArXiv":"2304.11632","DOI":"10.48550/arXiv.2304.11632","CorpusId":258298063},"title":"Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective"},{"paperId":"017844bd5429a27c46b434903dc0e60ebf215f90","externalIds":{"ArXiv":"2302.00965","DBLP":"conf/iclr/LiuH0YX23","DOI":"10.48550/arXiv.2302.00965","CorpusId":256503575},"title":"Visual Imitation Learning with Patch Rewards"},{"paperId":"344868b71c5beeb4ba86a746520bb5a5b49bea3d","externalIds":{"DBLP":"journals/ml/ZhangK24","DOI":"10.1007/s10994-022-06295-5","CorpusId":255656748},"title":"Learning state importance for preference-based reinforcement learning"},{"paperId":"e72c1c09c7992e7ffaeb652693955eadad522705","externalIds":{"DBLP":"journals/corr/abs-2211-12712","ArXiv":"2211.12712","DOI":"10.48550/arXiv.2211.12712","CorpusId":253801628},"title":"Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition"},{"paperId":"bf97bc56fb795bef14a33f138232883f637c2a2a","externalIds":{"DBLP":"journals/corr/abs-2211-03162","ArXiv":"2211.03162","DOI":"10.48550/arXiv.2211.03162","CorpusId":253383734},"title":"ProtoX: Explaining a Reinforcement Learning Agent via Prototyping"},{"paperId":"4580b0e3ede1b7ad97fb742fa28cd3ebe3bed7a9","externalIds":{"DBLP":"journals/corr/abs-2210-00066","ArXiv":"2210.00066","DOI":"10.48550/arXiv.2210.00066","CorpusId":252683531},"title":"Improving Policy Learning via Language Dynamics Distillation"},{"paperId":"261c0be2ad10b605d39e0d13d3b28d11d171b4d4","externalIds":{"DBLP":"conf/nips/BertoinZZR22","ArXiv":"2209.09203","CorpusId":256664361},"title":"Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning"},{"paperId":"44c19ed8c8fd70d72155802257453f002af75932","externalIds":{"DBLP":"conf/aaai/JinMJZCY22","DOI":"10.1609/aaai.v36i6.20663","CorpusId":245335262},"title":"Creativity of AI: Automatic Symbolic Option Discovery for Facilitating Deep Reinforcement Learning"},{"paperId":"c4e93e5822806f9b2241ba3c735ab79bf969ca13","externalIds":{"DBLP":"journals/corr/abs-2205-12449","ArXiv":"2205.12449","DOI":"10.48550/arXiv.2205.12449","CorpusId":249063058},"title":"MAVIPER: Learning Decision Tree Policies for Interpretable Multi-Agent Reinforcement Learning"},{"paperId":"036b7c0546691eda0258945bf38cea2140c560f7","externalIds":{"DBLP":"journals/corr/abs-2204-12568","ArXiv":"2204.12568","DOI":"10.48550/arXiv.2204.12568","CorpusId":248405658},"title":"Toward Policy Explanations for Multi-Agent Reinforcement Learning"},{"paperId":"525614d6ddabff376949d4feb4a8c0a736c50673","externalIds":{"DOI":"10.1109/ICCAR55106.2022.9782671","CorpusId":249242747},"title":"Fuzzy Action-Masked Reinforcement Learning Behavior Planning for Highly Automated Driving"},{"paperId":"721d0ccdbbc67c7058db08f78c32003a28882b0a","externalIds":{"MAG":"3092297010","DBLP":"journals/tits/ChenHTC22","DOI":"10.1109/tits.2020.3025671","CorpusId":226598814},"title":"Conditional DQN-Based Motion Planning With Fuzzy Logic for Autonomous Driving"},{"paperId":"125800084a0ef022801be6c1740709c877f9a88a","externalIds":{"DBLP":"journals/tcss/ZhangZXGG22","DOI":"10.1109/tcss.2021.3096824","CorpusId":243276577},"title":"Explainable AI in Deep Reinforcement Learning Models for Power System Emergency Control"},{"paperId":"3e8fc7f9bc7ff938e3b8f557990cc1986b11a18e","externalIds":{"ArXiv":"2301.09937","DBLP":"journals/csur/Vouros23","DOI":"10.1145/3527448","CorpusId":247795043},"title":"Explainable Deep Reinforcement Learning: State of the Art and Challenges"},{"paperId":"5762e9c654ae9230db5936f21780ae794a838533","externalIds":{"DBLP":"journals/corr/abs-2204-09597","ACL":"2022.acl-long.41","ArXiv":"2204.09597","DOI":"10.48550/arXiv.2204.09597","CorpusId":248266815},"title":"Perceiving the World: Question-guided Reinforcement Learning for Text-based Games"},{"paperId":"d8bf2e2545777b835b37654f7ef1dd33eb2602e9","externalIds":{"DBLP":"journals/tvt/ChenZLWC22","DOI":"10.1109/tvt.2022.3143840","CorpusId":247478727},"title":"ES-DQN: A Learning Method for Vehicle Intelligent Speed Control Strategy Under Uncertain Cut-In Scenario"},{"paperId":"1952fc35b919f149a4cc2647c114b7038593391c","externalIds":{"DBLP":"journals/corr/abs-2203-00054","ArXiv":"2203.00054","DOI":"10.48550/arXiv.2203.00054","CorpusId":247187938},"title":"LISA: Learning Interpretable Skill Abstractions from Language"},{"paperId":"6aef611ce097c12d7f194bbbfa21af7c018f8c8e","externalIds":{"DBLP":"journals/corr/abs-2202-11797","ArXiv":"2202.11797","CorpusId":247084183},"title":"Training Characteristic Functions with Reinforcement Learning: XAI-methods play Connect Four"},{"paperId":"60f47eb09931260b43812602c636134e9f69f4f7","externalIds":{"ArXiv":"2202.03597","DBLP":"conf/aaai/LussDL23","DOI":"10.1609/aaai.v37i7.26081","CorpusId":246652458},"title":"Local Explanations for Reinforcement Learning"},{"paperId":"2f77942a18514c8369b800a8ca635fba7c00396f","externalIds":{"DBLP":"conf/iccel/VashistSGD22","DOI":"10.1109/ICCE53296.2022.9730182","CorpusId":247477318},"title":"DQN Based Exit Selection in Multi-Exit Deep Neural Networks for Applications Targeting Situation Awareness"},{"paperId":"aa427d6e317067ff5114a4192401fedd79a680f3","externalIds":{"DBLP":"journals/isci/Fernandez-Gauna22","DOI":"10.1016/j.ins.2022.01.047","CorpusId":246417978},"title":"Actor-critic continuous state reinforcement learning for wind-turbine control robust optimization"},{"paperId":"47707f8392ba5558b85c88dfdffc3a10798d9ae2","externalIds":{"DBLP":"journals/ml/GlanoisWZLYHL24","ArXiv":"2112.13112","DOI":"10.1007/s10994-024-06543-w","CorpusId":245502590},"title":"A survey on interpretable reinforcement learning"},{"paperId":"049cb2a029702540399f06fe2889dd1d1961a186","externalIds":{"DBLP":"journals/corr/abs-2112-08907","ArXiv":"2112.08907","CorpusId":245219242},"title":"Inherently Explainable Reinforcement Learning in Natural Language"},{"paperId":"68b71377f47aa42bdeae383d1f953ddd4ab04c2b","externalIds":{"MAG":"3193977437","DOI":"10.1016/J.ENERGY.2021.121377","CorpusId":238669141},"title":"Dynamic energy dispatch strategy for integrated energy system based on improved deep reinforcement learning"},{"paperId":"05482ebbd8ab8771b911b4c1b707bbed00d229c5","externalIds":{"DBLP":"journals/corr/abs-2111-03474","ArXiv":"2111.03474","DOI":"10.1145/3488560.3498494","CorpusId":243832892},"title":"Supervised Advantage Actor-Critic for Recommender Systems"},{"paperId":"92ad52b7604ac4f4c63acb992fef79ba14db267c","externalIds":{"DBLP":"journals/sp/ChraibiAE21","DOI":"10.1155/2021/7216795","CorpusId":239477671},"title":"Makespan Optimisation in Cloudlet Scheduling with Improved DQN Algorithm in Cloud Computing"},{"paperId":"dd5c6cf926a257cf8d74506f08c5797f902bf863","externalIds":{"DBLP":"journals/corr/abs-2110-01307","ArXiv":"2110.01307","DOI":"10.1109/MCI.2021.3129959","CorpusId":238260027},"title":"Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning With Shapley Values"},{"paperId":"1e8a8a53c0258d67ee7f0428717f5245098d33cd","externalIds":{"DBLP":"journals/isci/GongYLL22","DOI":"10.1016/j.ins.2021.10.031","CorpusId":241000139},"title":"Actor-critic with familiarity-based trajectory experience replay"},{"paperId":"a3ba49276cbf9a5c548fc8e7ae04476ce3588fe6","externalIds":{"DBLP":"conf/iccvw/JiangZ21","DOI":"10.1109/ICCVW54120.2021.00175","CorpusId":244531804},"title":"Explainable Face Recognition based on Accurate Facial Compositions"},{"paperId":"15cdd86ff69bb5c0d89f9bd066874530f8ab0c0d","externalIds":{"DBLP":"conf/nips/TangH21","ArXiv":"2109.02869","CorpusId":237431287},"title":"The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","externalIds":{"DBLP":"conf/nips/TrivediZSL21","ArXiv":"2108.13643","CorpusId":237365528},"title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"873f50a378866e0e1172aa30d43a20caf663f98c","externalIds":{"MAG":"3198173190","DBLP":"journals/mlc/ShiFHLHC22","DOI":"10.1007/s13042-021-01417-2","CorpusId":239627212},"title":"Efficient hierarchical policy network with fuzzy rules"},{"paperId":"0b3e119248286aeedc95330ab7b67999f522d574","externalIds":{"DBLP":"journals/nca/DazeleyVC23","ArXiv":"2108.09003","DOI":"10.1007/s00521-023-08423-1","CorpusId":237259799},"title":"Explainable reinforcement learning for broad-XAI: a conceptual framework and survey"},{"paperId":"f7db7f32af54f2f35973c01eea93acef3353b370","externalIds":{"MAG":"3196572190","DBLP":"journals/vcomm/ParkYP22","DOI":"10.1016/j.vehcom.2021.100397","CorpusId":239661332},"title":"Applying DQN solutions in fog-based vehicular networks: Scheduling, caching, and collision control"},{"paperId":"0eb33a1a6150b066ea063c61e539d5267a66764f","externalIds":{"DBLP":"journals/ijon/FrancoNDAO22","MAG":"3186927324","DOI":"10.1016/J.NEUCOM.2021.05.109","CorpusId":237668150},"title":"Deep fair models for complex data: Graphs labeling and explainable face recognition"},{"paperId":"b7b1619c6e80fd7f137d7b4879f7fc1f5491656f","externalIds":{"ArXiv":"2106.00285","DBLP":"journals/corr/abs-2106-00285","DOI":"10.1145/3447548.3467420","CorpusId":235266049},"title":"Shapley Counterfactual Credits for Multi-Agent Reinforcement Learning"},{"paperId":"bfbc532ef307c1ad2e7ff51d9dadcda57b6f1d7c","externalIds":{"DBLP":"journals/frai/WellsB21","PubMedCentral":"8172805","DOI":"10.3389/frai.2021.550030","CorpusId":234786291,"PubMed":"34095817"},"title":"Explainable AI and Reinforcement Learningâ€”A Systematic Review of Current Approaches and Trends"},{"paperId":"38537de90fda155cd10794c42dd6e14602b2fa77","externalIds":{"DBLP":"conf/aaai/WuKP21","DOI":"10.1609/aaai.v35i12.17235","CorpusId":235349100},"title":"Self-Supervised Attention-Aware Reinforcement Learning"},{"paperId":"5e3712e84884e28b41f6ca775db786b4c66976b4","externalIds":{"DBLP":"conf/aaai/SilvaG21","DOI":"10.1609/aaai.v35i6.16638","CorpusId":235306591},"title":"Encoding Human Domain Knowledge to Warm Start Reinforcement Learning"},{"paperId":"0e6338c992b6b72da05cb783f4d422ebf0462451","externalIds":{"DBLP":"conf/aaai/ArousDYBCC21","DOI":"10.1609/aaai.v35i7.16734","CorpusId":231395350},"title":"MARTA: Leveraging Human Rationales for Explainable Text Classification"},{"paperId":"c1e35d934955ad10958fc2d338016fd9b52462de","externalIds":{"DBLP":"journals/ml/SkrljMLP21","PubMedCentral":"8550026","MAG":"3156197110","DOI":"10.1007/s10994-021-05968-x","CorpusId":234827660,"PubMed":"34720391"},"title":"autoBOT: evolving neuro-symbolic representations for explainable low resource text classification"},{"paperId":"256db9dba1978f004a67c86ffc321563b1aee79a","externalIds":{"ArXiv":"2103.11251","DBLP":"journals/corr/abs-2103-11251","DOI":"10.1214/21-ss133","CorpusId":232307437},"title":"Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges"},{"paperId":"3f0f6c19c6f5d4e4d5066984c5f3e922a2c2ff85","externalIds":{"DBLP":"conf/nips/MirchandaniKS21","ArXiv":"2103.05825","CorpusId":232170216},"title":"ELLA: Exploration through Learned Language Abstraction"},{"paperId":"622799c27af43fc91738537d96ae2ceb78b10eb0","externalIds":{"ArXiv":"2102.13034","DBLP":"conf/chi/ShenWDJD21","DOI":"10.1145/3411763.3451591","CorpusId":232045882},"title":"AutoPreview: A Framework for Autopilot Behavior Understanding"},{"paperId":"fff58c6686a95bbe6fda8ad9ea0cbb4ab017ac78","externalIds":{"DBLP":"journals/corr/abs-2102-13045","ArXiv":"2102.13045","MAG":"3130234572","DOI":"10.1609/aaai.v35i11.17192","CorpusId":232046462},"title":"Iterative Bounding MDPs: Learning Interpretable Policies via Non-Interpretable Methods"},{"paperId":"467704b051b96198fb38c83eb5dcf046c1ef6779","externalIds":{"DBLP":"journals/tmlr/ZimmerFGJZW0HL23","ArXiv":"2102.11529","CorpusId":232013473},"title":"Differentiable Logic Machines"},{"paperId":"96b78897fba37282038bde12e48f8995d1276008","externalIds":{"ArXiv":"2102.06177","DBLP":"conf/icml/Sodhani0P21","CorpusId":231879645},"title":"Multi-Task Reinforcement Learning with Context-based Representations"},{"paperId":"41c721aec8c3be4334f433b6359ab8075fbbd189","externalIds":{"DBLP":"conf/aaai/AmitaiA22","ArXiv":"2102.03064","DOI":"10.1609/aaai.v36i5.20463","CorpusId":244800797},"title":"\"I Don't Think So\": Summarizing Policy Disagreements for Agent Comparison"},{"paperId":"ea8fcd754845fbf517a6b7dce6da32a5b48c14d8","externalIds":{"DBLP":"journals/corr/abs-2101-12446","ArXiv":"2101.12446","DOI":"10.1016/j.artint.2021.103455","CorpusId":231728404},"title":"Counterfactual State Explanations for Reinforcement Learning Agents via Generative Deep Learning"},{"paperId":"05f0113b1bd067ce179ea880f9a4bdcb19920651","externalIds":{"ArXiv":"2101.03238","MAG":"3098218044","DBLP":"conf/nips/Inala0PPB0RS20","CorpusId":227276503},"title":"Neurosymbolic Transformers for Multi-Agent Communication"},{"paperId":"796a6402b009d73b4c35d54920f866bc73b9e878","externalIds":{"MAG":"3111053239","DBLP":"journals/corr/abs-2012-07723","ArXiv":"2012.07723","DOI":"10.1109/ACCESS.2023.3236260","CorpusId":229158316},"title":"Evolutionary Learning of Interpretable Decision Trees"},{"paperId":"c4dd19aeca31f15a51f2550f41f77c90686d3260","externalIds":{"ArXiv":"2011.05064","MAG":"3104098714","DBLP":"conf/nips/Yau0H20","CorpusId":226289628},"title":"What Did You Think Would Happen? Explaining Agent Behaviour Through Intended Outcomes"},{"paperId":"0d6a4e45acde6f47d704ed0752f17f7ab52223af","externalIds":{"MAG":"3096052269","DBLP":"journals/corr/abs-2011-00517","ArXiv":"2011.00517","CorpusId":226226934},"title":"Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning"},{"paperId":"5c4a2eb5297553f4a384416c0e45eb6f0b94f290","externalIds":{"ArXiv":"2010.15942","DBLP":"conf/nips/GuoZLZBHS21","CorpusId":235366272},"title":"Machine versus Human Attention in Deep Reinforcement Learning Tasks"},{"paperId":"d60f0bc0be7b8ae8e049ceb906530ba7744229c5","externalIds":{"DBLP":"journals/corr/abs-2010-11655","MAG":"3094404987","ArXiv":"2010.11655","CorpusId":225039769},"title":"Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games"},{"paperId":"b010b82f34a3c048d9c836a151c5341afb629562","externalIds":{"MAG":"3100407858","ArXiv":"2009.12612","DBLP":"conf/nips/AndersonVDC20","CorpusId":221970012},"title":"Neurosymbolic Reinforcement Learning with Formally Verified Exploration"},{"paperId":"0425e598031fa9a05284a5429add9f6c9e208614","externalIds":{"DBLP":"journals/corr/abs-2009-07015","ArXiv":"2009.07015","MAG":"3086710135","DOI":"10.1145/3412452.3423570","CorpusId":221702990},"title":"Navigation and exploration in 3D-game automated play testing"},{"paperId":"12910c48cfe1684be9cf88bfd54913048cce8096","externalIds":{"DBLP":"conf/icann/HanTLKL20","MAG":"3096763169","DOI":"10.1007/978-3-030-61616-8_30","CorpusId":224806039},"title":"Improving Multi-agent Reinforcement Learning with Imperfect Human Knowledge"},{"paperId":"85a8878a69da8af06e4bd7a3b3e31bfc7e14604d","externalIds":{"MAG":"3012083254","DOI":"10.1016/j.ijepes.2020.106016","CorpusId":216408450},"title":"Combined heat and power system intelligent economic dispatch: A deep reinforcement learning approach"},{"paperId":"ea5988da726cb50d2376584b04d668008aad0a3f","externalIds":{"MAG":"3048977193","ArXiv":"2008.06693","DBLP":"journals/kbs/HeuilletCR21","DOI":"10.1016/j.knosys.2020.106685","CorpusId":221203027},"title":"Explainability in Deep Reinforcement Learning"},{"paperId":"d3d9af665f1aaeb7643a4c3608ba184737bc467c","externalIds":{"DBLP":"conf/eccv/WillifordMB20","MAG":"3047300400","ArXiv":"2008.00916","DOI":"10.1007/978-3-030-58621-8_15","CorpusId":220935832},"title":"Explainable Face Recognition"},{"paperId":"ad6c2aab28b860bcb6fc0ab019ad95faac8b8190","externalIds":{"MAG":"3081056513","DBLP":"conf/kdd/PanHLZL20","DOI":"10.1145/3394486.3403186","CorpusId":221191678},"title":"xGAIL: Explainable Generative Adversarial Imitation Learning for Explainable Human Decision Analysis"},{"paperId":"fb1cd08c338c226cdd6d49bae3d7e3224ac0af8d","externalIds":{"MAG":"3089176068","DOI":"10.1109/ComPE49325.2020.9199993","CorpusId":221846415},"title":"Agent-based Learning for Auto-Navigation within the Virtual City"},{"paperId":"364deb949f6ad1abee7f10079b48b61da9203579","externalIds":{"DBLP":"conf/nips/GuanVGZK21","ArXiv":"2006.14804","CorpusId":238227077},"title":"Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation"},{"paperId":"08bfe0db80f150bc2f8372b9a9817b7c844c1bca","externalIds":{"MAG":"3037621545","DBLP":"journals/corr/abs-2006-14804","CorpusId":220128278},"title":"Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning"},{"paperId":"a4d3a5c81cf3d19fde3ae6c8b2feaa32300cb9ff","externalIds":{"DBLP":"conf/aistats/SilvaGKJS20","MAG":"3037467471","CorpusId":263887452},"title":"Optimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning"},{"paperId":"2c9ba5421964b64c2d0bd5a0e3503197c93184e8","externalIds":{"MAG":"3025195025","DBLP":"journals/corr/abs-2005-08874","ArXiv":"2005.08874","DOI":"10.1016/j.artint.2021.103571","CorpusId":218674094},"title":"Local and Global Explanations of Agent Behavior: Integrating Strategy Summaries with Saliency Maps"},{"paperId":"e1b77eb03d753da13d1c6ce7cb45dd98e64c7fb6","externalIds":{"DBLP":"journals/corr/abs-2005-06247","ArXiv":"2005.06247","MAG":"3025747022","DOI":"10.1007/978-3-030-57321-8_5","CorpusId":218613667},"title":"Explainable Reinforcement Learning: A Survey"},{"paperId":"0b4a7f0e8a73c093447d882064360f4560fbcbec","externalIds":{"MAG":"3014232838","ArXiv":"2004.02860","DBLP":"journals/corr/abs-2004-02860","CorpusId":214802310},"title":"Weakly-Supervised Reinforcement Learning for Controllable Behavior"},{"paperId":"9651e1987a39d100dc0f6696a2077199e5075ea2","externalIds":{"MAG":"3013618273","ArXiv":"2003.13350","DBLP":"conf/icml/BadiaPKSVGB20","CorpusId":214713757},"title":"Agent57: Outperforming the Atari Human Benchmark"},{"paperId":"9499947f35e871e88c81800ae0841763fbf4d16d","externalIds":{"ArXiv":"2003.10386","MAG":"3012747917","DBLP":"journals/corr/abs-2003-10386","CorpusId":214611868},"title":"Incorporating Relational Background Knowledge into Reinforcement Learning via Differentiable Inductive Logic Programming"},{"paperId":"8cf62055fa0faab9c325f4b30415f5b0dc285434","externalIds":{"MAG":"3101438731","DBLP":"journals/corr/abs-2003-08165","ArXiv":"2003.08165","DOI":"10.1145/3377930.3389847","CorpusId":212747499},"title":"Neuroevolution of self-interpretable agents"},{"paperId":"525380854d05dcd584011945167b781c5ff0f090","externalIds":{"ArXiv":"2002.07418","DBLP":"journals/corr/abs-2002-07418","MAG":"3034311880","DOI":"10.24963/ijcai.2020/317","CorpusId":211146308},"title":"KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge"},{"paperId":"b8e744b1f33b1f3f53fcffb1dafd592e992694ff","externalIds":{"MAG":"2998712570","DBLP":"journals/corr/abs-2001-06680","ArXiv":"2001.06680","DOI":"10.1609/AAAI.V34I07.6924","CorpusId":210839610},"title":"Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video"},{"paperId":"2b5910b649ce304cec8e49608e3caf2c82c40073","externalIds":{"DBLP":"journals/iotj/LinGPWMO20","MAG":"2999953070","DOI":"10.1109/JIOT.2020.2966232","CorpusId":213624646},"title":"Deep Reinforcement Learning for Economic Dispatch of Virtual Power Plant in Internet of Energy"},{"paperId":"32f39d580c7f200e2f6dd7c22ed460f6d9567426","externalIds":{"MAG":"2996988244","DBLP":"conf/nips/TasseJR20","ArXiv":"2001.01394","CorpusId":209862503},"title":"A Boolean Task Algebra for Reinforcement Learning"},{"paperId":"ed2bcfa4536eee4ed23d26e45e0d2a9af573567b","externalIds":{"MAG":"2972852318","DOI":"10.1109/TPWRS.2019.2941134","CorpusId":203127753},"title":"Deep-Reinforcement-Learning-Based Autonomous Voltage Control for Power Grid Operations"},{"paperId":"1933f4654acc42a4433e1c5bf67dddf3815cc234","externalIds":{"ArXiv":"1912.09007","DBLP":"journals/ai/SequeiraG20","MAG":"2996001543","DOI":"10.1016/J.ARTINT.2020.103367","CorpusId":209414627},"title":"Interestingness Elements for Explainable Reinforcement Learning: Understanding Agents' Capabilities and Limitations"},{"paperId":"b19729b27a1b4c24b52f87308c907653300afa7f","externalIds":{"MAG":"2996037775","DBLP":"journals/corr/abs-1912-06680","ArXiv":"1912.06680","CorpusId":209376771},"title":"Dota 2 with Large Scale Deep Reinforcement Learning"},{"paperId":"867cc74781225da4e08a77fc35037ba77911e455","externalIds":{"MAG":"2970811133","ArXiv":"1912.02503","DBLP":"journals/corr/abs-1912-02503","CorpusId":202781251},"title":"Hindsight Credit Assignment"},{"paperId":"9ab23008ae7a8edde1519f2fbf2ad44d50883afe","externalIds":{"ArXiv":"1911.12250","DBLP":"journals/corr/abs-1911-12250","MAG":"2991489433","CorpusId":208309914},"title":"Social Attention for Autonomous Decision-Making in Dense Traffic"},{"paperId":"b0c34618ffd1154f35863e2ce7250ac6b6f2c424","externalIds":{"MAG":"2999362542","DOI":"10.1201/9780367816377-16","CorpusId":209379623},"title":"Interpretable Machine Learning"},{"paperId":"3fdac1028bae028e1185b10f4264980a3f78d391","externalIds":{"DBLP":"conf/iros/ChenDPMMD19","MAG":"3004129452","DOI":"10.1109/IROS40897.2019.8968565","CorpusId":198119613},"title":"Attention-based Hierarchical Deep Reinforcement Learning for Lane Change Behaviors in Autonomous Driving"},{"paperId":"5688b8077117b3aafd54c2e71d959284f4d5c8b9","externalIds":{"DBLP":"conf/hcomp/BansalNKLWH19","MAG":"2984353433","DOI":"10.1609/hcomp.v7i1.5285","CorpusId":201685074},"title":"Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance"},{"paperId":"f9da80f9cf820b678bd891eedc5aa1f1a89d2103","externalIds":{"DBLP":"conf/hcomp/LageCHNKGD19","MAG":"2997560917","DOI":"10.1609/hcomp.v7i1.5280","CorpusId":213873486,"PubMed":"33623933"},"title":"Human Evaluation of Models Built for Interpretability"},{"paperId":"530a059cb48477ad1e3d4f8f4b153274c8997332","externalIds":{"ArXiv":"1910.10045","MAG":"2997428643","DBLP":"journals/inffus/ArrietaRSBTBGGM20","DOI":"10.1016/j.inffus.2019.12.012","CorpusId":204824113},"title":"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI"},{"paperId":"7a141e2ba1360f50ad5ba88842f7da88c93a38f7","externalIds":{"MAG":"2981702541","ArXiv":"1910.09986","DBLP":"journals/corr/abs-1910-09986","CorpusId":204824296},"title":"Faster and Safer Training by Embedding High-Level Knowledge into Deep Reinforcement Learning"},{"paperId":"ba5069f5a4f19005ab741b6cac4690e7c603472d","externalIds":{"DBLP":"conf/iccv/LiuWGTL19","MAG":"2983697263","DOI":"10.1109/ICCV.2019.00622","CorpusId":207980323},"title":"Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification"},{"paperId":"3ee45877f7f14c8ee4872a7249f74eef7dc2255f","externalIds":{"MAG":"3006755302","DBLP":"conf/iclr/PetersenLMSKK21","ArXiv":"1912.04871","CorpusId":212956516},"title":"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients"},{"paperId":"78a0c951706c88901c45fa1ee5709160beb7032b","externalIds":{"DBLP":"journals/corr/abs-1909-02982","MAG":"3043397348","ArXiv":"1909.02982","DOI":"10.1111/cgf.13962","CorpusId":202232743},"title":"DRLViz: Understanding Decisions and Memory in Deep Reinforcement Learning"},{"paperId":"cafeacd3eec868d2ab4e76d4b16a7da49cd369c4","externalIds":{"MAG":"2967595108","DBLP":"conf/sigcomm/KazakBKS19","DOI":"10.1145/3341216.3342218","CorpusId":199661770},"title":"Verifying Deep-RL-Driven Systems"},{"paperId":"9bd453ea1e4655311cb1172f7a188cb9c5ee367d","externalIds":{"DBLP":"journals/corr/abs-1909-09906","MAG":"2974316376","ArXiv":"1909.09906","DOI":"10.24963/ijcai.2019/884","CorpusId":189805612},"title":"Leveraging Human Guidance for Deep Reinforcement Learning Tasks"},{"paperId":"cf039163bc505e615ff77f12454b26c48195898a","externalIds":{"DBLP":"conf/kdd/WangZTWX19","ArXiv":"1908.02646","MAG":"3101796500","DOI":"10.1145/3292500.3330647","CorpusId":198952425},"title":"AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy using Interpretable Deep Reinforcement Attention Networks"},{"paperId":"5bf06fea2ec80b280471ab8f7f070af0f7c4b390","externalIds":{"MAG":"2903994599","DBLP":"conf/aaai/RamakrishnanKND19","DOI":"10.1609/AAAI.V33I01.33016137","CorpusId":69626346},"title":"Overcoming Blind Spots in the Real World: Leveraging Complementary Abilities for Joint Execution"},{"paperId":"3c742ebff7df12e3fd0194023975e2211e4ba67a","externalIds":{"MAG":"2957311447","DBLP":"conf/cav/KatzHIJLLSTWZDK19","DOI":"10.1007/978-3-030-25540-4_26","CorpusId":196610895},"title":"The Marabou Framework for Verification and Analysis of Deep Neural Networks"},{"paperId":"f23a57c0d546ed2a5dd151879338b1b48130ec82","externalIds":{"DBLP":"conf/gecco/0001UR19","MAG":"2961374456","DOI":"10.1145/3319619.3326755","CorpusId":195877386},"title":"Generating interpretable reinforcement learning policies using genetic programming"},{"paperId":"4cc8c3801ab3cbdfed7aa8997da210fcf1d11f81","externalIds":{"MAG":"2954804306","DBLP":"journals/corr/abs-1907-01180","ArXiv":"1907.01180","CorpusId":195776579},"title":"Conservative Q-Improvement: Reinforcement Learning for an Interpretable Decision-Tree Policy"},{"paperId":"27ffe05b33993618a42d7a6cfbb8dd1d14db8e99","externalIds":{"DBLP":"conf/nips/Verma0YC19","MAG":"2990881605","CorpusId":204892806},"title":"Imitation-Projected Programmatic Reinforcement Learning"},{"paperId":"28e815891b9b5b9751a82283b41e14d59e4e17a9","externalIds":{"MAG":"2953393016","DBLP":"conf/dcai/HitomiMRC19","DOI":"10.1007/978-3-030-23887-2_1","CorpusId":195656429},"title":"Development of a Dangerous Driving Suppression System Using Inverse Reinforcement Learning and Blockchain"},{"paperId":"c2c8482c713b94073f3d59895b373db4398ddfbb","externalIds":{"DBLP":"conf/nips/JiangGMF19","ArXiv":"1906.07343","MAG":"2951725892","CorpusId":189998275},"title":"Language as an Abstraction for Hierarchical Deep Reinforcement Learning"},{"paperId":"e85119cb887cc6ae8d496c7c08bc87768622da5a","externalIds":{"DBLP":"journals/corr/abs-1906-03523","ArXiv":"1906.03523","MAG":"2948209403","CorpusId":182952646},"title":"Inductive Logic Programming via Differentiable Deep Neural Logic Networks"},{"paperId":"d7aeae4a7acaf1b6004922840e7c0a986f9c925f","externalIds":{"DBLP":"conf/pldi/ZhuXMJ19","MAG":"2953466973","ArXiv":"1907.07273","DOI":"10.1145/3314221.3314638","CorpusId":174802999},"title":"An inductive synthesis framework for verifiable reinforcement learning"},{"paperId":"507d72eb16e12b6e3da547b7c49b1077c47355f2","externalIds":{"MAG":"2948829945","DBLP":"journals/corr/abs-1906-01998","ArXiv":"1906.01998","DOI":"10.1287/educ.2019.0200","CorpusId":174799556},"title":"The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to be More Effective at Data Analysis"},{"paperId":"3ebb8ba5fde914378ed839389a50ceeebfe1f9c5","externalIds":{"MAG":"2968202530","DBLP":"conf/cvpr/ChenDPMMD19a","DOI":"10.1109/CVPRW.2019.00172","CorpusId":215739714},"title":"Attention-Based Hierarchical Deep Reinforcement Learning for Lane Change Behaviors in Autonomous Driving"},{"paperId":"bb4c48e06dcf16dad1a9caca9da3ef1c077feaa5","externalIds":{"DBLP":"journals/corr/abs-1905-13420","ArXiv":"1905.13420","MAG":"2947137205","CorpusId":173188408},"title":"Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning"},{"paperId":"5f0bc5273dec29fa6d3bbd743e0190f8de58d582","externalIds":{"DBLP":"conf/nips/AnsuiniLMZ19","MAG":"2970935073","ArXiv":"1905.12784","CorpusId":170079070},"title":"Intrinsic dimension of data representations in deep neural networks"},{"paperId":"e7a635bd494f1d458721f54492c21b4f6ede69d0","externalIds":{"MAG":"2952561542","ArXiv":"1905.12044","DBLP":"conf/aaai/TopinV19","DOI":"10.1609/AAAI.V33I01.33012514","CorpusId":70180808},"title":"Generation of Policy-Level Explanations for Reinforcement Learning"},{"paperId":"cfb68baa23048e3e0f8845c099fa013797bd623f","externalIds":{"MAG":"2998004401","DBLP":"journals/corr/abs-1905-10958","ArXiv":"1905.10958","DOI":"10.1609/AAAI.V34I03.5631","CorpusId":166228224},"title":"Explainable Reinforcement Learning Through a Causal Lens"},{"paperId":"8b3b12f1388c36d1139a12811138ffe1b1ab08e0","externalIds":{"MAG":"2967059815","DBLP":"conf/icra/PanCCCY19","DOI":"10.1109/ICRA.2019.8794437","CorpusId":199541685},"title":"Semantic Predictive Control for Explainable and Efficient Policy Learning"},{"paperId":"a7859b059cfe01d01f1bd795e86eb3f0771fb53b","externalIds":{"MAG":"2920329255","DBLP":"journals/aamas/WuGK20","DOI":"10.1007/s10458-020-09451-0","CorpusId":211265896},"title":"Model primitives for hierarchical lifelong reinforcement learning"},{"paperId":"fae7f5c4f6f8233b3af6523061eee462092230aa","externalIds":{"ArXiv":"1905.02680","MAG":"2989958156","DBLP":"journals/corr/abs-1905-02680","DOI":"10.1109/TIV.2019.2955905","CorpusId":146808048},"title":"Combining Planning and Deep Reinforcement Learning in Tactical Decision Making for Autonomous Driving"},{"paperId":"1fe4d11d8ce3d033dbead6f4a567f86fe21bf8ee","externalIds":{"ArXiv":"1904.10729","MAG":"2940734525","DBLP":"conf/icml/JiangL19","CorpusId":129946158},"title":"Neural Logic Reinforcement Learning"},{"paperId":"5534cdc24c8fd3c962dee54fdfbd0f942bfce70f","externalIds":{"MAG":"2936971746","ArXiv":"1904.09503","DBLP":"conf/itsc/ChenYT19","DOI":"10.1109/ITSC.2019.8917306","CorpusId":128350527},"title":"Model-free Deep Reinforcement Learning for Urban Autonomous Driving"},{"paperId":"8f58bed542bc353769b9c92dacb6c67abefef665","externalIds":{"MAG":"2991859550","DBLP":"journals/tsg/YangWSGS20","ArXiv":"1904.09374","DOI":"10.1109/TSG.2019.2951769","CorpusId":208617841},"title":"Two-Timescale Voltage Control in Distribution Grids Using Deep Reinforcement Learning"},{"paperId":"adc4ddeb10dd8da115d4bde9569794ff409fcd40","externalIds":{"ArXiv":"1904.06703","MAG":"2968308334","DBLP":"conf/iros/BeyretSF19","DOI":"10.1109/IROS40897.2019.8968488","CorpusId":199543895},"title":"Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation"},{"paperId":"582a3ade413cb420b290485b1d223bea076f1498","externalIds":{"MAG":"2997423597","DBLP":"conf/aaai/ZhangWLGMWZHB20","DOI":"10.1609/AAAI.V34I04.6161","CorpusId":81982513,"PubMed":"32901213"},"title":"Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset"},{"paperId":"0fa1c75a452a046e11e775eb6120051c696d9366","externalIds":{"ArXiv":"1903.02020","DBLP":"conf/ijcai/GoyalNM19","MAG":"2920768993","DOI":"10.24963/ijcai.2019/331","CorpusId":70350059},"title":"Using Natural Language for Reward Shaping in Reinforcement Learning"},{"paperId":"8d044613d4e874c7ff3807a0fcf231db3d50f825","externalIds":{"DBLP":"conf/hri/TabrezH19","MAG":"2929352350","DOI":"10.1109/HRI.2019.8673198","CorpusId":85503001},"title":"Improving Human-Robot Interaction Through Explainable Reinforcement Learning"},{"paperId":"ef7df5eae54107c013885231eb7af4431f2e6158","externalIds":{"DBLP":"journals/corr/abs-1901-03729","MAG":"2909596867","ArXiv":"1901.03729","DOI":"10.1145/3301275.3302316","CorpusId":58004583},"title":"Automated rationale generation: a technique for explainable AI and its effects on human perceptions"},{"paperId":"2bc964de46c4a2068ea5b11591c390f242672a5a","externalIds":{"MAG":"2892051885","DBLP":"journals/tvcg/WangGSY19","DOI":"10.1109/TVCG.2018.2864504","CorpusId":52172108,"PubMed":"30188823"},"title":"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks"},{"paperId":"17dd3961829fb3797c6a28a0866abf574e017c5c","externalIds":{"DBLP":"journals/corr/abs-1812-11276","ArXiv":"1812.11276","MAG":"2907361867","CorpusId":57189412},"title":"Learn to Interpret Atari Agents"},{"paperId":"be711f681580d3a02c8bc4c4dab0c7a043f4e1d2","externalIds":{"ArXiv":"1812.04608","DBLP":"journals/corr/abs-1812-04608","MAG":"2903808828","CorpusId":54577009},"title":"Metrics for Explainable AI: Challenges and Prospects"},{"paperId":"df5d893638a668dda246e6bad9a5387f86f74bb6","externalIds":{"DBLP":"journals/tiis/MohseniZR21","MAG":"2992923261","DOI":"10.1145/3387166","CorpusId":208910731},"title":"A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems"},{"paperId":"19ddc8219562260ccc517be4c3b92d3648caf9f1","externalIds":{"MAG":"2902243259","ArXiv":"1811.11329","DBLP":"journals/corr/abs-1811-11329","CorpusId":53818607},"title":"Deep Reinforcement Learning for Autonomous Driving"},{"paperId":"b59646ddc6c102da27d42097d99f1deada65c84a","externalIds":{"ACL":"P19-1560","DBLP":"conf/acl/LiuYW19","ArXiv":"1811.00196","MAG":"2962863107","DOI":"10.18653/v1/P19-1560","CorpusId":53153643},"title":"Towards Explainable NLP: A Generative Explanation Framework for Text Classification"},{"paperId":"a9a95917f655631280ef8fe35466c53ac8d3e091","externalIds":{"DBLP":"conf/ssci/MishraDL18","MAG":"2912258973","DOI":"10.1109/SSCI.2018.8628887","CorpusId":53418965},"title":"Visual Sparse Bayesian Reinforcement Learning: A Framework for Interpreting What an Agent Has Learned"},{"paperId":"335afb1066996d2c1e2b33da28a45bbf851e1be6","externalIds":{"DBLP":"conf/aaai/LyuYLG19","ArXiv":"1811.00090","MAG":"2952274749","DOI":"10.1609/AAAI.V33I01.33012970","CorpusId":53171166},"title":"SDRL: Interpretable and Data-efficient Deep Reinforcement Learning Leveraging Symbolic Planning"},{"paperId":"dd54595a62a57b23f1f5ba25066618dcd044153b","externalIds":{"MAG":"2968104655","DBLP":"journals/corr/abs-1810-08700","ArXiv":"1810.08700","DOI":"10.1109/ICRA.2019.8793611","CorpusId":53046640},"title":"Safe Reinforcement Learning With Model Uncertainty Estimates"},{"paperId":"8dc8f3e0127adc6985d4695e9b69d04717b2fde8","externalIds":{"MAG":"2891612330","ArXiv":"1810.03292","DBLP":"journals/corr/abs-1810-03292","CorpusId":52938797},"title":"Sanity Checks for Saliency Maps"},{"paperId":"5803866e6c68eea11e55884f2f2cb30d8e47da2e","externalIds":{"DBLP":"conf/icml/GoldfeldBGMNKP19","MAG":"2962807446","CorpusId":170078981},"title":"Estimating Information Flow in Deep Neural Networks"},{"paperId":"4cb3fd057949624aa4f0bbe7a6dcc8777ff04758","externalIds":{"MAG":"2964067469","ArXiv":"1810.12894","DBLP":"journals/corr/abs-1810-12894","CorpusId":53115163},"title":"Exploration by Random Network Distillation"},{"paperId":"4e3af05208f54f22f5697e8e13bba9379e3c96f0","externalIds":{"MAG":"2891830784","DBLP":"journals/corr/abs-1809-06061","ArXiv":"1809.06061","DOI":"10.1145/3278721.3278776","CorpusId":3751451},"title":"Transparency and Explanation in Deep Reinforcement Learning Neural Networks"},{"paperId":"1715ffe2135fbf1ec8dbcb4a21dfb930253f24ca","externalIds":{"DBLP":"journals/corr/abs-1809-05630","MAG":"2962702317","ArXiv":"1809.05630","DOI":"10.1609/aaai.v33i01.33014561","CorpusId":52291522},"title":"Towards Better Interpretability in Deep Q-Networks"},{"paperId":"fa1723b216b1f41b085b62b450b7b0bd9f2fd281","externalIds":{"MAG":"2895299763","DBLP":"conf/eccv/LiLR18","DOI":"10.1007/978-3-030-01228-1_38","CorpusId":52233948},"title":"In the Eye of Beholder: Joint Learning of Gaze and Actions in First Person Video"},{"paperId":"3df952d4a724655f7520ff95d4b2cef90fff0cae","externalIds":{"MAG":"2886794383","DBLP":"journals/cacm/DuLH20","ArXiv":"1808.00033","DOI":"10.1145/3359786","CorpusId":51893222},"title":"Techniques for interpretable machine learning"},{"paperId":"9d0907770cd4619aa6a36139a859e8f09bc9f0ef","externalIds":{"MAG":"2885138528","ArXiv":"1807.11546","DBLP":"conf/eccv/KimRDCA18","DOI":"10.1007/978-3-030-01216-8_35","CorpusId":51887402},"title":"Textual Explanations for Self-Driving Vehicles"},{"paperId":"b618b5f212d8b93e8bb866a1e1ce18079946723b","externalIds":{"MAG":"2965046886","ArXiv":"1807.08706","DBLP":"journals/corr/abs-1807-08706","CorpusId":49907182},"title":"Contrastive Explanations for Reinforcement Learning in terms of Expected Consequences"},{"paperId":"6736ef589a70cf91091a53bbdc7240fd87a3d540","externalIds":{"ArXiv":"1807.08364","DBLP":"conf/iros/MendaDK19","MAG":"2883896749","DOI":"10.1109/IROS40897.2019.8968287","CorpusId":49903566},"title":"EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning"},{"paperId":"787c0a21cdcf59b50db201a564ce57fe0318360c","externalIds":{"MAG":"2883535494","DBLP":"journals/corr/abs-1807-05887","ArXiv":"1807.05887","DOI":"10.1007/978-3-030-10928-8_25","CorpusId":49865557},"title":"Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees"},{"paperId":"b7fcbb19c75ad65be522b64d5f4b23dbcb3b883b","externalIds":{"ArXiv":"1806.08049","DBLP":"journals/corr/abs-1806-08049","MAG":"2809283485","CorpusId":49352880},"title":"On the Robustness of Interpretability Methods"},{"paperId":"d00c7fc5201405d5411b5ad3da93c5575ce8f10e","externalIds":{"DBLP":"journals/corr/abs-1806-07421","ArXiv":"1806.07421","MAG":"2950772592","CorpusId":49324724},"title":"RISE: Randomized Input Sampling for Explanation of Black-box Models"},{"paperId":"c280c95c17194de83e5234cca3586cd567ecd47c","externalIds":{"DBLP":"journals/corr/abs-1809-07424","ArXiv":"1809.07424","MAG":"2952602496","DOI":"10.1609/hcomp.v6i1.13337","CorpusId":49299248},"title":"Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure"},{"paperId":"030c5f02c959fabc6abaf0f961c23704bab9cd40","externalIds":{"MAG":"2888100907","DBLP":"conf/urai/FayjieHOL18","DOI":"10.1109/URAI.2018.8441797","CorpusId":52127230},"title":"Driverless Car: Autonomous Driving Using Deep Reinforcement Learning in Urban Environment"},{"paperId":"9a8e6feb271bf1cce8b1393cf41e70692a7f6625","externalIds":{"MAG":"2964231903","DBLP":"journals/corr/abs-1805-08328","ArXiv":"1805.08328","CorpusId":46893003},"title":"Verifiable Reinforcement Learning via Policy Extraction"},{"paperId":"3cdb40d2cfa2d52ad0ce9626933d4fb04b5c4cbc","externalIds":{"ArXiv":"1805.07780","DBLP":"conf/nips/GoelWP18","MAG":"2964191931","CorpusId":29152511},"title":"Unsupervised Video Object Segmentation for Deep Reinforcement Learning"},{"paperId":"1d8f4f76ac6534627ef8a1c24b9937d8ab2a5c5f","externalIds":{"MAG":"2788403449","DBLP":"conf/aaai/Ribeiro0G18","DOI":"10.1609/aaai.v32i1.11491","CorpusId":3366554},"title":"Anchors: High-Precision Model-Agnostic Explanations"},{"paperId":"4c5a032f5c0ce3a9a609c94e9e5d290a8ba83d2e","externalIds":{"MAG":"2795559220","DBLP":"conf/chi/Coppers0LCLVV18","DOI":"10.1145/3173574.3174098","CorpusId":5047631},"title":"Intellingo: An Intelligible Translation Environment"},{"paperId":"c43bba87b4237a93d96b2a3e91da25d91fd0bb91","externalIds":{"MAG":"2796284132","DBLP":"journals/corr/abs-1804-02477","ArXiv":"1804.02477","CorpusId":4712980},"title":"Programmatically Interpretable Reinforcement Learning"},{"paperId":"124f6992202777c09169343d191c254592e4428c","externalIds":{"DBLP":"journals/corr/abs-1803-07140","ArXiv":"1803.07140","MAG":"2789903845","DOI":"10.1007/978-3-030-01267-0_16","CorpusId":4029761},"title":"Visual Psychophysics for Making Face Recognition Algorithms More Explainable"},{"paperId":"21af4ed208ea3ecdb20b75aa27cddd0bfe683eec","externalIds":{"MAG":"2792641098","DOI":"10.23915/DISTILL.00010","CorpusId":67440606},"title":"The Building Blocks of Interpretability"},{"paperId":"4debb99c0c63bfaa97dd433bc2828e4dac81c48b","externalIds":{"MAG":"2963923407","DBLP":"journals/corr/abs-1802-09477","ArXiv":"1802.09477","CorpusId":3544558},"title":"Addressing Function Approximation Error in Actor-Critic Methods"},{"paperId":"332327c6d00a776fdcf77637be481effa86e6de0","externalIds":{"MAG":"2962764167","DBLP":"conf/kdd/LinZXZ18","DOI":"10.1145/3219819.3219993","CorpusId":3334421},"title":"Efficient Large-Scale Fleet Management via Multi-Agent Deep Reinforcement Learning"},{"paperId":"71ff533c8b5001082a3aaade3097eb229864e69e","externalIds":{"MAG":"2949725973","DBLP":"conf/iclr/AdebayoGGK18","ArXiv":"1810.03307","CorpusId":52940306},"title":"Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values"},{"paperId":"1a0567770dff874efd92917a32662f8c0cfc1c58","externalIds":{"MAG":"2920068766","ArXiv":"1801.05500","DBLP":"journals/twc/ChallitaSB19","DOI":"10.1109/TWC.2019.2900035","CorpusId":96432042},"title":"Interference Management for Cellular-Connected UAVs: A Deep Reinforcement Learning Approach"},{"paperId":"811df72e210e20de99719539505da54762a11c6d","externalIds":{"MAG":"2962902376","ArXiv":"1801.01290","DBLP":"journals/corr/abs-1801-01290","CorpusId":28202810},"title":"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"},{"paperId":"ebc3bd9cd67193b3a8f84d43d5b4377107c680dc","externalIds":{"ArXiv":"1712.07294","DBLP":"journals/corr/abs-1712-07294","MAG":"2780057514","CorpusId":31598573},"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning"},{"paperId":"3f721241c97cee4670cb4e3e01eabfaccc7ed419","externalIds":{"MAG":"2951238463","ArXiv":"1712.04170","DBLP":"journals/eaai/HeinUR18","DOI":"10.1016/J.ENGAPPAI.2018.09.007","CorpusId":4589093},"title":"Interpretable Policies for Reinforcement Learning by Genetic Programming"},{"paperId":"682b9d2212258fd5edbfca589c86390c31a956b0","externalIds":{"ArXiv":"1711.11279","MAG":"2796885425","DBLP":"conf/icml/KimWGCWVS18","CorpusId":51737170},"title":"Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"},{"paperId":"956a17cb8f5db9cfa3c6256f1de76616268a4c5e","externalIds":{"ArXiv":"1711.07414","MAG":"2769542353","DBLP":"journals/corr/abs-1711-07414","CorpusId":26819361},"title":"The Promise and Peril of Human Evaluation for Model Interpretability"},{"paperId":"9662af7e969c6d17e9aefeb4d75625e01d95b7b6","externalIds":{"MAG":"2766691652","DBLP":"conf/iconip/FukuchiOYI17","DOI":"10.1007/978-3-319-70087-8_11","CorpusId":10022865},"title":"Application of Instruction-Based Behavior Explanation to a Reinforcement Learning Agent with Changing Policy"},{"paperId":"62e39c6dbcead1fe4d29017914591d929aa6ac4c","externalIds":{"MAG":"2766047647","ArXiv":"1711.00867","DBLP":"series/lncs/KindermansHAASDEK19","DOI":"10.1007/978-3-030-28954-6_14","CorpusId":28562869},"title":"The (Un)reliability of saliency methods"},{"paperId":"b4e669bae43216cb0e77836a411b88dea4bb6034","externalIds":{"MAG":"2962954781","DBLP":"conf/icml/GreydanusKDF18","ArXiv":"1711.00138","CorpusId":27585435},"title":"Visualizing and Understanding Atari Agents"},{"paperId":"c27db32efa8137cbf654902f8f728f338e55cd1c","externalIds":{"MAG":"2766447205","DBLP":"journals/nature/SilverSSAHGHBLB17","DOI":"10.1038/nature24270","CorpusId":205261034,"PubMed":"29052630"},"title":"Mastering the game of Go without human knowledge"},{"paperId":"78839e9d2dda0d9c7141dc95b3829feedbc36127","externalIds":{"MAG":"3103700104","DBLP":"conf/hai/FukuchiOYI17","ArXiv":"1810.08811","DOI":"10.1145/3125739.3125746","CorpusId":35107175},"title":"Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents"},{"paperId":"0ab3f7ecbdc5a33565a234215604a6ca9d155a33","externalIds":{"DBLP":"conf/aaai/HesselMHSODHPAS18","MAG":"2964291307","ArXiv":"1710.02298","DOI":"10.1609/aaai.v32i1.11796","CorpusId":19135734},"title":"Rainbow: Combining Improvements in Deep Reinforcement Learning"},{"paperId":"14e77acbe540bb08c1ab428813c74cf58145f93c","externalIds":{"MAG":"2752236613","DBLP":"journals/corr/abs-1709-02066","ArXiv":"1709.02066","DOI":"10.1109/ITSC.2017.8317735","CorpusId":3931005},"title":"Formulation of deep reinforcement learning architecture toward autonomous driving for on-ramp merge"},{"paperId":"abcf11a9af3d83f85c5fbfffc5901d416ca7a73f","externalIds":{"DBLP":"conf/aaai/WarnellWLS18","ArXiv":"1709.10163","MAG":"2963489214","DOI":"10.1609/aaai.v32i1.11485","CorpusId":4130751},"title":"Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces"},{"paperId":"51f4ea9831e23777bf1cbfed165c1cffceb209ec","externalIds":{"DBLP":"journals/corr/abs-1805-05769","MAG":"2742096003","ArXiv":"1805.05769","DOI":"10.1017/S0269888918000206","CorpusId":21674940},"title":"Leveraging human knowledge in tabular reinforcement learning: a study of human subjects"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"c1f4ef741242d629d1f56e442a09a7ba29595a0e","externalIds":{"MAG":"2951328804","DBLP":"conf/icml/BellemareDM17","ArXiv":"1707.06887","CorpusId":966543},"title":"A Distributional Perspective on Reinforcement Learning"},{"paperId":"a762ae907b7dd71a59bd8bd98aba69dfe2de13a2","externalIds":{"MAG":"2726187156","ArXiv":"1707.02286","DBLP":"journals/corr/HeessTSLMWTEWER17","CorpusId":30099687},"title":"Emergence of Locomotion Behaviours in Rich Environments"},{"paperId":"e89dfa306723e8ef031765e9c44e5f6f94fd8fda","externalIds":{"MAG":"2670253439","ArXiv":"1706.07269","DBLP":"journals/ai/Miller19","DOI":"10.1016/J.ARTINT.2018.07.007","CorpusId":36024272},"title":"Explanation in Artificial Intelligence: Insights from the Social Sciences"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","externalIds":{"MAG":"2626804490","ArXiv":"1706.03741","DBLP":"conf/nips/ChristianoLBMLA17","CorpusId":4787508},"title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"2b292ff89d808fba10579871591a22f1649cd039","externalIds":{"ArXiv":"1705.08926","DBLP":"journals/corr/FoersterFANW17","MAG":"2617547828","DOI":"10.1609/aaai.v32i1.11794","CorpusId":19141434},"title":"Counterfactual Multi-Agent Policy Gradients"},{"paperId":"78f05cbe614cfb8ab4aaebdda6701eb0eb5148ae","externalIds":{"DBLP":"conf/ieeevast/KrauseDSAB17","ArXiv":"1705.01968","MAG":"2612821743","DOI":"10.1109/VAST.2017.8585720","CorpusId":6784142},"title":"A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations"},{"paperId":"3f116042f50a499ab794bcc1255915bee507413c","externalIds":{"DBLP":"journals/corr/SzeCYE17","ArXiv":"1703.09039","MAG":"2951963285","DOI":"10.1109/JPROC.2017.2761740","CorpusId":3273340},"title":"Efficient Processing of Deep Neural Networks: A Tutorial and Survey"},{"paperId":"7db2afdc5eb5db46cc64185d0a51ed079b0976e8","externalIds":{"MAG":"2952901124","DBLP":"conf/ijcai/RossHD17","ArXiv":"1703.03717","DOI":"10.24963/ijcai.2017/371","CorpusId":7053611},"title":"Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations"},{"paperId":"dd83916f01c3245ca86fc41b9587b04ecbe4330f","externalIds":{"MAG":"2592141406","DBLP":"conf/iui/BerkovskyTC17","DOI":"10.1145/3025171.3025209","CorpusId":1717533},"title":"How to Recommend?: User Trust Factors in Movie Recommender Systems"},{"paperId":"ef731bc5b85867e97fbc01621282159c8a1f6ceb","externalIds":{"DBLP":"conf/hri/HayesS17","MAG":"2594336441","DOI":"10.1145/2909824.3020233","CorpusId":20988962},"title":"Improving Robot Controller Transparency Through Autonomous Policy Explanation"},{"paperId":"f302e136c41db5de1d624412f68c9174cf7ae8be","externalIds":{"MAG":"2949197630","DBLP":"conf/icml/SundararajanTY17","ArXiv":"1703.01365","CorpusId":16747630},"title":"Axiomatic Attribution for Deep Networks"},{"paperId":"7f2d2d14f0c07e4b3d9d636d904afa7087673b62","externalIds":{"DBLP":"journals/corr/Doshi-VelezK17","MAG":"2592895748","CorpusId":17244588},"title":"A Roadmap for a Rigorous Science of Interpretability"},{"paperId":"29069976eb7f828de91ed243cd12fd99fef56d94","externalIds":{"DBLP":"conf/iclr/ZintgrafCAW17","ArXiv":"1702.04595","MAG":"2950211972","CorpusId":2103669},"title":"Visualizing Deep Neural Network Decisions: Prediction Difference Analysis"},{"paperId":"c2ea7edbf76577ec5ac87116cc9cb71e7f744d88","externalIds":{"MAG":"2728455196","ArXiv":"1610.05984","DBLP":"journals/eaai/HeinHRU17","DOI":"10.1016/j.engappai.2017.07.005","CorpusId":8454523},"title":"Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies"},{"paperId":"b710f0c9ca89f997371ce06c142ad25be1b35cca","externalIds":{"DBLP":"conf/kdd/LakkarajuBL16","MAG":"2367397349","DOI":"10.1145/2939672.2939874","CorpusId":12533380,"PubMed":"27853627"},"title":"Interpretable Decision Sets: A Joint Framework for Description and Prediction"},{"paperId":"fdd025e077a36166b10120b448d0c4e4009824a9","externalIds":{"DBLP":"journals/corr/RibeiroSG16a","ArXiv":"1606.05386","MAG":"2420245003","CorpusId":8561410},"title":"Model-Agnostic Interpretability of Machine Learning"},{"paperId":"4ab53de69372ec2cd2d90c126b6a100165dc8ed1","externalIds":{"DBLP":"journals/corr/HoE16","ArXiv":"1606.03476","MAG":"2949080919","CorpusId":16153365},"title":"Generative Adversarial Imitation Learning"},{"paperId":"2d9d9d1f76200ddaf80bf21d872329e829d5ff87","externalIds":{"MAG":"1911487256","DOI":"10.1002/cne.23880","CorpusId":13954406,"PubMed":"26272220"},"title":"Components and characteristics of the dopamine reward utility signal"},{"paperId":"e75957f41a092a1a7b436375b949d205972d3474","externalIds":{"DBLP":"conf/chi/KayKHM16","MAG":"2398344594","DOI":"10.1145/2858036.2858558","CorpusId":9377480},"title":"When (ish) is My Bus?: User-centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems"},{"paperId":"c0883f5930a232a9c1ad601c978caede29155979","externalIds":{"DBLP":"conf/naacl/Ribeiro0G16","MAG":"2516809705","ArXiv":"1602.04938","ACL":"N16-3020","DOI":"10.1145/2939672.2939778","CorpusId":13029170},"title":"â€œWhy Should I Trust You?â€: Explaining the Predictions of Any Classifier"},{"paperId":"7200969d70cf6f3fd343f48e97b8ebf7d563a584","externalIds":{"ArXiv":"1602.02658","MAG":"2270696664","DBLP":"conf/icml/ZahavyBM16","CorpusId":7292590},"title":"Graying the black box: Understanding DQNs"},{"paperId":"69e76e16740ed69f4dc55361a3d319ac2f1293dd","externalIds":{"MAG":"2964043796","DBLP":"journals/corr/MnihBMGLHSK16","ArXiv":"1602.01783","CorpusId":6875312},"title":"Asynchronous Methods for Deep Reinforcement Learning"},{"paperId":"1dd87c38a6ab613165d3f3aad84dd3a05213271d","externalIds":{"MAG":"2264689927","DBLP":"journals/cin/LuRW16","PubMedCentral":"4706865","DOI":"10.1155/2016/1021378","CorpusId":14179803,"PubMed":"26819577"},"title":"Using Genetic Programming with Prior Formula Knowledge to Solve Symbolic Regression Problem"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"4c05d7caa357148f0bbd61720bdd35f0bc05eb81","externalIds":{"DBLP":"conf/icml/WangSHHLF16","ArXiv":"1511.06581","MAG":"2951799221","CorpusId":5389801},"title":"Dueling Network Architectures for Deep Reinforcement Learning"},{"paperId":"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca","externalIds":{"MAG":"2963477884","DBLP":"journals/corr/SchaulQAS15","ArXiv":"1511.05952","CorpusId":13022595},"title":"Prioritized Experience Replay"},{"paperId":"3b9732bb07dc99bde5e1f9f75251c6ea5039373e","externalIds":{"MAG":"2155968351","DBLP":"conf/aaai/HasseltGS16","ArXiv":"1509.06461","DOI":"10.1609/aaai.v30i1.10295","CorpusId":6208256},"title":"Deep Reinforcement Learning with Double Q-Learning"},{"paperId":"6df11b0bb0244d4d36e8955436067cc5d19734fa","externalIds":{"MAG":"2240067561","ArXiv":"1509.06321","DBLP":"journals/corr/SamekBMBM15","DOI":"10.1109/TNNLS.2016.2599820","CorpusId":7689122,"PubMed":"27576267"},"title":"Evaluating the Visualization of What a Deep Neural Network Has Learned"},{"paperId":"024006d4c2a89f7acacc6e4438d156525b60a98f","externalIds":{"MAG":"2173248099","DBLP":"journals/corr/LillicrapHPHETS15","ArXiv":"1509.02971","CorpusId":16326763},"title":"Continuous control with deep reinforcement learning"},{"paperId":"f5f323e62acb75f785e00b4c90ace16f1690076f","externalIds":{"ArXiv":"1507.06527","MAG":"2952684340","DBLP":"conf/aaaifs/HausknechtS15","CorpusId":8696662},"title":"Deep Recurrent Q-Learning for Partially Observable MDPs"},{"paperId":"1b5a24639fa80056d1a17b15f6997d10e76cc731","externalIds":{"ArXiv":"1506.06579","DBLP":"journals/corr/YosinskiCNFL15","MAG":"1825675169","CorpusId":9591565},"title":"Understanding Neural Networks Through Deep Visualization"},{"paperId":"d316c82c12cf4c45f9e85211ef3d1fa62497bff8","externalIds":{"MAG":"1191599655","DBLP":"journals/corr/SchulmanMLJA15","ArXiv":"1506.02438","CorpusId":3075448},"title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"paperId":"24f0e8db461cb8a35f642bf07ab407d546c9a0ab","externalIds":{"DBLP":"conf/chi/RaderG15","MAG":"2140553307","DOI":"10.1145/2702123.2702174","CorpusId":11081464},"title":"Understanding User Beliefs About Algorithmic Curation in the Facebook News Feed"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"340f48901f72278f6bf78a04ee5b01df208cc508","externalIds":{"DBLP":"journals/nature/MnihKSRVBGRFOPB15","MAG":"2145339207","DOI":"10.1038/nature14236","CorpusId":205242740,"PubMed":"25719670"},"title":"Human-level control through deep reinforcement learning"},{"paperId":"449532187c94af3dd3aa55e16d2c50f7854d2199","externalIds":{"MAG":"1771410628","ArXiv":"1502.05477","DBLP":"conf/icml/SchulmanLAJM15","CorpusId":16046818},"title":"Trust Region Policy Optimization"},{"paperId":"081651b38ff7533550a3adfc1c00da333a8fe86c","externalIds":{"DBLP":"conf/nips/YosinskiCBL14","ArXiv":"1411.1792","MAG":"2149933564","CorpusId":362467},"title":"How transferable are features in deep neural networks?"},{"paperId":"e15cf50aa89fee8535703b9f9512fca5bfc43327","externalIds":{"DBLP":"journals/corr/SzegedyLJSRAEVR14","MAG":"2097117768","ArXiv":"1409.4842","DOI":"10.1109/CVPR.2015.7298594","CorpusId":206592484},"title":"Going deeper with convolutions"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","externalIds":{"MAG":"2949429431","ArXiv":"1409.1556","DBLP":"journals/corr/SimonyanZ14a","CorpusId":14124313},"title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"01c4ff067657dbebe80aea9d3e4be666b42eea27","externalIds":{"DBLP":"conf/aaai/ZhengLN14","MAG":"2402734324","DOI":"10.1609/aaai.v28i1.8979","CorpusId":13973870},"title":"Robust Bayesian Inverse Reinforcement Learning with Sparse Behavior Noise"},{"paperId":"687d0e59d5c35f022ce4638b3e3a6142068efc94","externalIds":{"MAG":"2643747386","DBLP":"conf/icml/SilverLHDWR14","CorpusId":13928442},"title":"Deterministic Policy Gradient Algorithms"},{"paperId":"8c165562d6ae52fedab4774cb058164ea8cb65a8","externalIds":{"DBLP":"conf/sigdial/NothdurftRM14","MAG":"2251148681","ACL":"W14-4307","DOI":"10.3115/v1/W14-4307","CorpusId":18557001},"title":"Probabilistic Human-Computer Trust Handling"},{"paperId":"d0cddb1bcf2f2dd2671ef1971d608bd742064f95","externalIds":{"DBLP":"journals/ijmms/GedikliJG14","MAG":"2100171245","DOI":"10.1016/J.IJHCS.2013.12.007","CorpusId":17471203},"title":"How should I explain? A comparison of different explanation types for recommender systems"},{"paperId":"71c0ce36ee6c7aaa0623a6d574b56bb3183587d4","externalIds":{"DBLP":"journals/tse/GroceKZSBWSDSBM14","MAG":"2036779186","DOI":"10.1109/TSE.2013.59","CorpusId":6288024},"title":"You Are the Only Possible Oracle: Effective Test Selection for End Users of Interactive Machine Learning Systems"},{"paperId":"2319a491378867c7049b3da055c5df60e1671158","externalIds":{"DBLP":"journals/corr/MnihKSGAWR13","MAG":"1757796397","ArXiv":"1312.5602","CorpusId":15238391},"title":"Playing Atari with Deep Reinforcement Learning"},{"paperId":"b56b1e0acd3301c925bb2b074fe3fb8e0dbf5379","externalIds":{"MAG":"1994606570","DBLP":"conf/vl/KuleszaSBYKW13","DOI":"10.1109/VLHCC.2013.6645235","CorpusId":6960803},"title":"Too much, too little, or just right? Ways explanations impact end users' mental models"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"fa51a2c720e09fca5829d19bdca4d02ed3e093eb","externalIds":{"MAG":"11104910","DBLP":"conf/dis/MaesFWE12","DOI":"10.1007/978-3-642-33492-4_6","CorpusId":17502934},"title":"Policy Search in a Space of Simple Closed-form Formulas: Towards Interpretability of Reinforcement Learning"},{"paperId":"2e2089ae76fe914706e6fa90081a79c8fe01611e","externalIds":{"ArXiv":"1206.2944","MAG":"2950182411","DBLP":"conf/nips/SnoekLA12","CorpusId":632197},"title":"Practical Bayesian Optimization of Machine Learning Algorithms"},{"paperId":"644a079073969a92674f69483c4a85679d066545","externalIds":{"MAG":"2115211925","DBLP":"conf/nips/Hasselt10","CorpusId":5155799},"title":"Double Q-learning"},{"paperId":"79ab3c49903ec8cb339437ccf5cf998607fc313e","externalIds":{"MAG":"2962957031","DBLP":"journals/jmlr/RossGB11","ArXiv":"1011.0686","CorpusId":103456},"title":"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"},{"paperId":"ab3a6d560deb8ca59ada7d2fa13b1d97d69db49f","externalIds":{"MAG":"2069879691","DBLP":"conf/cvpr/KjellstromKB10","DOI":"10.1109/CVPR.2010.5540140","CorpusId":8506435},"title":"Tracking people interacting with objects"},{"paperId":"3c20d5e328a5c538cb638dd46222f8de18af4164","externalIds":{"MAG":"2086989313","DOI":"10.2501/S147078530920120X","CorpusId":167592906},"title":"Research into Questionnaire Design: A Summary of the Literature"},{"paperId":"e9260719b79cba3a35edcd5848e5d8c4f69e8d36","externalIds":{"DBLP":"conf/huc/LimD09","MAG":"2131251708","DOI":"10.1145/1620545.1620576","CorpusId":5557953},"title":"Assessing demand for intelligibility in context-aware applications"},{"paperId":"fcb2480ee2fe28ca697afdd192bf3ad7de05b442","externalIds":{"MAG":"2065267227","DBLP":"journals/ijmms/StumpfRLWBDSH09","DOI":"10.1016/j.ijhcs.2009.03.004","CorpusId":7067423},"title":"Interacting meaningfully with machine learning systems: Three experiments"},{"paperId":"b062423d2c021274300e5a9f58fc66eea39d1c55","externalIds":{"MAG":"2154157725","DBLP":"conf/chi/LimDA09","DOI":"10.1145/1518701.1519023","CorpusId":4507550},"title":"Why and why not explanations improve the intelligibility of context-aware intelligent systems"},{"paperId":"04b4359fd98beedc1686629fef95cf465220e639","externalIds":{"MAG":"2498956481","DOI":"10.1142/9789812771728_0001","CorpusId":57185087},"title":"An Introduction to Decision Trees"},{"paperId":"bb121cb2f3dd99d31d8c623229c81c8d55cb990d","externalIds":{"DBLP":"conf/hri/PowersK06","MAG":"2018372581","DOI":"10.1145/1121241.1121280","CorpusId":3923404},"title":"The advisor robot: tracing people's mental model from a robot's physical attributes"},{"paperId":"019b7f71d524c31d8ee8c96ae8866014099cc270","externalIds":{"MAG":"1995445252","DBLP":"conf/iui/PuC06","DOI":"10.1145/1111449.1111475","CorpusId":6274371},"title":"Trust building with explanation interfaces"},{"paperId":"3fdfa50a9224e22da8d2987f06346371ba26dd03","externalIds":{"MAG":"2083844448","DOI":"10.1016/S0304-3800(02)00257-0","CorpusId":85348798},"title":"Review and comparison of methods to study the contribution of variables in artificial neural network models"},{"paperId":"87272aaa0008372a3a1485993c7a6a8ef252110a","externalIds":{"MAG":"2150365139","DBLP":"conf/ijcai/HalpernP01","ArXiv":"cs/0208034","DOI":"10.1093/bjps/axi148","CorpusId":2491924},"title":"Causes and Explanations: A Structural-Model Approach. Part II: Explanations"},{"paperId":"df48fc2f540b2c47d8a10e2956c1bfd219965e6c","externalIds":{"MAG":"2027280740","DBLP":"journals/ijmms/Borgman99","DOI":"10.1145/253495.253533","CorpusId":20719436},"title":"The user's mental model of an information retrieval system"},{"paperId":"94066dc12fe31e96af7557838159bde598cb4f10","externalIds":{"DBLP":"conf/icml/NgHR99","MAG":"1777239053","CorpusId":5730166},"title":"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"},{"paperId":"04b4310181a2491d216981b84f2cfa216550e1c2","externalIds":{"MAG":"2018072436","DOI":"10.1111/J.1745-3984.1999.TB00552.X","CorpusId":143507716},"title":"\"Mental Model\" Comparison of Automated and Human Scoring"},{"paperId":"1f110b6ea97fa9e0e04cffb915682e72231390a9","externalIds":{"DBLP":"conf/aaai/UtherV98","MAG":"1492518272","CorpusId":5203075},"title":"Tree Based Discretization for Continuous State Space Reinforcement Learning"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"942dbd37812c7316498db9b273ab5514448493d1","externalIds":{"DBLP":"journals/siamrev/Feinberg96","MAG":"2077612644","DOI":"10.1137/1038137","CorpusId":207063706},"title":"Markov Decision Processes: Discrete Stochastic Dynamic Programming (Martin L. Puterman)"},{"paperId":"06509216ed9bfabdaf8df3ae0f910a1c680a28f8","externalIds":{"MAG":"2730088677","DOI":"10.1002/HRDQ.3920060303","CorpusId":143347655},"title":"Measuring mental models: Choosing the right tools for the job"},{"paperId":"0f13891132dff5d564f30c58c6c4a4bfdc632784","externalIds":{"DBLP":"journals/ml/DietterichF97","MAG":"2094438588","DOI":"10.1023/A:1007355226281","CorpusId":1906622},"title":"Explanation-Based Learning and Reinforcement Learning: A Unified View"},{"paperId":"a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63","externalIds":{"MAG":"2119567691","DOI":"10.2307/2291177","CorpusId":122678161},"title":"Markov Decision Processes: Discrete Stochastic Dynamic Programming"},{"paperId":"ec9fd6c8fdafdd20db5501e903fb57788478352e","externalIds":{"MAG":"1976457714","DOI":"10.1037/H0029675","CorpusId":12250623,"PubMed":"5483811"},"title":"A questionnaire measure of achievement motivation."},{"paperId":"45786063578e814444b8247028970758bbbd0488","externalIds":{"DBLP":"books/mc/22/Dijkstra22a","MAG":"2152064245","DOI":"10.1145/3544585.3544600","CorpusId":123284777},"title":"A note on two problems in connexion with graphs"},{"paperId":"9b769d65afebbbddd068b3ed235f28a2d6717f78","externalIds":{"MAG":"1967788631","DOI":"10.2466/pr0.1957.3.h.377","CorpusId":143295161},"title":"Human Trial-and-Error Learning"},{"paperId":"b512b21eefdfe02935177024668255f47b2b6c58","externalIds":{"PubMedCentral":"9196367","DOI":"10.1080/00332920701319442","CorpusId":219730239,"PubMed":"36488184"},"title":"Fidelity"},{"paperId":"c0de7a5e50976bbd56a89a5de21c0e2c76cfa03f","externalIds":{"DBLP":"conf/iclr/KennyTS23","CorpusId":259298828},"title":"Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes"},{"paperId":"68fd3160822b6bb345bee1c1e214ecce6e547b14","externalIds":{"DBLP":"journals/corr/abs-2203-01004","DOI":"10.48550/arXiv.2203.01004","CorpusId":247218457},"title":"Improving the Diversity of Bootstrapped DQN via Noisy Priors"},{"paperId":"5de68625d253882adce1927f04a8490efafdceae","externalIds":{"DBLP":"conf/aaai/RajendranEDM22","CorpusId":247321924},"title":"Human-in-the-loop Learning for Safe Exploration through Anomaly Prediction and Intervention"},{"paperId":"d197b52f2a252ab92cc789120c7d51cbaed8f0b1","externalIds":{"DBLP":"journals/corr/abs-2211-06665","DOI":"10.48550/arXiv.2211.06665","CorpusId":253510389},"title":"A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges"},{"paperId":"2f603bfd38d83204bb24bdb5c700fd193199c542","externalIds":{"DBLP":"journals/tkdd/LiuYGG22","DOI":"10.1145/3488378","CorpusId":246280842},"title":"Forecasting the Market with Machine Learning Algorithms: An Application of NMC-BERT-LSTM-DQN-X Algorithm in Quantitative Trading"},{"paperId":"61fff1deebd649264dcfac407c70d1f0db87adea","externalIds":{"DBLP":"conf/nips/AshwoodJP22","CorpusId":258509665},"title":"Dynamic Inverse Reinforcement Learning for Characterizing Animal Behavior"},{"paperId":"afd30d7c00cce7a8d809c5c5ab5d7b7351491bf7","externalIds":{"DBLP":"conf/cav/JinTZWZ22","DOI":"10.1007/978-3-031-13185-1_10","CorpusId":251447664},"title":"Trainify: A CEGAR-Driven Training and Verification Framework for Safe Deep Reinforcement Learning"},{"paperId":"9b8dc80a0d214ffee871c2960b31a3596460968c","externalIds":{"DBLP":"conf/nips/Stein21","CorpusId":245010946},"title":"Generating High-Quality Explanations for Navigation in Partially-Revealed Environments"},{"paperId":"9ec1b4287654784652c36dbfbef9cc2a59561733","externalIds":{"DBLP":"conf/icml/LarmaPKSGMPF21","CorpusId":235826281},"title":"Discovering symbolic policies with deep reinforcement learning"},{"paperId":"639acfebc47ac0b905d390510346c48c0e112624","externalIds":{"DBLP":"conf/nips/GuoWKX21","CorpusId":240291198},"title":"EDGE: Explaining Deep Reinforcement Learning Policies"},{"paperId":"8d01ef7736016c963cfcf2dfa13c7d8052d07bfb","externalIds":{"DBLP":"conf/nips/MundhenkLGSFP21","CorpusId":248497879},"title":"Symbolic Regression via Deep Reinforcement Learning Enhanced Genetic Programming Seeding"},{"paperId":"cabcec1eb7c742a3ce00ccd069e1b36a22bb6186","externalIds":{"MAG":"2972122474","ArXiv":"1907.05707","DBLP":"conf/aaai/WangZKG20","DOI":"10.1609/aaai.v34i05.6220","CorpusId":202538671},"title":"Shapley Q-value: A Local Reward Approach to Solve Global Reward Games"},{"paperId":"fcbba7b10c8c5167fe48ff6bf19e4e25948b4f40","externalIds":{"DBLP":"conf/pkdd/NguyenNI20","DOI":"10.1007/978-3-030-65742-0_6","CorpusId":230717340},"title":"A Model-Agnostic Approach to Quantifying the Informativeness of Explanation Methods for Time Series Classification"},{"paperId":"d47677337b1083d6bfa940748da0780b2c9faf7d","externalIds":{"CorpusId":204898543},"title":"Explainable Reinforcement Learning via Reward Decomposition"},{"paperId":"0e1f153576c7f9f2628cdd34a1067c4d26bdc096","externalIds":{"DBLP":"series/lncs/11700","MAG":"3000716014","DOI":"10.1007/978-3-030-28954-6","CorpusId":202160253},"title":"Explainable AI: Interpreting, Explaining and Visualizing Deep Learning"},{"paperId":"758ba118d7f161fe019d81c4f64a9069c342aa74","externalIds":{"DBLP":"conf/nips/ManiaGR18","MAG":"2890326782","CorpusId":53968446},"title":"Simple random search of static linear policies is competitive for reinforcement learning"},{"paperId":"84082634110fcedaaa32632f6cc16a034eedb2a0","externalIds":{"DBLP":"journals/jmlr/WirthANF17","MAG":"3006334608","CorpusId":703818},"title":"A Survey of Preference-Based Reinforcement Learning Methods"},{"paperId":"4f8549f0859330fd6e46f7579efc19652e5e09ae","externalIds":{"DBLP":"conf/nips/KimKK16","MAG":"2551974706","CorpusId":16533923},"title":"Examples are not enough, learn to criticize! Criticism for Interpretability"},{"paperId":"486f8172f672e2b1ea1076d487e2ac13765151db","externalIds":{"CorpusId":209424464},"title":"Analyzing and Validating Neural Networks Predictions"},{"paperId":"4f8d648c52edf74e41b0996128aa536e13cc7e82","externalIds":{"DBLP":"journals/ijsc/HaoZM16","DOI":"10.1142/S1793351X16500045","CorpusId":1779661},"title":"Deep Learning"},{"paperId":"44e5762b6ea8ddac549d47670d4c383350816856","externalIds":{"MAG":"88642373","DOI":"10.1007/978-3-642-32560-1_7","CorpusId":141217383},"title":"Artificial General Intelligence and the Human Mental Model"},{"paperId":"c01ac21ffffa17302a9a40971374c8c1045d78fa","externalIds":{"MAG":"2130369780","CorpusId":3228904},"title":"Explaining Recommendations: Satisfaction vs. Promotion"},{"paperId":"0a8149fb5aa8a5684e7d530c264451a5cb9250f5","externalIds":{"DBLP":"journals/deds/BartoM03","MAG":"1592847719","DOI":"10.1023/A:1022140919877","CorpusId":386824},"title":"Recent Advances in Hierarchical Reinforcement Learning"},{"paperId":"a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8","externalIds":{"DBLP":"journals/deds/BartoM03a","DOI":"10.1023/A:1025696116075","CorpusId":39988136},"title":"Recent Advances in Hierarchical Reinforcement Learning"},{"paperId":"97efafdb4a3942ab3efba53ded7413199f79c054","externalIds":{"MAG":"2121863487","DBLP":"journals/tnn/SuttonB98","DOI":"10.1109/TNN.1998.712192","CorpusId":60035920},"title":"Reinforcement Learning: An Introduction"},{"paperId":"352271eea6b402b8b2fd5a90279f64fc87347f3a","externalIds":{"DOI":"10.1038/331307c0","CorpusId":4311737},"title":"Optimization methods"},{"paperId":"9e16749652b4997d0dfdbb1a3ec6ce32b510e5b1","externalIds":{"MAG":"1580672106","DOI":"10.1016/S0049-237X(08)71097-8","CorpusId":86764460},"title":"An Introduction to First-Order Logic"}]}