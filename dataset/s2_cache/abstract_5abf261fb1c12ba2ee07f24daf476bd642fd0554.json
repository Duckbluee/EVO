{"abstract":"The detection of malicious deepfakes is a constantly evolving problem that requires continuous monitoring of de-tectors to ensure they can detect image manipulations gen-erated by the latest emerging models. In this paper, we in-vestigate the vulnerability of single-image deepfake detec-tors to black-box attacks created by the newest generation of generative methods, namely Denoising Diffusion Models (DDMs). Our experiments are run on FaceForensics++, a widely used deepfake benchmark consisting of manipulated images generated with various techniques for face iden-tity swapping and face reenactment. Attacks are crafted through guided reconstruction of existing deepfakes with a proposed DDM approach for face restoration. Our findings indicate that employing just a single denoising diffusion step in the reconstruction process of a deepfake can signif-icantly reduce the likelihood of detection, all without intro-ducing any perceptible image modifications. While training detectors using attack examples demonstrated some effectiveness, it was observed that discriminators trained on fully diffusion-based deepfakes exhibited limited generalizability when presented with our attacks."}