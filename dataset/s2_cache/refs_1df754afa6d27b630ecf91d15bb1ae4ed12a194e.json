{"references":[{"paperId":"995e469d8dc991c789c2d9be782c677d41cc733c","externalIds":{"DBLP":"journals/tits/WangJLLXWL23","DOI":"10.1109/TITS.2023.3275792","CorpusId":258900677},"title":"Decision-Making Driven by Driver Intelligence and Environment Reasoning for High-Level Autonomous Vehicles: A Survey"},{"paperId":"66e35501fab2ed0bb3c2fa2e7236367dee5fb288","externalIds":{"DBLP":"journals/corr/abs-2305-17678","ArXiv":"2305.17678","DOI":"10.48550/arXiv.2305.17678","CorpusId":258960556},"title":"Decoding the Underlying Meaning of Multimodal Hateful Memes"},{"paperId":"3d41d32ebd34969e13fcc96e6287fd113bc49887","externalIds":{"DBLP":"journals/corr/abs-2305-05964","ArXiv":"2305.05964","DOI":"10.48550/arXiv.2305.05964","CorpusId":258588437},"title":"Interpretable Multimodal Misinformation Detection with Logic Reasoning"},{"paperId":"3f758a13d3703b02bdf977f9189230276064da42","externalIds":{"DBLP":"journals/corr/abs-2305-03453","ArXiv":"2305.03453","DOI":"10.48550/arXiv.2305.03453","CorpusId":258546810},"title":"T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering"},{"paperId":"dbcc025b461e47a98bc4846e225173e21215df5d","externalIds":{"DBLP":"journals/corr/abs-2303-14348","ArXiv":"2303.14348","DOI":"10.1109/CVPR52729.2023.02236","CorpusId":257766783},"title":"Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style"},{"paperId":"a647ece46df6c6dc7837ce2b69774ae324cfbb6b","externalIds":{"ArXiv":"2303.14369","DBLP":"conf/cvpr/JinHXT0JY023","DOI":"10.1109/CVPR52729.2023.00244","CorpusId":257766265},"title":"Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning"},{"paperId":"3c45fd32b56efaa009a3ecef963d233dc5814194","externalIds":{"DBLP":"conf/cvpr/HsuM023","ArXiv":"2303.13483","DOI":"10.1109/CVPR52729.2023.00257","CorpusId":257687234},"title":"NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","externalIds":{"DBLP":"journals/corr/abs-2302-00923","ArXiv":"2302.00923","DOI":"10.48550/arXiv.2302.00923","CorpusId":256504063},"title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"90cd86b3c157e40cbaf1076f69cbd38d9c0781b9","externalIds":{"DBLP":"journals/mia/ZhanPWWLCW25","ArXiv":"2212.10729","DOI":"10.48550/arXiv.2212.10729","CorpusId":254926847,"PubMed":"39847954"},"title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"70feb009bc1e8b1cb8dff64bf9fd67789636438b","externalIds":{"DBLP":"conf/cvpr/Guo0LT0TH23","ArXiv":"2212.10846","DOI":"10.1109/CVPR52729.2023.01046","CorpusId":261081574},"title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models"},{"paperId":"bea07e85bebbf17486deeaa7c8ea4950cdf85d7d","externalIds":{"DBLP":"conf/taai/TsengLK22","DOI":"10.1109/TAAI57707.2022.00035","CorpusId":257408478},"title":"Relation-Aware Image Captioning for Explainable Visual Question Answering"},{"paperId":"4e6a2d863aeaafed82a8411f01be6e5a9f801b44","externalIds":{"DBLP":"journals/corr/abs-2211-16208","ArXiv":"2211.16208","DOI":"10.48550/arXiv.2211.16208","CorpusId":254069789},"title":"SLAN: Self-Locator Aided Network for Cross-Modal Understanding"},{"paperId":"af1c871282ec122869d03f5420ef5d9143358a91","externalIds":{"DBLP":"conf/cvpr/GuptaK23","ArXiv":"2211.11559","DOI":"10.1109/CVPR52729.2023.01436","CorpusId":253734854},"title":"Visual Programming: Compositional visual reasoning without training"},{"paperId":"34abded398688aaa6e2b2aaa7c3a7f5bbab92ddb","externalIds":{"DBLP":"conf/mm/XueQFX22","DOI":"10.1145/3503161.3548022","CorpusId":252782976},"title":"MMT: Image-guided Story Ending Generation with Multimodal Memory Transformer"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","externalIds":{"DBLP":"conf/nips/LuMX0CZTCK22","ArXiv":"2209.09513","DOI":"10.48550/arXiv.2209.09513","CorpusId":252383606},"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"3b8abd466697998c6e17df2cd30f48a7594d795b","externalIds":{"DBLP":"journals/csur/DwivediDNSRPQWS23","DOI":"10.1145/3561048","CorpusId":252051155},"title":"Explainable AI (XAI): Core Ideas, Techniques, and Solutions"},{"paperId":"9a66f6b5143d24aed9d3f4a8f13aa4e089f3a866","externalIds":{"ArXiv":"2207.07568","DBLP":"journals/corr/abs-2207-07568","DOI":"10.48550/arXiv.2207.07568","CorpusId":250607752},"title":"Reasoning about Actions over Visual and Linguistic Modalities: A Survey"},{"paperId":"25b1b2d95ee91139f697fc8960897c05f932abb3","externalIds":{"DBLP":"journals/pami/QianXFX23","DOI":"10.1109/TPAMI.2022.3188547","CorpusId":250281626,"PubMed":"35788462"},"title":"Integrating Multi-Label Contrastive Learning With Dual Adversarial Graph Neural Networks for Cross-Modal Retrieval"},{"paperId":"3b6a5088e3beeada39556cf0611048b5c2a7f88d","externalIds":{"DBLP":"conf/ijcai/LiFY22","DOI":"10.24963/ijcai.2022/447","CorpusId":250638186},"title":"Cross-modal Representation Learning and Relation Reasoning for Bidirectional Adaptive Manipulation"},{"paperId":"d0d9158f9a2096a01430799e23b90ebc8c1176b2","externalIds":{"ArXiv":"2207.00056","DBLP":"conf/iclr/LiangLCJD0MS23","CorpusId":257039090},"title":"MultiViz: Towards Visualizing and Understanding Multimodal Models"},{"paperId":"622428f5122ad12a40229e1768ecb929fd747ee7","externalIds":{"ArXiv":"2206.06488","DBLP":"journals/pami/XuZC23","DOI":"10.1109/TPAMI.2023.3275156","CorpusId":249642175,"PubMed":"37167049"},"title":"Multimodal Learning With Transformers: A Survey"},{"paperId":"7d57e3e332b6cdb22127f9fd738f3c464c6c52f3","externalIds":{"DBLP":"conf/cvpr/ZhangJZ22","DOI":"10.1109/CVPR52688.2022.01513","CorpusId":250184955},"title":"Query and Attention Augmentation for Knowledge-Based Explainable Reasoning"},{"paperId":"4da2b6b1677c0178416f3a613fcafdedf9ac25c4","externalIds":{"DBLP":"conf/sigir/YaoS0CH23","ArXiv":"2205.12487","DOI":"10.1145/3539618.3591879","CorpusId":249062580},"title":"End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models"},{"paperId":"74b3e774aba251986d0d545778aee359e8adc674","externalIds":{"DBLP":"conf/cvpr/GortiVMGVGY22","ArXiv":"2203.15086","DOI":"10.1109/CVPR52688.2022.00495","CorpusId":247778636},"title":"X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval"},{"paperId":"1911112ab501b7a60113a71273dcd8779a3c6a04","externalIds":{"DBLP":"journals/corr/abs-2203-09138","ArXiv":"2203.09138","DOI":"10.1109/CVPR52688.2022.00503","CorpusId":247518788},"title":"MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering"},{"paperId":"8c455bb530cc318f9bbbd68adc51181748e8c658","externalIds":{"DBLP":"journals/corr/abs-2203-06107","ArXiv":"2203.06107","DOI":"10.1109/CVPR52688.2022.01514","CorpusId":247411366},"title":"REX: Reasoning-aware and Grounded Explanation"},{"paperId":"a5bce173b77459793e1cbda9f9eefbca69af7767","externalIds":{"DBLP":"journals/tomccap/ChengZQWL22","DOI":"10.1145/3499027","CorpusId":247282961},"title":"Cross-modal Graph Matching Network for Image-text Retrieval"},{"paperId":"0fc5cd83fcc493a764b6b8ab8496a40f46d130d8","externalIds":{"DBLP":"journals/corr/abs-2203-02013","ArXiv":"2203.02013","DOI":"10.1145/3514094.3534148","CorpusId":247244569},"title":"DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations"},{"paperId":"02eae58e5ff7edb2f7cbb334e81c3af6b2768b59","externalIds":{"DBLP":"journals/inffus/MalkinskiM23","ArXiv":"2202.10284","DOI":"10.1016/j.inffus.2022.11.011","CorpusId":247012018},"title":"A Review of Emerging Research Directions in Abstract Visual Reasoning"},{"paperId":"6fca4e69d8d7fec0b1fe118e769017b3610d1bd5","externalIds":{"DBLP":"journals/corr/abs-2207-02127","ArXiv":"2207.02127","DOI":"10.1080/01691864.2022.2035253","CorpusId":247038603},"title":"A survey of multimodal deep generative models"},{"paperId":"455869f88df82b07ef7d5ab0dab5c28c6620daa1","externalIds":{"DBLP":"conf/cvpr/ChenAG22","ArXiv":"2202.01993","DOI":"10.1109/CVPR52688.2022.01851","CorpusId":246607802},"title":"Grounding Answers for Visual Questions Asked by Visually Impaired People"},{"paperId":"a07c55085bc02daa7e05857532a4a58939f9b14d","externalIds":{"DBLP":"journals/corr/abs-2112-15324","ArXiv":"2112.15324","DOI":"10.1609/aaai.v36i1.19983","CorpusId":245634574},"title":"Deconfounded Visual Grounding"},{"paperId":"7133d34e95af9122ece7614031a1c2875473c12c","externalIds":{"DBLP":"conf/bigdataconf/AmmarZHRMYS21","DOI":"10.1109/BigData52589.2021.9671303","CorpusId":245934971},"title":"SPACES: Explainable Multimodal AI for Active Surveillance, Diagnosis, and Management of Adverse Childhood Experiences (ACEs)"},{"paperId":"1e738e3087e5e660418d1a3879a7cba05248684b","externalIds":{"MAG":"3186595859","DBLP":"journals/eswa/SachanAYX21","DOI":"10.1016/J.ESWA.2021.115597","CorpusId":237656829},"title":"Evidential reasoning for preprocessing uncertain categorical data for trustworthy decisions: An application on healthcare and finance"},{"paperId":"9d38fe50ad4b4389273b682a802f77f4e2162143","externalIds":{"DBLP":"conf/mm/LiangLL00X21","DOI":"10.1145/3474085.3475190","CorpusId":239011968},"title":"Multi-Modal Sarcasm Detection with Interactive In-Modal and Cross-Modal Graphs"},{"paperId":"347a19a46147fdd77d1b46a3121a6dd3133d11b4","externalIds":{"DBLP":"conf/mm/ChengWYZZF21","DOI":"10.1145/3474085.3475677","CorpusId":239011621},"title":"Exploring Logical Reasoning for Referring Expression Comprehension"},{"paperId":"09f2b1f1bd313cf9183c138fca8f17bb228b4435","externalIds":{"DBLP":"journals/corr/abs-2110-02526","ArXiv":"2110.02526","DOI":"10.1109/CVPRW56347.2022.00502","CorpusId":238407862},"title":"Coarse-to-Fine Reasoning for Visual Question Answering"},{"paperId":"70017f5842ba41b00e447e79b9c50687eeb29481","externalIds":{"ArXiv":"2110.00804","DBLP":"conf/nips/ZhaoSCS21","CorpusId":238259858},"title":"ProTo: Program-Guided Transformer for Program-Guided Tasks"},{"paperId":"ed3f836976225c4637d08be3ef7cfdd3dde552f6","externalIds":{"DBLP":"conf/iccvw/ChenLP21a","DOI":"10.1109/ICCVW54120.2021.00441","CorpusId":244531482},"title":"Cross-modal Relational Reasoning Network for Visual Question Answering"},{"paperId":"45aebc07da279274a9a2c2fe1c864bd4d782cbf3","externalIds":{"DBLP":"conf/sigir/JinZZZHZ21","DOI":"10.1145/3404835.3462974","CorpusId":235792558},"title":"Hierarchical Cross-Modal Graph Consistency Learning for Video-Text Retrieval"},{"paperId":"9f82a887ebfb7e69705feff355012f26549d6a0b","externalIds":{"DBLP":"journals/tip/JiangZHCW21","DOI":"10.1109/TIP.2021.3089943","CorpusId":235610245,"PubMed":"34156941"},"title":"LayerCAM: Exploring Hierarchical Class Activation Maps for Localization"},{"paperId":"f46f77630b35a43e8c247916da5d809d6e5b4210","externalIds":{"MAG":"3167150513","DBLP":"journals/ivc/HeWMS21","DOI":"10.1016/J.IMAVIS.2021.104194","CorpusId":236277984},"title":"Interpretable visual reasoning: A survey"},{"paperId":"6e2a39601e2f215ea70d389ff3fed521c36b4794","externalIds":{"DBLP":"journals/corr/abs-2106-06509","ArXiv":"2106.06509","DOI":"10.24963/ijcai.2021/106","CorpusId":235417296},"title":"Step-Wise Hierarchical Alignment Network for Image-Text Matching"},{"paperId":"b78aa53020b70d7d30f31b15e3721c0a6891e703","externalIds":{"ACL":"2021.acl-long.43","ArXiv":"2105.13868","DBLP":"journals/corr/abs-2105-13868","DOI":"10.18653/v1/2021.acl-long.43","CorpusId":235247960},"title":"Learning Relation Alignment for Calibrated Cross-modal Retrieval"},{"paperId":"e7b04687dc41ea7b2b603a0a6149dc258321301a","externalIds":{"DBLP":"conf/aaai/QianXZFX21","DOI":"10.1609/aaai.v35i3.16345","CorpusId":235306301},"title":"Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval"},{"paperId":"50796b0f3edf9cb5ff1e447c298b33755378aa4f","externalIds":{"DBLP":"conf/acl/DuQLDQY022","ACL":"2022.acl-long.26","ArXiv":"2103.10360","DOI":"10.18653/v1/2022.acl-long.26","CorpusId":247519241},"title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling"},{"paperId":"87a5e46d9fa5f8546bfdb5d603e0ad0bdcd8adbc","externalIds":{"DBLP":"journals/tcsv/WenP21","MAG":"3021007069","DOI":"10.1109/TCSVT.2020.2991866","CorpusId":218928800},"title":"Multi-Level Knowledge Injecting for Visual Commonsense Reasoning"},{"paperId":"8acb379162b8e9a35a336cc99c04ec43c15db403","externalIds":{"DBLP":"journals/csr/KaurPM21","MAG":"3110846182","DOI":"10.1016/j.cosrev.2020.100336","CorpusId":230530536},"title":"Comparative analysis on cross-modal information retrieval: A review"},{"paperId":"d55705658df55e13709518a1e65247224349ffb9","externalIds":{"MAG":"3081876271","DBLP":"journals/ijon/LiangLYLJ21","DOI":"10.1016/j.neucom.2020.08.011","CorpusId":224899505},"title":"Explaining the black-box model: A survey of local interpretation methods for deep neural networks"},{"paperId":"2e89b9128fec87d8a25445c94910081284799d75","externalIds":{"ArXiv":"2010.12852","DBLP":"journals/corr/abs-2010-12852","MAG":"3094401045","DOI":"10.1109/CVPRW53098.2021.00178","CorpusId":225068216},"title":"Beyond VQA: Generating Multi-word Answers and Rationales to Visual Questions"},{"paperId":"04124bc9f1f04e52e6b44385b985d9598613f0b4","externalIds":{"DBLP":"journals/corr/abs-2010-08660","MAG":"3093463288","ArXiv":"2010.08660","DOI":"10.1109/MIC.2020.3031769","CorpusId":224704479},"title":"Semantics of the Black-Box: Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable?"},{"paperId":"c9940a17504a3b83bd1e9d613b095ddb204d2ad0","externalIds":{"MAG":"3105391665","DBLP":"conf/emnlp/MarasovicBPBSC20","ArXiv":"2010.07526","ACL":"2020.findings-emnlp.253","DOI":"10.18653/v1/2020.findings-emnlp.253","CorpusId":222379195},"title":"Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs"},{"paperId":"1fe32a88a2e4162f4b3fe73ffa1fcb120bd5b1bf","externalIds":{"DBLP":"journals/pami/DengWWHLT22","MAG":"3087365719","DOI":"10.1109/TPAMI.2020.3023438","CorpusId":52837841,"PubMed":"32956036"},"title":"Visual Grounding Via Accumulated Attention"},{"paperId":"f5d9f80456473dc56250459143f6956296383ae1","externalIds":{"DBLP":"conf/mm/JiangDQSY20","MAG":"3048602534","ArXiv":"2008.04858","DOI":"10.1145/3394171.3413826","CorpusId":221095743},"title":"KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning in Visual Dialogue"},{"paperId":"1187e4a921beee82ebd9f5f99a79f20397d3549d","externalIds":{"DBLP":"conf/iwssip/PadillaNS20","MAG":"3043995050","DOI":"10.1109/IWSSIP48289.2020.9145130","CorpusId":220734135},"title":"A Survey on Performance Metrics for Object-Detection Algorithms"},{"paperId":"68baac3ecbab44054aa03fa16bf70d53ae0b5840","externalIds":{"DOI":"10.1109/iwssip48289.2020","CorpusId":263859731},"title":"2020 International Conference on Systems, Signals and Image Processing (IWSSIP)"},{"paperId":"6b13065b4050800e30bb74e010b8aaba3355525d","externalIds":{"MAG":"3034972674","DBLP":"conf/ijcai/ZhuYWS0W20","ArXiv":"2006.09073","DOI":"10.24963/ijcai.2020/153","CorpusId":219708313},"title":"Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"0f75bb0439567abcef002fe4b42151b66af94c37","externalIds":{"MAG":"3022391993","ArXiv":"2005.00696","DBLP":"journals/corr/abs-2005-00696","ACL":"2020.findings-emnlp.172","DOI":"10.18653/v1/2020.findings-emnlp.172","CorpusId":218486839},"title":"Robust and Interpretable Grounding of Spatial References with Relation Networks"},{"paperId":"d1ac487f21829ef56c8ffdcd37ea414bce68c809","externalIds":{"DBLP":"journals/corr/abs-2004-14973","MAG":"3109097593","ArXiv":"2004.14973","DOI":"10.1007/978-3-030-58539-6_16","CorpusId":216868949},"title":"Improving Vision-and-Language Navigation with Image-Text Pairs from the Web"},{"paperId":"6f6d6da7b4c6219c55d0da09fd2b1f9809535d6d","externalIds":{"MAG":"2995120596","DBLP":"conf/iclr/SunWL20","CorpusId":209476572},"title":"Program Guided Agent"},{"paperId":"e4e90d80721fc24ac40f4711f8692aba08dc14e0","externalIds":{"MAG":"3012934141","DBLP":"journals/corr/abs-2003-10065","ArXiv":"2003.10065","CorpusId":214612170},"title":"Linguistically Driven Graph Capsule Network for Visual Question Reasoning"},{"paperId":"20dc158a6abd1f92a4534ae064d527821a91685d","externalIds":{"DBLP":"journals/corr/abs-2002-08325","MAG":"3007439415","ArXiv":"2002.08325","DOI":"10.1007/978-3-030-58589-1_23","CorpusId":211171653},"title":"VQA-LOL: Visual Question Answering under the Lens of Logic"},{"paperId":"9ebcac11090f3c5a7b987c668d91c3e5fec0718b","externalIds":{"DBLP":"journals/tmm/YuZLQHTW20","MAG":"3005881764","DOI":"10.1109/TMM.2020.2972830","CorpusId":214409728},"title":"Reasoning on the Relation: Enhancing Visual Representation for Visual Question Answering and Cross-Modal Retrieval"},{"paperId":"e9ae2f99dd2ae29f4bfd220446175bb854db2008","externalIds":{"PubMedCentral":"7805953","MAG":"2995503178","DBLP":"journals/firai/RileyS19","DOI":"10.3389/frobt.2019.00125","CorpusId":209149207,"PubMed":"33501140"},"title":"Integrating Non-monotonic Logical Reasoning and Inductive Learning With Deep Learning for Explainable Visual Question Answering"},{"paperId":"7c6d410891bef95ce4240eaa6d4908feb493527c","externalIds":{"DBLP":"conf/aaai/LiuWZH20","ArXiv":"1911.09042","MAG":"3037533539","DOI":"10.1609/aaai.v34i07.6833","CorpusId":208175619},"title":"Learning Cross-modal Context Graph for Visual Grounding"},{"paperId":"320464aa0231bc728c7d9ab7e71e552c12a7486b","externalIds":{"DBLP":"journals/corr/abs-1910-03230","ArXiv":"1910.03230","MAG":"2979729166","DOI":"10.1109/WACV48630.2021.00070","CorpusId":203902269},"title":"Meta Module Network for Compositional Visual Reasoning"},{"paperId":"4cede1c63336de84344922876e6ee23617e2afb3","externalIds":{"DBLP":"journals/corr/abs-1909-10128","MAG":"2974810665","ArXiv":"1909.10128","CorpusId":202719147},"title":"Explainable High-order Visual Question Reasoning: A New Benchmark and Knowledge-routed Network"},{"paperId":"26500af6bb97bf2dab1cc14dfb3b8b08fef67940","externalIds":{"MAG":"2988823324","DBLP":"journals/corr/abs-1909-02701","ArXiv":"1909.02701","DOI":"10.1109/ICCV.2019.00475","CorpusId":202234815},"title":"Visual Semantic Reasoning for Image-Text Matching"},{"paperId":"d0818dac77eee5b970736e57a478bcedfb1b15fe","externalIds":{"DBLP":"conf/aaai/ShahMYT19","MAG":"2966317026","DOI":"10.1609/aaai.v33i01.33018876","CorpusId":85558018},"title":"KVQA: Knowledge-Aware Visual Question Answering"},{"paperId":"5e59b5dedafb4f647f9cf5976b1f99eb1de12b77","externalIds":{"MAG":"2948506296","DBLP":"journals/corr/abs-1906-03952","ACL":"P19-2054","ArXiv":"1906.03952","DOI":"10.18653/v1/P19-2054","CorpusId":182953097},"title":"Multimodal Logical Inference System for Visual-Textual Entailment"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","externalIds":{"DBLP":"conf/cvpr/MarinoRFM19","ArXiv":"1906.00067","MAG":"2947312908","DOI":"10.1109/CVPR.2019.00331","CorpusId":173991173},"title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"0e09d47723cccf0a9bad7cb1badaa367ea37fe14","externalIds":{"MAG":"2971620415","ArXiv":"1905.00780","DBLP":"conf/nips/SrinivasF19","CorpusId":202539049},"title":"Full-Gradient Representation for Neural Network Visualization"},{"paperId":"d379ba96b8f400b23b2cd72c428af67e578959ea","externalIds":{"MAG":"3004349648","DBLP":"journals/corr/abs-1903-12314","ArXiv":"1903.12314","DOI":"10.1109/ICCV.2019.01041","CorpusId":88523817},"title":"Relation-Aware Graph Attention Network for Visual Question Answering"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","externalIds":{"MAG":"2950104027","DBLP":"conf/cvpr/HudsonM19","DOI":"10.1109/CVPR.2019.00686","CorpusId":152282269},"title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"cfc9ef5c7ef8056cff7bf1f1cfdd75e120f28231","externalIds":{"MAG":"2954367631","ArXiv":"1902.09487","DBLP":"journals/corr/abs-1902-09487","DOI":"10.1109/CVPR.2019.00209","CorpusId":67856593},"title":"MUREL: Multimodal Relational Reasoning for Visual Question Answering"},{"paperId":"ca1a2b86d39495be5524a0e39b663f7c423a0397","externalIds":{"MAG":"2903371384","ArXiv":"1812.01855","DBLP":"conf/cvpr/ShiZL19","DOI":"10.1109/CVPR.2019.00857","CorpusId":54458106},"title":"Explainable and Explicit Visual Reasoning Over Scene Graphs"},{"paperId":"f75f0750a00f6f85107985c70eca9c275b5e0962","externalIds":{"MAG":"3042113477","ArXiv":"1811.12354","DBLP":"conf/cvpr/ChenSMSA19","DOI":"10.1109/CVPR.2019.01282","CorpusId":54078068},"title":"TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments"},{"paperId":"f3d5130277fd028c0c9e621c73a4782621b14bf2","externalIds":{"MAG":"2903199940","DBLP":"conf/cvpr/0031WSH19","ArXiv":"1811.11903","DOI":"10.1109/CVPR.2019.00648","CorpusId":53955763},"title":"Visual Question Answering as Reading Comprehension"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","externalIds":{"MAG":"2958882215","ArXiv":"1811.10830","DBLP":"conf/cvpr/ZellersBFC19","DOI":"10.1109/CVPR.2019.00688","CorpusId":53734356},"title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"66ccd32be901da217799c78af229676418c7a882","externalIds":{"MAG":"3034726550","ArXiv":"1811.08481","DBLP":"journals/corr/abs-1811-08481","DOI":"10.1109/CVPR42600.2020.01039","CorpusId":53756412},"title":"VQA With No Questions-Answers Training"},{"paperId":"9d15ebe3f5aaf32a9f835f88703241461324c35b","externalIds":{"MAG":"2891021031","DBLP":"journals/corr/abs-1810-02338","ArXiv":"1810.02338","CorpusId":52919654},"title":"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"},{"paperId":"491d0101110fcacfad7c739d5fd807cf8b79de18","externalIds":{"ArXiv":"1809.02805","ACL":"W19-4812","DBLP":"journals/corr/abs-1809-02805","MAG":"2973009097","DOI":"10.18653/v1/W19-4812","CorpusId":52183384},"title":"Faithful Multimodal Explanation for Visual Question Answering"},{"paperId":"d88eb94d7054d2668b1a8dfa311721f37ae1f059","externalIds":{"MAG":"2950302449","DBLP":"conf/eccv/NarasimhanS18","ArXiv":"1809.01124","DOI":"10.1007/978-3-030-01237-3_28","CorpusId":52158102},"title":"Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering"},{"paperId":"c611b9c82e234b344a232bcbbe5436e06da69f0b","externalIds":{"MAG":"2883104598","DBLP":"journals/corr/abs-1807-08556","ArXiv":"1807.08556","DOI":"10.1007/978-3-030-01234-2_4","CorpusId":49908459},"title":"Explainable Neural Computation via Stack Neural Module Networks"},{"paperId":"6ac33d3dcecbed17580509a34bccdff2425f7ed8","externalIds":{"MAG":"2952646225","ArXiv":"1806.07243","DBLP":"journals/corr/abs-1806-07243","CorpusId":49317766},"title":"Learning Conditioned Graph Structures for Interpretable Visual Question Answering"},{"paperId":"d7701e78e0bfc92b03a89582e80cfb751ac03f26","externalIds":{"MAG":"2953315354","ArXiv":"1806.00069","DBLP":"conf/dsaa/GilpinBYBSK18","DOI":"10.1109/DSAA.2018.00018","CorpusId":59600034},"title":"Explaining Explanations: An Overview of Interpretability of Machine Learning"},{"paperId":"d2d79513f32c4d09b6255b18514d7ad07ebf43fe","externalIds":{"DBLP":"conf/mipro/DosilovicBH18","MAG":"2809925683","DOI":"10.23919/MIPRO.2018.8400040","CorpusId":49560076},"title":"Explainable artificial intelligence: A survey"},{"paperId":"06ba3492e3a9a2e98df2c81b91ec94787e3f97fb","externalIds":{"MAG":"2790777763","ArXiv":"1803.07464","DBLP":"conf/eccv/LiTJCL18","DOI":"10.1007/978-3-030-01234-2_34","CorpusId":4052735},"title":"VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions"},{"paperId":"0c41d7cfadcdaa5a66694bd57fa5eaf807043ef9","externalIds":{"MAG":"2950287127","ArXiv":"1803.08896","DBLP":"journals/corr/abs-1803-08896","DOI":"10.1609/aaai.v32i1.11324","CorpusId":4344713},"title":"Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering"},{"paperId":"ef153ece43ee50f8208f6197f0eaf3d324e4475b","externalIds":{"MAG":"2963609017","ArXiv":"1802.08129","DBLP":"conf/cvpr/ParkHARSDR18","DOI":"10.1109/CVPR.2018.00915","CorpusId":3604848},"title":"Multimodal Explanations: Justifying Decisions and Pointing to the Evidence"},{"paperId":"dd2f8bb5fa881797fad0448547e307a18bf897da","externalIds":{"MAG":"2964061310","ArXiv":"1801.09041","DBLP":"journals/corr/abs-1801-09041","ACL":"D18-1164","DOI":"10.18653/v1/D18-1164","CorpusId":5890185},"title":"Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions"},{"paperId":"b44999bb2e23cf8ca0a413a2d006cc9800794650","externalIds":{"DBLP":"conf/mm/ZhouJSWW17","MAG":"2766605158","DOI":"10.1145/3123266.3123335","CorpusId":27309478},"title":"More Than An Answer: Neural Pivot Network for Visual Qestion Answering"},{"paperId":"7cfa5c97164129ce3630511f639040d28db1d4b7","externalIds":{"DBLP":"journals/corr/abs-1709-07871","MAG":"2951555602","ArXiv":"1709.07871","DOI":"10.1609/aaai.v32i1.11671","CorpusId":19119291},"title":"FiLM: Visual Reasoning with a General Conditioning Layer"},{"paperId":"6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91","externalIds":{"ArXiv":"1705.09406","DBLP":"journals/pami/BaltrusaitisAM19","MAG":"2951127645","DOI":"10.1109/TPAMI.2018.2798607","CorpusId":10137425,"PubMed":"29994351"},"title":"Multimodal Machine Learning: A Survey and Taxonomy"},{"paperId":"a396a6febdacb84340d139096455e67049ac1e22","externalIds":{"MAG":"2951749065","ArXiv":"1704.05526","DBLP":"journals/corr/HuARDS17","DOI":"10.1109/ICCV.2017.93","CorpusId":18682},"title":"Learning to Reason: End-to-End Module Networks for Visual Question Answering"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","externalIds":{"MAG":"2962858109","DBLP":"conf/iccv/SelvarajuCDVPB17","ArXiv":"1610.02391","DOI":"10.1007/s11263-019-01228-7","CorpusId":15019293},"title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"b60630911d7746fba06de7c34abe98c9a61c6bcc","externalIds":{"MAG":"2964303913","ArXiv":"1606.05433","DBLP":"journals/corr/WangWSHD16","DOI":"10.1109/TPAMI.2017.2754246","CorpusId":7483388,"PubMed":"28945588"},"title":"FVQA: Fact-Based Visual Question Answering"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","externalIds":{"MAG":"2416885651","DBLP":"conf/cvpr/AndreasRDK16","ArXiv":"1511.02799","DOI":"10.1109/CVPR.2016.12","CorpusId":5276660},"title":"Neural Module Networks"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","externalIds":{"MAG":"2953106684","ArXiv":"1506.01497","DBLP":"journals/pami/RenHG017","DOI":"10.1109/TPAMI.2016.2577031","CorpusId":10328909,"PubMed":"27295650"},"title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"550f7dba7757f22afd87786749509574609d6180","externalIds":{"MAG":"2963572185","ArXiv":"1505.04406","DBLP":"journals/jmlr/BachBHG17","CorpusId":8492539},"title":"Hinge-Loss Markov Random Fields and Probabilistic Soft Logic"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","externalIds":{"DBLP":"journals/corr/AntolALMBZP15","MAG":"1933349210","ArXiv":"1505.00468","DOI":"10.1007/s11263-016-0966-6","CorpusId":3180429},"title":"VQA: Visual Question Answering"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"adb9acaf9184bdbd23105f1a383848eed9bc82fc","externalIds":{"DBLP":"journals/corr/abs-2305-16582","DOI":"10.48550/arXiv.2305.16582","CorpusId":258947684},"title":"Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","externalIds":{"DBLP":"journals/corr/abs-2305-14999","DOI":"10.48550/arXiv.2305.14999","CorpusId":258865297},"title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"28d5bbdd730424b63811f30f7517060fbab3057c","externalIds":{"ACL":"2022.aacl-main.9","DBLP":"conf/ijcnlp/LiuCDF22","DOI":"10.18653/v1/2022.aacl-main.9","CorpusId":253762087},"title":"WAX: A New Dataset for Word Association eXplanations"},{"paperId":"8bb041f47a100016be18d14e21343ac4afa8d2af","externalIds":{"MAG":"3176896951","DBLP":"journals/tmm/ZhangZX22","DOI":"10.1109/tmm.2021.3091882","CorpusId":237982826},"title":"Explicit Cross-Modal Representation Learning for Visual Commonsense Reasoning"},{"paperId":"4774432f02ef4c5285952dd8c7daff0852c3a601","externalIds":{"MAG":"2600463316","CorpusId":195944134},"title":"Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization"},{"paperId":"c87a36bad270f9b3f38e09148c71107fc0a38952","externalIds":{"MAG":"2035004935","DOI":"10.2307/2312443","CorpusId":121469356},"title":"Theory of formal systems"}]}