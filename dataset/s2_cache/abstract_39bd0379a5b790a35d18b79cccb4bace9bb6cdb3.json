{"abstract":"Domain adaptation assumes that samples from source and target domains are freely accessible during a training phase. However, such an assumption is rarely plausible in the real world and possibly causes data privacy issues, especially when the label of the source domain can be a sensitive attribute as an identifier. To avoid accessing source data that could contain sensitive information, we introduce source data free domain adaptation (SFDA). Our key idea is to leverage a pretrained model from the source domain and progressively update the target model in a self-learning manner. We observe that target samples with lower self-entropy measured by the pretrained source model are more likely to be classified correctly. From this, we select the reliable samples with the self-entropy criterion and define these as class prototypes. We then assign pseudolabels for every target sample based on the similarity score with class prototypes. We further propose point-to-set distance-based filtering, which does not require any tunable hyperparameters to reduce uncertainty from the pseudolabeling process. Finally, we train the target model with the filtered pseudolabels with regularization from the pretrained source model. Surprisingly, without direct usage of labeled source samples, our SFDA outperforms conventional domain adaptation methods on benchmark datasets. Impact Statementâ€”This study addresses the data privacy issue, especially in unsupervised domain adaptation. Based on our privacy-preserving domain adaptation, various stakeholders, including enterprises and government organizations, can be free of concern about privacy issues with their labeled source dataset. Furthermore, the proposed data-free approach can contribute to creating a positive social impact, especially in large-scale datasets. Recently, since the size of data across various fields has been scaling up, it is almost incapable for individual researchers to directly utilize a large scale of data during training. For this reason, a new social trend of sharing pretrained models, e.g., EfficientNet and BERT, led by global enterprises with their huge amount of resources has been rising up. From this viewpoint, our approach thus enables more people to participate in the domain adaptation field specifically when the source data are large scale and contain sensitive attributes."}