{"abstract":"We study the problem of joint question answering (QA) and question generation (QG) in this paper. \nOur intuition is that QA and QG have intrinsic connections and these two tasks could improve each other. \nOn one side, the QA model judges whether the generated question of a QG model is relevant to the answer. \nOn the other side, the QG model provides the probability of generating a question given the answer, which is a useful evidence that in turn facilitates QA. \nIn this paper we regard QA and QG as dual tasks. \nWe propose a training framework that trains the models of QA and QG simultaneously, and explicitly leverages their probabilistic correlation to guide the training process of both models. \nWe implement a QG model based on sequence-to-sequence learning, and a QA model based on recurrent neural network. \nAs all the components of the QA and QG models are differentiable, all the parameters involved in these two models could be conventionally learned with back propagation. \nWe conduct experiments on three datasets. Empirical results show that our training framework improves both QA and QG tasks. \nThe improved QA model performs comparably with strong baseline approaches on all three datasets."}