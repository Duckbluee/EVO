{"references":[{"paperId":"b1e3eaa8d51d53037d01ebb7805d882e5e7860b1","externalIds":{"ArXiv":"2403.19113","DBLP":"journals/corr/abs-2403-19113","DOI":"10.48550/arXiv.2403.19113","CorpusId":268733092},"title":"FACTOID: FACtual enTailment fOr hallucInation Detection"},{"paperId":"982bc3b4025e971f95d8ba071f7bb667cc71e7a4","externalIds":{"DBLP":"conf/acl/WangPDB24","ArXiv":"2403.18715","DOI":"10.48550/arXiv.2403.18715","CorpusId":268724017},"title":"Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding"},{"paperId":"032cbe48a88325c50ad9bc73e8d5f000fb84ffe7","externalIds":{"DBLP":"journals/corr/abs-2403-17306","ArXiv":"2403.17306","DOI":"10.48550/arXiv.2403.17306","CorpusId":268691502},"title":"Visual Hallucination: Definition, Quantification, and Prescriptive Remediations"},{"paperId":"0daceeb8f42ad6ec846f80f7407500192d2046ee","externalIds":{"DBLP":"conf/cvpr/LiuFXXSLK025","ArXiv":"2403.11116","DOI":"10.1109/CVPR52734.2025.01849","CorpusId":268513094},"title":"PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset"},{"paperId":"26101ff8b85a4d9004ed2937ba1335a6205d5dd9","externalIds":{"ArXiv":"2402.06986","DBLP":"journals/taslp/ZhuDD24","DOI":"10.1109/TASLP.2024.3485170","CorpusId":267627825},"title":"Cacophony: An Improved Contrastive Audio-Text Model"},{"paperId":"1cfb7fba7194860e8b8818eb5e87e0a8e14e518a","externalIds":{"DBLP":"conf/eccv/YanBCZHL24","ArXiv":"2402.06118","DOI":"10.48550/arXiv.2402.06118","CorpusId":267616939},"title":"ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling"},{"paperId":"18acf055db0d92f1e934ebb59c3d29d41ce840c5","externalIds":{"DBLP":"conf/interspeech/DeshmukhAEGISRW24","ArXiv":"2402.00282","DOI":"10.48550/arXiv.2402.00282","CorpusId":267365179},"title":"PAM: Prompting Audio-Language Models for Audio Quality Assessment"},{"paperId":"b939d40a8adec9908618fa87098df8387eacf18f","externalIds":{"DBLP":"conf/icassp/KimJLW24","ArXiv":"2401.17690","DOI":"10.1109/ICASSP48485.2024.10446672","CorpusId":267335272},"title":"EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning"},{"paperId":"574c634e4bd28d16d9c8206ffac7ec7566b6b8ff","externalIds":{"ArXiv":"2401.09774","DBLP":"journals/corr/abs-2401-09774","DOI":"10.48550/arXiv.2401.09774","CorpusId":267034899},"title":"On the Audio Hallucinations in Large Audio-Video Language Models"},{"paperId":"ca00f4056f9039d3c1a4c3a113f5ee0527149b66","externalIds":{"ArXiv":"2401.06209","DBLP":"journals/corr/abs-2401-06209","DOI":"10.1109/CVPR52733.2024.00914","CorpusId":266976992},"title":"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs"},{"paperId":"0725debf3183589626823dbb64107bba8ed22448","externalIds":{"ArXiv":"2401.01596","DBLP":"journals/corr/abs-2401-01596","DOI":"10.48550/arXiv.2401.01596","CorpusId":266741554},"title":"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries"},{"paperId":"f2b6323973955f9a1ebb9be76a616991de3d3a8f","externalIds":{"DBLP":"journals/corr/abs-2312-15915","ArXiv":"2312.15915","DOI":"10.48550/arXiv.2312.15915","CorpusId":266550948},"title":"ChartBench: A Benchmark for Complex Visual Reasoning in Charts"},{"paperId":"36cbea6919cf01f3e5488b160726b01f422b4b93","externalIds":{"DBLP":"conf/aaai/GhoshAJ0CS24","ArXiv":"2312.11541","DOI":"10.48550/arXiv.2312.11541","CorpusId":266362506},"title":"CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare"},{"paperId":"49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c","externalIds":{"DBLP":"journals/corr/abs-2311-17911","ArXiv":"2311.17911","DOI":"10.1109/CVPR52733.2024.01274","CorpusId":265498818},"title":"OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation"},{"paperId":"328eb183007bf4aefbf42437b42a15db375803e3","externalIds":{"DBLP":"conf/cvpr/LengZCLLMB24","ArXiv":"2311.16922","DOI":"10.1109/CVPR52733.2024.01316","CorpusId":265466833},"title":"Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding"},{"paperId":"2baf63dede1a96cae314c4be99bd3cf9f49b148e","externalIds":{"DBLP":"journals/corr/abs-2311-16839","ArXiv":"2311.16839","DOI":"10.48550/arXiv.2311.16839","CorpusId":265466428},"title":"Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization"},{"paperId":"5838b56f2c7ca3dd946428dae07bdc26a9265c67","externalIds":{"ArXiv":"2311.15548","DBLP":"journals/corr/abs-2311-15548","DOI":"10.48550/arXiv.2311.15548","CorpusId":265456905},"title":"Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination"},{"paperId":"4883fb9c38cfaad5efe81eecbedd579d186e9758","externalIds":{"ArXiv":"2311.10961","DBLP":"conf/wsdm/Roychowdhury24","DOI":"10.1145/3616855.3635737","CorpusId":265294746},"title":"Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers"},{"paperId":"404b562926c2bccd23c9366a9afef4e481a5b2c1","externalIds":{"DBLP":"journals/corr/abs-2311-04480","ArXiv":"2311.04480","DOI":"10.48550/arXiv.2311.04480","CorpusId":265050751},"title":"CLearViD: Curriculum Learning for Video Description"},{"paperId":"b7fc130764dc684f9376bff339c423a73b2e55b5","externalIds":{"DBLP":"conf/iclr/GhoshSKTESSNDM24","ArXiv":"2310.08753","DOI":"10.48550/arXiv.2310.08753","CorpusId":264128411},"title":"CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models"},{"paperId":"495c36dba73fa97e2f742827c03c3d61a83b3a01","externalIds":{"DBLP":"conf/emnlp/YangS023","ArXiv":"2310.06498","DOI":"10.48550/arXiv.2310.06498","CorpusId":263830009},"title":"A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection"},{"paperId":"be2b0125f3161739c685e9f86d9fd49f9f6d99c8","externalIds":{"DBLP":"journals/corr/abs-2311-01463","ArXiv":"2311.01463","DOI":"10.20944/preprints202310.1662.v1","CorpusId":265019447},"title":"Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI"},{"paperId":"c1e450284e7d6cac1855330a1197df8537df653f","externalIds":{"DBLP":"journals/corr/abs-2309-15112","ArXiv":"2309.15112","DOI":"10.48550/arXiv.2309.15112","CorpusId":262824937},"title":"InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","externalIds":{"DBLP":"journals/corr/abs-2309-14525","ArXiv":"2309.14525","DOI":"10.48550/arXiv.2309.14525","CorpusId":262824780},"title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"db72305f709bb73e8d641ef2d71bca7c7fc950fc","externalIds":{"DBLP":"conf/icassp/YuanLLHP024","ArXiv":"2309.08051","DOI":"10.1109/ICASSP48485.2024.10447898","CorpusId":262012867},"title":"Retrieval-Augmented Text-to-Audio Generation"},{"paperId":"71bc0c97c20fffce796a355b16bd202987260029","externalIds":{"ArXiv":"2309.05922","DBLP":"journals/corr/abs-2309-05922","DOI":"10.48550/arXiv.2309.05922","CorpusId":261696947},"title":"A Survey of Hallucination in Large Foundation Models"},{"paperId":"329960d5f4287077cdc258d49a66bdf277c1ee29","externalIds":{"DBLP":"conf/icassp/ElizaldeDW24","ArXiv":"2309.05767","DOI":"10.1109/ICASSP48485.2024.10448504","CorpusId":261696949},"title":"Natural Language Supervision For General-Purpose Audio Representations"},{"paperId":"73814a52609a9ee4c8f1b115e376b6a300ab6a57","externalIds":{"ArXiv":"2309.02301","DBLP":"journals/corr/abs-2309-02301","DOI":"10.48550/arXiv.2309.02301","CorpusId":261557047},"title":"CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning"},{"paperId":"2928e5a5ee488104c5d7b636f577baef2d470310","externalIds":{"DBLP":"journals/corr/abs-2308-16890","ArXiv":"2308.16890","DOI":"10.48550/arXiv.2308.16890","CorpusId":261397179},"title":"TouchStone: Evaluating Vision-Language Models by Language Models"},{"paperId":"e47d276bad18f441950c8136672ae6864e95323f","externalIds":{"ArXiv":"2308.12714","DBLP":"conf/aaai/WangWHPZZDLLWH24","DOI":"10.48550/arXiv.2308.12714","CorpusId":261100735},"title":"VIGC: Visual Instruction Generation and Correction"},{"paperId":"374ebdc8240a35820cb7ab8bfca37e180e21b605","externalIds":{"DBLP":"journals/corr/abs-2308-12792","ArXiv":"2308.12792","DOI":"10.48550/arXiv.2308.12792","CorpusId":261100979},"title":"Sparks of Large Audio Models: A Survey and Outlook"},{"paperId":"da96ec9c32d63292e506ba8f8ea8e838df998c02","externalIds":{"DBLP":"journals/corr/abs-2308-10253","ArXiv":"2308.10253","DOI":"10.48550/arXiv.2308.10253","CorpusId":261049617},"title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"},{"paperId":"464edfd902f652d3ab6a25dbb6d9fa47cc3246a9","externalIds":{"ArXiv":"2308.01546","DBLP":"journals/corr/abs-2308-01546","DOI":"10.1109/ICASSP48485.2024.10447265","CorpusId":260438807},"title":"MusicLDM: Enhancing Novelty in text-to-music Generation Using Beat-Synchronous mixup Strategies"},{"paperId":"ce9c0935c074a0ca8769f13fd4e8651cee263112","externalIds":{"DBLP":"conf/ismir/DohCLN23","ArXiv":"2307.16372","DOI":"10.48550/arXiv.2307.16372","CorpusId":260334763},"title":"LP-MusicCaps: LLM-Based Pseudo Music Captioning"},{"paperId":"7a5b44ea10a51708e18786595c8d70b18950da11","externalIds":{"DBLP":"journals/corr/abs-2307-13528","ArXiv":"2307.13528","DOI":"10.48550/arXiv.2307.13528","CorpusId":260154834},"title":"FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"},{"paperId":"a72975eb88eb31f193e9587e7415cb04e7bcdbee","externalIds":{"ACL":"2024.eacl-long.4","DBLP":"conf/eacl/MuhlgayRMLRBALSS24","ArXiv":"2307.06908","DOI":"10.48550/arXiv.2307.06908","CorpusId":259847758},"title":"Generating Benchmarks for Factuality Evaluation of Language Models"},{"paperId":"1827dd28ef866eaeb929ddf4bcfa492880aba4c7","externalIds":{"DBLP":"journals/corr/abs-2307-03987","ArXiv":"2307.03987","DOI":"10.48550/arXiv.2307.03987","CorpusId":263699899},"title":"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","externalIds":{"ArXiv":"2306.14565","DBLP":"conf/iclr/LiuLLWYW24","CorpusId":259251834},"title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"2a68cfffde314b717ca3fc4bd3ffab597f1b6ea9","externalIds":{"ArXiv":"2306.09525","DBLP":"journals/corr/abs-2306-09525","DOI":"10.48550/arXiv.2306.09525","CorpusId":259187949},"title":"Explaining Legal Concepts with Augmented Large Language Models (GPT-4)"},{"paperId":"405f8f5f1c6df1b3343c812832479aad5180b65f","externalIds":{"ArXiv":"2306.03341","DBLP":"journals/corr/abs-2306-03341","CorpusId":259088877},"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"},{"paperId":"44d74b0d77b4056ddd4c6611a76711c8bab2e0a7","externalIds":{"DBLP":"conf/icaa2/JhaJLBVN23","DOI":"10.1109/ICAA58325.2023.00029","CorpusId":260810131},"title":"Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting"},{"paperId":"8a172a13faef344fc1f9fb34681c93ab79b8d736","externalIds":{"DBLP":"journals/tcsv/HeDWDXW23","DOI":"10.1109/TCSVT.2022.3225549","CorpusId":254341992},"title":"Multi-Granularity Aggregation Transformer for Joint Video-Audio-Text Representation Learning"},{"paperId":"6b8ee194d3d2fc90829b3d1ef615f54b12adfbc1","externalIds":{"DOI":"10.1117/12.2681586","CorpusId":259032961},"title":"A context-aware model with a pre-trained context encoder for dense video captioning"},{"paperId":"3dbbe6909d7b53dd49e059c7f61a3613045a8db0","externalIds":{"DBLP":"conf/iclr/MundlerHJV24","ArXiv":"2305.15852","DOI":"10.48550/arXiv.2305.15852","CorpusId":258887694},"title":"Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation"},{"paperId":"7db7653c581d7823cb9c328f2d742ec70d7a0ce4","externalIds":{"DBLP":"journals/corr/abs-2305-14908","ArXiv":"2305.14908","DOI":"10.48550/arXiv.2305.14908","CorpusId":258865339},"title":"PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions"},{"paperId":"bd5deadc58ee45b5e004378ba1d54a96bc947b4a","externalIds":{"ArXiv":"2305.14251","DBLP":"conf/emnlp/MinKLLYKIZH23","DOI":"10.48550/arXiv.2305.14251","CorpusId":258841470},"title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"},{"paperId":"2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4","externalIds":{"DBLP":"journals/corr/abs-2305-14552","ArXiv":"2305.14552","DOI":"10.48550/arXiv.2305.14552","CorpusId":258865517},"title":"Sources of Hallucination by Large Language Models on Inference Tasks"},{"paperId":"6825ba09383bc758f9a2feaebabe35a6cd4adc4c","externalIds":{"DBLP":"conf/icml/ZhangPMLS24","ArXiv":"2305.13534","DOI":"10.48550/arXiv.2305.13534","CorpusId":258841857},"title":"How Language Model Hallucinations Can Snowball"},{"paperId":"f2b34c5bc6b20353aa531f860aefeadf8d96e3be","externalIds":{"ArXiv":"2304.14406","DBLP":"conf/cvpr/KulalBA0YLES23","DOI":"10.1109/CVPR52729.2023.01639","CorpusId":258352521},"title":"Putting People in Their Place: Affordance-Aware Human Insertion into Scenes"},{"paperId":"f406aceba4f29cc7cfbe7edb2f52f01374486589","externalIds":{"ArXiv":"2304.13734","DBLP":"journals/corr/abs-2304-13734","DOI":"10.18653/v1/2023.findings-emnlp.68","CorpusId":258352729},"title":"The Internal State of an LLM Knows When its Lying"},{"paperId":"f51bc74814a3452009ea5ca262d9768d08149ee6","externalIds":{"ArXiv":"2304.13731","DBLP":"journals/corr/abs-2304-13731","DOI":"10.48550/arXiv.2304.13731","CorpusId":258352270},"title":"Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model"},{"paperId":"00c367427d9135209d84008e6cb5e90f0adba881","externalIds":{"ArXiv":"2304.09116","DBLP":"conf/iclr/ShenJ0LL00Z024","DOI":"10.48550/arXiv.2304.09116","CorpusId":258187322},"title":"NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers"},{"paperId":"44bab2836177f8bf9775e7ca536b8e200757aac7","externalIds":{"DBLP":"journals/tacl/GuerreiroAWHBCM23","ArXiv":"2303.16104","DOI":"10.1162/tacl_a_00615","CorpusId":257771892},"title":"Hallucinations in Large Multilingual Translation Models"},{"paperId":"96e1c2cf427cf7eb3e360fdfb8b39fb8f2a9be4b","externalIds":{"DBLP":"journals/corr/abs-2303-07902","ArXiv":"2303.07902","DOI":"10.1145/3581783.3613820","CorpusId":257505109},"title":"BLAT: Bootstrapping Language-Audio Pre-training based on AudioSet Tag-guided Synthetic Data"},{"paperId":"01bc871c0ecb7f586e54b9f67d2e50e08cbedf3d","externalIds":{"DBLP":"conf/emnlp/Liu023","ArXiv":"2303.02961","DOI":"10.48550/arXiv.2303.02961","CorpusId":257364802},"title":"Models See Hallucinations: Evaluating the Factuality in Video Captioning"},{"paperId":"e5c72b92c48d68594b290c84a8904da7c8335554","externalIds":{"ArXiv":"2302.12813","DBLP":"journals/corr/abs-2302-12813","DOI":"10.48550/arXiv.2302.12813","CorpusId":257205781},"title":"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback"},{"paperId":"66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e","externalIds":{"ArXiv":"2210.08726","DBLP":"conf/acl/GaoDPCCFZLLJG23","ACL":"2023.acl-long.910","DOI":"10.18653/v1/2023.acl-long.910","CorpusId":254247260},"title":"RARR: Researching and Revising What Language Models Say, Using Language Models"},{"paperId":"7cfbd36c0043098589cbaf18dca2b41d8dc24abe","externalIds":{"DBLP":"conf/eacl/DaiLJSF23","ACL":"2023.eacl-main.156","ArXiv":"2210.07688","DOI":"10.48550/arXiv.2210.07688","CorpusId":252907639},"title":"Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training"},{"paperId":"4175003efc9cbf2781c0fd8ef8e6bcb756316296","externalIds":{"ArXiv":"2206.07696","DBLP":"journals/corr/abs-2206-07696","DOI":"10.48550/arXiv.2206.07696","CorpusId":249674747},"title":"Diffusion Models for Video Prediction and Infilling"},{"paperId":"50b17fc34d4ff0b6c64a24a934132ac954c4c2ce","externalIds":{"ArXiv":"2206.00100","DBLP":"journals/corr/abs-2206-00100","DOI":"10.1109/CVPR52688.2022.00515","CorpusId":247936049},"title":"VALHALLA: Visual Hallucination for Machine Translation"},{"paperId":"ecfe76d2dddfd98f9905b5c3497e5466edd7adcd","externalIds":{"DBLP":"journals/corr/abs-2111-08919","ArXiv":"2111.08919","DOI":"10.1109/CVPR52688.2022.01740","CorpusId":244270365},"title":"EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching"},{"paperId":"9c58df84d6eddd4830a430dc5e164429cf89a79a","externalIds":{"DBLP":"conf/dcase/YeWYZ21","ArXiv":"2110.06100","CorpusId":238634813},"title":"Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Semantic Information"},{"paperId":"1489591dd6a7ffb5eeb752a317c456376e7a777b","externalIds":{"DBLP":"journals/corr/abs-2108-01912","ArXiv":"2108.01912","DOI":"10.1109/ICCV48922.2021.01431","CorpusId":236912633},"title":"Internal Video Inpainting by Implicit Long-range Propagation"},{"paperId":"22c53a52c54cbd5238ac7e0cca057b236ef6d7b9","externalIds":{"DBLP":"journals/corr/abs-2106-04144","ArXiv":"2106.04144","DOI":"10.1109/WACV51458.2022.00390","CorpusId":235367757},"title":"Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"47bd0208f1db82786a74175c6e3082db0e1eeeb5","externalIds":{"MAG":"2997296614","DBLP":"conf/aaai/SuinR20","DOI":"10.1609/AAAI.V34I07.6881","CorpusId":214427672},"title":"An Efficient Framework for Dense Video Captioning"},{"paperId":"3ba94f4dd7db8c697401aa54e63ad318423fc83d","externalIds":{"ArXiv":"2003.07758","MAG":"3034815696","DBLP":"journals/corr/abs-2003-07758","DOI":"10.1109/CVPRW50498.2020.00487","CorpusId":212737076},"title":"Multi-modal Dense Video Captioning"},{"paperId":"0a0b3e80fcc5715b583a7069f4e5719ecfee7b3d","externalIds":{"MAG":"2959133507","DBLP":"journals/access/JiangFTLZ19","DOI":"10.1109/ACCESS.2019.2927384","CorpusId":198146801},"title":"Parallelized Convolutional Recurrent Neural Network With Spectral Features for Speech Emotion Recognition"},{"paperId":"c5a757427132fda0c66e18a0d059eca8e2472d13","externalIds":{"MAG":"2927018329","ArXiv":"1904.03870","DBLP":"journals/corr/abs-1904-03870","DOI":"10.1109/CVPR.2019.00675","CorpusId":102351043},"title":"Streamlined Dense Video Captioning"},{"paperId":"464ba7b06f38f245eb9ce154b77bc4508ba79c5f","externalIds":{"PubMedCentral":"6212510","MAG":"2898492499","DOI":"10.3389/fpsyg.2018.01958","CorpusId":53029373,"PubMed":"30416463"},"title":"Content Matters, a Qualitative Analysis of Verbal Hallucinations"},{"paperId":"e10a5e0baf2aa87d804795af071808a9377cc80a","externalIds":{"MAG":"2784025607","DBLP":"conf/aaai/ZhouXC18","ArXiv":"1703.09788","DOI":"10.1609/aaai.v32i1.12342","CorpusId":19713015},"title":"Towards Automatic Learning of Procedures From Web Instructional Videos"},{"paperId":"1291371fcfe04198bd2bf72fb4656fbb7deb1450","externalIds":{"MAG":"1569365875","DBLP":"conf/acivs/ZhangZW06","DOI":"10.1007/11864349_47","CorpusId":44952479},"title":"Video-Based Facial Expression Hallucination: A Two- Level Hierarchical Fusion Approach"},{"paperId":"a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824","externalIds":{"DBLP":"journals/corr/abs-2402-08680","DOI":"10.48550/arXiv.2402.08680","CorpusId":267636873},"title":"Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance"},{"paperId":"e17db9e4975bd4d3c078f7294272e1af3c5fc932","externalIds":{"DBLP":"conf/acl/GhoshTT0SS24","DOI":"10.18653/v1/2024.acl-long.708","CorpusId":271923579},"title":"From Sights to Insights: Towards Summarization of Multimodal Clinical Documents"},{"paperId":"aa7e3b2b88ae3b434e3d5a343c7e3109c9d48ea8","externalIds":{"DBLP":"conf/emnlp/JiYXLIF23","DOI":"10.18653/v1/2023.findings-emnlp.123","CorpusId":266176951},"title":"Towards Mitigating LLM Hallucination via Self Reflection"},{"paperId":"45a0c2ce3bbf70dfa1f6af8f80689a3485e1351f","externalIds":{"DBLP":"journals/corr/abs-2310-06594","DOI":"10.48550/arXiv.2310.06594","CorpusId":270227411},"title":"REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning Datasets"},{"paperId":"80fd20e175f83a699258b8780cf365418d1538b0","externalIds":{"DBLP":"journals/corr/abs-2305-13269","DOI":"10.48550/arXiv.2305.13269","CorpusId":258833025},"title":"Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases"},{"paperId":"3ffa598a32ed1e807b6ad975e33e395bb1706c4b","externalIds":{"DBLP":"conf/dcase/HanYLLY21","CorpusId":245426648},"title":"Automated Audio Captioning with Weakly Supervised Pre-Training and Word Selection Methods"}]}