{"abstract":"Sequential recommendation aims to predict the next item of interest for a user, based on her/his interaction history. In conventional sequential recommenders, a common approach is to learn sequence representations based on ID embeddings of items, which can be leveraged to predict the subsequent items of interest. Clearly, the sequence representations encode user behavioral patterns, which are critical to recommendation. Inspired by recent success in empowering large language models (LLMs) to understand diverse modality (e.g., image, audio), a compelling question arises: “Can LLMs understand and utilize representations from conventional recommenders?”. To answer this, we propose RecInterpreter, which examines the capacity of LLMs to decipher the representation space of pretrained recommenders. Specifically, with the multimodal pairs (i.e., interaction sequence representations and text narrations), RecInterpreter first uses a lightweight projector to map the representations into the token embedding space of the LLM, encouraging LLM to generate textual narrations for items within the sequence. Furthermore, upon interpreting recommenders, LLM can enhance its recommendation capabilities through fine-tuning with the projected representations, even without textual description of interaction sequences. Experiments showcase that RecInterpreter enhances LLMs to understand hidden representations from ID-based sequential recommenders and better accomplish recommendation task with the explicitly understanding of behavior patterns."}