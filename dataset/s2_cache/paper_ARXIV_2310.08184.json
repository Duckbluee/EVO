{"paperId":"b8e9716cfb5ad9309210f26198743d295371b7d2","externalIds":{"ArXiv":"2310.08184","DBLP":"journals/natmi/ZhengSTLHDWT25","DOI":"10.1038/s42256-024-00961-0","CorpusId":263908791},"title":"Learning from models beyond fine-tuning","openAccessPdf":{"url":"","status":"CLOSED","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2310.08184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2257435586","name":"Hongling Zheng"},{"authorId":"2248152216","name":"Li Shen"},{"authorId":"2178366354","name":"A. Tang"},{"authorId":"2150649639","name":"Yong Luo"},{"authorId":"2247556302","name":"Han Hu"},{"authorId":"2212029373","name":"Bo Du"},{"authorId":"2264395017","name":"Yonggang Wen"},{"authorId":"2135519749","name":"Dacheng Tao"}],"abstract":"Foundation models have demonstrated remarkable performance across various tasks, primarily due to their abilities to comprehend instructions and access extensive, high-quality data. These capabilities showcase the effectiveness of current foundation models and suggest a promising trajectory. Owing to multiple constraints, such as the extreme scarcity or inaccessibility of raw data used to train foundation models and the high cost of training large-scale foundation models from scratch, the use of pre-existing foundation models or application programming interfaces for downstream tasks has become a new research trend, which we call Learn from Model (LFM). LFM involves extracting and leveraging prior knowledge from foundation models through fine-tuning, editing and fusion methods and applying it to downstream tasks. We emphasize that maximizing the use of parametric knowledge in data-scarce scenarios is critical to LFM. Analysing the LFM paradigm can guide the selection of the most appropriate technology in a given scenario to minimize parameter storage and computational costs while improving the performance of foundation models on new tasks. This Review provides a comprehensive overview of current methods based on foundation models from the perspective of LFM. Large general-purpose models are becoming more prevalent and useful, but also harder to train and find suitable training data for. Zheng et al. discuss how models can be used to train other models."}