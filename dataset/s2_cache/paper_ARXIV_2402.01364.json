{"paperId":"bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e","externalIds":{"ArXiv":"2402.01364","DBLP":"journals/corr/abs-2402-01364","DOI":"10.48550/arXiv.2402.01364","CorpusId":267406164},"title":"Continual Learning for Large Language Models: A Survey","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2402.01364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"1514917592","name":"Tongtong Wu"},{"authorId":"2238130759","name":"Linhao Luo"},{"authorId":"2256011160","name":"Yuan-Fang Li"},{"authorId":"2265933101","name":"Shirui Pan"},{"authorId":"122699890","name":"Thuy-Trang Vu"},{"authorId":"2561045","name":"Gholamreza Haffari"}],"abstract":"Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task."}