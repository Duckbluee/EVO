{"abstract":"Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are <italic>environment- and task-specific:</italic> while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a <italic>universal</italic> solution to VPR â€“ a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or finetuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models <italic>with no VPR-specific training</italic> are the right substrate upon which to build such a universal VPR solution. Combining these derived features with <italic>unsupervised feature aggregation</italic> enables our suite of methods, <italic>AnyLoc</italic>, to achieve up to <inline-formula><tex-math notation=\"LaTeX\">$4\\times$</tex-math></inline-formula> significantly higher performance than existing approaches. We further obtain a 6% improvement in performance by characterizing the semantic properties of these features, uncovering unique <italic>domains</italic> which encapsulate datasets from similar environments. Our detailed experiments and analysis lay a foundation for building VPR solutions that may be deployed <italic>anywhere</italic>, <italic>anytime</italic>, and across <italic>anyview</italic>."}