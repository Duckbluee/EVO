{"abstract":"Existing source-free unsupervised domain adaptation uses the source model and unlabeled target data to train the target model under the assumption that the dataset is relatively balanced. However, in many practical applications, the class distribution is often skewed. Existing works tackle this problem mainly by oversampling the minority classes or undersampling the majority classes, which are not applicable when data are unlabeled. To address these challenges, we estimate class distribution by summing over model prediction probabilities to more flexibly tackle the uncertainty in an imbalanced dataset and employ information entropy of the classification results to select reliable balancing samples. Furthermore, since information entropy of the classification results tends to decrease as the model iterates in training, we propose Dynamic Entropy-based Balance (DEB) to more precisely determine the threshold for reliable balancing sample selection. Experimental results on several real datasets validate the effectiveness of our method."}