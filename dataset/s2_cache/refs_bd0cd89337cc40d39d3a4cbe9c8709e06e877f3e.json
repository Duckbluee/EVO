{"references":[{"paperId":"3179927f18c4f88fbe073dd950cce472f6724a84","externalIds":{"ArXiv":"2401.14626","DBLP":"journals/corr/abs-2401-14626","DOI":"10.48550/arXiv.2401.14626","CorpusId":267301016},"title":"Towards Lifelong Scene Graph Generation with Knowledge-ware In-context Prompt Learning"},{"paperId":"da217117909eb5ef310160795271f88bf606f0ab","externalIds":{"DBLP":"conf/acl/ZhaoWHZQZYXC24","ArXiv":"2401.08295","DOI":"10.18653/v1/2024.acl-long.625","CorpusId":267027714},"title":"SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models"},{"paperId":"f5b077e01f6e3d91f58cb4ed7158fa61eec5a1f8","externalIds":{"ArXiv":"2401.05268","DBLP":"conf/acl/Qiao0FLZJLC24","DOI":"10.18653/v1/2024.acl-long.165","CorpusId":266902590},"title":"AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning"},{"paperId":"04a340b15945c70e642227bf249639b171beb3f8","externalIds":{"DBLP":"journals/corr/abs-2401-01600","ArXiv":"2401.01600","DOI":"10.48550/arXiv.2401.01600","CorpusId":266741610},"title":"PLLaMa: An Open-source Large Language Model for Plant Science"},{"paperId":"e88d72202d449ad198308e21e7fa28680a8d2a21","externalIds":{"DBLP":"journals/corr/abs-2312-15696","ArXiv":"2312.15696","DOI":"10.48550/arXiv.2312.15696","CorpusId":266551564},"title":"EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data"},{"paperId":"3d7e5485fae2965ddf081dc64be6ab52f5834cf8","externalIds":{"ArXiv":"2311.11908","DBLP":"journals/corr/abs-2311-11908","DOI":"10.48550/arXiv.2311.11908","CorpusId":265294809},"title":"Continual Learning: Applications and the Road Forward"},{"paperId":"33eed6ea6805b59617fed7c41ef5825b5e4d1621","externalIds":{"DBLP":"journals/corr/abs-2311-11315","ArXiv":"2311.11315","DOI":"10.48550/arXiv.2311.11315","CorpusId":265294410},"title":"TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems"},{"paperId":"739cf040ed2c2af49077db48d489a46be5fb6157","externalIds":{"DBLP":"conf/acl/XieAA24","ArXiv":"2311.08545","DOI":"10.18653/v1/2024.findings-acl.606","CorpusId":265213147},"title":"Efficient Continual Pre-training for Building Domain Specific Large Language Models"},{"paperId":"098bc2d7cf933e7dbd951b30f4dbc42af7ec45f0","externalIds":{"DBLP":"conf/tsd/GogoulouLBN24","ArXiv":"2311.01200","CorpusId":264935145},"title":"Continual Learning Under Language Shift"},{"paperId":"0399533de2d1d21f456663d1bd5355c8b3c32a58","externalIds":{"DBLP":"journals/corr/abs-2310-20150","ArXiv":"2310.20150","DOI":"10.48550/arXiv.2310.20150","CorpusId":264828972},"title":"Unlearn What You Want to Forget: Efficient Unlearning for LLMs"},{"paperId":"75a85d74433d03a78a07bb95f6f261323a79eb80","externalIds":{"DBLP":"journals/corr/abs-2310-14510","ArXiv":"2310.14510","DOI":"10.48550/arXiv.2310.14510","CorpusId":264426357},"title":"CITB: A Benchmark for Continual Instruction Tuning"},{"paperId":"28fde851680a40fbbc5c6a44bd3ac6f5ca4ad284","externalIds":{"ArXiv":"2310.14152","DBLP":"conf/emnlp/WangCGXBZZGH23","DOI":"10.48550/arXiv.2310.14152","CorpusId":264426441},"title":"Orthogonal Subspace Learning for Language Model Continual Learning"},{"paperId":"b16c7d45183b9d595ab64301be019741b1528860","externalIds":{"DBLP":"conf/iclr/AzerbayevSPSMJD24","ArXiv":"2310.10631","DOI":"10.48550/arXiv.2310.10631","CorpusId":264172303},"title":"Llemma: An Open Language Model For Mathematics"},{"paperId":"90ff14a19419a0b6bb0965f0ab5e359462556172","externalIds":{"ArXiv":"2310.07343","DBLP":"journals/corr/abs-2310-07343","DOI":"10.48550/arXiv.2310.07343","CorpusId":263835243},"title":"How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances"},{"paperId":"7014b6700cfb74a6f5bb408dde553b9de7d1bb90","externalIds":{"DBLP":"journals/corr/abs-2310-06547","ArXiv":"2310.06547","DOI":"10.48550/arXiv.2310.06547","CorpusId":263829321},"title":"Rationale-Enhanced Language Models are Better Continual Relation Learners"},{"paperId":"a64067c6c4286fc60f4430829ae6b18519c088e3","externalIds":{"DBLP":"journals/corr/abs-2310-06762","ArXiv":"2310.06762","DOI":"10.48550/arXiv.2310.06762","CorpusId":263830425},"title":"TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models"},{"paperId":"5088a04d1a9f42b967f3dcf791145e8aa367fc54","externalIds":{"ArXiv":"2310.05492","DBLP":"journals/corr/abs-2310-05492","DOI":"10.48550/arXiv.2310.05492","CorpusId":263830318},"title":"How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"},{"paperId":"d78a2e6155fc9afd927850ca2d58f7b790a470dd","externalIds":{"DBLP":"conf/acl/WanZWCK24","ArXiv":"2310.03328","DOI":"10.48550/arXiv.2310.03328","CorpusId":263672135},"title":"Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise"},{"paperId":"0e0e706e13f160e74cac9556f28ab9a358c148d2","externalIds":{"DBLP":"journals/corr/abs-2310-03693","ArXiv":"2310.03693","DOI":"10.48550/arXiv.2310.03693","CorpusId":263671523},"title":"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"},{"paperId":"352244ac7602f13e16a08424db322364d0a2cef1","externalIds":{"DBLP":"journals/corr/abs-2309-14763","ArXiv":"2309.14763","DOI":"10.48550/arXiv.2309.14763","CorpusId":262822572},"title":"ConPET: Continual Parameter-Efficient Tuning for Large Language Models"},{"paperId":"6988596f88276920a4e555cbe624e1431bc8a9f7","externalIds":{"DBLP":"conf/iclr/KothaSR24","ArXiv":"2309.10105","DOI":"10.48550/arXiv.2309.10105","CorpusId":262054014},"title":"Understanding Catastrophic Forgetting in Language Models via Implicit Inference"},{"paperId":"aa638d5ffb01a84bed00d4c890da3cfcb386a8fa","externalIds":{"ArXiv":"2309.09530","CorpusId":262044959},"title":"Adapting Large Language Models to Domains via Reading Comprehension"},{"paperId":"6f8c4311e65efebb9da5a542c0405684e82a77cc","externalIds":{"ACL":"2024.emnlp-main.35","DBLP":"conf/emnlp/LinL0DLZP00ZDPZ24","ArXiv":"2309.06256","DOI":"10.48550/arXiv.2309.06256","CorpusId":261697277},"title":"Mitigating the Alignment Tax of RLHF"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","externalIds":{"ArXiv":"2308.10792","DBLP":"journals/corr/abs-2308-10792","DOI":"10.1145/3777411","CorpusId":261049152},"title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","externalIds":{"DBLP":"journals/corr/abs-2307-16789","ArXiv":"2307.16789","DOI":"10.48550/arXiv.2307.16789","CorpusId":260334759},"title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"72e46cabbe8fcb8c25ba83c516bebd3c857943a3","externalIds":{"ArXiv":"2307.02435","DBLP":"conf/acl/YadavSDLZTBMNRB23","ACL":"2023.acl-short.68","DOI":"10.48550/arXiv.2307.02435","CorpusId":259341641},"title":"Exploring Continual Learning for Code Generation Models"},{"paperId":"0d1c76d45afa012ded7ab741194baf142117c495","externalIds":{"DBLP":"conf/nips/RafailovSMMEF23","ArXiv":"2305.18290","CorpusId":258959321},"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"},{"paperId":"f5c73d9e6641b018b633690102121f5605d34fb0","externalIds":{"DBLP":"conf/emnlp/YaoWT0LDC023","ArXiv":"2305.13172","DOI":"10.48550/arXiv.2305.13172","CorpusId":258833129},"title":"Editing Large Language Models: Problems, Methods, and Opportunities"},{"paperId":"c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c","externalIds":{"DBLP":"journals/corr/abs-2305-11554","ArXiv":"2305.11554","DOI":"10.48550/arXiv.2305.11554","CorpusId":258823133},"title":"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings"},{"paperId":"28f03963ed4d081b671fb056b5a8aee1c5bbf31f","externalIds":{"DBLP":"journals/corr/abs-2305-08698","ArXiv":"2305.08698","DOI":"10.48550/arXiv.2305.08698","CorpusId":258685828},"title":"Continual Multimodal Knowledge Graph Construction"},{"paperId":"b05cfda924a147dfc100dc0b3eea451c6db32868","externalIds":{"DBLP":"conf/acl/QinQHLWXLSZ23","ArXiv":"2305.08702","DOI":"10.48550/arXiv.2305.08702","CorpusId":258686422},"title":"Recyclable Tuning for Continual Pre-training"},{"paperId":"05e003a34148d4663734d3f39deefa0979d2a0e6","externalIds":{"PubMedCentral":"10153281","DBLP":"journals/corr/abs-2304-09667","ArXiv":"2304.09667","DOI":"10.1093/bioinformatics/btae075","CorpusId":258298113,"PubMed":"38341654"},"title":"GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","externalIds":{"DBLP":"journals/corr/abs-2303-16434","ArXiv":"2303.16434","DOI":"10.48550/arXiv.2303.16434","CorpusId":257804802},"title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","externalIds":{"ArXiv":"2302.07842","DBLP":"journals/tmlr/MialonDLNPRRSDC23","CorpusId":256868474},"title":"Augmented Language Models: a Survey"},{"paperId":"e3ec55e9e6720194a0ed5d4033d93a941c8a4f99","externalIds":{"DBLP":"conf/iclr/KeSLKK023","ArXiv":"2302.03241","CorpusId":258079422},"title":"Continual Pre-training of Language Models"},{"paperId":"86d03160e6f05deb17d0169e515f5a55d6361f7c","externalIds":{"DBLP":"conf/icml/JangKYKLLLS23","ArXiv":"2302.03202","DOI":"10.48550/arXiv.2302.03202","CorpusId":256627673},"title":"Exploring the Benefits of Training Expert Language Models over Instruction Tuning"},{"paperId":"86478f285356b5c8d27423e6b939634d9e010fba","externalIds":{"DBLP":"journals/corr/abs-2301-12314","ArXiv":"2301.12314","DOI":"10.48550/arXiv.2301.12314","CorpusId":256390383},"title":"Progressive Prompts: Continual Learning for Language Models"},{"paperId":"53d62f2e7bf20c442ce7197b2d39e2f6ea77a47b","externalIds":{"DBLP":"conf/aaai/0002GWQLD23","ArXiv":"2211.11226","DOI":"10.48550/arXiv.2211.11226","CorpusId":253734197},"title":"Learn from Yesterday: A Semi-Supervised Continual Learning Method for Supervision-Limited Text-to-SQL Task Streams"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","externalIds":{"DBLP":"conf/ijcai/ZanCYLKGWCL22","ArXiv":"2206.06888","DOI":"10.48550/arXiv.2206.06888","CorpusId":249642442},"title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3","externalIds":{"ACL":"2022.emnlp-main.410","ArXiv":"2205.12393","DBLP":"conf/emnlp/ScialomCM22","DOI":"10.18653/v1/2022.emnlp-main.410","CorpusId":252815378},"title":"Fine-tuned Language Models are Continual Learners"},{"paperId":"b3bc37a15aa74c523d656ad89b1896651f5eef72","externalIds":{"ArXiv":"2205.09357","DBLP":"journals/corr/abs-2205-09357","DOI":"10.48550/arXiv.2205.09357","CorpusId":248887419,"PubMed":"38986187"},"title":"Continual Pre-Training Mitigates Forgetting in Language and Vision"},{"paperId":"a3ba7fdf789bcef381acd0d277a086428153bb9f","externalIds":{"ACL":"2022.emnlp-main.418","DBLP":"journals/corr/abs-2204-14211","ArXiv":"2204.14211","DOI":"10.48550/arXiv.2204.14211","CorpusId":248476156},"title":"TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","externalIds":{"ArXiv":"2204.05862","DBLP":"journals/corr/abs-2204-05862","DOI":"10.48550/arXiv.2204.05862","CorpusId":248118878},"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"0c8908707b4609bc53ea7a7c1d855088b7294dcf","externalIds":{"ArXiv":"2203.08512","DBLP":"journals/corr/abs-2203-08512","ACL":"2022.acl-long.218","DOI":"10.48550/arXiv.2203.08512","CorpusId":247476090},"title":"ConTinTin: Continual Learning from Task Instructions"},{"paperId":"ed8931af08ce757a92a01ed43a0619522e10e8ff","externalIds":{"ACL":"2022.bigscience-1.1","ArXiv":"2110.08534","DBLP":"journals/corr/abs-2110-08534","CorpusId":239016173},"title":"Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora"},{"paperId":"6c4bd57f7e70b9c037e44a96840e5ead0513abb0","externalIds":{"DBLP":"conf/icml/EthayarajhCS22","ArXiv":"2110.08420","CorpusId":250340652},"title":"Understanding Dataset Difficulty with V-Usable Information"},{"paperId":"ce828f9986b196308a3e40b1de58af1e8e68d728","externalIds":{"DBLP":"journals/corr/abs-2110-03215","ArXiv":"2110.03215","CorpusId":238419458},"title":"Towards Continual Knowledge Learning of Language Models"},{"paperId":"1b92ee2694a044b3555ad1c68d7c120fc275f3e6","externalIds":{"DBLP":"journals/corr/abs-2101-01926","ArXiv":"2101.01926","DOI":"10.1609/aaai.v35i12.17241","CorpusId":230770435},"title":"Curriculum-Meta Learning for Order-Robust Continual Relation Extraction"},{"paperId":"373bc164d7b552f8782988e7da6b0d00092a20b0","externalIds":{"ACL":"2020.coling-main.574","DBLP":"conf/coling/BiesialskaBC20","ArXiv":"2012.09823","MAG":"3112170794","DOI":"10.18653/v1/2020.coling-main.574","CorpusId":227231454},"title":"Continual Lifelong Learning in Natural Language Processing: A Survey"},{"paperId":"471e6735f47e3899a67f1270889cdacd0e990ad8","externalIds":{"MAG":"3042443324","DBLP":"journals/pami/HuSSK23","ArXiv":"2007.09335","DOI":"10.1109/TPAMI.2022.3218265","CorpusId":220646430,"PubMed":"36315549"},"title":"Drinking From a Firehose: Continual Learning With Web-Scale Natural Language"},{"paperId":"659bf9ce7175e1ec266ff54359e2bd76e0b7ff31","externalIds":{"DBLP":"conf/nips/LewisPPPKGKLYR020","MAG":"3027879771","ArXiv":"2005.11401","CorpusId":218869575},"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","externalIds":{"DBLP":"conf/acl/GururanganMSLBD20","ArXiv":"2004.10964","MAG":"3017961061","ACL":"2020.acl-main.740","DOI":"10.18653/v1/2020.acl-main.740","CorpusId":216080466},"title":"Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"7aa70e2c12c8ba2dcc828893adb8bb56e3766726","externalIds":{"ArXiv":"2001.09768","DBLP":"journals/corr/abs-2001-09768","MAG":"3002093512","DOI":"10.1007/s11023-020-09539-2","CorpusId":210920551},"title":"Artificial Intelligence, Values, and Alignment"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","externalIds":{"DBLP":"journals/corr/abs-1907-12412","ArXiv":"1907.12412","MAG":"2965210982","DOI":"10.1609/AAAI.V34I05.6428","CorpusId":198968327},"title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","externalIds":{"MAG":"2971274815","ArXiv":"1905.03197","DBLP":"journals/corr/abs-1905-03197","CorpusId":147704286},"title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"9c54962b0fd011d5fe3f5b5275cc6ba091a2c7ae","externalIds":{"MAG":"2947461406","CorpusId":173188188},"title":"On Tiny Episodic Memories in Continual Learning"},{"paperId":"d5bb3faa48b83469da1a01ef267886e71f4a931a","externalIds":{"MAG":"2950262738","DBLP":"conf/eccv/MallyaDL18","DOI":"10.1007/978-3-030-01225-0_5","CorpusId":3977226},"title":"Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","externalIds":{"MAG":"2736601468","DBLP":"journals/corr/SchulmanWDRK17","ArXiv":"1707.06347","CorpusId":28695052},"title":"Proximal Policy Optimization Algorithms"},{"paperId":"118fae4b4d07453561f1eded88654f812c7c61ec","externalIds":{"DBLP":"conf/nips/Lopez-PazR17","MAG":"2734314755","ArXiv":"1706.08840","CorpusId":37308416},"title":"Gradient Episodic Memory for Continual Learning"},{"paperId":"59a922212153d3407e658109f36c11a34ee7d283","externalIds":{"MAG":"2963559848","DBLP":"conf/nips/ShinLKK17","ArXiv":"1705.08690","CorpusId":1888776},"title":"Continual Learning with Deep Generative Replay"},{"paperId":"2e55ba6c97ce5eb55abd959909403fe8da7e9fe9","externalIds":{"DBLP":"journals/corr/KirkpatrickPRVD16","MAG":"2560647685","ArXiv":"1612.00796","DOI":"10.1073/pnas.1611835114","CorpusId":4704285,"PubMed":"28292907"},"title":"Overcoming catastrophic forgetting in neural networks"},{"paperId":"b6a2f5f0c41ae933bebb6cd09bf5d065d7f675b0","externalIds":{"DBLP":"journals/corr/abs-2310-15694","DOI":"10.48550/arXiv.2310.15694","CorpusId":264439275},"title":"COPF: Continual Learning Human Preference through Optimal Policy Fitting"},{"paperId":"85a5c0bb6660990c7b5e348c165c552c1d4a2c2a","externalIds":{"ACL":"2023.acl-long.703","DBLP":"conf/acl/MokDLTYY23","DOI":"10.18653/v1/2023.acl-long.703","CorpusId":259370786},"title":"Large-scale Lifelong Learning of In-context Instructions and How to Tackle It"},{"paperId":"ac771182d1780c863954243809d1e144433919f9","externalIds":{"DBLP":"journals/corr/abs-2307-12966","DOI":"10.48550/arXiv.2307.12966","CorpusId":260356605},"title":"Aligning Large Language Models with Human: A Survey"},{"paperId":"448c377dfd28ceb9c1e55dbdc55872fb6e34ba57","externalIds":{"DBLP":"conf/acl/CastellucciFC020","ACL":"2021.acl-short.106","DOI":"10.18653/v1/2021.acl-short.106","CorpusId":236460265},"title":"Learning to Solve NLP Tasks in an Incremental Number of Languages"}]}