{"paperId":"16ff264b2beb155f630621cded6e806b9dea288d","externalIds":{"DBLP":"journals/corr/abs-2305-19812","ArXiv":"2305.19812","DOI":"10.1109/TPAMI.2024.3416302","CorpusId":258987905,"PubMed":"38889032"},"title":"A Survey of Label-Efficient Deep Learning for 3D Point Clouds","openAccessPdf":{"url":"https://arxiv.org/pdf/2305.19812","status":"GREEN","license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2305.19812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"24646772","name":"Aoran Xiao"},{"authorId":"2144533482","name":"Xiaoqin Zhang"},{"authorId":"144082425","name":"Ling Shao"},{"authorId":"1771189","name":"Shijian Lu"}],"abstract":"In the past decade, deep neural networks have achieved significant progress in point cloud learning. However, collecting large-scale precisely-annotated point clouds is extremely laborious and expensive, which hinders the scalability of existing point cloud datasets and poses a bottleneck for efficient exploration of point cloud data in various tasks and applications. Label-efficient learning offers a promising solution by enabling effective deep network training with much-reduced annotation efforts. This paper presents the first comprehensive survey of label-efficient learning of point clouds. We address three critical questions in this emerging research field: i) the importance and urgency of label-efficient learning in point cloud processing, ii) the subfields it encompasses, and iii) the progress achieved in this area. To this end, we propose a taxonomy that organizes label-efficient learning methods based on the data prerequisites provided by different types of labels. We categorize four typical label-efficient learning approaches that significantly reduce point cloud annotation efforts: data augmentation, domain transfer learning, weakly-supervised learning, and pretrained foundation models. For each approach, we outline the problem setup and provide an extensive literature review that showcases relevant progress and challenges. Finally, we share our views on the current research challenges and potential future directions."}