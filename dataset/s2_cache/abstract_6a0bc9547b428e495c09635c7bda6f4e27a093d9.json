{"abstract":"With the growing use of Deep Learning (DL) to tackle various problems, securing these models against adversaries has become a primary concern for researchers. Recent studies have shown that DL-based malware detectors are vulnerable to adversarial examples. An adversary can create carefully crafted adversarial examples to evade DL-based malware detectors. In this paper, we propose Mal2GCN, a robust malware detection model that uses Function Call Graph (FCG) representation of executable files combined with Graph Convolution Network (GCN) to detect Windows malware. Since the FCG representation of executable files is more robust than the raw byte sequence representation, numerous proposed adversarial example generating methods are ineffective in evading Mal2GCN. Moreover, we use the non-negative training method to transform Mal2GCN into a monotonically non-decreasing function; thereby, making it theoretically robust against appending attacks. Besides, experimental results on a collected dataset of PE executables demonstrate that Mal2GCN can detect malware with 98.15% accuracy, outperforming its counterparts. We then present a black-box source code-based adversarial malware generation approach that can be used to evaluate the robustness of malware detection models against real-world adversaries. This approach injects adversarial code into various locations of malware source code, aiming to evade malware detection models. The experiments indicate that Mal2GCN with non-negative weights achieves high accuracy in detecting Windows malware while also exhibiting robustness against adversarial attacks that add benign features to the malware source code."}