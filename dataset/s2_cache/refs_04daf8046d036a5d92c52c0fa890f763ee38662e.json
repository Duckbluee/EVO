{"references":[{"paperId":"92a09cdfc19f3f582d89c28c1b4f386299cc69e1","externalIds":{"DBLP":"conf/iclr/RaviGHHR0KRRGMP25","ArXiv":"2408.00714","DOI":"10.48550/arXiv.2408.00714","CorpusId":271601113},"title":"SAM 2: Segment Anything in Images and Videos"},{"paperId":"1595f59a07b3fb09a182be55cacc99f1fcd3fde4","externalIds":{"DBLP":"conf/cvpr/ShangSQWM024","DOI":"10.1109/CVPR52733.2024.00395","CorpusId":272710586},"title":"Prompt-Driven Referring Image Segmentation with Instance Contrasting"},{"paperId":"ea26db335f0b4edce32166e709b14a420b7ece4e","externalIds":{"DBLP":"conf/cvpr/KweonY24","DOI":"10.1109/CVPR52733.2024.01844","CorpusId":271965850},"title":"From SAM to CAMs: Exploring Segment Anything Model for Weakly Supervised Semantic Segmentation"},{"paperId":"a2c5ebe123b106541546c76186b74cb35e037061","externalIds":{"DBLP":"conf/cvpr/ShahVP24","DOI":"10.1109/CVPR52733.2024.01226","CorpusId":272723646},"title":"LQMFormer: Language-Aware Query Mask Transformer for Referring Image Segmentation"},{"paperId":"3ad2ba8eca619fe2a45f41271f3624b2f5a3e8f5","externalIds":{"DBLP":"conf/cvpr/ZhouXSL24","DOI":"10.1109/CVPR52733.2024.00366","CorpusId":272723638},"title":"Unlocking the Potential of Pre-Trained Vision Transformers for Few-Shot Semantic Segmentation through Relationship Descriptors"},{"paperId":"45e441cd459765e713b499e6a9912cb598edabde","externalIds":{"ArXiv":"2406.08372","DBLP":"conf/cvpr/HeZZSYDS24","DOI":"10.1109/CVPR52733.2024.02243","CorpusId":270391732},"title":"APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic Segmentation"},{"paperId":"c39db860fd303ff1a836797fd411ffffb7c35267","externalIds":{"DBLP":"journals/corr/abs-2406-05821","ArXiv":"2406.05821","DOI":"10.1109/CVPR52734.2025.02301","CorpusId":270371965},"title":"F-LMM: Grounding Frozen Large Multimodal Models"},{"paperId":"c6c945b4b7681f7e7a306f17d4b32e09a3dcd211","externalIds":{"DBLP":"journals/ral/VodischPKVB25","ArXiv":"2405.19035","DOI":"10.1109/LRA.2024.3505779","CorpusId":270094707},"title":"A Good Foundation is Worth Many Labels: Label-Efficient Panoptic Segmentation"},{"paperId":"8a961993325677fbd94c4191b41c88da5fe039b9","externalIds":{"DBLP":"journals/corr/abs-2405-13686","ArXiv":"2405.13686","DOI":"10.48550/arXiv.2405.13686","CorpusId":269982657},"title":"Embedding Generalized Semantic Knowledge into Few-Shot Remote Sensing Segmentation"},{"paperId":"29a820c1be404843f931a479a9bf6252ebc79456","externalIds":{"DBLP":"conf/cvpr/FanZCLWZS24","ArXiv":"2405.10185","DOI":"10.1109/CVPR52733.2024.00382","CorpusId":269791012},"title":"DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data"},{"paperId":"89b95c7e26dc83b2de25222bdb982ae132861a9a","externalIds":{"ArXiv":"2405.08458","DBLP":"conf/cvpr/WangZPC024","DOI":"10.1109/CVPR52733.2024.00378","CorpusId":269761297},"title":"Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation"},{"paperId":"e417218e1529d834b3162030d87bb23858ee4eb1","externalIds":{"DBLP":"conf/cvpr/Zhao0CQZJL0C24","ArXiv":"2405.00587","DOI":"10.1109/CVPR52733.2024.00336","CorpusId":269484631},"title":"GraCo: Granularity-Controllable Interactive Segmentation"},{"paperId":"c678debb68a68db3a33461a1682251139d0a649e","externalIds":{"DBLP":"journals/corr/abs-2404-11998","ArXiv":"2404.11998","DOI":"10.1109/CVPR52733.2024.01301","CorpusId":269214097},"title":"Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation"},{"paperId":"a14c2e5e235392feced0cd1ab242e74e25a72660","externalIds":{"DBLP":"conf/iclr/ShiY24","ArXiv":"2404.11957","DOI":"10.48550/arXiv.2404.11957","CorpusId":269214119},"title":"The devil is in the object boundary: towards annotation-free instance segmentation using Foundation Models"},{"paperId":"e26f7120b9a0698a9873db3cd653a16dcf0ed1e4","externalIds":{"DBLP":"journals/tmlr/GuiSLYARP024","ArXiv":"2404.09447","DOI":"10.48550/arXiv.2404.09447","CorpusId":269149502},"title":"kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually Expanding Large Vocabularies"},{"paperId":"0b0e995d549427e2cdc4b9e30b26a3392d1682b6","externalIds":{"DBLP":"conf/icassp/YouW024","DOI":"10.1109/ICASSP48485.2024.10446831","CorpusId":268584793},"title":"Weakly Supervised Few-Shot Segmentation Through Textual Prompt"},{"paperId":"5c4f42e084e84a88bc8d964b613d4721618711a2","externalIds":{"DBLP":"conf/wacv/HajimiriAD25","ArXiv":"2404.08181","DOI":"10.1109/WACV61041.2025.00495","CorpusId":269137404},"title":"Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation"},{"paperId":"873226e63a83734ca15123d993830c0977967ab7","externalIds":{"ArXiv":"2404.08590","DBLP":"conf/wacv/Nguyen-TruongNV25","DOI":"10.1109/WACV61041.2025.00488","CorpusId":269137114},"title":"Vision-Aware Text Features in Referring Image Segmentation: From Object Understanding to Context Understanding"},{"paperId":"8ebaa076ab33ce56c2625dd647cf77b04a42348f","externalIds":{"DBLP":"journals/corr/abs-2404-08506","ArXiv":"2404.08506","DOI":"10.48550/arXiv.2404.08506","CorpusId":269137488},"title":"LaSagnA: Language-based Segmentation Assistant for Complex Queries"},{"paperId":"1e3a1e583ddf5e9f5cecb578b64fa90d40bbf242","externalIds":{"DBLP":"conf/cvpr/WangK22","ArXiv":"2404.08767","DOI":"10.1109/CVPRW63382.2024.00183","CorpusId":269149451},"title":"LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning"},{"paperId":"3dbcd91f65f2474e12ec3e56c5dec79b6de109d9","externalIds":{"DBLP":"journals/pami/ZhouW24a","DOI":"10.1109/TPAMI.2024.3387116","CorpusId":269041359,"PubMed":"38598386"},"title":"Prototype-Based Semantic Segmentation"},{"paperId":"1040127769d297a724cad802aa74af9945fae097","externalIds":{"DBLP":"conf/cvpr/BarsellottiAC0C24","ArXiv":"2404.06542","DOI":"10.1109/CVPR52733.2024.00354","CorpusId":269033038},"title":"Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation"},{"paperId":"e3b320020e98ca85d8ed756238816b0e950aa409","externalIds":{"ArXiv":"2404.00701","DBLP":"journals/corr/abs-2404-00701","DOI":"10.48550/arXiv.2404.00701","CorpusId":268819196},"title":"Training-Free Semantic Segmentation via LLM-Supervision"},{"paperId":"2d0f5e906fdb76c3803844cb9b8e679de0524216","externalIds":{"DBLP":"conf/mm/HuangLZTZSJ24","ArXiv":"2404.00650","DOI":"10.1145/3664647.3680571","CorpusId":268820009},"title":"Deep Instruction Tuning for Segment Anything Model"},{"paperId":"78b1be3b501f7f4877c2b27d6ef9f8344fce95f8","externalIds":{"DBLP":"journals/corr/abs-2404-00262","ArXiv":"2404.00262","DOI":"10.1109/CVPR52733.2024.00379","CorpusId":268819337},"title":"Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation"},{"paperId":"03a94b9b407e6c01e782baef273dc25b40a701cd","externalIds":{"ArXiv":"2403.20105","DBLP":"conf/ijcnn/CorradiniSCCSC25","DOI":"10.1109/IJCNN64981.2025.11227428","CorpusId":268793968},"title":"FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models"},{"paperId":"37d7e6719d8bba723f9ca3e14282b13413d5ea94","externalIds":{"ArXiv":"2403.16605","DBLP":"journals/corr/abs-2403-16605","DOI":"10.1109/CVPR52733.2024.02615","CorpusId":268680796},"title":"SatSynth: Augmenting Image-Mask Pairs Through Diffusion Models for Aerial Semantic Segmentation"},{"paperId":"91c82a8772af46f6a02dc10fc3b0a3d4b01f6c13","externalIds":{"ArXiv":"2403.16578","CorpusId":268681133},"title":"SegICL: A Multimodal In-context Learning Framework for Enhanced Segmentation in Medical Imaging"},{"paperId":"88ab0e8cd81612a91a97c67a39160908197dc668","externalIds":{"ArXiv":"2403.14183","DBLP":"conf/eccv/KimOY24","DOI":"10.48550/arXiv.2403.14183","CorpusId":268554179},"title":"OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation"},{"paperId":"a775ad748714f7ad6d1865741fd9073a213a2517","externalIds":{"ArXiv":"2403.14598","DBLP":"journals/corr/abs-2403-14598","DOI":"10.48550/arXiv.2403.14598","CorpusId":268553875},"title":"PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model"},{"paperId":"1b8187070625228336fbcd935b68be414c16f0c8","externalIds":{"DBLP":"journals/corr/abs-2403-14141","ArXiv":"2403.14141","DOI":"10.48550/arXiv.2403.14141","CorpusId":268553888},"title":"Empowering Segmentation Ability to Multi-modal Large Language Models"},{"paperId":"c5a6cce9c97dd3126e63842b4bf64750d0d1a74f","externalIds":{"ArXiv":"2403.14291","DBLP":"journals/corr/abs-2403-14291","DOI":"10.1109/CVPR52733.2024.00883","CorpusId":268553752},"title":"Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models"},{"paperId":"5d884761c558424fcce40efe743edc89787658c7","externalIds":{"DBLP":"journals/corr/abs-2403-11197","ArXiv":"2403.11197","DOI":"10.1109/ACCESS.2024.3418210","CorpusId":268512793},"title":"TAG: Guidance-Free Open-Vocabulary Semantic Segmentation"},{"paperId":"2b4a3ad8fc601400927cc822b1e6408dbb77cef7","externalIds":{"DBLP":"journals/access/KawanoA24a","ArXiv":"2403.11194","DOI":"10.1109/ACCESS.2024.3456442","CorpusId":268513010},"title":"MaskDiffusion: Exploiting Pre-Trained Diffusion Models for Semantic Segmentation"},{"paperId":"e60761df840d1aa2b772618b4e39e59dc141f7ad","externalIds":{"ArXiv":"2403.08426","DBLP":"journals/corr/abs-2403-08426","DOI":"10.1109/TCSVT.2024.3504816","CorpusId":268378918},"title":"Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation"},{"paperId":"76764daae65846191217455a605d12dc2ec787b4","externalIds":{"DBLP":"journals/corr/abs-2403-07700","ArXiv":"2403.07700","DOI":"10.1109/CVPR52733.2024.02180","CorpusId":268363960},"title":"CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers"},{"paperId":"470cacf502daf31e7da621666eb012ffa49305c1","externalIds":{"ArXiv":"2403.05433","DBLP":"journals/tmlr/ZhaoS25","CorpusId":268296900},"title":"Part-aware Prompted Segment Anything Model for Adaptive Segmentation"},{"paperId":"fe844a6090fa5a1f99bd494f86d75874d0462c20","externalIds":{"ArXiv":"2403.01482","DBLP":"conf/cvpr/0001HJH24","DOI":"10.1109/CVPR52733.2024.00338","CorpusId":268247452},"title":"EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation"},{"paperId":"04344bd734575f480d567b2acd5aa15e110e77c4","externalIds":{"ArXiv":"2402.17726","DBLP":"conf/cvpr/SunCZZCZDWL24","DOI":"10.1109/CVPR52733.2024.02224","CorpusId":268032326},"title":"VRP-SAM: SAM with Visual Reference Prompt"},{"paperId":"0d794d8d8204025b6459ce52a606a15648ac3931","externalIds":{"ArXiv":"2402.14891","DBLP":"journals/corr/abs-2402-14891","DOI":"10.48550/arXiv.2402.14891","CorpusId":267897370},"title":"LLMBind: A Unified Modality-Task Integration Framework"},{"paperId":"b8f049fc6cafce0d0ed177ad91ef2848ef427908","externalIds":{"DBLP":"journals/pami/ZhouW24","DOI":"10.1109/TPAMI.2024.3367952","CorpusId":267806416,"PubMed":"38386572"},"title":"Cross-Image Pixel Contrasting for Semantic Segmentation"},{"paperId":"114343d000f36554008e2ee0c9a09258e6009f40","externalIds":{"ArXiv":"2402.08960","DBLP":"journals/corr/abs-2402-08960","DOI":"10.48550/arXiv.2402.08960","CorpusId":267657592},"title":"Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision"},{"paperId":"80b6f17630ccfc5dd891f5f9024d5a1d9697fa29","externalIds":{"DBLP":"conf/nips/CaoJGW23","ArXiv":"2402.03311","DOI":"10.48550/arXiv.2402.03311","CorpusId":267411769},"title":"HASSOD: Hierarchical Adaptive Self-Supervised Object Detection"},{"paperId":"865d8ded3f0f938d9fdd72c5e82dbb10a1b13a1a","externalIds":{"DBLP":"journals/tce/HuangYYZATX24","DOI":"10.1109/TCE.2024.3373630","CorpusId":269486331},"title":"Few-Shot Semantic Segmentation for Consumer Electronics: An Inter-Class Relation Mining Approach"},{"paperId":"b195a77512558e3158accabfc9bcf498aba12173","externalIds":{"DBLP":"journals/corr/abs-2401-16459","ArXiv":"2401.16459","DOI":"10.48550/arXiv.2401.16459","CorpusId":267320592},"title":"Bridging Generative and Discriminative Models for Unified Visual Perception with Diffusion Priors"},{"paperId":"9cd6c6d85de6180dd92ba43e685663067cf3ab7f","externalIds":{"ArXiv":"2401.14159","DBLP":"journals/corr/abs-2401-14159","DOI":"10.48550/arXiv.2401.14159","CorpusId":267212047},"title":"Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks"},{"paperId":"50913b504e433244ec033436ea1cb633a5d83d4c","externalIds":{"DBLP":"conf/cvpr/RakicWOCGD24","ArXiv":"2401.13650","DOI":"10.1109/CVPR52733.2024.01061","CorpusId":267200013},"title":"Tyche: Stochastic in-Context Learning for Medical Image Segmentation"},{"paperId":"83d98d263613df9dd78d66a289d5d849c71cb1f9","externalIds":{"DBLP":"journals/tmlr/BensaidGLMHC25","ArXiv":"2401.11311","DOI":"10.48550/arXiv.2401.11311","CorpusId":267069357},"title":"A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models"},{"paperId":"4b3a1f3b3584637eccca3a9894ab0696960cb7d6","externalIds":{"ArXiv":"2401.09826","DBLP":"journals/vc/FengLLSCLV25","DOI":"10.1007/s00371-025-03809-9","CorpusId":267034949},"title":"Learning few-shot semantic segmentation with error-filtered segment anything model"},{"paperId":"bb880ed24924b2a51c023e0e04e265f48b813997","externalIds":{"ArXiv":"2401.10229","DBLP":"conf/cvpr/LiY0DWZLCL24","DOI":"10.1109/CVPR52733.2024.02640","CorpusId":267034977},"title":"OMG-Seg: Is One Model Good Enough for all Segmentation?"},{"paperId":"1e99d24f9e853f2f3f5718aef201a5550f02147b","externalIds":{"DBLP":"journals/corr/abs-2401-10227","ArXiv":"2401.10227","DOI":"10.48550/arXiv.2401.10227","CorpusId":267035260},"title":"A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting"},{"paperId":"6ee64897e219cdf7dad6e5918fb2e3c3e07b12c5","externalIds":{"ArXiv":"2401.03495","DBLP":"journals/cbm/ZhangSJ24","DOI":"10.48550/arXiv.2401.03495","CorpusId":266845736,"PubMed":"38422961"},"title":"Segment Anything Model for Medical Image Segmentation: Current Applications and Future Directions"},{"paperId":"904cbad95be3cc3e2a7cf48b333a8a37a44a30e9","externalIds":{"DBLP":"conf/eccv/YuanLZLCL24","ArXiv":"2401.02955","DOI":"10.1007/978-3-031-72775-7_24","CorpusId":266818213},"title":"Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively"},{"paperId":"ab31e40e94d2b168e3eb477f54416e9f11fa7255","externalIds":{"DBLP":"conf/cvpr/Niu0HLHD24","ArXiv":"2312.17243","DOI":"10.1109/CVPR52733.2024.02146","CorpusId":266573375},"title":"Unsupervised Universal Image Segmentation"},{"paperId":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","externalIds":{"DBLP":"journals/corr/abs-2312-17240","ArXiv":"2312.17240","DOI":"10.48550/arXiv.2312.17240","CorpusId":266573154},"title":"An Improved Baseline for Reasoning Segmentation with Large Language Model"},{"paperId":"466134a53cd65ded957ce2d102a10eed3170e0e7","externalIds":{"DBLP":"journals/corr/abs-2312-14733","ArXiv":"2312.14733","DOI":"10.48550/arXiv.2312.14733","CorpusId":266521041},"title":"Harnessing Diffusion Models for Visual Perception with Meta Prompts"},{"paperId":"ef3958c9677be808ba43ec0cf9059545cc52cd32","externalIds":{"DBLP":"journals/corr/abs-2312-14149","ArXiv":"2312.14149","DOI":"10.48550/arXiv.2312.14149","CorpusId":266435667},"title":"TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification"},{"paperId":"dc6a7257c19be0b48d200bcb66c493c4ac07a632","externalIds":{"DBLP":"conf/cvpr/ChngZHQ024","ArXiv":"2312.12198","DOI":"10.1109/CVPR52733.2024.02509","CorpusId":266362233},"title":"Mask Grounding for Referring Image Segmentation"},{"paperId":"d38a00348487b02dad98782506fb8ebe31aef477","externalIds":{"DBLP":"conf/eccv/WysoczanskaSRBTP24","ArXiv":"2312.12359","DOI":"10.48550/arXiv.2312.12359","CorpusId":266362718},"title":"CLIP-DINOiser: Teaching CLIP a few DINO tricks"},{"paperId":"9d992e40150955e03868113d90dbf870a07a0d02","externalIds":{"ArXiv":"2312.12425","DBLP":"journals/corr/abs-2312-12425","DOI":"10.48550/arXiv.2312.12425","CorpusId":266362666},"title":"SegRefiner: Towards Model-Agnostic Segmentation Refinement with Discrete Diffusion Process"},{"paperId":"b5503967d39557a77c70076c308183e92d6d775a","externalIds":{"DBLP":"conf/cvpr/YuanLLTLQZZ24","ArXiv":"2312.10032","DOI":"10.1109/CVPR52733.2024.02664","CorpusId":266335219},"title":"Osprey: Pixel Understanding with Visual Instruction Tuning"},{"paperId":"3cc0f0b40589f4e0f971280cad48919048d22a41","externalIds":{"DBLP":"conf/cvpr/XiaHHPSH24","ArXiv":"2312.10103","DOI":"10.1109/CVPR52733.2024.00370","CorpusId":266348796},"title":"GSVA: Generalized Segmentation via Multimodal Large Language Models"},{"paperId":"4e649c81d687aea1a27ee1de68e335c54f156fca","externalIds":{"DBLP":"journals/corr/abs-2312-10163","ArXiv":"2312.10163","DOI":"10.48550/arXiv.2312.10163","CorpusId":266348553},"title":"Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey"},{"paperId":"298d5bd2912270f4e53d6e6cbd1ff2df5422482d","externalIds":{"ArXiv":"2312.09128","DBLP":"conf/eccv/PanTWS24","DOI":"10.48550/arXiv.2312.09128","CorpusId":266210504},"title":"Tokenize Anything via Prompting"},{"paperId":"4d1194e9e990ba9decd597cd93c555e5d1d06d54","externalIds":{"DBLP":"journals/corr/abs-2312-08007","ArXiv":"2312.08007","DOI":"10.1109/CVPR52733.2024.01235","CorpusId":266191318},"title":"Unveiling Parts Beyond Objects: Towards Finer-Granularity Referring Expression Segmentation"},{"paperId":"f4cacee297bc7727de713571cdacdd832224d6b2","externalIds":{"DBLP":"conf/cvpr/SunL0GL24","ArXiv":"2312.07661","DOI":"10.1109/CVPR52733.2024.01251","CorpusId":266191302},"title":"CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor"},{"paperId":"769b794fe9f97268007676171f246d45e0631014","externalIds":{"ArXiv":"2312.02520","DBLP":"journals/corr/abs-2312-02520","DOI":"10.1109/CVPR52733.2024.01269","CorpusId":265659463},"title":"Towards More Unified In-Context Visual Understanding"},{"paperId":"e4ef825c9a6a710fe00a90c658b1926021b75207","externalIds":{"DBLP":"conf/cvpr/QiYGXDJ024","ArXiv":"2312.01985","DOI":"10.1109/CVPR52733.2024.00603","CorpusId":265609288},"title":"UniGS: Unified Representation for Image Generation and Segmentation"},{"paperId":"adb969668f191839d273af5743948ed10b28c43c","externalIds":{"DBLP":"conf/cvpr/RenHW0FFJ24","ArXiv":"2312.02228","DOI":"10.1109/CVPR52733.2024.02491","CorpusId":265659012},"title":"PixelLM: Pixel Reasoning with Large Multimodal Model"},{"paperId":"db26827a859ab3d63c84d151e6c2356c68560480","externalIds":{"DBLP":"journals/corr/abs-2312-01597","ArXiv":"2312.01597","DOI":"10.48550/arXiv.2312.01597","CorpusId":265609027},"title":"SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference"},{"paperId":"689c358c5f9b5b1693a8bcc7e6e0460012f5cf9e","externalIds":{"ArXiv":"2312.00785","DBLP":"journals/corr/abs-2312-00785","DOI":"10.1109/CVPR52733.2024.02157","CorpusId":265552038},"title":"Sequential Modeling Enables Scalable Learning for Large Vision Models"},{"paperId":"66d24fd4e1c56142df404634a42cc491178bc2be","externalIds":{"DBLP":"conf/cvpr/BousselhamPFK24","ArXiv":"2312.00878","DOI":"10.1109/CVPR52733.2024.00367","CorpusId":265609367},"title":"Grounding Everything: Emerging Localization Properties in Vision-Language Transformers"},{"paperId":"51f864c81edce7f0472993f7bfdf299ebd770ebe","externalIds":{"ArXiv":"2311.17921","DBLP":"conf/eccv/MukhopadhyayGYAPSZOS24","DOI":"10.48550/arXiv.2311.17921","CorpusId":265498931},"title":"Do text-free diffusion models learn discriminative visual representations?"},{"paperId":"0a95f7d0165671c922446663d3d4de45ea87ef2e","externalIds":{"DBLP":"conf/cvpr/ZhuCJY024","ArXiv":"2311.16926","DOI":"10.1109/CVPR52733.2024.00296","CorpusId":265466648},"title":"LLaFS: When Large Language Models Meet Few-Shot Segmentation"},{"paperId":"84b582739d242f97dae5e548295e9e49e94e36c6","externalIds":{"DBLP":"conf/eccv/HoyerTNGT24","ArXiv":"2311.16241","DOI":"10.48550/arXiv.2311.16241","CorpusId":265466701},"title":"SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance"},{"paperId":"847d317769a4e0a70c9c731983920dc3aecb60bd","externalIds":{"DBLP":"conf/icassp/WangLZW24","ArXiv":"2311.13865","DOI":"10.1109/ICASSP48485.2024.10447456","CorpusId":265445719},"title":"Language-Guided Few-Shot Semantic Segmentation"},{"paperId":"027bf71b278cd7533b26570a795778524ead48d4","externalIds":{"DBLP":"journals/corr/abs-2311-12065","ArXiv":"2311.12065","DOI":"10.48550/arXiv.2311.12065","CorpusId":265308999},"title":"Few-Shot Classification & Segmentation Using Large Language Models Agent"},{"paperId":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","externalIds":{"DBLP":"conf/cvpr/PiYGZZ24","ArXiv":"2311.06612","DOI":"10.1109/CVPR52733.2024.02561","CorpusId":265150065},"title":"PerceptionGPT: Effectively Fusing Visual Perception Into LLM"},{"paperId":"abb312b7cf508b91ae7a88b1890c13eb8b96ad1a","externalIds":{"DBLP":"conf/icml/Zhang000C24","ArXiv":"2311.04498","DOI":"10.48550/arXiv.2311.04498","CorpusId":265051059},"title":"NExT-Chat: An LMM for Chat, Detection and Segmentation"},{"paperId":"f57fae177e4e6e0a2b306b7b6bef899821ce9545","externalIds":{"DBLP":"journals/corr/abs-2311-03648","ArXiv":"2311.03648","DOI":"10.1109/WACV57701.2024.00258","CorpusId":265043469},"title":"Instruct Me More! Random Prompting for Visual In-Context Learning"},{"paperId":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","externalIds":{"DBLP":"journals/corr/abs-2311-03356","ArXiv":"2311.03356","DOI":"10.1109/CVPR52733.2024.01236","CorpusId":265043538},"title":"GLaMM: Pixel Grounding Large Multimodal Model"},{"paperId":"b62081daa866414a26743a23f7c92f2959ef0e90","externalIds":{"ArXiv":"2310.19001","DBLP":"journals/corr/abs-2310-19001","DOI":"10.48550/arXiv.2310.19001","CorpusId":264825419},"title":"Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation"},{"paperId":"b0fd9578843c410171feaba5567cf554b09fd0d4","externalIds":{"ArXiv":"2310.18642","DBLP":"journals/corr/abs-2310-18642","DOI":"10.48550/arXiv.2310.18642","CorpusId":264590227},"title":"One-shot Localization and Segmentation of Medical Images with Foundation Models"},{"paperId":"5623e34a67bf22131219dafc2d3cb6b7d49d8ac2","externalIds":{"DBLP":"conf/emnlp/SuoZ023","ArXiv":"2310.18049","DOI":"10.48550/arXiv.2310.18049","CorpusId":264555550},"title":"Text Augmented Spatial-aware Zero-shot Referring Image Segmentation"},{"paperId":"3398fe414d7d1f887bc909322c4c244ab2120442","externalIds":{"DBLP":"conf/mm/LiuZQX0Y23","DOI":"10.1145/3581783.3612117","CorpusId":264492486},"title":"CARIS: Context-Aware Referring Image Segmentation"},{"paperId":"bb67e7d347fd765e61d43a78d7fd997cb99e8f94","externalIds":{"DBLP":"conf/miccai/YuLLLWCL23","ArXiv":"2310.14197","DOI":"10.1007/978-3-031-43993-3_57","CorpusId":263673357},"title":"Diffusion-Based Data Augmentation for Nuclei Image Segmentation"},{"paperId":"5c18aca1f2f5cd8064024962b19d8ba3ec03311e","externalIds":{"DBLP":"journals/corr/abs-2310-13355","ArXiv":"2310.13355","DOI":"10.48550/arXiv.2310.13355","CorpusId":264405840},"title":"SILC: Improving Vision Language Pretraining with Self-Distillation"},{"paperId":"6036d4765c7608d8136cc425a2c84be6f1253c23","externalIds":{"DBLP":"journals/tmi/ZhangYWJDKMB25","ArXiv":"2310.12868","DOI":"10.1109/TMI.2024.3519307","CorpusId":264305679,"PubMed":"40030730"},"title":"DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model"},{"paperId":"f28ff71950e1bd3171090cce7a14b34dbc882026","externalIds":{"DBLP":"journals/corr/abs-2310-13026","ArXiv":"2310.13026","DOI":"10.1145/3707447","CorpusId":265936308},"title":"Weakly-supervised Semantic Segmentation with Image-level Labels: From Traditional Models to Foundation Models"},{"paperId":"5b038c1a93967072cc76689fd805e756f804cc42","externalIds":{"DBLP":"journals/corr/abs-2310-10196","ArXiv":"2310.10196","DOI":"10.48550/arXiv.2310.10196","CorpusId":264146957},"title":"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"},{"paperId":"c9242e402a8c12d616b793661d22ed0d56a9f5e1","externalIds":{"ArXiv":"2310.02296","DBLP":"journals/corr/abs-2310-02296","DOI":"10.48550/arXiv.2310.02296","CorpusId":263620347},"title":"CLIP Is Also a Good Teacher: A New Learning Framework for Inductive Zero-shot Semantic Segmentation"},{"paperId":"b6bb3b09d6b77d008adc4788c9212a90040f2376","externalIds":{"DBLP":"journals/corr/abs-2310-01403","ArXiv":"2310.01403","DOI":"10.48550/arXiv.2310.01403","CorpusId":263605948},"title":"CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction"},{"paperId":"2537ad7d72b089859119b8a614ec02358ec3ccb6","externalIds":{"DBLP":"conf/iccv/WangZCWS023","DOI":"10.1109/ICCV51070.2023.00110","CorpusId":267024015},"title":"SegGPT: Towards Segmenting Everything In Context"},{"paperId":"8254b2dce0a5c108777dd4d439f482d7b047c9f2","externalIds":{"DBLP":"conf/iccv/WuJ0LYL23","ArXiv":"2312.15715","DOI":"10.1109/ICCV51070.2023.00240","CorpusId":266550772},"title":"Segment Every Reference Object in Spatial and Temporal Spaces"},{"paperId":"37f9b34b8973a63606cfe20d1551d2c75ab6ede8","externalIds":{"DBLP":"conf/iccv/XuXDT23","DOI":"10.1109/ICCV51070.2023.00088","CorpusId":263269833},"title":"MasQCLIP for Open-Vocabulary Universal Image Segmentation"},{"paperId":"2008356d334fb27ba7dbc638119f9d567d4adbd8","externalIds":{"DBLP":"conf/iccv/ChenZQGYZXCE23","DOI":"10.1109/ICCV51070.2023.00071","CorpusId":267026472},"title":"Exploring Open-Vocabulary Semantic Segmentation from CLIP Vision Encoder Distillation Only"},{"paperId":"6a627c258084054b2648058a78c579539d7f7bc3","externalIds":{"DBLP":"journals/corr/abs-2310-00240","ArXiv":"2310.00240","DOI":"10.48550/arXiv.2310.00240","CorpusId":263334473},"title":"Learning Mask-aware CLIP Representations for Zero-Shot Segmentation"},{"paperId":"04ebb7c24f1b2b8991031958289c7d2f2b0d2214","externalIds":{"DBLP":"journals/tomccap/JiLFLYLZ25","ArXiv":"2309.17205","DOI":"10.1145/3701733","CorpusId":263310502},"title":"Toward Complex-query Referring Image Segmentation: A Novel Benchmark"},{"paperId":"01e8f7ade6f9a625e30d79d62b389c9f6fed8bcc","externalIds":{"ArXiv":"2310.00031","DBLP":"journals/corr/abs-2310-00031","DOI":"10.1109/CVPR52733.2024.01317","CorpusId":263334482},"title":"Text-Image Alignment for Diffusion-Based Perception"},{"paperId":"a6a13dd87672560b9e66e5cfeb25e353f3936848","externalIds":{"DBLP":"journals/corr/abs-2309-14303","ArXiv":"2309.14303","DOI":"10.48550/arXiv.2309.14303","CorpusId":262826056},"title":"Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation"},{"paperId":"b512f69eb45fa74d8d2531c6474bf8d5a5c4f174","externalIds":{"DBLP":"journals/corr/abs-2309-13042","ArXiv":"2309.13042","DOI":"10.1007/s11263-024-02223-3","CorpusId":262217291},"title":"MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation"},{"paperId":"3150c9ba91d7cb4f66a8594af091fb77294f3b0a","externalIds":{"DBLP":"conf/cvpr/SickEHR24","ArXiv":"2309.12378","DOI":"10.1109/CVPR52733.2024.00349","CorpusId":262217497},"title":"Unsupervised Semantic Segmentation Through Depth-Guided Feature Correlation and Sampling"},{"paperId":"c662770fb0095e4563eaa851e8c6f769207cf79b","externalIds":{"ArXiv":"2309.10726","DBLP":"conf/icra/KappelerPVBV24","DOI":"10.1109/ICRA57147.2024.10611624","CorpusId":262054179},"title":"Few-Shot Panoptic Segmentation With Foundation Models"},{"paperId":"616e2783f7e5f2b879a0d3171a73a31d99189259","externalIds":{"DBLP":"journals/corr/abs-2309-02773","ArXiv":"2309.02773","DOI":"10.1109/TIP.2025.3551648","CorpusId":261557357,"PubMed":"40126966"},"title":"Diffusion Model is Secretly a Training-Free Open Vocabulary Semantic Segmenter"},{"paperId":"95ce88b99ff31cb10eb466ad2d748083a61d72ce","externalIds":{"ArXiv":"2308.16777","DBLP":"journals/corr/abs-2308-16777","DOI":"10.48550/arXiv.2308.16777","CorpusId":261395160},"title":"Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models"},{"paperId":"d340f5f2581ea29439fd892d306c786b316791da","externalIds":{"DBLP":"journals/corr/abs-2308-15512","ArXiv":"2308.15512","DOI":"10.1109/ICCV51070.2023.01425","CorpusId":261338917},"title":"Shatter and Gather: Learning Referring Image Segmentation with Text Supervision"},{"paperId":"90d8e66a46915ef5d6ac1b5996e8dcd61b2c1a69","externalIds":{"ArXiv":"2308.14575","DBLP":"conf/iccv/LiuLKXZY0L23","DOI":"10.1109/ICCV51070.2023.02022","CorpusId":261243179},"title":"Referring Image Segmentation Using Text Supervision"},{"paperId":"c2764eeb1c7d1c7eb6287505e54818c032ca3d7e","externalIds":{"DBLP":"journals/corr/abs-2308-14133","ArXiv":"2308.14133","DOI":"10.48550/arXiv.2308.14133","CorpusId":261242605},"title":"Cheap Lunch for Medical Image Segmentation by Fine-tuning SAM on Few Exemplars"},{"paperId":"4200ad698a5c778328d399723eb4cd3f414460ee","externalIds":{"DBLP":"conf/iccv/HuWSXLHL23","ArXiv":"2308.13853","DOI":"10.1109/ICCV51070.2023.00376","CorpusId":261243468},"title":"Beyond One-to-One: Rethinking the Referring Image Segmentation"},{"paperId":"f17e946eafd8836428d00190e160e80f8ae03393","externalIds":{"DBLP":"journals/corr/abs-2308-12757","ArXiv":"2308.12757","DOI":"10.48550/arXiv.2308.12757","CorpusId":261100986},"title":"PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning"},{"paperId":"374ebdc8240a35820cb7ab8bfca37e180e21b605","externalIds":{"DBLP":"journals/corr/abs-2308-12792","ArXiv":"2308.12792","DOI":"10.48550/arXiv.2308.12792","CorpusId":261100979},"title":"Sparks of Large Audio Models: A Survey and Outlook"},{"paperId":"82e29cc0a07e25998021c9f9af426cae11a62953","externalIds":{"DBLP":"conf/cvpr/TianACKG24","ArXiv":"2308.12469","DOI":"10.1109/CVPR52733.2024.00341","CorpusId":261101006},"title":"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion"},{"paperId":"6018ebfed08d8de54fc119b0cd43148b2c621689","externalIds":{"DBLP":"journals/corr/abs-2308-09779","ArXiv":"2308.09779","DOI":"10.48550/arXiv.2308.09779","CorpusId":261048829},"title":"EAVL: Explicitly Align Vision and Language for Referring Image Segmentation"},{"paperId":"174dd3e5fbe1950666d8b1b0498cdf38a105fd6a","externalIds":{"DBLP":"journals/tmm/ChenMZQLWX24","ArXiv":"2308.07539","DOI":"10.1109/TMM.2024.3361181","CorpusId":260900111},"title":"Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond"},{"paperId":"5e779145a1f40edce3dc01e1be06530776bcf4db","externalIds":{"DBLP":"conf/miccai/WangIXZR23","ArXiv":"2308.07156","DOI":"10.48550/arXiv.2308.07156","CorpusId":260887698},"title":"SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation"},{"paperId":"d6bb9d02a65798a26cb0d01c1d7478a004a22cb5","externalIds":{"DBLP":"journals/corr/abs-2308-06160","ArXiv":"2308.06160","DOI":"10.48550/arXiv.2308.06160","CorpusId":260865973},"title":"DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models"},{"paperId":"2b26b17fe3a909bc0f5408b3328308153b31f22e","externalIds":{"ArXiv":"2308.02487","DBLP":"conf/nips/YuHDSC23","DOI":"10.48550/arXiv.2308.02487","CorpusId":260611350},"title":"Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","externalIds":{"DBLP":"journals/corr/abs-2308-00692","ArXiv":"2308.00692","DOI":"10.1109/CVPR52733.2024.00915","CorpusId":260351258},"title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","externalIds":{"ArXiv":"2307.13721","DBLP":"journals/corr/abs-2307-13721","DOI":"10.48550/arXiv.2307.13721","CorpusId":260164769},"title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"2bc6d41caf81e62eb60b0829a521cfee085715c0","externalIds":{"ArXiv":"2307.11545","DBLP":"journals/corr/abs-2307-11545","DOI":"10.1109/ICCV51070.2023.01605","CorpusId":260091742},"title":"Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation"},{"paperId":"b78a9007186f8db25e1890757959b5b0513e220c","externalIds":{"ArXiv":"2307.09220","DBLP":"journals/corr/abs-2307-09220","DOI":"10.1109/TPAMI.2024.3413013","CorpusId":259950727,"PubMed":"38875096"},"title":"A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future"},{"paperId":"4ec198f910d4cac4ef9b6e36da33b4b29b8b2412","externalIds":{"DBLP":"conf/iros/IiokaYWHS23","ArXiv":"2307.08597","DOI":"10.1109/IROS55552.2023.10341402","CorpusId":259936805},"title":"Multimodal Diffusion Segmentation Model for Object Segmentation from Manipulation Instructions"},{"paperId":"94ce1d5924e05e8d75e43ce70044293ddcef850a","externalIds":{"DOI":"10.1038/s41591-023-02448-8","CorpusId":259947046,"PubMed":"37460753"},"title":"Large language models in medicine"},{"paperId":"2eb617e1242e8f0856f2e7b36073461cb741570d","externalIds":{"ArXiv":"2307.04767","DBLP":"journals/corr/abs-2307-04767","DOI":"10.48550/arXiv.2307.04767","CorpusId":259501973},"title":"Semantic-SAM: Segment and Recognize Anything at Any Granularity"},{"paperId":"a2ad36515be97cb0bb743ea50e47474442ea379a","externalIds":{"DBLP":"journals/corr/abs-2307-00773","ArXiv":"2307.00773","DOI":"10.48550/arXiv.2307.00773","CorpusId":259317092},"title":"DifFSS: Diffusion Model for Few-Shot Semantic Segmentation"},{"paperId":"656e1ab75ff92733b57118d916a320c51c991fd3","externalIds":{"ArXiv":"2306.13465","DBLP":"journals/mia/GongZMLWZHD24","DOI":"10.1016/j.media.2024.103324","CorpusId":259243550,"PubMed":"39213939"},"title":"3DSAM-adapter: Holistic adaptation of SAM from 2D to 3D for promptable tumor segmentation"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","externalIds":{"ArXiv":"2306.13549","DBLP":"journals/corr/abs-2306-13549","PubMedCentral":"11645129","DOI":"10.1093/nsr/nwae403","CorpusId":259243718,"PubMed":"39679213"},"title":"A survey on multimodal large language models"},{"paperId":"fd6d5e39d4e6641f3a1b7bdebd9f649c2c3705a8","externalIds":{"DBLP":"conf/eccv/KarazijaLVR24","ArXiv":"2306.09316","DOI":"10.1007/978-3-031-72652-1_18","CorpusId":259171746},"title":"Diffusion Models for Open-Vocabulary Segmentation"},{"paperId":"cd51dda4f02b2294a0b61385862d4ed091076755","externalIds":{"ArXiv":"2306.09244","DBLP":"conf/nips/0002AWVS23","DOI":"10.48550/arXiv.2306.09244","CorpusId":259165099},"title":"Text Promptable Surgical Instrument Segmentation with Vision-Language Models"},{"paperId":"2df83637e56e89521857fae102c7949a6dbbb1d9","externalIds":{"ArXiv":"2306.08498","DBLP":"conf/naacl/KimKKPK24","ACL":"2024.naacl-long.258","DOI":"10.18653/v1/2024.naacl-long.258","CorpusId":259164781},"title":"Extending CLIPâ€™s Image-Text Alignment to Referring Image Segmentation"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"74e3b4dd3f12d0eae1de836be288fe19c3f5e2db","externalIds":{"ArXiv":"2306.03437","DBLP":"journals/corr/abs-2306-03437","DOI":"10.48550/arXiv.2306.03437","CorpusId":259088908},"title":"DFormer: Diffusion-guided Transformer for Universal Image Segmentation"},{"paperId":"5d3b032270276a0c17ef5fb76679291186dccdca","externalIds":{"DBLP":"conf/nips/GuCHRYZGKCCR23","ArXiv":"2306.01736","DOI":"10.48550/arXiv.2306.01736","CorpusId":259063828},"title":"DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model"},{"paperId":"36fff28902cfb6c99c7c98f284639ad8e0133c44","externalIds":{"DBLP":"journals/corr/abs-2306-01567","ArXiv":"2306.01567","DOI":"10.48550/arXiv.2306.01567","CorpusId":259063834},"title":"Segment Anything in High Quality"},{"paperId":"bd4c8507ec2fda5cd2e989b9a55c6d1879378654","externalIds":{"DBLP":"journals/corr/abs-2307-03407","ArXiv":"2307.03407","DOI":"10.1109/CVPR52729.2023.01880","CorpusId":259375795},"title":"Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation"},{"paperId":"1ec4bc98fafa8d338f676f7a1b1b1131e8ca978e","externalIds":{"DBLP":"conf/cvpr/0072DJ23","ArXiv":"2306.00968","DOI":"10.1109/CVPR52729.2023.02259","CorpusId":258999674},"title":"GRES: Generalized Referring Expression Segmentation"},{"paperId":"15e61063ddefcbe46d1ad5a1f4ce2e4a55e1a2ec","externalIds":{"DBLP":"conf/cvpr/YiCWYYL23","DOI":"10.1109/CVPR52729.2023.00683","CorpusId":260843463},"title":"A Simple Framework for Text-Supervised Semantic Segmentation"},{"paperId":"732db1ceced9bc1a22e8943a7d1468d104d69ae8","externalIds":{"DBLP":"journals/corr/abs-2306-11087","ArXiv":"2306.11087","DOI":"10.1109/CVPR52729.2023.01081","CorpusId":259203013},"title":"Primitive Generation and Semantic-Related Alignment for Universal Zero-Shot Segmentation"},{"paperId":"e49b1b6227afbe16f01174a72dbf2868915f5aac","externalIds":{"DBLP":"conf/nips/HedlinSMIKTY23","ArXiv":"2305.15581","DOI":"10.48550/arXiv.2305.15581","CorpusId":258887715},"title":"Unsupervised Semantic Correspondence Using Stable Diffusion"},{"paperId":"13a5140fc0b269c408ecfc666cb297410bc753c5","externalIds":{"DBLP":"conf/iclr/LiuZL0WS24","ArXiv":"2305.13310","DOI":"10.48550/arXiv.2305.13310","CorpusId":258832414},"title":"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching"},{"paperId":"43a55dbd95c9d5cd82de8db276f41adeec4a937d","externalIds":{"DBLP":"journals/corr/abs-2305-12799","ArXiv":"2305.12799","DOI":"10.48550/arXiv.2305.12799","CorpusId":258832615},"title":"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"},{"paperId":"1856bebc4cb35e68368d9c83bd2ac2d26cd4bcfa","externalIds":{"DBLP":"journals/corr/abs-2305-08196","ArXiv":"2305.08196","DOI":"10.48550/arXiv.2305.08196","CorpusId":258686670},"title":"A Comprehensive Survey on Segment Anything Model for Vision and Beyond"},{"paperId":"bbdc4118df106d4ba7af9d7d94d7f0a1144c11e2","externalIds":{"ArXiv":"2305.06558","DBLP":"journals/corr/abs-2305-06558","DOI":"10.48550/arXiv.2305.06558","CorpusId":258615204},"title":"Segment and Track Anything"},{"paperId":"6ba8c408bb833c2f2aeea70107d56184417077d4","externalIds":{"DBLP":"journals/corr/abs-2305-05803","ArXiv":"2305.05803","DOI":"10.48550/arXiv.2305.05803","CorpusId":258587943},"title":"Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation"},{"paperId":"d894ae9392554ff79dd23dc7c1c48c1d9e613b96","externalIds":{"DBLP":"journals/tip/LiuLGLZ23","DOI":"10.1109/TIP.2023.3272826","CorpusId":258566515,"PubMed":"37155388"},"title":"Tripartite Feature Enhanced Pyramid Network for Dense Prediction"},{"paperId":"90720183d174744680fa7a3028996b3399c64fa2","externalIds":{"DBLP":"conf/iclr/Zhang0GYP000024","ArXiv":"2305.03048","DOI":"10.48550/arXiv.2305.03048","CorpusId":258480276},"title":"Personalize Segment Anything Model with One Shot"},{"paperId":"3ae97a7f1391ab35fcf9952437a37b18b2676126","externalIds":{"DBLP":"journals/corr/abs-2305-02187","ArXiv":"2305.02187","DOI":"10.48550/arXiv.2305.02187","CorpusId":258461201},"title":"CLUSTSEG: Clustering for Universal Segmentation"},{"paperId":"57be0448d168e8d6d0b6e0d1a4405fb5fbaa1b56","externalIds":{"DBLP":"journals/corr/abs-2305-01115","ArXiv":"2305.01115","DOI":"10.48550/arXiv.2305.01115","CorpusId":258437037},"title":"In-Context Learning Unlocked for Diffusion Models"},{"paperId":"043d4663aa8e13cc66537f591dc6350e78f453ab","externalIds":{"DBLP":"conf/cvpr/HeJGR23","ArXiv":"2305.01040","DOI":"10.1109/CVPR52729.2023.01078","CorpusId":258436775},"title":"CLIP-S4: Language-Guided Self-Supervised Semantic Segmentation"},{"paperId":"108c383312694a5bfdaa6d59445172e0d02db6c6","externalIds":{"DBLP":"journals/corr/abs-2305-00035","ArXiv":"2305.00035","DOI":"10.48550/arXiv.2305.00035","CorpusId":258426709},"title":"SAM on Medical Images: A Comprehensive Study on Three Prompt Modes"},{"paperId":"6027059185e685a1ddd973e977d6b7b808268955","externalIds":{"ArXiv":"2304.14006","DBLP":"journals/corr/abs-2304-14006","DOI":"10.48550/arXiv.2304.14006","CorpusId":258352755},"title":"Edit Everything: A Text-Guided Generative System for Images Editing"},{"paperId":"705315602e9e78b155220169d7704475efeb4a11","externalIds":{"DBLP":"journals/corr/abs-2304-13785","ArXiv":"2304.13785","DOI":"10.48550/arXiv.2304.13785","CorpusId":258352583},"title":"Customized Segment Anything Model for Medical Image Segmentation"},{"paperId":"49f9882d5fd442f02f9c9dff780336f6dce2da4f","externalIds":{"DBLP":"journals/corr/abs-2304-12620","ArXiv":"2304.12620","DOI":"10.48550/arXiv.2304.12620","CorpusId":258309597,"PubMed":"40121809"},"title":"Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation"},{"paperId":"c6ac64877b2bcac2389be4d91a62dea8c6f73844","externalIds":{"DBLP":"journals/mia/MazurowskiDGYKZ23","ArXiv":"2304.10517","DOI":"10.1016/j.media.2023.102918","CorpusId":258236547,"PubMed":"37595404"},"title":"Segment Anything Model for Medical Image Analysis: an Experimental Study"},{"paperId":"d203076c28587895aa344d088b2788dbab5e82a1","externalIds":{"DBLP":"journals/pami/LiDYZPCCLL24","ArXiv":"2304.09854","DOI":"10.1109/TPAMI.2024.3434373","CorpusId":258212528,"PubMed":"39074008"},"title":"Transformer-Based Visual Segmentation: A Survey"},{"paperId":"25d0d032c76b9c09613faa35e25f2997aac261a5","externalIds":{"DBLP":"conf/cvpr/LiuTWWSZZZ23","ArXiv":"2304.08491","DOI":"10.1109/CVPR52729.2023.00293","CorpusId":258179191},"title":"Delving into Shape-aware Zero-shot Semantic Segmentation"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","externalIds":{"DBLP":"journals/corr/abs-2304-08485","ArXiv":"2304.08485","DOI":"10.48550/arXiv.2304.08485","CorpusId":258179774},"title":"Visual Instruction Tuning"},{"paperId":"171db6c9cd7ae2e0ae21a37e0ad17cd8ba99f44c","externalIds":{"DBLP":"journals/corr/abs-2304-07875","ArXiv":"2304.07875","DOI":"10.48550/arXiv.2304.07875","CorpusId":258179093},"title":"The Segment Anything foundation model achieves favorable brain tumor autosegmentation accuracy on MRI to support radiotherapy treatment planning"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"0819c1e60c13b9797f937282d06b54d252d9d6ec","externalIds":{"ArXiv":"2304.06718","DBLP":"conf/nips/ZouYZLLWWGL23","DOI":"10.48550/arXiv.2304.06718","CorpusId":258108410},"title":"Segment Everything Everywhere All at Once"},{"paperId":"34620b2bbe82a58b85dbaa4064226a772d136feb","externalIds":{"ArXiv":"2304.05653","DBLP":"journals/pr/LiWDZL25","DOI":"10.1016/j.patcog.2025.111409","CorpusId":258079047},"title":"A closer look at the explainability of Contrastive language-image pre-training"},{"paperId":"9cef5a098486aeab6ed3700c5e3d29488488d16f","externalIds":{"DBLP":"journals/tip/SunCWWL25","ArXiv":"2304.04748","DOI":"10.1109/TIP.2025.3554410","CorpusId":258048653,"PubMed":"40168207"},"title":"Exploring Effective Factors for Improving Visual In-Context Learning"},{"paperId":"0931888d5cd7c26427cc116af2ac33863552da27","externalIds":{"ArXiv":"2304.05396","DBLP":"journals/corr/abs-2304-05396","DOI":"10.48550/arXiv.2304.05396","CorpusId":258078785},"title":"SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model"},{"paperId":"f144738de47f109d7016cc817831387ed4a256df","externalIds":{"DBLP":"journals/corr/abs-2304-04745","ArXiv":"2304.04745","DOI":"10.1109/CVPR52729.2023.01110","CorpusId":258048896},"title":"Ambiguous Medical Image Segmentation Using Diffusion Models"},{"paperId":"b5af4c14297b0caa73e8ee532a9b6dadc3815646","externalIds":{"DBLP":"journals/corr/abs-2304-04155","ArXiv":"2304.04155","DOI":"10.48550/arXiv.2304.04155","CorpusId":258049163,"PubMed":"40190816"},"title":"Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","externalIds":{"DBLP":"conf/iccv/KirillovMRMRGXW23","ArXiv":"2304.02643","DOI":"10.1109/ICCV51070.2023.00371","CorpusId":257952310},"title":"Segment Anything"},{"paperId":"1775333ad759bc827b8fde72c93ccbaca821c1e7","externalIds":{"PubMedCentral":"10160739","DOI":"10.1088/1361-6560/acca5c","CorpusId":257954358,"PubMed":"37015231"},"title":"2D medical image synthesis using transformer-based denoising diffusion probabilistic model"},{"paperId":"0f144e94bba22979f5af7c04ca25d15f8c7dd270","externalIds":{"ArXiv":"2304.01114","DBLP":"journals/corr/abs-2304-01114","DOI":"10.48550/arXiv.2304.01114","CorpusId":257913065},"title":"Associating Spatially-Consistent Grouping with Text-supervised Semantic Segmentation"},{"paperId":"a08d35f4708162158f7a7e9e794afa1215c429e9","externalIds":{"ArXiv":"2304.00779","DBLP":"conf/cvpr/KwonSJKJS23","DOI":"10.1109/CVPR52729.2023.00654","CorpusId":257913122},"title":"Probabilistic Prompt Learning for Dense Prediction"},{"paperId":"e605b51e67a0cd0b097b244fee8add33b97f0094","externalIds":{"ArXiv":"2304.01198","DBLP":"conf/iccv/HanZLH023","DOI":"10.1109/ICCV51070.2023.00106","CorpusId":257913359},"title":"Open-Vocabulary Semantic Segmentation with Decoupled One-Pass Network"},{"paperId":"f9a7175198a2c9f3ab0134a12a7e9e5369428e42","externalIds":{"DBLP":"journals/corr/abs-2303-18223","ArXiv":"2303.18223","CorpusId":257900969},"title":"A Survey of Large Language Models"},{"paperId":"c70833ab04675e6e339739c11eebd20c60db3d9f","externalIds":{"DBLP":"conf/cvpr/YuSS23","ArXiv":"2303.17811","DOI":"10.1109/CVPR52729.2023.01864","CorpusId":257900942},"title":"Zero-shot Referring Image Segmentation with Global-Local Context Features"},{"paperId":"362983f6f6b3e0335f1267b7b9d0288cc4d2619f","externalIds":{"ArXiv":"2303.17225","DBLP":"conf/cvpr/OinWYLRXWWWPW23","DOI":"10.1109/CVPR52729.2023.01863","CorpusId":257833847},"title":"FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation"},{"paperId":"2b50e72ffd2db2915dd1c6bddab710195cc64583","externalIds":{"DBLP":"conf/iccv/JiCXHLLLLL23","ArXiv":"2303.17559","DOI":"10.1109/ICCV51070.2023.01987","CorpusId":257833921},"title":"DDP: Diffusion Model for Dense Visual Prediction"},{"paperId":"4702d5a163477c734a54f3ed2d171dca1504eaae","externalIds":{"DBLP":"conf/iccv/LiPDBP23","ArXiv":"2303.16203","DOI":"10.1109/ICCV51070.2023.00210","CorpusId":257771787},"title":"Your Diffusion Model is Secretly a Zero-Shot Classifier"},{"paperId":"525c293b19b9668100172285c295317e5b2d999a","externalIds":{"DBLP":"journals/corr/abs-2303-15014","ArXiv":"2303.15014","DOI":"10.1109/CVPR52729.2023.01872","CorpusId":257766466},"title":"Leveraging Hidden Positives for Unsupervised Semantic Segmentation"},{"paperId":"1bba2a9c6db3b356b8ae6ef2efff5645e4d96c2b","externalIds":{"DBLP":"journals/corr/abs-2303-15233","ArXiv":"2303.15233","DOI":"10.48550/arXiv.2303.15233","CorpusId":257767039},"title":"Text-to-Image Diffusion Models are Zero-Shot Classifiers"},{"paperId":"aa207668318fec38d60b79f407fb64982e46fce9","externalIds":{"ArXiv":"2303.14814","DBLP":"journals/corr/abs-2303-14814","DOI":"10.1109/CVPR52729.2023.01878","CorpusId":257766897},"title":"WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation"},{"paperId":"6a0880a69ac0226339616860553a2f449b520a3a","externalIds":{"DBLP":"conf/iccv/Rewatbowornwong23","ArXiv":"2303.13396","DOI":"10.1109/ICCV51070.2023.00113","CorpusId":257687412},"title":"Zero-guidance Segmentation Using Zero Segment Labels"},{"paperId":"a960f30c7a265678d7765dc0d81d6a5131b475fa","externalIds":{"DBLP":"journals/corr/abs-2303-12343","ArXiv":"2303.12343","DOI":"10.1109/ICCV51070.2023.00384","CorpusId":257663776},"title":"LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation"},{"paperId":"804fc51343267224a0c654bbf8bec67a96062fad","externalIds":{"DBLP":"conf/cvpr/ChoSHASK24","ArXiv":"2303.11797","DOI":"10.1109/CVPR52733.2024.00394","CorpusId":257636688},"title":"CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation"},{"paperId":"5b84bab8a8f18e803db8d3db7ab6e4fe08fc3959","externalIds":{"ArXiv":"2303.11681","DBLP":"conf/iccv/WuZSZS23","DOI":"10.1109/ICCV51070.2023.00117","CorpusId":257636752},"title":"DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models"},{"paperId":"41f4dc53ed4550653d07cb3d5472a6f0af3115b7","externalIds":{"DBLP":"journals/corr/abs-2303-11324","ArXiv":"2303.11324","DOI":"10.1109/ICCV51070.2023.00111","CorpusId":257632184},"title":"Open-vocabulary Panoptic Segmentation with Embedding Modulation"},{"paperId":"60bba8bfacfef6e5a9c08b31ecbe0a872fcc7a23","externalIds":{"DBLP":"journals/corr/abs-2303-11316","ArXiv":"2303.11316","DOI":"10.1109/CVPR52729.2023.00687","CorpusId":257631921},"title":"Generative Semantic Segmentation"},{"paperId":"0e23c9999c2afe2e494548a77a9ad6ec23897a3b","externalIds":{"DBLP":"conf/iccv/HanLLDLWTYFZW23","ArXiv":"2303.09181","DOI":"10.1109/ICCV51070.2023.00080","CorpusId":257557629},"title":"Global Knowledge Calibration for Fast Open-Vocabulary Segmentation"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"5fc1da3886407209151466f9b0b656e5883d9704","externalIds":{"DBLP":"journals/corr/abs-2303-08888","ArXiv":"2303.08888","DOI":"10.1109/ICCV51070.2023.00109","CorpusId":257557537},"title":"Stochastic Segmentation with Conditional Categorical Diffusion Models"},{"paperId":"21cbb159992abffdee87c2a1bc15a3d98bdfde8a","externalIds":{"ArXiv":"2303.06345","DBLP":"conf/aaai/0002WT0ZT23","DOI":"10.48550/arXiv.2303.06345","CorpusId":257496824},"title":"Semantics-Aware Dynamic Localization and Refinement for Referring Image Segmentation"},{"paperId":"a86846ad0893595896c36f465cf4f7ce07945247","externalIds":{"DBLP":"journals/corr/abs-2303-05105","ArXiv":"2303.05105","DOI":"10.48550/arXiv.2303.05105","CorpusId":257427135},"title":"MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation"},{"paperId":"c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0","externalIds":{"DBLP":"conf/eccv/LiuZRLZYJLYSZZ24","ArXiv":"2303.05499","DOI":"10.48550/arXiv.2303.05499","CorpusId":257427307},"title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"},{"paperId":"323400245885e08ad498cd108e30e18020662278","externalIds":{"DBLP":"conf/cvpr/XuLVBWM23","ArXiv":"2303.04803","DOI":"10.1109/CVPR52729.2023.00289","CorpusId":257405338},"title":"Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models"},{"paperId":"c09ca9da1fce13b1560f45c38321c7bb971f13fc","externalIds":{"ArXiv":"2303.02153","DBLP":"journals/corr/abs-2303-02153","DOI":"10.1109/ICCV51070.2023.00527","CorpusId":257353292},"title":"Unleashing Text-to-Image Diffusion Models for Visual Perception"},{"paperId":"641d7866db6691e22aa36de5c8ba05804233c016","externalIds":{"ArXiv":"2302.12242","DBLP":"conf/cvpr/XuZWHB23","DOI":"10.1109/CVPR52729.2023.00288","CorpusId":257102616},"title":"Side Adapter Network for Open-Vocabulary Semantic Segmentation"},{"paperId":"55cd2d0a8f26c4dc458303f937af2b6fb8f8b693","externalIds":{"DBLP":"conf/cvpr/LiuDCZSMM23","ArXiv":"2302.07387","DOI":"10.1109/CVPR52729.2023.01789","CorpusId":256869991},"title":"PolyFormer: Referring Image Segmentation as Sequential Polygon Generation"},{"paperId":"097368d420604dc33de6361c9353f62d607a967e","externalIds":{"ArXiv":"2302.06378","DBLP":"journals/corr/abs-2302-06378","DOI":"10.1561/0600000095","CorpusId":253028117},"title":"Semantic Image Segmentation: Two Decades of Research"},{"paperId":"daf61010eee0fbf6f9bab7db71c395ffca6f3ff3","externalIds":{"ArXiv":"2302.03027","DBLP":"conf/siggraph/ParmarS0LLZ23","DOI":"10.1145/3588432.3591513","CorpusId":256616002},"title":"Zero-shot Image-to-Image Translation"},{"paperId":"5f61ddc37476acf3741b0bfe5fcb59639cadbb86","externalIds":{"ArXiv":"2301.13670","DBLP":"conf/nips/ZhangZ023","DOI":"10.48550/arXiv.2301.13670","CorpusId":256416477},"title":"What Makes Good Examples for Visual In-Context Learning?"},{"paperId":"1944edf86dd46dbc92e88d296b270e8bf5fa3a87","externalIds":{"ArXiv":"2301.12171","DBLP":"journals/corr/abs-2301-12171","DOI":"10.48550/arXiv.2301.12171","CorpusId":256389445},"title":"ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts"},{"paperId":"c74b5d298cd5fe735f2c8bc18a94f28010a2ccfc","externalIds":{"DBLP":"conf/cvpr/0007GYM23","ArXiv":"2301.11320","DOI":"10.1109/CVPR52729.2023.00305","CorpusId":256274671},"title":"Cut and Learn for Unsupervised Object Detection and Instance Segmentation"},{"paperId":"967907503b24423b9b74621051811fcf684e3957","externalIds":{"DBLP":"journals/corr/abs-2212-11270","ArXiv":"2212.11270","DOI":"10.1109/CVPR52729.2023.01451","CorpusId":254926770},"title":"Generalized Decoding for Pixel, Image, and Language"},{"paperId":"3a27dfb4b87f74c3c663cc42cec83ccd58f72f23","externalIds":{"DBLP":"conf/cvpr/Lin00WLLL023","ArXiv":"2212.09506","DOI":"10.1109/CVPR52729.2023.01469","CorpusId":254854232},"title":"CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation"},{"paperId":"8ca316a10a2749e4c6bf3d0284e8cce2f56a4543","externalIds":{"ArXiv":"2212.04994","DBLP":"conf/cvpr/MukhotiLPWSTL23","DOI":"10.1109/CVPR52729.2023.01860","CorpusId":254535609},"title":"Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning"},{"paperId":"77d190cf4b0b3e3f80e65decba68cb51bb893ac4","externalIds":{"DBLP":"conf/icml/ZhaoSBC0WY0Z00Y23","ArXiv":"2212.03863","CorpusId":258987252},"title":"X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion"},{"paperId":"67db43cb6cc618c873c63fe2c83025c335b7a230","externalIds":{"DBLP":"journals/corr/abs-2212-03588","ArXiv":"2212.03588","DOI":"10.1109/CVPR52729.2023.01075","CorpusId":254366494},"title":"ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation"},{"paperId":"9ceaeff7117965832f4c05fd6355d021862d0a82","externalIds":{"DBLP":"conf/cvpr/WangWCS023","ArXiv":"2212.02499","DOI":"10.1109/CVPR52729.2023.00660","CorpusId":254246343},"title":"Images Speak in Images: A Generalist Painter for In-Context Visual Learning"},{"paperId":"19e01fe0194dfff19f113ce8cec07b808e945a08","externalIds":{"DBLP":"journals/corr/abs-2212-01769","ArXiv":"2212.01769","DOI":"10.48550/arXiv.2212.01769","CorpusId":254246885},"title":"CoupAlign: Coupling Word-Pixel with Sentence-Mask Alignments for Referring Image Segmentation"},{"paperId":"4bae689ade260c1624406b5bf2d58d637a0c5aa9","externalIds":{"DBLP":"conf/icml/Luo0W0023","ArXiv":"2211.14813","DOI":"10.48550/arXiv.2211.14813","CorpusId":254043520},"title":"SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation"},{"paperId":"288b24f16fe7341c91def471120fa23233e34acc","externalIds":{"DBLP":"journals/corr/abs-2211-13224","ArXiv":"2211.13224","DOI":"10.48550/arXiv.2211.13224","CorpusId":253801576},"title":"Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors"},{"paperId":"6a993404e07687b7edb7fb9a05092213a9419859","externalIds":{"DBLP":"conf/cvpr/Jain0C0OS23","ArXiv":"2211.06220","DOI":"10.1109/CVPR52729.2023.00292","CorpusId":253499214},"title":"OneFormer: One Transformer to Rule Universal Image Segmentation"},{"paperId":"df065b3cd3621211320d9900186243aa1d086067","externalIds":{"DBLP":"conf/bmvc/MaYW0X22","ArXiv":"2210.15138","DOI":"10.48550/arXiv.2210.15138","CorpusId":253157926},"title":"Open-vocabulary Semantic Segmentation with Frozen Vision-Language Models"},{"paperId":"ed3d86bab24ae0414602785e63b89944b58c0918","externalIds":{"ArXiv":"2210.09996","DBLP":"conf/iccv/RanasingheMRYTS23","DOI":"10.1109/ICCV51070.2023.00513","CorpusId":257557352},"title":"Perceptual Grouping in Contrastive Vision-Language Models"},{"paperId":"8f48171bf05474449777d9bbf6766d480332e09f","externalIds":{"DBLP":"journals/corr/abs-2210-06366","ArXiv":"2210.06366","DOI":"10.1109/ICCV51070.2023.00090","CorpusId":252846168},"title":"A Generalist Framework for Panoptic Segmentation of Images and Videos"},{"paperId":"37d626720f5003373336098ce7c01a1a38e6b63d","externalIds":{"ArXiv":"2210.04885","DBLP":"conf/acl/TangLPJYKSLT23","ACL":"2023.acl-long.310","DOI":"10.48550/arXiv.2210.04885","CorpusId":252780146},"title":"What the DAAM: Interpreting Stable Diffusion Using Cross Attention"},{"paperId":"29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76","externalIds":{"DBLP":"journals/corr/abs-2210-04150","ArXiv":"2210.04150","DOI":"10.1109/CVPR52729.2023.00682","CorpusId":252780581},"title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP"},{"paperId":"ce62ee331a43338ccaad0461faa0dfbe5db3d6a2","externalIds":{"DBLP":"conf/miccai/FernandezPBTGVC22","ArXiv":"2209.08256","DOI":"10.1007/978-3-031-16980-9_8","CorpusId":252368079},"title":"Can Segmentation Models Be Trained with Fully Synthetically Generated Data?"},{"paperId":"5b1a2d2016fdb87572a07baf65fb0d64cffdf03c","externalIds":{"DBLP":"journals/corr/abs-2209-07383","ArXiv":"2209.07383","DOI":"10.48550/arXiv.2209.07383","CorpusId":252284009},"title":"Visual Recognition with Deep Nearest Centroids"},{"paperId":"efa1647594b236361610a20d507127f0586a379b","externalIds":{"DBLP":"journals/corr/abs-2209-04747","ArXiv":"2209.04747","DOI":"10.1109/TPAMI.2023.3261988","CorpusId":252199918,"PubMed":"37030794"},"title":"Diffusion Models in Vision: A Survey"},{"paperId":"f2c899599e11ec7c503e7d1a9f07930c8d1d7f96","externalIds":{"DBLP":"journals/mia/ZhouLBLUK23","DOI":"10.1016/j.media.2022.102599","CorpusId":252139368,"PubMed":"36327652"},"title":"Volumetric memory network for interactive medical image segmentation"},{"paperId":"ba2c14ce1266861d5c92b2705c53821b034c4ce7","externalIds":{"DBLP":"journals/corr/abs-2209-00383","ArXiv":"2209.00383","DOI":"10.1109/TPAMI.2023.3305122","CorpusId":251979706,"PubMed":"37594874"},"title":"TokenCut: Segmenting Objects in Images and Videos With Self-Supervised Transformer and Normalized Cut"},{"paperId":"0a25c137edc7c9752aa6d99ae4084683c3fe6b56","externalIds":{"DBLP":"conf/nips/BarGDGE22","ArXiv":"2209.00647","DOI":"10.48550/arXiv.2209.00647","CorpusId":251979350},"title":"Visual Prompting via Image Inpainting"},{"paperId":"930e17321a3ad3721339c6dc4bd0b624d90ee695","externalIds":{"DBLP":"conf/icml/DingWT23","ArXiv":"2208.08984","CorpusId":259108833},"title":"Open-Vocabulary Universal Image Segmentation with MaskCLIP"},{"paperId":"b64537bdf7a103aa01972ba06ea24a9c08f7cd74","externalIds":{"DBLP":"conf/iclr/ChenZH23","ArXiv":"2208.04202","DOI":"10.48550/arXiv.2208.04202","CorpusId":251402961},"title":"Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning"},{"paperId":"04e541391e8dce14d099d00fb2c21dbbd8afe87f","externalIds":{"DBLP":"journals/corr/abs-2208-01626","ArXiv":"2208.01626","DOI":"10.48550/arXiv.2208.01626","CorpusId":251252882},"title":"Prompt-to-Prompt Image Editing with Cross Attention Control"},{"paperId":"c399b8d44dac36982b0d0b2b037c74740fa3dca7","externalIds":{"DBLP":"journals/corr/abs-2207-05027","ArXiv":"2207.05027","DOI":"10.48550/arXiv.2207.05027","CorpusId":250425754},"title":"Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations"},{"paperId":"41108bb9d75a7029bab949606a8e19f29d325ba5","externalIds":{"ArXiv":"2207.01223","DBLP":"journals/pami/ShenPWWCJXYT23","DOI":"10.1109/TPAMI.2023.3246102","CorpusId":250264410,"PubMed":"37027561"},"title":"A Survey on Label-Efficient Deep Image Segmentation: Bridging the Gap Between Weak Supervision and Dense Prediction"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","externalIds":{"DBLP":"conf/nips/LewkowyczADDMRS22","ArXiv":"2206.14858","DOI":"10.48550/arXiv.2206.14858","CorpusId":250144408},"title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"b893b495057c3abb353cfb75c35ef37696946215","externalIds":{"ArXiv":"2206.09592","CorpusId":254877779},"title":"DALL-E for Detection: Language-driven Compositional Image Synthesis for Object Detection"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","externalIds":{"DBLP":"journals/corr/abs-2206-07682","ArXiv":"2206.07682","DOI":"10.48550/arXiv.2206.07682","CorpusId":249674500},"title":"Emergent Abilities of Large Language Models"},{"paperId":"f10a5e608335ed3f2f7907c6da59c58e01bea69e","externalIds":{"DBLP":"journals/corr/abs-2206-06363","ArXiv":"2206.06363","DOI":"10.48550/arXiv.2206.06363","CorpusId":249626477},"title":"Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation"},{"paperId":"4f1e89f51bbc67bb61d69a1ee7ff1277322e1959","externalIds":{"ArXiv":"2205.07839","DBLP":"journals/corr/abs-2205-07839","DOI":"10.1109/CVPR52688.2022.00818","CorpusId":248811034},"title":"Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization"},{"paperId":"4d0f783971c8ef23a6be0e430d7c5fdfd787bb3d","externalIds":{"DBLP":"journals/corr/abs-2205-04725","ArXiv":"2205.04725","DOI":"10.48550/arXiv.2205.04725","CorpusId":248665489},"title":"Weakly-supervised segmentation of referring expressions"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","externalIds":{"DBLP":"journals/corr/abs-2204-14198","ArXiv":"2204.14198","CorpusId":248476411},"title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"23c13584e1549e88b20b8c1f8d601646958e4471","externalIds":{"DBLP":"conf/cvpr/ZieglerA22","ArXiv":"2204.13101","DOI":"10.1109/CVPR52688.2022.01410","CorpusId":248405951},"title":"Self-Supervised Learning of Object Parts for Semantic Segmentation"},{"paperId":"7475b217b16ce44c64e070f59972e999dca0a771","externalIds":{"DBLP":"journals/corr/abs-2203-15102","ArXiv":"2203.15102","DOI":"10.1109/CVPR52688.2022.00261","CorpusId":247778537},"title":"Rethinking Semantic Segmentation: A Prototype View"},{"paperId":"ada710008accd47d06014cb44d963457ec5cab41","externalIds":{"ArXiv":"2203.14335","DBLP":"journals/corr/abs-2203-14335","DOI":"10.1109/CVPR52688.2022.00131","CorpusId":247762253},"title":"Deep Hierarchical Semantic Segmentation"},{"paperId":"e70af9ee02523dc62c2096c09c1a638210f9dcde","externalIds":{"ArXiv":"2203.09773","DBLP":"journals/pami/LiangWZMLY23","DOI":"10.1109/TPAMI.2023.3262578","CorpusId":247593860,"PubMed":"37819831"},"title":"Local-Global Context Aware Transformer for Language-Guided Video Segmentation"},{"paperId":"918646809c7e22c94994fe80c3a2840b4c951a3c","externalIds":{"ArXiv":"2203.08414","DBLP":"journals/corr/abs-2203-08414","DOI":"10.48550/arXiv.2203.08414","CorpusId":247476291},"title":"Unsupervised Semantic Segmentation by Distilling Feature Correspondences"},{"paperId":"67804a5870b8a3a997c615784e17465889192bf6","externalIds":{"DBLP":"journals/corr/abs-2202-11539","ArXiv":"2202.11539","DOI":"10.1109/CVPR52688.2022.01414","CorpusId":247058696},"title":"Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut"},{"paperId":"0b5f27a5766c5d1394a6282ad94fec21d620bd6b","externalIds":{"ArXiv":"2202.11094","DBLP":"conf/cvpr/XuMLBBKW22","DOI":"10.1109/CVPR52688.2022.01760","CorpusId":247026092},"title":"GroupViT: Semantic Segmentation Emerges from Text Supervision"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"cc9826c222ac1e81b4b374dd9e0df130f298b1e8","externalIds":{"ArXiv":"2201.03546","DBLP":"journals/corr/abs-2201-03546","CorpusId":245836975},"title":"Language-driven Semantic Segmentation"},{"paperId":"ae92c1fd30ba49212e16152646e9f7f1ae385b08","externalIds":{"ArXiv":"2112.14757","DBLP":"conf/eccv/XuZWLCHB22","DOI":"10.1007/978-3-031-19818-2_42","CorpusId":253120388},"title":"A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre-trained Vision-Language Model"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","externalIds":{"ArXiv":"2112.10752","DBLP":"journals/corr/abs-2112-10752","DOI":"10.1109/CVPR52688.2022.01042","CorpusId":245335280},"title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"e77c484af99fc1eb3d3c36699ac81822e98cb74d","externalIds":{"DBLP":"conf/cvpr/LuddeckeE22","ArXiv":"2112.10003","DOI":"10.1109/CVPR52688.2022.00695","CorpusId":247794227},"title":"Image Segmentation Using Text and Image Prompts"},{"paperId":"eef9b31e672cfa18d323ebd653d11f204ee80039","externalIds":{"DBLP":"journals/corr/abs-2112-07910","ArXiv":"2112.07910","DOI":"10.1109/CVPR52688.2022.01129","CorpusId":245144732},"title":"Decoupling Zero-Shot Semantic Segmentation"},{"paperId":"3966121cb48c2195b5e6d63e3de4122284c67309","externalIds":{"ArXiv":"2112.05814","DBLP":"journals/corr/abs-2112-05814","CorpusId":245123845},"title":"Deep ViT Features as Dense Visual Descriptors"},{"paperId":"42f2271cebb7f272b0066c1f22d33381f139ee68","externalIds":{"ArXiv":"2112.03126","DBLP":"journals/corr/abs-2112-03126","CorpusId":244908617},"title":"Label-Efficient Semantic Segmentation with Diffusion Models"},{"paperId":"03a718819cdd3fcc102db640330037ff46431d67","externalIds":{"DBLP":"conf/midl/WollebSBVC22","ArXiv":"2112.03145","CorpusId":244908570},"title":"Diffusion Models for Implicit Image Segmentation Ensembles"},{"paperId":"ca1d5aa8f63707931692bf6a62642becf928c1fa","externalIds":{"ArXiv":"2112.02244","DBLP":"journals/corr/abs-2112-02244","DOI":"10.1109/CVPR52688.2022.01762","CorpusId":244909191},"title":"LAVT: Language-Aware Vision Transformer for Referring Image Segmentation"},{"paperId":"0a7e7347e16bf13d710f6f3d30748baabdbb96ad","externalIds":{"DBLP":"conf/eccv/ZhouLD22","ArXiv":"2112.01071","DOI":"10.1007/978-3-031-19815-1_40","CorpusId":251105026},"title":"Extract Free Dense Labels from CLIP"},{"paperId":"6d1ef4436904de111c8b1975bbf25d3fe2f165f7","externalIds":{"ArXiv":"2112.01518","DBLP":"conf/cvpr/RaoZ0TZH0L22","DOI":"10.1109/CVPR52688.2022.01755","CorpusId":244800733},"title":"DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting"},{"paperId":"658a017302d29e4acf4ca789cb5d9f27983717ff","externalIds":{"DBLP":"conf/cvpr/ChengMSKG22","ArXiv":"2112.01527","DOI":"10.1109/CVPR52688.2022.00135","CorpusId":244799297},"title":"Masked-attention Mask Transformer for Universal Image Segmentation"},{"paperId":"76a2b197b5427ffd1d3470c6d3ea026588eb5d0a","externalIds":{"DBLP":"journals/corr/abs-2111-15174","ArXiv":"2111.15174","DOI":"10.1109/CVPR52688.2022.01139","CorpusId":244729320},"title":"CRIS: CLIP-Driven Referring Image Segmentation"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","externalIds":{"DBLP":"conf/cvpr/HeCXLDG22","ArXiv":"2111.06377","DOI":"10.1109/CVPR52688.2022.01553","CorpusId":243985980},"title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"9d0340708906b5cea55ea4ef1f9a69b5b5c2ca75","externalIds":{"ArXiv":"2109.14279","DBLP":"journals/corr/abs-2109-14279","DOI":"10.5244/c.35.365","CorpusId":238215682},"title":"Localizing Objects with Self-Supervised Transformers and no Labels"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"260ad39a1dac4b451019e2bf17925f4df8e3b69a","externalIds":{"DBLP":"conf/nips/ChengSK21","ArXiv":"2107.06278","CorpusId":235829267},"title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"4f3ed96bd367930a5f9c46600a3bd3311c263c67","externalIds":{"ArXiv":"2107.01153","DBLP":"journals/pami/ZhouPCGW23","DOI":"10.1109/TPAMI.2022.3225573","CorpusId":235727754,"PubMed":"36449595"},"title":"A Survey on Deep Learning Technique for Video Segmentation"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","externalIds":{"ArXiv":"2102.05918","DBLP":"conf/icml/JiaYXCPPLSLD21","CorpusId":231879586},"title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"d29430adccb805ab57b349afa8553954347b3197","externalIds":{"ArXiv":"2012.15840","DBLP":"conf/cvpr/ZhengLZZLWFFXT021","DOI":"10.1109/CVPR46437.2021.00681","CorpusId":229924195},"title":"Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"},{"paperId":"47f7ec3d0a5e6e83b6768ece35206a94dc81919c","externalIds":{"ArXiv":"2012.09841","MAG":"3111551570","DBLP":"journals/corr/abs-2012-09841","DOI":"10.1109/CVPR46437.2021.01268","CorpusId":229297973},"title":"Taming Transformers for High-Resolution Image Synthesis"},{"paperId":"320f80bf1458e4312724d06557138e093ab4497c","externalIds":{"MAG":"3089250090","DBLP":"journals/iet-ipr/WangLCZMN22","ArXiv":"2009.13120","DOI":"10.1049/ipr2.12419","CorpusId":221970473},"title":"Medical Image Segmentation Using Deep Learning: A Survey"},{"paperId":"b8601c86905b0184b9387b042400609febb93d10","externalIds":{"ArXiv":"2006.14822","DBLP":"conf/cibcb/Jadon20","MAG":"3038091703","DOI":"10.1109/CIBCB48159.2020.9277638","CorpusId":220128180},"title":"A survey of loss functions for semantic segmentation"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"b149d6958689f16d01eb1ed016891621bae06614","externalIds":{"MAG":"3017097154","DBLP":"journals/air/WangWZ20","DOI":"10.1007/s10462-020-09830-9","CorpusId":215808943},"title":"Image segmentation evaluation: a survey of methods"},{"paperId":"5ce5c56a709b55ac7d21ea44aeec6d5f5358c39a","externalIds":{"DOI":"10.1145/3381897","CorpusId":211519382},"title":"Tuning"},{"paperId":"c7bbeaef75fa64c7e9cdf1b68bb487b9f8cd9a7d","externalIds":{"MAG":"2999607073","ArXiv":"2001.05566","DBLP":"journals/corr/abs-2001-05566","DOI":"10.1109/TPAMI.2021.3059968","CorpusId":210702798,"PubMed":"33596172"},"title":"Image Segmentation Using Deep Learning: A Survey"},{"paperId":"e065392ba6160ac27906e5a79398c9a71e4a4bb7","externalIds":{"MAG":"2991508719","DBLP":"conf/cvpr/ChengCZ0HAC20","ArXiv":"1911.10194","DOI":"10.1109/cvpr42600.2020.01249","CorpusId":208248153},"title":"Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation"},{"paperId":"441555b5cd09703e55c03e70bd2c9f82c0ffcf9b","externalIds":{"MAG":"3014641072","DBLP":"journals/pami/00010CJDZ0MTW0X21","ArXiv":"1908.07919","DOI":"10.1109/TPAMI.2020.2983686","CorpusId":201124533,"PubMed":"32248092"},"title":"Deep High-Resolution Representation Learning for Visual Recognition"},{"paperId":"147f1408e235bf5d1410474505ed773494e3ab01","externalIds":{"DBLP":"journals/corr/abs-1907-06119","MAG":"2960184797","ArXiv":"1907.06119","DOI":"10.1145/3329784","CorpusId":196621428},"title":"Understanding Deep Learning Techniques for Image Segmentation"},{"paperId":"03903af055d5345251efc844091a6d713adbd1a6","externalIds":{"DBLP":"journals/ijon/LateefR19","MAG":"2912327653","DOI":"10.1016/j.neucom.2019.02.003","CorpusId":106407125},"title":"Survey on semantic segmentation using deep learning techniques"},{"paperId":"a84906dbd4d6640f918d0b6ed2a7313dda0d55f1","externalIds":{"MAG":"2910628332","DBLP":"journals/corr/abs-1901-02446","ArXiv":"1901.02446","DOI":"10.1109/CVPR.2019.00656","CorpusId":57721164},"title":"Panoptic Feature Pyramid Networks"},{"paperId":"47d79963ac69111d8dc82a228d26e6a746a4d087","externalIds":{"DOI":"10.1201/9781351190435-21","CorpusId":240081945},"title":"Transformers"},{"paperId":"5e58f8b0c4b191f1b9fbb73450395c6b54b83580","externalIds":{"MAG":"3104468857","ArXiv":"1809.10198","DBLP":"journals/air/LiuDY19","DOI":"10.1007/s10462-018-9641-3","CorpusId":49471308},"title":"Recent progress in semantic image segmentation"},{"paperId":"9217e28b2273eb3b26e4e9b7b498b4661e6e09f5","externalIds":{"ArXiv":"1802.02611","MAG":"2787091153","DBLP":"journals/corr/abs-1802-02611","DOI":"10.1007/978-3-030-01234-2_49","CorpusId":3638670},"title":"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"},{"paperId":"2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d","externalIds":{"MAG":"2737258237","DBLP":"conf/cvpr/ZhouZPFB017","DOI":"10.1109/CVPR.2017.544","CorpusId":5636055},"title":"Scene Parsing through ADE20K Dataset"},{"paperId":"1a0912bb76777469295bb2c059faee907e7f3258","externalIds":{"ArXiv":"1703.06870","CorpusId":54465873},"title":"Mask R-CNN"},{"paperId":"c8c494ee5488fe20e0aa01bddf3fc4632086d654","externalIds":{"DBLP":"journals/corr/CordtsORREBFRS16","MAG":"2953139137","ArXiv":"1604.01685","DOI":"10.1109/CVPR.2016.350","CorpusId":502946},"title":"The Cityscapes Dataset for Semantic Urban Scene Understanding"},{"paperId":"96d288df7ca67dafe1642c427a4d9f4901267c8b","externalIds":{"MAG":"1909234690","DBLP":"conf/cvpr/ByeonBRL15","DOI":"10.1109/CVPR.2015.7298977","CorpusId":15134598},"title":"Scene labeling with LSTM recurrent neural networks"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","externalIds":{"MAG":"2953106684","ArXiv":"1506.01497","DBLP":"journals/pami/RenHG017","DOI":"10.1109/TPAMI.2016.2577031","CorpusId":10328909,"PubMed":"27295650"},"title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"7ffdbc358b63378f07311e883dddacc9faeeaf4b","externalIds":{"ArXiv":"1504.08083","CorpusId":206770307},"title":"Fast R-CNN"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"39ad6c911f3351a3b390130a6e4265355b4d593b","externalIds":{"DBLP":"journals/corr/ChenPKMY14","MAG":"1923697677","ArXiv":"1412.7062","CorpusId":1996665},"title":"Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs"},{"paperId":"6fc6803df5f9ae505cae5b2f178ade4062c768d0","externalIds":{"DBLP":"journals/corr/ShelhamerLD16","MAG":"2952632681","ArXiv":"1411.4038","DOI":"10.1109/CVPR.2015.7298965","CorpusId":1629541},"title":"Fully convolutional networks for semantic segmentation"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","externalIds":{"ACL":"D14-1086","DBLP":"conf/emnlp/KazemzadehOMB14","MAG":"2251512949","DOI":"10.3115/v1/D14-1086","CorpusId":6308361},"title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"616b246e332573af1f4859aa91440280774c183a","externalIds":{"DBLP":"journals/ijcv/EveringhamEGWWZ15","MAG":"2037227137","DOI":"10.1007/s11263-014-0733-5","CorpusId":207252270},"title":"The Pascal Visual Object Classes Challenge: A Retrospective"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"b3c785b99ec147049caa47f707f337b717705970","externalIds":{"MAG":"2118246710","DBLP":"journals/pami/AchantaSSLFS12","DOI":"10.1109/TPAMI.2012.120","CorpusId":1806278,"PubMed":"22641706"},"title":"SLIC Superpixels Compared to State-of-the-Art Superpixel Methods"},{"paperId":"bbc6228618ea4d1e0728433fbac8ae949790449a","externalIds":{"MAG":"2096669594","DOI":"10.1109/PACIIA.2009.5406610","CorpusId":17504378},"title":"A new method for image segmentation"},{"paperId":"af0eb96edf68ff8a1617170da61ddb435c60cdbd","externalIds":{"DBLP":"journals/tmi/NobleB06","MAG":"2119249988","DOI":"10.1109/TMI.2006.877092","CorpusId":14122909,"PubMed":"16894993"},"title":"Ultrasound image segmentation: a survey"},{"paperId":"2bdf9b643748b28b5d891f320908c817acce4e9d","externalIds":{"DBLP":"journals/pami/NockN04","MAG":"2152632881","DOI":"10.1109/TPAMI.2004.110","CorpusId":595377,"PubMed":"15521493"},"title":"Statistical region merging"},{"paperId":"363b56f85e12389017ba8894056a1b309e46a5f7","externalIds":{"DBLP":"conf/cvpr/HeZC04","MAG":"2095844239","DOI":"10.1109/CVPR.2004.173","CorpusId":11859305},"title":"Multiscale conditional random fields for image labeling"},{"paperId":"3b9eee8d496d1c08db7f4d55ca2fd8b447174483","externalIds":{"MAG":"2136704614","DBLP":"journals/pr/ChengJSW01","DOI":"10.1016/S0031-3203(00)00149-7","CorpusId":205904573},"title":"Color image segmentation: advances and prospects"},{"paperId":"862db6c4800e8e3e4c9ea5ba957c844e27f7b62b","externalIds":{"MAG":"2106582716","DBLP":"conf/cvpr/DengMS99","DOI":"10.1109/CVPR.1999.784719","CorpusId":5638609},"title":"Color image segmentation"},{"paperId":"97d9d482881a62253e575ac7ace540181ba55bbf","externalIds":{"DBLP":"conf/cvpr/MaM97","MAG":"2132126381","DOI":"10.1109/CVPR.1997.609409","CorpusId":2056985},"title":"Edge flow: A framework of boundary detection and image segmentation"},{"paperId":"b94c7ff9532ab26c3aedbee3988ec4c7a237c173","externalIds":{"MAG":"2121947440","DBLP":"journals/pami/ShiM00","DOI":"10.1109/CVPR.1997.609407","CorpusId":14848918},"title":"Normalized cuts and image segmentation"},{"paperId":"5410d3cbaebce000d47c248f24da4ba1d9d66db7","externalIds":{"MAG":"1979432452","DBLP":"journals/pr/Zhang96","DOI":"10.1016/0031-3203(95)00169-7","CorpusId":15744805},"title":"A survey on evaluation methods for image segmentation"},{"paperId":"78820f823d244ef710fc4c63c5524840c30aa7ea","externalIds":{"MAG":"2096579040","DBLP":"journals/pami/AdamsB94","DOI":"10.1109/34.295913","CorpusId":11754708},"title":"Seeded Region Growing"},{"paperId":"ab67b9d0da50e251a4f7e42370540547b891ceb1","externalIds":{"MAG":"2010560725","DOI":"10.1016/0146-664X(80)90047-7","CorpusId":121890149},"title":"Color information for region segmentation"},{"paperId":"e04469ecae602db2ef41db1aded9caedc15c8595","externalIds":{"DBLP":"conf/iclr/WangL0SLDX024","CorpusId":271532799},"title":"BarLeRIa: An Efficient Tuning Framework for Referring Image Segmentation"},{"paperId":"29c7f009df21d0112c48dec254ff80cc45fac3af","externalIds":{"DBLP":"conf/nips/SchaefferMK23","ArXiv":"2304.15004","DOI":"10.48550/arXiv.2304.15004","CorpusId":258418299},"title":"Are Emergent Abilities of Large Language Models a Mirage?"},{"paperId":"4dd869a2c17cacd03b3e8a9b7efcd48f40651925","externalIds":{"DBLP":"journals/corr/abs-2304-12306","DOI":"10.48550/arXiv.2304.12306","CorpusId":258298289},"title":"Segment Anything in Medical Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"1df6a36226586d5e0159e081039e7d3623602dbd","externalIds":{"CorpusId":36859424},"title":"A Survey on Image Segmentation Techniques"},{"paperId":"91343f9ce2e7d702c2ff837830bd35259ff8fd99","externalIds":{"DBLP":"journals/tsmc/Otsu79","MAG":"2133059825","DOI":"10.1109/TSMC.1979.4310076","CorpusId":15326934},"title":"A Threshold Selection Method from Gray-Level Histograms"}]}