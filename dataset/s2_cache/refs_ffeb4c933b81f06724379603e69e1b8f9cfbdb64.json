{"references":[{"paperId":"00d4fea24baae6ac9a77ca2b0744f466b268e780","externalIds":{"DBLP":"conf/acl/GandhiG0WN24","ArXiv":"2404.14361","DOI":"10.48550/arXiv.2404.14361","CorpusId":269292964},"title":"Better Synthetic Data by Retrieving and Transforming Existing Datasets"},{"paperId":"f9fc7b5ebd5fe30257abb03ad87d8138eeeb28d9","externalIds":{"ArXiv":"2404.07503","DBLP":"journals/corr/abs-2404-07503","DOI":"10.48550/arXiv.2404.07503","CorpusId":269042851},"title":"Best Practices and Lessons Learned on Synthetic Data for Language Models"},{"paperId":"58e194695187fe4daeb04ea694e0f59af2441177","externalIds":{"DBLP":"journals/corr/abs-2403-18802","ArXiv":"2403.18802","DOI":"10.48550/arXiv.2403.18802","CorpusId":268724304},"title":"Long-form factuality in large language models"},{"paperId":"ca757b42595d40dd0683d508e2485e030bc4eb54","externalIds":{"ArXiv":"2403.01081","DBLP":"journals/corr/abs-2403-01081","DOI":"10.48550/arXiv.2403.01081","CorpusId":268230871},"title":"LAB: Large-Scale Alignment for ChatBots"},{"paperId":"0a0c3d8650d2c0ffeecd491356393b93bba63fed","externalIds":{"DBLP":"conf/icml/SeedatHBS24","ArXiv":"2312.12112","DOI":"10.48550/arXiv.2312.12112","CorpusId":266362265},"title":"Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes"},{"paperId":"46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5","externalIds":{"ArXiv":"2312.10997","DBLP":"journals/corr/abs-2312-10997","CorpusId":266359151},"title":"Retrieval-Augmented Generation for Large Language Models: A Survey"},{"paperId":"2aa86b563e760b89e01f652224f24018eb9843e2","externalIds":{"DBLP":"conf/bigdataconf/WeiKHZDYMQ23","DOI":"10.1109/BigData59044.2023.10386911","CorpusId":267144695},"title":"Empirical Study of LLM Fine-Tuning for Text Classification in Legal Document Review"},{"paperId":"40cd34f260d5596263654caf9d911d4355bf4f4e","externalIds":{"DBLP":"journals/corr/abs-2311-16483","ArXiv":"2311.16483","DOI":"10.48550/arXiv.2311.16483","CorpusId":265466206},"title":"ChartLlama: A Multimodal LLM for Chart Understanding and Generation"},{"paperId":"d2ea161e6b2114529d875d16cfaad4c824e17a8c","externalIds":{"DBLP":"journals/corr/abs-2311-11045","ArXiv":"2311.11045","DOI":"10.48550/arXiv.2311.11045","CorpusId":265295592},"title":"Orca 2: Teaching Small Language Models How to Reason"},{"paperId":"3c0c14882318fd7ad3fd51cdc5fc515a7f78effd","externalIds":{"DBLP":"conf/emnlp/ZhangLMZ023","ArXiv":"2310.19596","DOI":"10.48550/arXiv.2310.19596","CorpusId":264814421},"title":"LLMaAA: Making Large Language Models as Active Annotators"},{"paperId":"2a74fc66beea8bce542581560ca6ec5a0e1bb024","externalIds":{"DBLP":"journals/corr/abs-2310-15638","ArXiv":"2310.15638","DOI":"10.18653/v1/2023.emnlp-main.92","CorpusId":264439555},"title":"CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation"},{"paperId":"2166f1dc62759e9ed20955393b9c41683968d44b","externalIds":{"ArXiv":"2310.13671","DBLP":"journals/corr/abs-2310-13671","DOI":"10.48550/arXiv.2310.13671","CorpusId":264405716},"title":"Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models"},{"paperId":"e973018e5de2cf3d8303492edc838505a3010820","externalIds":{"DBLP":"conf/pakdd/ZhaoBC25","ArXiv":"2310.12746","DOI":"10.48550/arXiv.2310.12746","CorpusId":264305952},"title":"TabuLa: Harnessing Language Models for Tabular Data Synthesis"},{"paperId":"3b918b15178bcc84fd22af5094fe1efbcd388e72","externalIds":{"DBLP":"journals/corr/abs-2310-09168","ArXiv":"2310.09168","DOI":"10.48550/arXiv.2310.09168","CorpusId":264128344},"title":"Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration"},{"paperId":"9ebf47129c15f61f4b77bbfe305c522480c20347","externalIds":{"DBLP":"conf/iclr/KimS0JLLYSKTS24","ArXiv":"2310.08491","DOI":"10.48550/arXiv.2310.08491","CorpusId":265675839},"title":"Prometheus: Inducing Fine-grained Evaluation Capability in Language Models"},{"paperId":"6f39442852656f8a9decc95854a2ed461b3a83ab","externalIds":{"DBLP":"conf/emnlp/CuiW24","ArXiv":"2310.04484","DOI":"10.48550/arXiv.2310.04484","CorpusId":263829618},"title":"Ada-Instruct: Adapting Instruction Generators for Complex Reasoning"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","externalIds":{"ArXiv":"2310.03744","DBLP":"conf/cvpr/LiuLLL24","DOI":"10.1109/CVPR52733.2024.02484","CorpusId":263672058},"title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"8a9f75692f946250c70c50b14b71875bf06b2270","externalIds":{"DBLP":"conf/iclr/HoskingBB24","ArXiv":"2309.16349","DOI":"10.48550/arXiv.2309.16349","CorpusId":263134280},"title":"Human Feedback is not Gold Standard"},{"paperId":"29f032fc875576b5c3c6b1c2d76af8639bacfb88","externalIds":{"ArXiv":"2309.11235","DBLP":"journals/corr/abs-2309-11235","DOI":"10.48550/arXiv.2309.11235","CorpusId":262064307},"title":"OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"},{"paperId":"e26888285436bc7998e5c95102a9beb60144be5e","externalIds":{"DBLP":"journals/corr/abs-2309-05463","ArXiv":"2309.05463","DOI":"10.48550/arXiv.2309.05463","CorpusId":261696657},"title":"Textbooks Are All You Need II: phi-1.5 technical report"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","externalIds":{"DBLP":"journals/corr/abs-2308-09583","ArXiv":"2308.09583","DOI":"10.48550/arXiv.2308.09583","CorpusId":261030818},"title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"703035b483c181953de1b55b5fd59cd4cd4cf211","externalIds":{"DBLP":"journals/corr/abs-2308-00352","ArXiv":"2308.00352","DOI":"10.48550/arXiv.2308.00352","CorpusId":260351380},"title":"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","externalIds":{"DBLP":"journals/corr/abs-2307-16789","ArXiv":"2307.16789","DOI":"10.48550/arXiv.2307.16789","CorpusId":260334759},"title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"3004a6298dd5216f163b43074a90db7cdad52df9","externalIds":{"ArXiv":"2307.16833","DBLP":"journals/corr/abs-2307-16833","DOI":"10.48550/arXiv.2307.16833","CorpusId":260333922},"title":"Data Augmentation for Neural Machine Translation using Generative Language Model"},{"paperId":"c494b5d73fa8cf79f352c5195a179925e9159f75","externalIds":{"DBLP":"journals/corr/abs-2307-02839","ArXiv":"2307.02839","DOI":"10.48550/arXiv.2307.02839","CorpusId":259360634},"title":"Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation"},{"paperId":"78c488e2d84bd193a40006b1fceb03e3845b81d4","externalIds":{"ArXiv":"2306.15895","DBLP":"journals/corr/abs-2306-15895","DOI":"10.48550/arXiv.2306.15895","CorpusId":259275123},"title":"Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias"},{"paperId":"2af358b4771d7bffe491077466fc4d225a16a74b","externalIds":{"ArXiv":"2306.15766","DBLP":"journals/corr/abs-2306-15766","DOI":"10.48550/arXiv.2306.15766","CorpusId":259274939},"title":"Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost"},{"paperId":"454c8fef2957aa2fb13eb2c7a454393a2ee83805","externalIds":{"DBLP":"journals/corr/abs-2306-08568","ArXiv":"2306.08568","CorpusId":259164815},"title":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct"},{"paperId":"a0a79dad89857a96f8f71b14238e5237cbfc4787","externalIds":{"ArXiv":"2306.05685","DBLP":"journals/corr/abs-2306-05685","CorpusId":259129398},"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"},{"paperId":"ccd94602e3acecf999d0c9ba62b1a8bc02e9f696","externalIds":{"ArXiv":"2306.05087","DBLP":"journals/corr/abs-2306-05087","DOI":"10.48550/arXiv.2306.05087","CorpusId":259108266},"title":"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization"},{"paperId":"5aa26e0b2bb27162a4de07bf8c3d5e0e9d3b0853","externalIds":{"ArXiv":"2306.04140","DBLP":"journals/corr/abs-2306-04140","ACL":"2023.acl-long.34","DOI":"10.18653/v1/2023.acl-long.34","CorpusId":259096160},"title":"Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions"},{"paperId":"cd0db1266961a9cd3e61aeab6456d35c32d77a39","externalIds":{"DBLP":"journals/corr/abs-2306-01684","ArXiv":"2306.01684","DOI":"10.48550/arXiv.2306.01684","CorpusId":259063934},"title":"Harnessing large-language models to generate private synthetic text"},{"paperId":"e58231569f45704a2460bb6de3ec6f52efc5cf95","externalIds":{"ArXiv":"2306.00176","DBLP":"journals/corr/abs-2306-00176","DOI":"10.48550/arXiv.2306.00176","CorpusId":259000016},"title":"Automated Annotation with Generative AI Requires Validation"},{"paperId":"28a7ced384549eaae74ea9ad3ee21189a0625afe","externalIds":{"ArXiv":"2305.19148","DBLP":"journals/corr/abs-2305-19148","ACL":"2023.acl-long.783","DOI":"10.48550/arXiv.2305.19148","CorpusId":258967265},"title":"Mitigating Label Biases for In-context Learning"},{"paperId":"a122863d239643453195424c04067e89406246e1","externalIds":{"DBLP":"conf/emnlp/DingCXQHL0Z23","ArXiv":"2305.14233","DOI":"10.48550/arXiv.2305.14233","CorpusId":258840897},"title":"Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"},{"paperId":"6bf981314d81ca838d2cc55fc6f6265717792b67","externalIds":{"ArXiv":"2305.13917","DBLP":"journals/corr/abs-2305-13917","DOI":"10.48550/arXiv.2305.13917","CorpusId":258841402},"title":"Generating Data for Symbolic Language with Large Language Models"},{"paperId":"96df2b9d5c13d39ade0862f86c93ecabc5858f51","externalIds":{"ArXiv":"2305.10703","DBLP":"conf/acl/YuZZ0SZ23","DOI":"10.48550/arXiv.2305.10703","CorpusId":258762711},"title":"ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval"},{"paperId":"6d0656d9bb60a2bea50c4b894fbcc5d1e32134e7","externalIds":{"DBLP":"journals/jdiq/NavigliCR23","DOI":"10.1145/3597307","CorpusId":258688053},"title":"Biases in Large Language Models: Origins, Inventory, and Discussion"},{"paperId":"663d743272e9ab04f54d9105a3c3a3f6e22dd1dd","externalIds":{"ArXiv":"2305.08281","DBLP":"conf/emnlp/FengBBT23","DOI":"10.48550/arXiv.2305.08281","CorpusId":258685429},"title":"FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge"},{"paperId":"28085f480ce456a376ebace9b899e3bc93dbc048","externalIds":{"DBLP":"journals/corr/abs-2305-07759","ArXiv":"2305.07759","CorpusId":258686446},"title":"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"},{"paperId":"e01515c6138bc525f7aec30fc85f2adf028d4156","externalIds":{"DBLP":"journals/corr/abs-2305-03047","ArXiv":"2305.03047","DOI":"10.48550/arXiv.2305.03047","CorpusId":258479665},"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"},{"paperId":"08a80cb34d785258c770acecd302ab41ead46eed","externalIds":{"DBLP":"conf/iclr/XuSZG0FTLJ24","ArXiv":"2304.12244","CorpusId":258298159},"title":"WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"},{"paperId":"b4cfe9b3671d194e9b22f97456e19cfcd07981ee","externalIds":{"ArXiv":"2304.12206","DBLP":"journals/corr/abs-2304-12206","DOI":"10.48550/arXiv.2304.12206","CorpusId":258298856},"title":"PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale"},{"paperId":"70da4fb798a86cbe8cad96c27ced0415885bbd9d","externalIds":{"DBLP":"conf/naacl/HeLGJZLJYDC24","ArXiv":"2303.16854","DOI":"10.48550/arXiv.2303.16854","CorpusId":257805087},"title":"AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators"},{"paperId":"a9e155fda1d97baa2b8712f580cc61887cc64e9b","externalIds":{"PubMedCentral":"10372638","DBLP":"journals/corr/abs-2303-15056","ArXiv":"2303.15056","DOI":"10.1073/pnas.2305016120","CorpusId":257766307,"PubMed":"37463210"},"title":"ChatGPT outperforms crowd workers for text-annotation tasks"},{"paperId":"bdf7bf9e81a6c12e22323d0402885b2ba62f623e","externalIds":{"DBLP":"journals/corr/abs-2303-04360","ArXiv":"2303.04360","DOI":"10.48550/arXiv.2303.04360","CorpusId":257405132},"title":"Does Synthetic Data Generation of LLMs Help Clinical Text Mining?"},{"paperId":"f64e49d76048c902cc02e8ae27dcd4ac0dbcb97f","externalIds":{"ArXiv":"2303.04132","DBLP":"conf/emnlp/JosifoskiSP023","DOI":"10.48550/arXiv.2303.04132","CorpusId":257378179},"title":"Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction"},{"paperId":"357613ea0e90bd41fb942fd65f39498e71e2dbc3","externalIds":{"ArXiv":"2303.01580","DBLP":"conf/emnlp/ChenLLRY23","DOI":"10.48550/arXiv.2303.01580","CorpusId":257353840},"title":"Mixture of Soft Prompts for Controllable Data Generation"},{"paperId":"44b0d2e884efa5344e50424dbe2edf616981f201","externalIds":{"ArXiv":"2303.00807","DBLP":"journals/corr/abs-2303-00807","DOI":"10.48550/arXiv.2303.00807","CorpusId":257279774},"title":"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers"},{"paperId":"5848737f78397f72ceae2ba6f3419a6a8502b8ba","externalIds":{"DBLP":"journals/inffus/KoconCKKSBBGJKKKMMOPRWWK23","ArXiv":"2302.10724","DOI":"10.1016/j.inffus.2023.101861","CorpusId":257050407},"title":"ChatGPT: Jack of all trades, master of none"},{"paperId":"cb29cf52f0f7d2e4324c68690a55b22890f2212d","externalIds":{"ArXiv":"2301.07597","DBLP":"journals/corr/abs-2301-07597","DOI":"10.48550/arXiv.2301.07597","CorpusId":255998637},"title":"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"70b98d90767345b15e0569082c0e4ac661279b5d","externalIds":{"ArXiv":"2212.10450","DBLP":"conf/acl/DingQLCLJB23","ACL":"2023.acl-long.626","DOI":"10.48550/arXiv.2212.10450","CorpusId":254877171},"title":"Is GPT-3 a Good Data Annotator?"},{"paperId":"6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","externalIds":{"ACL":"2023.acl-long.806","ArXiv":"2212.09689","DBLP":"conf/acl/HonovichSLS23","DOI":"10.48550/arXiv.2212.09689","CorpusId":254853659},"title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"},{"paperId":"3936fd3c6187f606c6e4e2e20b196dbc41cc4654","externalIds":{"DBLP":"journals/corr/abs-2212-08073","ArXiv":"2212.08073","DOI":"10.48550/arXiv.2212.08073","CorpusId":254823489},"title":"Constitutional AI: Harmlessness from AI Feedback"},{"paperId":"95d54e3ce577f7d91ab4b2c52c73b501245e484d","externalIds":{"DBLP":"journals/corr/abs-2210-12329","ArXiv":"2210.12329","DOI":"10.48550/arXiv.2210.12329","CorpusId":253098311},"title":"ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback"},{"paperId":"3fa70115248377c3d1517c9f978791a296fbc1dd","externalIds":{"DBLP":"conf/emnlp/0001GHW00023","ArXiv":"2210.11610","DOI":"10.48550/arXiv.2210.11610","CorpusId":253080328},"title":"Large Language Models Can Self-Improve"},{"paperId":"86d0d3855f94105e25d81cab9f3d269c6062a9c4","externalIds":{"DBLP":"conf/iclr/SuKWSWX0OZS023","ArXiv":"2209.01975","DOI":"10.48550/arXiv.2209.01975","CorpusId":252089424},"title":"Selective Annotation Makes Language Models Better Few-Shot Learners"},{"paperId":"62595a575c2c4a01b868de226aa68cfe17ea1fa3","externalIds":{"DBLP":"journals/corr/abs-2207-02516","ArXiv":"2207.02516","DOI":"10.48550/arXiv.2207.02516","CorpusId":250311310},"title":"Ask Me What You Need: Product Retrieval using Knowledge from GPT-3"},{"paperId":"f12a6168ed8de1aee69fee51b469b1aecd5f903e","externalIds":{"ArXiv":"2206.04624","DBLP":"conf/nips/LeePXPFSC22","DOI":"10.48550/arXiv.2206.04624","CorpusId":249538460},"title":"Factuality Enhanced Language Models for Open-Ended Text Generation"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","externalIds":{"ArXiv":"2206.04615","DBLP":"journals/corr/abs-2206-04615","CorpusId":263625818},"title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"08c43a7b09cead543bca41d2f4f64260f3b9d574","externalIds":{"DBLP":"journals/corr/abs-2205-02318","ArXiv":"2205.02318","DOI":"10.48550/arXiv.2205.02318","CorpusId":248524894},"title":"Language Models in the Loop: Incorporating Prompting into Weak Supervision"},{"paperId":"06d7cb8c8816360feb33c3367073e0ef66d7d0b0","externalIds":{"ACL":"2022.emnlp-main.340","ArXiv":"2204.07705","DBLP":"conf/emnlp/WangMAKMNADASPK22","DOI":"10.18653/v1/2022.emnlp-main.340","CorpusId":253098274},"title":"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"},{"paperId":"382ba0c4452aab6ecdaf8a62d567bb3c4684e4f0","externalIds":{"DBLP":"journals/corr/abs-2203-09509","ArXiv":"2203.09509","ACL":"2022.acl-long.234","DOI":"10.48550/arXiv.2203.09509","CorpusId":247519233},"title":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection"},{"paperId":"2145fcceeb69385e108bf1796d52f974854d4c0b","externalIds":{"ArXiv":"2202.07922","ACL":"2022.emnlp-main.801","DBLP":"journals/corr/abs-2202-07922","DOI":"10.18653/v1/2022.emnlp-main.801","CorpusId":246867045},"title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation"},{"paperId":"23c265ba884b92ecbd9d18641078d964697e4590","externalIds":{"ArXiv":"2202.04538","DBLP":"journals/corr/abs-2202-04538","CorpusId":246680398},"title":"Generating Training Data with Language Models: Towards Zero-Shot Language Understanding"},{"paperId":"fd8e176087335355ff5e81821a616d15ec8d3346","externalIds":{"ArXiv":"2110.03484","DBLP":"journals/corr/abs-2110-03484","CorpusId":238419211},"title":"Creating Training Sets via Weak Indirect Supervision"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","externalIds":{"DBLP":"journals/corr/abs-2109-07958","ACL":"2022.acl-long.229","ArXiv":"2109.07958","DOI":"10.18653/v1/2022.acl-long.229","CorpusId":237532606},"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"4e263b4cd6998bff2501dd143e685f413179b12d","externalIds":{"DBLP":"conf/emnlp/WangLXZZ21","ArXiv":"2108.13487","DOI":"10.18653/v1/2021.findings-emnlp.354","CorpusId":237363383},"title":"Want To Reduce Labeling Cost? GPT-3 Can Help"},{"paperId":"bbfdcbfee1762d48cae9db8637f21ea3c234ba30","externalIds":{"DBLP":"conf/emnlp/YooPKLP21","ArXiv":"2104.08826","DOI":"10.18653/v1/2021.findings-emnlp.192","CorpusId":233296100},"title":"GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","externalIds":{"DBLP":"conf/nips/HendrycksBKABTS21","ArXiv":"2103.03874","CorpusId":232134851},"title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","externalIds":{"DBLP":"conf/acl-deelio/LiuSZDCC22","ArXiv":"2101.06804","ACL":"2022.deelio-1.10","DOI":"10.18653/v1/2022.deelio-1.10","CorpusId":231632658},"title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"4450493ecb806cb889b341ae8e430886f2549a61","externalIds":{"ACL":"2020.acl-main.372","DBLP":"journals/corr/abs-2005-00547","ArXiv":"2005.00547","MAG":"3022113899","DOI":"10.18653/v1/2020.acl-main.372","CorpusId":218486942},"title":"GoEmotions: A Dataset of Fine-Grained Emotions"},{"paperId":"5a96a270cbc73e3b448c2404ff31670c21da7b49","externalIds":{"ACL":"2020.nlp4convai-1.5","MAG":"3045492832","ArXiv":"2003.04807","DBLP":"journals/corr/abs-2003-04807","DOI":"10.18653/v1/2020.nlp4convai-1.5","CorpusId":212645349},"title":"Efficient Intent Detection with Dual Sentence Encoders"},{"paperId":"845b4941d8c016aa5f8967da2f86d38ef6c18fa3","externalIds":{"DBLP":"journals/corr/abs-2002-00388","MAG":"3003265726","ArXiv":"2002.00388","DOI":"10.1109/TNNLS.2021.3070843","CorpusId":211010433,"PubMed":"33900922"},"title":"A Survey on Knowledge Graphs: Representation, Acquisition, and Applications"},{"paperId":"693cce5d9764f9e9e0c9c583bf840ac019e2179f","externalIds":{"MAG":"3105662186","DBLP":"journals/tacl/BartoloRWRS20","ArXiv":"2002.00293","DOI":"10.1162/tacl_a_00338","CorpusId":211010520},"title":"Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension"},{"paperId":"2873f78efd7adcb118a70f8ea3ca7fa1501e320a","externalIds":{"DBLP":"journals/corr/abs-1910-07124","ACL":"D19-1649","MAG":"2971136144","ArXiv":"1910.07124","DOI":"10.18653/v1/D19-1649","CorpusId":202789603},"title":"FewRel 2.0: Towards More Challenging Few-Shot Relation Classification"},{"paperId":"499556c6ed1daec62e5a57456213cf4f921460f1","externalIds":{"DBLP":"journals/corr/abs-1909-02027","ArXiv":"1909.02027","ACL":"D19-1131","MAG":"2971418622","DOI":"10.18653/v1/D19-1131","CorpusId":202540203},"title":"An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","externalIds":{"MAG":"2963310665","DBLP":"conf/emnlp/WangSMHLB18","ACL":"W18-5446","ArXiv":"1804.07461","DOI":"10.18653/v1/W18-5446","CorpusId":5034059},"title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","externalIds":{"DBLP":"journals/corr/ZhangZL15","MAG":"2963012544","ArXiv":"1509.01626","CorpusId":368182},"title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"db6b56557d4b8c722e1fc504af4a361cfe1e22d8","externalIds":{"DBLP":"conf/doceng/AlmeidaHY11","MAG":"2036166268","DOI":"10.1145/2034691.2034742","CorpusId":13871930},"title":"Contributions to the study of SMS spam filtering: new collection and results"},{"paperId":"1c61f9ef06fe74505775a833ff849185757199e7","externalIds":{"MAG":"2113459411","ACL":"P11-1015","DBLP":"conf/acl/MaasDPHNP11","CorpusId":1428702},"title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"f37ae3723a4452ad9e0ceebcf07f84c3733cb56d","externalIds":{"DBLP":"conf/acl/2023-1","CorpusId":263487747},"title":"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023"},{"paperId":"d4cb9248f12d43dbc3e4c7878209215368fd78e5","externalIds":{"DBLP":"conf/emnlp/HanG23","DOI":"10.18653/v1/2023.findings-emnlp.918","CorpusId":266166029},"title":"Multilingual Generation and Answering of Questions from Texts and Knowledge Graphs"},{"paperId":"edc11420b3f2aa6638d78cceb3b12778fe07bb85","externalIds":{"CorpusId":271293795},"title":"ChemCrow: Augmenting large-language models with chemistry tools"},{"paperId":"e730164e17975547564a1eaa70cea5884b16c89d","externalIds":{"CorpusId":259937133},"title":"Instruction : Translate the phrase ” Bonne chance ” into English Response : Good Luck"},{"paperId":"1e122149779c644855d1cccca5d96135db0482cb","externalIds":{"DBLP":"journals/corr/abs-2212-08635","DOI":"10.48550/arXiv.2212.08635","CorpusId":254823646},"title":"Self-Prompting Large Language Models for Open-Domain QA"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"}]}