{"references":[{"paperId":"f44dba48520c7af65e5e349c40bc8041c235da6d","externalIds":{"DBLP":"journals/corr/abs-2307-10008","ArXiv":"2307.10008","DOI":"10.1109/ICCV51070.2023.02104","CorpusId":259982892},"title":"MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions"},{"paperId":"a474501612900c3b8540841cb2a76fa30f703089","externalIds":{"DBLP":"conf/interspeech/AnwarSGH0W23","ArXiv":"2303.00628","DOI":"10.48550/arXiv.2303.00628","CorpusId":257255284},"title":"MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation"},{"paperId":"f7865a3f3c5795829e49d7285e1696a18d5f08c8","externalIds":{"DBLP":"journals/corr/abs-2301-13430","ArXiv":"2301.13430","DOI":"10.48550/arXiv.2301.13430","CorpusId":256416230},"title":"GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis"},{"paperId":"219125dcad6ffe7fa8d1b1bbd1900e8a42782b0c","externalIds":{"DBLP":"conf/cvpr/XingXZC0W23","ArXiv":"2301.02379","DOI":"10.1109/CVPR52729.2023.01229","CorpusId":255522829},"title":"CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior"},{"paperId":"1869af93364552a9e5b5e9dc55c6e305d3fbb6c7","externalIds":{"DBLP":"journals/corr/abs-2211-12368","ArXiv":"2211.12368","DOI":"10.1007/s11263-025-02481-9","CorpusId":253761098},"title":"Real-Time Neural Radiance Talking Portrait Synthesis via Audio-Spatial Decomposition"},{"paperId":"a7019f9a2ecb9983b66456ae32978e2574625980","externalIds":{"DBLP":"journals/corr/abs-2211-12194","ArXiv":"2211.12194","DOI":"10.1109/CVPR52729.2023.00836","CorpusId":253761522},"title":"SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation"},{"paperId":"ee6ca9ee3f097e2fe9b065a13599921ebc5e0148","externalIds":{"DBLP":"conf/eccv/ShenLZDZL22","ArXiv":"2207.11770","DOI":"10.48550/arXiv.2207.11770","CorpusId":251040886},"title":"Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis"},{"paperId":"261e9f21292a44dbf065385aa06996e9811d0ba2","externalIds":{"DBLP":"conf/aaai/0004FYW22","DOI":"10.1609/aaai.v36i3.20210","CorpusId":250303474},"title":"Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation"},{"paperId":"c013a7eecbba0dd3f0ca7cbdf9e242eb7860590f","externalIds":{"DBLP":"conf/cvpr/LiangPGZHHHLD022","DOI":"10.1109/CVPR52688.2022.00338","CorpusId":250520154},"title":"Expressive Talking Head Generation with Granular Audio-Visual Control"},{"paperId":"4315c902ba17148c06c00d98ec15b9ba947d9d8e","externalIds":{"DBLP":"conf/icassp/KoumparoulisP22","DOI":"10.1109/ICASSP43922.2022.9747729","CorpusId":249436732},"title":"Accurate and Resource-Efficient Lipreading with Efficientnetv2 and Transformers"},{"paperId":"3af6f6ccda3f711de2f8e2ad77dc5462048bca57","externalIds":{"ArXiv":"2204.02090","DBLP":"journals/corr/abs-2204-02090","DOI":"10.48550/arXiv.2204.02090","CorpusId":247957949},"title":"VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices"},{"paperId":"74a992e242130d6d59302c2450a48069faad4f9e","externalIds":{"ArXiv":"2201.07786","DBLP":"conf/eccv/LiuXWZWZ22","DOI":"10.1007/978-3-031-19836-6_7","CorpusId":246035721},"title":"Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation"},{"paperId":"ed91d63243ea48657c9b92095313f042aefd89fe","externalIds":{"DBLP":"journals/corr/abs-2201-05986","ArXiv":"2201.05986","DOI":"10.1109/TMM.2022.3142387","CorpusId":245949719},"title":"Audio-Driven Talking Face Video Generation With Dynamic Convolution Kernels"},{"paperId":"dd23991294bf53c6301ab79fa49752e6458d2eab","externalIds":{"DBLP":"conf/cvpr/FanLSWK22","ArXiv":"2112.05329","DOI":"10.1109/CVPR52688.2022.01821","CorpusId":245117892},"title":"FaceFormer: Speech-Driven 3D Facial Animation with Transformers"},{"paperId":"a5ec595ac6a70a50d9cb29702728ed14454d0515","externalIds":{"ArXiv":"2112.02749","DBLP":"journals/corr/abs-2112-02749","DOI":"10.1609/aaai.v36i3.20154","CorpusId":244909125},"title":"One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning"},{"paperId":"43f1e9001dbadc8cde28a5a135feb7fe142e95c9","externalIds":{"ArXiv":"2110.09951","DBLP":"journals/corr/abs-2110-09951","DOI":"10.5244/c.35.78","CorpusId":239024632},"title":"Talking Head Generation with Audio and Speech Related Facial Action Units"},{"paperId":"df17d3d7558f071e65785300883550ec1d9e0ec9","externalIds":{"DBLP":"conf/mm/ShengPTL21","DOI":"10.1145/3474085.3475415","CorpusId":239011921},"title":"Cross-modal Self-Supervised Learning for Lip Reading: When Contrastive Learning meets Adversarial Training"},{"paperId":"22dd7421ba6bafcf01e02cc5b2e26b0b987ffac1","externalIds":{"ArXiv":"2111.00203","DBLP":"journals/corr/abs-2111-00203","DOI":"10.1145/3474085.3475280","CorpusId":239011811},"title":"Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis"},{"paperId":"e70461ef2955c5562941d517b8f4d8e1f6ec4927","externalIds":{"ArXiv":"2110.07603","DBLP":"journals/corr/abs-2110-07603","DOI":"10.1109/CVPR52688.2022.00510","CorpusId":238857103},"title":"Sub-word Level Lip Reading With Visual Attention"},{"paperId":"7d6fedd4df465f822925f0d6f20c6221bbb10810","externalIds":{"DBLP":"conf/iccv/ZhangZHZNBG21","ArXiv":"2108.07938","DOI":"10.1109/ICCV48922.2021.00384","CorpusId":237194731},"title":"FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning"},{"paperId":"94fb01deaf197cfd73e8ffc740416389a28440b5","externalIds":{"DBLP":"conf/ijcai/SunZ0K21","DOI":"10.24963/ijcai.2021/141","CorpusId":237100638},"title":"Speech2Talking-Face: Inferring and Driving a Face with Synchronized Audio-Visual Representation"},{"paperId":"9318d7a44a4098222ccd573ad14e968ba09fbdd4","externalIds":{"DBLP":"conf/ijcai/WangLDFY21","ArXiv":"2107.09293","DOI":"10.24963/ijcai.2021/152","CorpusId":236134151},"title":"Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion"},{"paperId":"73ede03fe57e40380a0b0814adb135fd235622b3","externalIds":{"ArXiv":"2107.04806","DBLP":"journals/corr/abs-2107-04806","DOI":"10.21437/interspeech.2021-1996","CorpusId":235794851},"title":"Speech2Video: Cross-Modal Distillation for Speech to Video Generation"},{"paperId":"dcdea4685097d288de424c33065fabf1f67eacde","externalIds":{"DBLP":"conf/interspeech/0001MPSP21","ArXiv":"2106.09171","DOI":"10.21437/interspeech.2021-1360","CorpusId":235458259},"title":"LiRA: Learning Visual Speech Representations from Audio through Self-supervision"},{"paperId":"c7e644cc3a7d3c364b658a4126d3bcf3b2e0bf36","externalIds":{"DBLP":"conf/cvpr/RenDLHH21","DOI":"10.1109/CVPR46437.2021.01312","CorpusId":235691551},"title":"Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading"},{"paperId":"c9e8cb4bb2c8e5d6dd4e631fc347f7f1415f2150","externalIds":{"DBLP":"conf/cvpr/ZhangL0F21","DOI":"10.1109/CVPR46437.2021.00366","CorpusId":235657610},"title":"Flow-guided One-shot Talking Face Generation with a High-resolution Audio-visual Dataset"},{"paperId":"57f2468c6cdc52a0645f6e4c9ada726602cfc814","externalIds":{"ArXiv":"2106.04185","DBLP":"journals/corr/abs-2106-04185","DOI":"10.1109/CVPR46437.2021.00278","CorpusId":235368245},"title":"LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization"},{"paperId":"122d8f3209c8c0e2516181efda7fa6c911ebd70b","externalIds":{"DBLP":"journals/corr/abs-2104-11116","ArXiv":"2104.11116","DOI":"10.1109/CVPR46437.2021.00416","CorpusId":233346930},"title":"Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation"},{"paperId":"5746ead65eb1bf1a671500a1482fc14b0eb1f45e","externalIds":{"ArXiv":"2104.08223","DBLP":"journals/corr/abs-2104-08223","DOI":"10.1109/ICCV48922.2021.00121","CorpusId":233289953},"title":"MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement"},{"paperId":"ad5c0cbc88e2ab0599f3f5a47eee27d8163de7d1","externalIds":{"DBLP":"conf/aaai/LiWZDZYF21","ArXiv":"2104.07995","DOI":"10.1609/aaai.v35i3.16286","CorpusId":233289555},"title":"Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation"},{"paperId":"3fd34d7476789087f7db6a86ec1910e0044e57c5","externalIds":{"ArXiv":"2104.07452","DBLP":"conf/cvpr/JiZWWLCX21","DOI":"10.1109/CVPR46437.2021.01386","CorpusId":233240801},"title":"Audio-Driven Emotional Video Portraits"},{"paperId":"a81ef3138365f762334736769174738413cccdd9","externalIds":{"DBLP":"conf/iccv/GuoCLLBZ21","ArXiv":"2103.11078","DOI":"10.1109/ICCV48922.2021.00573","CorpusId":232307613},"title":"AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis"},{"paperId":"fed1159caec29e0d1996b51984cff81d98185d2f","externalIds":{"DBLP":"conf/icassp/0001PP21a","ArXiv":"2102.06657","DOI":"10.1109/ICASSP39728.2021.9414567","CorpusId":231918851},"title":"End-To-End Audio-Visual Speech Recognition with Conformers"},{"paperId":"d40c77c010c8dbef6142903a02f2a73a85012d5d","externalIds":{"ArXiv":"2012.12556","DBLP":"journals/corr/abs-2012-12556","DOI":"10.1109/TPAMI.2022.3152247","CorpusId":236986986,"PubMed":"35180075"},"title":"A Survey on Vision Transformer"},{"paperId":"cefef7a421d46a177a054698cc2d0bb4a788d00a","externalIds":{"MAG":"3110828520","DBLP":"conf/cvpr/HaliassosVPP21","ArXiv":"2012.07657","DOI":"10.1109/CVPR46437.2021.00500","CorpusId":229156239},"title":"Lips Don't Lie: A Generalisable and Robust Approach to Face Forgery Detection"},{"paperId":"f7984d3ee29eeb4ef9b0dc5fa6ad780bdca14e4b","externalIds":{"ArXiv":"2011.10688","DBLP":"journals/corr/abs-2011-10688","MAG":"3107570227","DOI":"10.1145/3449063","CorpusId":227126804},"title":"Iterative Text-Based Editing of Talking-Heads Using Neural Retargeting"},{"paperId":"9bd00f2545b89e538a524bf2330ba32d39d9bfcd","externalIds":{"ArXiv":"2011.07557","DBLP":"journals/corr/abs-2011-07557","MAG":"3103322708","CorpusId":226964489},"title":"Learn an Effective Lip Reading Model without Pains"},{"paperId":"854a9626dc0ad5d52e51a41301b2f69489540d04","externalIds":{"DBLP":"conf/interspeech/LiuCY20a","MAG":"3095622232","DOI":"10.21437/interspeech.2020-3146","CorpusId":226202014},"title":"Lip Graph Assisted Audio-Visual Speech Recognition Using Bidirectional Synchronous Fusion"},{"paperId":"79a511eca03dbfd0b48bd876f6bc99ca1690d1cc","externalIds":{"MAG":"3093450153","DBLP":"conf/mm/ZengLLG20","DOI":"10.1145/3394171.3413844","CorpusId":222278585},"title":"Talking Face Generation with Expression-Tailored Generative Adversarial Network"},{"paperId":"a4dabcac75e2a6e52b2ec915eef1fd74bd5f677f","externalIds":{"MAG":"3103801904","DBLP":"journals/corr/abs-2009-05784","ArXiv":"2009.05784","DOI":"10.1145/3394171.3413623","CorpusId":221655811},"title":"DualLip: A System for Joint Lip Reading and Generation"},{"paperId":"26738278bfee3cc9e5b9b82d199ef3bf83e7e56e","externalIds":{"ArXiv":"2008.10004","DBLP":"journals/tvcg/LiuHLLLZLY22","MAG":"3081242988","DOI":"10.1109/TVCG.2021.3107669","CorpusId":221266436,"PubMed":"34449390"},"title":"Geometry-Guided Dense Perspective Network for Speech-Driven Facial Animation"},{"paperId":"9c160a71d3265eedaf7645c39be073c966f10433","externalIds":{"MAG":"3101631197","DBLP":"conf/mm/PrajwalMNJ20","ArXiv":"2008.10010","DOI":"10.1145/3394171.3413532","CorpusId":221266065},"title":"A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild"},{"paperId":"c748ae0688a7ba0b6dbc33436a0fe76fa30786c2","externalIds":{"MAG":"3089177030","DBLP":"conf/eccv/0003BSB20","DOI":"10.1007/978-3-030-58577-8_25","CorpusId":221885852},"title":"Speech-Driven Facial Animation Using Cascaded GANs for Learning of Motion and Texture"},{"paperId":"e1859067487893f6580e934e9ee3408a2fa8b7e1","externalIds":{"ArXiv":"2008.09586","DBLP":"journals/corr/abs-2008-09586","MAG":"3081461879","DOI":"10.1109/TASLP.2021.3066303","CorpusId":221246385},"title":"An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation"},{"paperId":"dee8650c0a65588a09eb86751c600fb67a030bbc","externalIds":{"MAG":"3047985687","ArXiv":"2008.03592","DBLP":"journals/corr/abs-2008-03592","DOI":"10.1109/tmm.2021.3099900","CorpusId":221141930},"title":"Speech Driven Talking Face Generation From a Single Image and an Emotion Condition"},{"paperId":"3c72ce6b8b575d78d189dfc9ce89002d7a4db209","externalIds":{"DBLP":"journals/corr/abs-2007-06504","ArXiv":"2007.06504","MAG":"3046589207","DOI":"10.1109/ICASSP39728.2021.9415063","CorpusId":220496451},"title":"Towards Practical Lipreading with Distilled and Efficient Models"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","externalIds":{"MAG":"3138154797","ArXiv":"2006.05525","DBLP":"journals/ijcv/GouYMT21","DOI":"10.1007/s11263-021-01453-z","CorpusId":219559263},"title":"Knowledge Distillation: A Survey"},{"paperId":"24b3701937bd05ef6377f14682f4f67b40d09b50","externalIds":{"DBLP":"journals/corr/abs-2005-12318","ArXiv":"2005.12318","MAG":"3029279979","DOI":"10.1109/IJCNN48605.2020.9206665","CorpusId":218889762},"title":"Identity-Preserving Realistic Talking Face Generation"},{"paperId":"b63a9dc18baf645b4766b2b2ec8461c2c843275a","externalIds":{"MAG":"3023706973","ArXiv":"2005.03201","DBLP":"journals/corr/abs-2005-03201","CorpusId":218537849},"title":"What comprises a good talking-head video generation?: A Survey and Benchmark"},{"paperId":"db15b6799d7a09fcd41388d9e55d9c267e454965","externalIds":{"DBLP":"conf/icassp/EskimezMXD20","MAG":"3015814181","DOI":"10.1109/ICASSP40776.2020.9054103","CorpusId":216508929},"title":"End-To-End Generation of Talking Faces from Noisy Speech"},{"paperId":"d6ec232e30ae03c1f97981fb00c84195f9f9bff3","externalIds":{"ArXiv":"2004.14326","MAG":"3023351797","DBLP":"journals/corr/abs-2004-14326","DOI":"10.21437/INTERSPEECH.2020-1113","CorpusId":216641596},"title":"Seeing voices and hearing voices: learning discriminative embeddings using cross-modal self-supervision"},{"paperId":"7a7c4e666e5dd8a489770643b72fd99cf708f7c1","externalIds":{"ArXiv":"2004.12992","DBLP":"journals/corr/abs-2004-12992","MAG":"3019952993","CorpusId":216553251},"title":"MakeItTalk: Speaker-Aware Talking Head Animation"},{"paperId":"1de68f12db136526116c6ac7064fd13965f2c966","externalIds":{"DBLP":"journals/tnn/LiLYPZ22","MAG":"3014140399","ArXiv":"2004.02806","DOI":"10.1109/TNNLS.2021.3084827","CorpusId":214803074,"PubMed":"34111009"},"title":"A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects"},{"paperId":"634daa582ab0736e7c24a627b96db4cdefb106f1","externalIds":{"ArXiv":"2003.06439","MAG":"3011839522","DBLP":"journals/corr/abs-2003-06439","DOI":"10.1109/FG47880.2020.00133","CorpusId":212725274},"title":"Mutual Information Maximization for Effective Lip Reading"},{"paperId":"4129a667527cf7227c55fdf22743de6e2dedc490","externalIds":{"MAG":"3010647513","DBLP":"journals/corr/abs-2003-05709","ArXiv":"2003.05709","DOI":"10.1109/FG47880.2020.00132","CorpusId":212675425},"title":"Deformation Flow Based Two-Stream Network for Lip Reading"},{"paperId":"0872f4a6e14c6da98424e6daefe4d9b626ba4d3d","externalIds":{"DBLP":"conf/fgr/LuoYSC20","ArXiv":"2003.03983","MAG":"3010132389","DOI":"10.1109/FG47880.2020.00010","CorpusId":212634242},"title":"Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading"},{"paperId":"7431525dd5b821532191c7c078972bc457565d86","externalIds":{"DBLP":"journals/jstsp/ChungCK20","MAG":"3016663370","DOI":"10.1109/JSTSP.2020.2987720","CorpusId":218780999},"title":"Perfect Match: Self-Supervised Embeddings for Cross-Modal Retrieval"},{"paperId":"797389ca052efd160ed759d7ef7adf9c30a917d6","externalIds":{"DBLP":"journals/corr/abs-2003-00196","ArXiv":"2003.00196","MAG":"2970315999","CorpusId":202767986},"title":"First Order Motion Model for Image Animation"},{"paperId":"bec253e82076dc363b8fd72d5c8fadf8f5b7e475","externalIds":{"ArXiv":"2002.10137","MAG":"3010434693","CorpusId":212414741},"title":"Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose"},{"paperId":"62d04f01a3e91d4304de012263a7f1a265d7b905","externalIds":{"MAG":"3001079424","DBLP":"journals/sivp/ChenDZ20","DOI":"10.1007/s11760-019-01630-1","CorpusId":214376123},"title":"Lipreading with DenseNet and resBi-LSTM"},{"paperId":"d8da125d2511d037df036c0f0da7c57135e24409","externalIds":{"ArXiv":"2001.08702","DBLP":"conf/icassp/Martinez0PP20","MAG":"3003133812","DOI":"10.1109/ICASSP40776.2020.9053841","CorpusId":210868410},"title":"Lipreading Using Temporal Convolutional Networks"},{"paperId":"369e9e0bb58bb4602a205a085bd683978a6d1f77","externalIds":{"DBLP":"journals/corr/abs-2001-05201","ArXiv":"2001.05201","MAG":"2999966482","DOI":"10.1109/tifs.2022.3146783","CorpusId":210701290},"title":"Everybody’s Talkin’: Let Me Talk as You Want"},{"paperId":"239c77b203716478bfbd522f6c5b3c81a4d414c5","externalIds":{"DBLP":"conf/icassp/YuZWGWKLLMY20","MAG":"2997540481","ArXiv":"2001.01656","DOI":"10.1109/ICASSP40776.2020.9054127","CorpusId":209862704},"title":"Audio-Visual Recognition of Overlapped Speech for the LRS2 Dataset"},{"paperId":"b7de44a2edb7570fddbdcb2ee69df9f17adc3396","externalIds":{"DBLP":"journals/corr/abs-1912-05566","ArXiv":"1912.05566","MAG":"2995238198","DOI":"10.1007/978-3-030-58517-4_42","CorpusId":209323961},"title":"Neural Voice Puppetry: Audio-driven Facial Reenactment"},{"paperId":"77779ec8eaf8fb3d2791a2be7caa9b4fa7b3935b","externalIds":{"MAG":"2990159822","ArXiv":"1911.12747","DBLP":"journals/corr/abs-1911-12747","DOI":"10.1109/ICASSP40776.2020.9054253","CorpusId":208513145},"title":"ASR is All You Need: Cross-Modal Distillation for Lip Reading"},{"paperId":"a19274a50938d70755269ff6aeefe818e44469ad","externalIds":{"DBLP":"journals/corr/abs-1911-11502","MAG":"2989671705","ArXiv":"1911.11502","DOI":"10.1609/AAAI.V34I04.6174","CorpusId":208290896},"title":"Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers"},{"paperId":"a4f0556a6225135c4525c5fda2fcab00c91609a3","externalIds":{"ArXiv":"1911.04890","DBLP":"conf/asru/MakinoLASGBS19","MAG":"3006974783","DOI":"10.1109/ASRU46091.2019.9004036","CorpusId":207863368},"title":"Recurrent Neural Network Transducer for Audio-Visual Speech Recognition"},{"paperId":"97412aded29b05ee69a63ea9ec2b22f41c321149","externalIds":{"DBLP":"journals/corr/abs-1910-12607","MAG":"3016011332","ArXiv":"1910.12607","DOI":"10.1109/ICASSP40776.2020.9054438","CorpusId":204904767},"title":"Generative Pre-Training for Speech with Autoregressive Predictive Coding"},{"paperId":"2466d8b7b99b2928f091c91d672d36f1b3dbf15e","externalIds":{"DBLP":"conf/mm/RMPJNJ19","MAG":"3006796983","ArXiv":"2003.00418","DOI":"10.1145/3343031.3351066","CorpusId":204836931},"title":"Towards Automatic Face-to-Face Translation"},{"paperId":"2a8e0d6f4c6d0203b9892bfe02eaea13d0e9aa6c","externalIds":{"DBLP":"conf/iccv/ZhangCW19","MAG":"2981501041","DOI":"10.1109/ICCV.2019.00080","CorpusId":204956885},"title":"Spatio-Temporal Fusion Based Convolutional Sequence Learning for Lip Reading"},{"paperId":"442ee1d9a38309fc850455b5e63e4e1499a29a50","externalIds":{"DBLP":"conf/interspeech/KoumparoulisP19","MAG":"2973085229","DOI":"10.21437/interspeech.2019-2618","CorpusId":202725301},"title":"MobiLipNet: Resource-Efficient Deep Learning Based Lipreading"},{"paperId":"af5ec4a13c07841ff2113ca734eb007e6744cf18","externalIds":{"DBLP":"journals/corr/abs-1909-02518","MAG":"2971634123","ArXiv":"1909.02518","DOI":"10.1145/3355089.3356500","CorpusId":202538634},"title":"Neural style-preserving visual dubbing"},{"paperId":"3ad2cda8d34ce747c6dabc0e325311ffa8c44576","externalIds":{"MAG":"2970270554","DBLP":"journals/corr/abs-1908-11618","ArXiv":"1908.11618","CorpusId":201698086},"title":"Multi-Grained Spatio-temporal Modeling for Lip-reading"},{"paperId":"f06ab43069480c09d7170075a6ea74669a71f139","externalIds":{"MAG":"2950448185","ArXiv":"1906.06337","DBLP":"journals/corr/abs-1906-06337","DOI":"10.1007/s11263-019-01251-8","CorpusId":189928265},"title":"Realistic Speech-Driven Facial Animation with GANs"},{"paperId":"ed45bc75ca3406866e1bb9e95ad251536e9b985c","externalIds":{"MAG":"2990452356","ArXiv":"1905.08233","DBLP":"conf/iccv/ZakharovSBL19","DOI":"10.1109/ICCV.2019.00955","CorpusId":159040543},"title":"Few-Shot Adversarial Learning of Realistic Neural Talking Head Models"},{"paperId":"0d599e1b5deb2c0a95438960ce0fe7b62723cb16","externalIds":{"MAG":"2980562267","ArXiv":"1905.06860","DBLP":"journals/corr/abs-1905-06860","DOI":"10.1145/3340555.3353745","CorpusId":155099808},"title":"Speaker-Independent Speech-Driven Visual Speech Synthesis using Domain-Adapted Acoustic Models"},{"paperId":"a0852cd9a026bc90168fa85fa422cb0e48f98394","externalIds":{"DBLP":"journals/corr/abs-1905-03820","MAG":"3046779505","ArXiv":"1905.03820","DOI":"10.1109/CVPR.2019.00802","CorpusId":109936942},"title":"Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss"},{"paperId":"91a1370d26ee903296bb4990a84c23f2fa8e8c83","externalIds":{"DBLP":"conf/cvpr/CudeiroBLRB19","MAG":"2981263323","ArXiv":"1905.03079","DOI":"10.1109/CVPR.2019.01034","CorpusId":122437581},"title":"Capture, Learning, and Synthesis of 3D Speaking Styles"},{"paperId":"8d26b787d3f765be81f2d0268f86003a5942760c","externalIds":{"MAG":"2943459728","DBLP":"journals/corr/abs-1905-01680","ArXiv":"1905.01680","DOI":"10.1145/3306346.3322999","CorpusId":146120721},"title":"Learning character-agnostic motion for motion retargeting in 2D"},{"paperId":"2dc08aeb27bb8b9b73cef89b326e7d0204cbebb3","externalIds":{"DBLP":"conf/icassp/LiWLSL19","MAG":"2936037929","DOI":"10.1109/icassp.2019.8682868","CorpusId":146084515},"title":"Improving Audio-visual Speech Recognition Performance with Cross-modal Student-teacher Training"},{"paperId":"607048b431cea997ae9dd01f029a73c502d0273f","externalIds":{"MAG":"3021355978","ArXiv":"1905.02540","DBLP":"conf/bmvc/WengK19","CorpusId":146808419},"title":"Learning Spatio-Temporal Features with Two-Stream Deep 3D CNNs for Lipreading"},{"paperId":"a3e8f459230b91b4b055665cbf73bed01d8947e7","externalIds":{"DBLP":"journals/corr/abs-1904-07002","MAG":"2939738131","ArXiv":"1904.07002","DOI":"10.1109/FG47880.2020.00100","CorpusId":119105505},"title":"Synthesising 3D Facial Motion from “In-the-Wild” Speech"},{"paperId":"c5651aea43997f71891c2cc7694ddf51af95c2c0","externalIds":{"MAG":"2924744988","DBLP":"journals/corr/abs-1903-08527","ArXiv":"1903.08527","DOI":"10.1109/CVPRW.2019.00038","CorpusId":84187285},"title":"Accurate 3D Face Reconstruction With Weakly-Supervised Learning: From Single Image to Image Set"},{"paperId":"0e8cd058ae29c6f60a8750c1df3caa5dc0e99543","externalIds":{"MAG":"2914217321","DBLP":"journals/ijcv/JamaludinCZ19","DOI":"10.1007/s11263-019-01150-y","CorpusId":61156031},"title":"You Said That?: Synthesising Talking Faces from Audio"},{"paperId":"b4f8c1353aa2d88cacfaef1b3afba74dbf427d89","externalIds":{"DBLP":"journals/corr/abs-1901-08971","MAG":"2912336782","ArXiv":"1901.08971","DOI":"10.1109/ICCV.2019.00009","CorpusId":59292011},"title":"FaceForensics++: Learning to Detect Manipulated Facial Images"},{"paperId":"6eb0b0ddc1f87df9c74259feef5c6ccafc334a8f","externalIds":{"ArXiv":"1812.08685","MAG":"2904573504","DBLP":"journals/corr/abs-1812-08685","CorpusId":56517175},"title":"DeepFakes: a New Threat to Face Recognition? Assessment and Detection"},{"paperId":"bba63bb4a4e80ec3d431d2dd0886f8ced7a1d75b","externalIds":{"MAG":"3025498998","DBLP":"conf/ijcai/ZhuHLZH20","ArXiv":"1812.06589","DOI":"10.24963/ijcai.2020/327","CorpusId":218614068},"title":"Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning"},{"paperId":"60fa866c24c78b3f022651370cc194f3865560d0","externalIds":{"MAG":"2897492880","ArXiv":"1810.06990","DBLP":"conf/fgr/YangZFYWXLSC19","DOI":"10.1109/FG.2019.8756582","CorpusId":53094248},"title":"LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild"},{"paperId":"8294e72a443dc1429ae7111c0ba28c7d1cd67ab4","externalIds":{"MAG":"2897318954","DBLP":"conf/uist/0003YSLS18","DOI":"10.1145/3242587.3242599","CorpusId":52981632},"title":"Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands"},{"paperId":"263ad0264a19ac58905804715f14fb53fe2f181a","externalIds":{"MAG":"2895226286","DBLP":"conf/icmi/PhamWP18","DOI":"10.1145/3242969.3243017","CorpusId":52897721},"title":"End-to-end Learning for 3D Facial Animation from Speech"},{"paperId":"938717d66533d900abac7451e15c1f322e78db81","externalIds":{"MAG":"2886945201","DBLP":"journals/ivc/Fernandez-Lopez18","DOI":"10.1016/J.IMAVIS.2018.07.002","CorpusId":52881955},"title":"Survey on automatic lip-reading in the era of deep learning"},{"paperId":"4d66090e0bddb6f751241acd6e59cf25756e57a9","externalIds":{"MAG":"2950864153","ArXiv":"1809.08001","DBLP":"conf/icassp/ChungCK19","DOI":"10.1109/ICASSP.2019.8682524","CorpusId":52338130},"title":"Perfect Match: Improved Cross-modal Embeddings for Audio-visual Synchronisation"},{"paperId":"fcecc4ef2c32dbedda61648febb39a0f905c367e","externalIds":{"MAG":"2963469293","DBLP":"journals/corr/abs-1809-02108","ArXiv":"1809.02108","DOI":"10.1109/TPAMI.2018.2889052","CorpusId":52168058,"PubMed":"30582526"},"title":"Deep Audio-Visual Speech Recognition"},{"paperId":"a46174aa635759070984ed7062c9402695bce830","externalIds":{"ArXiv":"1809.00496","MAG":"2891205112","DBLP":"journals/corr/abs-1809-00496","CorpusId":52155419},"title":"LRS3-TED: a large-scale dataset for visual speech recognition"},{"paperId":"b9b2355f1d637c04ccd450c183bebd545f77f4b7","externalIds":{"DBLP":"conf/icassp/HalperinEP19","MAG":"2949307595","ArXiv":"1808.06250","DOI":"10.1109/ICASSP.2019.8682863","CorpusId":52053623},"title":"Dynamic Temporal Alignment of Speech to Lips"},{"paperId":"1816f98e2a4dd54690c2689cf529699d8843e847","externalIds":{"MAG":"2883082281","DBLP":"conf/aaai/Zhou000W19","ArXiv":"1807.07860","DOI":"10.1609/aaai.v33i01.33019299","CorpusId":49905113},"title":"Talking Face Generation by Adversarially Disentangled Audio-Visual Representation"},{"paperId":"e5befd105f7bbd373208522d5b85682116b59c38","externalIds":{"ArXiv":"1807.05162","MAG":"2972756321","DBLP":"journals/corr/abs-1807-05162","DOI":"10.21437/interspeech.2019-1669","CorpusId":49742238},"title":"Large-Scale Visual Speech Recognition"},{"paperId":"04187519dc8c468f2b5b17442413ada7830068e5","externalIds":{"MAG":"2949396501","DBLP":"journals/corr/abs-1806-06053","ArXiv":"1806.06053","DOI":"10.21437/Interspeech.2018-1943","CorpusId":49274645},"title":"Deep Lip Reading: a comparison of models and an online application"},{"paperId":"8875ae233bc074f5cd6c4ebba447b536a7e847a5","externalIds":{"MAG":"2950872062","DBLP":"journals/corr/abs-1806-05622","ArXiv":"1806.05622","DOI":"10.21437/Interspeech.2018-1929","CorpusId":49211906},"title":"VoxCeleb2: Deep Speaker Recognition"},{"paperId":"8b0e2250a2eb250682cfef8e369742cf10d0dfa2","externalIds":{"MAG":"2963822910","DBLP":"journals/taffco/SadoughiB21","ArXiv":"1806.00154","DOI":"10.1109/TAFFC.2019.2916031","CorpusId":44113436},"title":"Speech-Driven Expressive Talking Lips with Conditional Sequential Generative Adversarial Networks"},{"paperId":"6f0ce4d957c4e9556b04b539105837a5db63b925","externalIds":{"DBLP":"conf/nips/JiaZWWSRCNPLW18","MAG":"2963432880","ArXiv":"1806.04558","CorpusId":48363067},"title":"Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis"},{"paperId":"8dac02f61e12560607f857cee3c1d5abaf40ecd0","externalIds":{"MAG":"2964048159","ArXiv":"1807.00230","DBLP":"conf/nips/KorbarTT18","CorpusId":53280782},"title":"Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization"},{"paperId":"efb0a4574736a976b6dc3c23be45e5fc0ac6fe41","externalIds":{"ArXiv":"1805.11714","MAG":"2949235719","DBLP":"journals/corr/abs-1805-11714","DOI":"10.1145/3197517.3201283","CorpusId":44073530},"title":"Deep video portraits"},{"paperId":"f722b0a7a9b7709d693b9d39195c779832a943fe","externalIds":{"MAG":"2804600264","DBLP":"journals/corr/abs-1805-09313","ArXiv":"1805.09313","CorpusId":44061183},"title":"End-to-End Speech-Driven Facial Animation with Temporal GANs"},{"paperId":"69977d60dc7eb94543c6fb64b86b7c7a4ed1b6d1","externalIds":{"MAG":"2799930024","ArXiv":"1805.05563","DBLP":"journals/corr/abs-1805-05563","DOI":"10.1007/s11263-018-1097-z","CorpusId":13689658},"title":"Facial Landmark Detection: A Literature Survey"},{"paperId":"7c223baccf679fc212dd4d9ecbebf30b7a8616af","externalIds":{"DBLP":"conf/ijcai/SongZLWQ19","ArXiv":"1804.04786","MAG":"2964559396","DOI":"10.24963/ijcai.2019/129","CorpusId":4867611},"title":"Talking Face Generation by Conditional Recurrent Adversarial Network"},{"paperId":"e3cac1f3fa0ca9ba41f1cb0fbbd28a0f320903e3","externalIds":{"MAG":"2950841531","DBLP":"journals/corr/abs-1804-04121","ArXiv":"1804.04121","DOI":"10.21437/Interspeech.2018-1400","CorpusId":4797928},"title":"The Conversation: Deep Audio-Visual Speech Enhancement"},{"paperId":"171f8f1090ef0533ff470ed5a4d31ecfefcc74be","externalIds":{"DBLP":"journals/corr/abs-1804-03641","ArXiv":"1804.03641","MAG":"2796992393","DOI":"10.1007/978-3-030-01231-1_39","CorpusId":4724792},"title":"Audio-Visual Scene Analysis with Self-Supervised Multisensory Features"},{"paperId":"d7e12f93339dc9a97cc325a4a3e9a13bdffb4988","externalIds":{"MAG":"2952005872","DBLP":"conf/eccv/ChenLMDX18","ArXiv":"1803.10404","DOI":"10.1007/978-3-030-01234-2_32","CorpusId":4435268},"title":"Lip Movements Generation at a Glance"},{"paperId":"3774776f2a5ec29c946a89d6ad6b6e16d2f471ec","externalIds":{"DBLP":"journals/corr/abs-1803-07461","ArXiv":"1803.07461","MAG":"2790649793","CorpusId":4708467},"title":"Speech-Driven Facial Reenactment Using Conditional Generative Adversarial Networks"},{"paperId":"dfa043929dbf123438cbbc1648b0d2252010f34a","externalIds":{"MAG":"2790322981","DBLP":"journals/corr/abs-1803-04988","ArXiv":"1803.04988","DOI":"10.1109/FG.2018.00088","CorpusId":3827301},"title":"LCANet: End-to-End Lipreading with Cascaded Attention-CTC"},{"paperId":"b91d738cd1f5d550c5b27f328e55308a0a73b2d2","externalIds":{"DBLP":"journals/corr/abs-1803-03849","MAG":"2792330714","ArXiv":"1803.03849","DOI":"10.1109/CVPR.2018.00458","CorpusId":3841418},"title":"Learning to Localize Sound Source in Visual Scenes"},{"paperId":"aadcf6136b9eaa0c96358e4351a070258a3bcc40","externalIds":{"MAG":"2952729343","DBLP":"conf/icassp/PetridisSMCTP18","ArXiv":"1802.06424","DOI":"10.1109/ICASSP.2018.8461326","CorpusId":3354694},"title":"End-to-End Audiovisual Speech Recognition"},{"paperId":"ca235ce0decdb4f80024a429a20ae4437ceae09e","externalIds":{"DBLP":"conf/cvpr/DengGXZ19","MAG":"2969985801","DOI":"10.1109/CVPR.2019.00482","CorpusId":8923541},"title":"ArcFace: Additive Angular Margin Loss for Deep Face Recognition"},{"paperId":"efeaa6e3114d6d6ae5c3041b66ac9a9ae9bf52bf","externalIds":{"MAG":"2784435047","ArXiv":"1801.07455","DBLP":"journals/corr/abs-1801-07455","DOI":"10.1609/aaai.v32i1.12328","CorpusId":19167105},"title":"Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition"},{"paperId":"dfc504536e8434eb008680343abb77010965169e","externalIds":{"DBLP":"journals/corr/abs-1712-06651","ArXiv":"1712.06651","MAG":"2777181663","DOI":"10.1007/978-3-030-01246-5_27","CorpusId":36022762},"title":"Objects that Sound"},{"paperId":"478d6102a2df86b0f4e69e398f96619312ecdc8c","externalIds":{"DBLP":"conf/icassp/KannanWNSCP18","ArXiv":"1712.01996","MAG":"2963240019","DOI":"10.1109/ICASSP.2018.8462682","CorpusId":41638977},"title":"An Analysis of Incorporating an External Language Model into a Sequence-to-Sequence Model"},{"paperId":"e119ce794f45069df77946cce94fb12c15b4e44c","externalIds":{"DBLP":"journals/corr/abs-1801-01442","MAG":"2782422271","ArXiv":"1801.01442","CorpusId":9633469},"title":"ObamaNet: Photo-realistic lip-sync from text"},{"paperId":"f997d69d78af086dec4462e4319c6d241f42c0c1","externalIds":{"MAG":"2963082324","DBLP":"conf/interspeech/GabbaySP18","DOI":"10.21437/Interspeech.2018-1955","CorpusId":4372017},"title":"Visual Speech Enhancement"},{"paperId":"fb37561499573109fc2cebb6a7b08f44917267dd","externalIds":{"MAG":"2963420686","DBLP":"journals/corr/abs-1709-01507","ArXiv":"1709.01507","DOI":"10.1109/CVPR.2018.00745","CorpusId":140309863},"title":"Squeeze-and-Excitation Networks"},{"paperId":"c17d280b8f3adc8593bf4c58f445a5f947ed32f2","externalIds":{"DBLP":"conf/avsp/KoumparoulisPMR17","MAG":"2891226622","DOI":"10.21437/AVSP.2017-13","CorpusId":3531386},"title":"Exploring ROI size in deep learning based lipreading"},{"paperId":"0ebc58bb5d517db0111f3565c4eb378d93dad908","externalIds":{"DBLP":"journals/corr/abs-1708-06767","ArXiv":"1708.06767","MAG":"2750143381","CorpusId":195346379},"title":"Seeing Through Noise: Speaker Separation and Enhancement using Visually-derived Speech"},{"paperId":"2b598c73e9335277106fcb8acdad6cda227c6cdf","externalIds":{"DBLP":"journals/corr/abs-1708-01204","MAG":"2949190577","ArXiv":"1708.01204","DOI":"10.1109/ICCVW.2017.61","CorpusId":10815436},"title":"Improved Speech Reconstruction from Silent Video"},{"paperId":"95b803d07c37e8349bd7b1318367d8237c76cbc0","externalIds":{"MAG":"2739192055","DBLP":"journals/tog/KarrasALHL17","DOI":"10.1145/3072959.3073658","CorpusId":13515193},"title":"Audio-driven facial animation by joint end-to-end learning of pose and emotion"},{"paperId":"cc63b9cf84b1fb0b3eca84372919f74a40b7c132","externalIds":{"DBLP":"journals/tog/TaylorKYMKRHM17","MAG":"2737658251","DOI":"10.1145/3072959.3073699","CorpusId":32124381},"title":"A deep learning approach for generalized speech animation"},{"paperId":"e76edb86f270c3a77ed9f5a1e1b305461f36f96f","externalIds":{"MAG":"2951910147","DBLP":"conf/cvpr/Tulyakov0YK18","ArXiv":"1707.04993","DOI":"10.1109/CVPR.2018.00165","CorpusId":4475365},"title":"MoCoGAN: Decomposing Motion and Content for Video Generation"},{"paperId":"9da734397acd7ff7c557960c62fb1b400b27bd89","externalIds":{"DBLP":"journals/corr/ZhangZLS17","MAG":"2724359148","ArXiv":"1707.01083","DOI":"10.1109/CVPR.2018.00716","CorpusId":24982157},"title":"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"},{"paperId":"cb7cf162fb44ef06abd6aa30026c99ded8cbcdf8","externalIds":{"MAG":"2745771616","DBLP":"conf/cvpr/PhamCP17","DOI":"10.1109/CVPRW.2017.287","CorpusId":13140860},"title":"Speech-Driven 3D Facial Animation with Implicit Emotional Awareness: A Deep Learning Approach"},{"paperId":"8a26431833b0ea8659ef1d24bff3ac9e56dcfcd0","externalIds":{"MAG":"2950102398","DBLP":"conf/interspeech/NagraniCZ17","ArXiv":"1706.08612","DOI":"10.21437/Interspeech.2017-950","CorpusId":10475843},"title":"VoxCeleb: A Large-Scale Speaker Identification Dataset"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"b61a3f8b80bbd44f24544dc915f52fd30bbdf485","externalIds":{"ArXiv":"1705.07750","MAG":"2619082050","DBLP":"conf/cvpr/CarreiraZ17","DOI":"10.1109/CVPR.2017.502","CorpusId":206596127},"title":"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"},{"paperId":"a8632cf6c1ef4319966564328d187876d3bef363","externalIds":{"MAG":"2613087992","ArXiv":"1705.02966","DBLP":"conf/bmvc/ChungJZ17","DOI":"10.5244/C.31.109","CorpusId":11058994},"title":"You said that?"},{"paperId":"2f5d7ff5eb22042e91ab5208795415a9c91ccb3b","externalIds":{"MAG":"2609613276","DBLP":"conf/fgr/Fernandez-Lopez17","ArXiv":"1704.08028","DOI":"10.1109/FG.2017.34","CorpusId":25992841},"title":"Towards Estimating the Upper Bound of Visual-Speech Recognition: The Visual Lip-Reading Feasibility Database"},{"paperId":"3647d6d0f151dc05626449ee09cc7bce55be497e","externalIds":{"DBLP":"journals/corr/HowardZCKWWAA17","ArXiv":"1704.04861","MAG":"2612445135","CorpusId":12670695},"title":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"},{"paperId":"4afdb836301c0233bb8cf0d8a33212ac0c113381","externalIds":{"MAG":"2951196083","DBLP":"conf/interspeech/StafylakisT17","ArXiv":"1703.04105","DOI":"10.21437/Interspeech.2017-85","CorpusId":12218393},"title":"Combining Residual Networks with LSTMs for Lipreading"},{"paperId":"cd54941cc46656005d31b1b24e3a002a7acd5b3f","externalIds":{"MAG":"2570575067","DBLP":"journals/jiis/CzyzewskiKBKS17","DOI":"10.1007/s10844-016-0438-z","CorpusId":21036636},"title":"An audio-visual corpus for multimodal automatic speech recognition"},{"paperId":"5c87c275ddde2e0b75264fe9dad7b130db410601","externalIds":{"ArXiv":"1701.00495","MAG":"2950479517","DBLP":"journals/corr/EphratP17","DOI":"10.1109/ICASSP.2017.7953127","CorpusId":6912103},"title":"Vid2speech: Speech reconstruction from silent video"},{"paperId":"8acbe90d5b852dadea7810345451a99608ee54c7","externalIds":{"MAG":"2963073614","DBLP":"conf/cvpr/IsolaZZE17","ArXiv":"1611.07004","DOI":"10.1109/CVPR.2017.632","CorpusId":6200260},"title":"Image-to-Image Translation with Conditional Adversarial Networks"},{"paperId":"74f1c93dd3a8c3f9fa59fadef9a744234b2977eb","externalIds":{"DBLP":"conf/accv/ChungZ16","MAG":"2594690981","DOI":"10.1007/978-3-319-54184-6_6","CorpusId":19806033},"title":"Lip Reading in the Wild"},{"paperId":"87defac1045bfa9af0162cd248d193e9be6eb25b","externalIds":{"DBLP":"conf/accv/ChungZ16a","MAG":"2604379605","DOI":"10.1007/978-3-319-54427-4_19","CorpusId":26294509},"title":"Out of Time: Automated Lip Sync in the Wild"},{"paperId":"bed6d0097df1e9ac82f789f6da268cdb3dd65bc3","externalIds":{"MAG":"2551572271","DBLP":"conf/cvpr/ChungSVZ17","ArXiv":"1611.05358","DOI":"10.1109/CVPR.2017.367","CorpusId":1662180},"title":"Lip Reading Sentences in the Wild"},{"paperId":"291c0e453503a704c0fd932a067ca054cc7edad6","externalIds":{"ArXiv":"1611.01599","MAG":"2578229578","CorpusId":7421075},"title":"LipNet: End-to-End Sentence-level Lipreading"},{"paperId":"23b559b5ab27f2fca6f56c0a7b6478bcf69db509","externalIds":{"ArXiv":"1611.00179","DBLP":"journals/corr/XiaHQWYLM16","MAG":"2950359962","CorpusId":5758868},"title":"Dual Learning for Machine Translation"},{"paperId":"5694e46284460a648fe29117cbc55f6c9be3fa3c","externalIds":{"MAG":"2963446712","ArXiv":"1608.06993","DBLP":"journals/corr/HuangLW16a","DOI":"10.1109/CVPR.2017.243","CorpusId":9433631},"title":"Densely Connected Convolutional Networks"},{"paperId":"711237cd57b1e10b530a8a2be3c36b75d09153f5","externalIds":{"DBLP":"conf/specom/VerkhodanovaRKI16","MAG":"2513259840","DOI":"10.1007/978-3-319-43958-7_40","CorpusId":34549549},"title":"HAVRUS Corpus: High-Speed Recordings of Audio-Visual Russian Speech"},{"paperId":"ba11b4feb04a472cb5e5962697ed6faa653dc647","externalIds":{"MAG":"2906447902","DBLP":"journals/cacm/ThiesZSTN19","ArXiv":"2007.14808","DOI":"10.1145/3292039","CorpusId":52858569},"title":"Face2Face: Real-Time Face Capture and Reenactment of RGB Videos"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"f0b2395f29930568ecd206dc9d67ea8bb4876277","externalIds":{"DBLP":"journals/ieeesp/AkhtarMF15","MAG":"1962124124","DOI":"10.1109/MSP.2015.116","CorpusId":14347643},"title":"Biometric Liveness Detection: Challenges and Research Opportunities"},{"paperId":"4753931ed99db986046ac2b0183b684560fdcefa","externalIds":{"MAG":"2408589826","DBLP":"conf/acivs/RekikBM15a","DOI":"10.1007/978-3-319-25903-1_49","CorpusId":8857211},"title":"Human Machine Interaction via Visual Speech Spotting"},{"paperId":"d6c1d9c121a4535ec23910b807bde9f4f6263907","externalIds":{"MAG":"2120067677","DBLP":"conf/kdd/KimYTM15","DOI":"10.1145/2783258.2783356","CorpusId":17154868},"title":"A Decision Tree Framework for Spatiotemporal Sequence Prediction"},{"paperId":"3056add22b20e3361c38c0472d294a79d4031cb4","externalIds":{"DBLP":"conf/icassp/ChanJLV16","MAG":"2327501763","ArXiv":"1508.01211","DOI":"10.1109/ICASSP.2016.7472621","CorpusId":18165915},"title":"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"},{"paperId":"b624504240fa52ab76167acfe3156150ca01cf3b","externalIds":{"ArXiv":"1506.07503","MAG":"2953022181","DBLP":"journals/corr/ChorowskiBSCB15","CorpusId":1921173},"title":"Attention-Based Models for Speech Recognition"},{"paperId":"f9c990b1b5724e50e5632b94fdb7484ece8a6ce7","externalIds":{"ArXiv":"1506.04214","DBLP":"conf/nips/ShiCWYWW15","MAG":"2953118818","CorpusId":6352419},"title":"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting"},{"paperId":"653b957d4c70d6cbf8de443df497e47edcab77b4","externalIds":{"DBLP":"conf/fgr/AninaZZP15","MAG":"1526392145","DOI":"10.1109/FG.2015.7163155","CorpusId":33658102},"title":"OuluVS2: A multi-view audiovisual database for non-rigid mouth motion analysis"},{"paperId":"8128fb68dc94c086ba213a28ac132e53eb8da02a","externalIds":{"MAG":"2146991130","DBLP":"journals/cgf/GarridoVSSVPT15","DOI":"10.1111/cgf.12552","CorpusId":18087669},"title":"VDub: Modifying Face Video of Actors for Plausible Visual Alignment to a Dubbed Audio Track"},{"paperId":"8c028b86f5551757becd4c4304bddebb49e880b3","externalIds":{"MAG":"1569907127","DBLP":"conf/icassp/FanWSX15","DOI":"10.1109/ICASSP.2015.7178899","CorpusId":15362638},"title":"Photo-real talking head with deep bidirectional LSTM"},{"paperId":"7001c0cf18a29453b43c39f913c6203362cc0cf9","externalIds":{"DBLP":"journals/speech/MattheysesV15","MAG":"2338517121","DOI":"10.1016/j.specom.2014.11.001","CorpusId":12173489},"title":"Audiovisual speech synthesis: An overview of the state-of-the-art"},{"paperId":"2b91c03e7be00b2eee610a73ef2becc669e01781","externalIds":{"DBLP":"conf/icassp/MrouehMG15","MAG":"2949547965","ArXiv":"1501.05396","DOI":"10.1109/ICASSP.2015.7178347","CorpusId":351326},"title":"Deep multimodal learning for Audio-Visual Speech Recognition"},{"paperId":"24741d280869ad9c60321f5ab6e5f01b7852507d","externalIds":{"ArXiv":"1412.5567","MAG":"1922655562","DBLP":"journals/corr/HannunCCCDEPSSCN14","CorpusId":16979536},"title":"Deep Speech: Scaling up end-to-end speech recognition"},{"paperId":"353ecf7b66b3e9ff5e9f41145a147e899a2eea5c","externalIds":{"DBLP":"journals/corr/MirzaO14","ArXiv":"1411.1784","MAG":"2125389028","CorpusId":12803511},"title":"Conditional Generative Adversarial Nets"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","externalIds":{"MAG":"2949429431","ArXiv":"1409.1556","DBLP":"journals/corr/SimonyanZ14a","CorpusId":14124313},"title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"4c842fbd4c032dd4d931eb6ff1eaa2a13450b7af","externalIds":{"MAG":"2060510034","DBLP":"journals/ivc/ZhouZHP14","DOI":"10.1016/j.imavis.2014.06.004","CorpusId":205398164},"title":"A review of recent advances in visual speech decoding"},{"paperId":"d78b6a5b0dcaa81b1faea5fb0000045a62513567","externalIds":{"MAG":"2087681821","DBLP":"conf/cvpr/KazemiS14","DOI":"10.1109/CVPR.2014.241","CorpusId":2031947},"title":"One millisecond face alignment with an ensemble of regression trees"},{"paperId":"a285f7d80e47b05ebff83387424cd0df8cb7833d","externalIds":{"ArXiv":"1602.02651","MAG":"2080277992","DBLP":"journals/corr/0001VRTPT16","DOI":"10.1109/CVPR.2014.537","CorpusId":2505656},"title":"Automatic Face Reenactment"},{"paperId":"c75ba6ef724c0c3a9c9510a70da4cc8729b59a35","externalIds":{"DBLP":"journals/tvcg/CaoWZTZ14","MAG":"2017107803","DOI":"10.1109/TVCG.2013.249","CorpusId":206804955,"PubMed":"24434222"},"title":"FaceWarehouse: A 3D Facial Expression Database for Visual Computing"},{"paperId":"2d8de70f089b581d386aaa0ced5f9421a3fd08c0","externalIds":{"MAG":"2129360799","DBLP":"conf/cvpr/AndersonSWC13","DOI":"10.1109/CVPR.2013.434","CorpusId":1075172},"title":"Expressive Visual Text-to-Speech Using Active Appearance Models"},{"paperId":"4177ec52d1b80ed57f2e72b0f9a42365f1a8598d","externalIds":{"DBLP":"journals/corr/abs-1303-5778","MAG":"2950689855","ArXiv":"1303.5778","DOI":"10.1109/ICASSP.2013.6638947","CorpusId":206741496},"title":"Speech recognition with deep recurrent neural networks"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"98ab45160269f7c1545f7924f989d5da1895e9a5","externalIds":{"MAG":"2084235337","DBLP":"journals/ram/MoriMK12","DOI":"10.1109/MRA.2012.2192811","CorpusId":39190841},"title":"The Uncanny Valley [From the Field]"},{"paperId":"c6954d60ab29daee7fc256c316cd77387b8bb07b","externalIds":{"MAG":"2142490914","DBLP":"conf/eusipco/CappellettaH11","CorpusId":17892563},"title":"Viseme definitions comparison for visual-only speech recognition"},{"paperId":"5d2fd3dccd9dc154cec7ada22325b2bb9731980d","externalIds":{"MAG":"2124765650","DBLP":"conf/icmi/DeenaHG10","DOI":"10.1145/1891903.1891942","CorpusId":1263913},"title":"Visual speech synthesis by modelling coarticulation dynamics using a non-parametric switching state-space model"},{"paperId":"84e318faaf7945e618579ac1b0b11306e5a7f3d7","externalIds":{"MAG":"2162220380","DOI":"10.1109/QOMEX.2009.5246972","CorpusId":23547663},"title":"A no-reference perceptual image sharpness metric based on a cumulative probability of blur detection"},{"paperId":"bafc2727929bc1f421d3b74ac2f7da4279c6e04e","externalIds":{"DBLP":"journals/tmm/XieL07","MAG":"2114336453","DOI":"10.1109/TMM.2006.888009","CorpusId":16584961},"title":"Realistic Mouth-Synching for Speech-Driven Talking Face Using Articulatory Modelling"},{"paperId":"73d5eec9b37b33890f01123714ad92354baaeb08","externalIds":{"MAG":"1968263568","DOI":"10.1121/1.4781387","CorpusId":24007468,"PubMed":"17804980"},"title":"Audiovisual integration and lipreading abilities of older adults with normal and impaired hearing."},{"paperId":"5129350ec0bd8f1fe78a9b864865709f8d8de058","externalIds":{"MAG":"2015143272","DOI":"10.1121/1.2229005","CorpusId":15852230,"PubMed":"17139705"},"title":"An audio-visual corpus for speech perception and automatic speech recognition."},{"paperId":"46f30e94dd3d5902141c5fbe58d0bc9189545c76","externalIds":{"DBLP":"conf/cvpr/HadsellCL06","MAG":"2138621090","DOI":"10.1109/CVPR.2006.100","CorpusId":8281592},"title":"Dimensionality Reduction by Learning an Invariant Mapping"},{"paperId":"79ad463104c7b7afeab11c2046fe7c18d5108ac6","externalIds":{"DOI":"10.1080/10131750485310161","CorpusId":218497666},"title":"Pattern"},{"paperId":"2edf13d0430f4df490e2c0842668b5fa80c1f511","externalIds":{"MAG":"2070726616","DBLP":"journals/tmm/FuGEKG05","DOI":"10.1109/TMM.2005.843341","CorpusId":3043669},"title":"Audio/visual mapping with cross-modal hidden Markov models"},{"paperId":"3326838b53788bdb79d3b37b3ddad6a619ce53b1","externalIds":{"DBLP":"journals/pieee/PotamianosNGGS03","MAG":"2096391593","DOI":"10.1109/JPROC.2003.817150","CorpusId":15512141},"title":"Recent advances in the automatic recognition of audiovisual speech"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f78867834f7f6797ca6396f98edb10aad2a864fb","externalIds":{"DBLP":"journals/pami/MatthewsCBCH02","MAG":"2113814270","DOI":"10.1109/34.982900","CorpusId":599027},"title":"Extraction of Visual Features for Lipreading"},{"paperId":"8943a6703948d71eeb48ce20b1ef94799233d757","externalIds":{"MAG":"2121486117","DBLP":"journals/tmm/DupontL00","DOI":"10.1109/6046.865479","CorpusId":12995675},"title":"Audio-Visual Speech Modeling for Continuous Speech Recognition"},{"paperId":"dae224a9e084e6a0da6703d9da8d2a73e30a6949","externalIds":{"MAG":"2411745509","DBLP":"conf/icip/PotamianosGC98","DOI":"10.1109/ICIP.1998.999008","CorpusId":2412445},"title":"An image transform approach for HMM based automatic lipreading"},{"paperId":"3a78995510cf33edf0ee4265abe23ffdc55986cb","externalIds":{"DBLP":"conf/siggraph/BreglerCS97","MAG":"2147885303","DOI":"10.1145/3596711.3596787","CorpusId":2341707},"title":"Video Rewrite: Driving Visual Speech with Audio"},{"paperId":"21924168415196b125c9cb09578774cdae2d9f5a","externalIds":{"MAG":"2950268498","DBLP":"journals/pami/RistadY98","ArXiv":"cmp-lg/9610005","DOI":"10.1109/34.682181","CorpusId":8844862},"title":"Learning String-Edit Distance"},{"paperId":"eef41ae597a20ea377461d522fd5100da6a7a9b7","externalIds":{"MAG":"2015394094","DOI":"10.1038/264746A0","CorpusId":4171157,"PubMed":"1012311"},"title":"Hearing lips and seeing voices"},{"paperId":"40e77e4fb57cbb20a092f4b2e9e3341d06721bb9","externalIds":{"DBLP":"journals/tmm/ShengZXPL22","DOI":"10.1109/tmm.2021.3102433","CorpusId":240499607},"title":"Adaptive Semantic-Spatio-Temporal Graph Convolutional Network for Lip Reading"},{"paperId":"b2b0628e81e312ad7252c68f0b87593e28ca0919","externalIds":{"DBLP":"journals/access/FenghourCGLX21","DOI":"10.1109/ACCESS.2021.3107946","CorpusId":237446439},"title":"Deep Learning-Based Automated Lip-Reading: A Survey"},{"paperId":"9b946f3e907994d84333bebfc0925be97ac288a6","externalIds":{"DOI":"10.1088/1742-6596/1883/1/012083","CorpusId":235285021},"title":"Lip Reading using Local-Adjacent Feature Extractor and Multi-Level Feature Fusion"},{"paperId":"ce752f69e167db27d4c186f5dfcd064e9b6e7f3b","externalIds":{"MAG":"3099284785","DBLP":"conf/eccv/WangWSYWQHQL20","DOI":"10.1007/978-3-030-58589-1_42","CorpusId":221727985},"title":"MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation"},{"paperId":"658dac82a4f292ecd7c631c6a61408db75f05d07","externalIds":{"MAG":"2921657301","DOI":"10.5040/9781501332647.CH-006","CorpusId":16444779},"title":"The Uncanny Valley"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"82e66c4832386cafcec16b92ac88088ffd1a1bc9","externalIds":{"MAG":"2513140567","CorpusId":16506546},"title":"OpenFace: A general-purpose face recognition library with mobile applications"},{"paperId":"a98331623add655b3b7d3371d77eca8fc792b492","externalIds":{"MAG":"2083792893","DBLP":"journals/spm/Chen01","DOI":"10.1109/79.911195","CorpusId":62764453},"title":"Audiovisual speech processing"},{"paperId":"3aee994b8badc5de8a713992728d35af5c8a93f9","externalIds":{"MAG":"1490506669","DBLP":"phd/dnb/Kirchhoff99","CorpusId":18743488},"title":"Robust speech recognition using articulatory information"},{"paperId":"7f80dd219794f55733d0e6176ff78d9587cda2e7","externalIds":{"MAG":"2120026102","DOI":"10.1787/oif-2007-7-en","CorpusId":204569692},"title":"環境 Environment について"},{"paperId":"261a056f8b21918e8616a429b2df6e1d5d33be41","externalIds":{"CorpusId":9901844},"title":"Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"}]}