{"paperId":"a06d3e9e90008c64c45a0029d580541d5f646771","externalIds":{"DBLP":"journals/corr/abs-2401-00812","ArXiv":"2401.00812","DOI":"10.48550/arXiv.2401.00812","CorpusId":266693465},"title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2401.00812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2277527247","name":"Ke Yang"},{"authorId":"33456794","name":"Jiateng Liu"},{"authorId":"2277421308","name":"John Wu"},{"authorId":"2329787250","name":"Chaoqi Yang"},{"authorId":"51135899","name":"Y. Fung"},{"authorId":"2262396117","name":"Sha Li"},{"authorId":"2277416897","name":"Zixuan Huang"},{"authorId":"2344961610","name":"Xu Cao"},{"authorId":"2144803999","name":"Xingyao Wang"},{"authorId":"2277247982","name":"Yiquan Wang"},{"authorId":"2277409745","name":"Heng Ji"},{"authorId":"2261082008","name":"ChengXiang Zhai"}],"abstract":"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code."}