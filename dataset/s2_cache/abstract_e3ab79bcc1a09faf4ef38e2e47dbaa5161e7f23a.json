{"abstract":"Artificial Neural Networks (ANN) have been shown to be effective for many predictive tasks, such as system identification and reinforcement learning. However, as they have become more ubiquitous, there have been several examples of models exhibiting anthropomorphic bias (e.g. making predictions correlated with race or gender for unrelated tasks) due to over fitting, amplifying and systematizing bias already inherent in training data. To address this problem, we consider a novel regularization approach for deep learning, inspired by the constrained optimization literature, that directly penalizes unwanted disparities in treatment of populations proportionally to their impact on observed bias. Using this method, we can control bias at training time, as opposed to in a pre- or post-processing step; this results in concurrent out-of-sample improvements in both fairness and accuracy for some data sets. Our methods fit well into existing optimization and training approaches and can be easily generalized across network architectures and notions of fairness. We validate our methods empirically on several real world data sets that contain implicit bias. Namely we consider the impact of race on recidivism prediction, gender on income, and wine color on quality. We also consider fairness in a reinforcement learning setting by controlling the dose of Heparin while being certifiably fair with respect to the patientâ€™s insurance provider."}