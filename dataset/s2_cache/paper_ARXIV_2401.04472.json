{"paperId":"88a30d7676108ecafcd8a85c2c60b3d5d1fbde50","externalIds":{"DBLP":"journals/corr/abs-2401-04472","ArXiv":"2401.04472","DOI":"10.24963/ijcai.2024/919","CorpusId":266899949},"title":"A Survey on Efficient Federated Learning Methods for Foundation Model Training","openAccessPdf":{"url":"","status":null,"license":null,"disclaimer":"Notice: Paper or abstract available at https://arxiv.org/abs/2401.04472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."},"authors":[{"authorId":"2220057323","name":"Herbert Woisetschl√§ger"},{"authorId":"2154763233","name":"Alexander Isenko"},{"authorId":"2255363698","name":"Shiqiang Wang"},{"authorId":"39509913","name":"R. Mayer"},{"authorId":"2254179381","name":"Hans-Arno Jacobsen"}],"abstract":"Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training across a multitude of clients. However, new approaches to FL often discuss their contributions involving small deep-learning models only and focus on training full models on clients. In the wake of Foundation Models (FM), the reality is different for many deep learning applications. Typically, FMs have already been pre-trained across a wide variety of tasks and can be fine-tuned to specific downstream tasks over significantly smaller datasets than required for full model training. However, access to such datasets is often challenging. By its design, FL can help to open data silos. With this survey, we introduce a novel taxonomy focused on computational and communication efficiency, the vital elements to make use of FMs in FL systems. We discuss the benefits and drawbacks of parameter-efficient fine-tuning (PEFT) for FL applications, elaborate on the readiness of FL frameworks to work with FMs and provide future research opportunities on how to evaluate generative models in FL as well as the interplay of privacy and PEFT."}