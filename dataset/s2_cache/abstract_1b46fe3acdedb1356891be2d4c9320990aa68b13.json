{"abstract":"Currently, named entity recognition (NER) task can be mainly divided into three types: flat, nested, and discontinuous. In previous work, different types of NER tasks were usually studied separately, which as a result ignored the correlation between different NER tasks and brought inconvenience to their practical applications. Therefore, recently some researchers have begun to focus on how to handle three different types of NER tasks in a unified model by mainly using sequence-to-sequence (Seq2Seq) methods. However, the recognition performance of these Seq2Seq models still lags behind the best models corresponding to various NER tasks. To further improve the recognition performance of Seq2Seq models, this paper proposes a NER unified model based on Multi-Label Sequence Generation (MLSG). Firstly, to avoid the problem of information confusion caused by concatenating multiple unrelated entities, we propose a multi-label and multi-sequence methods in MLSG to output each entity as an independent sequence. Secondly, to address the issue of not utilizing non-entity information in existing Seq2Seq models, MLSG enumerates each word in the sentence and generates its corresponding target sequence, which allows to utilize non-entity information to strengthen the model's local attention on each starting word and thus reduce the learning difficulty of the task. Thirdly, to further take interactions between multiple labels for specific states into consideration, MLSG designs a multi-label attention module to learn the relationships between candidate labels for different states. Experiments conduced on eight popular NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets, demonstrate that our MLSG model achieves current best or near best performance."}