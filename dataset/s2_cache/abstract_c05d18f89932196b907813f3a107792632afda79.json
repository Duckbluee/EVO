{"abstract":"One of the crucial issues in federated learning is how to develop efficient optimization algorithms. Most of the current ones require full device participation and/or impose strong assumptions for convergence. Different from the widely-used gradient descent-based algorithms, in this article, we develop an inexact alternating direction method of multipliers (ADMM), which is both computation- and communication-efficient, capable of combating the stragglersâ€™ effect, and convergent under mild conditions. Furthermore, it has high numerical performance compared with several state-of-the-art algorithms for federated learning."}