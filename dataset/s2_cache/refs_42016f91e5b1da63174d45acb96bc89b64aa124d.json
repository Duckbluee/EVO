{"references":[{"paperId":"3a86c487ee324c7627806622b26be484e7ef616a","externalIds":{"DBLP":"journals/corr/abs-2405-00664","ArXiv":"2405.00664","DOI":"10.48550/arXiv.2405.00664","CorpusId":269484228},"title":"Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3"},{"paperId":"5664f7f52264ff656faa3671d83e2009f4f390fd","externalIds":{"ArXiv":"2404.03646","DBLP":"journals/corr/abs-2404-03646","DOI":"10.48550/arXiv.2404.03646","CorpusId":268889738},"title":"Locating and Editing Factual Associations in Mamba"},{"paperId":"65d9e725278da08fa29144b4f9a80d7d31f1c27f","externalIds":{"DBLP":"conf/emnlp/GuptaSA24","ArXiv":"2403.14236","DOI":"10.48550/arXiv.2403.14236","CorpusId":268553573},"title":"A Unified Framework for Model Editing"},{"paperId":"33c8910107f3fcb17d140cc88554652508ae3674","externalIds":{"ArXiv":"2403.14472","DBLP":"conf/acl/Wang0XXDYZY0C24","DOI":"10.48550/arXiv.2403.14472","CorpusId":268553537},"title":"Detoxifying Large Language Models via Knowledge Editing"},{"paperId":"77744ab3d5d7570bdc2a63f17eaf3e6a1b50a1b3","externalIds":{"DBLP":"journals/corr/abs-2402-11905","ArXiv":"2402.11905","DOI":"10.48550/arXiv.2402.11905","CorpusId":267751255},"title":"Learning to Edit: Aligning LLMs with Knowledge Editing"},{"paperId":"b6d2f5c7c488d3de265559e005dec13d65ef7607","externalIds":{"DBLP":"conf/acl/GangadharS24","ArXiv":"2402.11078","DOI":"10.18653/v1/2024.findings-acl.352","CorpusId":267750254},"title":"Model Editing by Standard Fine-Tuning"},{"paperId":"6acd1576ed0abfb034035cb52bd34bf43abcfea3","externalIds":{"ArXiv":"2402.08631","DBLP":"journals/corr/abs-2402-08631","DOI":"10.48550/arXiv.2402.08631","CorpusId":267636544},"title":"Knowledge Editing on Black-box Large Language Models"},{"paperId":"574da0a8844c0e553b864386d31998516525cfe8","externalIds":{"ArXiv":"2401.10471","DBLP":"journals/corr/abs-2401-10471","DOI":"10.48550/arXiv.2401.10471","CorpusId":267060897},"title":"DeepEdit: Knowledge Editing as Decoding with Constraints"},{"paperId":"d2b48bcee27974c10025422b76c8943ed524722c","externalIds":{"ACL":"2024.emnlp-main.934","ArXiv":"2401.04700","DBLP":"conf/emnlp/GuXMLLCP24","DOI":"10.18653/v1/2024.emnlp-main.934","CorpusId":266899568},"title":"Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue"},{"paperId":"74f20c57ac323f5fdd6acbab558eb704253ef4e6","externalIds":{"DBLP":"conf/acl/0002ZHL0024","ArXiv":"2312.15194","DOI":"10.48550/arXiv.2312.15194","CorpusId":266551175},"title":"PokeMQA: Programmable knowledge editing for Multi-hop Question Answering"},{"paperId":"5e2ce535acf3a2ce8fd36d0f97e36c7777f65504","externalIds":{"DBLP":"conf/acl/WangHB24","ArXiv":"2312.13040","DOI":"10.48550/arXiv.2312.13040","CorpusId":266374418},"title":"Retrieval-augmented Multilingual Knowledge Editing"},{"paperId":"0b9eea26e5818981c173d03209adb472b8e3a38e","externalIds":{"ArXiv":"2312.11795","DBLP":"journals/corr/abs-2312-11795","DOI":"10.48550/arXiv.2312.11795","CorpusId":266362196},"title":"MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA"},{"paperId":"46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5","externalIds":{"ArXiv":"2312.10997","DBLP":"journals/corr/abs-2312-10997","CorpusId":266359151},"title":"Retrieval-Augmented Generation for Large Language Models: A Survey"},{"paperId":"6abd2c18bc5c9441465ae094eb1e93c25b3732c3","externalIds":{"DBLP":"conf/emnlp/PinterE23","ArXiv":"2310.11958","DOI":"10.48550/arXiv.2310.11958","CorpusId":264288692},"title":"Emptying the Ocean with a Spoon: Should We Edit Models?"},{"paperId":"1256cd7ecea1c312b854c6673fab0c2908276d8a","externalIds":{"ArXiv":"2310.10322","DBLP":"journals/corr/abs-2310-10322","DOI":"10.48550/arXiv.2310.10322","CorpusId":264146289},"title":"Untying the Reversal Curse via Bidirectional Language Model Editing"},{"paperId":"e8d513bc7554a83161f2fb26c8299b471581cdb6","externalIds":{"DBLP":"conf/emnlp/0008TL0WC023","ArXiv":"2310.08475","DOI":"10.48550/arXiv.2310.08475","CorpusId":263908997},"title":"Can We Edit Multimodal Large Language Models?"},{"paperId":"838cd69a0b6c9c244a6eebb0f4742c0625132de6","externalIds":{"DBLP":"journals/corr/abs-2308-08747","ArXiv":"2308.08747","DOI":"10.1109/TASLPRO.2025.3606231","CorpusId":261031244},"title":"An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-Tuning"},{"paperId":"a95ee02fd85a3734bf19bb4bbc6ef3ea22c96cc9","externalIds":{"DBLP":"journals/corr/abs-2308-08742","ArXiv":"2308.08742","DOI":"10.48550/arXiv.2308.08742","CorpusId":261030625},"title":"PMET: Precise Model Editing in a Transformer"},{"paperId":"1fdf449c96fbac0789cf8dfae15b788905407fd3","externalIds":{"DBLP":"journals/tacl/CohenBYGG24","ACL":"2024.tacl-1.16","ArXiv":"2307.12976","DOI":"10.1162/tacl_a_00644","CorpusId":260356612},"title":"Evaluating the Ripple Effects of Knowledge Editing in Language Models"},{"paperId":"a35f1315e91513ff0bec0c488fe175214fd9636c","externalIds":{"DBLP":"journals/corr/abs-2307-02046","ArXiv":"2307.02046","DOI":"10.1109/TKDE.2024.3392335","CorpusId":259342486},"title":"Recommender Systems in the Era of Large Language Models (LLMs)"},{"paperId":"19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e","externalIds":{"DBLP":"conf/aaai/00010LYHLW24","ArXiv":"2306.17492","DOI":"10.48550/arXiv.2306.17492","CorpusId":259308873},"title":"Preference Ranking Optimization for Human Alignment"},{"paperId":"22ae20f20d0b4e6451ae41cc76e58a9221e90df9","externalIds":{"DBLP":"journals/corr/abs-2306-03819","ArXiv":"2306.03819","DOI":"10.48550/arXiv.2306.03819","CorpusId":259088549},"title":"LEACE: Perfect linear concept erasure in closed form"},{"paperId":"3dcf2db20082b480c6c091eea025465cc4fe57a6","externalIds":{"ArXiv":"2306.01941","DBLP":"journals/corr/abs-2306-01941","DOI":"10.48550/arXiv.2306.01941","CorpusId":259075521},"title":"AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap"},{"paperId":"38d64919ba526868a850a0e5f6239d4c474b7e7e","externalIds":{"DBLP":"journals/corr/abs-2305-17926","ArXiv":"2305.17926","DOI":"10.48550/arXiv.2305.17926","CorpusId":258960339},"title":"Large Language Models are not Fair Evaluators"},{"paperId":"56e952fd463accff09cf2e35432aaabd7c7c57f3","externalIds":{"DBLP":"conf/emnlp/ZhongWMPC23","ArXiv":"2305.14795","DOI":"10.48550/arXiv.2305.14795","CorpusId":258865984},"title":"MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions"},{"paperId":"2b72888cc3ff048038f6011b8e3d89ba106540b6","externalIds":{"ArXiv":"2305.14956","DBLP":"conf/emnlp/GuptaMS00WT23","DOI":"10.18653/v1/2023.emnlp-main.511","CorpusId":258865393},"title":"Editing Common Sense in Transformers"},{"paperId":"f5c73d9e6641b018b633690102121f5605d34fb0","externalIds":{"DBLP":"conf/emnlp/YaoWT0LDC023","ArXiv":"2305.13172","DOI":"10.48550/arXiv.2305.13172","CorpusId":258833129},"title":"Editing Large Language Models: Problems, Methods, and Opportunities"},{"paperId":"ff2a0fb125e7f03428420230c6ecbeafd4cf07a8","externalIds":{"DBLP":"journals/corr/abs-2305-12740","ArXiv":"2305.12740","DOI":"10.48550/arXiv.2305.12740","CorpusId":258832407},"title":"Can We Edit Factual Knowledge by In-Context Learning?"},{"paperId":"cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa","externalIds":{"DBLP":"journals/corr/abs-2305-14387","ArXiv":"2305.14387","DOI":"10.48550/arXiv.2305.14387","CorpusId":258865545},"title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"},{"paperId":"03055978e278960de9fbb5c648b1779ef9f26cd1","externalIds":{"ArXiv":"2305.01937","DBLP":"conf/acl/ChiangL23","ACL":"2023.acl-long.870","DOI":"10.48550/arXiv.2305.01937","CorpusId":258461287},"title":"Can Large Language Models Be an Alternative to Human Evaluations?"},{"paperId":"56da914761e445a24481629cfc116336a0aec978","externalIds":{"DBLP":"journals/corr/abs-2305-01651","ArXiv":"2305.01651","ACL":"2023.acl-long.300","DOI":"10.48550/arXiv.2305.01651","CorpusId":258437155},"title":"Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge"},{"paperId":"fdb117c68332d23cb1bd57e3cc36b8c9cfbdcbf7","externalIds":{"DBLP":"conf/chi/ZhouZLPC23","DOI":"10.1145/3544548.3581318","CorpusId":257633591},"title":"Synthetic Lies: Understanding AI-Generated Misinformation and Evaluating Algorithmic and Human Solutions"},{"paperId":"a206a0c96d6076c6ab081288b0c2c95d3c7efd64","externalIds":{"ArXiv":"2304.00740","CorpusId":258833476},"title":"Inspecting and Editing Knowledge Representations in Language Models"},{"paperId":"a98862ffe4c18634a67a3df8a965a35e5e0d7ec8","externalIds":{"DOI":"10.1016/j.lindif.2023.102274","CorpusId":257445349},"title":"ChatGPT for good? On opportunities and challenges of large language models for education"},{"paperId":"628d23e976ffc7f65dab68ffc5b47da0cb6978ff","externalIds":{"PubMedCentral":"10032023","DOI":"10.1186/s13054-023-04393-x","CorpusId":257641567,"PubMed":"36945051"},"title":"Large language models and the perils of their hallucinations"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","externalIds":{"DBLP":"journals/corr/abs-2302-13971","ArXiv":"2302.13971","CorpusId":257219404},"title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"e5c72b92c48d68594b290c84a8904da7c8335554","externalIds":{"ArXiv":"2302.12813","DBLP":"journals/corr/abs-2302-12813","DOI":"10.48550/arXiv.2302.12813","CorpusId":257205781},"title":"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback"},{"paperId":"cb3125e4f63f3d058a2a39270ecb585e86c3d1ff","externalIds":{"DBLP":"journals/corr/abs-2302-02676","ArXiv":"2302.02676","DOI":"10.48550/arXiv.2302.02676","CorpusId":257038005},"title":"Chain of Hindsight Aligns Language Models with Feedback"},{"paperId":"8bbdf825e6ccd197adfff5ebeecc9a5a5210e02f","externalIds":{"ArXiv":"2301.10405","DBLP":"journals/corr/abs-2301-10405","DOI":"10.48550/arXiv.2301.10405","CorpusId":256231427},"title":"Editing Language Model-based Knowledge Graph Embeddings"},{"paperId":"a9be51698e7c2247853b7b6f1f70fc4d6d7ef605","externalIds":{"DBLP":"journals/corr/abs-2301-09785","ArXiv":"2301.09785","DOI":"10.48550/arXiv.2301.09785","CorpusId":256194369},"title":"Transformer-Patcher: One Mistake worth One Neuron"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","externalIds":{"DBLP":"journals/corr/abs-2212-10560","ArXiv":"2212.10560","ACL":"2023.acl-long.754","DOI":"10.48550/arXiv.2212.10560","CorpusId":254877310},"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","externalIds":{"ACL":"2023.acl-long.806","ArXiv":"2212.09689","DBLP":"conf/acl/HonovichSLS23","DOI":"10.48550/arXiv.2212.09689","CorpusId":254853659},"title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"},{"paperId":"de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a","externalIds":{"DBLP":"journals/corr/abs-2211-15006","ArXiv":"2211.15006","DOI":"10.48550/arXiv.2211.15006","CorpusId":254043997},"title":"Fine-tuning language models to find agreement among humans with diverse preferences"},{"paperId":"560b1bc012588731b26748e33236570df777baa0","externalIds":{"ArXiv":"2211.11031","DBLP":"conf/nips/HartvigsenSPKG23","DOI":"10.48550/arXiv.2211.11031","CorpusId":253735429},"title":"Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors"},{"paperId":"ee8de585183763ff64cb3c81ecda2fc75fa81507","externalIds":{"ArXiv":"2211.05110","DBLP":"journals/corr/abs-2211-05110","DOI":"10.48550/arXiv.2211.05110","CorpusId":253420654},"title":"Large Language Models with Controllable Working Memory"},{"paperId":"61ddf932488405ab1c7b275460d2b3c5dfa274a0","externalIds":{"DBLP":"journals/corr/abs-2211-03318","ArXiv":"2211.03318","ACL":"2022.emnlp-main.797","DOI":"10.48550/arXiv.2211.03318","CorpusId":249147353},"title":"Fixing Model Bugs with Natural Language Patches"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","externalIds":{"DBLP":"journals/corr/abs-2210-11416","ArXiv":"2210.11416","DOI":"10.48550/arXiv.2210.11416","CorpusId":253018554},"title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413","externalIds":{"DBLP":"conf/iclr/MengSABB23","ArXiv":"2210.07229","DOI":"10.48550/arXiv.2210.07229","CorpusId":252873467},"title":"Mass-Editing Memory in a Transformer"},{"paperId":"7471cb40a33e9d971a922b5dff5ca9b4a73ca609","externalIds":{"DBLP":"journals/corr/abs-2210-03329","ArXiv":"2210.03329","DOI":"10.48550/arXiv.2210.03329","CorpusId":252762125},"title":"Calibrating Factual Knowledge in Pretrained Language Models"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","externalIds":{"DBLP":"journals/corr/abs-2209-14375","ArXiv":"2209.14375","DOI":"10.48550/arXiv.2209.14375","CorpusId":252596089},"title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"86d0d3855f94105e25d81cab9f3d269c6062a9c4","externalIds":{"DBLP":"conf/iclr/SuKWSWX0OZS023","ArXiv":"2209.01975","DOI":"10.48550/arXiv.2209.01975","CorpusId":252089424},"title":"Selective Annotation Makes Language Models Better Few-Shot Learners"},{"paperId":"0dc90d5c311de8bc11554e87b059fbb7c27c88ac","externalIds":{"DBLP":"journals/corr/abs-2209-00731","ArXiv":"2209.00731","DOI":"10.1007/s13347-023-00606-x","CorpusId":252070540},"title":"In Conversation with Artificial Intelligence: Aligning language Models with Human Values"},{"paperId":"17bcb1edbe068e8fe6a97da552c70a77a15bbce7","externalIds":{"DBLP":"journals/corr/abs-2209-07858","ArXiv":"2209.07858","DOI":"10.48550/arXiv.2209.07858","CorpusId":252355458},"title":"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"},{"paperId":"422d8c989adeb904563d0c96d5038f6c8596fa99","externalIds":{"ArXiv":"2208.00399","DBLP":"conf/nlpcc/DaiJDLS23","DOI":"10.48550/arXiv.2208.00399","CorpusId":251223709},"title":"Neural Knowledge Bank for Pretrained Transformers"},{"paperId":"0adb7ab0ec2e92315877aa75560c5dc68dd914eb","externalIds":{"ArXiv":"2207.04806","DBLP":"conf/nips/TannoPNL22","DOI":"10.48550/arXiv.2207.04806","CorpusId":250425883},"title":"Repairing Neural Networks by Leaving the Right Past Behind"},{"paperId":"1d650f1afd45c59ff907396fe8b678595dcb85ea","externalIds":{"DBLP":"conf/icml/MitchellLBMF22","ArXiv":"2206.06520","CorpusId":249642147},"title":"Memory-Based Model Editing at Scale"},{"paperId":"d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3","externalIds":{"ACL":"2022.emnlp-main.410","ArXiv":"2205.12393","DBLP":"conf/emnlp/ScialomCM22","DOI":"10.18653/v1/2022.emnlp-main.410","CorpusId":252815378},"title":"Fine-tuned Language Models are Continual Learners"},{"paperId":"03d19fde1df67c7ea8dedc750dcd3a6291032577","externalIds":{"DBLP":"journals/corr/abs-2205-11005","ArXiv":"2205.11005","DOI":"10.48550/arXiv.2205.11005","CorpusId":248986195},"title":"Parameter-Efficient Sparsity for Large Language Models Fine-Tuning"},{"paperId":"7cdaa08890895e1ad92afb5fad429690ad7b1dac","externalIds":{"DBLP":"conf/nips/LiuTMMHBR22","ArXiv":"2205.05638","DOI":"10.48550/arXiv.2205.05638","CorpusId":248693283},"title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"},{"paperId":"00642bcfa319e0da3e2f56b59e9e0614fffd02de","externalIds":{"ArXiv":"2205.02832","DBLP":"journals/corr/abs-2205-02832","DOI":"10.48550/arXiv.2205.02832","CorpusId":248525074},"title":"Entity Cloze By Date: What LMs Know About Unseen Entities"},{"paperId":"bd742d976aa0a7dfa91eb3e4a9136a00d763efd1","externalIds":{"ACL":"2022.findings-acl.37","DBLP":"journals/corr/abs-2204-12785","ArXiv":"2204.12785","DOI":"10.48550/arXiv.2204.12785","CorpusId":248405835},"title":"Plug-and-Play Adaptation for Continuously-updated QA"},{"paperId":"cf36236015c9f93f15bfafbf282f69e08bdc9c16","externalIds":{"DBLP":"conf/emnlp/GevaCWG22","ArXiv":"2203.14680","ACL":"2022.emnlp-main.3","DOI":"10.48550/arXiv.2203.14680","CorpusId":247762385},"title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"},{"paperId":"8666f9f379389a5dff31e72fb0f992a37763ba41","externalIds":{"ArXiv":"2203.11147","DBLP":"journals/corr/abs-2203-11147","DOI":"10.48550/arXiv.2203.11147","CorpusId":247594830},"title":"Teaching language models to support answers with verified quotes"},{"paperId":"4003193ef9fa5d408fcba8f9b1893b2be321283a","externalIds":{"DBLP":"conf/acl/ChalkidisP0TSS22","ArXiv":"2203.07228","ACL":"2022.acl-long.301","DOI":"10.48550/arXiv.2203.07228","CorpusId":247447641},"title":"FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","externalIds":{"DBLP":"conf/nips/MengBAB22","ArXiv":"2202.05262","CorpusId":255825985},"title":"Locating and Editing Factual Associations in GPT"},{"paperId":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","externalIds":{"DBLP":"journals/corr/abs-2202-03286","ACL":"2022.emnlp-main.225","ArXiv":"2202.03286","DOI":"10.18653/v1/2022.emnlp-main.225","CorpusId":246634238},"title":"Red Teaming Language Models with Language Models"},{"paperId":"46258ab268e3d940e72ca9370899924f76e7ef38","externalIds":{"DBLP":"journals/jbd/ChicheY22","DOI":"10.1186/s40537-022-00561-y","CorpusId":246281655},"title":"Part of speech tagging: a systematic review of deep learning and machine learning approaches"},{"paperId":"e2a193a191290fa60ac39748eeb1118ff44b9129","externalIds":{"DBLP":"journals/corr/abs-2112-08634","ArXiv":"2112.08634","ACL":"2022.naacl-main.269","DOI":"10.18653/v1/2022.naacl-main.269","CorpusId":245218924},"title":"FRUIT: Faithfully Reflecting Updated Information in Text"},{"paperId":"6296aa7cab06eaf058f7291040b320b5a83c0091","externalIds":{"DBLP":"journals/corr/abs-2203-00667","ArXiv":"2203.00667","DOI":"10.1109/ICCCNT56998.2023.10306417","CorpusId":1033682},"title":"Generative Adversarial Networks"},{"paperId":"8cda255351168fab95c7b59d12d24590ad57a6ac","externalIds":{"DBLP":"journals/corr/abs-2112-03572","ArXiv":"2112.03572","DOI":"10.37896/jxu15.4/014","CorpusId":242823477},"title":"Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices"},{"paperId":"ec4f3b701c6d97fc73d5afdede41a6510092320f","externalIds":{"ArXiv":"2112.01008","DBLP":"conf/nips/SanturkarTEBTM21","CorpusId":244799629},"title":"Editing a classifier by rewriting its prediction rules"},{"paperId":"4a247cbfca9dcf91e2da24e6d2d84601a9041a8f","externalIds":{"DBLP":"journals/corr/abs-2111-13654","ArXiv":"2111.13654","CorpusId":244709666},"title":"Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","externalIds":{"ArXiv":"2111.02114","DBLP":"journals/corr/abs-2111-02114","CorpusId":241033103},"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","externalIds":{"ArXiv":"2111.01243","DBLP":"journals/corr/abs-2111-01243","DOI":"10.1145/3605943","CorpusId":240420063},"title":"Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"},{"paperId":"d83c7aa12420d5d035f43d7737bfa6893e9e2c61","externalIds":{"ArXiv":"2110.07867","CorpusId":248722227},"title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning"},{"paperId":"8e3b613f8a6d775a5bf97aa73328a2f4795dd407","externalIds":{"ArXiv":"2109.06157","DBLP":"conf/emnlp/ZhangC21","ACL":"2021.emnlp-main.586","DOI":"10.18653/v1/2021.emnlp-main.586","CorpusId":237491751},"title":"SituatedQA: Incorporating Extra-Linguistic Contexts into QA"},{"paperId":"9289826beb6206eeaf500105f7329d6d5a495d8a","externalIds":{"DBLP":"journals/corr/abs-2109-01903","ArXiv":"2109.01903","DOI":"10.1109/CVPR52688.2022.00780","CorpusId":237420687},"title":"Robust fine-tuning of zero-shot models"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","externalIds":{"DBLP":"journals/corr/abs-2109-01652","ArXiv":"2109.01652","CorpusId":237416585},"title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","externalIds":{"DBLP":"journals/corr/abs-2107-03374","ArXiv":"2107.03374","CorpusId":235755472},"title":"Evaluating Large Language Models Trained on Code"},{"paperId":"339b2b711fb5b228d097b03ebc3e62a521779235","externalIds":{"DBLP":"conf/acl/ZakenGR22","ACL":"2022.acl-short.1","ArXiv":"2106.10199","DOI":"10.18653/v1/2022.acl-short.1","CorpusId":231672601},"title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"2c871df72c52b58f05447fcb3afc838168d94505","externalIds":{"ArXiv":"2104.08696","DBLP":"journals/corr/abs-2104-08696","ACL":"2022.acl-long.581","DOI":"10.18653/v1/2022.acl-long.581","CorpusId":233296761},"title":"Knowledge Neurons in Pretrained Transformers"},{"paperId":"420c897bc67e6f438db522d919d925df1a10aa8c","externalIds":{"DBLP":"journals/jbi/KalyanRS22","ArXiv":"2105.00827","DOI":"10.1016/j.jbi.2021.103982","CorpusId":233481730,"PubMed":"34974190"},"title":"AMMU: A survey of transformer-based biomedical pretrained language models"},{"paperId":"240b0caabb415578bdea4da7d0a32bdff2e8163f","externalIds":{"DBLP":"journals/corr/abs-2104-08164","ArXiv":"2104.08164","ACL":"2021.emnlp-main.522","DOI":"10.18653/v1/2021.emnlp-main.522","CorpusId":233289412},"title":"Editing Factual Knowledge in Language Models"},{"paperId":"eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a","externalIds":{"ArXiv":"2103.08541","DBLP":"conf/naacl/SchusterFB21","MAG":"3170180819","ACL":"2021.naacl-main.52","DOI":"10.18653/V1/2021.NAACL-MAIN.52","CorpusId":232233599},"title":"Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence"},{"paperId":"73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","externalIds":{"DBLP":"journals/corr/abs-2102-01017","ArXiv":"2102.01017","DOI":"10.1162/tacl_a_00410","CorpusId":231740560},"title":"Measuring and Improving Consistency in Pretrained Language Models"},{"paperId":"4c2733d191e347753bb28afa46a1c55c65e085be","externalIds":{"DBLP":"conf/aies/AbidF021","ArXiv":"2101.05783","DOI":"10.1145/3461702.3462624","CorpusId":231603388},"title":"Persistent Anti-Muslim Bias in Large Language Models"},{"paperId":"4a54d58a4b20e4f3af25cea3c188a12082a95e02","externalIds":{"DBLP":"conf/emnlp/GevaSBL21","ACL":"2021.emnlp-main.446","ArXiv":"2012.14913","DOI":"10.18653/v1/2021.emnlp-main.446","CorpusId":229923720},"title":"Transformer Feed-Forward Layers Are Key-Value Memories"},{"paperId":"e54ffc76d805c48660bb0fd20019ca82ac94ba0d","externalIds":{"ArXiv":"2012.13255","DBLP":"conf/acl/AghajanyanGZ20","ACL":"2021.acl-long.568","DOI":"10.18653/v1/2021.acl-long.568","CorpusId":229371560},"title":"Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"},{"paperId":"5270b626feb66c8c363e93ba6608daae93c5003b","externalIds":{"DBLP":"journals/corr/abs-2012-00363","ArXiv":"2012.00363","MAG":"3107969673","CorpusId":227238659},"title":"Modifying Memories in Transformer Models"},{"paperId":"b68b2e81ae2de647394ec05ee62ecf108bf2b50a","externalIds":{"MAG":"3098267758","ACL":"2020.emnlp-main.346","DBLP":"journals/corr/abs-2010-15980","ArXiv":"2010.15980","DOI":"10.18653/v1/2020.emnlp-main.346","CorpusId":226222232},"title":"Eliciting Knowledge from Language Models Using Automatically Generated Prompts"},{"paperId":"332c44793b70776b9b966128c52e694222b1ab73","externalIds":{"ArXiv":"2010.03522","DBLP":"journals/corr/abs-2010-03522","MAG":"3092053846","DOI":"10.1007/s10462-021-10004-4","CorpusId":222177134},"title":"A survey of deep meta-learning"},{"paperId":"3af229f3da1bd24378fd8d76c88fd393b4464058","externalIds":{"MAG":"3083182073","DBLP":"journals/corr/abs-2009-02252","ArXiv":"2009.02252","ACL":"2021.naacl-main.200","DOI":"10.18653/V1/2021.NAACL-MAIN.200","CorpusId":221507798},"title":"KILT: a Benchmark for Knowledge Intensive Language Tasks"},{"paperId":"bfe8c0617ca61496e224380f896c0990fdbf542d","externalIds":{"MAG":"3001276927","DOI":"10.2139/ssrn.3489963","CorpusId":213724433},"title":"More than a Feeling: Benchmarks for Sentiment Analysis Accuracy"},{"paperId":"51bf7a3aee6b1f61b902625f6badffedf200d31a","externalIds":{"ArXiv":"2007.15646","DBLP":"conf/eccv/BauLWZT20","MAG":"3046090006","DOI":"10.1007/978-3-030-58452-8_21","CorpusId":220871229},"title":"Rewriting a Deep Generative Model"},{"paperId":"5d4cbdd2172039b84b8628f1a2f77b83ba1fa551","externalIds":{"DBLP":"journals/bib/FeiRZJL21","MAG":"3037979089","DOI":"10.1093/bib/bbaa110","CorpusId":220120928,"PubMed":"32591802"},"title":"Enriching contextualized language model from knowledge graph for biomedical information extraction"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"a378c132ccd4c39186eb7edbabf30687ba9763fc","externalIds":{"DBLP":"journals/corr/abs-2005-05710","PubMedCentral":"7893249","ArXiv":"2005.05710","MAG":"3024009241","DOI":"10.1016/j.osnem.2020.100104","CorpusId":218596117,"PubMed":"33623836"},"title":"An exploratory study of COVID-19 misinformation on Twitter"},{"paperId":"9b539d413393047b28bb7be9b195f142aaf7a80e","externalIds":{"ACL":"2021.eacl-main.24","MAG":"3023786569","DBLP":"conf/eacl/RollerDGJWLXOSB21","ArXiv":"2004.13637","DOI":"10.18653/v1/2021.eacl-main.24","CorpusId":216562425},"title":"Recipes for Building an Open-Domain Chatbot"},{"paperId":"f2f3c83db919a2429c4fcad2d0a0ed4e5294354a","externalIds":{"MAG":"3104215796","ArXiv":"2004.12651","ACL":"2020.emnlp-main.634","DBLP":"conf/emnlp/ChenHCCLY20","DOI":"10.18653/v1/2020.emnlp-main.634","CorpusId":216553067},"title":"Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting"},{"paperId":"020bb2ba5f3923858cd6882ba5c5a44ea8041ab6","externalIds":{"MAG":"3015606043","DBLP":"journals/pami/HospedalesAMS22","ArXiv":"2004.05439","DOI":"10.1109/TPAMI.2021.3079209","CorpusId":215744839,"PubMed":"33974543"},"title":"Meta-Learning in Neural Networks: A Survey"},{"paperId":"e165b379f983152874299e0f5a6e0c9596c9a3e8","externalIds":{"DBLP":"conf/iclr/SinitsinPPPB20","ArXiv":"2004.00345","MAG":"3014924543","CorpusId":213938729},"title":"Editable Neural Networks"},{"paperId":"4f03e69963b9649950ba29ae864a0de8c14f1f86","externalIds":{"DBLP":"conf/acl/WangTDWHJCJZ21","ArXiv":"2002.01808","MAG":"3005441132","ACL":"2021.findings-acl.121","DOI":"10.18653/v1/2021.findings-acl.121","CorpusId":211031933},"title":"K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"},{"paperId":"4d08dcd2cc1e9691defe664a10f021424a896a1e","externalIds":{"MAG":"2992693294","DBLP":"journals/corr/abs-1912-02047","ArXiv":"1912.02047","DOI":"10.1613/JAIR.1.12007","CorpusId":208617582},"title":"Neural Machine Translation: A Review"},{"paperId":"a75649771901a4881b44c0ceafa469fcc6e6f968","externalIds":{"MAG":"3044438666","ArXiv":"1911.12543","DBLP":"journals/tacl/JiangXAN20","DOI":"10.1162/tacl_a_00324","CorpusId":208513249},"title":"How Can We Know What Language Models Know?"},{"paperId":"18d026ec5d0eebd17ee2c762da89540c0b3d7bde","externalIds":{"ArXiv":"1911.02685","MAG":"3041133507","DBLP":"journals/pieee/ZhuangQDXZZXH21","DOI":"10.1109/JPROC.2020.3004555","CorpusId":207847753},"title":"A Comprehensive Survey on Transfer Learning"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","externalIds":{"MAG":"2982399380","ACL":"2020.acl-main.703","DBLP":"journals/corr/abs-1910-13461","ArXiv":"1910.13461","DOI":"10.18653/v1/2020.acl-main.703","CorpusId":204960716},"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a15950dc71079285a4eaf195de5aadd87c41b40","externalIds":{"MAG":"2973379954","DBLP":"journals/corr/abs-1909-08593","ArXiv":"1909.08593","CorpusId":202660943},"title":"Fine-Tuning Language Models from Human Preferences"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","externalIds":{"DBLP":"journals/corr/abs-1909-01066","MAG":"2996758945","ArXiv":"1909.01066","ACL":"D19-1250","DOI":"10.18653/v1/D19-1250","CorpusId":202539551},"title":"Language Models as Knowledge Bases?"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","externalIds":{"DBLP":"journals/corr/abs-1908-10084","MAG":"2970641574","ArXiv":"1908.10084","ACL":"D19-1410","DOI":"10.18653/v1/D19-1410","CorpusId":201646309},"title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","externalIds":{"ACL":"Q19-1026","MAG":"2912924812","DBLP":"journals/tacl/KwiatkowskiPRCP19","DOI":"10.1162/tacl_a_00276","CorpusId":86611921},"title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","externalIds":{"DBLP":"journals/corr/abs-1907-11692","ArXiv":"1907.11692","MAG":"2965373594","CorpusId":198953378},"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"b499228aa74b59be32711c3926e44de208d6b636","externalIds":{"MAG":"2971310675","DBLP":"conf/nips/HerdadeKBS19","ArXiv":"1906.05963","CorpusId":189898359},"title":"Image Captioning: Transforming Objects into Words"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","externalIds":{"MAG":"2971274815","ArXiv":"1905.03197","DBLP":"journals/corr/abs-1905-03197","CorpusId":147704286},"title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"d593a5830a7e7d84443473c3912b59165056d45a","externalIds":{"MAG":"2953099775","ArXiv":"1903.05485","DBLP":"journals/corr/abs-1903-05485","DOI":"10.1007/978-3-030-21348-0_30","CorpusId":76663467},"title":"MMKG: Multi-Modal Knowledge Graphs"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","externalIds":{"ACL":"D19-1371","DBLP":"conf/emnlp/BeltagyLC19","MAG":"2973154071","ArXiv":"1903.10676","DOI":"10.18653/v1/D19-1371","CorpusId":202558505},"title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"c4afa2b3eda95a1194313394901e0e96e24cefaa","externalIds":{"DBLP":"conf/fat/De-ArteagaRWCBC19","MAG":"3105536512","ArXiv":"1901.09451","DOI":"10.1145/3287560.3287572","CorpusId":58006082},"title":"Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting"},{"paperId":"cc7827a17a7759a04aa389290d1a874db56e85e5","externalIds":{"DBLP":"journals/corr/abs-1810-03548","MAG":"2895531857","ArXiv":"1810.03548","DOI":"10.1007/978-3-030-05318-5_2","CorpusId":52938664},"title":"Meta-Learning: A Survey"},{"paperId":"a75869d69cc86f501939c237ae4711aa2885f6a6","externalIds":{"MAG":"2952518244","DBLP":"conf/emnlp/GuWCLC18","ArXiv":"1808.08437","ACL":"D18-1398","DOI":"10.18653/v1/D18-1398","CorpusId":52100101},"title":"Meta-Learning for Low-Resource Neural Machine Translation"},{"paperId":"11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25","externalIds":{"MAG":"2785611959","DBLP":"conf/lrec/ElSaharVRGHLS18","ACL":"L18-1544","CorpusId":4612975},"title":"T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"},{"paperId":"b1d24e8e08435b7c52335485a0d635abf9bc604c","externalIds":{"MAG":"2789566302","ACL":"N18-1074","ArXiv":"1803.05355","DBLP":"conf/naacl/ThorneVCM18","DOI":"10.18653/v1/N18-1074","CorpusId":4711425},"title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification"},{"paperId":"d55d1d035e91220335edff0fe8f5d249d8c4a00b","externalIds":{"MAG":"2963477238","DBLP":"conf/iclr/LiFLY18","ArXiv":"1804.08838","CorpusId":13739955},"title":"Measuring the Intrinsic Dimension of Objective Landscapes"},{"paperId":"fa025e5d117929361bcf798437957762eb5bb6d4","externalIds":{"DBLP":"conf/conll/LevySCZ17","MAG":"2962881743","ACL":"K17-1034","ArXiv":"1706.04115","DOI":"10.18653/v1/K17-1034","CorpusId":793385},"title":"Zero-Shot Relation Extraction via Reading Comprehension"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","externalIds":{"MAG":"2604763608","DBLP":"journals/corr/FinnAL17","ArXiv":"1703.03400","CorpusId":6719686},"title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"5ba2218b708ca64ab556e39d5997202e012717d5","externalIds":{"MAG":"2593116425","DBLP":"conf/icassp/GemmekeEFJLMPR17","DOI":"10.1109/ICASSP.2017.7952261","CorpusId":21519176},"title":"Audio Set: An ontology and human-labeled dataset for audio events"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","externalIds":{"MAG":"3016211260","DBLP":"conf/cvpr/GoyalKSBP17","ArXiv":"1612.00837","DOI":"10.1007/s11263-018-1116-0","CorpusId":8081284},"title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"29c887794eed2ca9462638ff853e6fe1ab91d5d8","externalIds":{"MAG":"2753160622","DBLP":"conf/iclr/RaviL17","CorpusId":67413369},"title":"Optimization as a Model for Few-Shot Learning"},{"paperId":"563783de03452683a9206e85fe6d661714436686","externalIds":{"MAG":"2884340900","ArXiv":"1609.09106","DBLP":"conf/iclr/HaDL17","CorpusId":208981547},"title":"HyperNetworks"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","externalIds":{"DBLP":"journals/corr/AntolALMBZP15","MAG":"1933349210","ArXiv":"1505.00468","DOI":"10.1007/s11263-016-0966-6","CorpusId":3180429},"title":"VQA: Visual Question Answering"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","externalIds":{"ArXiv":"1504.00325","DBLP":"journals/corr/ChenFLVGDZ15","MAG":"1889081078","CorpusId":2210455},"title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"18c7fb55ff796db5c5a604e0ca44b6baaeb12239","externalIds":{"MAG":"2950052605","DBLP":"journals/corr/LeSS14","ArXiv":"1408.3060","CorpusId":15634106},"title":"Fastfood: Approximate Kernel Expansions in Loglinear Time"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","externalIds":{"DBLP":"conf/cvpr/DengDSLL009","MAG":"2108598243","DOI":"10.1109/CVPR.2009.5206848","CorpusId":57246310},"title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"32622973ea102abb5f0dea32e2944b7656cd4798","externalIds":{"MAG":"1985610876","DBLP":"journals/nle/ReiterD97","DOI":"10.1017/S1351324997001502","CorpusId":8460470},"title":"Building applied natural language generation systems"},{"paperId":"97671dde585824f5aa1192067e98e1a5db42c10f","externalIds":{"MAG":"88754628","DOI":"10.1016/0025-5564(72)90075-2","CorpusId":59837579},"title":"A simple neural network generating an interactive memory"},{"paperId":"d7f7f6dfbdbe5220aded6a211c50079e28b217ad","externalIds":{"MAG":"2129217160","DBLP":"journals/tc/Kohonen72","DOI":"10.1109/TC.1972.5008975","CorpusId":21483100},"title":"Correlation Matrix Memories"},{"paperId":"ae6b76791404d7e0034b702e0d28e7b591f7231d","externalIds":{"ACL":"2024.lrec-tutorials.6","CorpusId":269804217},"title":"Knowledge Editing for Large Language Models"},{"paperId":"c917c9ddd454c5b530db5852b6f762eb4e799d21","externalIds":{"DBLP":"journals/corr/abs-2402-11078","DOI":"10.48550/arXiv.2402.11078","CorpusId":271931415},"title":"Model Editing by Pure Fine-Tuning"},{"paperId":"afbc7ff9854e532ac27fd601d27c15d624042857","externalIds":{"DBLP":"journals/corr/abs-2401-17809","DOI":"10.48550/arXiv.2401.17809","CorpusId":275920466},"title":"SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering"},{"paperId":"7d5b03befa4b276e581e74d300cf0f18a7253890","externalIds":{"DBLP":"journals/corr/abs-2304-00740","DOI":"10.48550/arXiv.2304.00740","CorpusId":257913329},"title":"Measuring and Manipulating Knowledge Representations in Language Models"},{"paperId":"ac771182d1780c863954243809d1e144433919f9","externalIds":{"DBLP":"journals/corr/abs-2307-12966","DOI":"10.48550/arXiv.2307.12966","CorpusId":260356605},"title":"Aligning Large Language Models with Human: A Survey"},{"paperId":"8aa98fbfb6f1e979dead13ce24075503fe47658e","externalIds":{"DBLP":"journals/corr/abs-2301-00234","CorpusId":263886074},"title":"A Survey for In-context Learning"},{"paperId":"f727f928e7e179307d8d4a1da2387393f2bd7915","externalIds":{"ACL":"2023.eacl-main.199","DBLP":"conf/eacl/HaseDCLKSBI23","DOI":"10.18653/v1/2023.eacl-main.199","CorpusId":258378150},"title":"Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models"},{"paperId":"c373c792bcf5d0add8de812425d384ff101ef070","externalIds":{"DBLP":"conf/acl/YuJKYJ23","DOI":"10.18653/v1/2023.findings-acl.375","CorpusId":259859034},"title":"Unlearning Bias in Language Models by Partitioning Gradients"},{"paperId":"448e1493034dafe35699ae054ff4480b31dcf64a","externalIds":{"DBLP":"conf/emnlp/MadaanTCY22","DOI":"10.18653/v1/2022.emnlp-main.183","CorpusId":246016194},"title":"Memory-assisted prompt editing to improve GPT-3 after deployment"},{"paperId":"be7cb8f79bc018e57467168fc0c7f8ad59bba04f","externalIds":{"ACL":"2022.acl-long.230","DBLP":"conf/acl/RibeiroL22","DOI":"10.18653/v1/2022.acl-long.230","CorpusId":248779886},"title":"Adaptive Testing and Debugging of NLP Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","externalIds":{"MAG":"2952331680","ACL":"N19-1421","DBLP":"journals/corr/abs-1811-00937","ArXiv":"1811.00937","DOI":"10.18653/v1/N19-1421","CorpusId":53296520},"title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","externalIds":{"MAG":"2965425874","CorpusId":49313245},"title":"Improving Language Understanding by Generative Pre-Training"}]}