{"references":[{"paperId":"60071051e176b6e2fadbededadbb08b05e125c13","externalIds":{"DBLP":"journals/corr/abs-2403-19211","ArXiv":"2403.19211","DOI":"10.48550/arXiv.2403.19211","CorpusId":268732951},"title":"Dual-Personalizing Adapter for Federated Foundation Models"},{"paperId":"06eae596ea3e996453039f6a2cc68732cbba884b","externalIds":{"DBLP":"conf/icml/QinCQDLD24","ArXiv":"2312.06353","DOI":"10.48550/arXiv.2312.06353","CorpusId":266162335},"title":"Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes"},{"paperId":"004699d8ed6d1929e484aba461391fadb1f5b0a7","externalIds":{"ArXiv":"2310.03150","DBLP":"conf/deem/WoisetschlagerE24","DOI":"10.1145/3650203.3663331","CorpusId":263671932},"title":"Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly"},{"paperId":"529ff7d6441d244212cf2becafd12a7e67ac56d9","externalIds":{"DBLP":"conf/kdd/KuangQLCGPXLDZ24","ArXiv":"2309.00363","DOI":"10.1145/3637528.3671573","CorpusId":261494317},"title":"FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"},{"paperId":"5ce0fa3f0882910a5dfa2697bb04c8785d914725","externalIds":{"ArXiv":"2308.06522","DBLP":"journals/corr/abs-2308-06522","DOI":"10.48550/arXiv.2308.06522","CorpusId":260887495},"title":"SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models"},{"paperId":"a062bd65c60a761aaa76b50c131cf6feb5daf9ba","externalIds":{"PubMedCentral":"10290652","DOI":"10.1038/s41597-023-02256-2","CorpusId":259244944,"PubMed":"37355751"},"title":"A guide to sharing open healthcare data under the General Data Protection Regulation"},{"paperId":"8af8932d0ffd7e418e7733951fcda2ddbc4d7730","externalIds":{"DBLP":"journals/corr/abs-2306-03163","ArXiv":"2306.03163","DOI":"10.14778/3648160.3648165","CorpusId":259089113},"title":"How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study"},{"paperId":"1d2967d96b5e2daa172cb052b22c094beeec3068","externalIds":{"DBLP":"conf/acl/0002ZACKMRZ23","ACL":"2023.acl-industry.60","ArXiv":"2305.18465","DOI":"10.48550/arXiv.2305.18465","CorpusId":258967363},"title":"Federated Learning of Gboard Language Models with Differential Privacy"},{"paperId":"aa6ba4ade170abfb6c6c99d3ab5f1957b6ccec83","externalIds":{"DBLP":"journals/corr/abs-2305-11414","ArXiv":"2305.11414","ACL":"2024.lrec-main.630","DOI":"10.48550/arXiv.2305.11414","CorpusId":258823148},"title":"Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models"},{"paperId":"76b19363b10d7ea783e4a6494eae40d73c8e9628","externalIds":{"DBLP":"journals/natmi/DingQYWYSHCCCYZWLZCLTLS23","DOI":"10.1038/s42256-023-00626-4","CorpusId":257316425},"title":"Parameter-efficient fine-tuning of large-scale pre-trained language models"},{"paperId":"922729f3c8bebc4dea35c1b0dcc2769345759c8c","externalIds":{"DBLP":"journals/debu/LuH0023","ArXiv":"2302.13485","DOI":"10.48550/arXiv.2302.13485","CorpusId":257220009},"title":"FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning"},{"paperId":"9c86fa0c81e5338c635611e0da633f04e5d42b80","externalIds":{"DBLP":"conf/mlsys/WangCCKL23","ArXiv":"2302.12862","DOI":"10.48550/arXiv.2302.12862","CorpusId":257220077},"title":"FLINT: A Platform for Federated Learning Integration"},{"paperId":"3599a236f285af48782fc30b1341d13ec7320735","externalIds":{"ArXiv":"2302.09419","DBLP":"journals/corr/abs-2302-09419","DOI":"10.48550/arXiv.2302.09419","CorpusId":257039063},"title":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"},{"paperId":"f2b0017ddd77fa38760a18145e63553105a1a236","externalIds":{"DBLP":"journals/corr/abs-2301-13688","ArXiv":"2301.13688","DOI":"10.48550/arXiv.2301.13688","CorpusId":256415991},"title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"},{"paperId":"5278b81db686b4d36143941bff1c683bea963a63","externalIds":{"DBLP":"journals/corr/abs-2301-11913","ArXiv":"2301.11913","DOI":"10.48550/arXiv.2301.11913","CorpusId":251542095},"title":"SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient"},{"paperId":"c0e7a85fbedee72ad171b80d74160d887acca0de","externalIds":{"DBLP":"conf/icdcs/HuangZSFYW23","ArXiv":"2212.01977","DOI":"10.1109/ICDCS57875.2023.00036","CorpusId":259695965},"title":"Distributed Pruning Towards Tiny Neural Networks in Federated Learning"},{"paperId":"4ab619779f7123fe85e789facb8a43b8fcb5d9c0","externalIds":{"DBLP":"journals/ipm/BanabilahAAMJ22","DOI":"10.1016/j.ipm.2022.103061","CorpusId":251878302},"title":"Federated learning review: Fundamentals, enabling technologies, and future applications"},{"paperId":"04315fbe6554edc9b5d76c99f64b8d1a6f26f7af","externalIds":{"ArXiv":"2210.13291","DBLP":"journals/corr/abs-2210-13291","DOI":"10.48550/arXiv.2210.13291","CorpusId":253098004},"title":"NVIDIA FLARE: Federated Learning from Simulation to Real-World"},{"paperId":"ffe13079f6e1d475e5e358ac5a1dbdd93c409015","externalIds":{"DBLP":"conf/iclr/IsikPGWZ23","ArXiv":"2209.15328","DOI":"10.48550/arXiv.2209.15328","CorpusId":252668589},"title":"Sparse Random Networks for Communication-Efficient Federated Learning"},{"paperId":"15abd9759bc65f560abf74eb5bf14ce40a0c7526","externalIds":{"DBLP":"conf/icassp/ZhaoDLLL23","ArXiv":"2208.12268","DOI":"10.1109/ICASSP49357.2023.10095356","CorpusId":252762685},"title":"FedPrompt: Communication-Efficient and Privacy-Preserving Prompt Tuning in Federated Learning"},{"paperId":"1569e5e95f3c659810fad5473d3fbebf4c8d2f83","externalIds":{"ArXiv":"2208.05174","DBLP":"conf/ijcai/ChenCWY23","DOI":"10.48550/arXiv.2208.05174","CorpusId":251468165},"title":"FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning"},{"paperId":"11460f944d740129ac9c7044869b6adb2decbe24","externalIds":{"DBLP":"journals/corr/abs-2206-09888","ArXiv":"2206.09888","DOI":"10.48550/arXiv.2206.09888","CorpusId":249888977},"title":"SoteriaFL: A Unified Framework for Private Federated Learning with Communication Compression"},{"paperId":"3b56b5410e65f68a81cbeb03d134c36e37abeb5b","externalIds":{"ArXiv":"2204.05011","DBLP":"journals/pvldb/XieWGCYKLDZ23","DOI":"10.14778/3579075.3579081","CorpusId":248506229},"title":"FederatedScope: A Flexible Federated Learning Platform for Heterogeneity"},{"paperId":"220590e7815ea278959329058a5de3e4c9df9f4e","externalIds":{"DBLP":"journals/tist/TianWLYJS22","DOI":"10.1145/3510033","CorpusId":246531797},"title":"FedBERT: When Federated Learning Meets Pre-training"},{"paperId":"08460ecff91b8a54358b9c1709d7dc6a77417f62","externalIds":{"ArXiv":"2109.11105","DBLP":"conf/emnlp/HeSMZ0K21","ACL":"2021.sustainlp-1.13","DOI":"10.18653/v1/2021.sustainlp-1.13","CorpusId":237605176},"title":"Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing"},{"paperId":"01b1293ddea9bcd6df1185b0b934503de01d6561","externalIds":{"ArXiv":"2109.04838","DBLP":"journals/corr/abs-2109-04838","ACL":"2021.emnlp-main.829","DOI":"10.18653/v1/2021.emnlp-main.829","CorpusId":237485472},"title":"Block Pruning For Faster Transformers"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","externalIds":{"ArXiv":"2108.05542","DBLP":"journals/corr/abs-2108-05542","CorpusId":236987275},"title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"339b2b711fb5b228d097b03ebc3e62a521779235","externalIds":{"DBLP":"conf/acl/ZakenGR22","ACL":"2022.acl-short.1","ArXiv":"2106.10199","DOI":"10.18653/v1/2022.acl-short.1","CorpusId":231672601},"title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"},{"paperId":"e2d9a57487d62b754d8ea7df75fe1e4c1c8cd97a","externalIds":{"DBLP":"journals/corr/abs-2106-00275","ArXiv":"2106.00275","DOI":"10.24963/ijcai.2021/67","CorpusId":235265921},"title":"H-FL: A Hierarchical Communication-Efficient and Privacy-Protected Architecture for Federated Learning"},{"paperId":"e2d9926fafc75e91f20134b9cbf14db5c098ffa7","externalIds":{"ArXiv":"2105.06413","DBLP":"journals/corr/abs-2105-06413","PubMedCentral":"9715347","DOI":"10.1088/1361-6560/ac97d9","CorpusId":234482966,"PubMed":"36198326"},"title":"OpenFL: the open federated learning library"},{"paperId":"f938cffd498ffb81ee9d66b4cd473e82c2e12c72","externalIds":{"DBLP":"journals/corr/abs-2104-14362","ArXiv":"2104.14362","DOI":"10.1007/s10115-022-01664-x","CorpusId":233444166},"title":"From distributed machine learning to federated learning: a survey"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","externalIds":{"DBLP":"journals/corr/abs-2104-08691","ArXiv":"2104.08691","ACL":"2021.emnlp-main.243","DOI":"10.18653/v1/2021.emnlp-main.243","CorpusId":233296808},"title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"f03333b06c1b2e356dcc03f6bd3ef3d849d531e5","externalIds":{"ArXiv":"2104.07914","DBLP":"journals/comsur/NguyenDPSLP21","DOI":"10.1109/COMST.2021.3075439","CorpusId":233289549},"title":"Federated Learning for Internet of Things: A Comprehensive Survey"},{"paperId":"38fdd7d958708e124fd2bf65771fe5151b9ff03b","externalIds":{"DBLP":"conf/www/0002LS21","ArXiv":"2103.02885","DOI":"10.1145/3442381.3450068","CorpusId":232110804},"title":"Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework"},{"paperId":"06c644405821c106ced577d53e6dac69a7b72b2c","externalIds":{"ArXiv":"2102.13451","DBLP":"conf/nips/HorvathLALVL21","CorpusId":232068701},"title":"FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout"},{"paperId":"405f2de412eff9e04ba2ae74fe21fdb692c9da60","externalIds":{"ArXiv":"2102.07053","DBLP":"conf/nips/MitraJPH21","CorpusId":237396004},"title":"Linear Convergence in Federated Learning: Tackling Client Heterogeneity and Sparse Gradients"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","externalIds":{"ArXiv":"2010.11929","MAG":"3119786062","DBLP":"conf/iclr/DosovitskiyB0WZ21","CorpusId":225039882},"title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"7dc04d1f17f1a71d684c01693d0de66fd6517f46","externalIds":{"DBLP":"conf/icca/LiFL20","MAG":"3106673115","DOI":"10.1109/ICCA51439.2020.9264412","CorpusId":227278354},"title":"A Survey on federated learning*"},{"paperId":"f9b3ed20d6da7dfbfbd8a58e2bde173e5e9c768c","externalIds":{"MAG":"3090393599","DBLP":"conf/iclr/Diao0T21","ArXiv":"2010.01264","CorpusId":222133374},"title":"HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients"},{"paperId":"953cc563497f8498b8f0653676fbb195b8296998","externalIds":{"ArXiv":"2008.03371","MAG":"3048121440","DBLP":"journals/corr/abs-2008-03371","CorpusId":221090217},"title":"LotteryFL: Personalized and Communication-Efficient Federated Learning with Lottery Ticket Hypothesis on Non-IID Datasets"},{"paperId":"e00c69fb70095b92ac3c0bb0c1d737dfa696d232","externalIds":{"MAG":"3046653923","DBLP":"journals/access/AledhariRPS20","DOI":"10.1109/access.2020.3013541","CorpusId":221131992,"PubMed":"32999795"},"title":"Federated Learning: A Survey on Enabling Technologies, Protocols, and Applications"},{"paperId":"a199a03e11b68c4132be880b5fcabc57251bc477","externalIds":{"ArXiv":"2007.14390","CorpusId":220831008},"title":"Flower: A Friendly Federated Learning Research Framework"},{"paperId":"c5c4142a01981787a71bf6ebcb791520c458ab5d","externalIds":{"DBLP":"journals/corr/abs-2007-13518","MAG":"3044211235","ArXiv":"2007.13518","CorpusId":220793772},"title":"FedML: A Research Library and Benchmark for Federated Machine Learning"},{"paperId":"6e9fb743a27d4d471d14b578f84cd0c57e9d3e55","externalIds":{"MAG":"3044097461","ArXiv":"2007.10987","DBLP":"journals/corr/abs-2007-10987","CorpusId":220686488},"title":"IBM Federated Learning: an Enterprise Framework White Paper V0.1"},{"paperId":"66f0f35fc78bdf2af9de46093d49a428970cde2e","externalIds":{"MAG":"3099715410","DBLP":"conf/nips/Sanh0R20","ArXiv":"2005.07683","CorpusId":218665313},"title":"Movement Pruning: Adaptive Sparsity by Fine-Tuning"},{"paperId":"2f5e6ec8904e84738fdff37b39220c0c837529a1","externalIds":{"DBLP":"journals/corr/abs-1911-02054","MAG":"2986413680","ArXiv":"1911.02054","CorpusId":207880633},"title":"Federated Adversarial Domain Adaptation"},{"paperId":"bfb35423cc621e41eb74ef32b8ac8205222a201b","externalIds":{"ArXiv":"1910.11567","MAG":"2981410936","DBLP":"journals/corr/abs-1910-11567","CorpusId":204915903},"title":"Substra: a framework for privacy-preserving, traceable and collaborative Machine Learning"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","externalIds":{"MAG":"2981852735","DBLP":"journals/corr/abs-1910-10683","ArXiv":"1910.10683","CorpusId":204838007},"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"52d655cadb4ab977b951c1d57e740688f54032dd","externalIds":{"DBLP":"conf/micro/ZhuZG019","MAG":"2980186997","DOI":"10.1145/3352460.3358269","CorpusId":204732514},"title":"Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs"},{"paperId":"0088cd8408c2a77101412f37bfada0a57669b8bc","externalIds":{"DBLP":"journals/corr/abs-1909-13014","ArXiv":"1909.13014","MAG":"3037209298","CorpusId":203593931},"title":"FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization"},{"paperId":"99fc962a0609a8bc0dfb60721cfe62b984cc6b07","externalIds":{"DBLP":"journals/corr/abs-1909-12326","MAG":"2977090839","ArXiv":"1909.12326","DOI":"10.1109/TNNLS.2022.3166101","CorpusId":203592134,"PubMed":"35468066"},"title":"Model Pruning Enables Efficient Federated Learning on Edge Devices"},{"paperId":"93d6752f11d5db3687cc9f895f219b1bed7e1023","externalIds":{"MAG":"2992272656","ArXiv":"1907.09693","DBLP":"journals/tkde/LiWWHWLLH23","DOI":"10.1109/TKDE.2021.3124599","CorpusId":198179889},"title":"A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection"},{"paperId":"f7c6ab8303a03dc9e8454c070030c3e6b0233d40","externalIds":{"ArXiv":"1904.07933","MAG":"2939663618","DBLP":"conf/wacv/PerezSMM20","DOI":"10.1109/WACV45572.2020.9093307","CorpusId":119314564},"title":"Audio-Visual Model Distillation Using Acoustic Images"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","externalIds":{"DBLP":"journals/corr/abs-1902-00751","ArXiv":"1902.00751","MAG":"2964303773","CorpusId":59599816},"title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"4f78624defde3b60551cfeb37e3943b267ea704a","externalIds":{"DBLP":"journals/corr/abs-1901-09269","ArXiv":"1901.09269","MAG":"2914138317","DOI":"10.1080/10556788.2024.2358790","CorpusId":59316742},"title":"Distributed learning with compressed gradient differences"},{"paperId":"21937ecd9d66567184b83eca3d3e09eb4e6fbd60","externalIds":{"ArXiv":"1803.03635","MAG":"2951099858","DBLP":"conf/iclr/FrankleC19","CorpusId":53388625},"title":"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"},{"paperId":"b4702ebf71b74023d9769677aff2b2f30900a6cf","externalIds":{"MAG":"2950463721","DBLP":"conf/aaai/ZhouMCF18","ArXiv":"1712.01048","DOI":"10.1609/aaai.v32i1.11623","CorpusId":19227870},"title":"Adaptive Quantization for Deep Neural Network"},{"paperId":"3b4d671a8c7018c0b42673ba581e5ff3ae762d6c","externalIds":{"MAG":"2764043458","ArXiv":"1710.01878","DBLP":"conf/iclr/ZhuG18","CorpusId":27494814},"title":"To prune, or not to prune: exploring the efficacy of pruning for model compression"},{"paperId":"c318aca0408f23e2c32a4dfae216801b4029e9e9","externalIds":{"DBLP":"journals/corr/OssiaSTRLH17","MAG":"2596378825","ArXiv":"1703.02952","DOI":"10.1109/JIOT.2020.2967734","CorpusId":4909695},"title":"A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics"},{"paperId":"c9d64aaa2007b60ef7814acc895dd90f15578a20","externalIds":{"MAG":"2769644379","DBLP":"conf/nips/AlistarhG0TV17","CorpusId":263894534},"title":"QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding"},{"paperId":"d1dbf643447405984eeef098b1b320dee0b3b8a7","externalIds":{"MAG":"2950745363","DBLP":"conf/aistats/McMahanMRHA17","ArXiv":"1602.05629","CorpusId":14955348},"title":"Communication-Efficient Learning of Deep Networks from Decentralized Data"},{"paperId":"f2f8f7a2ec1b2ede48cbcd189b376ab9fa0735ef","externalIds":{"MAG":"2337093093","DBLP":"conf/allerton/ShokriS15","DOI":"10.1145/2810103.2813687","CorpusId":20714},"title":"Privacy-preserving deep learning"},{"paperId":"93107fdda9df3b4ee68299ee97dcbdba4cd39d3a","externalIds":{"DBLP":"journals/corr/abs-2210-01708","DOI":"10.48550/arXiv.2210.01708","CorpusId":252693316},"title":"Exploring Parameter-Efficient Fine-tuning for Improving Communication Efficiency in Federated Learning"},{"paperId":"9c530d2e71f9a66319c924cef583897f01807e8b","externalIds":{"MAG":"3172018708","DOI":"10.1007/978-3-030-70604-3_5","CorpusId":236690571},"title":"PySyft: A Library for Easy Federated Learning"},{"paperId":"3d73e21af71bde8dc7984bd72f7077fb691b2523","externalIds":{"DBLP":"journals/jmlr/LiuFCXY21","CorpusId":246432494},"title":"FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce","externalIds":{"MAG":"2525491769","DOI":"10.1201/b18055-8","CorpusId":208945385},"title":"google,我,萨娜"}]}