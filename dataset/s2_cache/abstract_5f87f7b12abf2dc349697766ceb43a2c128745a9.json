{"abstract":"It is quite laborious and costly to manually label LiDAR point cloud data for training high-quality 3D object detectors. This work proposes a <italic>weakly supervised</italic> framework which allows learning 3D detection from a few weakly annotated examples. This is achieved by a two-stage architecture design. Stage-1 learns to generate cylindrical object proposals under inaccurate and inexact supervision, obtained by our proposed BEV center-click annotation strategy, where only the horizontal object centers are click-annotated in bird's view scenes. Stage-2 learns to predict cuboids and confidence scores in a <italic>coarse-to-fine, cascade</italic> manner, under incomplete supervision, i.e., only a small portion of object cuboids are precisely annotated. With KITTI dataset, using only 500 weakly annotated scenes and 534 precisely labeled vehicle instances, our method achieves <inline-formula><tex-math notation=\"LaTeX\">$86-97$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>86</mml:mn><mml:mo>-</mml:mo><mml:mn>97</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"wang-ieq1-3063611.gif\"/></alternatives></inline-formula> percent the performance of current top-leading, fully supervised detectors (which require 3,712 exhaustively annotated scenes with 15,654 instances). More importantly, with our elaborately designed network architecture, our trained model can be applied as a 3D object annotator, supporting both automatic and active (human-in-the-loop) working modes. The annotations generated by our model can be used to train 3D object detectors, achieving over 95 percent of their original performance (with manually labeled training data). Our experiments also show our model's potential in boosting performance when given more training data. The above designs make our approach highly practical and open-up opportunities for learning 3D detection at reduced annotation cost."}