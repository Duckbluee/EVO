{"references":[{"paperId":"a6c4c667242f0ae280da5057fb2bf46f8f467d09","externalIds":{"DBLP":"journals/corr/abs-2501-11041","ArXiv":"2501.11041","DOI":"10.18653/v1/2024.findings-acl.199","CorpusId":271931254},"title":"Enhancing Semantic Consistency of Large Language Models through Model Editing: An Interpretability-Oriented Approach"},{"paperId":"b247c4f4dd95e355deca68a2dd581eb7c244ea9f","externalIds":{"DBLP":"journals/corr/abs-2411-10115","ArXiv":"2411.10115","DOI":"10.48550/arXiv.2411.10115","CorpusId":274116621},"title":"Memorization in Attention-only Transformers"},{"paperId":"f64f0b8d2b0886196aa38edff165f3797c76108a","externalIds":{"DBLP":"journals/corr/abs-2410-19750","ArXiv":"2410.19750","PubMedCentral":"12025678","DOI":"10.3390/e27040344","CorpusId":273654439,"PubMed":"40282579"},"title":"The Geometry of Concepts: Sparse Autoencoder Feature Structure"},{"paperId":"7fbf028793a5252941e358366f57837888fcbf11","externalIds":{"DBLP":"journals/corr/abs-2408-12599","ArXiv":"2408.12599","DOI":"10.48550/arXiv.2408.12599","CorpusId":271924120},"title":"Controllable Text Generation for Large Language Models: A Survey"},{"paperId":"fa5fc9f50d9eb406ce8cd2e81d105699b21c73d6","externalIds":{"DBLP":"conf/acl/KimVF25","ArXiv":"2408.08590","DOI":"10.18653/v1/2025.findings-acl.525","CorpusId":271892176},"title":"Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference"},{"paperId":"a7a63c50547f252d8fdbc9d6d593fc95cad7b5cf","externalIds":{"ArXiv":"2408.00137","DBLP":"conf/naacl/YuSHKCCJLGY25","DOI":"10.48550/arXiv.2408.00137","CorpusId":271600971},"title":"Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment"},{"paperId":"fa0cbfba4e41b9f2487df251fcc3b93c21381167","externalIds":{"ArXiv":"2407.19842","DBLP":"journals/corr/abs-2407-19842","DOI":"10.24963/ijcai.2024/43","CorpusId":271508212},"title":"Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability"},{"paperId":"d5f82199e97d325427dffdc027baca75578561f5","externalIds":{"ArXiv":"2407.15891","DBLP":"conf/iclr/TangLLHKHYW25","DOI":"10.48550/arXiv.2407.15891","CorpusId":271334610},"title":"RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"},{"paperId":"3a588665fb59801fcbdce825ed7a4cf59984567f","externalIds":{"DBLP":"journals/corr/abs-2407-14507","ArXiv":"2407.14507","DOI":"10.48550/arXiv.2407.14507","CorpusId":271310469},"title":"Internal Consistency and Self-Feedback in Large Language Models: A Survey"},{"paperId":"d334a921861284a370d7aa97bd4a177baa294635","externalIds":{"DBLP":"journals/corr/abs-2407-10153","ArXiv":"2407.10153","DOI":"10.48550/arXiv.2407.10153","CorpusId":271212672},"title":"Look Within, Why LLMs Hallucinate: A Causal Perspective"},{"paperId":"8aa8f60fd63fc8cea29fe1722fc2bcdf5462d30d","externalIds":{"DBLP":"conf/sigir/GuoR0CLH24","DOI":"10.1145/3626772.3657819","CorpusId":271114517},"title":"Steering Large Language Models for Cross-lingual Information Retrieval"},{"paperId":"4fba55c4477577e744bf0ee946a77a2c0615c6c9","externalIds":{"ArXiv":"2407.07011","DBLP":"journals/corr/abs-2407-07011","DOI":"10.48550/arXiv.2407.07011","CorpusId":271064685},"title":"Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning"},{"paperId":"143a05fb36be8198d7675b594c0656b5652da3cb","externalIds":{"ArXiv":"2407.07071","DBLP":"conf/emnlp/ChuangQHKKG24","ACL":"2024.emnlp-main.84","DOI":"10.48550/arXiv.2407.07071","CorpusId":271064935},"title":"Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps"},{"paperId":"0ac43cb23cdb84b6c7dc6986c036fb3152e9a286","externalIds":{"ArXiv":"2407.03282","DBLP":"journals/corr/abs-2407-03282","ACL":"2024.blackboxnlp-1.6","DOI":"10.48550/arXiv.2407.03282","CorpusId":270923744},"title":"LLM Internal States Reveal Hallucination Risk Faced With a Query"},{"paperId":"b778fd5f23b91499e4186539e66596a0ac67a13b","externalIds":{"ArXiv":"2407.06204","DBLP":"journals/tkde/CaiJWTKH25","DOI":"10.1109/TKDE.2025.3554028","CorpusId":271064424},"title":"A Survey on Mixture of Experts in Large Language Models"},{"paperId":"966a36fdc59702889a2fe44f47631fd0065ed5ef","externalIds":{"ArXiv":"2406.14909","CorpusId":270688596},"title":"Mixture of Attention Spans: Optimizing LLM Inference Efficiency with Heterogeneous Sliding-Window Lengths"},{"paperId":"078e4ff3739a96365482483beca71fc505a7e7eb","externalIds":{"DBLP":"conf/nips/CabannesABYCK24","ArXiv":"2406.02128","DOI":"10.48550/arXiv.2406.02128","CorpusId":270226246},"title":"Iteration Head: A Mechanistic Study of Chain-of-Thought"},{"paperId":"5012568ce3a750679b7e874728ddd0b11f7ffeb8","externalIds":{"DBLP":"journals/corr/abs-2406-01563","ArXiv":"2406.01563","DOI":"10.48550/arXiv.2406.01563","CorpusId":270214221},"title":"LoFiT: Localized Fine-tuning on LLM Representations"},{"paperId":"f1481b4eba72c1e1d355413af37352a0bcfc50e9","externalIds":{"ArXiv":"2405.17969","DBLP":"journals/corr/abs-2405-17969","DOI":"10.48550/arXiv.2405.17969","CorpusId":270068372},"title":"Knowledge Circuits in Pretrained Transformers"},{"paperId":"d04a7906161835440daaff90265dacca01ed3ba3","externalIds":{"ArXiv":"2405.14992","DBLP":"conf/nips/LiZBM24","DOI":"10.48550/arXiv.2405.14992","CorpusId":270045551},"title":"Linking In-context Learning in Transformers to Human Episodic Memory"},{"paperId":"015dd4ca002c14b83e0e53bce83b7eacea9f5a28","externalIds":{"DBLP":"conf/aistats/Garcia-Carrasco24","ArXiv":"2405.04156","DOI":"10.48550/arXiv.2405.04156","CorpusId":269614509},"title":"How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability"},{"paperId":"2d7d313c276d64b70f9839732ba5257c9e1a1110","externalIds":{"DBLP":"journals/corr/abs-2405-03340","ArXiv":"2405.03340","DOI":"10.48550/arXiv.2405.03340","CorpusId":269604799},"title":"Functional Equivalence with NARS"},{"paperId":"b51dbe60e8ee7cece43f8f8e3a54bfc123d1fb15","externalIds":{"DOI":"10.1016/j.intell.2024.101832","CorpusId":269015060},"title":"Defining intelligence: Bridging the gap between human and artificial perspectives"},{"paperId":"edd705ebe3546272b7fe952e2ed6088200adad76","externalIds":{"DBLP":"conf/iclr/WuWX0F25","ArXiv":"2404.15574","DOI":"10.48550/arXiv.2404.15574","CorpusId":269330144},"title":"Retrieval Head Mechanistically Explains Long-Context Factuality"},{"paperId":"a0b775b9ff82ce1fb7dd34d53a7d09f70b171895","externalIds":{"ArXiv":"2404.15255","DBLP":"journals/corr/abs-2404-15255","DOI":"10.48550/arXiv.2404.15255","CorpusId":269302704},"title":"How to use and interpret activation patching"},{"paperId":"63a87feede94433b44b2c2b194e5902c3c5158f2","externalIds":{"DBLP":"conf/icml/SinghMHCS24","ArXiv":"2404.07129","DOI":"10.48550/arXiv.2404.07129","CorpusId":269033179},"title":"What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation"},{"paperId":"9cf6d8ad63fd43658d4cdae58a78013852a759ff","externalIds":{"ArXiv":"2403.19521","DBLP":"journals/corr/abs-2403-19521","DOI":"10.48550/arXiv.2403.19521","CorpusId":268732668},"title":"Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models"},{"paperId":"9b4091d43d6ddc35efcbfb60263d1f46d2ad5d82","externalIds":{"DBLP":"conf/acl/LiCTSWDLXMPL24","ArXiv":"2403.00862","DOI":"10.18653/v1/2024.acl-long.538","CorpusId":270210580},"title":"NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism"},{"paperId":"4c0a545b1eb73084e3d395435af9cf6649c2b1da","externalIds":{"DBLP":"conf/acl/JinCY0XLJ0024","ArXiv":"2402.18154","DOI":"10.48550/arXiv.2402.18154","CorpusId":268041323},"title":"Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models"},{"paperId":"1784cd42207a12bdc4061bb0f78bc8d5c82f61dd","externalIds":{"DBLP":"conf/emnlp/FerrandoV24","ACL":"2024.emnlp-main.965","ArXiv":"2403.00824","DOI":"10.48550/arXiv.2403.00824","CorpusId":268230705},"title":"Information Flow Routes: Automatically Interpreting Language Models at Scale"},{"paperId":"4b84736c87d9d927cc2b68a9fdc769f91dc56d71","externalIds":{"ArXiv":"2402.13055","DBLP":"conf/acl/RenG0LZQL24","DOI":"10.48550/arXiv.2402.13055","CorpusId":267759700},"title":"Identifying Semantic Induction Heads to Understand In-Context Learning"},{"paperId":"585ab11510d0e8504289991919976afba64d4c96","externalIds":{"DBLP":"conf/acl/OrtuJDSCS24","ArXiv":"2402.11655","DOI":"10.48550/arXiv.2402.11655","CorpusId":267751331},"title":"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"},{"paperId":"36681edd6a32b1521844f2e3904587c8f5750d09","externalIds":{"ArXiv":"2402.11004","DBLP":"journals/corr/abs-2402-11004","DOI":"10.48550/arXiv.2402.11004","CorpusId":267751253},"title":"The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains"},{"paperId":"ebfc54627cdd61a07c44667ab3a8ea77099fe59b","externalIds":{"DBLP":"journals/corr/abs-2402-07321","ArXiv":"2402.07321","DOI":"10.48550/arXiv.2402.07321","CorpusId":267627186},"title":"Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs"},{"paperId":"46955794ab197d56b40595fcb8e74b6948097075","externalIds":{"ACL":"2024.emnlp-main.192","ArXiv":"2402.02872","DBLP":"journals/corr/abs-2402-02872","DOI":"10.48550/arXiv.2402.02872","CorpusId":267411978},"title":"How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning"},{"paperId":"a66b56be4e521c0510befa5a172629c305de4839","externalIds":{"DBLP":"journals/natmi/MischlerLBMM24","ArXiv":"2401.17671","DOI":"10.1038/s42256-024-00925-4","CorpusId":267334935},"title":"Contextual feature extraction hierarchies converge in large language models and the brain"},{"paperId":"3f877562995d1408b0b3abd5dfbbe8eeecb6061e","externalIds":{"ArXiv":"2401.12874","DBLP":"journals/corr/abs-2401-12874","DOI":"10.48550/arXiv.2401.12874","CorpusId":267095032},"title":"From Understanding to Utilization: A Survey on Explainability for Large Language Models"},{"paperId":"85447eeb6e5276e713957835125a2273f9ac0694","externalIds":{"DBLP":"journals/corr/abs-2401-12973","ArXiv":"2401.12973","DOI":"10.48550/arXiv.2401.12973","CorpusId":267095070},"title":"In-Context Language Learning: Architectures and Algorithms"},{"paperId":"caf60d1120c2d5a894098f01b51d2e2ad32301d7","externalIds":{"ArXiv":"2312.14033","DBLP":"conf/acl/ChenDZLLZZZLCZ24","DOI":"10.18653/v1/2024.acl-long.515","CorpusId":266999933},"title":"T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step"},{"paperId":"07c2c3b4af7e4ff1577f36f47f1c93398a6df648","externalIds":{"DBLP":"conf/iclr/GouldOOC24","ArXiv":"2312.09230","DOI":"10.48550/arXiv.2312.09230","CorpusId":266210012},"title":"Successor Heads: Recurring, Interpretable Attention Heads In The Wild"},{"paperId":"f483c00cc45dd387364dc485794c27afae9289bb","externalIds":{"DBLP":"journals/corr/abs-2312-03002","ArXiv":"2312.03002","DOI":"10.48550/arXiv.2312.03002","CorpusId":265689952},"title":"The mechanistic basis of data dependence and abrupt learning in an in-context classification task"},{"paperId":"eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4","externalIds":{"ArXiv":"2311.03658","DBLP":"journals/corr/abs-2311-03658","DOI":"10.48550/arXiv.2311.03658","CorpusId":265042984},"title":"The Linear Representation Hypothesis and the Geometry of Large Language Models"},{"paperId":"9c6f8d0a8734dca217690a9dfdc3737aa4712df5","externalIds":{"ArXiv":"2311.03839","DBLP":"journals/corr/abs-2311-03839","DOI":"10.48550/arXiv.2311.03839","CorpusId":265042975},"title":"Aspects of human memory and Large Language Models"},{"paperId":"b13947c7598aa91992cf04048afa19c7cfe69795","externalIds":{"ArXiv":"2310.15213","DBLP":"conf/iclr/ToddLSMWB24","DOI":"10.48550/arXiv.2310.15213","CorpusId":264439657},"title":"Function Vectors in Large Language Models"},{"paperId":"012dda8aef89ffa57128b15dc3cad2c02eedfa80","externalIds":{"ArXiv":"2310.15154","DBLP":"journals/corr/abs-2310-15154","DOI":"10.48550/arXiv.2310.15154","CorpusId":264591569},"title":"Linear Representations of Sentiment in Large Language Models"},{"paperId":"6a6cae8ead5953b69c42990f8d0942841135e80a","externalIds":{"PubMedCentral":"10629494","DOI":"10.3389/fpsyg.2023.1279317","CorpusId":264398751,"PubMed":"37941751"},"title":"Language models and psychological sciences"},{"paperId":"c0d9a48547d728dd320b453b01a0ab1ce2f96098","externalIds":{"DBLP":"conf/iclr/MerulloEP24","ArXiv":"2310.08744","DOI":"10.48550/arXiv.2310.08744","CorpusId":264127896},"title":"Circuit Component Reuse Across Tasks in Transformer Language Models"},{"paperId":"0eae83a76b2db97547d8ed42bb048c3eaaf78027","externalIds":{"ArXiv":"2310.04625","DBLP":"journals/corr/abs-2310-04625","DOI":"10.48550/arXiv.2310.04625","CorpusId":263831290},"title":"Copy Suppression: Comprehensively Understanding an Attention Head"},{"paperId":"c16c05ca0a3d24519405849fd24604fc1ce47751","externalIds":{"DBLP":"conf/iclr/ZhangN24","ArXiv":"2309.16042","DOI":"10.48550/arXiv.2309.16042","CorpusId":263131114},"title":"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods"},{"paperId":"c413a339d7784574ed43debea494ef405ee09d81","externalIds":{"DBLP":"journals/corr/abs-2309-07311","ArXiv":"2309.07311","DOI":"10.48550/arXiv.2309.07311","CorpusId":261822542},"title":"Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs"},{"paperId":"110804428354df709b3693f9efc81946a9036ebf","externalIds":{"DBLP":"journals/corr/abs-2309-04827","ArXiv":"2309.04827","DOI":"10.48550/arXiv.2309.04827","CorpusId":261682169},"title":"Neurons in Large Language Models: Dead, N-gram, Positional"},{"paperId":"55c562c0de9d011c91965a34ba784c9d4b72fecb","externalIds":{"DBLP":"conf/iclr/HernandezSHMWAB24","ArXiv":"2308.09124","DOI":"10.48550/arXiv.2308.09124","CorpusId":261031179},"title":"Linearity of Relation Decoding in Transformer Language Models"},{"paperId":"77f02ff24909896856fec410968aef7999c29440","externalIds":{"DBLP":"journals/corr/abs-2307-09458","ArXiv":"2307.09458","DOI":"10.48550/arXiv.2307.09458","CorpusId":259950939},"title":"Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla"},{"paperId":"405f8f5f1c6df1b3343c812832479aad5180b65f","externalIds":{"ArXiv":"2306.03341","DBLP":"journals/corr/abs-2306-03341","CorpusId":259088877},"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"},{"paperId":"11ae58636a5daf0ea1297f1c4ee94042fcebefa8","externalIds":{"DBLP":"conf/nips/BiettiCBJB23","ArXiv":"2306.00802","DOI":"10.48550/arXiv.2306.00802","CorpusId":258999187},"title":"Birth of a Transformer: A Memory Viewpoint"},{"paperId":"5dc15ac1c92ab7492f121471823fb13a95d273ba","externalIds":{"ArXiv":"2305.15054","DBLP":"conf/emnlp/StolfoBS23","DOI":"10.18653/v1/2023.emnlp-main.435","CorpusId":258865170},"title":"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis"},{"paperId":"56e952fd463accff09cf2e35432aaabd7c7c57f3","externalIds":{"DBLP":"conf/emnlp/ZhongWMPC23","ArXiv":"2305.14795","DOI":"10.48550/arXiv.2305.14795","CorpusId":258865984},"title":"MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions"},{"paperId":"7fa85f9c0fe44f1bf9e58a55f0f009296578c2f0","externalIds":{"DBLP":"journals/corr/abs-2305-09731","ArXiv":"2305.09731","DOI":"10.48550/arXiv.2305.09731","CorpusId":258740972},"title":"What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning"},{"paperId":"aec826ff336ca442697d5f908ab1668f1ea18987","externalIds":{"DBLP":"journals/corr/abs-2305-00586","ArXiv":"2305.00586","DOI":"10.48550/arXiv.2305.00586","CorpusId":258426987},"title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"},{"paperId":"133b97e40017a9bbbadd10bcd7f13088a97ca3cc","externalIds":{"DBLP":"conf/emnlp/GevaBFG23","ArXiv":"2304.14767","DOI":"10.48550/arXiv.2304.14767","CorpusId":258417932},"title":"Dissecting Recall of Factual Associations in Auto-Regressive Language Models"},{"paperId":"eefbd8b384a58f464827b19e30a6920ba976def9","externalIds":{"ArXiv":"2304.14997","DBLP":"conf/nips/ConmyMLHG23","DOI":"10.48550/arXiv.2304.14997","CorpusId":258418244},"title":"Towards Automated Circuit Discovery for Mechanistic Interpretability"},{"paperId":"cb1b5d949a1d711b1bd3a2924e045ad3f893252c","externalIds":{"ArXiv":"2303.02536","DBLP":"journals/corr/abs-2303-02536","DOI":"10.48550/arXiv.2303.02536","CorpusId":257365438},"title":"Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations"},{"paperId":"93a9fe0ea501e83a8f2a73ceb6a0431671bc707a","externalIds":{"PubMedCentral":"11399123","ArXiv":"2302.01308","DOI":"10.1038/s41598-024-72071-1","CorpusId":259171738,"PubMed":"39271909"},"title":"Large language models predict human sensory judgments across six modalities"},{"paperId":"9ad02d98c3421cf93be11c81e7b19faab12aea4a","externalIds":{"DBLP":"conf/eacl/MohebbiZCA23","ArXiv":"2301.12971","ACL":"2023.eacl-main.245","DOI":"10.48550/arXiv.2301.12971","CorpusId":256389522},"title":"Quantifying Context Mixing in Transformers"},{"paperId":"6edd112383ad494f5f2eba72b6f4ffae122ce61f","externalIds":{"DBLP":"journals/corr/abs-2211-00593","ArXiv":"2211.00593","DOI":"10.48550/arXiv.2211.00593","CorpusId":253244237},"title":"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"},{"paperId":"c90a99eeb57019732a6cc996bb9eaf13faedf00f","externalIds":{"ArXiv":"2209.11895","DBLP":"journals/corr/abs-2209-11895","DOI":"10.48550/arXiv.2209.11895","CorpusId":252532078},"title":"In-context Learning and Induction Heads"},{"paperId":"2c709ef6186bd607494a3344c903552ea500e449","externalIds":{"ArXiv":"2207.13243","DBLP":"conf/satml/RaukerHCH23","DOI":"10.1109/SaTML54575.2023.00039","CorpusId":251104722},"title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks"},{"paperId":"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","externalIds":{"DBLP":"journals/corr/abs-2207-07051","ArXiv":"2207.07051","DOI":"10.48550/arXiv.2207.07051","CorpusId":250526626},"title":"Language models show human-like content effects on reasoning"},{"paperId":"41b6cc4acedea461646ea85426f4f750a753a33b","externalIds":{"DBLP":"journals/access/GoncalvesRTC22","ArXiv":"2204.12406","DOI":"10.1109/ACCESS.2022.3206449","CorpusId":248391921},"title":"A Survey on Attention Mechanisms for Medical Applications: are we Moving Toward Better Algorithms?"},{"paperId":"5a71bf38cf409b55b14b2d5159c0b06bef9ad603","externalIds":{"DBLP":"journals/corr/abs-2203-14263","ArXiv":"2203.14263","DOI":"10.1109/TKDE.2021.3126456","CorpusId":243973878},"title":"A General Survey on Attention Mechanisms in Deep Learning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a2bb8ccedba93e23b51e4ed1a544a3f85e3c289f","externalIds":{"ArXiv":"2201.09305","DBLP":"journals/corr/abs-2201-09305","CorpusId":246240693},"title":"An Analysis and Comparison of ACT-R and Soar"},{"paperId":"7211f8fca79962cc1d4f440524b0dad6481abdfb","externalIds":{"ArXiv":"2112.05909","DBLP":"journals/corr/abs-2112-05909","CorpusId":245124239},"title":"Neural Attention Models in Deep Learning: Survey and Taxonomy"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","externalIds":{"ArXiv":"2110.14168","DBLP":"journals/corr/abs-2110-14168","CorpusId":239998651},"title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"42fc019b2668c9d9d984154d4c57f6c6d5a91619","externalIds":{"ArXiv":"2109.07684","ACL":"2021.mrl-1.1","DBLP":"journals/corr/abs-2109-07684","DOI":"10.18653/v1/2021.mrl-1.1","CorpusId":237532173},"title":"Language Models are Few-shot Multilingual Learners"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","externalIds":{"DBLP":"journals/corr/abs-2109-07958","ACL":"2022.acl-long.229","ArXiv":"2109.07958","DOI":"10.18653/v1/2022.acl-long.229","CorpusId":237532606},"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"95c20f35d352f23b19c378c0758b8dc1d7622872","externalIds":{"MAG":"3192360612","DOI":"10.37282/991819.21.35","CorpusId":238655569},"title":"On Aspects of the Theory of Syntax"},{"paperId":"998628588f7850d533a172c883872057a9198d82","externalIds":{"DBLP":"journals/corr/abs-2107-12979","ArXiv":"2107.12979","CorpusId":236447532},"title":"Predictive Coding: a Theoretical and Experimental Review"},{"paperId":"5951ede73a1dbb9c5ea8b4d95f31c5ed646aacbd","externalIds":{"DBLP":"conf/nips/GeigerLIP21","ArXiv":"2106.02997","CorpusId":235358214},"title":"Causal Abstractions of Neural Networks"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","externalIds":{"DBLP":"journals/ijon/SuALPBL24","ArXiv":"2104.09864","DOI":"10.1016/j.neucom.2023.127063","CorpusId":233307138},"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"2c871df72c52b58f05447fcb3afc838168d94505","externalIds":{"ArXiv":"2104.08696","DBLP":"journals/corr/abs-2104-08696","ACL":"2022.acl-long.581","DOI":"10.18653/v1/2022.acl-long.581","CorpusId":233296761},"title":"Knowledge Neurons in Pretrained Transformers"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","externalIds":{"DBLP":"conf/nips/HendrycksBKABTS21","ArXiv":"2103.03874","CorpusId":232134851},"title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"dd24067c396f4b5a6500a71101ff1dc8ccb8811f","externalIds":{"DBLP":"conf/aaai/PandeBNKK21","ArXiv":"2101.09115","DOI":"10.1609/aaai.v35i15.17605","CorpusId":231693031},"title":"The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in BERT"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","externalIds":{"DBLP":"journals/jmlr/FedusZS22","ArXiv":"2101.03961","CorpusId":231573431},"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"4a54d58a4b20e4f3af25cea3c188a12082a95e02","externalIds":{"DBLP":"conf/emnlp/GevaSBL21","ACL":"2021.emnlp-main.446","ArXiv":"2012.14913","DOI":"10.18653/v1/2021.emnlp-main.446","CorpusId":229923720},"title":"Transformer Feed-Forward Layers Are Key-Value Memories"},{"paperId":"814a4f680b9ba6baba23b93499f4b48af1a27678","externalIds":{"ArXiv":"2009.03300","DBLP":"journals/corr/abs-2009-03300","MAG":"3083410900","CorpusId":221516475},"title":"Measuring Massive Multitask Language Understanding"},{"paperId":"a942cc84412d6a52adab325909b52296c017df48","externalIds":{"MAG":"3096835037","DBLP":"journals/corr/abs-2010-15875","ArXiv":"2010.15875","DOI":"10.24963/ijcai.2020/505","CorpusId":220483148},"title":"Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning"},{"paperId":"644a33399711b31f8a5a1b464f6ffd7c2264fedc","externalIds":{"MAG":"3103949942","DOI":"10.1101/2020.06.26.174482","CorpusId":222359195,"PubMed":"34737231"},"title":"The neural architecture of language: Integrative modeling converges on predictive processing"},{"paperId":"8d908042f139575d6688c745e94156c9df6eae07","externalIds":{"MAG":"3103334733","ArXiv":"2004.08249","DBLP":"journals/corr/abs-2004-08249","ACL":"2020.emnlp-main.463","DOI":"10.18653/v1/2020.emnlp-main.463","CorpusId":215814515},"title":"Understanding the Difficulty of Training Transformers"},{"paperId":"a0cfd36e6c7abf070f492ae52a35af895a1c5592","externalIds":{"MAG":"3010694149","DOI":"10.23915/distill.00024.001","CorpusId":215930358},"title":"Zoom In: An Introduction to Circuits"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","externalIds":{"MAG":"3033156098","CorpusId":226096901},"title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","externalIds":{"MAG":"3006439205","DBLP":"journals/corr/abs-2002-05202","ArXiv":"2002.05202","CorpusId":211096588},"title":"GLU Variants Improve Transformer"},{"paperId":"b45d656ac8cc2e940609580cf291ee76ffcac20a","externalIds":{"DBLP":"conf/icml/XiongYHZZXZLWL20","ArXiv":"2002.04745","MAG":"3034772996","CorpusId":211082816},"title":"On Layer Normalization in the Transformer Architecture"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","externalIds":{"MAG":"3001279689","ArXiv":"2001.08361","DBLP":"journals/corr/abs-2001-08361","CorpusId":210861095},"title":"Scaling Laws for Neural Language Models"},{"paperId":"d78aed1dac6656affa4a04cbf225ced11a83d103","externalIds":{"DBLP":"conf/emnlp/KovalevaRRR19","ArXiv":"1908.08593","ACL":"D19-1445","MAG":"2970120757","DOI":"10.18653/v1/D19-1445","CorpusId":201645145},"title":"Revealing the Dark Secrets of BERT"},{"paperId":"f6390beca54411b06f3bde424fb983a451789733","externalIds":{"MAG":"2970777192","ArXiv":"1909.00015","DBLP":"journals/corr/abs-1909-00015","ACL":"D19-1223","DOI":"10.18653/v1/D19-1223","CorpusId":202538495},"title":"Adaptively Sparse Transformers"},{"paperId":"07a64686ce8e43ac475a8d820a8a9f1d87989583","externalIds":{"MAG":"2951528897","DBLP":"journals/corr/abs-1905-09418","ACL":"P19-1580","ArXiv":"1905.09418","DOI":"10.18653/v1/P19-1580","CorpusId":162183964},"title":"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"},{"paperId":"a8427ce5aee6d62800c725588e89940ed4910e0d","externalIds":{"DBLP":"journals/corr/abs-1904-02874","MAG":"2941531368","ArXiv":"1904.02874","DOI":"10.1145/3465055","CorpusId":102350916},"title":"An Attentive Survey of Attention Models"},{"paperId":"cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022","externalIds":{"MAG":"2954922414","DBLP":"conf/naacl/Koncel-Kedziorski19","ArXiv":"1904.02342","ACL":"N19-1238","DOI":"10.18653/v1/N19-1238","CorpusId":102354588},"title":"Text Generation from Knowledge Graphs with Graph Transformers"},{"paperId":"d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18","externalIds":{"ArXiv":"1902.04094","MAG":"2913129712","ACL":"W19-2304","DBLP":"journals/corr/abs-1902-04094","DOI":"10.18653/v1/W19-2304","CorpusId":60441316},"title":"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model"},{"paperId":"305b2cf37e5dece81e95c92883d5a6e28ac93b22","externalIds":{"DBLP":"conf/emnlp/NarayanCL18","MAG":"2888482885","ArXiv":"1808.08745","ACL":"D18-1206","DOI":"10.18653/v1/D18-1206","CorpusId":215768182},"title":"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"},{"paperId":"e085d5f5088dc8515d2928e06070ad06dee6d557","externalIds":{"MAG":"2899651677","PubMedCentral":"6217996","DOI":"10.1017/pen.2018.16","CorpusId":53242679,"PubMed":"30411087"},"title":"Human Metacognition Across Domains: Insights from Individual Differences and Neuroimaging"},{"paperId":"d7701e78e0bfc92b03a89582e80cfb751ac03f26","externalIds":{"MAG":"2953315354","ArXiv":"1806.00069","DBLP":"conf/dsaa/GilpinBYBSK18","DOI":"10.1109/DSAA.2018.00018","CorpusId":59600034},"title":"Explaining Explanations: An Overview of Interpretability of Machine Learning"},{"paperId":"a002e71561c90767240672f357b7d9e6d4d95186","externalIds":{"DBLP":"journals/corr/MontavonSM17","MAG":"2657631929","ArXiv":"1706.07979","DOI":"10.1016/j.dsp.2017.10.011","CorpusId":207170725},"title":"Methods for interpreting and understanding deep neural networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","externalIds":{"MAG":"2612431505","ArXiv":"1705.03551","ACL":"P17-1147","DBLP":"journals/corr/JoshiCWZ17","DOI":"10.18653/v1/P17-1147","CorpusId":26501419},"title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"815c84ab906e43f3e6322f2ca3fd5e1360c64285","externalIds":{"MAG":"2341708432","DOI":"10.1126/science.aab3050","CorpusId":11790493,"PubMed":"26659050"},"title":"Human-level concept learning through probabilistic program induction"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","externalIds":{"DBLP":"journals/corr/ZhangZL15","MAG":"2963012544","ArXiv":"1509.01626","CorpusId":368182},"title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","externalIds":{"ACL":"D13-1170","DBLP":"conf/emnlp/SocherPWCMNP13","MAG":"2251939518","DOI":"10.18653/v1/d13-1170","CorpusId":990233},"title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"e6e6875f14686490e95fff269ee0cee59a7e733a","externalIds":{"MAG":"1577919095","ArXiv":"1306.0125","DBLP":"journals/corr/Whitehill13","CorpusId":16246658},"title":"Understanding ACT-R - an Outsider's Perspective"},{"paperId":"fda1e13a2eaeaa0b4434833d3ee0eb8e79b0ba94","externalIds":{"MAG":"2026301027","DBLP":"journals/cogsr/WangC10","DOI":"10.1016/j.cogsys.2008.08.003","CorpusId":16238486},"title":"On the cognitive process of human problem solving"},{"paperId":"211e0ff9a6e3f464e932245707e3aaa068aec5eb","externalIds":{"MAG":"2049592515","DBLP":"journals/ijcini/Wang07c","DOI":"10.4018/jcini.2007070105","CorpusId":10863940},"title":"The OAR Model of Neural Informatics for Internal Knowledge Representation in the Brain"},{"paperId":"39a05307d03a528392be8f3c35c45265910579a5","externalIds":{"MAG":"2335988278","DBLP":"journals/sigact/Ross04a","DOI":"10.1145/992287.992306","CorpusId":3183437},"title":"Mental models"},{"paperId":"4fb0ae0075371fa38d78a212d689406fab44c25f","externalIds":{"MAG":"2108984806","DOI":"10.1093/CERCOR/12.8.818","CorpusId":260437630,"PubMed":"12122030"},"title":"Machine psychology: autonomous behavior, perceptual categorization and conditioning in a brain-based device."},{"paperId":"6a6fc284e6e83935e2ededb1b8501aa07a28cf2e","externalIds":{"MAG":"2136816081","DOI":"10.1016/S1364-6613(99)01319-4","CorpusId":7939521,"PubMed":"10354575"},"title":"Models of word production"},{"paperId":"4e6412fc27a6d10555fee95325a9ef556a819c05","externalIds":{"MAG":"2481238518","DOI":"10.2307/2589308","CorpusId":124652071},"title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"37b187e3df04fe7dd31293222407b4b86f3089fb","externalIds":{"DOI":"10.1016/S1364-6613(97)82741-6","CorpusId":208785104},"title":"Attention!"},{"paperId":"ccb5eb4c222e396e738e772f93f97b0c2147af49","externalIds":{"MAG":"2162792036","DOI":"10.4324/9781315806938","CorpusId":61326110},"title":"Rules of the Mind"},{"paperId":"69f6963992e7b82f6c58855af35d661e3fea6036","externalIds":{"MAG":"1984214648","DOI":"10.1037/0033-295X.99.2.195","CorpusId":14104324,"PubMed":"1594723"},"title":"Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans."},{"paperId":"289d3a9562f57d0182d1aae9376b0e3793d80272","externalIds":{"MAG":"2084015864","DOI":"10.1037/0033-295X.95.2.163","CorpusId":15246663,"PubMed":"3375398"},"title":"The role of knowledge in discourse comprehension: a construction-integration model."},{"paperId":"56019756e85646883855e3583523317de465af42","externalIds":{"DBLP":"journals/corr/abs-2404-07066","DOI":"10.48550/arXiv.2404.07066","CorpusId":269033222},"title":"Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?"},{"paperId":"31048d9161dad19723b5c0733c7da0ea8b592e8f","externalIds":{"DBLP":"journals/corr/abs-2407-15018","DOI":"10.48550/arXiv.2407.15018","CorpusId":271328730},"title":"Answer, Assemble, Ace: Understanding How Transformers Answer Multiple Choice Questions"},{"paperId":"e6c6836da35dcd93639e49095af1c3191a3b4ce0","externalIds":{"DBLP":"journals/corr/abs-2405-11874","DOI":"10.48550/arXiv.2405.11874","CorpusId":280635833},"title":"xFinder: Robust and Pinpoint Answer Extraction for Large Language Models"},{"paperId":"e661de406d8105e52a5351a2cd66db84cc4af115","externalIds":{"DBLP":"journals/corr/abs-2303-13988","DOI":"10.48550/arXiv.2303.13988","CorpusId":257757370},"title":"Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods"},{"paperId":"78d08b8ab4132defffe98ec7f80a51452203f70d","externalIds":{"DBLP":"conf/nips/VigGBQNSS20","MAG":"3104142662","CorpusId":227275068},"title":"Investigating Gender Bias in Language Models Using Causal Mediation Analysis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4c5f22e071387cfb4f5474ac29487d32f83c9017","externalIds":{"CorpusId":3833303},"title":"Author contributions"},{"paperId":"94238dead40b12735d79ed63e29ead70730261a2","externalIds":{"MAG":"2912351236","ACL":"W18-5431","DBLP":"conf/emnlp/RaganatoT18","DOI":"10.18653/v1/W18-5431","CorpusId":53596423},"title":"An Analysis of Encoder Representations in Transformer-Based Machine Translation"},{"paperId":"4a389a347c99f986cf45500540936c96a4dd5b4a","externalIds":{"DOI":"10.1146/annurev.psych.54.101601.145124","CorpusId":267918820,"PubMed":"12415075"},"title":"Operant conditioning."},{"paperId":"50922627d8e93d3eb34562360fec2823c5998c99","externalIds":{"MAG":"2004286722","CorpusId":273576153},"title":"The Number Sense: How the Mind Creates Mathematics"},{"paperId":"d792562462dbb687015954805d31620240db57a1","externalIds":{"MAG":"3041214984","DOI":"10.4135/9781446212967.n15","CorpusId":140830322},"title":"Episodic and semantic memory"},{"paperId":"78272e671bd859f777b09851d29b2542f4ad070b","externalIds":{"CorpusId":274529972},"title":"On the Difficulty of Faithful Chain-of-Thought Reasoning in Large Language Models"}]}