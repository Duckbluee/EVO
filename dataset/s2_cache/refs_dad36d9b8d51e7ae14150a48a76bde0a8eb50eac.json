{"references":[{"paperId":"05ef276b965fd51178a8cc9490f4f0f503649037","externalIds":{"DOI":"10.1257/pandp.20181018","CorpusId":261427620},"title":"Algorithmic Fairness"},{"paperId":"b7eb32ed73688d8bdf28f4d1307fffdff5d5d2d6","externalIds":{"DBLP":"journals/ipm/ShenLBMS23","DOI":"10.1016/j.ipm.2022.103139","CorpusId":253618574},"title":"Towards understanding and mitigating unintended biases in language model-driven conversational recommendation"},{"paperId":"c1db510e41ded424542f141681316e05e506ea05","externalIds":{"ArXiv":"2209.08648","DBLP":"conf/hci/RajabiYGS23","DOI":"10.48550/arXiv.2209.08648","CorpusId":252368107},"title":"Through a fair looking-glass: mitigating bias in image datasets"},{"paperId":"67ea3bbea7be29df77190a4024872fd64c9bd0fb","externalIds":{"DBLP":"conf/eccv/DuHBHG22","ArXiv":"2208.10013","DOI":"10.48550/arXiv.2208.10013","CorpusId":251719548},"title":"FairDisCo: Fairer AI in Dermatology via Disentanglement Contrastive Learning"},{"paperId":"2d69f026f76ab9ee9e609ef888e19bb8498f61b9","externalIds":{"DBLP":"journals/corr/abs-2207-10653","ArXiv":"2207.10653","DOI":"10.48550/arXiv.2207.10653","CorpusId":250918055},"title":"RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping"},{"paperId":"27eec716b3d5f2253ced3d6a30eaaa5558ae94d6","externalIds":{"DBLP":"conf/sigir/ZerveasRCE22","DOI":"10.1145/3477495.3531891","CorpusId":248837769},"title":"Mitigating Bias in Search Results Through Contextual Document Reranking and Neutrality Regularization"},{"paperId":"1f18b9f52f84fcfc849215fa1d70c19c007b6fe3","externalIds":{"DBLP":"journals/sensors/KangKY22","PubMedCentral":"9317083","DOI":"10.3390/s22145271","CorpusId":250587893,"PubMed":"35890949"},"title":"Fair Facial Attribute Classification via Causal Graph-Based Attribute Translation"},{"paperId":"52808c881730c907332eb04b7b89133f8145a69e","externalIds":{"ArXiv":"2205.12586","ACL":"2022.emnlp-main.646","DBLP":"journals/corr/abs-2205-12586","DOI":"10.48550/arXiv.2205.12586","CorpusId":249062690},"title":"Perturbation Augmentation for Fairer NLP"},{"paperId":"1ff7224423a72052eea7df484b3806e7bd6ec0a7","externalIds":{"DBLP":"journals/corr/abs-2205-10762","ArXiv":"2205.10762","DOI":"10.48550/arXiv.2205.10762","CorpusId":248986922},"title":"How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts"},{"paperId":"2775900e7633f218618f9f7ca5cfda984fae323d","externalIds":{"DBLP":"journals/corr/abs-2205-09240","ACL":"2022.gebnlp-1.5","ArXiv":"2205.09240","DOI":"10.48550/arXiv.2205.09240","CorpusId":248887448},"title":"Debiasing Neural Retrieval via In-batch Balancing Regularization"},{"paperId":"9b78140aa26f738a7a225d505ad34023ff3b6d61","externalIds":{"DBLP":"journals/corr/abs-2205-06135","ArXiv":"2205.06135","DOI":"10.48550/arXiv.2205.06135","CorpusId":247745203},"title":"Fair NLP Models with Differentially Private Text Encoders"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","externalIds":{"DBLP":"journals/corr/abs-2205-01068","ArXiv":"2205.01068","CorpusId":248496292},"title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"95b855541bce7c1c71fd3d51a58e0559422390bb","externalIds":{"DBLP":"journals/ijon/KimC22a","DOI":"10.1016/j.neucom.2021.09.081","CorpusId":248873596},"title":"An information theoretic approach to reducing algorithmic bias for machine learning"},{"paperId":"dc37a8c6b415c464db2ae27677d5e829008c0de5","externalIds":{"DBLP":"conf/sac/GaciBCB22","DOI":"10.1145/3477314.3507274","CorpusId":248545735},"title":"Iterative adversarial removal of gender bias in pretrained word embeddings"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","externalIds":{"ACL":"2022.bigscience-1.9","DBLP":"journals/corr/abs-2204-06745","ArXiv":"2204.06745","DOI":"10.48550/arXiv.2204.06745","CorpusId":248177957},"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","externalIds":{"ArXiv":"2204.06125","DBLP":"journals/corr/abs-2204-06125","DOI":"10.48550/arXiv.2204.06125","CorpusId":248097655},"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"d9e393b15831394687f8873a743470e672eb34fa","externalIds":{"DBLP":"journals/ijon/OdbalZA22","DOI":"10.1016/j.neucom.2022.04.057","CorpusId":248197670},"title":"Examining and mitigating gender bias in text emotion detection task"},{"paperId":"50d0933d114d14be8d27339ddfc7206cffb89bbf","externalIds":{"DBLP":"journals/fi/PatilP22","DOI":"10.3390/fi14040110","CorpusId":247878946},"title":"Decorrelation-Based Deep Learning for Bias Mitigation"},{"paperId":"72a6b51c492aa9333b857477ff19f76c37053aa9","externalIds":{"DBLP":"journals/corr/abs-2203-15395","ArXiv":"2203.15395","DOI":"10.1109/CVPR52688.2022.01309","CorpusId":247778463},"title":"Quantifying Societal Bias Amplification in Image Captioning"},{"paperId":"6bdebf88b07e22f9955f9b06590e8ff030697299","externalIds":{"DBLP":"journals/nca/TianZLZ22","DOI":"10.1007/s00521-022-07136-1","CorpusId":247723355},"title":"Image fairness in deep learning: problems, models, and challenges"},{"paperId":"3c759e2f16bfde8d31189631e4893d3ac8ff05f2","externalIds":{"ArXiv":"2203.12574","DBLP":"journals/corr/abs-2203-12574","ACL":"2022.findings-acl.55","DOI":"10.48550/arXiv.2203.12574","CorpusId":247619104},"title":"Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal"},{"paperId":"efabdd27929796b712cb1b3a3051ea5358dc1200","externalIds":{"DBLP":"conf/ijcnlp/BergHBKSB22","ArXiv":"2203.11933","ACL":"2022.aacl-main.61","DOI":"10.48550/arXiv.2203.11933","CorpusId":247596835},"title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning"},{"paperId":"eba32a2130108261baba4552a0405b30143dff3c","externalIds":{"ArXiv":"2203.10675","DBLP":"journals/corr/abs-2203-10675","DOI":"10.48550/arXiv.2203.10675","CorpusId":247594904},"title":"Mitigating Gender Bias in Machine Translation through Adversarial Learning"},{"paperId":"7335fbc509da851b9d141a40e6463b5e82dea01c","externalIds":{"ArXiv":"2204.09591","DBLP":"journals/corr/abs-2204-09591","DOI":"10.48550/arXiv.2204.09591","CorpusId":248266397},"title":"A Survey on Bias and Fairness in Natural Language Processing"},{"paperId":"2714740aacc642cb273f7eb68e41f78d7fbfc1d5","externalIds":{"DBLP":"journals/corr/abs-2203-02110","ArXiv":"2203.02110","DOI":"10.48550/arXiv.2203.02110","CorpusId":247244624},"title":"FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis"},{"paperId":"29e3ff317a1a053939af736abd158cdd91577e4e","externalIds":{"DBLP":"journals/corr/abs-2202-09662","ArXiv":"2202.09662","DOI":"10.1007/s10489-022-03944-z","CorpusId":247012038},"title":"Reward modeling for mitigating toxicity in transformer-based language models"},{"paperId":"6b1e71bc8d511d8a75f0f5fd05fc9c21d781363c","externalIds":{"DBLP":"journals/corr/abs-2202-09275","ArXiv":"2202.09275","CorpusId":246996702},"title":"Rethinking Pareto Frontier for Performance Evaluation of Deep Neural Networks"},{"paperId":"6ec7a6c0cd8679e64792f38c61f79a624a1268d4","externalIds":{"DBLP":"journals/corr/abs-2202-06240","ArXiv":"2202.06240","DOI":"10.1007/978-3-031-19778-9_33","CorpusId":246823309},"title":"FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations"},{"paperId":"212732c649d84382f4e74ca047b13f3c835591d7","externalIds":{"ArXiv":"2202.04053","DBLP":"conf/iccv/0001ZB23","DOI":"10.1109/ICCV51070.2023.00283","CorpusId":253510037},"title":"DALL-EVAL: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models"},{"paperId":"29322b01fc4f9679075df44539e8bc86f70bd8f6","externalIds":{"DBLP":"journals/corr/abs-2201-06224","ArXiv":"2201.06224","CorpusId":246016280},"title":"Unintended Bias in Language Model-driven Conversational Recommendation"},{"paperId":"397cf82a2257549728292b5ec19fceeacd511791","externalIds":{"DBLP":"conf/wacv/TanjimSSMASC22","DOI":"10.1109/WACV51458.2022.00396","CorpusId":243797442},"title":"Generating and Controlling Diversity in Image Search"},{"paperId":"67ad491b16bf77e9a54a8b8b1dc23dadc5545467","externalIds":{"ArXiv":"2112.07447","DBLP":"journals/corr/abs-2112-07447","CorpusId":245131370},"title":"Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models"},{"paperId":"93cb543e9e5ffc99e0fb0b89c62e4554dbeb8c92","externalIds":{"DBLP":"journals/corr/abs-2112-05700","ArXiv":"2112.05700","CorpusId":245117903},"title":"A Framework for Fairness: A Systematic Review of Existing Fair AI Solutions"},{"paperId":"924940e72ca1ba1630c9d08f88292c1d0a8d4d99","externalIds":{"DBLP":"journals/corr/abs-2111-03638","ArXiv":"2111.03638","DOI":"10.32473/flairs.36.133311","CorpusId":243832683},"title":"Increasing Fairness in Predictions Using Bias Parity Score Based Loss Function Regularization"},{"paperId":"01a05a104b83cc2f0e2e8487dcfe81e0818f3ac1","externalIds":{"ArXiv":"2111.01705","DBLP":"journals/corr/abs-2111-01705","DOI":"10.1145/3531146.3533780","CorpusId":240420004},"title":"AI Ethics Statements: Analysis and Lessons Learnt from NeurIPS Broader Impact Statements"},{"paperId":"a5c7bd7ccd2e3db114dd467304eb4e6e928c0ef8","externalIds":{"DBLP":"journals/corr/abs-2110-10389","ArXiv":"2110.10389","DOI":"10.1109/WACV51458.2022.00395","CorpusId":239049762},"title":"Does Data Repair Lead to Fair Models? Curating Contextually Fair Data To Reduce Model Bias"},{"paperId":"de6807676d8171472ed6cf421c4e4ed3cbb47699","externalIds":{"ArXiv":"2110.08527","ACL":"2022.acl-long.132","DBLP":"journals/corr/abs-2110-08527","DOI":"10.18653/v1/2022.acl-long.132","CorpusId":239015827},"title":"An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models"},{"paperId":"01c39795715404593230cb0f75007b48f156039f","externalIds":{"ACL":"2023.acl-short.108","DBLP":"conf/acl/FatemiXLX23","ArXiv":"2110.05367","DOI":"10.18653/v1/2023.acl-short.108","CorpusId":238582879},"title":"Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting"},{"paperId":"ebb0df8229e7dcf47f79c96e77b37e6fa3dae485","externalIds":{"DBLP":"journals/corr/abs-2110-00530","ArXiv":"2110.00530","DOI":"10.1002/widm.1452","CorpusId":238252945},"title":"A survey on datasets for fairness‚Äêaware machine learning"},{"paperId":"6adb6d0c65eda79494772392de23773cef792e2d","externalIds":{"DBLP":"journals/corr/abs-2109-13767","ArXiv":"2109.13767","CorpusId":238198294},"title":"Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings"},{"paperId":"92e7aea7704fb1daf2f733bdfecfd523ebd3c034","externalIds":{"DBLP":"journals/corr/abs-2109-10645","ArXiv":"2109.10645","CorpusId":237593031},"title":"Contrastive Learning for Fair Representations"},{"paperId":"258e5b18b0f2bb027080ed347e856e6940a20ce5","externalIds":{"DBLP":"conf/emnlp/SubramanianHBCF21","ACL":"2021.emnlp-main.193","ArXiv":"2109.10441","DOI":"10.18653/v1/2021.emnlp-main.193","CorpusId":237593027},"title":"Evaluating Debiasing Techniques for Intersectional Biases"},{"paperId":"60c498956cb5737c4964aaca0b920592bd7f5689","externalIds":{"DBLP":"conf/emnlp/WangLW21","ACL":"2021.emnlp-main.151","ArXiv":"2109.05433","DOI":"10.18653/v1/2021.emnlp-main.151","CorpusId":237490811},"title":"Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search"},{"paperId":"130ab5c480860e330b65280a3410f17bb2d50fe1","externalIds":{"DBLP":"conf/emnlp/LauscherLG21","ArXiv":"2109.03646","DOI":"10.18653/v1/2021.findings-emnlp.411","CorpusId":237440429},"title":"Sustainable Modular Debiasing of Language Models"},{"paperId":"e4a9c8f91acdc0620e25adce9d67f49c14e990b9","externalIds":{"DBLP":"journals/tcss/WuDFTYMH22","MAG":"3198214341","DOI":"10.1109/tcss.2021.3106003","CorpusId":239647104},"title":"Understanding Social Biases Behind Location Names in Contextual Word Embedding Models"},{"paperId":"5f8df3300b2051178ed2070c4ec9bd7a5fd6c59b","externalIds":{"DBLP":"journals/corr/abs-2108-08504","ArXiv":"2108.08504","DOI":"10.1109/ICCV48922.2021.01471","CorpusId":237213296},"title":"Understanding and Mitigating Annotation Bias in Facial Expression Recognition"},{"paperId":"76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2","externalIds":{"DBLP":"journals/corr/abs-2108-07258","ArXiv":"2108.07258","CorpusId":237091588},"title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"564c7e51f930b208cf05f68a8c478d1195ccad05","externalIds":{"DBLP":"journals/corr/abs-2108-04983","ArXiv":"2108.04983","CorpusId":236975921},"title":"Learning Fair Face Representation With Progressive Cross Transformer"},{"paperId":"ee580dddd8c8eddfcb2d1570c3310096c67a897f","externalIds":{"ArXiv":"2107.00067","DBLP":"conf/wacv/MazumderSN22","DOI":"10.1109/WACV51458.2022.00394","CorpusId":235694419},"title":"Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation"},{"paperId":"fa4e3c66649f354eac688ea95067b0954b5a274b","externalIds":{"ArXiv":"2106.14829","DBLP":"journals/corr/abs-2106-14829","CorpusId":235658913},"title":"Dataset Bias Mitigation Through Analysis of CNN Training Scores"},{"paperId":"1fb1edc2263910e325388e879b36bc949fe1cdcf","externalIds":{"ArXiv":"2106.13382","DBLP":"journals/corr/abs-2106-13382","CorpusId":235652274},"title":"A Source-Criticism Debiasing Method for GloVe Embeddings"},{"paperId":"114aa720872462b0ca1b97bfdec0ebd56c36fd0a","externalIds":{"ArXiv":"2106.13219","DBLP":"conf/icml/LiangWMS21","CorpusId":235623756},"title":"Towards Understanding and Mitigating Social Biases in Language Models"},{"paperId":"5f0f4a3fa3cff7ffcedabbc9ed0dad2dd71f7028","externalIds":{"ArXiv":"2106.05945","DBLP":"conf/nips/StantonIKAW21","CorpusId":235390933},"title":"Does Knowledge Distillation Really Work?"},{"paperId":"3cf69ac8cae3d365db9139faee93d1631f42b5ed","externalIds":{"ArXiv":"2106.03761","DBLP":"conf/iclr/SalvadorCVMO22","CorpusId":238531802},"title":"FairCal: Fairness Calibration for Face Verification"},{"paperId":"fb537697b227b05cf94da34637f287a918b77dda","externalIds":{"ArXiv":"2106.02866","CorpusId":250089469},"title":"Conditional Contrastive Learning for Improving Fairness in Self-Supervised Learning"},{"paperId":"df47dec4be311dc8fb330fa239f1a3ef85ebddc9","externalIds":{"DBLP":"conf/aies/ShahGDB21","ArXiv":"2105.14890","DOI":"10.1145/3461702.3462592","CorpusId":235254698},"title":"Rawlsian Fair Adaptation of Deep Learning Classifiers"},{"paperId":"c502b3d801a687ad26f592aa3416a6b2520d0f7c","externalIds":{"ArXiv":"2106.04411","DBLP":"journals/corr/abs-2106-04411","DOI":"10.1109/CVPR46437.2021.01194","CorpusId":235368045},"title":"Fair Feature Distillation for Visual Recognition"},{"paperId":"9d02d4d5f4dc33928dc21eae402e5200d1cb2e75","externalIds":{"DBLP":"conf/aaai/ParkH0B21","DOI":"10.1609/aaai.v35i3.16341","CorpusId":235306405},"title":"Learning Disentangled Representation for Fair Facial Attribute Classification via Fairness-aware Information Alignment"},{"paperId":"17f357f3c6fdd86f7e8a141d1e3b9acb2e59a89a","externalIds":{"DBLP":"conf/aies/NanayakkaraHD21","ArXiv":"2105.04760","DOI":"10.1145/3461702.3462608","CorpusId":234358073},"title":"Unpacking the Expressed Consequences of AI Research in Broader Impact Statements"},{"paperId":"7eb76290929d644bb075bf573d495a23071831f9","externalIds":{"DBLP":"conf/chi/RichardsonGWTC21","DOI":"10.1145/3411764.3445604","CorpusId":233987727},"title":"Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits"},{"paperId":"60f016adc29f6f2e9b29031c9c2e8cbea00774d6","externalIds":{"ArXiv":"2104.14795","DBLP":"conf/aaai/LiuJWXWV21","DOI":"10.1609/aaai.v35i17.17744","CorpusId":233476528},"title":"Mitigating Political Bias in Language Models Through Reinforced Calibration"},{"paperId":"d867b7dca02c97ef7fe92fc0da83dbec77bfc6c6","externalIds":{"ArXiv":"2104.13640","DBLP":"conf/sigir/RekabsazKS21","DOI":"10.1145/3404835.3462949","CorpusId":233423485},"title":"Societal Biases in Retrieved Contents: Measurement Framework and Adversarial Mitigation of BERT Rankers"},{"paperId":"437727b6c00a5eb4944600091f66f41626d1002d","externalIds":{"ArXiv":"2104.07496","DBLP":"journals/corr/abs-2104-07496","DOI":"10.1609/aaai.v36i11.21453","CorpusId":233241161},"title":"Unmasking the Mask - Evaluating Social Biases in Masked Language Models"},{"paperId":"9adfa9306a3beef2d978220ac01b43ace0bf4206","externalIds":{"DBLP":"journals/corr/abs-2104-06973","ArXiv":"2104.06973","CorpusId":233231360},"title":"[RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation"},{"paperId":"289c34e7c5dfa1ab9ce2b7fe4dd8e3df1a15b7d7","externalIds":{"DBLP":"conf/icdm/RuchteG21","ArXiv":"2103.13392","DOI":"10.1109/ICDM51629.2021.00162","CorpusId":238856652},"title":"Scalable Pareto Front Approximation for Deep Multi-Objective Learning"},{"paperId":"dd7c8b557350cb4d88c77ff2b34e08573c68f053","externalIds":{"DBLP":"journals/cacm/FriedlerSV21","DOI":"10.1145/3433949","CorpusId":1769114},"title":"The (Im)possibility of fairness"},{"paperId":"df157cb42b574c3f46b269504c18375bfa5bc5b1","externalIds":{"DBLP":"journals/corr/abs-2103-06413","ArXiv":"2103.06413","CorpusId":232185104},"title":"FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders"},{"paperId":"e6cf7c8a624d85448745d11f9b01011506db032e","externalIds":{"DBLP":"conf/cvpr/YangZQ021","ArXiv":"2103.03493","DOI":"10.1109/CVPR46437.2021.00972","CorpusId":232135026},"title":"Causal Attention for Vision-Language Tasks"},{"paperId":"ca2f1088d3e581b2c6c75cf0ebc96506d620f64d","externalIds":{"DBLP":"conf/fat/BenderGMS21","DOI":"10.1145/3442188.3445922","CorpusId":262580630},"title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú"},{"paperId":"c0e6cd2ec3bc9eb46c7d45bb708854da3327339e","externalIds":{"MAG":"3134678353","DOI":"10.20944/PREPRINTS202103.0049.V1","CorpusId":233832881},"title":"A Survey on Bias in Deep NLP"},{"paperId":"3d8ff14c93a29f24d9dfb0cf908d9968d83126e4","externalIds":{"ArXiv":"2103.02023","DBLP":"conf/cvpr/TartaglioneBG21","DOI":"10.1109/CVPR46437.2021.01330","CorpusId":232105100},"title":"EnD: Entangling and Disentangling deep representations for bias correction"},{"paperId":"ce9ca56036307217ea565644d3d3bd74b879e045","externalIds":{"DBLP":"journals/tacl/SchickUS21","ArXiv":"2103.00453","DOI":"10.1162/tacl_a_00434","CorpusId":232075876},"title":"Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","externalIds":{"DBLP":"conf/chi/ReynoldsM21","ArXiv":"2102.07350","DOI":"10.1145/3411763.3451760","CorpusId":231925131},"title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"b41e07349b87a178d904e6b5d05a2f90b16f8e1e","externalIds":{"ArXiv":"2102.04130","DBLP":"conf/nips/KirkJVIBDSA21","CorpusId":236950797},"title":"Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models"},{"paperId":"1149827da438a8507e7e25ffa5877be54ee03810","externalIds":{"ArXiv":"2106.11039","DBLP":"journals/natmi/PrunklAAWLD21","MAG":"3129706735","DOI":"10.1038/s42256-021-00298-y","CorpusId":233966037},"title":"Institutionalizing ethics in AI through broader impact requirements"},{"paperId":"29f0d5b9c6a653b4ff1d9a811d94beba21d03043","externalIds":{"DOI":"10.3233/BMR-1996-6212","CorpusId":60547,"PubMed":"24572445"},"title":"Call for papers."},{"paperId":"61ca0040d81c5ed71d3f9b9e5f7b528275048440","externalIds":{"ArXiv":"2101.09523","DBLP":"conf/eacl/KanekoB21a","ACL":"2021.eacl-main.107","DOI":"10.18653/v1/2021.eacl-main.107","CorpusId":231698657},"title":"Debiasing Pre-trained Contextualised Embeddings"},{"paperId":"3713672a73ddccbc75e24e99cde182a78772da21","externalIds":{"ArXiv":"2101.09525","DBLP":"conf/eacl/KanekoB21","ACL":"2021.eacl-main.16","DOI":"10.18653/v1/2021.eacl-main.16","CorpusId":231698423},"title":"Dictionary-based Debiasing of Pre-trained Word Embeddings"},{"paperId":"aee9693db23c63afd9ead39e0fe06ca4000c52dd","externalIds":{"MAG":"3108989460","DBLP":"journals/corr/abs-2012-01469","ArXiv":"2012.01469","DOI":"10.1109/CVPR46437.2021.00918","CorpusId":227253684},"title":"Fair Attribute Classification through Latent Space De-biasing"},{"paperId":"a7bb20f6d5d07c2043712f9b17aa1150aedec362","externalIds":{"ArXiv":"2011.11878","DBLP":"journals/corr/abs-2011-11878","MAG":"3110081891","DOI":"10.1609/aaai.v35i9.16990","CorpusId":227151686},"title":"Counterfactual Fairness with Disentangled Causal Effect Variational Autoencoder"},{"paperId":"7dbe56c7707d3eb90caf90f4e120b88906151c05","externalIds":{"MAG":"3101118213","DOI":"10.18653/v1/2020.findings-emnlp","CorpusId":285072669},"title":"Findings of the Association for Computational Linguistics: EMNLP 2020"},{"paperId":"ba212de1258aa800f147046ef91f5e0eede3292c","externalIds":{"DBLP":"conf/icmi/YanHS20","MAG":"3094268584","DOI":"10.1145/3382507.3418889","CorpusId":224817491},"title":"Mitigating Biases in Multimodal Personality Assessment"},{"paperId":"8b9fcd46ffe309a2eaa8eb675940191f5d41744c","externalIds":{"MAG":"3093000437","DBLP":"conf/wmt/StafanovicsPB20","ACL":"2020.wmt-1.73","ArXiv":"2010.06203","DOI":"10.18653/v1/2020.wmt-1.73","CorpusId":222310217},"title":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations"},{"paperId":"3d864a8bc5a55ccab9993aa66203d8e70b88148c","externalIds":{"ArXiv":"2010.06032","MAG":"3093211917","DBLP":"journals/corr/abs-2010-06032","CorpusId":222310622},"title":"Measuring and Reducing Gendered Correlations in Pre-trained Models"},{"paperId":"fee8f63972906214b77f16cfeca0b93ee8f36ba2","externalIds":{"DBLP":"journals/csur/CatonH24","MAG":"3092541244","ArXiv":"2010.04053","DOI":"10.1145/3616865","CorpusId":222208640},"title":"Fairness in Machine Learning: A Survey"},{"paperId":"645bd6eadc247989abc5e0b0aa0be79ec8b11ea6","externalIds":{"MAG":"3089430725","DBLP":"journals/corr/abs-2010-00133","ArXiv":"2010.00133","ACL":"2020.emnlp-main.154","DOI":"10.18653/v1/2020.emnlp-main.154","CorpusId":222090785},"title":"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","externalIds":{"MAG":"3088599783","ACL":"2020.findings-emnlp.301","DBLP":"journals/corr/abs-2009-11462","ArXiv":"2009.11462","DOI":"10.18653/v1/2020.findings-emnlp.301","CorpusId":221878771},"title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"56fdad6964cf6da35d836f9b4777ad0f94c2b487","externalIds":{"MAG":"3101934021","ACL":"2020.emnlp-main.232","DBLP":"conf/emnlp/VargasC20","ArXiv":"2009.09435","DOI":"10.18653/v1/2020.emnlp-main.232","CorpusId":221819206},"title":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation"},{"paperId":"63ce0fa559437b4118b63953428adf284464eedd","externalIds":{"DBLP":"conf/wacv/DashB022","ArXiv":"2009.08270","DOI":"10.1109/WACV51458.2022.00393","CorpusId":245769976},"title":"Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals"},{"paperId":"fce68e178987d2daf4a4a6310f0e79553d950c91","externalIds":{"DBLP":"conf/wacv/CaoBTAL0SE22","ArXiv":"2009.05283","DOI":"10.1109/WACV51458.2022.00292","CorpusId":243938300},"title":"Fair and accurate age prediction using distribution aware data curation and augmentation"},{"paperId":"74f23063ca77f5b1caa3770a5957ae5fc565843e","externalIds":{"DBLP":"journals/corr/abs-2009-09796","ArXiv":"2009.09796","MAG":"3087549734","CorpusId":221819295},"title":"Multi-Task Learning with Deep Neural Networks: A Survey"},{"paperId":"ddc0c5f697b84d501bc6ad7e60d396596dfc7ae4","externalIds":{"ArXiv":"2008.07433","MAG":"3106489865","DBLP":"journals/corr/abs-2008-07433","DOI":"10.1145/3340531.3412705","CorpusId":221139572},"title":"LiFT: A Scalable Framework for Measuring Fairness in ML Applications"},{"paperId":"5366470340d4741fe17cd0db64d5d79651cc6308","externalIds":{"DBLP":"journals/corr/abs-2007-10075","MAG":"3042235754","ArXiv":"2007.10075","DOI":"10.1007/978-3-030-65414-6_35","CorpusId":220647333},"title":"Investigating Bias and Fairness in Facial Expression Recognition"},{"paperId":"0d965ed237a3b4592ecefdb618c29f63adedff76","externalIds":{"ArXiv":"2007.08100","ACL":"2020.acl-main.488","DBLP":"conf/acl/LiangLZLSM20","MAG":"3042791954","DOI":"10.18653/v1/2020.acl-main.488","CorpusId":207996257},"title":"Towards Debiasing Sentence Representations"},{"paperId":"b99bd19bed56de300dfa241b9698a35e95b95925","externalIds":{"ArXiv":"2011.01821","MAG":"3095753892","DBLP":"conf/icml/MartinezBS20","CorpusId":220683906,"PubMed":"33644764"},"title":"Minimax Pareto Fairness: A Multi Objective Perspective"},{"paperId":"a61854cb81538a2edd667d84b6127d6b8b4501b2","externalIds":{"DBLP":"journals/corr/abs-2006-16742","ArXiv":"2006.16742","MAG":"3038896787","DOI":"10.1609/aaai.v35i5.16573","CorpusId":220266008},"title":"Fairness-aware News Recommendation with Decomposed Adversarial Learning"},{"paperId":"09fba948316e04f0e1641926948b67b0799c8e0d","externalIds":{"DBLP":"journals/corr/abs-2007-00049","ACL":"2021.emnlp-main.411","MAG":"3039559565","ArXiv":"2007.00049","DOI":"10.18653/v1/2021.emnlp-main.411","CorpusId":220281039},"title":"OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings"},{"paperId":"a6efd9a6bc72326d199bd4366d23596cfd089c2d","externalIds":{"ArXiv":"2006.11642","DBLP":"conf/sbp-brims/DuJ20","MAG":"3036338130","DOI":"10.1007/978-3-030-61255-9_5","CorpusId":219965895},"title":"MDR Cluster-Debias: A Nonlinear WordEmbedding Debiasing Pipeline"},{"paperId":"922ca07d02384c9b8e807912f3b64e48c36a2357","externalIds":{"DBLP":"journals/corr/abs-2006-08315","MAG":"3034651947","ArXiv":"2006.08315","DOI":"10.1145/3442381.3449950","CorpusId":219687817},"title":"Mitigating Gender Bias in Captioning Systems"},{"paperId":"3e3f55cb25b919c4e8158195fd3ce2f23cfa7723","externalIds":{"ArXiv":"2006.04315","DBLP":"journals/corr/abs-2006-04315","MAG":"3033665206","DOI":"10.1109/CVPR46437.2021.01251","CorpusId":219530451},"title":"Counterfactual VQA: A Cause-Effect Look at Language Bias"},{"paperId":"7ea0e91c5d5dc73f2133bc46d7ebb6cb83034dae","externalIds":{"DBLP":"conf/aies/GuoC21","MAG":"3034115845","ArXiv":"2006.03955","DOI":"10.1145/3461702.3462536","CorpusId":219530686},"title":"Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases"},{"paperId":"c2ecc9073672a8d8bf21fe442dbf3f76356858ee","externalIds":{"MAG":"3034021819","ArXiv":"2006.01938","DBLP":"journals/tacl/KumarBKC20","DOI":"10.1162/tacl_a_00327","CorpusId":219260204},"title":"Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings"},{"paperId":"fd1201b44d28f6c2d722c8c4faada6bb82d79ebf","externalIds":{"MAG":"3034747430","DBLP":"conf/cvpr/LiZL20","DOI":"10.1109/cvpr42600.2020.00909","CorpusId":219963282},"title":"Deep Fair Clustering for Visual Learning"},{"paperId":"046f1c5cc8c7b4f24ab62a536d8b0a989209824b","externalIds":{"DBLP":"conf/cvpr/AbbasnejadTPSH20","MAG":"3035145964","DOI":"10.1109/CVPR42600.2020.01006","CorpusId":219965288},"title":"Counterfactual Vision and Language Learning"},{"paperId":"d47a682723f710395454687319bb55635e653105","externalIds":{"DBLP":"journals/corr/abs-2005-14050","MAG":"3032388710","ArXiv":"2005.14050","ACL":"2020.acl-main.485","DOI":"10.18653/v1/2020.acl-main.485","CorpusId":218971825},"title":"Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"5894d57ea49bd5c136ebefb1e6c3986555908ea0","externalIds":{"MAG":"3030081171","CorpusId":219772478},"title":"Fairlearn: A toolkit for assessing and improving fairness in AI"},{"paperId":"64a3fd2b73720467ffb29b2da85f5e9073842bfc","externalIds":{"MAG":"2995214047","ACL":"2020.acl-main.484","ArXiv":"2005.00965","DBLP":"journals/corr/abs-2005-00965","DOI":"10.18653/v1/2020.acl-main.484","CorpusId":204770514},"title":"Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation"},{"paperId":"f0246cf8b98251ae219d4e536a554fd0237b19a9","externalIds":{"MAG":"3099635335","ACL":"2020.findings-emnlp.291","DBLP":"journals/corr/abs-2005-00268","ArXiv":"2005.00268","DOI":"10.18653/v1/2020.findings-emnlp.291","CorpusId":218470535},"title":"Towards Controllable Biases in Language Generation"},{"paperId":"00e6834700e9805cb1618433f05915c261a6ba08","externalIds":{"MAG":"3085046840","DBLP":"journals/pami/VandenhendeGGPD22","DOI":"10.1109/TPAMI.2021.3054719","CorpusId":221771219,"PubMed":"33497328"},"title":"Multi-Task Learning for Dense Prediction Tasks: A Survey"},{"paperId":"82b07b79e77089439526eda8d3202282a3504b9f","externalIds":{"DBLP":"journals/corr/abs-2004-11246","ArXiv":"2004.11246","MAG":"3018301625","DOI":"10.1016/j.artint.2022.103682","CorpusId":216080550},"title":"SensitiveLoss: Improving Accuracy and Fairness of Face Representations with Discrimination-Aware Deep Learning"},{"paperId":"58bb221c1e375f254826b7b7341f74057e87676c","externalIds":{"DBLP":"conf/chi/MadaioSVW20","MAG":"3014972121","DOI":"10.1145/3313831.3376445","CorpusId":210156214},"title":"Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI"},{"paperId":"babeda48b10a4d638252118f2238d05a06f4ec55","externalIds":{"ACL":"2021.acl-long.416","DBLP":"journals/corr/abs-2004-09456","MAG":"3019416653","ArXiv":"2004.09456","DOI":"10.18653/v1/2021.acl-long.416","CorpusId":215828184},"title":"StereoSet: Measuring stereotypical bias in pretrained language models"},{"paperId":"a05cb60388e1d7cbf399422b2f9bc954219021c9","externalIds":{"MAG":"3016300649","DBLP":"conf/cvpr/YucerAMB20","ArXiv":"2004.08945","DOI":"10.1109/CVPRW50498.2020.00017","CorpusId":215828231},"title":"Exploring Racial Bias within Face Recognition via per-subject Adversarially-Enabled Data Augmentation"},{"paperId":"e924bfb95435153185f8d89e77f5a3534e2a29bd","externalIds":{"MAG":"3039690871","ArXiv":"2004.07173","DBLP":"conf/cvpr/PenaSMF20","DOI":"10.1109/CVPRW50498.2020.00022","CorpusId":215768624},"title":"Bias in Multimodal AI: Testbed for Fair Automatic Recruitment"},{"paperId":"dc08d90005b12f66a12798fd79959a8f7f8c4885","externalIds":{"ArXiv":"2003.06576","DBLP":"journals/corr/abs-2003-06576","MAG":"3035517717","DOI":"10.1109/cvpr42600.2020.01081","CorpusId":212725353},"title":"Counterfactual Samples Synthesizing for Robust Visual Question Answering"},{"paperId":"5c4d8de85225a9a003cb2e30f03e206c1bd8ecaa","externalIds":{"MAG":"3139925357","DOI":"10.1093/OSO/9780198826491.003.0055","CorpusId":235049261},"title":"Article 22 Automated individual decision-making, including profiling"},{"paperId":"27cd5a3eb55d2df2a4c06e96247b79f215516a67","externalIds":{"ArXiv":"2001.09784","DBLP":"journals/corr/abs-2001-09784","MAG":"3001325760","CorpusId":210921184},"title":"Algorithmic Fairness"},{"paperId":"22a0e7c06f1e64729211177f80d1b9dcaab3c706","externalIds":{"DBLP":"journals/corr/abs-2001-00964","ArXiv":"2001.00964","MAG":"3004542466","DOI":"10.1145/3375627.3375820","CorpusId":209862419},"title":"Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing"},{"paperId":"307ed22ce979f1a6c5d4afd996d53cd022217f57","externalIds":{"MAG":"2998538409","DOI":"10.1117/12.2557465","CorpusId":210879755},"title":"deb2viz: Debiasing gender in word embedding data using subspace visualization"},{"paperId":"0702f398468c23c96e3313e6efc798c964fba289","externalIds":{"MAG":"2997646596","DBLP":"journals/corr/abs-1912-12854","ArXiv":"1912.12854","CorpusId":202786860},"title":"Pareto Multi-Task Learning"},{"paperId":"cda670005c0849a9d94f1f157e576af46e1f1875","externalIds":{"DBLP":"journals/access/NgxandeTB20","ArXiv":"1912.12123","MAG":"3010981289","DOI":"10.1109/ACCESS.2020.2981912","CorpusId":209501179},"title":"Bias Remediation in Driver Drowsiness Detection Systems Using Generative Adversarial Networks"},{"paperId":"94a20d1d7062224e9aeba05aea901fd8d6881e85","externalIds":{"MAG":"2997352032","ArXiv":"1911.10787","DBLP":"conf/aaai/YangF20","DOI":"10.1609/AAAI.V34I05.6486","CorpusId":208267842},"title":"A Causal Inference Method for Reducing Gender Bias in Word Embedding Relations"},{"paperId":"26d8293b6f94d951f8cd79e2dc6d6ebb8212fd2c","externalIds":{"MAG":"2991334938","DBLP":"journals/corr/abs-1911-10692","ArXiv":"1911.10692","CorpusId":208267540},"title":"Mitigate Bias in Face Recognition using Skewness-Aware Reinforcement Learning"},{"paperId":"eef4df3a5232c7ce70123aaebb326ff9169a3c8c","externalIds":{"MAG":"3101004475","ArXiv":"1912.11078","ACL":"2020.acl-main.468","DBLP":"conf/acl/ShahSH20","DOI":"10.18653/v1/2020.acl-main.468","CorpusId":209461005},"title":"Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview"},{"paperId":"b488f536cdeaafb9ee70c10b1ae83c5d78681487","externalIds":{"DBLP":"journals/corr/abs-1910-12008","ArXiv":"1910.12008","MAG":"2981374076","CorpusId":204904806},"title":"Fair Generative Modeling via Weak Supervision"},{"paperId":"4efafeba13d7e22238583bab2ed3d5b9a3359465","externalIds":{"MAG":"3005317064","DBLP":"conf/wacv/Adeli-MosabbebZ21","DOI":"10.1109/WACV48630.2021.00256","CorpusId":211069024,"PubMed":"34522832"},"title":"Representation Learning with Statistical Independence to Mitigate Bias"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","externalIds":{"DBLP":"journals/corr/abs-1910-01108","ArXiv":"1910.01108","MAG":"2978017171","CorpusId":203626972},"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"4dc81c9ac23f8c056f6622f6dd4e3c035dcd1554","externalIds":{"MAG":"2975043431","DBLP":"journals/access/GongLSRPNKE19","DOI":"10.1109/ACCESS.2019.2943604","CorpusId":204095241},"title":"A Comparison of Loss Weighting Strategies for Multi task Learning in Deep Neural Networks"},{"paperId":"4e1f9b2ad5cc81ccc7a9521483d068a3304019ec","externalIds":{"DBLP":"journals/corr/abs-1910-03676","ArXiv":"1910.03676","MAG":"2979327556","CorpusId":203952108},"title":"Bias-Resilient Neural Network"},{"paperId":"d5513ae4aee1f0b27dc260585ec05211f1dce274","externalIds":{"MAG":"2972644974","DBLP":"conf/aaai/LauscherGPV20","ArXiv":"1909.06092","DOI":"10.1609/AAAI.V34I05.6325","CorpusId":202572693},"title":"A General Framework for Implicit and Explicit Debiasing of Distributional Word Vector Spaces"},{"paperId":"0e9b89f034b9a8c2828fe7daaee3894d6bfe3e50","externalIds":{"DBLP":"conf/aaai/DevLPS20","ArXiv":"1908.09369","MAG":"2969426522","DOI":"10.1609/AAAI.V34I05.6267","CorpusId":201670701},"title":"On Measuring and Mitigating Biased Inferences of Word Embeddings"},{"paperId":"0090023afc66cd2741568599057f4e82b566137c","externalIds":{"ArXiv":"1908.09635","MAG":"2969896603","DBLP":"journals/csur/MehrabiMSLG21","DOI":"10.1145/3457607","CorpusId":201666566},"title":"A Survey on Bias and Fairness in Machine Learning"},{"paperId":"c7d3d0ef7129dea47c5a1caa8f56791088a02408","externalIds":{"ArXiv":"1908.08843","DBLP":"journals/corr/abs-1908-08843","MAG":"2969288534","DOI":"10.1109/MIS.2020.3000681","CorpusId":201645479},"title":"Fairness in Deep Learning: A Computational Perspective"},{"paperId":"3caf34532597683c980134579b156cd0d7db2f40","externalIds":{"ArXiv":"1908.07125","DBLP":"conf/emnlp/WallaceFKGS19","MAG":"2970290563","ACL":"D19-1221","DOI":"10.18653/v1/D19-1221","CorpusId":201698258},"title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"1846bb80fbd235bcf3316b5ffb09a6d3e22ebeab","externalIds":{"MAG":"2966613548","DBLP":"conf/ijcai/XuWYZW19","DOI":"10.24963/ijcai.2019/201","CorpusId":199465699},"title":"Achieving Causal Fairness through Generative Adversarial Networks"},{"paperId":"758a29b97b416ef718daa8b4e6be672e13af1d2e","externalIds":{"PubMedCentral":"7931947","MAG":"2563852449","DBLP":"journals/fdata/Olteanu00K19","DOI":"10.3389/fdata.2019.00013","CorpusId":7384019,"PubMed":"33693336"},"title":"Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries"},{"paperId":"1f6b9766374d14d81c225c2ced5bb02fe0bccd43","externalIds":{"DBLP":"journals/tvcg/WexlerPBWVW20","ArXiv":"1907.04135","MAG":"2969560340","DOI":"10.1109/TVCG.2019.2934619","CorpusId":195848259,"PubMed":"31442996"},"title":"The What-If Tool: Interactive Probing of Machine Learning Models"},{"paperId":"333671a5fbbf726f8819138f3670524ec0405726","externalIds":{"MAG":"2974817986","DOI":"10.1147/jrd.2019.2942287","CorpusId":203897430},"title":"AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias"},{"paperId":"59bc88cce76d733522a1b07795328fab8398fe8c","externalIds":{"DBLP":"journals/mansci/LambrechtT19","DOI":"10.1287/mnsc.2018.3093","CorpusId":263870936},"title":"Algorithmic Bias? An Empirical Study of Apparent Gender-Based Discrimination in the Display of STEM Career Ads"},{"paperId":"a652d70b7af8f91f21f4eac5bdb32b0939b37255","externalIds":{"ArXiv":"1906.10244","MAG":"2953793649","DBLP":"journals/corr/abs-1906-10244","CorpusId":109936512},"title":"Generating User-friendly Explanations for Loan Denials using GANs"},{"paperId":"a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd","externalIds":{"MAG":"2950437211","ArXiv":"1906.07337","DBLP":"journals/corr/abs-1906-07337","ACL":"W19-3823","DOI":"10.18653/v1/W19-3823","CorpusId":190000105},"title":"Measuring Bias in Contextualized Word Representations"},{"paperId":"23490331125fb73a4b442e337ebff24d7e4265dd","externalIds":{"ArXiv":"1906.06668","MAG":"3008669272","DBLP":"journals/natmi/Mittelstadt19","DOI":"10.1038/s42256-019-0114-4","CorpusId":207888555},"title":"Principles alone cannot guarantee ethical AI"},{"paperId":"8ea6f7fcd75d7651b5c4f5c7cd121854f3369693","externalIds":{"ACL":"P19-1160","MAG":"2949969209","ArXiv":"1906.00742","DBLP":"journals/corr/abs-1906-00742","DOI":"10.18653/v1/P19-1160","CorpusId":173991106},"title":"Gender-preserving Debiasing for Pre-trained Word Embeddings"},{"paperId":"b25a30451518d372817967a72e125d638c85379e","externalIds":{"MAG":"2947529249","DBLP":"journals/corr/abs-1905-13662","ArXiv":"1905.13662","CorpusId":173187920},"title":"On the Fairness of Disentangled Representations"},{"paperId":"623b1c61aa36048a38485a44551cb3fdcbcc827b","externalIds":{"ACL":"P19-2031","DBLP":"journals/corr/abs-1905-12801","MAG":"2953029503","ArXiv":"1905.12801","DOI":"10.18653/v1/P19-2031","CorpusId":170078973},"title":"Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function"},{"paperId":"8ec127925a8680928d546df7248963e772e07a5d","externalIds":{"MAG":"2946221515","DBLP":"journals/corr/abs-1905-11361","ArXiv":"1905.11361","DOI":"10.4230/LIPIcs.FORC.2020.1","CorpusId":166227886},"title":"Efficient candidate screening under multiple tests and implications for fairness"},{"paperId":"45a0936668026005beccf9009e8a555f96a50d7e","externalIds":{"ArXiv":"1906.02589","DBLP":"journals/corr/abs-1906-02589","MAG":"2945445411","CorpusId":174800294},"title":"Flexibly Fair Representation Learning by Disentanglement"},{"paperId":"7d3d85f9020be9b44ab3714cf2094f54fc65770f","externalIds":{"MAG":"2942659792","DOI":"10.22148/16.036","CorpusId":155861169},"title":"Data Is the New What? Popular Metaphors & Professional Ethics in Emerging Data Culture"},{"paperId":"e91dca6e99f2d392953524986f2125be2008d9fc","externalIds":{"DBLP":"conf/cvpr/LiV19a","MAG":"2935947500","ArXiv":"1904.07911","DOI":"10.1109/CVPR.2019.00980","CorpusId":119314167},"title":"REPAIR: Removing Representation Bias by Dataset Resampling"},{"paperId":"a4e67bcbf912e13cebbb1241d05d1ca0a1df9df8","externalIds":{"ArXiv":"1904.03035","DBLP":"conf/naacl/BordiaB19","MAG":"2955635700","ACL":"N19-3002","DOI":"10.18653/v1/N19-3002","CorpusId":102352788},"title":"Identifying and Reducing Gender Bias in Word-Level Language Models"},{"paperId":"5e9c85235210b59a16bdd84b444a904ae271f7e7","externalIds":{"MAG":"2963078909","ACL":"N19-1063","DBLP":"conf/naacl/MayWBBR19","ArXiv":"1903.10561","DOI":"10.18653/v1/N19-1063","CorpusId":85518027},"title":"On Measuring Social Biases in Sentence Encoders"},{"paperId":"e39b8e9eedff7c53f845acbb16f9e55897f69123","externalIds":{"ArXiv":"1903.12262","DBLP":"journals/corr/abs-1903-12262","MAG":"2935234963","CorpusId":88516160},"title":"Towards Standardization of Data Licenses: The Montreal Data License"},{"paperId":"ac9f3c0b58e3bd17214e52b4b14cd55c1dba2234","externalIds":{"MAG":"2966833090","DBLP":"conf/wsdm/BirdKKM19","DOI":"10.1145/3289600.3291383","CorpusId":59528510},"title":"Fairness-Aware Machine Learning: Practical Challenges and Lessons Learned"},{"paperId":"f4bc6b284bc4508856ddc10d60b30635de9cfb7f","externalIds":{"ArXiv":"1901.10002","DBLP":"conf/eaamo/SureshG21","DOI":"10.1145/3465416.3483305","CorpusId":235436386},"title":"A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle"},{"paperId":"61c425bdda0e053074e96c3e6761ff1d7e0dd469","externalIds":{"MAG":"2912457762","DBLP":"journals/corr/abs-1901-10002","CorpusId":59336269},"title":"A Framework for Understanding Unintended Consequences of Machine Learning"},{"paperId":"f471c7693e6f25f11bb5550a25ea1ca1e5367b5d","externalIds":{"DBLP":"conf/aies/AminiSSBR19","MAG":"2957285709","DOI":"10.1145/3306618.3314243","CorpusId":96428734},"title":"Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure"},{"paperId":"c4afa2b3eda95a1194313394901e0e96e24cefaa","externalIds":{"DBLP":"conf/fat/De-ArteagaRWCBC19","MAG":"3105536512","ArXiv":"1901.09451","DOI":"10.1145/3287560.3287572","CorpusId":58006082},"title":"Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting"},{"paperId":"b16314c914d7b52262100c58e03a93eb724bf671","externalIds":{"ArXiv":"1901.07656","DBLP":"journals/corr/abs-1901-07656","MAG":"2913897682","CorpusId":59158788},"title":"Attenuating Bias in Word Vectors"},{"paperId":"b79fe48ae523dc66185aa04df2dac7041afa8683","externalIds":{"MAG":"2963350032","DBLP":"journals/corr/abs-1812-10352","ArXiv":"1812.10352","DOI":"10.1109/CVPR.2019.00922","CorpusId":56895575},"title":"Learning Not to Learn: Training Deep Neural Networks With Biased Data"},{"paperId":"1772cb98a2218793ea08bd46d9ebe6a88d9f8580","externalIds":{"MAG":"2904233591","DBLP":"conf/chi/HolsteinVDDW19","ArXiv":"1812.05239","DOI":"10.1145/3290605.3300830","CorpusId":54895308},"title":"Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?"},{"paperId":"ceb2ebef0b41e31c1a21b28c2734123900c005e2","externalIds":{"DBLP":"journals/corr/abs-1812-04948","MAG":"2904367110","ArXiv":"1812.04948","DOI":"10.1109/CVPR.2019.00453","CorpusId":54482423},"title":"A Style-Based Generator Architecture for Generative Adversarial Networks"},{"paperId":"e4323d36739db0c091f3fdc8626c2edde7f57133","externalIds":{"MAG":"2903695278","CorpusId":158203520},"title":"Help wanted: an examination of hiring algorithms, equity, and bias"},{"paperId":"33e73f7efbc0f02d3665a71202d496789a8cb9f2","externalIds":{"ArXiv":"1812.02230","DBLP":"journals/corr/abs-1812-02230","MAG":"2902476877","CorpusId":54447715},"title":"Towards a Definition of Disentangled Representations"},{"paperId":"59c47e49d8211953b1acd68984650b807ce69a71","externalIds":{"DBLP":"conf/iccv/WangDHTH19","MAG":"2982232682","DOI":"10.1109/ICCV.2019.00078","CorpusId":198968250},"title":"Racial Faces in the Wild: Reducing Racial Bias by Information Maximization Adaptation Network"},{"paperId":"97bfa89addc6e5d76361e4c1e296949cad887b86","externalIds":{"DBLP":"journals/tacl/BenderF18","ACL":"Q18-1041","MAG":"2911227954","DOI":"10.1162/tacl_a_00041","CorpusId":52255687},"title":"Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science"},{"paperId":"9c5c794094fbf5da8c48df5c3242615dc0b1d245","externalIds":{"ArXiv":"1811.12359","DBLP":"conf/icml/LocatelloBLRGSB19","MAG":"3006182312","CorpusId":54089884},"title":"Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"},{"paperId":"4ffa7587767462be2544264d7ad3d5144b9e687e","externalIds":{"MAG":"2961158440","DBLP":"conf/iccv/WangZYCO19","DOI":"10.1109/ICCV.2019.00541","CorpusId":195847929},"title":"Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations"},{"paperId":"36845e5d13e6d8698561d416b694122cfa754332","externalIds":{"ArXiv":"1811.05577","DBLP":"journals/corr/abs-1811-05577","MAG":"2900572965","CorpusId":53305131},"title":"Aequitas: A Bias and Fairness Audit Toolkit"},{"paperId":"b88d076674e4f8d5cd9a362af483210ae64312d9","externalIds":{"MAG":"2898147565","DBLP":"journals/interactions/CramerGSR18","DOI":"10.1145/3278156","CorpusId":53112422},"title":"Assessing and addressing algorithmic bias in practice"},{"paperId":"7365f887c938ca21a6adbef08b5a520ebbd4638f","externalIds":{"ArXiv":"1810.03993","DBLP":"journals/corr/abs-1810-03993","MAG":"2897042519","DOI":"10.1145/3287560.3287596","CorpusId":52946140},"title":"Model Cards for Model Reporting"},{"paperId":"790a35629b9de5ca58bb8e7882bfb8189f783861","externalIds":{"MAG":"2963265460","ArXiv":"1809.03332","DBLP":"journals/corr/abs-1809-03332","CorpusId":52181973},"title":"Assessing and Addressing Algorithmic Bias - But Before We Get There"},{"paperId":"fef9d9eb2d527174ac5b329b0a044e98a1808971","externalIds":{"DBLP":"journals/corr/abs-1807-11714","ArXiv":"1807.11714","MAG":"3128232076","DOI":"10.1007/978-3-030-62077-6_14","CorpusId":51888520},"title":"Gender Bias in Neural Natural Language Processing"},{"paperId":"1694363a78fba7ef564f4c57a491b80397ce18e7","externalIds":{"DBLP":"journals/corr/abs-1806-06122","MAG":"2962922829","ArXiv":"1806.06122","DOI":"10.4230/LIPIcs.ITCS.2019.33","CorpusId":49303187},"title":"Fairness Under Composition"},{"paperId":"a78f9467070992fc8742641ec97f9972597d869a","externalIds":{"DBLP":"conf/icse/VermaR18","MAG":"2809878087","DOI":"10.1145/3194770.3194776","CorpusId":49561627},"title":"Fairness Definitions Explained"},{"paperId":"b0c5dc3fa19a2bc97606ccb6f55226b913984395","externalIds":{"DBLP":"journals/corr/abs-1807-00517","MAG":"2810512327","ArXiv":"1803.09797","DOI":"10.1007/978-3-030-01219-9_47","CorpusId":4384334},"title":"Women also Snowboard: Overcoming Bias in Captioning Models"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","externalIds":{"DBLP":"journals/cacm/GebruMVVWDC21","ArXiv":"1803.09010","MAG":"2795038878","DOI":"10.1145/3458723","CorpusId":4421027},"title":"Datasheets for datasets"},{"paperId":"f7325d232c7ac7d2daaf6605377058db5b5b83cc","externalIds":{"MAG":"2951278035","DBLP":"journals/csur/GuidottiMRTGP19","ArXiv":"1802.01933","DOI":"10.1145/3236009","CorpusId":3342225},"title":"A Survey of Methods for Explaining Black Box Models"},{"paperId":"c7330852a07170cd0e6990f5fbde5fca12b6ccd6","externalIds":{"MAG":"2963116854","DBLP":"journals/corr/abs-1801-07593","ArXiv":"1801.07593","DOI":"10.1145/3278721.3278779","CorpusId":9424845},"title":"Mitigating Unwanted Biases with Adversarial Learning"},{"paperId":"18858cc936947fc96b5c06bbe3c6c2faa5614540","externalIds":{"MAG":"2788481061","DBLP":"conf/fat/BuolamwiniG18","CorpusId":3298854},"title":"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"},{"paperId":"f350fe92a2d246a02df2a41928affa1c526cb609","externalIds":{"DBLP":"journals/corr/abs-1710-03184","MAG":"2764040709","ArXiv":"1710.03184","CorpusId":37380264},"title":"On formalizing fairness in prediction with machine learning"},{"paperId":"135bafc83e9a73c88e759f98a28edfdb5c02f81d","externalIds":{"DBLP":"journals/corr/ZhaoWYOC17","MAG":"2951685111","ArXiv":"1707.09457","ACL":"D17-1323","DOI":"10.18653/v1/D17-1323","CorpusId":1389483},"title":"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"043f084e379a44608c470059c2aa174a323e9774","externalIds":{"DBLP":"journals/corr/KusnerLRS17","MAG":"2952517774","ArXiv":"1703.06856","CorpusId":2014883},"title":"Counterfactual Fairness"},{"paperId":"a90226c41b79f8b06007609f39f82757073641e2","externalIds":{"MAG":"2753738274","CorpusId":46798026},"title":"beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"},{"paperId":"37f5d47019f467c74acff22a38ffd4b98bdcb5d4","externalIds":{"DBLP":"journals/bigdata/Chouldechova17","MAG":"2952812878","ArXiv":"1610.07524","DOI":"10.1089/big.2016.0047","CorpusId":1443041,"PubMed":"28632438"},"title":"Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","externalIds":{"MAG":"2962858109","DBLP":"conf/iccv/SelvarajuCDVPB17","ArXiv":"1610.02391","DOI":"10.1007/s11263-019-01228-7","CorpusId":15019293},"title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"5966d7c7f60898d610812e24c64d4d57855ad86a","externalIds":{"MAG":"3105700579","ArXiv":"1608.07187","DBLP":"journals/corr/IslamBN16","DOI":"10.1126/science.aal4230","CorpusId":23163324,"PubMed":"28408601"},"title":"Semantics derived automatically from language corpora contain human-like biases"},{"paperId":"ccf6a69a7f33bcf052aa7def176d3b9de495beb7","externalIds":{"DBLP":"conf/nips/BolukbasiCZSK16","MAG":"2950018712","ArXiv":"1607.06520","CorpusId":1704893},"title":"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"},{"paperId":"5557f2bab78e91c8c83ba094f246ac26e698081f","externalIds":{"ArXiv":"1607.02452","MAG":"2486654687","DBLP":"journals/joi/Perianes-Rodriguez16","DOI":"10.1016/J.JOI.2016.10.006","CorpusId":14368291},"title":"Constructing bibliometric networks: A comparison between full and fractional counting"},{"paperId":"2558231cadaf0b1a4ac79d1a5c79322c8fbd327f","externalIds":{"DBLP":"journals/corr/BolukbasiCZSK16","MAG":"2462418454","ArXiv":"1606.06121","CorpusId":8798676},"title":"Quantifying and Reducing Stereotypes in Word Embeddings"},{"paperId":"86c37cd1109ce5b465116695b7705444a45185cf","externalIds":{"DBLP":"journals/corr/EdwardsS15","ArXiv":"1511.05897","MAG":"2950932154","CorpusId":4986726},"title":"Censoring Representations with an Adversary"},{"paperId":"ee21abcd21e010b53f2c6c21ba2bdd53abdcc40a","externalIds":{"MAG":"1905153633","DBLP":"conf/cvpr/LeviH15","DOI":"10.1109/CVPRW.2015.7301352","CorpusId":15398231},"title":"Age and gender classification using convolutional neural networks"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"bee044c8e8903fb67523c1f8c105ab4718600cdb","externalIds":{"MAG":"2952181735","DBLP":"journals/corr/GoodfellowSS14","ArXiv":"1412.6572","CorpusId":6706414},"title":"Explaining and Harnessing Adversarial Examples"},{"paperId":"6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","externalIds":{"MAG":"1834627138","ArXiv":"1411.7766","DBLP":"journals/corr/LiuLWT14","DOI":"10.1109/ICCV.2015.425","CorpusId":459456},"title":"Deep Learning Face Attributes in the Wild"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","externalIds":{"DBLP":"conf/emnlp/PenningtonSM14","ACL":"D14-1162","MAG":"2250539671","DOI":"10.3115/v1/D14-1162","CorpusId":1957433},"title":"GloVe: Global Vectors for Word Representation"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","externalIds":{"ArXiv":"1409.0575","DBLP":"journals/corr/RussakovskyDSKSMHKKBBF14","MAG":"2546241758","DOI":"10.1007/s11263-015-0816-y","CorpusId":2930547},"title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","externalIds":{"MAG":"2950577311","DBLP":"journals/corr/abs-1301-3781","ArXiv":"1301.3781","CorpusId":5959482},"title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"184ac0766262312ba76bbdece4e7ffad0aa8180b","externalIds":{"DBLP":"journals/pami/BengioCV13","MAG":"2952111767","ArXiv":"1206.5538","DOI":"10.1109/TPAMI.2013.50","CorpusId":393948,"PubMed":"23787338"},"title":"Representation Learning: A Review and New Perspectives"},{"paperId":"1866ac6b77538c868d23277e091f1643e4849ced","externalIds":{"MAG":"2121275048","DOI":"10.1093/pan/mpj004","CorpusId":1311037},"title":"The Dangers of Extreme Counterfactuals"},{"paperId":"24d651a9b0e00ec94e782521eb24ba43f6e9925f","externalIds":{"MAG":"1974423219","DOI":"10.1080/01621459.1972.10482387","CorpusId":120883490},"title":"On Simpson's Paradox and the Sure-Thing Principle"},{"paperId":"53447ee64bc01b62795ce9014339f2bc799c67ae","externalIds":{"DBLP":"journals/corr/abs-2206-10043","DOI":"10.48550/arXiv.2206.10043","CorpusId":249889122},"title":"Achieving Utility, Fairness, and Compactness via Tunable Information Bottleneck Measures"},{"paperId":"058dee85d522f6565fe1502cafcf9a5e3f6a6f0e","externalIds":{"ACL":"2022.naacl-main.122","DBLP":"conf/naacl/DelobelleTCB22","DOI":"10.18653/v1/2022.naacl-main.122","CorpusId":250390561},"title":"Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models"},{"paperId":"804b27dc02becf7bbbd89ba949e1e07e8677c459","externalIds":{"DBLP":"journals/corr/abs-2202-04053","CorpusId":246652218},"title":"DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers"},{"paperId":"ecee71b8599818dedb46ce3e1245f1c94b6cbb82","externalIds":{"ACL":"2022.gebnlp-1.14","DOI":"10.18653/v1/2022.gebnlp-1.14","CorpusId":250391071},"title":"Unsupervised Mitigating Gender Bias by Character Components: A Case Study of Chinese Word Embedding"},{"paperId":"d4fb836846b79d8692df8bf54d20d1a9d02ffe7d","externalIds":{"ACL":"2022.ltedi-1.8","DBLP":"conf/ltedi/GiraZL22","DOI":"10.18653/v1/2022.ltedi-1.8","CorpusId":248780268},"title":"Debiasing Pre-Trained Language Models via Efficient Fine-Tuning"},{"paperId":"833af392d6822021248b8f9638f93a177cd3295c","externalIds":{"DBLP":"journals/eswa/PessachS21","DOI":"10.1016/j.eswa.2021.115667","CorpusId":238222776},"title":"Improving fairness of artificial intelligence algorithms in Privileged-Group Selection Bias data settings"},{"paperId":"f4cea2cd1c29707d386da8bf0734934ea23bd488","externalIds":{"CorpusId":237483522},"title":"Fairness-Aware Machine Learning An Extensive Overview"},{"paperId":"958284df0d0eca1b9cb234411aa15940c99e7ce6","externalIds":{"DBLP":"journals/access/ParkHHB20","MAG":"3107220497","DOI":"10.1109/ACCESS.2020.3041503","CorpusId":228090629},"title":"Fair-VQA: Fairness-Aware Visual Question Answering Through Sensitive Attribute Prediction"},{"paperId":"78d08b8ab4132defffe98ec7f80a51452203f70d","externalIds":{"DBLP":"conf/nips/VigGBQNSS20","MAG":"3104142662","CorpusId":227275068},"title":"Investigating Gender Bias in Language Models Using Causal Mediation Analysis"},{"paperId":"a3cc3a49bfce756a4164c20887e76a45ae9a7dfa","externalIds":{"MAG":"2990881513","DBLP":"conf/icis/Haas19","CorpusId":209150054},"title":"The Price of Fairness - A Framework to Explore Trade-Offs in Algorithmic Fairness"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"7d76a09aa363685bc0f04a502ed853dc09a574e2","externalIds":{"MAG":"2530010084","CorpusId":217472699},"title":"Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization"},{"paperId":"6cf35ec34efa592f83e3a1b748aea14957fc784a","externalIds":{"MAG":"36434594","CorpusId":3237155},"title":"The Need for Biases in Learning Generalizations"},{"paperId":"b19270b120b855f41ca727a74b5e8395096d8b9c","externalIds":{"CorpusId":7083152},"title":"To appear: IEEE Transactions on Information Forensics and Security Face Recognition Performance: Role of Demographic Information"},{"paperId":"8a264121249f807fdef979b8f2c55dfb62476f1a","externalIds":{"CorpusId":281326786},"title":"Generating User-Friendly Explanations for Loan Denials Using Generative Adversarial Networks"}]}