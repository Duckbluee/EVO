{"abstract":"We focus on few-shot learning with cross-domain shift. While most existing few-shot learning assumed similar distributions between the source and target data, we aim to deal with the scenarios when we need to experience domain shift, the scenarios that fit to many real-world applications such as customization from lab-made toys to industrial products. More specifically, we consider the domain shift when we do not have much overlap between the source and the target domains, such as they do not have many overlapped classes or they do not have many common features. In fact, we experience significant improvement from existing work especially when we do not have much overlap between the source and the target domains. We propose a simple yet effective dropout-style method given a model that has been trained based on low-complexity concept from the source domain. The main idea is to sample several sub-networks by dropping neurons (or feature maps) to construct a bunch of models with diverse features for the target domain. Afterward, we choose the most suitable sub-networks to construct the ensemble for the target domain learning. The proposed method requires almost no external storage other than the original model for the source domain learning. As another advantage over other learning methods is the computation in the method is highly parallelizable for the purpose of efficiency. To evaluate the proposed method, we conduct experiments and show that the proposed method can be in cooperated with many metric-based few-shot learning base models, with consistent improvement under various settings."}