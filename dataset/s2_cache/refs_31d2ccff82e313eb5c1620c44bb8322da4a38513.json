{"references":[{"paperId":"56a9aa8825b6c6441b0d22b9bc04852e1af80852","externalIds":{"ArXiv":"2502.18600","DBLP":"journals/corr/abs-2502-18600","DOI":"10.48550/arXiv.2502.18600","CorpusId":276618268},"title":"Chain of Draft: Thinking Faster by Writing Less"},{"paperId":"656a680c61d79c63367d1c88a0ab542b50ebe162","externalIds":{"DBLP":"conf/nips/ZhouTZLW024","ArXiv":"2410.23856","DOI":"10.48550/arXiv.2410.23856","CorpusId":273707060},"title":"Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?"},{"paperId":"e87f566a2dd10aa4a9a9e68184fc494fcf4963d3","externalIds":{"DBLP":"conf/emnlp/DengZLWXWZZB25","ArXiv":"2410.17885","DOI":"10.18653/v1/2025.emnlp-main.38","CorpusId":273532733},"title":"Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning"},{"paperId":"0aeeba8ddd3cbf9d9d913c26594bc1c19053b9f7","externalIds":{"DBLP":"conf/emnlp/ZhangB024a","ArXiv":"2410.05558","DOI":"10.48550/arXiv.2410.05558","CorpusId":273228175},"title":"Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives"},{"paperId":"d0da6b0d60d6400e24368d80c5412026a403997e","externalIds":{"DBLP":"journals/corr/abs-2409-20441","ArXiv":"2409.20441","DOI":"10.48550/arXiv.2409.20441","CorpusId":272987478},"title":"Instance-adaptive Zero-shot Chain-of-Thought Prompting"},{"paperId":"bf0479119679fb627883bffe6087722df5ba4e5b","externalIds":{"DBLP":"conf/naacl/LiuXHZWWYL25","ArXiv":"2409.17539","DOI":"10.48550/arXiv.2409.17539","CorpusId":272910569},"title":"Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models"},{"paperId":"fe310e7f96c63e4e3f6976ad77c4f85350fe1e40","externalIds":{"DBLP":"journals/corr/abs-2409-14880","ArXiv":"2409.14880","DOI":"10.1109/CLNLP64123.2024.00016","CorpusId":272827036},"title":"End-to-End Graph Flattening Method for Large Language Models"},{"paperId":"b00d92a2ebb024dc6c82d2cefbcd0782ce32f74c","externalIds":{"ArXiv":"2406.04271","DBLP":"conf/nips/YangYZCXZG024","DOI":"10.48550/arXiv.2406.04271","CorpusId":270285926},"title":"Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models"},{"paperId":"c14010990c9d75a6e836e1c86d42f405a5d3d0a6","externalIds":{"ArXiv":"2405.09589","DBLP":"conf/emnlp/SahooMG0JC24","DOI":"10.18653/v1/2024.findings-emnlp.685","CorpusId":269790923},"title":"A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models"},{"paperId":"215c345579e6f230191d2b5a591bdaa75e2fe2f5","externalIds":{"DBLP":"journals/corr/abs-2401-10065","ArXiv":"2401.10065","ACL":"2024.emnlp-main.629","DOI":"10.48550/arXiv.2401.10065","CorpusId":267035047},"title":"Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs"},{"paperId":"39e0bf77300bb6df8716ce83eb8a3f6a5e3d6b20","externalIds":{"DBLP":"conf/iclr/0002ZLEP0MFSLP24","ArXiv":"2401.04398","DOI":"10.48550/arXiv.2401.04398","CorpusId":266899992},"title":"Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding"},{"paperId":"5272acad9e4201e93dabe3fd99bd7ead9b1a544d","externalIds":{"DBLP":"journals/corr/abs-2401-01313","ArXiv":"2401.01313","DOI":"10.48550/arXiv.2401.01313","CorpusId":266725532},"title":"A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models"},{"paperId":"3a56bc074b8f3f985599627404b70e16fc5bce1b","externalIds":{"DBLP":"conf/icml/0002LZCHSL0XI24","ArXiv":"2312.04474","DOI":"10.48550/arXiv.2312.04474","CorpusId":266051661},"title":"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator"},{"paperId":"850538c1759c56a9f2dab8e84ec63801c41d6396","externalIds":{"DBLP":"journals/corr/abs-2311-11829","ArXiv":"2311.11829","DOI":"10.48550/arXiv.2311.11829","CorpusId":265295357},"title":"System 2 Attention (is something you might need too)"},{"paperId":"6489640b1d30a8a3e7cb906bb6557f1ccd0d799d","externalIds":{"DBLP":"journals/corr/abs-2311-09210","ACL":"2024.emnlp-main.813","ArXiv":"2311.09210","DOI":"10.48550/arXiv.2311.09210","CorpusId":265212816},"title":"Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models"},{"paperId":"11b95e33f1a61079105e06090984b9dd8742887e","externalIds":{"ArXiv":"2311.09277","DBLP":"journals/corr/abs-2311-09277","DOI":"10.48550/arXiv.2311.09277","CorpusId":265221368},"title":"Contrastive Chain-of-Thought Prompting"},{"paperId":"cfd7550d6c25f4771549726b289665c8aa5ae1be","externalIds":{"DBLP":"journals/corr/abs-2311-08734","ArXiv":"2311.08734","DOI":"10.48550/arXiv.2311.08734","CorpusId":265213130},"title":"Thread of Thought Unraveling Chaotic Contexts"},{"paperId":"5aa834e8088818e6ba138d5a1b7b66f69d619555","externalIds":{"ArXiv":"2311.04205","DBLP":"journals/corr/abs-2311-04205","DOI":"10.48550/arXiv.2311.04205","CorpusId":265043536},"title":"Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves"},{"paperId":"0786c88990235414611478099e43611542d973b0","externalIds":{"DBLP":"journals/corr/abs-2310-06117","ArXiv":"2310.06117","DOI":"10.48550/arXiv.2310.06117","CorpusId":263830368},"title":"Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models"},{"paperId":"eed2a631d672a4130407f8d69a0ad9118a1e6e7d","externalIds":{"DBLP":"conf/coling/ZhaoLLW0CW24","ACL":"2024.lrec-main.543","ArXiv":"2309.13339","DOI":"10.48550/arXiv.2309.13339","CorpusId":262465259},"title":"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic"},{"paperId":"4b0b56be0ae9479d2bd5c2f0943db1906343c10f","externalIds":{"DBLP":"journals/corr/abs-2309-11495","ArXiv":"2309.11495","DOI":"10.48550/arXiv.2309.11495","CorpusId":262062565},"title":"Chain-of-Verification Reduces Hallucination in Large Language Models"},{"paperId":"04e838c16f3d1fb8d69d34fe0a0a92c59717875b","externalIds":{"DBLP":"journals/corr/abs-2309-10687","ACL":"2024.naacl-short.35","ArXiv":"2309.10687","DOI":"10.48550/arXiv.2309.10687","CorpusId":262055578},"title":"EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning"},{"paperId":"f8a2dca1e8fe56e698984c077f7ff58d8ca867e9","externalIds":{"DBLP":"journals/corr/abs-2309-03409","ArXiv":"2309.03409","DOI":"10.48550/arXiv.2309.03409","CorpusId":261582296},"title":"Large Language Models as Optimizers"},{"paperId":"01286de359fa9dd3d8f78c48157a3e929533d94e","externalIds":{"ArXiv":"2307.11760","CorpusId":260126019},"title":"Large Language Models Understand and Can be Enhanced by Emotional Stimuli"},{"paperId":"e468ed6b824e60f45ba9a20b034e4090c6630751","externalIds":{"DBLP":"conf/iclr/LiZCDJPB24","ArXiv":"2305.13269","CorpusId":263610099},"title":"Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources"},{"paperId":"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","externalIds":{"ArXiv":"2305.10601","DBLP":"journals/corr/abs-2305-10601","DOI":"10.48550/arXiv.2305.10601","CorpusId":258762525},"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models"},{"paperId":"bda605928d6ebe4db906e69ab5d343df75918727","externalIds":{"DBLP":"journals/corr/abs-2305-08291","ArXiv":"2305.08291","DOI":"10.48550/arXiv.2305.08291","CorpusId":258686311},"title":"Large Language Model Guided Tree-of-Thought"},{"paperId":"94beb9f249d6d2f1c00d8edfa2db861633aee6f9","externalIds":{"DBLP":"journals/tosem/LiLLJ25","ArXiv":"2305.06599","DOI":"10.1145/3690635","CorpusId":258615421},"title":"Structured Chain-of-Thought Prompting for Code Generation"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","externalIds":{"ArXiv":"2303.09014","DBLP":"journals/corr/abs-2303-09014","DOI":"10.48550/arXiv.2303.09014","CorpusId":257557449},"title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"3fc3460c4554a28e489a0ea6ef067b79b7d301d9","externalIds":{"DBLP":"journals/corr/abs-2302-12246","ArXiv":"2302.12246","DOI":"10.48550/arXiv.2302.12246","CorpusId":257102707},"title":"Active Prompting with Chain-of-Thought for Large Language Models"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","externalIds":{"DBLP":"journals/tmlr/ChenM0C23","ArXiv":"2211.12588","CorpusId":253801709},"title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"4610ffb1b016acaa82a2065ffd1a3adbae1ce722","externalIds":{"DBLP":"journals/corr/abs-2211-01910","ArXiv":"2211.01910","DOI":"10.48550/arXiv.2211.01910","CorpusId":253265328},"title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","externalIds":{"DBLP":"journals/corr/abs-2210-03493","ArXiv":"2210.03493","CorpusId":252762275},"title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","externalIds":{"DBLP":"conf/iclr/YaoZYDSN023","ArXiv":"2210.03629","CorpusId":252762395},"title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"aa8c61c9f6bb21c57e49611ccb995cfda1b53b10","externalIds":{"ArXiv":"2203.17274","CorpusId":249375187},"title":"Exploring Visual Prompts for Adapting Large-Scale Models"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","externalIds":{"DBLP":"conf/iclr/0002WSLCNCZ23","ArXiv":"2203.11171","CorpusId":247595263},"title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","externalIds":{"ArXiv":"2201.11903","DBLP":"conf/nips/Wei0SBIXCLZ22","CorpusId":246411621},"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","externalIds":{"DBLP":"journals/corr/abs-2112-00114","ArXiv":"2112.00114","CorpusId":244773644},"title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","externalIds":{"DBLP":"journals/csur/LiuYFJHN23","ArXiv":"2107.13586","DOI":"10.1145/3560815","CorpusId":236493269},"title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","externalIds":{"ArXiv":"2005.14165","DBLP":"conf/nips/BrownMRSKDNSSAA20","MAG":"3030163527","CorpusId":218971783},"title":"Language Models are Few-Shot Learners"},{"paperId":"659bf9ce7175e1ec266ff54359e2bd76e0b7ff31","externalIds":{"DBLP":"conf/nips/LewisPPPKGKLYR020","MAG":"3027879771","ArXiv":"2005.11401","CorpusId":218869575},"title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"c2408a3a8da4f12d3eb156fe359a96b428e5aff1","externalIds":{"DBLP":"journals/corr/abs-2305-10276","DOI":"10.48550/arXiv.2305.10276","CorpusId":258740762},"title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models"},{"paperId":"7d083d654f66f763302d8a5f0678beb753f6507b","externalIds":{"DBLP":"journals/corr/abs-2310-14735","CorpusId":264426395},"title":"Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review"},{"paperId":"adb9acaf9184bdbd23105f1a383848eed9bc82fc","externalIds":{"DBLP":"journals/corr/abs-2305-16582","DOI":"10.48550/arXiv.2305.16582","CorpusId":258947684},"title":"Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"}]}