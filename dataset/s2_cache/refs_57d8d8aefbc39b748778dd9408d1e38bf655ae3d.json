{"references":[{"paperId":"271dc7834dfed519be923bb53b9eb1b7524d7212","externalIds":{"DBLP":"conf/eccv/WangYWZCZ24","ArXiv":"2403.11868","DOI":"10.48550/arXiv.2403.11868","CorpusId":268531130},"title":"View-Consistent 3D Editing with Gaussian Splatting"},{"paperId":"90607c8d66c9e397d697214f326560231f7834ba","externalIds":{"ArXiv":"2403.11134","DBLP":"journals/cvm/WuYZYCYG24","DOI":"10.1007/s41095-024-0436-y","CorpusId":268513119},"title":"Recent advances in 3D Gaussian splatting"},{"paperId":"b16378d058d3c8ec639ce0cbf2eded59125f752e","externalIds":{"ArXiv":"2403.10050","DBLP":"conf/eccv/XuHLSZ24","DOI":"10.48550/arXiv.2403.10050","CorpusId":268510017},"title":"Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing"},{"paperId":"20bb420095144ab4ee75e22b350ae9a01fbe4689","externalIds":{"DBLP":"conf/cvpr/ChenYYFFFLL24","ArXiv":"2403.09140","DOI":"10.1109/CVPR52733.2024.00974","CorpusId":268385469},"title":"Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior"},{"paperId":"58deeb11dc024962b1810952a946027f402ddb71","externalIds":{"DBLP":"conf/eccv/LiuWCSD24","ArXiv":"2403.09625","DOI":"10.48550/arXiv.2403.09625","CorpusId":268385447},"title":"Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation"},{"paperId":"3a713aaf8fff8841573f576beaef0bb20d094d34","externalIds":{"DBLP":"conf/eccv/WuBLWRTP24","ArXiv":"2403.08733","DOI":"10.48550/arXiv.2403.08733","CorpusId":268378999},"title":"GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing"},{"paperId":"9db25ca30dcc8aa75c034ccaee0f6ee2ca934956","externalIds":{"DBLP":"journals/corr/abs-2403-06738","ArXiv":"2403.06738","DOI":"10.48550/arXiv.2403.06738","CorpusId":268357902},"title":"V3D: Video Diffusion Models are Effective 3D Generators"},{"paperId":"2177967931f395f88faa630019e3cd9b1831ffc1","externalIds":{"ArXiv":"2403.05034","DBLP":"conf/eccv/WangWCXCYLSZ24","DOI":"10.48550/arXiv.2403.05034","CorpusId":268297409},"title":"CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model"},{"paperId":"76c024288e92c64a0a8767bb58683f85b1c9f3c4","externalIds":{"DBLP":"journals/corr/abs-2403-02234","ArXiv":"2403.02234","DOI":"10.48550/arXiv.2403.02234","CorpusId":268247806},"title":"3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors"},{"paperId":"1f2dfc535180bf6806b13086fe4f25d622483ada","externalIds":{"ArXiv":"2403.01800","DBLP":"journals/corr/abs-2403-01800","DOI":"10.48550/arXiv.2403.01800","CorpusId":268247485},"title":"AtomoVideo: High Fidelity Image-to-Video Generation"},{"paperId":"888e36b348b786538e3a74c459fe884a3457a36f","externalIds":{"ArXiv":"2402.12712","DBLP":"journals/corr/abs-2402-12712","DOI":"10.48550/arXiv.2402.12712","CorpusId":267759835},"title":"MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction"},{"paperId":"0f76333fd1a5c0751ba02a0f6add9a49f3caa78c","externalIds":{"DBLP":"journals/tvcg/FeiXZZYH25","ArXiv":"2402.07181","DOI":"10.1109/TVCG.2024.3397828","CorpusId":267626952,"PubMed":"38713572"},"title":"3D Gaussian Splatting as a New Era: A Survey"},{"paperId":"45f61c7c99e729c5666af436ad90c2c82c480375","externalIds":{"ArXiv":"2402.07207","DBLP":"journals/corr/abs-2402-07207","DOI":"10.48550/arXiv.2402.07207","CorpusId":267627441},"title":"GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting"},{"paperId":"c1bd6c981c946b1954bfe75b38397eaa2cc61c7f","externalIds":{"DBLP":"conf/eccv/ZhouMFYY24","ArXiv":"2402.06149","DOI":"10.48550/arXiv.2402.06149","CorpusId":267616686},"title":"HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting"},{"paperId":"11665dbecb17ef4d3d71b75b8666ce0e61bd43fa","externalIds":{"DBLP":"conf/eccv/TangCCWZL24","ArXiv":"2402.05054","DOI":"10.48550/arXiv.2402.05054","CorpusId":267523413},"title":"LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation"},{"paperId":"8231dfb2f5fbffa3816acad3dc75de8ecd74a4ac","externalIds":{"ArXiv":"2401.17807","DBLP":"journals/corr/abs-2401-17807","DOI":"10.48550/arXiv.2401.17807","CorpusId":267334657},"title":"Advances in 3D Generation: A Survey"},{"paperId":"6d9609ccf84e0ecb2d59844b5135e77933939003","externalIds":{"DBLP":"journals/tog/ZhuangKCLLS24","ArXiv":"2401.14828","DOI":"10.1145/3658205","CorpusId":267301595},"title":"TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts"},{"paperId":"94f7d8bce3bb848d127c8f113afc5bb0243579df","externalIds":{"DBLP":"conf/siggrapha/Bar-TalCTHPZEHL24","ArXiv":"2401.12945","DOI":"10.1145/3680528.3687614","CorpusId":267095113},"title":"Lumiere: A Space-Time Diffusion Model for Video Generation"},{"paperId":"c052d0d8945edd149c7226bb7dc551b1d683afbc","externalIds":{"ArXiv":"2401.09720","DBLP":"journals/corr/abs-2401-09720","DOI":"10.48550/arXiv.2401.09720","CorpusId":267035166},"title":"GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting"},{"paperId":"834f595fb25b8306b46f9e744ef8150f4971322f","externalIds":{"ArXiv":"2401.09050","DBLP":"conf/cvpr/Wu0YYZ24","DOI":"10.1109/CVPR52733.2024.00944","CorpusId":267027883},"title":"Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior"},{"paperId":"e382fe486f698c2e06bf416e84ceeff06ceb3966","externalIds":{"DBLP":"journals/corr/abs-2401-07727","ArXiv":"2401.07727","DOI":"10.48550/arXiv.2401.07727","CorpusId":266999363},"title":"HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation"},{"paperId":"a6e46344388dc3686bff3efb78ba75790ce11cd6","externalIds":{"ArXiv":"2401.03890","DBLP":"journals/corr/abs-2401-03890","DOI":"10.48550/arXiv.2401.03890","CorpusId":266844057},"title":"A Survey on 3D Gaussian Splatting"},{"paperId":"ac1c6ed84bdbe6d592ce594b1e4a2e0e1118bda9","externalIds":{"DBLP":"journals/corr/abs-2401-04099","ArXiv":"2401.04099","DOI":"10.48550/arXiv.2401.04099","CorpusId":266844895},"title":"AGG: Amortized Generative 3D Gaussians for Single Image to 3D"},{"paperId":"afd078249f3137290cfffc5ba258211ba8e1f920","externalIds":{"DBLP":"conf/aistats/WangFXWMIRLLWC25","ArXiv":"2401.00604","DOI":"10.48550/arXiv.2401.00604","CorpusId":266693363},"title":"SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity"},{"paperId":"65022cdf3cba83ca2000ce18e694ff54ce2ca787","externalIds":{"DBLP":"journals/corr/abs-2312-17142","ArXiv":"2312.17142","DOI":"10.48550/arXiv.2312.17142","CorpusId":266573581},"title":"DreamGaussian4D: Generative 4D Gaussian Splatting"},{"paperId":"abf2c13ec16b7b60753ed64ad6be7aa1a4e59826","externalIds":{"DBLP":"journals/corr/abs-2312-15258","ArXiv":"2312.15258","DOI":"10.48550/arXiv.2312.15258","CorpusId":266551025},"title":"Human101: Training 100+FPS Human Gaussians in 100s from 1 View"},{"paperId":"32032367b4e57d1fa355864982b62fb44b52cfc8","externalIds":{"DBLP":"conf/cvpr/Szymanowicz0V24","ArXiv":"2312.13150","DOI":"10.1109/CVPR52733.2024.00972","CorpusId":266374851},"title":"Splatter Image: Ultra-Fast Single-View 3D Reconstruction"},{"paperId":"632a4956aebedd85f61445718ec5df9589bb5843","externalIds":{"ArXiv":"2312.12337","DBLP":"conf/cvpr/CharatanLTS24","DOI":"10.1109/CVPR52733.2024.01840","CorpusId":266362208},"title":"PixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction"},{"paperId":"a7d6d07fdb631ce263ec2ddad72df269587fd3c9","externalIds":{"ArXiv":"2312.09222","DBLP":"journals/corr/abs-2312-09222","DOI":"10.1109/CVPR52733.2024.00443","CorpusId":266209832},"title":"Mosaic-SDF for 3D Generative Models"},{"paperId":"210e63599d49abdb848a4440d4244cdcdedeadff","externalIds":{"DBLP":"journals/corr/abs-2312-08754","ArXiv":"2312.08754","DOI":"10.48550/arXiv.2312.08754","CorpusId":266210477},"title":"UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation"},{"paperId":"fa35524739d5c60d94befc3f8e77488b4dd810db","externalIds":{"DBLP":"journals/corr/abs-2312-09228","ArXiv":"2312.09228","DOI":"10.1109/CVPR52733.2024.00480","CorpusId":266210317},"title":"3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting"},{"paperId":"536dcef9e5f22bb694bb154120050bfed07f8e90","externalIds":{"ArXiv":"2312.09147","DBLP":"conf/cvpr/ZouYGLLCZ24","DOI":"10.1109/CVPR52733.2024.00983","CorpusId":266209913},"title":"Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers"},{"paperId":"6be2644ed0faaf3aa6a18bd9b487c4eb46ba1ee9","externalIds":{"DBLP":"conf/siggraph/LiuWWSS0C24","ArXiv":"2312.07539","DOI":"10.1145/3641519.3657512","CorpusId":266174272},"title":"HeadArtist: Text-conditioned 3D Head Generation with Self Score Distillation"},{"paperId":"44246731109d54cbe9bfa5b319540093a8f79581","externalIds":{"DBLP":"conf/cvpr/LinD0024","ArXiv":"2312.03431","DOI":"10.1109/CVPR52733.2024.01997","CorpusId":265696561},"title":"Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle"},{"paperId":"e4679a63505651f3a7dfddd596b747af77f52efb","externalIds":{"ArXiv":"2312.03029","DOI":"10.1109/TPAMI.2025.3597940","CorpusId":265693957,"PubMed":"40802635"},"title":"HHAvatar: Gaussian Head Avatar with Dynamic Hairs."},{"paperId":"54fa9f6c32e41e72b230773dcec9e3491df43738","externalIds":{"ArXiv":"2312.02973","DBLP":"conf/cvpr/HuHL24a","DOI":"10.1109/CVPR52733.2024.01930","CorpusId":265659537},"title":"GauHuman: Articulated Gaussian Splatting from Monocular Human Videos"},{"paperId":"b54efb1ef315ecb68571b99ae7489d1834da02ba","externalIds":{"DBLP":"journals/corr/abs-2312-14937","ArXiv":"2312.14937","DOI":"10.1109/CVPR52733.2024.00404","CorpusId":266551723},"title":"SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes"},{"paperId":"745095916bb16d8e11562ede6ce30846c37e9ebd","externalIds":{"ArXiv":"2312.02155","DBLP":"journals/corr/abs-2312-02155","DOI":"10.1109/CVPR52733.2024.01861","CorpusId":265608762},"title":"GPS-Gaussian: Generalizable Pixel-Wise 3D Gaussian Splatting for Real-Time Human Novel View Synthesis"},{"paperId":"a8898778e9ea9b9d653a5d341379a237ff05b58d","externalIds":{"DBLP":"conf/cvpr/KwakDJKMY24","ArXiv":"2312.01305","DOI":"10.1109/CVPR52733.2024.00647","CorpusId":265609368},"title":"ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models"},{"paperId":"b8f37149729b919bb4f972a88665e6f66976997a","externalIds":{"ArXiv":"2312.01196","DBLP":"journals/corr/abs-2312-01196","DOI":"10.1109/CVPR52733.2024.01019","CorpusId":265609668},"title":"Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction"},{"paperId":"4c8608124dd92b84effedb7dd1965aeb13420e9c","externalIds":{"DBLP":"journals/corr/abs-2312-02201","ArXiv":"2312.02201","DOI":"10.48550/arXiv.2312.02201","CorpusId":265659122},"title":"ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation"},{"paperId":"ff0baeb316df06d017f381c80a2c3aa6399fe938","externalIds":{"DBLP":"conf/cvpr/BahmaniSRWGWTPT24","ArXiv":"2311.17984","DOI":"10.1109/CVPR52733.2024.00764","CorpusId":265506421},"title":"4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling"},{"paperId":"7665642af9e682e012bec045102a4d009421067c","externalIds":{"DBLP":"conf/cvpr/LiuZTSZLLL24","ArXiv":"2311.17061","DOI":"10.1109/CVPR52733.2024.00635","CorpusId":265466220},"title":"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting"},{"paperId":"c12e2dcf2b07252b9729b4a19ec61d46002a3c94","externalIds":{"ArXiv":"2311.16737","DBLP":"journals/corr/abs-2311-16737","DOI":"10.48550/arXiv.2311.16737","CorpusId":265466470},"title":"Point'n Move: Interactive Scene Object Manipulation on Gaussian Splatting Radiance Fields"},{"paperId":"b89cfdb239c33efa529385a84bae92df816de495","externalIds":{"DBLP":"conf/mm/LiuHQLW24","ArXiv":"2311.16482","DOI":"10.48550/arXiv.2311.16482","CorpusId":265466675},"title":"Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars"},{"paperId":"1206b05eae5a06ba662ae79fb291b50e359c4f42","externalIds":{"ArXiv":"2311.15127","DBLP":"journals/corr/abs-2311-15127","DOI":"10.48550/arXiv.2311.15127","CorpusId":265312551},"title":"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets"},{"paperId":"bbc6531afdfe41fe8664002a80d9d73a07a080d2","externalIds":{"DBLP":"conf/cvpr/ChenCZWYWCYLL24","ArXiv":"2311.14521","DOI":"10.1109/CVPR52733.2024.02029","CorpusId":265445359},"title":"GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting"},{"paperId":"f673d3a0f98094190c5ed1fe8e18673803f1d38c","externalIds":{"ArXiv":"2311.13384","DBLP":"journals/tvcg/0002LNLL25","DOI":"10.1109/TVCG.2025.3611489","CorpusId":265351721,"PubMed":"40966147"},"title":"LucidDreamer: Domain-Free Generation of 3D Gaussian Splatting Scenes"},{"paperId":"e3f80d950e6f841bd7eea4c24d4e1e5aa2bd85c7","externalIds":{"DBLP":"journals/corr/abs-2311-12775","ArXiv":"2311.12775","DOI":"10.1109/CVPR52733.2024.00512","CorpusId":265308825},"title":"SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering"},{"paperId":"7b5fd8828f23c723bc3cd40c7d5310df710c1cbd","externalIds":{"DBLP":"journals/corr/abs-2311-12024","ArXiv":"2311.12024","DOI":"10.48550/arXiv.2311.12024","CorpusId":265295290},"title":"PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction"},{"paperId":"9e4d7b3504c0900d596a432278d0deca6243a0db","externalIds":{"DBLP":"journals/corr/abs-2311-09217","ArXiv":"2311.09217","DOI":"10.48550/arXiv.2311.09217","CorpusId":265213192},"title":"DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model"},{"paperId":"cad7ec01f4acb2ea4260b8bb7f8de920f538eaff","externalIds":{"ArXiv":"2311.08581","DBLP":"conf/3dim/ZielonkaBSZTR25","DOI":"10.1109/3DV66043.2025.00095","CorpusId":265213240},"title":"Drivable 3D Gaussian Avatars"},{"paperId":"05782b157d5468c08de9dbde9635ee9902f40b60","externalIds":{"DBLP":"conf/cvpr/LiuSCZXW00G024","ArXiv":"2311.07885","DOI":"10.1109/CVPR52733.2024.00960","CorpusId":265157982},"title":"One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion"},{"paperId":"3af134a559a618b3185390646d49d1d4e7ffab45","externalIds":{"ArXiv":"2311.06214","DBLP":"journals/corr/abs-2311-06214","DOI":"10.48550/arXiv.2311.06214","CorpusId":265128529},"title":"Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model"},{"paperId":"eb2cbd12f749f14716296f7f415e921562c9079b","externalIds":{"ArXiv":"2311.04400","DBLP":"conf/iclr/Hong0GBZLLSB024","DOI":"10.48550/arXiv.2311.04400","CorpusId":265050698},"title":"LRM: Large Reconstruction Model for Single Image to 3D"},{"paperId":"3b94dba88b1b6f09f5928a8741eb7036ce106645","externalIds":{"ArXiv":"2311.01015","DBLP":"conf/nips/JinWFSY023","DOI":"10.48550/arXiv.2311.01015","CorpusId":264935374},"title":"Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs"},{"paperId":"d2c5565a039f464b778e0f2263da418ef42e98b0","externalIds":{"DBLP":"conf/cvpr/LongGLLDLMZHTW24","ArXiv":"2310.15008","DOI":"10.1109/CVPR52733.2024.00951","CorpusId":264436465},"title":"Wonder3D: Single Image to 3D Using Cross-Domain Diffusion"},{"paperId":"186c2b2386799fa599e2d77bc6945f7da66e5fab","externalIds":{"DBLP":"journals/corr/abs-2310-15110","ArXiv":"2310.15110","DOI":"10.48550/arXiv.2310.15110","CorpusId":264436559},"title":"Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model"},{"paperId":"d1d6cbfcdcfb30e692132c3e893dd4230720ddb9","externalIds":{"ArXiv":"2310.11784","DBLP":"conf/iclr/ChengYWL0Z024","DOI":"10.48550/arXiv.2310.11784","CorpusId":264288859},"title":"Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","externalIds":{"ArXiv":"2310.10647","DBLP":"journals/corr/abs-2310-10647","DOI":"10.1145/3696415","CorpusId":264172934},"title":"A Survey on Video Diffusion Models"},{"paperId":"2b11dc5fb3cf109b05c041c549eebbe22946a8b8","externalIds":{"DBLP":"conf/iclr/ShiWCTQYHL0S24","ArXiv":"2310.10644","DOI":"10.48550/arXiv.2310.10644","CorpusId":264172720},"title":"TOSS: High-quality Text-guided Novel View Synthesis from a Single Image"},{"paperId":"6487ec82f6d8082a5b402a5416ea03009acb1679","externalIds":{"DBLP":"journals/cgf/PoYGABBCDHKLLMNOTWW24","ArXiv":"2310.07204","DOI":"10.1111/cgf.15063","CorpusId":263835355},"title":"State of the Art on Diffusion Models for Visual Computing"},{"paperId":"9fc74f4201d6fdcf3fe5419356512d537b340842","externalIds":{"ArXiv":"2310.03020","DBLP":"conf/3dim/YeWLSW24","DOI":"10.1109/3DV62453.2024.00027","CorpusId":263620211},"title":"Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models"},{"paperId":"438e9fb79c9e37d43223e61bb575ebd2dae0b0a7","externalIds":{"DBLP":"conf/iclr/LiCCT24","ArXiv":"2310.02596","DOI":"10.48550/arXiv.2310.02596","CorpusId":263620393},"title":"SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D"},{"paperId":"04d9cc34fdd128ae94819a5a0bcbc99cc16e1f38","externalIds":{"ArXiv":"2310.01406","DBLP":"journals/corr/abs-2310-01406","DOI":"10.1109/CVPR52733.2024.00437","CorpusId":263605906},"title":"HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation"},{"paperId":"53e5551310a69697a2183464da5a499095904398","externalIds":{"DBLP":"conf/iccv/YangYZMR23","DOI":"10.1109/ICCV51070.2023.00362","CorpusId":267026659},"title":"PPR: Physically Plausible Reconstruction from Monocular Videos"},{"paperId":"778b5d957b1423c3f19143b47775fc8ffc50da52","externalIds":{"ArXiv":"2309.17261","DBLP":"conf/mm/LinHGX0024","DOI":"10.1145/3664647.3680994","CorpusId":263310837},"title":"Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors"},{"paperId":"86b5318b0a69ccdeec17abb0120e4bd7688a4b59","externalIds":{"ArXiv":"2309.16585","DBLP":"journals/corr/abs-2309-16585","DOI":"10.1109/CVPR52733.2024.02022","CorpusId":263139613},"title":"Text-to-3D using Gaussian Splatting"},{"paperId":"cc1a674bb164d09a060cf5b26fe518c02fae0ddc","externalIds":{"DBLP":"journals/corr/abs-2309-16653","ArXiv":"2309.16653","DOI":"10.48550/arXiv.2309.16653","CorpusId":263131552},"title":"DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation"},{"paperId":"de8412872a0577da2c082facca83af41ed60d599","externalIds":{"DBLP":"journals/corr/abs-2309-07125","ArXiv":"2309.07125","DOI":"10.1109/3DV62453.2024.00151","CorpusId":261705909},"title":"TECA: Text-Guided Generation and Editing of Compositional 3D Avatars"},{"paperId":"fcd0de4066d93fa3822a14898008fa2dd99f7be6","externalIds":{"DBLP":"conf/iclr/LiuLZLLKW24","ArXiv":"2309.03453","DOI":"10.48550/arXiv.2309.03453","CorpusId":261582503},"title":"SyncDreamer: Generating Multiview-consistent Images from a Single-view Image"},{"paperId":"43f8b8b750d4505c456c422422b8227d6164cbed","externalIds":{"DBLP":"journals/corr/abs-2309-00796","ArXiv":"2309.00796","DOI":"10.1109/ICCV51070.2023.00053","CorpusId":261530775},"title":"AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism"},{"paperId":"9aa01997226b5c4d705ae2e2f52c32681006654b","externalIds":{"DBLP":"conf/iclr/ShiWYMLY24","ArXiv":"2308.16512","DOI":"10.48550/arXiv.2308.16512","CorpusId":261395233},"title":"MVDream: Multi-view Diffusion for 3D Generation"},{"paperId":"303f466fb823112f79a9f36637c7084dd8363fc5","externalIds":{"DBLP":"journals/corr/abs-2308-10899","ArXiv":"2308.10899","DOI":"10.1109/3DV62453.2024.00150","CorpusId":261064940},"title":"TADA! Text to Animatable Digital Avatars"},{"paperId":"6b403d2a98c2c0639f22c19a4d53518651578540","externalIds":{"DBLP":"journals/corr/abs-2308-09705","ArXiv":"2308.09705","DOI":"10.48550/arXiv.2308.09705","CorpusId":261031006},"title":"Guide3D: Create 3D Avatars from Text and Image Guidance"},{"paperId":"50cb348c5853d7c5843c949fa1db4eb9c1d9994d","externalIds":{"DBLP":"conf/3dim/HuangYXLTCT24","ArXiv":"2308.08545","DOI":"10.1109/3DV62453.2024.00152","CorpusId":260926600},"title":"TeCH: Text-Guided Reconstruction of Lifelike Clothed Humans"},{"paperId":"32d3048a4fe4becc7c4638afd05f2354b631cfca","externalIds":{"DOI":"10.1145/3596711.3596800","CorpusId":5328073},"title":"SMPL: A Skinned Multi-Person Linear Model"},{"paperId":"2cc1d857e86d5152ba7fe6a8355c2a0150cc280a","externalIds":{"DBLP":"journals/tog/KerblKLD23","ArXiv":"2308.04079","DOI":"10.1145/3592433","CorpusId":259267917},"title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering"},{"paperId":"d2413bc6ce6a2ca90c065f28f7f3f2b7cd0708d9","externalIds":{"DBLP":"journals/tog/ShenMHYWCGFSG23","ArXiv":"2308.05371","DOI":"10.1145/3592430","CorpusId":260167800},"title":"Flexible Isosurface Extraction for Gradient-Based Mesh Optimization"},{"paperId":"051a7d88d82734bb143341815b5cceb1f9014f54","externalIds":{"DBLP":"journals/pami/ZhuMRCZSGTW24","ArXiv":"2307.10894","DOI":"10.1109/TPAMI.2023.3330935","CorpusId":263796023,"PubMed":"37938938"},"title":"Human Motion Generation: A Survey"},{"paperId":"1b90e9e9734bed6b379ae87d688cb3b887baf597","externalIds":{"DBLP":"conf/nips/DeitkeLWNMKFLVG23","ArXiv":"2307.05663","DOI":"10.48550/arXiv.2307.05663","CorpusId":259836993},"title":"Objaverse-XL: A Universe of 10M+ 3D Objects"},{"paperId":"2605f0776dcda957fa540f88cdec53badab848b1","externalIds":{"ArXiv":"2306.17843","DBLP":"journals/corr/abs-2306-17843","DOI":"10.48550/arXiv.2306.17843","CorpusId":259309218},"title":"Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors"},{"paperId":"c22ef963b2388cdbbfcc7a00b24f68710a7febd2","externalIds":{"ArXiv":"2306.16928","DBLP":"journals/corr/abs-2306-16928","DOI":"10.48550/arXiv.2306.16928","CorpusId":259286991},"title":"One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization"},{"paperId":"11bc5494d733d935bf865995cd82e709fafa9d53","externalIds":{"DBLP":"journals/corr/abs-2306-09864","ArXiv":"2306.09864","DOI":"10.48550/arXiv.2306.09864","CorpusId":259187900},"title":"AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation"},{"paperId":"a0f8bc8e59430ce0edc0be0422d7cd1075419d24","externalIds":{"DBLP":"conf/siggrapha/HuHLZF23","ArXiv":"2306.08226","DOI":"10.1145/3610548.3618144","CorpusId":259164518},"title":"CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration"},{"paperId":"1e8403af2e1e7a8f803d8df9e8daac584f99c2a0","externalIds":{"DBLP":"journals/corr/abs-2306-07349","ArXiv":"2306.07349","DOI":"10.1109/ICCV51070.2023.01645","CorpusId":259145353},"title":"ATT3D: Amortized Text-to-3D Object Synthesis"},{"paperId":"4e8cf9602d4ef714dcdb8580de40e1a2a717ab11","externalIds":{"DBLP":"journals/corr/abs-2306-03038","ArXiv":"2306.03038","DOI":"10.48550/arXiv.2306.03038","CorpusId":259076344},"title":"HeadSculpt: Crafting 3D Head Avatars with Text"},{"paperId":"6aba63c191f098f3d828cc8fc2c9895c7c4426b6","externalIds":{"DBLP":"conf/cvpr/VoPYTL23","DOI":"10.1109/CVPRW59228.2023.00375","CorpusId":260780167},"title":"DNA: Deformable Neural Articulations Network for Template-free Dynamic 3D Human Reconstruction from Monocular RGB-D Video"},{"paperId":"2f319a4b6aa091073af5dfea4201ec19bda66b6c","externalIds":{"DBLP":"journals/tog/MendirattaPETRTGKT23","ArXiv":"2306.00547","DOI":"10.1145/3618368","CorpusId":258999776},"title":"AvatarStudio: Text-Driven Editing of 3D Dynamic Human Head Avatars"},{"paperId":"d865fe9a755473301f0560c1d3b102035a14848a","externalIds":{"DBLP":"conf/iccv/0001PRKM23","ArXiv":"2305.20091","DOI":"10.1109/ICCV51070.2023.01358","CorpusId":258987755},"title":"Humans in 4D: Reconstructing and Tracking Humans with Transformers"},{"paperId":"c5e9fd131cde68c218d0ea69cd617a67c7f35d42","externalIds":{"DBLP":"conf/nips/Wang00BL0023","ArXiv":"2305.16213","DOI":"10.48550/arXiv.2305.16213","CorpusId":258887357},"title":"ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation"},{"paperId":"7ed1961910f83d7c30725cf360e60196299154fb","externalIds":{"ArXiv":"2305.16411","DBLP":"journals/corr/abs-2305-16411","DOI":"10.48550/arXiv.2305.16411","CorpusId":258947774},"title":"ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image"},{"paperId":"7316596b1f02f288e3b76546d90646524e35fd40","externalIds":{"ArXiv":"2305.12529","DBLP":"journals/corr/abs-2305-12529","DOI":"10.48550/arXiv.2305.12529","CorpusId":258833547},"title":"DreamWaltz: Make a Scene with Complex 3D Animatable Avatars"},{"paperId":"78ebaef85485dc605fabdf72b24770a3deb582ac","externalIds":{"ArXiv":"2305.06351","DBLP":"conf/cvpr/YangWRR23","DOI":"10.1109/CVPR52729.2023.01630","CorpusId":258588310},"title":"Reconstructing Animatable Categories from Videos"},{"paperId":"e01ed6611f9c998c237cda814ff8366a5acb6c3d","externalIds":{"ArXiv":"2305.06131","DBLP":"journals/corr/abs-2305-06131","DOI":"10.48550/arXiv.2305.06131","CorpusId":258588157},"title":"Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era"},{"paperId":"012d7d3ee690e5acadf416787651a8fe425e8eb3","externalIds":{"DBLP":"conf/cvpr/WuZHZLC23","ArXiv":"2305.03302","DOI":"10.1109/CVPR52729.2023.00439","CorpusId":258547320},"title":"High-fidelity 3D Face Generation from Natural Language Descriptions"},{"paperId":"dc7b2a39421f93ae134fbb43ef62763056ebe1c7","externalIds":{"DBLP":"journals/corr/abs-2305-02463","ArXiv":"2305.02463","DOI":"10.48550/arXiv.2305.02463","CorpusId":258480331},"title":"Shap-E: Generating Conditional 3D Implicit Functions"},{"paperId":"daec9129f0fe200493d204963cd1a71e640725cb","externalIds":{"ArXiv":"2305.00976","DBLP":"journals/corr/abs-2305-00976","DOI":"10.1109/ICCV51070.2023.00870","CorpusId":258436810},"title":"TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis"},{"paperId":"5e56d99b5a9b6768a7313bcdfd464af3742ff9bd","externalIds":{"DBLP":"conf/iccv/MuSV023","ArXiv":"2304.14401","DOI":"10.1109/ICCV51070.2023.01686","CorpusId":258352406},"title":"ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs"},{"paperId":"5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891","externalIds":{"DBLP":"journals/corr/abs-2304-07193","ArXiv":"2304.07193","DOI":"10.48550/arXiv.2304.07193","CorpusId":258170077},"title":"DINOv2: Learning Robust Visual Features without Supervision"},{"paperId":"e966b71579cec41aa98d24d45ea2f6f76aceb7e2","externalIds":{"DBLP":"conf/cvpr/DingZXJTZ23","ArXiv":"2304.06711","DOI":"10.1109/CVPR52729.2023.01225","CorpusId":258108166},"title":"DiffusionRig: Learning Personalized Priors for Facial Appearance Editing"},{"paperId":"cf923fb70bbad20c485cef355444a08096747f68","externalIds":{"DBLP":"journals/corr/abs-2304-02602","ArXiv":"2304.02602","DOI":"10.1109/ICCV51070.2023.00389","CorpusId":257952179},"title":"Generative Novel View Synthesis with 3D-Aware Diffusion Models"},{"paperId":"b5fb909d436856ba7c4d5e15bfdb83a847e7ff8a","externalIds":{"ArXiv":"2304.02001","DBLP":"journals/corr/abs-2304-02001","DOI":"10.1109/CVPR52729.2023.01625","CorpusId":257921835},"title":"MonoHuman: Animatable Human Neural Field from Monocular Video"},{"paperId":"0fa1501c7378a0dca2ac913fce9dcdcc2b1958a7","externalIds":{"DBLP":"conf/cvpr/CaoC0SW24","ArXiv":"2304.00916","DOI":"10.1109/CVPR52733.2024.00097","CorpusId":257912580},"title":"DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models"},{"paperId":"731ac2fd7a5586d17cb8959c190158d429821a60","externalIds":{"ArXiv":"2304.01116","DBLP":"conf/iccv/ZhangGPCHLYL23","DOI":"10.1109/ICCV51070.2023.00040","CorpusId":257913363},"title":"ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model"},{"paperId":"21ec358dcd5d54ec1b512748c369d0a05e3daddc","externalIds":{"ArXiv":"2304.00359","DBLP":"journals/corr/abs-2304-00359","DOI":"10.1109/CVPR52729.2023.00451","CorpusId":257913858},"title":"SeSDF: Self-Evolved Signed Distance Field for Implicit 3D Clothed Human Reconstruction"},{"paperId":"836f0d803332853bb12a89495ea30f0e91c97bf6","externalIds":{"DBLP":"journals/corr/abs-2303-17606","ArXiv":"2303.17606","DOI":"10.1109/ICCV51070.2023.01322","CorpusId":257834153},"title":"AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control"},{"paperId":"0cbb518c364067200476a51e5ce7476a4f582770","externalIds":{"DBLP":"journals/corr/abs-2303-13873","ArXiv":"2303.13873","DOI":"10.1109/ICCV51070.2023.02033","CorpusId":257757213},"title":"Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation"},{"paperId":"e996c09708a56ce074f4e72d1ae910a9a39f8b4f","externalIds":{"DBLP":"journals/corr/abs-2303-13497","ArXiv":"2303.13497","DOI":"10.1109/WACV57701.2024.00303","CorpusId":257687746},"title":"TriPlaneNet: An Encoder for EG3D Inversion"},{"paperId":"26c22380282a00166273038bc5ba785d845d61ad","externalIds":{"DBLP":"conf/iccv/HaqueTEHK23","ArXiv":"2303.12789","DOI":"10.1109/ICCV51070.2023.01808","CorpusId":257663414},"title":"Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions"},{"paperId":"ff3e319f4b41d0cacd1100a35303489107a4ddbf","externalIds":{"ArXiv":"2303.12791","DBLP":"conf/iccv/HuHPMYL23","DOI":"10.1109/ICCV51070.2023.00858","CorpusId":257663613},"title":"SHERF: Generalizable Human NeRF from a Single Image"},{"paperId":"602417aec279a68efeacfbf2df587384cbfef370","externalIds":{"ArXiv":"2303.12048","DBLP":"conf/iccv/SellaFHA23","DOI":"10.1109/ICCV51070.2023.00046","CorpusId":257636627},"title":"Vox-E: Text-guided Voxel Editing of 3D Objects"},{"paperId":"2c70684973bc4d7b6f8404a647b8031c4d3c8383","externalIds":{"ArXiv":"2303.11328","DBLP":"journals/corr/abs-2303-11328","DOI":"10.1109/ICCV51070.2023.00853","CorpusId":257631738},"title":"Zero-1-to-3: Zero-shot One Image to 3D Object"},{"paperId":"5356c3dac654854a0842753bcc2e3433dc4a2afd","externalIds":{"ArXiv":"2303.07937","DBLP":"journals/corr/abs-2303-07937","DOI":"10.48550/arXiv.2303.07937","CorpusId":257505182},"title":"Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation"},{"paperId":"ba3b2273f168d87f0a224ed142ec07c8c82e758c","externalIds":{"ArXiv":"2302.11566","DBLP":"journals/corr/abs-2302-11566","DOI":"10.1109/CVPR52729.2023.01236","CorpusId":257078760},"title":"Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition"},{"paperId":"84cce9b8aea35e4fa38eef63da439573f21c0728","externalIds":{"ArXiv":"2302.10663","DBLP":"conf/cvpr/Melas-KyriaziL023","DOI":"10.1109/CVPR52729.2023.00816","CorpusId":261092262},"title":"RealFusion 360Â° Reconstruction of Any Object from a Single Image"},{"paperId":"df1ab83959b3dc49a2f4bb93dbe5f44e1c0db2f6","externalIds":{"DBLP":"journals/corr/abs-2302-08504","ArXiv":"2302.08504","DOI":"10.1109/CVPR52729.2023.00058","CorpusId":256900741},"title":"PersonNeRF : Personalized Reconstruction from Photo Collections"},{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","externalIds":{"DBLP":"journals/corr/abs-2302-05543","ArXiv":"2302.05543","DOI":"10.1109/ICCV51070.2023.00355","CorpusId":256827727},"title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"5affc264027ec9fc95709c059613333b946cf271","externalIds":{"DBLP":"journals/corr/abs-2302-01721","ArXiv":"2302.01721","DOI":"10.1145/3588432.3591503","CorpusId":256597953},"title":"TEXTure: Text-Guided Texturing of 3D Shapes"},{"paperId":"8848d097f07bae98ff01094cf87e59eee94e4692","externalIds":{"DBLP":"journals/corr/abs-2301-07525","ArXiv":"2301.07525","DOI":"10.1109/CVPR52729.2023.00084","CorpusId":255998491},"title":"OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation"},{"paperId":"99bd96961a880adad88d087c9dc45f6cfbae1ea6","externalIds":{"DBLP":"conf/cvpr/ZhangZCZZLSY23","ArXiv":"2301.06052","DOI":"10.1109/CVPR52729.2023.01415","CorpusId":255942203},"title":"Generating Human Motion from Textual Descriptions with Discrete Representations"},{"paperId":"12f99b597fd65c9eb730cfef498b47f3fb3a5ec8","externalIds":{"DBLP":"conf/cvpr/AbdalL0CSWT23","ArXiv":"2301.02700","DOI":"10.1109/CVPR52729.2023.00442","CorpusId":255546292},"title":"3DAvatarGAN: Bridging Domains for Personalized Editable Avatars"},{"paperId":"9432bd77fa374d794bf2c8703b0e0e460380771f","externalIds":{"ArXiv":"2212.14704","DBLP":"conf/cvpr/XuW0CSQG23","DOI":"10.1109/CVPR52729.2023.02003","CorpusId":255340806},"title":"Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models"},{"paperId":"1b8a734dd28a9d766a5d3dbc0871e76b6a452b65","externalIds":{"ArXiv":"2212.08751","DBLP":"journals/corr/abs-2212-08751","DOI":"10.48550/arXiv.2212.08751","CorpusId":254854214},"title":"Point-E: A System for Generating 3D Point Clouds from Complex Prompts"},{"paperId":"1b31dbf44e68b698120552366df03e6e35a1e428","externalIds":{"ArXiv":"2212.08051","DBLP":"conf/cvpr/DeitkeSSWMVSEKF23","DOI":"10.1109/CVPR52729.2023.01263","CorpusId":254685588},"title":"Objaverse: A Universe of Annotated 3D Objects"},{"paperId":"7e993a9ca01dcd4538362454aaac29a18a63c000","externalIds":{"ArXiv":"2212.06135","DBLP":"conf/cvpr/Wang0ZGBBS00CG23","DOI":"10.1109/CVPR52729.2023.00443","CorpusId":254564523},"title":"RODIN: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion"},{"paperId":"1058616afc6e73a95b459a0f3f54b193a4602dc4","externalIds":{"DBLP":"journals/corr/abs-2212-05321","ArXiv":"2212.05321","DOI":"10.1109/CVPR52729.2023.01207","CorpusId":254564767},"title":"HumanGen: Generating Human Radiance Fields with Explicit Priors"},{"paperId":"9d5e20f90fa3e7b126641ac6df5f42478ff7fb24","externalIds":{"DBLP":"journals/corr/abs-2212-02500","ArXiv":"2212.02500","DOI":"10.1109/ICCV51070.2023.01467","CorpusId":254246839},"title":"PhysDiff: Physics-Guided Human Motion Diffusion Model"},{"paperId":"b4ece600c6dadd41b0b38d8359ce8e5b544305a9","externalIds":{"ArXiv":"2212.00792","DBLP":"conf/cvpr/ZhouT23","DOI":"10.1109/CVPR52729.2023.01211","CorpusId":254125457},"title":"SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction"},{"paperId":"2808b8bf0508dc0890a4f765afb95496b45d758a","externalIds":{"ArXiv":"2211.11208","DBLP":"conf/cvpr/SunWWLZZL23","DOI":"10.1109/CVPR52729.2023.02011","CorpusId":253735045},"title":"Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars"},{"paperId":"60ec9d5580bb3266c2cc8606887ac5663c44b461","externalIds":{"DBLP":"journals/corr/abs-2211-11610","ArXiv":"2211.11610","DOI":"10.1109/CVPR52729.2023.01596","CorpusId":253734521},"title":"Tensor4D: Efficient Neural 4D Decomposition for High-Fidelity Dynamic Reconstruction and Rendering"},{"paperId":"bdf4af8311637c681904e71cf50f96fd0026f578","externalIds":{"DBLP":"journals/corr/abs-2211-10440","ArXiv":"2211.10440","DOI":"10.1109/CVPR52729.2023.00037","CorpusId":253708074},"title":"Magic3D: High-Resolution Text-to-3D Content Creation"},{"paperId":"24b9ceb67b48c5d85658031342cc1431bc4d4b92","externalIds":{"ArXiv":"2211.07955","DBLP":"journals/corr/abs-2211-07955","DOI":"10.1007/978-3-031-20086-1_19","CorpusId":253523325},"title":"IntegratedPIFu: Integrated Pixel Aligned Implicit Function for Single-View Human Reconstruction"},{"paperId":"793939b83e10903f58d8edbb7534963df627a1fe","externalIds":{"DBLP":"journals/corr/abs-2211-07600","ArXiv":"2211.07600","DOI":"10.1109/CVPR52729.2023.01218","CorpusId":253510536},"title":"Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures"},{"paperId":"6d9b0bb216c36aba29a5b11a656a468dc39cad66","externalIds":{"DBLP":"journals/corr/abs-2210-10036","ArXiv":"2210.10036","DOI":"10.48550/arXiv.2210.10036","CorpusId":252968216},"title":"ARAH: Animatable Volume Rendering of Articulated Human SDFs"},{"paperId":"73fcfd1155cd7abfbdbd989c756da0014b7218fb","externalIds":{"DBLP":"journals/corr/abs-2210-06108","ArXiv":"2210.06108","DOI":"10.1145/3550454.3555501","CorpusId":252846034},"title":"Reconstructing Personalized Semantic Facial NeRF Models from Monocular Video"},{"paperId":"8cb9f266e2a051301352493b3e8c480195c08424","externalIds":{"DBLP":"journals/corr/abs-2210-04888","ArXiv":"2210.04888","DOI":"10.48550/arXiv.2210.04888","CorpusId":252780848},"title":"EVA3D: Compositional 3D Human Generation from 2D Image Collections"},{"paperId":"23a1b0fdb857fbbe328ffe254df33ff615acc5ea","externalIds":{"DBLP":"journals/corr/abs-2210-04628","ArXiv":"2210.04628","DOI":"10.48550/arXiv.2210.04628","CorpusId":252780361},"title":"Novel View Synthesis with Diffusion Models"},{"paperId":"15736f7c205d961c00378a938daffaacb5a0718d","externalIds":{"DBLP":"journals/corr/abs-2209-14916","ArXiv":"2209.14916","DOI":"10.48550/arXiv.2209.14916","CorpusId":252595883},"title":"Human Motion Diffusion Model"},{"paperId":"4c94d04afa4309ec2f06bdd0fe3781f91461b362","externalIds":{"DBLP":"conf/iclr/PooleJBM23","ArXiv":"2209.14988","DOI":"10.48550/arXiv.2209.14988","CorpusId":252596091},"title":"DreamFusion: Text-to-3D using 2D Diffusion"},{"paperId":"a57d47b762341340656d5b5caa84f370d9d31063","externalIds":{"ArXiv":"2209.11163","DBLP":"conf/nips/0004SWCYLLGF22","DOI":"10.48550/arXiv.2209.11163","CorpusId":252438648},"title":"GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images"},{"paperId":"efa1647594b236361610a20d507127f0586a379b","externalIds":{"DBLP":"journals/corr/abs-2209-04747","ArXiv":"2209.04747","DOI":"10.1109/TPAMI.2023.3261988","CorpusId":252199918,"PubMed":"37030794"},"title":"Diffusion Models in Vision: A Survey"},{"paperId":"35a29c47d5292e8967e5a9a8a21b23d8637b7d07","externalIds":{"DBLP":"journals/tkde/CaoTGXCHL24","ArXiv":"2209.02646","DOI":"10.1109/TKDE.2024.3361474","CorpusId":265039918},"title":"A Survey on Generative Diffusion Models"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","externalIds":{"DBLP":"journals/corr/abs-2209-00796","ArXiv":"2209.00796","DOI":"10.1145/3626235","CorpusId":252070859},"title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"1bbf99b5bfe9869876ac3bdd2999e16b2632c283","externalIds":{"ArXiv":"2208.15001","DBLP":"journals/pami/ZhangCPHGYL24","DOI":"10.1109/TPAMI.2024.3355414","CorpusId":251953565,"PubMed":"38285589"},"title":"MotionDiffuse: Text-Driven Human Motion Generation With Diffusion Model"},{"paperId":"5406129d9d7d00dc310671c43597101b0ee93629","externalIds":{"ArXiv":"2208.01618","DBLP":"journals/corr/abs-2208-01618","DOI":"10.48550/arXiv.2208.01618","CorpusId":251253049},"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"},{"paperId":"1b3c9965a5f4d5fceb0d752c59cf4444a1d9cada","externalIds":{"ArXiv":"2211.14589","DBLP":"journals/corr/abs-2208-00561","DOI":"10.48550/arXiv.2208.00561","CorpusId":251224064},"title":"AvatarGen: a 3D Generative Model for Animatable Human Avatars"},{"paperId":"c91eec8943a376bfb15db40c9cc5679ed8c801d5","externalIds":{"DBLP":"conf/nips/BergmanKWCLW22","ArXiv":"2206.14314","DOI":"10.48550/arXiv.2206.14314","CorpusId":250113850},"title":"Generative Neural Articulated Radiance Fields"},{"paperId":"5a365ad81138028dacf989317f34f46852e3e88e","externalIds":{"ArXiv":"2206.08929","DBLP":"conf/eccv/LiTVZGKL22","DOI":"10.48550/arXiv.2206.08929","CorpusId":249848144},"title":"TAVA: Template-free Animatable Volumetric Actors"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","externalIds":{"DBLP":"journals/corr/abs-2205-11487","ArXiv":"2205.11487","DOI":"10.48550/arXiv.2205.11487","CorpusId":248986576},"title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"e4d66b15fce00531b96af6330238301ebbb76291","externalIds":{"ArXiv":"2204.11823","DBLP":"journals/corr/abs-2204-11823","DOI":"10.48550/arXiv.2204.11823","CorpusId":248377018},"title":"StyleGAN-Human: A Data-Centric Odyssey of Human Generation"},{"paperId":"5754fe1e0905cf0890e967622511ff81ced3b9ac","externalIds":{"DBLP":"conf/cvpr/CaoC0YW22","ArXiv":"2204.10549","DOI":"10.1109/CVPR52688.2022.00275","CorpusId":248366560},"title":"JIFF: Jointly-aligned Implicit Face Function for High Quality Single View Clothed Human Reconstruction"},{"paperId":"33bd7d7fba56e7b74691b286e0d148803b709b2a","externalIds":{"DBLP":"journals/corr/abs-2204-08839","ArXiv":"2204.08839","DOI":"10.48550/arXiv.2204.08839","CorpusId":248239659},"title":"Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations"},{"paperId":"f7271bfcf9bcc537d8c500fce0e7ddab7c3ee382","externalIds":{"ArXiv":"2203.16875","DBLP":"journals/pami/GaoYKPLT25","DOI":"10.1109/TPAMI.2022.3205910","CorpusId":247839362,"PubMed":"36094968"},"title":"MPS-NeRF: Generalizable 3D Human Rendering From Multiview Images"},{"paperId":"8941e477b2f39eb92712f04400412da60d349ec1","externalIds":{"DBLP":"conf/siggrapha/KhalidXBP22","ArXiv":"2203.13333","DOI":"10.1145/3550469.3555392","CorpusId":252089441},"title":"CLIP-Mesh: Generating textured meshes from text using pretrained image-text models"},{"paperId":"17df7e87a5d25e6e83d773e6f686eb0a85f6827e","externalIds":{"DBLP":"conf/eccv/JiangYSTR22","ArXiv":"2203.12575","DOI":"10.48550/arXiv.2203.12575","CorpusId":247618711},"title":"NeuMan: Neural Human Radiance Field from a Single Video"},{"paperId":"e82df4b6a3628501fce67835ad8316d6525ad133","externalIds":{"ArXiv":"2203.08063","DBLP":"journals/corr/abs-2203-08063","DOI":"10.48550/arXiv.2203.08063","CorpusId":247450907},"title":"MotionCLIP: Exposing Human Motion Generation to CLIP Space"},{"paperId":"f3b1b7f230a9d8f8a50cd88734aaabd1eb8e6e44","externalIds":{"DBLP":"journals/pami/TianZLW23","ArXiv":"2203.01923","DOI":"10.1109/TPAMI.2023.3298850","CorpusId":247222795,"PubMed":"37494160"},"title":"Recovering 3D Human Mesh From Monocular Images: A Survey"},{"paperId":"17a4c0e0e859b8e36a0591f4b5ff26b62e83ea60","externalIds":{"DBLP":"journals/corr/abs-2201-12792","ArXiv":"2201.12792","DOI":"10.1109/CVPR52688.2022.00552","CorpusId":246430332},"title":"SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","externalIds":{"DBLP":"journals/corr/abs-2201-12086","ArXiv":"2201.12086","CorpusId":246411402},"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"f763a59644e27a2215095943224f2564e670a504","externalIds":{"ArXiv":"2201.04127","DBLP":"journals/corr/abs-2201-04127","DOI":"10.1109/CVPR52688.2022.01573","CorpusId":245853751},"title":"HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video"},{"paperId":"9b10376043a63de4d7b6b058996eedb8118b3f4a","externalIds":{"DBLP":"conf/cvpr/XuFM22","ArXiv":"2201.01683","DOI":"10.1109/CVPR52688.2022.01542","CorpusId":245704538},"title":"Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis"},{"paperId":"bbfebb10f41254ff88937feafcc653df1cfacb5d","externalIds":{"DBLP":"conf/cvpr/Or-ElLSSPK22","ArXiv":"2112.11427","DOI":"10.1109/CVPR52688.2022.01314","CorpusId":245353683},"title":"StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation"},{"paperId":"4ffd87551ab02eca22bd6d5ad945c8d036e80b1d","externalIds":{"ArXiv":"2112.09127","DBLP":"journals/corr/abs-2112-09127","DOI":"10.1109/CVPR52688.2022.01294","CorpusId":245219054},"title":"ICON: Implicit Clothed humans Obtained from Normals"},{"paperId":"7c0a7419114db2209c2f386bc1537e90417cf9d4","externalIds":{"DBLP":"conf/cvpr/ChanLCNPMGGTKKW22","ArXiv":"2112.07945","DOI":"10.1109/CVPR52688.2022.01565","CorpusId":245144673},"title":"Efficient Geometry-aware 3D Generative Adversarial Networks"},{"paperId":"792d86d09c2fe992d4ebfb5b80d6dd7ff76e1b3c","externalIds":{"DBLP":"conf/cvpr/ZhengABCBH22","ArXiv":"2112.07471","DOI":"10.1109/CVPR52688.2022.01318","CorpusId":245131174},"title":"I M Avatar: Implicit Morphable Head Avatars from Videos"},{"paperId":"e91f73aaef155391b5b07e6612f5346dea888f64","externalIds":{"ArXiv":"2112.05131","DBLP":"conf/cvpr/Fridovich-KeilY22","DOI":"10.1109/CVPR52688.2022.00542","CorpusId":245006364},"title":"Plenoxels: Radiance Fields without Neural Networks"},{"paperId":"d15b27edf3630728cdb40f49946365d9011641cf","externalIds":{"ArXiv":"2112.03221","DBLP":"conf/cvpr/MichelBLBH22","DOI":"10.1109/CVPR52688.2022.01313","CorpusId":244908764},"title":"Text2Mesh: Text-Driven Neural Stylization for Meshes"},{"paperId":"03e1c3b5fdad9b21bbed3d13af7e8d6c73cbcfa6","externalIds":{"DBLP":"journals/corr/abs-2112-01455","ArXiv":"2112.01455","DOI":"10.1109/CVPR52688.2022.00094","CorpusId":244799255},"title":"Zero-Shot Text-Guided Object Generation with Dream Fields"},{"paperId":"ec90ffa017a2cc6a51342509ce42b81b478aefb3","externalIds":{"ArXiv":"2111.12077","DBLP":"conf/cvpr/BarronMVSH22","DOI":"10.1109/CVPR52688.2022.00539","CorpusId":244488448},"title":"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields"},{"paperId":"4f7eb65f8d3c1eeb97e30f7ac68977ff16e1e942","externalIds":{"ArXiv":"2111.11215","DBLP":"conf/cvpr/0004SC22","DOI":"10.1109/CVPR52688.2022.00538","CorpusId":244477646},"title":"Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction"},{"paperId":"8e970913466a81207230a09f6516bab944563cc0","externalIds":{"ArXiv":"2111.04276","DBLP":"journals/corr/abs-2111-04276","CorpusId":243848115},"title":"Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis"},{"paperId":"102516bd65bcf42837251f579cf15943499be343","externalIds":{"DBLP":"conf/nips/KwonKCF21","ArXiv":"2109.07448","CorpusId":237513692},"title":"Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering"},{"paperId":"05f907e437a14d0db9b7479662d0cd587cd54634","externalIds":{"DBLP":"journals/corr/abs-2108-10842","ArXiv":"2108.10842","DOI":"10.1109/ICCV48922.2021.00541","CorpusId":237278058},"title":"imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose"},{"paperId":"6d45ea67e124a4d4e09677812305274e596023c6","externalIds":{"DBLP":"journals/corr/abs-2108-07845","ArXiv":"2108.07845","DOI":"10.1109/ICCV48922.2021.01086","CorpusId":237195035},"title":"ARCH++: Animation-Ready Clothed Human Reconstruction Revisited"},{"paperId":"f671a09e3e5922e6d38cb77dda8d76d5ceac2a27","externalIds":{"DBLP":"conf/iclr/MengHSSWZE22","ArXiv":"2108.01073","CorpusId":245704504},"title":"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations"},{"paperId":"0b5b6598e3e108147842f35ff66a95d989f9ec89","externalIds":{"DBLP":"journals/corr/abs-2111-05849","MAG":"3185841081","ArXiv":"2111.05849","DOI":"10.1111/cgf.14507","CorpusId":236162433},"title":"Advances in Neural Rendering"},{"paperId":"c1ff08b59f00c44f34dfdde55cd53370733a2c19","externalIds":{"MAG":"3174807077","DBLP":"conf/nips/KarrasALHHLA21","ArXiv":"2106.12423","CorpusId":235606261},"title":"Alias-Free Generative Adversarial Networks"},{"paperId":"cf5647cb2613f5f697729eab567383006dcd4913","externalIds":{"ArXiv":"2106.10689","DBLP":"journals/corr/abs-2106-10689","CorpusId":235490453},"title":"NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","externalIds":{"DBLP":"conf/iclr/HuSWALWWC22","ArXiv":"2106.09685","CorpusId":235458009},"title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"e3a3f61f3ab5f1c310f3062a2e40fed49fc2caa4","externalIds":{"ArXiv":"2106.03798","DBLP":"conf/cvpr/ShaoZZCC0L22","DOI":"10.1109/CVPR52688.2022.01541","CorpusId":244727701},"title":"DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering"},{"paperId":"ffba7fc5a4b350d50c406b8e52a57cc31ac971dd","externalIds":{"DBLP":"conf/cvpr/LassnerZ21","DOI":"10.1109/CVPR46437.2021.00149","CorpusId":235601887},"title":"Pulsar: Efficient Sphere-based Neural Rendering"},{"paperId":"e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60","externalIds":{"DBLP":"conf/nips/XieWYAAL21","ArXiv":"2105.15203","CorpusId":235254713},"title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"},{"paperId":"35e940ace815548f620709d9c1803da34c581e86","externalIds":{"DBLP":"conf/iccv/PengDWZSZB21","ArXiv":"2105.02872","DOI":"10.1109/ICCV48922.2021.01405","CorpusId":238419696},"title":"Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies"},{"paperId":"7e5fefa74f383ec2d77b72b639e3d6f30dea2970","externalIds":{"DBLP":"conf/cvpr/0007ZGLDL21","ArXiv":"2105.01859","DOI":"10.1109/CVPR46437.2021.00569","CorpusId":233305207},"title":"Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors"},{"paperId":"3c8084e019508db5f68fc28d19641435a18876c9","externalIds":{"DBLP":"conf/iccv/ZhengSZ0ZDL21","ArXiv":"2105.00261","DOI":"10.1109/ICCV48922.2021.00618","CorpusId":233481082},"title":"DeepMultiCap: Performance Capture of Multiple Characters Using Sparse Multiview Cameras"},{"paperId":"ad4a0938c48e61b7827869e4ac3baffd0aefab35","externalIds":{"ArXiv":"2104.14294","DBLP":"journals/corr/abs-2104-14294","DOI":"10.1109/ICCV48922.2021.00951","CorpusId":233444273},"title":"Emerging Properties in Self-Supervised Vision Transformers"},{"paperId":"a629321af8e3ee34e4a4e65c23c4d949b89380af","externalIds":{"ArXiv":"2104.08363","DBLP":"journals/corr/abs-2104-08363","DOI":"10.1109/CVPR46437.2021.00511","CorpusId":233296899},"title":"StylePeople: A Generative Model of Fullbody Human Avatars"},{"paperId":"133be7a7e81d7c282ed676a125c6815a985394ce","externalIds":{"ArXiv":"2104.05289","DBLP":"conf/cvpr/0003ZJGLB21","DOI":"10.1109/CVPR46437.2021.00060","CorpusId":233210544},"title":"StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision"},{"paperId":"07c9f7fbf91a03a386e89b24ac2da53ee3550f25","externalIds":{"DBLP":"journals/corr/abs-2104-03110","ArXiv":"2104.03110","DOI":"10.1109/ICCV48922.2021.00571","CorpusId":233168649},"title":"Neural Articulated Radiance Field"},{"paperId":"fbe34c9add92077f1ea0dc2d3cad1ab95c49f7d5","externalIds":{"ArXiv":"2103.07700","DBLP":"journals/corr/abs-2103-07700","DOI":"10.1109/CVPR46437.2021.00616","CorpusId":232233063},"title":"NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras"},{"paperId":"6674629dc3801881ddba498bbba0da285fe9baee","externalIds":{"DBLP":"journals/corr/abs-2103-02690","ArXiv":"2103.02690","CorpusId":232110848},"title":"A comprehensive survey on point cloud registration"},{"paperId":"43b77c7e55a206c7de3449ec7e898bc6f4986734","externalIds":{"DBLP":"journals/tog/LombardiSSZSS21","ArXiv":"2103.01954","DOI":"10.1145/3476576.3476608","CorpusId":232092799},"title":"Mixture of volumetric primitives for efficient neural rendering"},{"paperId":"af8faec7c0b8f4b2a28d42a86e0e7d499016c560","externalIds":{"DBLP":"journals/corr/abs-2012-15838","ArXiv":"2012.15838","DOI":"10.1109/CVPR46437.2021.00894","CorpusId":229924396},"title":"Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans"},{"paperId":"e6c1b2cda9dc3fc31b94a3d8ac03ef83d1ce4e1d","externalIds":{"ArXiv":"2012.13392","DBLP":"journals/csur/ZhengWCYZSKS24","DOI":"10.1145/3603618","CorpusId":195493170},"title":"Deep Learning-based Human Pose Estimation: A Survey"},{"paperId":"4365f51fc270c55005adb794002685078a6fca1d","externalIds":{"DBLP":"conf/cvpr/YuYTK21","ArXiv":"2012.02190","MAG":"3109025584","DOI":"10.1109/CVPR46437.2021.00455","CorpusId":227254854},"title":"pixelNeRF: Neural Radiance Fields from One or Few Images"},{"paperId":"ee2bdf977668d40ecaeeb646bb460e95345bd065","externalIds":{"ArXiv":"2009.09458","MAG":"3087649526","DBLP":"journals/corr/abs-2009-09458","DOI":"10.1007/978-3-030-66096-3_48","CorpusId":221819535},"title":"Implicit Feature Networks for Texture Completion from Partial 3D Data"},{"paperId":"03e5d09cc7682df9f3bdac63fcec87eb298fcff8","externalIds":{"DBLP":"journals/corr/abs-2007-11432","ArXiv":"2007.11432","MAG":"3101022589","DOI":"10.1007/978-3-030-58536-5_19","CorpusId":220686868},"title":"Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction"},{"paperId":"5ab1c70f9cc0d692dea6b392bdb8f3100d7ca5a7","externalIds":{"DBLP":"journals/corr/abs-2007-03858","ArXiv":"2007.03858","MAG":"3041416670","DOI":"10.1109/TPAMI.2021.3050505","CorpusId":220404307,"PubMed":"33434121"},"title":"PaMIR: Parametric Model-Conditioned Implicit Representation for Image-Based Human Reconstruction"},{"paperId":"942dac57251ec1b8af129096a5cd417c06362a8a","externalIds":{"DBLP":"journals/corr/abs-2006-08072","ArXiv":"2006.08072","MAG":"3035003225","CorpusId":219687707},"title":"Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction"},{"paperId":"e3d5ee10d0489c768e943546038e3f53e7697349","externalIds":{"DBLP":"journals/corr/abs-2004-03805","ArXiv":"2004.03805","MAG":"3016007010","DOI":"10.1111/cgf.14022","CorpusId":215416317},"title":"State of the Art on Neural Rendering"},{"paperId":"0ff2c939d136df8988f845ae5cdfb725939a82ab","externalIds":{"ArXiv":"2004.04572","MAG":"3016099659","DBLP":"conf/cvpr/HuangXL0T20","DOI":"10.1109/cvpr42600.2020.00316","CorpusId":215548941},"title":"ARCH: Animatable Reconstruction of Clothed Humans"},{"paperId":"3d0148ea1425d87d3b812139599dff61bb09df9a","externalIds":{"MAG":"3035291735","DBLP":"journals/corr/abs-2004-00452","ArXiv":"2004.00452","DOI":"10.1109/cvpr42600.2020.00016","CorpusId":214743286},"title":"PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization"},{"paperId":"5a6732513a1dc0bea059543f208a7556e3e31067","externalIds":{"DBLP":"conf/eccv/PengNMP020","ArXiv":"2003.04618","MAG":"3011697381","DOI":"10.1007/978-3-030-58580-8_31","CorpusId":212646575},"title":"Convolutional Occupancy Networks"},{"paperId":"9b0e23435d4527659d27174e260bf6c6ed2eb670","externalIds":{"DBLP":"journals/corr/abs-2002-08988","MAG":"3008721991","ArXiv":"2002.08988","CorpusId":211252382},"title":"BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images"},{"paperId":"0311ace1d499cadd1cc0c515a625d1d045f60d25","externalIds":{"MAG":"2997320365","DBLP":"journals/corr/abs-1912-12033","ArXiv":"1912.12033","DOI":"10.1109/TPAMI.2020.3005434","CorpusId":209501181,"PubMed":"32750799"},"title":"Deep Learning for 3D Point Clouds: A Survey"},{"paperId":"0d4eb14da547ef3dc5c8ecfa35b14937ecd21bc5","externalIds":{"DBLP":"journals/corr/abs-1912-03207","MAG":"2993562255","ArXiv":"1912.03207","DOI":"10.1007/978-3-030-58571-6_36","CorpusId":208857381},"title":"NASA: Neural Articulated Shape Approximation"},{"paperId":"e6c71df73d6a77f6fd88a0c70454fff46a32702d","externalIds":{"MAG":"3042719542","ArXiv":"2007.08501","DBLP":"journals/corr/abs-2007-08501","DOI":"10.1145/3415263.3419160","CorpusId":220546516},"title":"Accelerating 3D deep learning with PyTorch3D"},{"paperId":"aa08a7b0d2fa0877bc13793f113a7c8dbc31da66","externalIds":{"ArXiv":"1908.06903","DBLP":"journals/corr/abs-1908-06903","MAG":"2967247499","DOI":"10.1109/ICCV.2019.00552","CorpusId":201070729},"title":"Multi-Garment Net: Learning to Dress 3D People From Images"},{"paperId":"d5381bfb45abf0f92a7344deb9d530963fa91408","externalIds":{"MAG":"2995807908","DBLP":"conf/cvpr/MaYRPPTB20","DOI":"10.1109/cvpr42600.2020.00650","CorpusId":209386419},"title":"Learning to Dress 3D People in Generative Clothing"},{"paperId":"343da6d4cff7ce8c04270487a1f7a037ea0572d6","externalIds":{"DBLP":"conf/iccv/SaitoHNMLK19","ArXiv":"1905.05172","MAG":"2981978060","DOI":"10.1109/ICCV.2019.00239","CorpusId":152282359},"title":"PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization"},{"paperId":"9ce6d7373d307eb14f7ee76357e442f920be631a","externalIds":{"MAG":"2943990982","DBLP":"journals/corr/abs-1905-03244","ArXiv":"1905.03244","DOI":"10.1109/CVPR.2019.00463","CorpusId":119064973},"title":"Convolutional Mesh Regression for Single-Image Human Shape Reconstruction"},{"paperId":"a2264b13076bba404f725dcb85c3f5290b03b42d","externalIds":{"MAG":"2971467054","DBLP":"conf/iccv/AlldieckPTM19","ArXiv":"1904.08645","DOI":"10.1109/ICCV.2019.00238","CorpusId":121298124},"title":"Tex2Shape: Detailed Full Human Body Geometry From a Single Image"},{"paperId":"4be4707aba8d622a0553aa159dc92ae7f9af9c5e","externalIds":{"MAG":"2978956737","DBLP":"conf/cvpr/PavlakosCGBOTB19","ArXiv":"1904.05866","DOI":"10.1109/CVPR.2019.01123","CorpusId":109932872},"title":"Expressive Body Capture: 3D Hands, Face, and Body From a Single Image"},{"paperId":"8ccb88958358ea59bdc9b76f660b01c8f631b2c0","externalIds":{"MAG":"2933283236","DBLP":"conf/iccvw/Nguyen-PhuocLTR19","DOI":"10.1109/ICCVW.2019.00255","CorpusId":91184364},"title":"HoloGAN: Unsupervised Learning of 3D Representations From Natural Images"},{"paperId":"dd81523b9accdf1c13cd37f76b22ab27d84b7a42","externalIds":{"MAG":"2951382975","DBLP":"journals/corr/abs-1901-05103","ArXiv":"1901.05103","DOI":"10.1109/CVPR.2019.00025","CorpusId":58007025},"title":"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"},{"paperId":"c3294425af6e2c059835ec7f0dca7290b48a8faf","externalIds":{"DBLP":"conf/cvpr/ChenZ19","ArXiv":"1812.02822","MAG":"2962849139","DOI":"10.1109/CVPR.2019.00609","CorpusId":54457478},"title":"Learning Implicit Fields for Generative Shape Modeling"},{"paperId":"8c94385d45f5896e748e43171eeaaa259009faab","externalIds":{"MAG":"2785694322","ArXiv":"1802.00434","DBLP":"journals/corr/abs-1802-00434","DOI":"10.1109/CVPR.2018.00762","CorpusId":13637778},"title":"DensePose: Dense Human Pose Estimation in the Wild"},{"paperId":"1d6a5d0299ed8458191e4e0407d4d513e6a7dd7e","externalIds":{"MAG":"2769297824","DBLP":"journals/corr/abs-1711-10275","ArXiv":"1711.10275","DOI":"10.1109/CVPR.2018.00961","CorpusId":10154243},"title":"3D Semantic Segmentation with Submanifold Sparse Convolutional Networks"},{"paperId":"a84d6c761fbbc66a3d6e0a5ab8a1dcfc944f6753","externalIds":{"MAG":"2769666294","DBLP":"journals/tog/LiBBL017","DOI":"10.1145/3130800.3130813","CorpusId":9882090},"title":"Learning a model of facial shape and expression from 4D scans"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"8674494bd7a076286b905912d26d47f7501c4046","externalIds":{"DBLP":"conf/nips/QiYSG17","MAG":"2950697424","ArXiv":"1706.02413","CorpusId":1745976},"title":"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"},{"paperId":"066d7e4adf6ef40ddd1eaf81efc93f35c4f462e2","externalIds":{"ArXiv":"1703.07834","MAG":"2951863354","DBLP":"journals/corr/JacksonBAT17","DOI":"10.1109/ICCV.2017.117","CorpusId":420414},"title":"Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression"},{"paperId":"d997beefc0922d97202789d2ac307c55c2c52fba","externalIds":{"MAG":"2950642167","DBLP":"conf/cvpr/QiSMG17","ArXiv":"1612.00593","DOI":"10.1109/CVPR.2017.16","CorpusId":5115938},"title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"},{"paperId":"385ae5201434ac8d903f1f6bb1b0d420a1ef2c4f","externalIds":{"DBLP":"conf/cvpr/LiuLQWT16","MAG":"2471768434","DOI":"10.1109/CVPR.2016.124","CorpusId":206593370},"title":"DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations"},{"paperId":"848938e6199bad08f1db6f3239b260cfa901e95f","externalIds":{"ArXiv":"1603.06937","DBLP":"journals/corr/NewellYD16","MAG":"2950762923","DOI":"10.1007/978-3-319-46484-8_29","CorpusId":13613792},"title":"Stacked Hourglass Networks for Human Pose Estimation"},{"paperId":"d468062762decb2470e2840d4bffcd323e599812","externalIds":{"DBLP":"journals/tog/ColletCSGECHKS15","MAG":"2044618760","DOI":"10.1145/2766945","CorpusId":968962},"title":"High-quality streamable free-viewpoint video"},{"paperId":"1c021964d37a9307175de4ca34eb71ffc716595d","externalIds":{"MAG":"1992642990","DBLP":"journals/tog/KazhdanH13","DOI":"10.1145/2487228.2487237","CorpusId":1371704},"title":"Screened poisson surface reconstruction"},{"paperId":"8f6d018b669a95bc5762699b322b19145c24821b","externalIds":{"DBLP":"journals/tog/SumnerSP07","MAG":"2047947369","DOI":"10.1145/1275808.1276478","CorpusId":35682459},"title":"Embedded deformation for shape manipulation"},{"paperId":"5e9f41ebb3a5c5ca0905734f3698ab18d9988bea","externalIds":{"MAG":"2151636374","DBLP":"conf/sgp/SorkineA07","DOI":"10.2312/SGP/SGP07/109-116","CorpusId":11952089},"title":"As-rigid-as-possible surface modeling"},{"paperId":"616c488dd3bfdc78d85137365d59cb29f75b20da","externalIds":{"MAG":"2134484928","DBLP":"conf/cvpr/BalanSBDH07","DOI":"10.1109/CVPR.2007.383340","CorpusId":13181540},"title":"Detailed Human Shape and Pose from Images"},{"paperId":"adddc04ec7f00f3c3823b66c3369f668df15f957","externalIds":{"MAG":"1989191365","DBLP":"journals/tog/AnguelovSKTRD05","DOI":"10.1145/1186822.1073207","CorpusId":3423879},"title":"SCAPE: shape completion and animation of people"},{"paperId":"07c1f9b74eb9a419b9ad5af083c74d00499aacbd","externalIds":{"MAG":"2163680646","DBLP":"conf/cvpr/CheungBK03","DOI":"10.1109/CVPR.2003.1211340","CorpusId":12283655},"title":"Shape-from-silhouette of articulated objects and its use for human body kinematics estimation and motion capture"},{"paperId":"c8a679622d3ac764aec5a928b9b84910133a1d78","externalIds":{"MAG":"1539604522","DBLP":"journals/jirs/PantazopoulosT02","DOI":"10.1023/A:1021175220384","CorpusId":46139505},"title":"Occlusion Culling Algorithms: A Comprehensive Survey"},{"paperId":"88d527a4fa5c1b48dc72159932c48dc155c2d608","externalIds":{"DBLP":"conf/siggraph/RamamoorthiH01a","MAG":"2105649179","DOI":"10.1145/383259.383317","CorpusId":6496578},"title":"An efficient representation for irradiance environment maps"},{"paperId":"71d67283157475c4e6460c52408c00e9f6b8d2fe","externalIds":{"MAG":"2237250383","DBLP":"conf/siggraph/BlanzV99","DOI":"10.1145/3596711.3596730","CorpusId":203705211},"title":"A Morphable Model For The Synthesis Of 3D Faces"},{"paperId":"d92f735b0773b4e697e7e72798eccae2f647acd6","externalIds":{"MAG":"2229412420","DBLP":"conf/siggraph/LorensenC87","DOI":"10.1145/37401.37422","CorpusId":15545924},"title":"Marching cubes: A high resolution 3D surface construction algorithm"},{"paperId":"5749fa6343d757caa7ef6b2a25ca14c14f22cee0","externalIds":{"DBLP":"journals/corr/abs-2305-20082","DOI":"10.48550/arXiv.2305.20082","CorpusId":258988047},"title":"Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor"},{"paperId":"ec816fd1286d404ba48f489f9b4236d086f7d342","externalIds":{"DBLP":"journals/corr/abs-2312-16693","DOI":"10.48550/arXiv.2312.16693","CorpusId":266573150},"title":"I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models"},{"paperId":"0c4d8df762be5d5cfe9ba81fd733f109c5abbbb6","externalIds":{"DBLP":"journals/corr/abs-2310-08529","DOI":"10.48550/arXiv.2310.08529","CorpusId":263909160},"title":"GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors"},{"paperId":"f36e88b2052b2be089406dde7521090b00c37f63","externalIds":{"DBLP":"journals/corr/abs-2212-07422","DOI":"10.48550/arXiv.2212.07422","CorpusId":254636413},"title":"ECON: Explicit Clothed humans Obtained from Normals"},{"paperId":"f90d6bf48292c49aa472002761b5ee031cd77416","externalIds":{"DBLP":"conf/nips/ChanLZL22","CorpusId":258509066},"title":"S-PIFu: Integrating Parametric Human Models with PIFu for Single-view Clothed Human Reconstruction"},{"paperId":"e9c5390ca0f6dd25fb80a212ec96be8a58324f24","externalIds":{"DBLP":"conf/eccv/CaoSSOM22","DOI":"10.1007/978-3-031-19769-7_32","CorpusId":253120031},"title":"Bilateral Normal Integration"},{"paperId":"ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f","externalIds":{"CorpusId":211146177},"title":"AUTO-ENCODING VARIATIONAL BAYES"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"0efb841403aa6252b39ae6975c1cc5410554ef7b","externalIds":{"MAG":"2122111042","DBLP":"journals/tit/CoverH67","DOI":"10.1109/TIT.1967.1053964","CorpusId":5246200},"title":"Nearest neighbor pattern classification"},{"paperId":"a31e9d09d90261fc68acffe097df592cfdcb7706","externalIds":{"MAG":"2156598602","DBLP":"journals/tog/SnavelySS06","DOI":"10.1145/3596711.3596766","CorpusId":13385757},"title":"Photo tourism: exploring photo collections in 3D"},{"paperId":"14021336683ff95e02a28662e0d6c19cc1c24325","externalIds":{"CorpusId":236241840},"title":"UvA-DARE (Digital Academic Repository) Deep 3D human pose estimation: A review"}]}