{"abstract":"Large language models (LLMs) are capable of playing the ‘human’ role as participants in economic games. We investigated the capability of GPT-3.5 to play the one-shot dictator game (DG) and the repeated Prisoner’s Dilemma game (PDG), the latter of which introduced tit-for-tat scenarios. In particular, we investigated whether the LLMs could be prompted to play in accordance to five roles (‘personalities’) assigned prior to game play: the five ‘simulacra’ were: (1) cooperative, (2) competitive, (3) altruistic, (4) selfish, and (5) control, all of which were natural language descriptions (‘ruthless equities trader …’, ‘selfless philanthropist …’, etc). We predicted that the LLM-participant would play in accordance to the semantic content of the prompt (ruthless would play ruthlessly, etc). Across five simulacra (roles), we tested the AI equivalent of 450 human participants (32 400 observations in total, qua counterbalancing and re-testability). Using a general linear mixed model for the PDG, and a cumulative link mixed model for the DG, we found that level of cooperation/donation followed the general pattern of altruistic ⩾ cooperative > control > selfish ⩾ competitive. We proposed ten hypotheses, three of which were convincingly supported: cooperative/altruistic did cooperate more than competitive/selfish; cooperation was higher in repeated games (PDG); cooperative/altruistic were sensitive to the opponent’s behavior in repeated games. We also found some variation among the three versions of GPT-3.5 we used. Our study demonstrates the potential of using prompt engineering for LLM-chatbots to study the mechanisms of cooperation in both real and artificial worlds."}