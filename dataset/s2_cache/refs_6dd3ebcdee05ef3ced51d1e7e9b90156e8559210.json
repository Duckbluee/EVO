{"references":[{"paperId":"8e7dbcb0e3c78e21c9b963f0052d89417fa54993","externalIds":{"ArXiv":"2310.18933","DBLP":"conf/nips/JhaHO23","DOI":"10.48550/arXiv.2310.18933","CorpusId":264590393},"title":"Label Poisoning is All You Need"},{"paperId":"6ad93900b1c956020242653e33ac447824f75fc6","externalIds":{"DBLP":"conf/icassp/Yao0Q24","ArXiv":"2310.12439","DOI":"10.1109/ICASSP48485.2024.10446267","CorpusId":264306255},"title":"PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models"},{"paperId":"e4e3ff07e7de5eb7433092c833f2c516c94b534d","externalIds":{"DBLP":"journals/corr/abs-2310-09744","ArXiv":"2310.09744","DOI":"10.1109/TDSC.2025.3596981","CorpusId":264146740},"title":"Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks"},{"paperId":"9f2b87ecd53ff065f5a1413e605731ea6bb813ad","externalIds":{"DBLP":"conf/iccv/WuH0023","DOI":"10.1109/ICCV51070.2023.00443","CorpusId":267027082},"title":"Computation and Data Efficient Backdoor Attacks"},{"paperId":"cb1ba7bd7b6e93cfb2777fcc8e3f14f8abbacf57","externalIds":{"ArXiv":"2307.07328","DBLP":"journals/corr/abs-2307-07328","DOI":"10.48550/arXiv.2307.07328","CorpusId":259924668},"title":"Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy"},{"paperId":"6ecd45137b421b3729c5b28ebb1bd17e71f98b27","externalIds":{"ArXiv":"2306.16093","DBLP":"journals/corr/abs-2306-16093","DOI":"10.48550/arXiv.2306.16093","CorpusId":14508569},"title":"Retrospective: Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors"},{"paperId":"2079269d164a340eff8e82e9dfa482bd8d79e78e","externalIds":{"ArXiv":"2306.08313","DBLP":"journals/tifs/LiSXXRZGFL24","DOI":"10.1109/TIFS.2024.3472510","CorpusId":259165111},"title":"A Proxy Attack-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks"},{"paperId":"5ac5cf1e0c2186c43c067ca04a24d80d6194a56a","externalIds":{"DBLP":"conf/nips/ChouCH23","ArXiv":"2306.06874","DOI":"10.48550/arXiv.2306.06874","CorpusId":259138934},"title":"VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models"},{"paperId":"33e7f54c2b31849ea5f4a36f0a3470ea57857ff6","externalIds":{"DBLP":"conf/nips/XueZHSLBL23","ArXiv":"2306.06815","CorpusId":259137375},"title":"TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models"},{"paperId":"db4cf9f6a653d5c15973e836c800ea47743251ae","externalIds":{"DBLP":"journals/corr/abs-2306-05499","ArXiv":"2306.05499","DOI":"10.48550/arXiv.2306.05499","CorpusId":259129807},"title":"Prompt Injection attack against LLM-integrated Applications"},{"paperId":"3a742fa9d9c7408a62a832fdbf1a5cb0a45633d1","externalIds":{"DBLP":"journals/pami/WangCZLW26","ArXiv":"2306.00816","DOI":"10.1109/TPAMI.2025.3642161","CorpusId":258999318,"PubMed":"41364567"},"title":"Versatile Backdoor Attack With Visible, Semantic, Sample-Specific and Compatible Triggers"},{"paperId":"31d76bc7724ad3b3d44a5bbffcfdef09b11f0565","externalIds":{"DBLP":"conf/cvpr/Jiang0X023","DOI":"10.1109/CVPR52729.2023.00786","CorpusId":260068548},"title":"Color Backdoor: A Robust Poisoning Attack in Color Space"},{"paperId":"d8dd4e91a242949f46476163257a4aed872a46ad","externalIds":{"DBLP":"conf/cvpr/ChenCHL023","DOI":"10.1109/CVPR52729.2023.02355","CorpusId":260085622},"title":"The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection"},{"paperId":"bf52c9d94fd61fae0d231a7e43d45d673584c282","externalIds":{"ArXiv":"2305.00944","DBLP":"journals/corr/abs-2305-00944","DOI":"10.48550/arXiv.2305.00944","CorpusId":258426823},"title":"Poisoning Language Models During Instruction Tuning"},{"paperId":"14b9c9d8bb8c84e939e52e59fe6461b34db67b15","externalIds":{"DBLP":"conf/cvpr/ChenS023","ArXiv":"2303.05762","DOI":"10.1109/CVPR52729.2023.00393","CorpusId":257482560},"title":"TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets"},{"paperId":"72a3de3cc62c7f99f0870c056f3db1d0f93d0922","externalIds":{"DBLP":"conf/cvpr/YuWYLTK23","ArXiv":"2302.14677","DOI":"10.1109/CVPR52729.2023.01179","CorpusId":257232876},"title":"Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger"},{"paperId":"c50b9b97227971f5ed7a8fc0e5a1e45c6b4cb5c0","externalIds":{"DBLP":"journals/corr/abs-2304-12298","ArXiv":"2304.12298","DOI":"10.48550/arXiv.2304.12298","CorpusId":258298999},"title":"BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT"},{"paperId":"0a34c81636b793c50991199cfbcc2f0a7d55296f","externalIds":{"ArXiv":"2301.00364","DBLP":"journals/pami/YinZWFZFY24","DOI":"10.1109/TPAMI.2022.3194988","CorpusId":255372300,"PubMed":"37021863"},"title":"Generalizable Black-Box Adversarial Attack With Meta Learning"},{"paperId":"a5a1196b6bd79031994c09caa3e0f41cbb1c94a3","externalIds":{"ArXiv":"2212.05400","DBLP":"conf/cvpr/ChouCH23","DOI":"10.1109/CVPR52729.2023.00391","CorpusId":254564071},"title":"How to Backdoor Diffusion Models?"},{"paperId":"97ae76460fa593a554336adab39eb06ea01219b1","externalIds":{"ArXiv":"2211.14719","DBLP":"conf/nips/CaiXXZY22","DOI":"10.48550/arXiv.2211.14719","CorpusId":254043765},"title":"BadPrompt: Backdoor Attacks on Continuous Prompts"},{"paperId":"9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6","externalIds":{"DBLP":"journals/corr/abs-2211-09527","ArXiv":"2211.09527","DOI":"10.48550/arXiv.2211.09527","CorpusId":253581710},"title":"Ignore Previous Prompt: Attack Techniques For Language Models"},{"paperId":"e5649046e24721dc4bb2353ae1e35f76972a814a","externalIds":{"ArXiv":"2210.09305","DBLP":"journals/corr/abs-2210-09305","DOI":"10.48550/arXiv.2210.09305","CorpusId":252917803},"title":"Thinking Two Moves Ahead: Anticipating Other Users Improves Backdoor Attacks in Federated Learning"},{"paperId":"5cd0cc4290f39468da1cac66261b44d020c049f6","externalIds":{"ArXiv":"2210.09194","DBLP":"journals/corr/abs-2210-09194","DOI":"10.48550/arXiv.2210.09194","CorpusId":252917727},"title":"Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class"},{"paperId":"bc8d5957446b53f7798c209fb7ac6cfd45289bdf","externalIds":{"DBLP":"journals/corr/abs-2210-05929","ArXiv":"2210.05929","DOI":"10.48550/arXiv.2210.05929","CorpusId":252846609},"title":"Few-shot Backdoor Attacks via Neural Tangent Kernels"},{"paperId":"538f0c8379f9e091d4769ec3bdff6eb45130554f","externalIds":{"ArXiv":"2210.05968","DBLP":"conf/nips/QinFL0Z0W22","DOI":"10.48550/arXiv.2210.05968","CorpusId":252846342},"title":"Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation"},{"paperId":"918a8303ce469b934d05f600d88203aafe58c500","externalIds":{"ArXiv":"2210.04213","DBLP":"journals/tip/ZhuCLC00ZCH22","DOI":"10.1109/TIP.2022.3211736","CorpusId":252780124,"PubMed":"36223353"},"title":"Toward Understanding and Boosting Adversarial Transferability From a Distribution Perspective"},{"paperId":"843e5dcc8a9337be9ab2690623d3820393b87c97","externalIds":{"DBLP":"conf/iccv/SuZWLW023","ArXiv":"2209.15304","DOI":"10.1109/ICCV51070.2023.00402","CorpusId":259950676},"title":"Hiding Visual Information via Obfuscating Adversarial Perturbations"},{"paperId":"15f87ce1634093c8760e534bf2dd5daf66c7b54b","externalIds":{"DBLP":"journals/corr/abs-2210-00875","ArXiv":"2210.00875","DOI":"10.48550/arXiv.2210.00875","CorpusId":252683790},"title":"Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection"},{"paperId":"251269b9e16ab1da20cb57a669b2bfdbd0d1cd72","externalIds":{"DBLP":"conf/nlpcc/ShiLYHZL22","ArXiv":"2209.01882","DOI":"10.48550/arXiv.2209.01882","CorpusId":252088971},"title":"PromptAttack: Prompt-based Attack for Language Models via Gradient Search"},{"paperId":"b1e29e25741daf2074358bd60854baa7084897fe","externalIds":{"DBLP":"journals/tifs/WangZLYN22","ArXiv":"2208.09336","DOI":"10.1109/TIFS.2022.3202687","CorpusId":251710149},"title":"Dispersed Pixel Perturbation-Based Imperceptible Backdoor Trigger for Image Classifier Models"},{"paperId":"8adcad33fcede5f745f9fdbcadc56b02804f4049","externalIds":{"DBLP":"journals/corr/abs-2208-08052","ArXiv":"2208.08052","DOI":"10.1109/TIFS.2023.3333687","CorpusId":251622274},"title":"Imperceptible and Robust Backdoor Attack in 3D Point Cloud"},{"paperId":"3e6e6a157fcf23e3808cdcf3b8323a256e0de2da","externalIds":{"ArXiv":"2208.03610","DBLP":"journals/corr/abs-2208-03610","DOI":"10.48550/arXiv.2208.03610","CorpusId":251402368},"title":"Blackbox Attacks via Surrogate Ensemble Search"},{"paperId":"d28aebd3c71def61a020373c1f76452590939426","externalIds":{"DBLP":"journals/tifs/LiZYJWX23","ArXiv":"2209.06015","DOI":"10.1109/TIFS.2023.3265535","CorpusId":257901245},"title":"Black-Box Dataset Ownership Verification via Backdoor Watermarking"},{"paperId":"22aefc9f8aab71f4a318cd247d043b485a12c06c","externalIds":{"DBLP":"journals/corr/abs-2207-12405","ArXiv":"2207.12405","DOI":"10.1109/TPAMI.2023.3296408","CorpusId":251066821,"PubMed":"37463082"},"title":"Versatile Weight Attack via Flipping Limited Bits"},{"paperId":"8680c075abfb7832ff1321b5f409f2a5e57570f2","externalIds":{"DBLP":"journals/corr/abs-2207-05382","ArXiv":"2207.05382","DOI":"10.48550/arXiv.2207.05382","CorpusId":250451607},"title":"Frequency Domain Model Augmentation for Adversarial Attack"},{"paperId":"b5da0ffa7b60abefa036bc450e8e333087943787","externalIds":{"DBLP":"conf/ijcai/DuZLLW22","DOI":"10.24963/ijcai.2022/96","CorpusId":250629290},"title":"PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning"},{"paperId":"09d4826c5f461bbeed325a058a098c88485ebdd7","externalIds":{"DBLP":"journals/tbbis/SarkarBKGM22","DOI":"10.1109/tbiom.2021.3132132","CorpusId":244867412},"title":"FaceHack: Attacking Facial Recognition Systems Using Malicious Facial Characteristics"},{"paperId":"89e9872cf16e058499877efbd578646ebb04db87","externalIds":{"DBLP":"conf/aaai/NingLXWW22","DOI":"10.1609/aaai.v36i9.21272","CorpusId":250107115},"title":"Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks"},{"paperId":"d3b071c61992078d6cb77377621cf80f620136b4","externalIds":{"DBLP":"journals/corr/abs-2206-12654","ArXiv":"2206.12654","DOI":"10.48550/arXiv.2206.12654","CorpusId":250072872},"title":"BackdoorBench: A Comprehensive Benchmark of Backdoor Learning"},{"paperId":"e466852cfeb09941b18d9510de7c4e87e01405bf","externalIds":{"ArXiv":"2206.08514","DBLP":"conf/nips/CuiYHCLS22","DOI":"10.48550/arXiv.2206.08514","CorpusId":249848279},"title":"A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks"},{"paperId":"4ac1bfc3f4af5e356912a2a714bf4ce926f0c376","externalIds":{"DBLP":"journals/corr/abs-2206-07840","ArXiv":"2206.07840","DOI":"10.1109/CVPR52729.2023.02356","CorpusId":249712217},"title":"Architectural Backdoors in Neural Networks"},{"paperId":"e7f9b8036c020ef525f5be97e033661e47bc2721","externalIds":{"DBLP":"journals/corr/abs-2206-10341","ArXiv":"2206.10341","DOI":"10.48550/arXiv.2206.10341","CorpusId":249889464},"title":"Neurotoxin: Durable Backdoors in Federated Learning"},{"paperId":"2e7920001f78b5ded7364f6b5ea74553c87b477a","externalIds":{"DBLP":"journals/corr/abs-2206-04823","ArXiv":"2206.04823","DOI":"10.48550/arXiv.2206.04823","CorpusId":249605407},"title":"Membership Inference via Backdooring"},{"paperId":"092a3f775be041a49a939f77b5ba7775cffb4f4f","externalIds":{"DBLP":"conf/cvpr/FengWFL0X22","DOI":"10.1109/CVPR52688.2022.01467","CorpusId":249878917},"title":"Boosting Black-Box Attack with Partially Transferred Conditional Adversarial Distribution"},{"paperId":"d2238af352969a6a2cc9c4ae410cd6319d13616f","externalIds":{"DBLP":"conf/cvpr/Zhao0XDWL22","DOI":"10.1109/CVPR52688.2022.01478","CorpusId":250055668},"title":"DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints"},{"paperId":"052d4787586a7e918638bd82c3034089648d62e7","externalIds":{"DBLP":"journals/corr/abs-2205-13383","ArXiv":"2205.13383","DOI":"10.1109/CVPR52688.2022.01465","CorpusId":249097522},"title":"BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning"},{"paperId":"812ff246291fe4a618f174facf09ac8854b10ae0","externalIds":{"ArXiv":"2205.12134","DBLP":"conf/nips/ChenHTWXH22","DOI":"10.48550/arXiv.2205.12134","CorpusId":249017491},"title":"Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box Score-Based Query Attacks"},{"paperId":"2068a0be69dffc72adf2a102f88755aa515521a2","externalIds":{"DBLP":"conf/icassp/FengLQZ22","DOI":"10.1109/icassp43922.2022.9746008","CorpusId":249436773},"title":"Stealthy Backdoor Attack with Adversarial Training"},{"paperId":"e2e5035f4bebb7ac500bac9c98a5686dc808c305","externalIds":{"DBLP":"conf/icassp/PhanXLCY22","DOI":"10.1109/icassp43922.2022.9747582","CorpusId":249437547},"title":"Invisible and Efficient Backdoor Attacks for Compressed Deep Neural Networks"},{"paperId":"319b2656c0c1c9d1c8260eee3300d12548e5a3eb","externalIds":{"ArXiv":"2205.03190","DBLP":"conf/ijcai/ZhongQZ22","DOI":"10.48550/arXiv.2205.03190","CorpusId":248562973},"title":"Imperceptible Backdoor Attack: From Input Space to Feature Representation"},{"paperId":"85014c0fd19701850183e491075b1b7c56a53039","externalIds":{"DBLP":"journals/pami/ShiHHYT23","DOI":"10.1109/TPAMI.2022.3169802","CorpusId":248390794,"PubMed":"35468057"},"title":"Query-Efficient Black-Box Adversarial Attack With Customized Iteration and Sampling"},{"paperId":"9adcf96784c51c8fb30375d930be03e05de0838c","externalIds":{"DBLP":"conf/ijcai/XiaLZL22","ArXiv":"2204.12281","DOI":"10.48550/arXiv.2204.12281","CorpusId":248392246},"title":"Data-Efficient Backdoor Attacks"},{"paperId":"6303855a23c4fa5ab78450b0a93e9b0c34ca4a5a","externalIds":{"DBLP":"journals/corr/abs-2204-05239","ArXiv":"2204.05239","DOI":"10.48550/arXiv.2204.05239","CorpusId":248085108},"title":"Exploring the Universal Vulnerability of Prompt-based Learning Paradigm"},{"paperId":"411b07870690a9492aec0331e07ede019f3d6814","externalIds":{"DBLP":"journals/corr/abs-2204-00008","ArXiv":"2204.00008","DOI":"10.1109/CVPR52688.2022.01457","CorpusId":247922429},"title":"Improving Adversarial Transferability via Neuron Attribution-based Attacks"},{"paperId":"8d2125e3cfab60545c37aef1d3f4df6d199c7031","externalIds":{"DBLP":"journals/corr/abs-2203-09123","ArXiv":"2203.09123","DOI":"10.1109/CVPR52688.2022.01481","CorpusId":247518569},"title":"Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input"},{"paperId":"936a3371a8d8320a9bf85e1ca2ccb43c092d26bd","externalIds":{"DBLP":"journals/corr/abs-2203-06616","ArXiv":"2203.06616","DOI":"10.1109/CVPR52688.2022.01304","CorpusId":247446946},"title":"LAS-AT: Adversarial Training with Learnable Attack Strategy"},{"paperId":"6f759911a170a158d73ba3db73c8f140b557e059","externalIds":{"DBLP":"conf/mm/HanXZYLZ22","ArXiv":"2203.00858","DOI":"10.1145/3503161.3548171","CorpusId":250491602},"title":"Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving"},{"paperId":"6c20a12376619a3119e53202692b091635ff03c5","externalIds":{"DBLP":"conf/iclr/Huang0WQ022","ArXiv":"2202.03423","CorpusId":246652381},"title":"Backdoor Defense via Decoupling the Training Process"},{"paperId":"9a15c604696e672d2ccbcc07d36bf38ba3817bba","externalIds":{"ArXiv":"2201.05647","DBLP":"journals/corr/abs-2201-05647","CorpusId":246016543},"title":"Tools and Practices for Responsible AI Engineering"},{"paperId":"2fb828f6043bd1dd8a40c88343db695748751d2b","externalIds":{"DBLP":"journals/corr/abs-2112-13162","ArXiv":"2112.13162","DOI":"10.1109/ISQED54688.2022.9806152","CorpusId":245501885},"title":"Stealthy Attack on Algorithmic-Protected DNNs via Smart Bit Flipping"},{"paperId":"1f8e12a8fa1222de8406962588f702dc700e2bb3","externalIds":{"DBLP":"conf/cvpr/FengMZZXT22","ArXiv":"2112.01148","DOI":"10.1109/CVPR52688.2022.02021","CorpusId":244798899},"title":"FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis"},{"paperId":"cb1d6f4fbb2d60d5083f97ed7cdaf77b20ad1503","externalIds":{"DBLP":"conf/icmla/ChattopadhyayC21","DOI":"10.1109/ICMLA52953.2021.00274","CorpusId":246289687},"title":"ROWBACK: RObust Watermarking for neural networks using BACKdoors"},{"paperId":"6faa45c5c064a01700e4bc2e21aac064afff95e4","externalIds":{"ArXiv":"2111.12965","DBLP":"conf/cvpr/QiXPZYB22","DOI":"10.1109/CVPR52688.2022.01299","CorpusId":244708886},"title":"Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks"},{"paperId":"29fbf409aaad5e3a0f3468c44637a140dc2c545b","externalIds":{"DBLP":"conf/cvpr/XiongLZH022","ArXiv":"2111.10752","DOI":"10.1109/CVPR52688.2022.01456","CorpusId":244478092},"title":"Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability"},{"paperId":"ccdc9444667f206bfd82253354852e61148c9ccc","externalIds":{"DBLP":"journals/pami/DongCPSZ22","ArXiv":"2203.06560","DOI":"10.1109/TPAMI.2021.3126733","CorpusId":243941001,"PubMed":"34752388"},"title":"Query-Efficient Black-box Adversarial Attacks Guided by a Transfer-based Prior"},{"paperId":"8436897e713c2242d6291df9a6a33c1544d4dd39","externalIds":{"DBLP":"journals/corr/abs-2111-02840","ArXiv":"2111.02840","CorpusId":242757097},"title":"Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models"},{"paperId":"a3b9434fa809570f8baa7355a751cc432acf28c3","externalIds":{"DBLP":"conf/nips/YatsuraMH21","ArXiv":"2111.01714","CorpusId":240419817},"title":"Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks"},{"paperId":"dea3b07c92017525f443b527824dff502e5ed590","externalIds":{"DBLP":"conf/ccs/ShenJ0LCSFYW21","ArXiv":"2111.00197","DOI":"10.1145/3460120.3485370","CorpusId":240354696},"title":"Backdoor Pre-trained Models Can Transfer to All"},{"paperId":"ed3458ab5b39802eea519f5f1e5450ad292c58c5","externalIds":{"DBLP":"conf/iccv/DoanL0L21","DOI":"10.1109/ICCV48922.2021.01175","CorpusId":244397177},"title":"LIRA: Learnable, Imperceptible and Robust Backdoor Attacks"},{"paperId":"d0c623d64f245ac01af122fe5209f5e151c9291d","externalIds":{"DBLP":"conf/iccv/FengWZZZ21","DOI":"10.1109/iccv48922.2021.00769","CorpusId":247173444},"title":"Meta-Attack: Class-agnostic and Model-agnostic Physical Adversarial Attack"},{"paperId":"12cdc1423ac0eb27a26d3883cf485b2f0a4f4e50","externalIds":{"DBLP":"conf/iccv/LiangWFWC21","ArXiv":"2201.08970","DOI":"10.1109/iccv48922.2021.00760","CorpusId":246240694},"title":"Parallel Rectangle Flip Attack: A Query-based Black-box Attack against Object Detection"},{"paperId":"79d21c3a2efd0b64f607eacd60ac2ea2af02e9c8","externalIds":{"DBLP":"conf/iccv/ChenFZK21","DOI":"10.1109/ICCV48922.2021.00762","CorpusId":244350557},"title":"ProFlip: Targeted Trojan Attack with Progressive Bit Flips"},{"paperId":"1723464a5acca27c5fd1659169dff92f422eac72","externalIds":{"DBLP":"conf/iccv/YuanH21","DOI":"10.1109/ICCV48922.2021.00768","CorpusId":244906903},"title":"Consistency-Sensitivity Guided Ensemble Black-Box Adversarial Attacks in Low-Dimensional Spaces"},{"paperId":"198a3e7b7ea8b9c61b3b3e6f8cba920da3e5ab7b","externalIds":{"DBLP":"journals/pr/ZhengFWZWP23","ArXiv":"2109.09320","DOI":"10.1016/j.patcog.2022.109009","CorpusId":237571366},"title":"Robust Physical-World Attacks on Face Recognition"},{"paperId":"5223b4b3d19d553a66eca98b4b31122d37468154","externalIds":{"ArXiv":"2109.01300","DBLP":"journals/corr/abs-2109-01300","CorpusId":237416749},"title":"How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data"},{"paperId":"2ad898669cf696dccd9879d6e203c13aee3376e4","externalIds":{"DBLP":"journals/corr/abs-2108-13888","ArXiv":"2108.13888","ACL":"2021.emnlp-main.241","DOI":"10.18653/v1/2021.emnlp-main.241","CorpusId":237365058},"title":"Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning"},{"paperId":"b4075b25bf107270fc589784aaa6d933c1ce918d","externalIds":{"DBLP":"journals/corr/abs-2108-04204","ArXiv":"2108.04204","DOI":"10.1109/ICCV48922.2021.00765","CorpusId":236956844},"title":"Meta Gradient Adversarial Attack"},{"paperId":"95009b5967fabd65f426f778dcb6f19fb30fe72d","externalIds":{"DBLP":"journals/corr/abs-2108-02488","MAG":"3188772578","ArXiv":"2108.02488","DOI":"10.1109/TIP.2022.3201472","CorpusId":236924281,"PubMed":"36040942"},"title":"Poison Ink: Robust and Invisible Backdoor Attack"},{"paperId":"b18580c6746a47603955074931d4687e268781ea","externalIds":{"ArXiv":"2107.14185","DBLP":"conf/iccv/WangGZLQ021","DOI":"10.1109/ICCV48922.2021.00754","CorpusId":236493523},"title":"Feature Importance-aware Transferable Adversarial Attacks"},{"paperId":"7697b3f32a4481c3e488ffdde7f5c942fc0afe2e","externalIds":{"DBLP":"journals/corr/abs-2107-07240","ArXiv":"2107.07240","CorpusId":235898849},"title":"Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting"},{"paperId":"98b4c6077a382093184e740ccde4bd6c2bf7d2ac","externalIds":{"DBLP":"conf/issta/ZhangDTGYJ21","DOI":"10.1145/3460319.3464809","CorpusId":235770142},"title":"AdvDoor: adversarial backdoor attack of deep learning system"},{"paperId":"53e84565f4a2de385a14686b1a6d8b524bc25c6f","externalIds":{"DBLP":"journals/corr/abs-2106-08970","ArXiv":"2106.08970","CorpusId":235446588},"title":"Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch"},{"paperId":"c7ab0728d263e6c2ae3182a4ea5a81e4bccf3a4b","externalIds":{"MAG":"3166022359","DBLP":"journals/jsac/GongCWHMSZ21","DOI":"10.1109/JSAC.2021.3087237","CorpusId":236149736},"title":"Defense-Resistant Backdoor Attacks Against Deep Neural Networks in Outsourced Cloud Environment"},{"paperId":"721489ef0b26e7ef589b06f94eb35638b52a14f1","externalIds":{"DBLP":"journals/corr/abs-2106-04690","ArXiv":"2106.04690","CorpusId":235377373},"title":"Handcrafted Backdoors in Deep Neural Networks"},{"paperId":"311c281c6c511f6bd56a8a77d38a811d32694a35","externalIds":{"ArXiv":"2106.02105","DBLP":"conf/nips/SpringerMK21","CorpusId":239886016},"title":"A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks"},{"paperId":"5beafcd6c0222a2f45863058280873c1ef088ec4","externalIds":{"DBLP":"conf/cvpr/MaCY21","DOI":"10.1109/CVPR46437.2021.01166","CorpusId":263875465},"title":"Simulating Unknown Target Models for Query-Efficient Black-box Attacks"},{"paperId":"dc5d70b48a04a6e9c3310e6fda30748a37031d0c","externalIds":{"MAG":"3173757585","DBLP":"conf/aaai/YanLTWLCP21","DOI":"10.1609/aaai.v35i12.17266","CorpusId":235349094},"title":"DeHiB: Deep Hidden Backdoor Attack on Semi-supervised Learning via Adversarial Perturbation"},{"paperId":"5756d4bc9b9f77c8cd849db645af31db07a9b369","externalIds":{"DBLP":"conf/infocom/NingLXW21","DOI":"10.1109/INFOCOM42981.2021.9488902","CorpusId":236480506},"title":"Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks"},{"paperId":"c586f3a69102fffdff178ca79b0be767d384da43","externalIds":{"DBLP":"journals/corr/abs-2105-00164","ArXiv":"2105.00164","DOI":"10.1145/3460120.3484576","CorpusId":233481877},"title":"Hidden Backdoors in Human-Centric Language Models"},{"paperId":"67770f34ba45826f9b45ab6f194dc778ce164da3","externalIds":{"DBLP":"journals/corr/abs-2104-15129","ArXiv":"2104.15129","DOI":"10.1109/tifs.2022.3160359","CorpusId":233476471},"title":"Stealthy Backdoors as Compression Artifacts"},{"paperId":"07b65c99f615ad2333eb427a5c3da204725939d9","externalIds":{"ArXiv":"2104.11470","DBLP":"conf/nips/QinFZW21","CorpusId":233387874},"title":"Random Noise Defense Against Query-Based Black-Box Attacks"},{"paperId":"2970f2c2c928ba8f8ce55ddd312b463869e75c84","externalIds":{"DBLP":"journals/corr/abs-2104-09667","ArXiv":"2104.09667","CorpusId":233307382},"title":"Manipulating SGD with Data Ordering Attacks"},{"paperId":"0174203a8d53a850c083eb9189a6c2e03a41576f","externalIds":{"ArXiv":"2104.05808","DBLP":"conf/iccv/Xiang0CLK21","DOI":"10.1109/ICCV48922.2021.00750","CorpusId":233219636},"title":"A Backdoor Attack against 3D Point Cloud Classifiers"},{"paperId":"44a03e36633088aca67be1edd615cb6af4820a3a","externalIds":{"ArXiv":"2103.16074","DBLP":"journals/corr/abs-2103-16074","DOI":"10.1109/ICCV48922.2021.01618","CorpusId":232417064},"title":"PointBA: Towards Backdoor Attacks in 3D Point Cloud"},{"paperId":"7301d58eb8a750e44b7da5ab0266b134d1651237","externalIds":{"DBLP":"conf/cvpr/Wang021","ArXiv":"2103.15571","DOI":"10.1109/CVPR46437.2021.00196","CorpusId":232404127},"title":"Enhancing the Transferability of Adversarial Attacks through Variance Tuning"},{"paperId":"d1399a3c5c92cffe46d1a31ab02a4639ba3e8d5b","externalIds":{"DBLP":"journals/corr/abs-2103-14641","ArXiv":"2103.14641","DOI":"10.1109/ICCV48922.2021.00761","CorpusId":232379979},"title":"On Generating Transferable Targeted Perturbations"},{"paperId":"07322a941dc6ebe16ac2a31fddd356bd726fe8dd","externalIds":{"ArXiv":"2103.10609","DBLP":"journals/corr/abs-2103-10609","DOI":"10.5244/c.35.186","CorpusId":232290454},"title":"Boosting Adversarial Transferability through Enhanced Momentum"},{"paperId":"1045b77db15e56cc1c87d35fe90332164f6ac5a8","externalIds":{"ArXiv":"2102.10496","DBLP":"journals/corr/abs-2102-10496","CorpusId":231985837},"title":"Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits"},{"paperId":"b350a0eb1977cbb7384c657f33eed95772eae191","externalIds":{"MAG":"3047891567","DBLP":"journals/pr/LiZH21","DOI":"10.1016/j.patcog.2020.107584","CorpusId":225514093},"title":"Universal adversarial perturbations against object detection"},{"paperId":"558f7fa8c8e909e7aa67b20d41ba5896b68dcf54","externalIds":{"DBLP":"journals/corr/abs-2102-00436","ArXiv":"2102.00436","DOI":"10.1109/ICCV48922.2021.01585","CorpusId":231741167},"title":"Admix: Enhancing the Transferability of Adversarial Attacks"},{"paperId":"355b3561f5aee7dc1e2ef6ca324e379cbeea9075","externalIds":{"DBLP":"conf/sdm/LiCW21","DOI":"10.1137/1.9781611976700.24","CorpusId":235691611},"title":"Turning Attacks into Protection: Social Media Privacy Protection Using Adversarial Attacks"},{"paperId":"2cd35c6e53580d2adf5f693b05d5caa19844c19c","externalIds":{"ArXiv":"2012.11212","DBLP":"journals/corr/abs-2012-11212","DOI":"10.1609/aaai.v35i2.16201","CorpusId":229339767},"title":"Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification"},{"paperId":"10d05886c555258fcb2ba68d280c7cf5287a7a02","externalIds":{"DBLP":"journals/corr/abs-2012-11207","ArXiv":"2012.11207","CorpusId":229340618},"title":"On Success and Simplicity: A Second Look at Transferable Targeted Attacks"},{"paperId":"e5000e58c3be361247717bf378ca4b6a07601ca1","externalIds":{"DBLP":"conf/eurosp/PangZGXJCLW22","ArXiv":"2012.09302","DOI":"10.1109/EuroSP53844.2022.00048","CorpusId":247411498},"title":"TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors"},{"paperId":"9df35283dc71e95f0451a478b531b544dd1d6d0d","externalIds":{"ArXiv":"2012.03816","DBLP":"conf/iccv/LiLWLHL21","DOI":"10.1109/ICCV48922.2021.01615","CorpusId":237054216},"title":"Invisible Backdoor Attack with Sample-Specific Triggers"},{"paperId":"bcac1b00badd7c8830e10ba4eb813ad79a3222b5","externalIds":{"MAG":"3099873944","DBLP":"conf/nips/GuoLC20","ArXiv":"2012.03528","CorpusId":227276380},"title":"Backpropagating Linearly Improves Transferability of Adversarial Examples"},{"paperId":"9554acf6c050a1c35277603ed0ff904b9075fc71","externalIds":{"MAG":"3106736359","ArXiv":"2011.13375","DBLP":"conf/cvpr/SaylesHG0F21","DOI":"10.1109/CVPR46437.2021.01443","CorpusId":227208746},"title":"Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect"},{"paperId":"7acb1f1c9539bf108de3db202a60821278735043","externalIds":{"MAG":"3106646114","DBLP":"conf/ccs/LinXL020","DOI":"10.1145/3372297.3423362","CorpusId":226227859},"title":"Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features"},{"paperId":"8a7d037abb7285740c58f578ef201eb6bcf18a58","externalIds":{"MAG":"3096240177","DBLP":"journals/corr/abs-2010-13773","ArXiv":"2010.13773","CorpusId":225075918},"title":"GreedyFool: Distortion-Aware Sparse Adversarial Attack"},{"paperId":"a556a1ca23e4b2aaa7dacf928a2803d4faeaa162","externalIds":{"MAG":"3099232569","DBLP":"conf/nips/YangJHNZ20","ArXiv":"2010.11742","CorpusId":225039996},"title":"Learning Black-Box Attackers with Transferable Priors and Query Feedback"},{"paperId":"2aab97e35c43d961d645e650808d5b052ec180ab","externalIds":{"DBLP":"conf/nips/CroceASDFCM021","ArXiv":"2010.09670","MAG":"3093235747","CorpusId":224705419},"title":"RobustBench: a standardized adversarial robustness benchmark"},{"paperId":"1c65c46a4bcc3a2980e6ea005894f34820808dcf","externalIds":{"ArXiv":"2010.08138","DBLP":"conf/nips/NguyenT20","MAG":"3102670879","CorpusId":223953609},"title":"Input-Aware Dynamic Backdoor Attack"},{"paperId":"07aed51541e6b72fc17203bab0334d8d9a67d723","externalIds":{"DBLP":"conf/icdm/ZhangRW020","ArXiv":"2010.07788","MAG":"3093347641","DOI":"10.1109/ICDM50108.2020.00186","CorpusId":222378820},"title":"Generalizing Universal Adversarial Attacks Beyond Additive Perturbations"},{"paperId":"3204d706fb3f6ca71e76c1365c0f09d138f4033a","externalIds":{"ArXiv":"2010.05821","MAG":"3093403688","DBLP":"journals/corr/abs-2010-05821","CorpusId":222290496},"title":"Open-sourced Dataset Protection via Backdoor Watermarking"},{"paperId":"9cd2a5e97b41b9a90de931e056626985c0b39679","externalIds":{"DBLP":"conf/mm/XuCXWYS20","MAG":"3096804204","DOI":"10.1145/3394171.3413543","CorpusId":222278393},"title":"Learning Optimization-based Adversarial Perturbations for Attacking Sequential Recognition Models"},{"paperId":"47b4744162537f40572cdd723f8f37fb489a3e75","externalIds":{"ArXiv":"2010.04055","DBLP":"journals/corr/abs-2010-04055","MAG":"3092178728","CorpusId":222208621},"title":"A Unified Approach to Interpreting and Boosting Adversarial Transferability"},{"paperId":"cc3725e66aa600eb12b244e82880f8e0bd225065","externalIds":{"ArXiv":"2010.02684","MAG":"3092271175","DBLP":"conf/emnlp/ChanTOZ20","ACL":"2020.findings-emnlp.373","DOI":"10.18653/v1/2020.findings-emnlp.373","CorpusId":222140954},"title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder"},{"paperId":"8495b8b87faf72bd6f0d6e54b2366929b3f5d240","externalIds":{"MAG":"3022742351","DBLP":"journals/isci/ChenZXSS20","DOI":"10.1016/j.ins.2020.04.019","CorpusId":219043330},"title":"MAG-GAN: Massive attack generator via GAN"},{"paperId":"95968b89040146cb015827aee8ff6f77d67bbaf1","externalIds":{"ACL":"2021.acl-demo.43","ArXiv":"2009.09191","MAG":"3087231533","DBLP":"journals/corr/abs-2009-09191","DOI":"10.18653/v1/2021.acl-demo.43","CorpusId":221819315},"title":"OpenAttack: An Open-source Textual Adversarial Attack Toolkit"},{"paperId":"863e7ddacc77babf46393c9e0cc4bafb6177e478","externalIds":{"MAG":"3086120435","DBLP":"journals/corr/abs-2009-06996","ArXiv":"2009.06996","CorpusId":221702848},"title":"Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems"},{"paperId":"8f124ac264204aed2cdad8d2ec86e2cd2971ac33","externalIds":{"DBLP":"journals/corr/abs-2009-00097","ArXiv":"2009.00097","MAG":"3082803330","DOI":"10.1109/CVPR52688.2022.01482","CorpusId":221397388},"title":"Adversarial Eigen Attack on BlackBox Models"},{"paperId":"3700e8a06107262bb072e48c3c48d115f872f1cb","externalIds":{"MAG":"3047587361","ArXiv":"2008.01761","DBLP":"journals/corr/abs-2008-01761","DOI":"10.1145/3340531.3412130","CorpusId":220968846},"title":"Can Adversarial Weight Perturbations Inject Neural Backdoors"},{"paperId":"40d54bea8bbec461fdb438d7e0697b5ffd904423","externalIds":{"DBLP":"journals/geoinformatica/ZhaoZLLW22","MAG":"3046235874","DOI":"10.1007/s10707-020-00418-7","CorpusId":220907607},"title":"AP-GAN: Adversarial patch attack on content-based image retrieval systems"},{"paperId":"11fe33206746251656698bf5188fc622aea7fc21","externalIds":{"ArXiv":"2008.00312","MAG":"3046664463","DBLP":"journals/corr/abs-2008-00312","DOI":"10.1109/EuroSP51992.2021.00022","CorpusId":220936152},"title":"Trojaning Language Models for Fun and Profit"},{"paperId":"cfe9ab4e98d6c4ff456332e5bae1695e7d5d0714","externalIds":{"ArXiv":"2007.12861","MAG":"3045370568","DBLP":"journals/corr/abs-2007-12861","DOI":"10.1145/3394171.3413906","CorpusId":220793632},"title":"Adversarial Privacy-preserving Filter"},{"paperId":"69dd1b9e8391430a667214a9ca6c0bc94560deb2","externalIds":{"DBLP":"journals/corr/abs-2007-10760","MAG":"3044223678","ArXiv":"2007.10760","CorpusId":220665637},"title":"Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review"},{"paperId":"857b1c3f171afb3cdf9df23d23e5d0cdfaa83efb","externalIds":{"MAG":"3043057662","DBLP":"journals/corr/abs-2007-08450","ArXiv":"2007.08450","CorpusId":220546448},"title":"Learning perturbation sets for robust machine learning"},{"paperId":"9f86a02715509479245e5a7fe584523d2e051b7b","externalIds":{"DBLP":"conf/nips/DolatabadiEL20","MAG":"3043119335","ArXiv":"2007.07435","CorpusId":220525368},"title":"AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows"},{"paperId":"0a93fce82afa33b218728a19d19d0ae40401b396","externalIds":{"MAG":"3041107652","ArXiv":"2007.05084","DBLP":"conf/nips/WangSRVASLP20","CorpusId":220487047},"title":"Attack of the Tails: Yes, You Really Can Backdoor Federated Learning"},{"paperId":"23811906b2fc98b2591f1d9f69d57814d5151108","externalIds":{"MAG":"3042075786","DBLP":"conf/uss/LovisottoTSSM21","ArXiv":"2007.04137","CorpusId":220403405},"title":"SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations"},{"paperId":"a5ac68f2172b5c5a7fa19f0f638cafee7dead1b7","externalIds":{"ArXiv":"2007.03608","MAG":"3041668106","DBLP":"journals/corr/abs-2007-03608","CorpusId":220381029},"title":"Backdoor attacks and defenses in feature-partitioned collaborative learning"},{"paperId":"9d5434885b76ed7cd83f00cb81c929d087775e9a","externalIds":{"ArXiv":"2007.02343","DBLP":"conf/eccv/LiuM0020","MAG":"3039176595","DOI":"10.1007/978-3-030-58607-2_11","CorpusId":220363879},"title":"Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks"},{"paperId":"9e75112b213fcdb3253b18da84d85cca2250036b","externalIds":{"DBLP":"conf/cvpr/WengerPBY0Z21","MAG":"3108700588","ArXiv":"2006.14580","DOI":"10.1109/CVPR46437.2021.00614","CorpusId":227261697},"title":"Backdoor Attacks Against Deep Learning Systems in the Physical World"},{"paperId":"123877109e4a203b0db35984aa4808583573ab51","externalIds":{"MAG":"3080297477","DBLP":"journals/corr/abs-2006-12792","ArXiv":"2006.12792","DOI":"10.1145/3394486.3403225","CorpusId":219981888},"title":"RayS: A Ray Searching Method for Hard-label Adversarial Attack"},{"paperId":"207e947534a33ac2995216cb6d982eaf5c12d174","externalIds":{"MAG":"3034478342","DBLP":"conf/kdd/TangDLYH20","ArXiv":"2006.08131","DOI":"10.1145/3394486.3403064","CorpusId":219687277},"title":"An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks"},{"paperId":"3fe186e3e3c2c85d5a32639c8493301a71b3a10d","externalIds":{"DBLP":"journals/corr/abs-2006-07026","MAG":"3035690533","ArXiv":"2006.07026","CorpusId":219635800},"title":"Backdoor Attacks on Federated Meta-Learning"},{"paperId":"c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9","externalIds":{"ArXiv":"2006.01043","DBLP":"conf/acsac/Chen0C0MSW021","DOI":"10.1145/3485832.3485837","CorpusId":238354397},"title":"BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements"},{"paperId":"060c6c8312cd9277c97379e82db7c36b9321cce2","externalIds":{"DBLP":"conf/cvpr/LiDLYGH20","MAG":"3035569111","DOI":"10.1109/cvpr42600.2020.00072","CorpusId":219963354},"title":"Towards Transferable Targeted Attack"},{"paperId":"e83bc1edaecf2dcc6839a94521d4795053fe092c","externalIds":{"DBLP":"conf/cvpr/XuCXGSS20","MAG":"3035402938","DOI":"10.1109/cvpr42600.2020.01232","CorpusId":219617821},"title":"What Machines See Is Not What They Get: Fooling Scene Text Recognition Models With Adversarial Text Images"},{"paperId":"454814bad09c40c0d9c298973f0979f039fcd0f2","externalIds":{"DBLP":"journals/corr/abs-2011-00566","ArXiv":"2011.00566","MAG":"3095749375","DOI":"10.1109/cvpr42600.2020.01037","CorpusId":219962874},"title":"LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of Point Cloud Based Deep Networks"},{"paperId":"7b78530a02b677a4942461e06d4a36a9afb87b40","externalIds":{"DBLP":"journals/corr/abs-2005-14137","MAG":"3031859055","ArXiv":"2005.14137","DOI":"10.1109/cvpr42600.2020.00130","CorpusId":218972155},"title":"QEBA: Query-Efficient Boundary-Based Blackbox Attack"},{"paperId":"8b070b70f18b387a64b814e7c237e9b596f2a2d6","externalIds":{"MAG":"3032614845","DBLP":"journals/tkde/DingFZCJ22","DOI":"10.1109/tkde.2020.2997604","CorpusId":219952896},"title":"Privacy-Preserving Feature Extraction via Adversarial Training"},{"paperId":"0bc468fdbad864699b4cf48ded94709e6d0c0ce0","externalIds":{"ArXiv":"2005.06149","DBLP":"journals/corr/abs-2005-06149","MAG":"3025863369","CorpusId":218613936},"title":"DeepRobust: A PyTorch Library for Adversarial Attacks and Defenses"},{"paperId":"16d446f22f86596d7bee3a177e5cc846fc8bc02d","externalIds":{"MAG":"3022739615","DBLP":"conf/cvpr/LiJ0LZDT20","ArXiv":"2005.03837","DOI":"10.1109/cvpr42600.2020.00044","CorpusId":218571079},"title":"Projection & Probability-Driven Black-Box Attack"},{"paperId":"4b2df153540a64b293818bbd9b1c00d28a01025c","externalIds":{"DBLP":"conf/iclr/XieHCL20","MAG":"2995164118","CorpusId":213447399},"title":"DBA: Distributed Backdoor Attacks against Federated Learning"},{"paperId":"9ac39596269d614d63c828e9747b8dba08d87b54","externalIds":{"DBLP":"conf/iclr/Al-DujailiO20","MAG":"2994702848","CorpusId":213991250},"title":"Sign Bits Are All You Need for Black-Box Attacks"},{"paperId":"c9b56cb026a38e39bb0228faac57accd6f65e6f7","externalIds":{"MAG":"3105604018","DBLP":"conf/emnlp/MorrisLYGJQ20","ACL":"2020.emnlp-demos.16","DOI":"10.18653/v1/2020.emnlp-demos.16","CorpusId":220714040},"title":"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP"},{"paperId":"c723da5454a0f00cf7804fbf6ba71831355ce9da","externalIds":{"MAG":"3020621865","ArXiv":"2004.12519","DBLP":"journals/corr/abs-2004-12519","CorpusId":213179475},"title":"Transferable Perturbations of Deep Feature Distributions"},{"paperId":"ddbeddfe4d4afe5d1cf008447a6d657511c1ca76","externalIds":{"DBLP":"journals/corr/abs-2004-12261","ArXiv":"2004.12261","MAG":"3017720918","DOI":"10.1609/aaai.v35i16.17663","CorpusId":216553621},"title":"Enabling Fast and Universal Audio Adversarial Attack Using Generative Model"},{"paperId":"c66d8dadca2007545e9e211fc6f5f744b2237ad8","externalIds":{"DBLP":"journals/corr/abs-2010-03300","ArXiv":"2010.03300","MAG":"3092091881","DOI":"10.1609/AAAI.V34I04.6154","CorpusId":212941460},"title":"CD-UAP: Class Discriminative Universal Adversarial Perturbation"},{"paperId":"6a1465abc2714aa214e54452276db8cac8e3ec1e","externalIds":{"MAG":"3022021750","DBLP":"conf/nips/InkawhichLWICC20","ArXiv":"2004.14861","CorpusId":216867088},"title":"Perturbing Across the Feature Hierarchy to Improve Standard and Strict Blackbox Attack Transferability"},{"paperId":"efcfc3963577520f6d04e575018f07601ef058be","externalIds":{"MAG":"3011074732","DBLP":"conf/cvpr/RahmatiMFD20","ArXiv":"2003.06468","DOI":"10.1109/cvpr42600.2020.00847","CorpusId":212725250},"title":"GeoDA: A Geometric Framework for Black-Box Adversarial Attacks"},{"paperId":"0f68573a482a2b75b5db5306b8cb61529c866566","externalIds":{"MAG":"3009148757","ArXiv":"2003.04247","DBLP":"journals/corr/abs-2003-04247","CorpusId":212633829},"title":"Towards Probabilistic Verification of Machine Unlearning"},{"paperId":"26cb37bf23299d18af4a76b11907e821910f8e88","externalIds":{"DBLP":"conf/cvpr/DuanM00QY20","ArXiv":"2003.08757","MAG":"3010867827","DOI":"10.1109/cvpr42600.2020.00108","CorpusId":213175526},"title":"Adversarial Camouflage: Hiding Physical-World Attacks With Natural Styles"},{"paperId":"685b27cf6dc891da649af035507b7b43b3b4b0b9","externalIds":{"DBLP":"conf/eurosp/SalemWBMZ22","ArXiv":"2003.03675","MAG":"3010216907","DOI":"10.1109/EuroSP53844.2022.00049","CorpusId":212633480},"title":"Dynamic Backdoor Attacks Against Machine Learning Models"},{"paperId":"c44eb1e28f118d486d39d1b22354f58a95acd439","externalIds":{"DBLP":"journals/corr/abs-2003-03030","ArXiv":"2003.03030","MAG":"3034414373","DOI":"10.1109/cvpr42600.2020.01445","CorpusId":212628208},"title":"Clean-Label Backdoor Attacks on Video Recognition Models"},{"paperId":"18939eadc9c4460c8385e0591cde214a1ead067b","externalIds":{"MAG":"3034994123","ArXiv":"2003.01690","DBLP":"conf/icml/Croce020a","CorpusId":211818320},"title":"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks"},{"paperId":"2fb43a4c5cab8215d510fc585ca81fb5ee8a3abb","externalIds":{"MAG":"3006076803","DBLP":"journals/corr/abs-2002-05990","ArXiv":"2002.05990","CorpusId":211126665},"title":"Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets"},{"paperId":"4e363c443f0373e5f202fbf24113a939ed33196c","externalIds":{"DBLP":"journals/corr/abs-2001-05574","ArXiv":"2001.05574","MAG":"2999401213","CorpusId":210702315},"title":"Advbox: a toolbox to generate adversarial examples that fool neural networks"},{"paperId":"02d1168344c969761397b575b64765405752c354","externalIds":{"DBLP":"journals/corr/abs-1912-02771","ArXiv":"1912.02771","MAG":"2993846550","CorpusId":208637053},"title":"Label-Consistent Backdoor Attacks"},{"paperId":"8733fe2371b615609b04e2e910b1ecfa8e77cbc2","externalIds":{"MAG":"2991240978","DBLP":"journals/corr/abs-1912-00049","ArXiv":"1912.00049","DOI":"10.1007/978-3-030-58592-1_29","CorpusId":208527215},"title":"Square Attack: a query-efficient black-box adversarial attack via random search"},{"paperId":"3f2095bbbf3e47fbac26da70e95a219c23e3bac9","externalIds":{"DBLP":"journals/corr/abs-1911-07140","MAG":"2996603157","ArXiv":"1911.07140","CorpusId":208139568},"title":"Black-Box Adversarial Attack with Transferable Model-based Embedding"},{"paperId":"9584881e2f7aeadaebd4743bc876d1b492f3ab94","externalIds":{"DBLP":"conf/ccs/ZhaoZLSZ019","MAG":"2984260944","DOI":"10.1145/3319535.3354259","CorpusId":207947087},"title":"Seeing isn't Believing: Towards More Robust Adversarial Attack Against Real World Object Detectors"},{"paperId":"d477457f8ea5b7169ff370751653a6244c1c96c4","externalIds":{"MAG":"2985913519","DBLP":"conf/ccs/YaoLZZ19","DOI":"10.1145/3319535.3354209","CorpusId":202235190},"title":"Latent Backdoor Attacks on Deep Neural Networks"},{"paperId":"06fa914208bff3b5579ced913241868142daaeff","externalIds":{"ArXiv":"1910.14667","MAG":"2982663445","DBLP":"conf/eccv/WuLDG20","DOI":"10.1007/978-3-030-58548-8_1","CorpusId":207757900},"title":"Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors"},{"paperId":"af3d0e25266bf07b95aad67a16e19cd43d6db79e","externalIds":{"MAG":"2989807128","DBLP":"conf/eccv/XuZ0FSCCWL20","DOI":"10.1007/978-3-030-58558-7_39","CorpusId":208310168},"title":"Adversarial T-Shirt! Evading Person Detectors in a Physical World"},{"paperId":"fdd1844738a0e909f0c70015438ea78df22e478d","externalIds":{"DBLP":"conf/iccv/0009JLZ0WH19","MAG":"2983042735","DOI":"10.1109/ICCV.2019.00303","CorpusId":204957902},"title":"Universal Adversarial Perturbation via Prior Driven Uncertainty Approximation"},{"paperId":"31675c0dea68199af03a8c58b638b89df4b9db3f","externalIds":{"ArXiv":"1910.00033","MAG":"2996800219","DBLP":"conf/aaai/SahaSP20","DOI":"10.1609/AAAI.V34I07.6871","CorpusId":203610516},"title":"Hidden Trigger Backdoor Attacks"},{"paperId":"e8c46dade1aaedce96ecd03178379b5921a90306","externalIds":{"MAG":"2972650807","DBLP":"journals/corr/abs-1909-05040","ArXiv":"1909.05040","DOI":"10.1109/ICCV.2019.00482","CorpusId":202558502},"title":"Sparse and Imperceivable Adversarial Attacks"},{"paperId":"87536a1c9267ddda86eb7763dc475121b33caa83","externalIds":{"DBLP":"journals/corr/abs-1909-05193","MAG":"2972646903","ArXiv":"1909.05193","DOI":"10.1109/CVPR42600.2020.01321","CorpusId":202558533},"title":"TBT: Targeted Neural Network Attack With Bit Trojan"},{"paperId":"4b0a779731c4dbca1d23eca00de8c8d43426bd5b","externalIds":{"MAG":"3028815699","DBLP":"conf/cvpr/HuangGZXYZL20","ArXiv":"1909.04326","DOI":"10.1109/CVPR42600.2020.00080","CorpusId":219099302},"title":"Universal Physical Camouflage Attacks on Object Detectors"},{"paperId":"1d82424c0f152e33461e4a96dda672115d54babf","externalIds":{"DBLP":"journals/tdsc/LiXZZZ21","MAG":"3043789969","DOI":"10.1109/tdsc.2020.3021407","CorpusId":220633516},"title":"Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization"},{"paperId":"9928273360ae8dbefa6a16b618394da1932a6d0c","externalIds":{"DBLP":"conf/iccv/WangZSWRQ19","ArXiv":"1908.09327","MAG":"2998904801","DOI":"10.1109/ICCV.2019.00843","CorpusId":201650275},"title":"advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns"},{"paperId":"e3dacba4399fc66f31779060db730ea423f341d8","externalIds":{"DBLP":"conf/icpr/KomkovP20","ArXiv":"1908.08705","MAG":"2969664989","DOI":"10.1109/ICPR48806.2021.9412236","CorpusId":201645162},"title":"AdvHat: Real-World Adversarial Attack on ArcFace Face ID System"},{"paperId":"03d0978de148862e3bf7027110940212d1aa93da","externalIds":{"MAG":"2969333443","DBLP":"journals/corr/abs-1908-07000","ArXiv":"1908.07000","CorpusId":201103748},"title":"Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries"},{"paperId":"fb0568dcb546bbb4d7d1ed71ce395f4b66691003","externalIds":{"MAG":"2976752987","ArXiv":"1908.06281","DBLP":"conf/iclr/LinS00H20","CorpusId":202750057},"title":"Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks"},{"paperId":"5d973cd1a383b6bed1bf7a42a7cef21524d87d29","externalIds":{"DBLP":"conf/icb/DebZJ20","MAG":"2968618655","ArXiv":"1908.05008","DOI":"10.1109/IJCB48548.2020.9304898","CorpusId":199577709},"title":"AdvFaces: Adversarial Face Synthesis"},{"paperId":"90dcaf5b7ded95f61666711aa94a59eae77bc304","externalIds":{"ArXiv":"1907.10823","MAG":"2984699060","DBLP":"journals/corr/abs-1907-10823","DOI":"10.1109/ICCV.2019.00483","CorpusId":198893678},"title":"Enhancing Adversarial Example Transferability With an Intermediate Level Attack"},{"paperId":"4e39f266dc2864e34979b3363311a210df6f0c72","externalIds":{"MAG":"2905311601","DBLP":"conf/aaai/JanMLH019","DOI":"10.1609/AAAI.V33I01.3301962","CorpusId":53694866},"title":"Connecting the Digital and Physical World: Improving the Robustness of Adversarial Attacks"},{"paperId":"fea909742eeb8d6140220b069ebc4607f9a329ca","externalIds":{"DBLP":"conf/aaai/LiuLFMZXT19","MAG":"2905423756","DOI":"10.1609/AAAI.V33I01.33011028","CorpusId":70346871},"title":"Perceptual-Sensitive GAN for Generating Adversarial Patches"},{"paperId":"86f02b9f2042769b9a11e92bf68f65540cbfa1f6","externalIds":{"MAG":"3035524670","ArXiv":"1907.04449","DBLP":"conf/cvpr/KongGLL20","DOI":"10.1109/cvpr42600.2020.01426","CorpusId":208201893},"title":"PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving"},{"paperId":"edabab811aaf44b7b626efe5278a32bddd3bb77f","externalIds":{"MAG":"2971126145","ArXiv":"1906.06919","DBLP":"conf/nips/ChengDPSZ19","CorpusId":189928401},"title":"Improving Black-box Adversarial Attacks with a Transfer-based Prior"},{"paperId":"28caba783f6fa54369c101a3a3a83b83e94e179e","externalIds":{"MAG":"2950808778","ArXiv":"1906.04392","DBLP":"journals/corr/abs-1906-04392","CorpusId":184486994},"title":"Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks"},{"paperId":"11154f0b587691dfee5bd1484f0be539a853e6f1","externalIds":{"DBLP":"conf/iclr/DuZZYF20","MAG":"2948963698","ArXiv":"1906.02398","CorpusId":174802411},"title":"Query-efficient Meta Attack to Deep Neural Networks"},{"paperId":"301c36778a8581a2526d674e7bd5472279f35e07","externalIds":{"MAG":"2945335799","DBLP":"conf/cvpr/LiWYFZL19","DOI":"10.1109/CVPR.2019.00410","CorpusId":181726313},"title":"Compressing Convolutional Neural Networks via Factorized Convolutional Filters"},{"paperId":"553debb7e35f89daae199c1a3043b775b53babdd","externalIds":{"MAG":"3096024389","DBLP":"conf/eurosp/TanS20","ArXiv":"1905.13409","DOI":"10.1109/EuroSP48549.2020.00019","CorpusId":173188678},"title":"Bypassing Backdoor Detection Algorithms in Deep Learning"},{"paperId":"d2e2be68f6b0c0c604dd2154d32ef12a0cd439d3","externalIds":{"MAG":"2946859542","DBLP":"conf/aaai/LuH20","ArXiv":"1905.13288","DOI":"10.1609/AAAI.V34I04.5940","CorpusId":173188746},"title":"Structured Output Learning with Conditional Generative Flows"},{"paperId":"fb38fc75f58cf8e171d59b868b1afbddbb9a28eb","externalIds":{"MAG":"2947028053","DBLP":"journals/corr/abs-1906-00001","ArXiv":"1906.00001","CorpusId":173991164},"title":"Functional Adversarial Attacks"},{"paperId":"549180d76bc15ec3fcb9fa0e34d06bf7173faa6b","externalIds":{"DBLP":"conf/dac/ZhaoWGWFL19","MAG":"2945429037","ArXiv":"1905.12032","DOI":"10.1145/3316781.3317825","CorpusId":163164543},"title":"Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural Networks"},{"paperId":"65fd9ded2c411d90bcf6d38132463797754d2d21","externalIds":{"MAG":"2915002466","ArXiv":"1905.07121","DBLP":"conf/icml/GuoGYWW19","CorpusId":86541092},"title":"Simple Black-box Adversarial Attacks"},{"paperId":"9d26aa7cb2cba8f172b4299c61835509b1090ecd","externalIds":{"MAG":"2946814535","ArXiv":"1905.06635","DBLP":"conf/icml/MoonAS19","CorpusId":155100229},"title":"Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization"},{"paperId":"aacdfd9fd9bf59afdc6612678440581f229d270e","externalIds":{"ArXiv":"1905.04016","DBLP":"journals/corr/abs-1905-04016","MAG":"2944082840","DOI":"10.1109/CVPR.2019.00426","CorpusId":150373766},"title":"Exact Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables"},{"paperId":"1a6446a451472b1c5815ce3e00cf84eb7e641a2a","externalIds":{"MAG":"2943984343","DBLP":"conf/interspeech/NeekharaHPDMK19","ArXiv":"1905.03828","DOI":"10.21437/interspeech.2019-1353","CorpusId":150373723},"title":"Universal Adversarial Perturbations for Speech Recognition Systems"},{"paperId":"717425092f2e7a2a00c98e8e1778036e9b5a8b4e","externalIds":{"ArXiv":"1905.00441","MAG":"2963361074","DBLP":"journals/corr/abs-1905-00441","CorpusId":143424840},"title":"NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks"},{"paperId":"fda5f4facce9d5567c090d7ac733158e0fe93dc7","externalIds":{"MAG":"2902543210","DBLP":"conf/sp/LingJZWWLW19","DOI":"10.1109/SP.2019.00023","CorpusId":53075610},"title":"DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model"},{"paperId":"43e78439c6e37918133da064f2eed872b3b99ac5","externalIds":{"MAG":"2937975254","DBLP":"journals/corr/abs-1904-07793","CorpusId":119306993},"title":"AT-GAN: A Generative Attack Model for Adversarial Transferring on Generative Adversarial Nets"},{"paperId":"bd4336b6015d4d680a27c25a0ed296df5692ddf1","externalIds":{"DBLP":"journals/access/GuLDG19","MAG":"2942091739","DOI":"10.1109/ACCESS.2019.2909068","CorpusId":131777414},"title":"BadNets: Evaluating Backdooring Attacks on Deep Neural Networks"},{"paperId":"e7fabcd45e8077bdff75a8764186c92ee3f2670a","externalIds":{"DBLP":"conf/cvpr/TancikMN20","ArXiv":"1904.05343","MAG":"2936400017","DOI":"10.1109/cvpr42600.2020.00219","CorpusId":131773730},"title":"StegaStamp: Invisible Hyperlinks in Physical Photographs"},{"paperId":"2346706f89192fb3fcd5cc2a02732f928ef1c408","externalIds":{"MAG":"2972986629","DBLP":"journals/corr/abs-1904-04433","ArXiv":"1904.04433","DOI":"10.1109/CVPR.2019.00790","CorpusId":104291897},"title":"Efficient Decision-Based Black-Box Adversarial Attacks on Face Recognition"},{"paperId":"44d43bfbd23d55b1e7c4c4fd91fe101c0eaf1a06","externalIds":{"DBLP":"journals/corr/abs-1904-02884","ArXiv":"1904.02884","MAG":"2969542116","DOI":"10.1109/CVPR.2019.00444","CorpusId":102350868},"title":"Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks"},{"paperId":"493d5f344eea1468260946b29a80dc81b2be409c","externalIds":{"MAG":"2947657997","DBLP":"conf/sp/ChenJW20","ArXiv":"1904.02144","DOI":"10.1109/SP40000.2020.00045","CorpusId":173991158},"title":"HopSkipJumpAttack: A Query-Efficient Decision-Based Attack"},{"paperId":"42658c812d60d26a0bdad91b4d81e8620b994bf6","externalIds":{"MAG":"2934843808","DBLP":"conf/sp/WangYSLVZZ19","DOI":"10.1109/SP.2019.00031","CorpusId":67846878},"title":"Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks"},{"paperId":"8c66964f3bb4f8245cd1e328f6491f2204aba6c6","externalIds":{"ArXiv":"1903.10826","DBLP":"journals/corr/abs-1903-10826","MAG":"2924578333","DOI":"10.1109/ICCV.2019.00499","CorpusId":85518037},"title":"A Geometry-Inspired Decision-Based Attack"},{"paperId":"361c7858fa8f55928dc6358bc25d18fe3316d735","externalIds":{"MAG":"2952730822","ArXiv":"1903.10346","DBLP":"journals/corr/abs-1903-10346","CorpusId":85502769},"title":"Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition"},{"paperId":"364d8b12ace0aa0f80a3e56aff95baa1942cf3ad","externalIds":{"DBLP":"journals/corr/abs-1902-07906","MAG":"2915401840","ArXiv":"1902.07906","CorpusId":67788180},"title":"Wasserstein Adversarial Examples via Projected Sinkhorn Iterations"},{"paperId":"4f53bb893ca92f0a1b9b3d1bd2ee5de3cdb7c0da","externalIds":{"MAG":"2916286792","DBLP":"journals/corr/abs-1902-07623","ArXiv":"1902.07623","CorpusId":67769546},"title":"advertorch v0.1: An Adversarial Robustness Toolbox based on PyTorch"},{"paperId":"cb8889d0fd4db6baa3ffb1336e16724a6dc323de","externalIds":{"ArXiv":"1902.11237","MAG":"2915783303","DBLP":"journals/corr/abs-1902-11237","DOI":"10.1109/ICIP.2019.8802997","CorpusId":67855469},"title":"A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning"},{"paperId":"67e856f76905f2269089cede51c9a6a7c4fd2f8c","externalIds":{"MAG":"2997583194","DBLP":"journals/corr/abs-1812-03413","ArXiv":"1812.03413","DOI":"10.1609/AAAI.V34I07.6810","CorpusId":54462231},"title":"Learning Transferable Adversarial Examples via Ghost Networks"},{"paperId":"f2c782ca16a1c833e9c620a6a0f73bc7f60dcceb","externalIds":{"DBLP":"conf/iccv/LiJLH0019","MAG":"2984942115","ArXiv":"1812.00552","DOI":"10.1109/ICCV.2019.00500","CorpusId":54203800},"title":"Universal Perturbation Attack Against Image Retrieval"},{"paperId":"6c66108edb9af0533309055e7b2ecb8922db03d8","externalIds":{"MAG":"2952060552","ArXiv":"1811.12470","DBLP":"journals/corr/abs-1811-12470","CorpusId":54203999},"title":"Analyzing Federated Learning through an Adversarial Lens"},{"paperId":"8bf18d546ae7aea144663136c5704049953ce4e2","externalIds":{"DBLP":"conf/cvpr/ModasMF19","MAG":"2952518992","ArXiv":"1811.02248","DOI":"10.1109/CVPR.2019.00930","CorpusId":53229868},"title":"SparseFool: A Few Pixels Make a Big Difference"},{"paperId":"4e755130f1f48d32610b431154aee7335b88e1c3","externalIds":{"MAG":"2965734111","ArXiv":"1810.11793","DBLP":"journals/corr/abs-1810-11793","DOI":"10.24963/ijcai.2019/741","CorpusId":53096484},"title":"Robust Audio Adversarial Example for a Physical Attack"},{"paperId":"2bf8827eb761ea755909065a06af18b86d363001","externalIds":{"MAG":"2941733693","DBLP":"conf/btas/Agarwal0VR18","DOI":"10.1109/BTAS.2018.8698548","CorpusId":53124252},"title":"Are Image-Agnostic Universal Adversarial Perturbations for Face Recognition Difficult to Detect?"},{"paperId":"a2b5d224895d96bfe2e384e2dcf1ebd136ac3782","externalIds":{"DBLP":"conf/iclr/TonevaSCTBG19","MAG":"2903996579","ArXiv":"1812.05159","CorpusId":55481903},"title":"An Empirical Study of Example Forgetting during Deep Neural Network Learning"},{"paperId":"1879d6b29eee6efab8f6217a7a6f47ec04f25b3e","externalIds":{"ArXiv":"1809.04790","DBLP":"journals/tnn/0002L20","MAG":"2891828758","DOI":"10.1109/TNNLS.2019.2933524","CorpusId":52269752,"PubMed":"31722487"},"title":"Adversarial Examples: Opportunities and Challenges"},{"paperId":"98735e57075ed6e8ef9d98d7ca4895013492e35b","externalIds":{"DBLP":"conf/codaspy/ZhongLSZ020","MAG":"2889233174","ArXiv":"1808.10307","DOI":"10.1145/3374664.3375751","CorpusId":52138086},"title":"Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation"},{"paperId":"5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1","externalIds":{"MAG":"2887906331","DBLP":"conf/iclr/XuLZCZFEWL19","ArXiv":"1808.01664","CorpusId":51928102},"title":"Structured Adversarial Attack: Towards General Implementation and Better Interpretability"},{"paperId":"4bd23d951846832bdf550df574cdec07bc08dec1","externalIds":{"MAG":"2887447938","ArXiv":"1808.00278","DBLP":"conf/eccv/LiuWLYLC18","DOI":"10.1007/978-3-030-01267-0_44","CorpusId":51892264},"title":"Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm"},{"paperId":"f58ed426354e3a6b6234fe132807b9fb30f725b4","externalIds":{"DBLP":"journals/corr/abs-1903-07282","MAG":"2902437702","ArXiv":"1903.07282","DOI":"10.1109/ICPR.2018.8545152","CorpusId":54198027},"title":"Generating Adversarial Examples With Conditional Generative Adversarial Net"},{"paperId":"10ab21b120e305b6d3cbf81c5a906d36521152f1","externalIds":{"MAG":"2951992509","DBLP":"journals/corr/abs-1807-07978","ArXiv":"1807.07978","CorpusId":49907212},"title":"Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors"},{"paperId":"c67855ff840c9f8d256486158c63a242d912e41e","externalIds":{"MAG":"2964175514","DBLP":"conf/woot/SongEEF0RTPK18","ArXiv":"1807.07769","CorpusId":49904930},"title":"Physical Adversarial Examples for Object Detectors"},{"paperId":"b862efa06baea0b032214675eb3c3645d5d69d46","externalIds":{"ArXiv":"1807.04457","DBLP":"journals/corr/abs-1807-04457","MAG":"2951006199","CorpusId":49672236},"title":"Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach"},{"paperId":"14d8b4fdb0262c30ae9afe20ea8e7227b115c63e","externalIds":{"MAG":"3037024761","ArXiv":"1807.00459","DBLP":"conf/aistats/BagdasaryanVHES20","CorpusId":49557410},"title":"How To Backdoor Federated Learning"},{"paperId":"3e16de062b9cdeecfcbda0de022f1fc4e741a2e6","externalIds":{"MAG":"2951174316","ArXiv":"1806.10348","ACL":"C18-1315","DBLP":"journals/corr/abs-1806-10348","CorpusId":49480783},"title":"Learning Visually-Grounded Semantics from Contrastive Adversarial Samples"},{"paperId":"f0c5991dbb130fa6b5de011cf7a04f6ed815ef68","externalIds":{"MAG":"2798302089","DBLP":"conf/cvpr/EykholtEF0RXPKS18","DOI":"10.1109/CVPR.2018.00175","CorpusId":29162614},"title":"Robust Physical-World Attacks on Deep Learning Visual Classification"},{"paperId":"fd7789de401811fd8692466b8d49230e7184655f","externalIds":{"MAG":"2889210204","ArXiv":"1805.09190","DBLP":"conf/iclr/SchottRBB19","CorpusId":52135921},"title":"Towards the first adversarially robust neural network model on MNIST"},{"paperId":"5023544ad6fa49b35526a62f22207e43c4db870d","externalIds":{"ArXiv":"1805.07894","MAG":"2890591829","DBLP":"conf/nips/SongSKE18","CorpusId":52309169},"title":"Constructing Unrestricted Adversarial Examples with Generative Models"},{"paperId":"b3f83e8416010e9c3a705a0b6390d268e5ddf5c0","externalIds":{"ArXiv":"1804.08598","DBLP":"journals/corr/abs-1804-08598","MAG":"2963062382","CorpusId":5046541},"title":"Black-box Adversarial Attacks with Limited Queries and Information"},{"paperId":"e77fb2fa94916505bc8af5b56bc503d8bfac7fb9","externalIds":{"MAG":"3105806188","ArXiv":"1804.05810","DBLP":"conf/pkdd/ChenCMC18","DOI":"10.1007/978-3-030-10925-7_4","CorpusId":4891043},"title":"ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector"},{"paperId":"ab2f8be8cd0474b3ab2c77b1bfeee25f25cad076","externalIds":{"MAG":"2950287232","ArXiv":"1804.03193","DBLP":"conf/mm/Zhao0WL18","DOI":"10.1145/3240508.3240639","CorpusId":4752963},"title":"An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks"},{"paperId":"c6d4ef1a98b1b9e5875b79efd3ef2a73403a0884","externalIds":{"MAG":"2949630528","DBLP":"conf/nips/ShafahiHNSSDG18","ArXiv":"1804.00792","CorpusId":4626477},"title":"Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks"},{"paperId":"f78a911f516625d6b7b76a9a33c1eb14613341c4","externalIds":{"MAG":"2962847335","ArXiv":"1803.06978","DBLP":"conf/cvpr/XieZZBWRY19","DOI":"10.1109/CVPR.2019.00284","CorpusId":3972825},"title":"Improving Transferability of Adversarial Examples With Input Diversity"},{"paperId":"12ccfc188de0b40c84d6a427999239c6a379cd66","externalIds":{"MAG":"2793079679","DBLP":"conf/aaai/WeiZYS19","ArXiv":"1803.02536","DOI":"10.1609/aaai.v33i01.33018973","CorpusId":3735440},"title":"Sparse Adversarial Perturbations for Videos"},{"paperId":"ba28252bea09d49202d45f04b69f9b357dff0a6f","externalIds":{"MAG":"2949717599","DBLP":"journals/corr/abs-1802-04633","ArXiv":"1802.04633","CorpusId":3322503},"title":"Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring"},{"paperId":"60f997a400188233f2448d00a77c25784c4c8dc3","externalIds":{"MAG":"2964171870","DBLP":"journals/pami/MopuriGB19","ArXiv":"1801.08092","DOI":"10.1109/TPAMI.2018.2861800","CorpusId":27329456,"PubMed":"30072314"},"title":"Generalizable Data-Free Objective for Crafting Universal Adversarial Perturbations"},{"paperId":"ff9318366692bdf16b31b0b74ff71c0deeb4b227","externalIds":{"MAG":"2952181799","ArXiv":"1801.02608","DBLP":"conf/icml/KarmonZG18","CorpusId":3641286},"title":"LaVAN: Localized and Visible Adversarial Noise"},{"paperId":"d3c071dbbb4520ed5875f7e064a9da87240534db","externalIds":{"ArXiv":"1801.02612","DBLP":"conf/iclr/XiaoZ0HLS18","MAG":"2953291620","CorpusId":3463636},"title":"Spatially Transformed Adversarial Examples"},{"paperId":"d5577abcc1fbf57d66017e3b5b2211a82022842c","externalIds":{"MAG":"2949138316","DBLP":"conf/ijcai/XiaoLZHLS18","ArXiv":"1801.02610","DOI":"10.24963/ijcai.2018/543","CorpusId":21949142},"title":"Generating Adversarial Examples with Adversarial Networks"},{"paperId":"6a5704ac5fdacb7121a0c02a9be4de2bdc5a40fc","externalIds":{"ArXiv":"1801.01944","MAG":"2964301649","DBLP":"journals/corr/abs-1801-01944","DOI":"10.1109/SPW.2018.00009","CorpusId":4475201},"title":"Audio Adversarial Examples: Targeted Attacks on Speech-to-Text"},{"paperId":"b514949ad8344071c0f342f182390d2d88bcc26d","externalIds":{"MAG":"2962700793","DBLP":"journals/corr/abs-1801-00553","ArXiv":"1801.00553","DOI":"10.1109/ACCESS.2018.2807385","CorpusId":3536399},"title":"Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"},{"paperId":"7b874e6fc3e3db83b350824f372761a415f5725f","externalIds":{"MAG":"2932026309","DBLP":"journals/tissec/SharifBBR19","ArXiv":"1801.00349","DOI":"10.1145/3317611","CorpusId":132058467},"title":"A General Framework for Adversarial Examples with Objectives"},{"paperId":"cb4c2a2d7e50667914d1a648f1a9134056724780","externalIds":{"DBLP":"journals/corr/abs-1712-05526","MAG":"2774423163","ArXiv":"1712.05526","CorpusId":36122023},"title":"Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning"},{"paperId":"1b225474e7a5794f98cdfbde8b12ccbc56799409","externalIds":{"MAG":"2963070423","DBLP":"journals/corr/abs-1712-04248","ArXiv":"1712.04248","CorpusId":2410333},"title":"Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models"},{"paperId":"0314e777333a63aca5735ea136c74e113aa8801d","externalIds":{"MAG":"2972410041","DBLP":"conf/icml/EngstromTTSM19","CorpusId":174800822},"title":"Exploring the Landscape of Spatial Robustness"},{"paperId":"e8da4ff1519011ed018202bb96dee4b611f5d842","externalIds":{"DBLP":"conf/cvpr/PoursaeedKGB18","MAG":"2963855547","ArXiv":"1712.02328","DOI":"10.1109/CVPR.2018.00465","CorpusId":4670982},"title":"Generative Adversarial Perturbations"},{"paperId":"96e53b9c4e7ce52b32f090ceb3b069786559a2b7","externalIds":{"MAG":"2951280437","ACL":"P18-1241","DBLP":"conf/acl/HsiehYCZC18","DOI":"10.18653/v1/P18-1241","CorpusId":44220219},"title":"Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning"},{"paperId":"d511567a87218a555a4cc662c42ee89f044a5852","externalIds":{"MAG":"2771036112","DBLP":"conf/nips/Baluja17","CorpusId":29764034},"title":"Hiding Images in Plain Sight: Deep Steganography"},{"paperId":"47eb8d7ea4f7c209040ddd82e264edf3945df6cb","externalIds":{"MAG":"2950836761","DBLP":"journals/corr/abs-1711-09115","ArXiv":"1711.09115","DOI":"10.1109/CVPR.2018.00467","CorpusId":2605689},"title":"Geometric Robustness of Deep Networks: Analysis and Improvement"},{"paperId":"900d130f25f453948fedf7df5faf4ab710fb5e94","externalIds":{"MAG":"2771112233","DBLP":"conf/iccad/LiuWLX17","DOI":"10.1109/ICCAD.2017.8203770","CorpusId":4702121},"title":"Fault injection attack on deep neural network"},{"paperId":"a6f835ca6e12245a835ab6074bc6ec2c3c60b85a","externalIds":{"MAG":"2964006983","DBLP":"journals/tec/SuVS19","ArXiv":"1710.08864","DOI":"10.1109/TEVC.2019.2890858","CorpusId":2698863},"title":"One Pixel Attack for Fooling Deep Neural Networks"},{"paperId":"8e37a3b227b68953f8067215828dc8b8714cb21b","externalIds":{"DBLP":"conf/cvpr/DongLPS0HL18","MAG":"2774644650","DOI":"10.1109/CVPR.2018.00957","CorpusId":4119221},"title":"Boosting Adversarial Attacks with Momentum"},{"paperId":"2fa9b32ebc329d57fa2e3fabb9e12382f019f47a","externalIds":{"DBLP":"conf/cvpr/XuCLRDS18","MAG":"2950176936","DOI":"10.1109/CVPR.2018.00520","CorpusId":44533048},"title":"Fooling Vision and Language Models Despite Localization and Attention Mechanism"},{"paperId":"8dce99e33c6fceb3e79023f5894fdbe733c91e92","externalIds":{"MAG":"2963557656","DBLP":"conf/icml/AthalyeEIK18","CorpusId":2645819},"title":"Synthesizing Robust Adversarial Examples"},{"paperId":"21f360cce06ffdb3e7a8a56f7499cc79ff86405a","externalIds":{"MAG":"2738229973","DBLP":"conf/bmvc/MopuriGR17","ArXiv":"1707.05572","DOI":"10.5244/C.31.30","CorpusId":38305772},"title":"Fast Feature Fool: A data independent approach to universal adversarial perturbations"},{"paperId":"7aa38b85fa8cba64d6a4010543f6695dbf5f1386","externalIds":{"DBLP":"conf/iclr/MadryMSTV18","MAG":"2952649158","ArXiv":"1706.06083","CorpusId":3488815},"title":"Towards Deep Learning Models Resistant to Adversarial Attacks"},{"paperId":"03215fa5b945ee7ab1ac7dba1576c43f96665a11","externalIds":{"DBLP":"conf/iccv/OhFS17","MAG":"2605289356","ArXiv":"1703.09471","DOI":"10.1109/ICCV.2017.165","CorpusId":206770916},"title":"Adversarial Image Perturbation for Privacy Protection A Game Theory Perspective"},{"paperId":"2a76bf5895759af6a561d81a0e2960cd00f1167e","externalIds":{"DBLP":"journals/corr/NarodytskaK16","MAG":"2561498661","ArXiv":"1612.06299","CorpusId":12443915},"title":"Simple Black-Box Adversarial Perturbations for Deep Networks"},{"paperId":"16aa01ca0834a924c25faad5d8bfef3fd1acfcfe","externalIds":{"DBLP":"conf/cvpr/Moosavi-Dezfooli17","MAG":"2543927648","ArXiv":"1610.08401","DOI":"10.1109/CVPR.2017.17","CorpusId":11558223},"title":"Universal Adversarial Perturbations"},{"paperId":"7f57e9939560562727344c1c987416285ef76cda","externalIds":{"DBLP":"conf/ccs/SharifBBR16","MAG":"2535873859","DOI":"10.1145/2976749.2978392","CorpusId":207241700},"title":"Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition"},{"paperId":"7fcb90f68529cbfab49f471b54719ded7528d0ef","externalIds":{"ArXiv":"1610.05492","MAG":"2535838896","DBLP":"journals/corr/KonecnyMYRSB16","CorpusId":14999259},"title":"Federated Learning: Strategies for Improving Communication Efficiency"},{"paperId":"d86c9623d56e469aec73a76758a829891b0b2a09","externalIds":{"ArXiv":"1610.00768","MAG":"2810611310","CorpusId":125627987},"title":"Technical Report on the CleverHans v2.1.0 Adversarial Examples Library"},{"paperId":"df40ce107a71b770c9d0354b78fdd8989da80d2f","externalIds":{"DBLP":"conf/sp/Carlini017","MAG":"2951755642","ArXiv":"1608.04644","DOI":"10.1109/SP.2017.49","CorpusId":2893830},"title":"Towards Evaluating the Robustness of Neural Networks"},{"paperId":"b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b","externalIds":{"DBLP":"conf/iclr/KurakinGB17a","MAG":"2460937040","ArXiv":"1607.02533","DOI":"10.1201/9781351251389-8","CorpusId":1257772},"title":"Adversarial examples in the physical world"},{"paperId":"7568d13a82f7afa4be79f09c295940e48ec6db89","externalIds":{"MAG":"2475287302","DBLP":"conf/cvpr/GatysEB16","DOI":"10.1109/CVPR.2016.265","CorpusId":206593710},"title":"Image Style Transfer Using Convolutional Neural Networks"},{"paperId":"0a7b8aa019f49b99ad9aececeb0ea7fbadd2262d","externalIds":{"ArXiv":"1604.07666","MAG":"2964333506","DOI":"10.1109/TPAMI.2018.2845842","CorpusId":260743279,"PubMed":"29994196"},"title":"lp-Box ADMM: A Versatile Framework for Integer Programming."},{"paperId":"21a0b0fbdde1aee56fe10e69e897decaf21f43a6","externalIds":{"MAG":"2149479912","DBLP":"journals/focm/NesterovS17","DOI":"10.1007/s10208-015-9296-2","CorpusId":2147817},"title":"Random Gradient-Free Minimization of Convex Functions"},{"paperId":"819167ace2f0caae7745d2f25a803979be5fbfae","externalIds":{"MAG":"2949152835","ArXiv":"1511.07528","DBLP":"conf/eurosp/PapernotMJFCS16","DOI":"10.1109/EUROSP.2016.36","CorpusId":7004303},"title":"The Limitations of Deep Learning in Adversarial Settings"},{"paperId":"52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35","externalIds":{"ArXiv":"1511.04599","MAG":"2243397390","DBLP":"conf/cvpr/Moosavi-Dezfooli16","DOI":"10.1109/CVPR.2016.282","CorpusId":12387176},"title":"DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks"},{"paperId":"45715d15904bf40e39004f98daf8960a9b36545e","externalIds":{"MAG":"2962968216","DBLP":"conf/bmvc/FawziF15","ArXiv":"1507.06535","DOI":"10.5244/C.29.106","CorpusId":1173981},"title":"Manitest: Are classifiers really invariant?"},{"paperId":"bee044c8e8903fb67523c1f8c105ab4718600cdb","externalIds":{"MAG":"2952181735","DBLP":"journals/corr/GoodfellowSS14","ArXiv":"1412.6572","CorpusId":6706414},"title":"Explaining and Harnessing Adversarial Examples"},{"paperId":"7304b5ae51dc9d91bd988141d6d7b1f02aa3f86f","externalIds":{"MAG":"2104067967","DOI":"10.1002/CPA.21423","CorpusId":17820269},"title":"A Family of Nonparametric Density Estimation Algorithms"},{"paperId":"847aebda115d8d64f3b6c4f2c5960c4202f34e36","externalIds":{"MAG":"2951330223","ArXiv":"1109.0337","DBLP":"journals/corr/abs-1109-0337","DOI":"10.4314/NJTR.V7I1.87943","CorpusId":1195959},"title":"On discrete cosine transform"},{"paperId":"0ddae0a1b2ade9f8f35895e98c6ec15e882282bb","externalIds":{"MAG":"2053381548","DBLP":"journals/siamcomp/FeigeMV11","DOI":"10.1109/FOCS.2007.29","CorpusId":6124416},"title":"Maximizing Non-Monotone Submodular Functions"},{"paperId":"5ae20e0bdfddc1888148e0fcde88d937e96318d2","externalIds":{"DBLP":"conf/icml/YuJ09","MAG":"2147196093","DOI":"10.1145/1553374.1553523","CorpusId":10240161},"title":"Learning structural SVMs with latent variables"},{"paperId":"dd5cf95a7af93d2733120d177c593989b19b98fe","externalIds":{"DBLP":"conf/cec/WierstraSPS08","MAG":"2166160300","DOI":"10.1109/CEC.2008.4631255","CorpusId":4654385},"title":"Natural Evolution Strategies"},{"paperId":"668b1277fbece28c4841eeab1c97e4ebd0079700","externalIds":{"MAG":"2009086942","DBLP":"journals/technometrics/Neal07","DOI":"10.1007/978-0-387-45528-0","CorpusId":31993898},"title":"Pattern Recognition and Machine Learning"},{"paperId":"d98ef875e2cbde3e2cc8fad521e3cbfe1bddbd69","externalIds":{"MAG":"2138019504","DOI":"10.1111/j.1467-9868.2005.00532.x","CorpusId":6162124},"title":"Model selection and estimation in regression with grouped variables"},{"paperId":"8ef97a7ccaf28ea6bdf8551ade0fe982880d2fa3","externalIds":{"DBLP":"journals/pr/ChangHC03","MAG":"2073449274","DOI":"10.1016/S0031-3203(02)00289-3","CorpusId":29695236},"title":"Finding optimal least-significant-bit substitution in image hiding by dynamic programming strategy"},{"paperId":"35516916cd8840566acc05d0226f711bee1b563b","externalIds":{"MAG":"1520914943","DBLP":"conf/iptps/Douceur02","DOI":"10.1007/3-540-45748-8_24","CorpusId":5310675},"title":"The Sybil Attack"},{"paperId":"2aa59791b76955de19d0ea52c3b47940d5d0f6e1","externalIds":{"MAG":"2066021566","DOI":"10.1109/4236.935180","CorpusId":18036992},"title":"Digital steganography: hiding data within data"},{"paperId":"c973cd07dc3cd3d768dabb122dce346fb8b44199","externalIds":{"DBLP":"conf/iclr/QiXLMM23","CorpusId":259298183},"title":"Revisiting the Assumption of Latent Separability for Backdoor Defenses"},{"paperId":"548f19e36d53ecbf2473234eefb2d3ee6549bb06","externalIds":{"CorpusId":17098047},"title":"Deep Neural Networks"},{"paperId":"a5ee25f920ba92d7d89ec7789a820d6da31a4f3a","externalIds":{"DBLP":"conf/nips/ZhangJCLW23","CorpusId":265214908},"title":"A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning"},{"paperId":"442f1327b0dd4b66c2186dbf14f500266e17c762","externalIds":{"DBLP":"conf/nips/NguyenNTDW23","CorpusId":268064991},"title":"IBA: Towards Irreversible Backdoor Attacks in Federated Learning"},{"paperId":"e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4","externalIds":{"DBLP":"journals/corr/abs-2310-15140","DOI":"10.48550/arXiv.2310.15140","CorpusId":268100153},"title":"AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models"},{"paperId":"d2a7640459ac5fb1809089f8a98142d75357d6b9","externalIds":{"DBLP":"conf/iclr/ZhuSL22","CorpusId":251649168},"title":"Rethinking Adversarial Transferability from a Data Distribution Perspective"},{"paperId":"8cdd954fb8c4e46abdbb7da29c92ad41037cd757","externalIds":{"DBLP":"conf/eccv/WangYXATW22","DOI":"10.1007/978-3-031-19778-9_23","CorpusId":253448651},"title":"An Invisible Black-Box Backdoor Attack Through Frequency Domain"},{"paperId":"c6d9b72bf69091071fd92ea3ccfcc53f2b070434","externalIds":{"DBLP":"journals/compsec/XueHWSZWL22","CorpusId":261291958},"title":"PTB: Robust physical backdoor attacks against deep neural networks in real world"},{"paperId":"6dc724ddae9f5e1913036dd532af6a89b3267c1f","externalIds":{"DBLP":"conf/nips/ChenWW22","CorpusId":258509702},"title":"Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples"},{"paperId":"30fff834803a610981264a08e90ce6bbfa086008","externalIds":{"DBLP":"conf/nips/DoanLL21","CorpusId":245011163},"title":"Backdoor Attack with Imperceptible Input and Latent Modification"},{"paperId":"a39767bc5080dd184b6e488a1fe86790c26f0393","externalIds":{"MAG":"3109309187","DBLP":"conf/acml/WuZ20","CorpusId":227179997},"title":"Towards Understanding and Improving the Transferability of Adversarial Examples in Deep Neural Networks"},{"paperId":"0004e150d93d52624a2caf7a9d37e2be9d4dae3f","externalIds":{"DBLP":"conf/eccv/FanWLZLLY20","MAG":"3104218734","DOI":"10.1007/978-3-030-58542-6_3","CorpusId":227032974},"title":"Sparse Adversarial Attack via Perturbation Factorization"},{"paperId":"ac881cdee55f9dd13e3ef23ec01a1aa8650ad3e3","externalIds":{"MAG":"3105976275","DBLP":"conf/eccv/ChenZHW20","DOI":"10.1007/978-3-030-58555-6_17","CorpusId":227032832},"title":"Boosting Decision-Based Black-Box Adversarial Attacks with Random Sign Flip"},{"paperId":"52f2fceeca09116016784616f0fb94316db5e6cf","externalIds":{"DBLP":"conf/raid/FungYB20","CorpusId":221542915},"title":"The Limitations of Federated Learning in Sybil Settings"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3ae1544865a18d8649a5c4939f9eb17165b23bea","externalIds":{"MAG":"2913235840","DBLP":"conf/iclr/LiuCCH19","CorpusId":108298677},"title":"signSGD via Zeroth-Order Oracle"},{"paperId":"08f7ac64b420210aa46fcbbdb0f206215f2e0644","externalIds":{"DBLP":"conf/ndss/LiuMALZW018","CorpusId":31806516},"title":"Trojaning Attack on Neural Networks"},{"paperId":"c68796f833a7151f0a63d1d1608dc902b4fdc9b6","externalIds":{"CorpusId":10319744},"title":"GENERATIVE ADVERSARIAL NETS"},{"paperId":"59d9318f07331ec15e54fe2a4218bc4a5c247a38","externalIds":{"CorpusId":3995611},"title":"Foolbox: A Python toolbox to benchmark the robustness of machine learning models"},{"paperId":"a733492f597c4d0a27a24c8fffdf5d116fa7e09f","externalIds":{"DOI":"10.1201/b13752-8","CorpusId":4660196},"title":"Automatic Speech Recognition"}]}