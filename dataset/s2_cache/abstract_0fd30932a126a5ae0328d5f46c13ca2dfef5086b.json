{"abstract":"In edge computing, an important problem is how to allocate dependent tasks to resource-limited edge servers, where some tasks can only be performed after accomplishing some other tasks. Most related studies assume that server status remains unchanged, which might be invalid in some real-world scenarios. Thus, this paper studies the new problem of how to dynamically allocate dependent tasks in resource-limited edge computing. This problem poses two challenges: 1) how to cope with dynamic changes in server status and task arrival, and 2) how to handle the dependency information for decisionmaking in task allocation. Our solution is a graph convolutional reinforcement learning-based task-allocation agent consisting of an encoding part and a decision-making part. The encoding part represents the dependent tasks as directed acyclic graphs and employs a graph convolutional network (GCN) to embed the dependency information of the tasks. It can effectively deal with the dependency and so permit decision-making. The decision-making part formulates the task allocation problem as a Markov decision process to cope with the dynamic changes. Specially, the agent employs deep reinforcement learning to achieve dynamic decision-making for task allocation with the target of optimizing some metric (e.g., minimizing delay costs and energy cost). Experiments verify that our algorithm offers significantly better performance than the existing algorithms examined."}