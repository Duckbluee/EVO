{"references":[{"paperId":"d2673a3c21b461373b9fa3ce5b0cc785db7fbfe7","externalIds":{"DBLP":"journals/tcsv/ShengCZDZL23","DOI":"10.1109/TCSVT.2023.3276518","CorpusId":258731424},"title":"PDR: Progressive Depth Regularization for Monocular 3D Object Detection"},{"paperId":"4da9c7c96f18160576cca66cc580f652245fbb13","externalIds":{"DBLP":"journals/tcsv/WangDHZZZ23","DOI":"10.1109/TCSVT.2023.3272734","CorpusId":258738572},"title":"Long-Short Range Adaptive Transformer With Dynamic Sampling for 3D Object Detection"},{"paperId":"163b4d6a79a5b19af88b8585456363340d9efd04","externalIds":{"ArXiv":"2303.08774","CorpusId":257532815},"title":"GPT-4 Technical Report"},{"paperId":"640ef43c4aaa75516acfcef569cd9e7819cef649","externalIds":{"DBLP":"journals/vc/SeniorSYR25","ArXiv":"2303.03761","DOI":"10.1007/s00371-024-03343-0","CorpusId":257378523},"title":"Graph neural networks in vision-language image understanding: a survey"},{"paperId":"6e31cedc02984a564107d15716071c86894bedf0","externalIds":{"DBLP":"conf/cvpr/ZengZLWCW23","ArXiv":"2303.02437","DOI":"10.1109/CVPR52729.2023.02247","CorpusId":257365573},"title":"ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing"},{"paperId":"98be3a878abef9ba0fc624a6ada7b8607da26aae","externalIds":{"ArXiv":"2303.02489","DBLP":"journals/corr/abs-2303-02489","DOI":"10.1109/CVPR52729.2023.01462","CorpusId":257365027},"title":"CapDet: Unifying Dense Captioning and Open-World Detection Pretraining"},{"paperId":"0938d0ccc1c633fa0f8c067d914358b1ef53a44b","externalIds":{"ArXiv":"2302.14115","DBLP":"conf/cvpr/YangNSMPLSS23","DOI":"10.1109/CVPR52729.2023.01032","CorpusId":257232853},"title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning"},{"paperId":"92d3f7cea95bba8cb905454324c3eeb84d2b6e58","externalIds":{"ArXiv":"2301.02508","DBLP":"journals/corr/abs-2301-02508","DOI":"10.1109/CVPR52729.2023.01070","CorpusId":255522451},"title":"End-to-End 3D Dense Captioning with Vote2Cap-DETR"},{"paperId":"64d0de48e288056320216f7905b2f4690e994840","externalIds":{"DBLP":"journals/corr/abs-2212-06785","ArXiv":"2212.06785","DOI":"10.1109/CVPR52729.2023.02085","CorpusId":254591457},"title":"Learning 3D Representations from 2D Pre-Trained Models via Image-to-Point Masked Autoencoders"},{"paperId":"a6a59c9e4cd446d0d04f76587699e3e8ab5197c2","externalIds":{"DBLP":"journals/corr/abs-2212-03099","ArXiv":"2212.03099","DOI":"10.1109/CVPR52729.2023.02237","CorpusId":254275438},"title":"Semantic-Conditional Diffusion Networks for Image Captioning*"},{"paperId":"c4abd21df3ca0d7a7b59516fb7b6502c16edeec1","externalIds":{"ArXiv":"2210.12513","CorpusId":259076408},"title":"Learning Point-Language Hierarchical Alignment for 3D Visual Grounding"},{"paperId":"99d84af9a2818af9b9997931fe841b33334e0f8f","externalIds":{"ArXiv":"2210.03925","DBLP":"journals/corr/abs-2210-03925","DOI":"10.48550/arXiv.2210.03925","CorpusId":252781099},"title":"Contextual Modeling for 3D Dense Captioning on Point Clouds"},{"paperId":"f255ada7ab4db15191376090584c3f43533cc3f3","externalIds":{"DBLP":"journals/tcsv/CaoAZW22","DOI":"10.1109/TCSVT.2022.3178844","CorpusId":249220818},"title":"Vision-Enhanced and Consensus-Aware Transformer for Image Captioning"},{"paperId":"30a12c17253b5033c208e6bf43e9a4f4525aa705","externalIds":{"DBLP":"journals/tcsv/XianLTM22","DOI":"10.1109/TCSVT.2022.3155795","CorpusId":247353013},"title":"Adaptive Path Selection for Dynamic Image Captioning"},{"paperId":"b64537bdf7a103aa01972ba06ea24a9c08f7cd74","externalIds":{"DBLP":"conf/iclr/ChenZH23","ArXiv":"2208.04202","DOI":"10.48550/arXiv.2208.04202","CorpusId":251402961},"title":"Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning"},{"paperId":"fade0ef67bcad3369e83348111a73c0f9578786f","externalIds":{"DBLP":"conf/cvpr/CaiZZSX22","DOI":"10.1109/CVPR52688.2022.01597","CorpusId":250980730},"title":"3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds"},{"paperId":"140f83a8774b1b847aa0aa6a5b0ac63fce27bb00","externalIds":{"DBLP":"journals/corr/abs-2204-12974","ACL":"2022.emnlp-main.226","ArXiv":"2204.12974","DOI":"10.48550/arXiv.2204.12974","CorpusId":248405744},"title":"CapOnImage: Context-driven Dense-Captioning on Image"},{"paperId":"6fbefc60c862737eeaa9e464b9e8af58e1e78020","externalIds":{"DBLP":"conf/ijcai/0007ZY022","ArXiv":"2204.10688","DOI":"10.48550/arXiv.2204.10688","CorpusId":248366424},"title":"Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds"},{"paperId":"751b7e1d3c94ad8afaed5668f1bcc72a1406f42a","externalIds":{"ACL":"2022.coling-1.498","ArXiv":"2204.08121","DBLP":"journals/corr/abs-2204-08121","DOI":"10.48550/arXiv.2204.08121","CorpusId":248227589},"title":"End-to-end Dense Video Captioning as Sequence Generation"},{"paperId":"07546f1f0b35012a5d8ca850580438f306f3bdd3","externalIds":{"ArXiv":"2203.05203","DBLP":"conf/eccv/JiaoCJCMJ22","DOI":"10.48550/arXiv.2203.05203","CorpusId":247362768},"title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes"},{"paperId":"a9861b0c4fc76c96493e67f9398cb17e70a19e1a","externalIds":{"ArXiv":"2203.00843","DBLP":"conf/cvpr/YuanYLGLCL22","DOI":"10.1109/CVPR52688.2022.00837","CorpusId":247218430},"title":"X -Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning"},{"paperId":"a37bf9d65a4d1ebf95c79b3ab973bb7a8019ac3e","externalIds":{"DBLP":"journals/corr/abs-2201-12944","ArXiv":"2201.12944","DOI":"10.1145/3617592","CorpusId":246430542},"title":"Deep Learning Approaches on Image Captioning: A Review"},{"paperId":"e8a72d29771d1a33b4a0e43c74adcee6c73d74c7","externalIds":{"ArXiv":"2201.08264","DBLP":"conf/cvpr/SeoNAS22","DOI":"10.1109/CVPR52688.2022.01743","CorpusId":246063824},"title":"End-to-end Generative Pretraining for Multimodal Video Captioning"},{"paperId":"cbdd3e2fcfd85d546f3ab18f644434097baa7590","externalIds":{"ArXiv":"2111.13196","DBLP":"conf/cvpr/LinLL0G0LW22","DOI":"10.1109/CVPR52688.2022.01742","CorpusId":244709307},"title":"SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning"},{"paperId":"a7aa150b55d64d339b1c154d6d88455fc3cbc44f","externalIds":{"DBLP":"journals/corr/abs-2111-09734","ArXiv":"2111.09734","CorpusId":244346239},"title":"ClipCap: CLIP Prefix for Image Captioning"},{"paperId":"69ff4686b6517a0f9ae59503fedd8ed6e7be9983","externalIds":{"DBLP":"conf/iccv/ZhaoCS021","DOI":"10.1109/ICCV48922.2021.00292","CorpusId":244127479},"title":"3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds"},{"paperId":"03a2befad038a9f29859295fdfcdbfa52c564622","externalIds":{"ArXiv":"2109.08141","DBLP":"journals/corr/abs-2109-08141","DOI":"10.1109/ICCV48922.2021.00290","CorpusId":237532397},"title":"An End-to-End Transformer Model for 3D Object Detection"},{"paperId":"0ff31ef772a22cb208f0424e99d43be0226b7ded","externalIds":{"DBLP":"journals/tcsv/LiuZXNLZ22","MAG":"3195680250","DOI":"10.1109/tcsvt.2021.3107035","CorpusId":238646832},"title":"Region-Aware Image Captioning via Interaction Learning"},{"paperId":"76e64bb7cd283d448740dc1dafb9be69cc34765b","externalIds":{"DBLP":"conf/iccv/WangZLZC021","ArXiv":"2108.07781","DOI":"10.1109/ICCV48922.2021.00677","CorpusId":237142167},"title":"End-to-End Dense Video Captioning with Parallel Decoding"},{"paperId":"0a4e7b3b98c1eea83bf253cb0bcb99e19e31659b","externalIds":{"ArXiv":"2108.02388","DBLP":"conf/mm/HeZLHHZ021","DOI":"10.1145/3474085.3475397","CorpusId":236924276},"title":"TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding"},{"paperId":"bc320788232352b9b245b4e8271e492964cb1620","externalIds":{"DBLP":"conf/cvpr/DengCCH021","DOI":"10.1109/CVPR46437.2021.00030","CorpusId":235692559},"title":"Sketch, Ground, and Refine: Top-Down Dense Video Captioning"},{"paperId":"b784b2023593af98692b1d2063b53eca897512cd","externalIds":{"DBLP":"journals/corr/abs-2105-11450","ArXiv":"2105.11450","DOI":"10.1109/ICCV48922.2021.00187","CorpusId":235166799},"title":"SAT: 2D Semantics Assisted Training for 3D Visual Grounding"},{"paperId":"7ba268c6d5489dd3b3c08e3642f1385c6235118e","externalIds":{"DBLP":"conf/aaai/Hu0LZGW021","DOI":"10.1609/aaai.v35i2.16249","CorpusId":235306184},"title":"VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning"},{"paperId":"5f1913828e30c3070f32c154d2d142ec17e91189","externalIds":{"DBLP":"conf/aaai/HuangLCL21","DOI":"10.1609/aaai.v35i2.16253","CorpusId":235306096},"title":"Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation"},{"paperId":"27ed05b1354ebc2451653a5356702f8260c7f97b","externalIds":{"ArXiv":"2103.16381","DBLP":"journals/corr/abs-2103-16381","DOI":"10.1109/ICCV48922.2021.00370","CorpusId":232417286},"title":"Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud"},{"paperId":"988c00ee92890e0b26e3d31cafe40392259d6df6","externalIds":{"MAG":"3136792391","DBLP":"journals/tcsv/YanHLYLMCG22","DOI":"10.1109/TCSVT.2021.3067449","CorpusId":233689007},"title":"Task-Adaptive Attention for Image Captioning"},{"paperId":"cffe7b3c03ddd7f2e7403fffba110dd2e018241c","externalIds":{"DBLP":"journals/tcsv/Yu0YH021","MAG":"3027790991","DOI":"10.1109/TCSVT.2020.2995959","CorpusId":219465531},"title":"Long-Term Video Question Answering via Multimodal Hierarchical Memory Attentive Networks"},{"paperId":"91408ef7aa1c5278e01c2deaf681f2ee7e9343ff","externalIds":{"DBLP":"journals/corr/abs-2103-01128","ArXiv":"2103.01128","DOI":"10.1109/ICCV48922.2021.00181","CorpusId":232092539},"title":"InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","externalIds":{"DBLP":"conf/icml/RadfordKHRGASAM21","ArXiv":"2103.00020","CorpusId":231591445},"title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","externalIds":{"ArXiv":"2102.03334","DBLP":"journals/corr/abs-2102-03334","CorpusId":231839613},"title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"7a4ba78d377eea9650e5e399a0878e30bd22f648","externalIds":{"DBLP":"conf/cvpr/ChenGNC21","MAG":"3111353235","ArXiv":"2012.02206","DOI":"10.1109/CVPR46437.2021.00321","CorpusId":227305513},"title":"Scan2Cap: Context-aware Dense Captioning in RGB-D Scans"},{"paperId":"20288ff29a8d292af10c1509b541de2696953a45","externalIds":{"MAG":"3090496396","ArXiv":"2010.01733","DBLP":"journals/corr/abs-2010-01733","CorpusId":222134007},"title":"D3Net: Densely connected multidilated DenseNet for music source separation"},{"paperId":"53794499a3830c3ebb365ecc57f0e8c8a20a682d","externalIds":{"MAG":"3095974555","DBLP":"conf/eccv/AchlioptasAXEG20","DOI":"10.1007/978-3-030-58452-8_25","CorpusId":221378802},"title":"ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes"},{"paperId":"ec7cdb1ab0a9a4aefbee9e10c9c011d4cb764416","externalIds":{"DBLP":"journals/tcsv/WangZYTH21","MAG":"3047922786","DOI":"10.1109/TCSVT.2020.3014606","CorpusId":226645784},"title":"Event-Centric Hierarchical Representation for Dense Video Captioning"},{"paperId":"5c126ae3421f05768d8edd97ecd44b1364e2c99a","externalIds":{"DBLP":"conf/nips/HoJA20","MAG":"3100572490","ArXiv":"2006.11239","CorpusId":219955663},"title":"Denoising Diffusion Probabilistic Models"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","externalIds":{"DBLP":"journals/corr/abs-2006-06195","MAG":"3102995547","ArXiv":"2006.06195","CorpusId":219573512},"title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"ed29d4630505cac194defb0d6e1c71960cef2573","externalIds":{"ArXiv":"2005.08271","MAG":"3025796084","DBLP":"conf/bmvc/IashinR20","DOI":"10.5244/c.34.29","CorpusId":218674428},"title":"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer"},{"paperId":"b5ef0f91663f0cbd6910dec9a890c138f7ec10e0","externalIds":{"DBLP":"journals/corr/abs-2004-06165","MAG":"3091588028","ArXiv":"2004.06165","DOI":"10.1007/978-3-030-58577-8_8","CorpusId":215754208},"title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"9712cc6fc96f463842f9d41c565e3a8781bfba40","externalIds":{"MAG":"3014878043","DBLP":"conf/cvpr/JiangZS0FJ20","ArXiv":"2004.01658","DOI":"10.1109/cvpr42600.2020.00492","CorpusId":214795000},"title":"PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation"},{"paperId":"4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0","externalIds":{"MAG":"3014966943","DBLP":"conf/cvpr/PanYLM20","ArXiv":"2003.14080","DOI":"10.1109/cvpr42600.2020.01098","CorpusId":214727638},"title":"X-Linear Attention Networks for Image Captioning"},{"paperId":"3ba94f4dd7db8c697401aa54e63ad318423fc83d","externalIds":{"ArXiv":"2003.07758","MAG":"3034815696","DBLP":"journals/corr/abs-2003-07758","DOI":"10.1109/CVPRW50498.2020.00487","CorpusId":212737076},"title":"Multi-modal Dense Video Captioning"},{"paperId":"8b2c80788f789d4ce7849c13943fa920d9e3c95f","externalIds":{"DBLP":"journals/corr/abs-2002-08565","MAG":"3007456797","ArXiv":"2002.08565","DOI":"10.1007/978-3-030-58520-4_25","CorpusId":211204968},"title":"Captioning Images Taken by People Who Are Blind"},{"paperId":"aa58b78207bd248dbd66b912863850ca67d5fd5c","externalIds":{"MAG":"3006320872","ArXiv":"2002.06353","DBLP":"journals/corr/abs-2002-06353","CorpusId":211132410},"title":"UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation"},{"paperId":"85079d64d6fdd0ba5318fda119d152f2d2946391","externalIds":{"DBLP":"journals/corr/abs-2001-05876","MAG":"2998988444","ArXiv":"2001.05876","DOI":"10.1609/AAAI.V34I07.6898","CorpusId":210702035},"title":"Show, Recall, and Tell: Image Captioning with Recall Mechanism"},{"paperId":"6c161841cdb547f77930942e4ab46f4369751676","externalIds":{"DBLP":"journals/corr/abs-1912-08830","ArXiv":"1912.08830","MAG":"2995439012","DOI":"10.1007/978-3-030-58565-5_13","CorpusId":209414687},"title":"ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language"},{"paperId":"fc9c52f55ffe0e860b1bb4222fe86cce60c05551","externalIds":{"DBLP":"journals/corr/abs-1912-08226","MAG":"2995753487","ArXiv":"1912.08226","DOI":"10.1109/cvpr42600.2020.01059","CorpusId":219635470},"title":"Meshed-Memory Transformer for Image Captioning"},{"paperId":"cfd58b384650fb07054353ab6067c503fce5359c","externalIds":{"MAG":"2981646256","DBLP":"conf/mm/SongWCJ19","DOI":"10.1145/3343031.3350913","CorpusId":204836964},"title":"MUCH: Mutual Coupling Enhancement of Scene Recognition and Dense Captioning"},{"paperId":"6648b4db5f12c30941ea78c695e77aded19672bb","externalIds":{"MAG":"2997591391","ArXiv":"1909.11059","DBLP":"journals/corr/abs-1909-11059","DOI":"10.1609/AAAI.V34I07.7005","CorpusId":202734445},"title":"Unified Vision-Language Pre-Training for Image Captioning and VQA"},{"paperId":"fcf74c4a3042eca2994aac122f1ed045be098902","externalIds":{"MAG":"2973642807","ArXiv":"1909.09060","DBLP":"journals/corr/abs-1909-09060","CorpusId":202677610},"title":"Adaptively Aligned Image Captioning via Adaptive Attention Time"},{"paperId":"4c163d4942117179d3e97182e1b280027d7d60a9","externalIds":{"DBLP":"conf/iccv/HuangWCW19","MAG":"2968549119","ArXiv":"1908.06954","DOI":"10.1109/ICCV.2019.00473","CorpusId":201070367},"title":"Attention on Attention for Image Captioning"},{"paperId":"2bc1c8bd00bbf7401afcb5460277840fd8bab029","externalIds":{"ArXiv":"1908.06066","DBLP":"journals/corr/abs-1908-06066","MAG":"2998356391","DOI":"10.1609/AAAI.V34I07.6795","CorpusId":201058752},"title":"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"},{"paperId":"b499228aa74b59be32711c3926e44de208d6b636","externalIds":{"MAG":"2971310675","DBLP":"conf/nips/HerdadeKBS19","ArXiv":"1906.05963","CorpusId":189898359},"title":"Image Captioning: Transforming Objects into Words"},{"paperId":"99b5153235b0ee583803bbd7cd6bd9da161d5348","externalIds":{"MAG":"2981165461","ArXiv":"1905.07841","DBLP":"journals/tcsv/YuLYH20","DOI":"10.1109/TCSVT.2019.2947482","CorpusId":159041705},"title":"Multimodal Transformer With Multi-View Visual Representation for Image Captioning"},{"paperId":"1ba1191d88a804d5224a224a2a2a4fa3a253b946","externalIds":{"MAG":"3012397227","DBLP":"conf/cvpr/HouDN20","ArXiv":"1904.12012","DOI":"10.1109/cvpr42600.2020.00217","CorpusId":212675091},"title":"RevealNet: Seeing Behind Objects in RGB-D Scans"},{"paperId":"e60da8d3a79801a3ccbf1abcdd001bb6e001b267","externalIds":{"DBLP":"journals/corr/abs-1904-09664","MAG":"2988715931","ArXiv":"1904.09664","DOI":"10.1109/ICCV.2019.00937","CorpusId":127956465},"title":"Deep Hough Voting for 3D Object Detection in Point Clouds"},{"paperId":"c5a757427132fda0c66e18a0d059eca8e2472d13","externalIds":{"MAG":"2927018329","ArXiv":"1904.03870","DBLP":"journals/corr/abs-1904-03870","DOI":"10.1109/CVPR.2019.00675","CorpusId":102351043},"title":"Streamlined Dense Video Captioning"},{"paperId":"eba62fe8050e475ffe533b9f70db538074d8d0d1","externalIds":{"MAG":"2927788011","DBLP":"conf/cvpr/YinSLYWS19","ArXiv":"1904.01410","DOI":"10.1109/CVPR.2019.00640","CorpusId":91184088},"title":"Context and Attribute Grounded Dense Captioning"},{"paperId":"e2751a898867ce6687e08a5cc7bdb562e999b841","externalIds":{"ArXiv":"1904.01355","DBLP":"journals/corr/abs-1904-01355","MAG":"2925359305","DOI":"10.1109/ICCV.2019.00972","CorpusId":91184137},"title":"FCOS: Fully Convolutional One-Stage Object Detection"},{"paperId":"740db108d536a8b6f53111407898f4ecaecf80ea","externalIds":{"ArXiv":"1904.00560","MAG":"2928594414","DBLP":"journals/corr/abs-1904-00560","DOI":"10.1109/CVPR.2019.00207","CorpusId":90259087},"title":"Scene Graph Generation With External Knowledge and Image Reconstruction"},{"paperId":"79de42d6ca8d1bf2952c46eb74e6e0561f979257","externalIds":{"ArXiv":"1903.05942","MAG":"2976658881","DBLP":"conf/cvpr/0003COK19","DOI":"10.1109/CVPR.2019.00643","CorpusId":76665989},"title":"Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning"},{"paperId":"f6feb1af1809dfd872d868dfcc13021cc42f496c","externalIds":{"MAG":"2902243109","DBLP":"journals/corr/abs-1812-02378","ArXiv":"1812.02378","DOI":"10.1109/CVPR.2019.01094","CorpusId":54460890},"title":"Auto-Encoding Scene Graphs for Image Captioning"},{"paperId":"7e27d44e3fac723ccb703e0a83b22711bd42efe8","externalIds":{"MAG":"2896348597","ArXiv":"1810.04020","DBLP":"journals/csur/HossainSSL19","DOI":"10.1145/3295748","CorpusId":52947736},"title":"A Comprehensive Survey of Deep Learning for Image Captioning"},{"paperId":"0000fcfd467a19cf0e59169c2f07d730a0f3a8b9","externalIds":{"MAG":"2890531016","ArXiv":"1809.07041","DBLP":"conf/eccv/YaoPLM18","DOI":"10.1007/978-3-030-01264-9_42","CorpusId":52304560},"title":"Exploring Visual Relationship for Image Captioning"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","externalIds":{"MAG":"2886641317","ACL":"P18-1238","DBLP":"conf/acl/SoricutDSG18","DOI":"10.18653/v1/P18-1238","CorpusId":51876975},"title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"19d7f83c3d7147f0eed1e1471438066eb4fe51fb","externalIds":{"MAG":"2963753226","ArXiv":"1804.08274","DBLP":"journals/corr/abs-1804-08274","DOI":"10.1109/CVPR.2018.00782","CorpusId":5046218},"title":"Jointly Localizing and Describing Events for Dense Video Captioning"},{"paperId":"35ed258aede3df17ee20a6635364cb5fd2461049","externalIds":{"ArXiv":"1804.00819","DBLP":"conf/cvpr/ZhouZCSX18","MAG":"2949624860","DOI":"10.1109/CVPR.2018.00911","CorpusId":4564155},"title":"End-to-End Dense Video Captioning with Masked Transformer"},{"paperId":"bb4e2d6a6e3e1067f21a4cad087fc91c671e495d","externalIds":{"MAG":"2962799512","ArXiv":"1804.00100","DBLP":"journals/corr/abs-1804-00100","DOI":"10.1109/CVPR.2018.00751","CorpusId":4621662},"title":"Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning"},{"paperId":"7c1802d8d43dfe783650a03f03d41609fa5ae91e","externalIds":{"ArXiv":"1803.04376","MAG":"2790600496","DBLP":"conf/cvpr/LuoPCS18","DOI":"10.1109/CVPR.2018.00728","CorpusId":3875506},"title":"Discriminability Objective for Training Descriptive Captions"},{"paperId":"e1799aaf23c12af6932dc0ef3dfb1638f01413d1","externalIds":{"DBLP":"journals/tog/WangSLSBS19","MAG":"2785053089","ArXiv":"1801.07829","DOI":"10.1145/3326362","CorpusId":94822},"title":"Dynamic Graph CNN for Learning on Point Clouds"},{"paperId":"799537fa855caf53a6a3a7cf20301a81e90da127","externalIds":{"MAG":"2950045970","DBLP":"conf/nips/SchwartzSH17","ArXiv":"1711.04323","CorpusId":26356195},"title":"High-Order Attention Models for Visual Question Answering"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","externalIds":{"MAG":"2745461083","DBLP":"conf/cvpr/00010BT0GZ18","ArXiv":"1707.07998","DOI":"10.1109/CVPR.2018.00636","CorpusId":3753452},"title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"561ed7e47524fb3218e6a38f41cd877a9c33d3b9","externalIds":{"MAG":"2625940279","DBLP":"conf/cvpr/GanGHGD17","DOI":"10.1109/CVPR.2017.108","CorpusId":23414983},"title":"StyleNet: Generating Attractive Visual Captions with Styles"},{"paperId":"51b2c1e750b1d3b893072829d012f2352d6bd373","externalIds":{"MAG":"2739107216","DBLP":"journals/tmm/GaoGZXS17","DOI":"10.1109/TMM.2017.2729019","CorpusId":25497516},"title":"Video Captioning With Attention-Based LSTM and Semantic Consistency"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"8674494bd7a076286b905912d26d47f7501c4046","externalIds":{"DBLP":"conf/nips/QiYSG17","MAG":"2950697424","ArXiv":"1706.02413","CorpusId":1745976},"title":"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"},{"paperId":"96dd1fc39a368d23291816d57763bc6eb4f7b8d6","externalIds":{"MAG":"2963916161","DBLP":"journals/corr/KrishnaHRLN17","ArXiv":"1705.00754","DOI":"10.1109/ICCV.2017.83","CorpusId":1026139},"title":"Dense-Captioning Events in Videos"},{"paperId":"f94841ec597dcf6d1c23e7f40ba35e121f6a81c1","externalIds":{"DBLP":"conf/iccv/GaoYSCN17","ArXiv":"1703.06189","MAG":"2597958930","DOI":"10.1109/ICCV.2017.392","CorpusId":3060010},"title":"TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals"},{"paperId":"5f56b1043c59727ebac5b6f7c31b5c30a0b84a6f","externalIds":{"MAG":"2588822708","DBLP":"conf/cscw/WuWFS17","DOI":"10.1145/2998181.2998364","CorpusId":10857293},"title":"Automatic Alt-text: Computer-generated Image Descriptions for Blind Users on a Social Network Service"},{"paperId":"e52e37cd91366f07df1f98e88f87010f494dd16e","externalIds":{"DBLP":"conf/cvpr/DaiCSHFN17","MAG":"2594519801","ArXiv":"1702.04405","DOI":"10.1109/CVPR.2017.261","CorpusId":7684883},"title":"ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"},{"paperId":"34b73c1aa158b892bbe41705b4ae5bf01ecaea86","externalIds":{"ArXiv":"1701.02426","DBLP":"journals/corr/XuZCF17","MAG":"2579549467","DOI":"10.1109/CVPR.2017.330","CorpusId":1780254},"title":"Scene Graph Generation by Iterative Message Passing"},{"paperId":"9f4d7d622d1f7319cc511bfef661cd973e881a4c","externalIds":{"DBLP":"journals/corr/LuXPS16","MAG":"2952469094","ArXiv":"1612.01887","DOI":"10.1109/CVPR.2017.345","CorpusId":18347865},"title":"Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning"},{"paperId":"6c8353697cdbb98dfba4f493875778c4286d3e3a","externalIds":{"DBLP":"conf/cvpr/RennieMMRG17","MAG":"2963084599","ArXiv":"1612.00563","DOI":"10.1109/CVPR.2017.131","CorpusId":206594923},"title":"Self-Critical Sequence Training for Image Captioning"},{"paperId":"21fa67345e49642b8ebb22a59c4b2799a56e996f","externalIds":{"DBLP":"conf/cvpr/YangTYL17","MAG":"2950371778","ArXiv":"1611.06949","DOI":"10.1109/CVPR.2017.214","CorpusId":1189091},"title":"Dense Captioning with Joint Inference and Visual Context"},{"paperId":"b8ecc8512ff231d6e90f7373584ef9d7e658dbd4","externalIds":{"MAG":"2563685048","DBLP":"conf/3dim/HuaPNTYY16","DOI":"10.1109/3DV.2016.18","CorpusId":11245438},"title":"SceneNN: A Scene Meshes Dataset with aNNotations"},{"paperId":"36eff562f65125511b5dfab68ce7f7a943c27478","externalIds":{"ArXiv":"1609.02907","MAG":"2519887557","DBLP":"journals/corr/KipfW16","CorpusId":3144218},"title":"Semi-Supervised Classification with Graph Convolutional Networks"},{"paperId":"5694e46284460a648fe29117cbc55f6c9be3fa3c","externalIds":{"MAG":"2963446712","ArXiv":"1608.06993","DBLP":"journals/corr/HuangLW16a","DOI":"10.1109/CVPR.2017.243","CorpusId":9433631},"title":"Densely Connected Convolutional Networks"},{"paperId":"f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f","externalIds":{"DBLP":"journals/corr/AndersonFJG16","MAG":"2950201573","ArXiv":"1607.08822","DOI":"10.1007/978-3-319-46454-1_24","CorpusId":11933981},"title":"SPICE: Semantic Propositional Image Caption Evaluation"},{"paperId":"dc3f8c8513441915408ab0549e9ac5f2f2f31eec","externalIds":{"MAG":"2460657278","DBLP":"conf/cvpr/ArmeniSZJBFS16","DOI":"10.1109/CVPR.2016.170","CorpusId":9649070},"title":"3D Semantic Parsing of Large-Scale Indoor Spaces"},{"paperId":"daf74c34f7da0695b154f645c8b78a7397a98f16","externalIds":{"DBLP":"journals/corr/PaszkeCKC16","ArXiv":"1606.02147","MAG":"2419448466","CorpusId":16231053},"title":"ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation"},{"paperId":"bac994dda1385cd709e08e24170c711d8c573676","externalIds":{"MAG":"2471143248","DBLP":"conf/cvpr/HeilbronNG16","DOI":"10.1109/CVPR.2016.211","CorpusId":827570},"title":"Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","externalIds":{"DBLP":"journals/corr/KrishnaZGJHKCKL16","MAG":"2277195237","ArXiv":"1602.07332","DOI":"10.1007/s11263-016-0981-7","CorpusId":4492210},"title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","externalIds":{"DBLP":"conf/cvpr/HeZRS16","MAG":"2949650786","ArXiv":"1512.03385","DOI":"10.1109/cvpr.2016.90","CorpusId":206594692},"title":"Deep Residual Learning for Image Recognition"},{"paperId":"9b686d76914befea66377ec79c1f9258d70ea7e3","externalIds":{"MAG":"2190691619","ArXiv":"1512.03012","DBLP":"journals/corr/ChangFGHHLSSSSX15","CorpusId":2554264},"title":"ShapeNet: An Information-Rich 3D Model Repository"},{"paperId":"d7ce5665a72c0b607f484c1b448875f02ddfac3b","externalIds":{"DBLP":"journals/corr/JohnsonKL15","MAG":"2254252455","ArXiv":"1511.07571","DOI":"10.1109/CVPR.2016.494","CorpusId":14521054},"title":"DenseCap: Fully Convolutional Localization Networks for Dense Captioning"},{"paperId":"dc0d60f6e8bb7a04ba8a77d51d10b6dba8117480","externalIds":{"ArXiv":"1511.02300","DBLP":"journals/corr/SongX15","MAG":"2949768986","DOI":"10.1109/CVPR.2016.94","CorpusId":206594775},"title":"Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images"},{"paperId":"7da9c26ea68a31d119e8222d1a5c33ef136ebed8","externalIds":{"MAG":"2963062932","DBLP":"journals/corr/MathewsXH15","ArXiv":"1510.01431","DOI":"10.1609/aaai.v30i1.10475","CorpusId":2875390},"title":"SentiCap: Generating Image Descriptions with Sentiments"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","externalIds":{"MAG":"2953106684","ArXiv":"1506.01497","DBLP":"journals/pami/RenHG017","DOI":"10.1109/TPAMI.2016.2577031","CorpusId":10328909,"PubMed":"27295650"},"title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","externalIds":{"MAG":"1901129140","DBLP":"journals/corr/RonnebergerFB15","ArXiv":"1505.04597","DOI":"10.1007/978-3-319-24574-4_28","CorpusId":3719281},"title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"7ffdbc358b63378f07311e883dddacc9faeeaf4b","externalIds":{"ArXiv":"1504.08083","CorpusId":206770307},"title":"Fast R-CNN"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","externalIds":{"ArXiv":"1503.02531","MAG":"1821462560","DBLP":"journals/corr/HintonVD15","CorpusId":7200347},"title":"Distilling the Knowledge in a Neural Network"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","externalIds":{"MAG":"2950178297","DBLP":"conf/icml/XuBKCCSZB15","ArXiv":"1502.03044","CorpusId":1055111},"title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"ac3ee98020251797c2b401e1389461df88e52e62","externalIds":{"MAG":"1924770834","DBLP":"journals/corr/ChungGCB14","ArXiv":"1412.3555","CorpusId":5201925},"title":"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"},{"paperId":"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","externalIds":{"ArXiv":"1412.2306","DBLP":"journals/corr/KarpathyF14","MAG":"1905882502","DOI":"10.1109/CVPR.2015.7298932","CorpusId":8517067},"title":"Deep visual-semantic alignments for generating image descriptions"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","externalIds":{"DBLP":"journals/corr/VedantamZP14a","MAG":"2952574180","ArXiv":"1411.5726","DOI":"10.1109/CVPR.2015.7299087","CorpusId":9026666},"title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","externalIds":{"MAG":"1895577753","DBLP":"journals/corr/VinyalsTBE14","ArXiv":"1411.4555","DOI":"10.1109/CVPR.2015.7298935","CorpusId":1169492},"title":"Show and tell: A neural image caption generator"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","externalIds":{"DBLP":"conf/emnlp/PenningtonSM14","ACL":"D14-1162","MAG":"2250539671","DOI":"10.3115/v1/D14-1162","CorpusId":1957433},"title":"GloVe: Global Vectors for Word Representation"},{"paperId":"7c8a51d04522496c43db68f2582efd45eaf59fea","externalIds":{"MAG":"1920022804","DBLP":"conf/cvpr/WuSKYZTX15","DOI":"10.1109/CVPR.2015.7298801","CorpusId":206592833},"title":"3D ShapeNets: A deep representation for volumetric shapes"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","externalIds":{"MAG":"2950635152","DBLP":"conf/emnlp/ChoMGBBSB14","ACL":"D14-1179","ArXiv":"1406.1078","DOI":"10.3115/v1/D14-1179","CorpusId":5590763},"title":"Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","externalIds":{"ArXiv":"1405.0312","DBLP":"conf/eccv/LinMBHPRDZ14","MAG":"2952122856","DOI":"10.1007/978-3-319-10602-1_48","CorpusId":14113767},"title":"Microsoft COCO: Common Objects in Context"},{"paperId":"44040913380206991b1991daf1192942e038fe31","externalIds":{"ACL":"Q14-1006","DBLP":"journals/tacl/YoungLHH14","MAG":"2185175083","DOI":"10.1162/tacl_a_00166","CorpusId":3104920},"title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"723e52c0d0140df7a6e264f1042af89ce9277e6e","externalIds":{"MAG":"1985238052","DBLP":"conf/iccv/XiaoOT13","DOI":"10.1109/ICCV.2013.458","CorpusId":6033252},"title":"SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels"},{"paperId":"00f2a152454db273b0d6831b6550beb37a135890","externalIds":{"DBLP":"conf/cvpr/ShottonGZICF13","MAG":"1989476314","DOI":"10.1109/CVPR.2013.377","CorpusId":8632684},"title":"Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images"},{"paperId":"5eb4c55740165defacf08329beaae5314d7fbfe6","externalIds":{"MAG":"2021851106","DBLP":"conf/iros/SturmEEBC12","DOI":"10.1109/IROS.2012.6385773","CorpusId":206942855},"title":"A benchmark for the evaluation of RGB-D SLAM systems"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","externalIds":{"DBLP":"conf/nips/KrizhevskySH12","MAG":"2618530766","DOI":"10.1145/3065386","CorpusId":195908774},"title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"c1994ba5946456fc70948c549daf62363f13fa2d","externalIds":{"MAG":"125693051","DBLP":"conf/eccv/SilbermanHKF12","DOI":"10.1007/978-3-642-33715-4_54","CorpusId":545361},"title":"Indoor Segmentation and Support Inference from RGBD Images"},{"paperId":"eaaed23a2d94feb2f1c3ff22a25777c7a78f3141","externalIds":{"DBLP":"conf/eccv/FarhadiHSYRHF10","MAG":"1897761818","DOI":"10.1007/978-3-642-15561-1_2","CorpusId":13272863},"title":"Every Picture Tells a Story: Generating Sentences from Images"},{"paperId":"c6f789ced24234a37d4fbca0201842807c262a4d","externalIds":{"MAG":"2056621158","DOI":"10.1167/7.1.10","CorpusId":6452122,"PubMed":"17461678"},"title":"What do we perceive in a glance of a real-world scene?"},{"paperId":"7533d30329cfdbf04ee8ee82bfef792d08015ee5","externalIds":{"MAG":"2123301721","ACL":"W05-0909","DBLP":"conf/acl/BanerjeeL05","CorpusId":7164502},"title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"8397ab573dd6c97a39ff4feb9c2d9b3c1e16c705","externalIds":{"DBLP":"journals/jd/Robertson04","MAG":"2024932032","DOI":"10.1108/00220410410560582","CorpusId":8864928},"title":"Understanding inverse document frequency: on theoretical arguments for IDF"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","externalIds":{"MAG":"2154652894","ACL":"W04-1013","CorpusId":964287},"title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"74d2ad28be32a5802a1b15d4e9a430db2234a3dd","externalIds":{"ACL":"P04-1077","MAG":"2108325777","DBLP":"conf/acl/LinO04","DOI":"10.3115/1218955.1219032","CorpusId":1586456},"title":"Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","externalIds":{"DBLP":"conf/acl/PapineniRWZ02","MAG":"2101105183","ACL":"P02-1040","DOI":"10.3115/1073083.1073135","CorpusId":11080756},"title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","externalIds":{"DBLP":"journals/neco/HochreiterS97","MAG":"2064675550","DOI":"10.1162/neco.1997.9.8.1735","CorpusId":1915014,"PubMed":"9377276"},"title":"Long Short-Term Memory"},{"paperId":"e1590861002466864a6a5bf51ee8657222d078ff","externalIds":{"DBLP":"journals/corr/abs-2210-12513","DOI":"10.48550/arXiv.2210.12513","CorpusId":253098951},"title":"HAM: Hierarchical Attention Model with High Performance for 3D Visual Grounding"},{"paperId":"ef33aedbab68b0771f6dd7ca8ec2492f12d7ea51","externalIds":{"MAG":"2974161034","DBLP":"journals/tip/YuYYT20","DOI":"10.1109/TIP.2019.2940677","CorpusId":202690153,"PubMed":"31535995"},"title":"Compositional Attention Networks With Two-Stream Fusion for Video Question Answering"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","externalIds":{"MAG":"2951055169","ACL":"N19-1423","DBLP":"journals/corr/abs-1810-04805","ArXiv":"1810.04805","DOI":"10.18653/v1/N19-1423","CorpusId":52967399},"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8b55402ffee2734bfc7d5d7595500916e1ef04e8","externalIds":{"MAG":"2904565150","DBLP":"conf/iccv/AgrawalAD0CJ0BP19","ArXiv":"1812.08658","DOI":"10.1109/ICCV.2019.00904","CorpusId":56517630},"title":"nocaps: novel object captioning at scale"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","externalIds":{"MAG":"2955855238","CorpusId":160025533},"title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"684d0400b5747008346ac4db3561b805375c4248","externalIds":{"CorpusId":28567229,"PubMed":"10680421"},"title":"Computing & technology."}]}