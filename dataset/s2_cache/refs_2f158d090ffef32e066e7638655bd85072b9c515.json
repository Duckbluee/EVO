{"references":[{"paperId":"c02857ef7ef0a260c372f035de10d7291747bae5","externalIds":{"DBLP":"journals/tnn/HuangSDL24","DOI":"10.1109/TNNLS.2024.3416328","CorpusId":271062044,"PubMed":"38976472"},"title":"Learning Joint 2-D and 3-D Graph Diffusion Models for Complete Molecule Generation"},{"paperId":"271353ff70098ceaf6757240a6805b1192fe57da","externalIds":{"DBLP":"journals/tkde/HeLZFTDAV24","DOI":"10.1109/TKDE.2023.3327777","CorpusId":265007159},"title":"Focused Contrastive Loss for Classification With Pre-Trained Language Models"},{"paperId":"a1bbe6a765a369cf220174d71e0567d0124ef72e","externalIds":{"DBLP":"journals/tnn/WangZLJLY25","ArXiv":"2405.19779","DOI":"10.1109/TNNLS.2024.3440269","CorpusId":270123540,"PubMed":"39288035"},"title":"Automatic Graph Topology-Aware Transformer"},{"paperId":"685283fce76d53b55f1e0d54168c634d66031e26","externalIds":{"DBLP":"journals/tkde/LinRLW24","DOI":"10.1109/TKDE.2023.3304385","CorpusId":260853541},"title":"A Survey on Neural Data-to-Text Generation"},{"paperId":"8cfaad5d0fc52e24acb75f0f80bd1d482671a553","externalIds":{"PubMedCentral":"11258334","DOI":"10.1038/s41467-024-45566-8","CorpusId":268029376,"PubMed":"38409255"},"title":"Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting"},{"paperId":"e390105f53d6f7b68f57fefbbd22e2374855d4c5","externalIds":{"DOI":"10.1016/j.dajour.2024.100417","CorpusId":267530476},"title":"Integrating sentiment analysis with graph neural networks for enhanced stock prediction: A comprehensive survey"},{"paperId":"37db151549206bfecfd82898caab6a8fe0253db2","externalIds":{"DBLP":"journals/corr/abs-2402-03358","ArXiv":"2402.03358","DOI":"10.48550/arXiv.2402.03358","CorpusId":267341687},"title":"A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation"},{"paperId":"5873075c75d90bb3f2275ee689102efad27fdc35","externalIds":{"ArXiv":"2401.11631","DBLP":"journals/corr/abs-2401-11631","DOI":"10.48550/arXiv.2401.11631","CorpusId":267068435},"title":"Text-to-Image Cross-Modal Generation: A Systematic Review"},{"paperId":"2422c75795ffadfed339841260e8118df454ec4a","externalIds":{"DBLP":"journals/tnn/GuoGZBCJ24","DOI":"10.1109/TNNLS.2023.3336894","CorpusId":265707419,"PubMed":"38051606"},"title":"Object Counting via Group and Graph Attention Network"},{"paperId":"59116a07dbdb3cdeebb20085fdfde8b899de8f6a","externalIds":{"PubMedCentral":"10731821","DBLP":"journals/bmcbi/DjeddiHYD23","DOI":"10.1186/s12859-023-05593-6","CorpusId":266364142,"PubMed":"38114937"},"title":"Advancing drug–target interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining"},{"paperId":"c806bf7395c14a237588dd443a6ae9aa1a39bbf3","externalIds":{"DBLP":"journals/mta/YusufFMDAC24","DOI":"10.1007/s11042-023-17594-x","CorpusId":265261855},"title":"Graph neural networks for visual question answering: a systematic review"},{"paperId":"ddcc5bdd28355652e29393ee76d17601e5983b3f","externalIds":{"ArXiv":"2310.08008","CorpusId":266203535},"title":"Effects of Human Adversarial and Affable Samples on BERT Generalization"},{"paperId":"644afd030021f67bb379b91b9d2b1b177f1161e3","externalIds":{"DBLP":"journals/eswa/NamJ24","DOI":"10.1016/j.eswa.2023.121168","CorpusId":260876677},"title":"A survey on multimodal bidirectional machine learning translation of image and natural language processing"},{"paperId":"11ffa913be481b452495957aa6c191d0c178a1b0","externalIds":{"DBLP":"conf/cikm/ShiWGSSC23a","ArXiv":"2307.11341","DOI":"10.1145/3583780.3615127","CorpusId":260091620},"title":"OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning"},{"paperId":"41ea36bc2d796408dc9aa68d1c60ee371efbbd69","externalIds":{"DBLP":"conf/sigir/WeiLLWNC23","DOI":"10.1145/3539618.3591716","CorpusId":259949740},"title":"LightGT: A Light Graph Transformer for Multimedia Recommendation"},{"paperId":"d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9","externalIds":{"ArXiv":"2307.03759","DBLP":"journals/corr/abs-2307-03759","DOI":"10.1109/TPAMI.2024.3443141","CorpusId":259501265,"PubMed":"39141471"},"title":"A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection"},{"paperId":"40812911e56a53e7a22333247b217b7e7f2a5fca","externalIds":{"DOI":"10.1016/j.ces.2023.119057","CorpusId":259378681},"title":"A simple and efficient graph Transformer architecture for molecular properties prediction"},{"paperId":"65577c96b81d22eb1b2ede6a6949f216b59056de","externalIds":{"PubMedCentral":"10338137","DBLP":"journals/bioinformatics/Gu0CDL23","DOI":"10.1093/bioinformatics/btad410","CorpusId":259271497,"PubMed":"37369035"},"title":"Hierarchical graph transformer with contrastive learning for protein function prediction"},{"paperId":"01de6d0c00e7e77050a90945246b2b4acde497a2","externalIds":{"DBLP":"journals/corr/abs-2306-08385","ArXiv":"2306.08385","DOI":"10.48550/arXiv.2306.08385","CorpusId":258509408},"title":"NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification"},{"paperId":"e49cdb15413f236e937d215684346c5dbab03bd2","externalIds":{"DBLP":"journals/corr/abs-2306-07114","ArXiv":"2306.07114","DOI":"10.1109/TETC.2023.3280577","CorpusId":259057503},"title":"Coupled Attention Networks for Multivariate Time Series Anomaly Detection"},{"paperId":"cf9f4575087d162b1c82f22769e5868fd6843619","externalIds":{"DBLP":"conf/kdd/TanGDL23","ArXiv":"2306.06063","DOI":"10.1145/3580305.3599541","CorpusId":259129743},"title":"Virtual Node Tuning for Few-shot Node Classification"},{"paperId":"ee1c3275182cfbc6e393862f6d0784ac977ecd99","externalIds":{"DBLP":"journals/nca/KumarS23a","DOI":"10.1007/s00521-023-08687-7","CorpusId":259041195},"title":"An abstractive text summarization technique using transformer model with self-attention mechanism"},{"paperId":"d2f8d3bd5cdddf1f3d607714f21deaab019a87cb","externalIds":{"ArXiv":"2305.17589","DBLP":"conf/icml/Ma0LRDCTL23","DOI":"10.48550/arXiv.2305.17589","CorpusId":258960285},"title":"Graph Inductive Biases in Transformers without Message Passing"},{"paperId":"e0bc91243e4e446f6b8871b4fc40b4a413f93c73","externalIds":{"DBLP":"conf/aaai/GuiYX24","ArXiv":"2305.10329","DOI":"10.48550/arXiv.2305.10329","CorpusId":258740838},"title":"G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks"},{"paperId":"234e75c55a315ea527112fe77fe0db5de17759ba","externalIds":{"DBLP":"journals/corr/abs-2305-07521","ArXiv":"2305.07521","DOI":"10.48550/arXiv.2305.07521","CorpusId":258676501},"title":"AGFormer: Efficient Graph Representation with Anchor-Graph Transformer"},{"paperId":"bb624a2a88a3c960d5b59360baf9a8259cbcfa81","externalIds":{"DBLP":"conf/ijcai/ZhuWSMW23","ArXiv":"2305.02866","DOI":"10.48550/arXiv.2305.02866","CorpusId":258480381},"title":"Hierarchical Transformer for Scalable Graph Learning"},{"paperId":"09347fec99884058e4b86fae49319a8b8022d3d0","externalIds":{"ACL":"2023.acl-long.501","ArXiv":"2305.02549","DBLP":"journals/corr/abs-2305-02549","DOI":"10.48550/arXiv.2305.02549","CorpusId":258480094},"title":"FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction"},{"paperId":"a8d559a4f4ac9da3e6bc308df02c5928f20d57d1","externalIds":{"ACL":"2023.acl-long.694","DBLP":"conf/acl/YangPWXYLHHLZ23","ArXiv":"2305.02177","DOI":"10.18653/v1/2023.acl-long.694","CorpusId":258461558},"title":"Transforming Visual Scene Graphs to Image Captions"},{"paperId":"0751d3980b13802bd8beb5586d1cf14d86306e97","externalIds":{"DBLP":"conf/www/MaCWSWZ23","DOI":"10.1145/3543507.3583464","CorpusId":258333811},"title":"Rethinking Structural Encodings: Adaptive Graph Transformer for Node Classification Task"},{"paperId":"67ae3f41ce20b352d2be461bbc1984467b870adf","externalIds":{"DBLP":"conf/www/WanF00XY023","DOI":"10.1145/3543507.3583441","CorpusId":258333782},"title":"Self-Supervised Teaching and Learning of Representations on Graphs"},{"paperId":"bb18b4e30eddd16f3dbf0a7a359fdac0932fe636","externalIds":{"DBLP":"journals/bioinformatics/ChenMLC23","PubMedCentral":"10311325","DOI":"10.1093/bioinformatics/btad203","CorpusId":256599490,"PubMed":"37387159"},"title":"A gated graph transformer for protein complex structure quality assessment and its performance in CASP15"},{"paperId":"29052ddd048acb1afa2c42613068b63bb7428a34","externalIds":{"DBLP":"journals/tnn/LiSHCQ24","DOI":"10.1109/TNNLS.2023.3262937","CorpusId":257954021,"PubMed":"37018088"},"title":"Position-Aware Relational Transformer for Knowledge Graph Embedding"},{"paperId":"0ad0e3e965318d83f53f8d5980f3a3d4cc81922f","externalIds":{"DBLP":"journals/corr/abs-2303-15682","ArXiv":"2303.15682","DOI":"10.48550/arXiv.2303.15682","CorpusId":257771370},"title":"Pre-training Transformers for Knowledge Graph Completion"},{"paperId":"c521a86db8ec9da3f2c68e3a44c65856cf902b75","externalIds":{"DBLP":"journals/corr/abs-2303-14467","ArXiv":"2303.14467","DOI":"10.1145/3653298","CorpusId":257766323},"title":"A Survey on the Densest Subgraph Problem and its Variants"},{"paperId":"59b7448f816908cfb49a2ab5e63b2fa5786387f7","externalIds":{"DBLP":"journals/corr/abs-2303-06147","ArXiv":"2303.06147","DOI":"10.48550/arXiv.2303.06147","CorpusId":257482539},"title":"Exphormer: Sparse Transformers for Graphs"},{"paperId":"9e44843fe11c6db994e634d77baa294e8e65b95f","externalIds":{"DBLP":"journals/cviu/SortinoPRS23","ArXiv":"2303.04634","DOI":"10.48550/arXiv.2303.04634","CorpusId":257405149},"title":"Transformer-based Image Generation from Scene Graphs"},{"paperId":"c7f1c0330abe3506d98b4262899a60c9dc5db2b8","externalIds":{"ArXiv":"2303.03922","DBLP":"journals/corr/abs-2303-03922","DOI":"10.1145/3543507.3583301","CorpusId":257378588},"title":"Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer"},{"paperId":"c193011099906126fe7b6cfcb04062cf4591ccf9","externalIds":{"DBLP":"conf/iclr/BoSWL23","ArXiv":"2303.01028","DOI":"10.48550/arXiv.2303.01028","CorpusId":257279837},"title":"Specformer: Spectral Graph Neural Networks Meet Transformers"},{"paperId":"5b43ab59cbd57c3986fa9c9aa6f8c308ee393ef0","externalIds":{"DBLP":"journals/corr/abs-2302-13668","ArXiv":"2302.13668","DOI":"10.1109/TPAMI.2023.3292266","CorpusId":257219910,"PubMed":"37402185"},"title":"Contrastive Video Question Answering via Video Graph Transformer"},{"paperId":"d1226b7875bd3b035acba3b755d1bc260fc871c0","externalIds":{"DBLP":"journals/jcheminf/LiuSDCH23","PubMedCentral":"9968697","DOI":"10.1186/s13321-023-00698-9","CorpusId":257208914,"PubMed":"36843022"},"title":"ABT-MPNN: an atom-bond transformer-based message-passing neural network for molecular property prediction"},{"paperId":"adf825fe9fc984a527dbeff5dc47aed1d5ff5557","externalIds":{"DOI":"10.3390/math11051073","CorpusId":257128976},"title":"Generative Transformer with Knowledge-Guided Decoding for Academic Knowledge Graph Completion"},{"paperId":"91166a75f0e32b782a57028f1501aba6335ac550","externalIds":{"DBLP":"journals/corr/abs-2302-06015","ArXiv":"2302.06015","DOI":"10.48550/arXiv.2302.06015","CorpusId":256827101},"title":"A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity"},{"paperId":"ae5cb6fa8d5707a6d4585080ad8a102f11f0ab5a","externalIds":{"DBLP":"journals/corr/abs-2302-05631","ArXiv":"2302.05631","DOI":"10.48550/arXiv.2302.05631","CorpusId":256826989},"title":"A Survey on Spectral Graph Neural Networks"},{"paperId":"30258c205060af5ce958dc6c9e184c9498ee48ed","externalIds":{"ArXiv":"2302.04181","DBLP":"journals/tmlr/Muller00R24","DOI":"10.48550/arXiv.2302.04181","CorpusId":256662315},"title":"Attending to Graph Transformers"},{"paperId":"b12488cf6cf9c630ab2d344964bf2233bbd9dee8","externalIds":{"DBLP":"journals/corr/abs-2302-00049","ArXiv":"2302.00049","DOI":"10.48550/arXiv.2302.00049","CorpusId":256459900},"title":"Transformers Meet Directed Graphs"},{"paperId":"9b4f564e5d33625fa88fc4e1045e9d5681fa0cca","externalIds":{"DBLP":"journals/corr/abs-2301-09474","ArXiv":"2301.09474","DOI":"10.48550/arXiv.2301.09474","CorpusId":256105170},"title":"DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion"},{"paperId":"4d4f41fa429f37ce41de0422938affb7805cf9a8","externalIds":{"ArXiv":"2301.08210","DBLP":"journals/corr/abs-2301-08210","DOI":"10.1016/j.sbi.2023.102538","CorpusId":256000029,"PubMed":"36764042"},"title":"Everything is Connected: Graph Neural Networks"},{"paperId":"dcffef7c94546389c02c837e0e9290039938b4d2","externalIds":{"DBLP":"journals/corr/abs-2301-07507","ArXiv":"2301.07507","DOI":"10.48550/arXiv.2301.07507","CorpusId":255998567},"title":"Graphix-T5: Mixing Pre-Trained Transformers with Graph-Aware Layers for Text-to-SQL Parsing"},{"paperId":"517802b9381246dff16756fe5299fa62bb29e228","externalIds":{"ArXiv":"2212.13350","DBLP":"journals/corr/abs-2212-13350","DOI":"10.48550/arXiv.2212.13350","CorpusId":255186280},"title":"A Generalization of ViT/MLP-Mixer to Graphs"},{"paperId":"b9fa8a915a9f0cf3e91c1659e19ba057ac78a498","externalIds":{"DBLP":"journals/corr/abs-2211-14425","ArXiv":"2211.14425","DOI":"10.48550/arXiv.2211.14425","CorpusId":254043522},"title":"PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations"},{"paperId":"cbb0aee609f9cee64df027d5d2050ebecfaf4332","externalIds":{"DBLP":"journals/air/WaikhomP23","DOI":"10.1007/s10462-022-10321-2","CorpusId":253868187},"title":"A survey of graph neural networks in various learning paradigms: methods, applications, and challenges"},{"paperId":"36c2a93eae519681578365b531660300a3d06268","externalIds":{"DBLP":"journals/tnn/SunXLXSA24","DOI":"10.1109/TNNLS.2022.3221100","CorpusId":253760257,"PubMed":"36409805"},"title":"Attributed Graph Force Learning"},{"paperId":"c268947c047de41a0f1cadf5145bb2c63d840bd4","externalIds":{"ArXiv":"2211.11220","DBLP":"journals/tnn/LiangLZL24","DOI":"10.1109/TNNLS.2023.3294998","CorpusId":253735000,"PubMed":"37494176"},"title":"STGlow: A Flow-Based Generative Framework With Dual-Graphormer for Pedestrian Trajectory Prediction"},{"paperId":"a1ec107ca5e5e3ec22949aa283462947180ffd66","externalIds":{"ArXiv":"2211.11853","DBLP":"journals/corr/abs-2211-11853","DOI":"10.48550/arXiv.2211.11853","CorpusId":253761014},"title":"Learnable Graph Convolutional Attention Networks"},{"paperId":"f1dee23b27ee932e7299c833bf573b6e884375dd","externalIds":{"DBLP":"journals/bmcbi/WangGDWC22","PubMedCentral":"9635108","DOI":"10.1186/s12859-022-04812-w","CorpusId":252967065,"PubMed":"36329406"},"title":"A novel method for drug-target interaction prediction based on graph transformers model"},{"paperId":"d76f1600e09e1ca4787a2df6a07d656483416a01","externalIds":{"DBLP":"conf/icdm/SunYPLNX22","DOI":"10.1109/ICDMW58026.2022.00118","CorpusId":256669105},"title":"Abnormal Entity-Aware Knowledge Graph Completion"},{"paperId":"5152b9391762aefc532f85a801093bd38a6688c6","externalIds":{"DBLP":"conf/nips/Zhang0HL22","ArXiv":"2210.03930","DOI":"10.48550/arXiv.2210.03930","CorpusId":252780913},"title":"Hierarchical Graph Transformer with Adaptive Node Sampling"},{"paperId":"c65c4b68ac153176354a4c33c37f0ba1d86772c0","externalIds":{"DBLP":"conf/iclr/VignacKSWCF23","ArXiv":"2209.14734","DOI":"10.48550/arXiv.2209.14734","CorpusId":252595881},"title":"DiGress: Discrete Denoising diffusion for graph generation"},{"paperId":"741a7faf9dbefd418cda878c61c5b839ecc02977","externalIds":{"DBLP":"journals/pami/ChenWDZXYHY24","ArXiv":"2209.13232","DOI":"10.1109/TPAMI.2024.3445463","CorpusId":252545348,"PubMed":"39159038"},"title":"A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective"},{"paperId":"adf56c96978fad861e92917f95708ae9935cc132","externalIds":{"DBLP":"journals/corr/abs-2209-09004","ArXiv":"2209.09004","DOI":"10.48550/arXiv.2209.09004","CorpusId":252367214},"title":"EcoFormer: Energy-Saving Attention with Linear Complexity"},{"paperId":"c94f6e9c4deebf01fc4e389529713210554918c8","externalIds":{"PubMedCentral":"9481536","DOI":"10.1038/s41598-022-19999-4","CorpusId":252334426,"PubMed":"36114278"},"title":"Prediction of drug-drug interaction events using graph neural networks based feature extraction"},{"paperId":"31fda8f0984a8983a85a22684742a442cbd1ad35","externalIds":{"DOI":"10.1016/j.mlwa.2022.100334","CorpusId":249358572},"title":"Pre-trained transformers: an empirical comparison"},{"paperId":"334f4d63c8974c685389ffee8d8ea907c8b583f3","externalIds":{"DBLP":"journals/corr/abs-2208-07638","ArXiv":"2208.07638","DOI":"10.1145/3534678.3539472","CorpusId":251518175},"title":"Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries"},{"paperId":"25f48eaa0810c02e6622af15dd006dadc949a503","externalIds":{"DBLP":"conf/kdd/Xia0Z22","ArXiv":"2207.14338","DOI":"10.1145/3534678.3539473","CorpusId":251197028},"title":"Self-Supervised Hypergraph Transformer for Recommender Systems"},{"paperId":"dcaf3718e173a30a5d01491582acbf5b5355099c","externalIds":{"DBLP":"journals/corr/abs-2207-10603","ArXiv":"2207.10603","DOI":"10.48550/arXiv.2207.10603","CorpusId":250918374,"PubMed":"37473609"},"title":"Unsupervised pre-training of graph transformers on patient population graphs"},{"paperId":"009e40cdc9d98b9e5f6279d38b46936ceffcc124","externalIds":{"DBLP":"journals/corr/abs-2207-05342","ArXiv":"2207.05342","DOI":"10.48550/arXiv.2207.05342","CorpusId":250451482},"title":"Video Graph Transformer for Video Question Answering"},{"paperId":"5eda60d4940d4185df45c5703e103458171d465d","externalIds":{"DBLP":"journals/corr/abs-2207-02505","ArXiv":"2207.02505","DOI":"10.48550/arXiv.2207.02505","CorpusId":250311113},"title":"Pure Transformers are Powerful Graph Learners"},{"paperId":"915ab268e705cac0e675bfb20284d26388b197b7","externalIds":{"DBLP":"conf/icpr/SortinoPS22","ArXiv":"2207.00545","DOI":"10.1109/ICPR56361.2022.9956620","CorpusId":250244084},"title":"Transforming Image Generation from Scene Graphs"},{"paperId":"1fb264b21eaf9046387ffddd7a8bf5262674682c","externalIds":{"DBLP":"journals/pami/ParkYPKJKHK25","ArXiv":"2206.14337","DOI":"10.1109/TPAMI.2025.3550281","CorpusId":250113636,"PubMed":"40085455"},"title":"Deformable Graph Transformer"},{"paperId":"95a03fd44f01d71bc2299d2fcec1778c9d8f27bd","externalIds":{"DBLP":"journals/tnn/ZhangFSW24","DOI":"10.1109/TNNLS.2022.3185320","CorpusId":250116447,"PubMed":"35767480"},"title":"Adaptive Semantic-Enhanced Transformer for Image Captioning"},{"paperId":"d451901a6a12c61179289cac7a4588a86c234112","externalIds":{"DBLP":"conf/aaai/0004HWCCC22","DOI":"10.1609/aaai.v36i3.20222","CorpusId":250294994},"title":"Width & Depth Pruning for Vision Transformers"},{"paperId":"100b3dc3200b6d0d57f96fab5ff3654c9d3ee048","externalIds":{"DBLP":"conf/sigmod/0002GC0L22","DOI":"10.1145/3514221.3517872","CorpusId":249579015},"title":"Entity Resolution with Hierarchical Graph Attention Networks"},{"paperId":"73d724880f1d12327a8d1299bfb2f57c35251d10","externalIds":{"DBLP":"journals/corr/abs-2206-04355","ArXiv":"2206.04355","DOI":"10.1145/3534678.3539121","CorpusId":237267087},"title":"Graph Attention Multi-Layer Perceptron"},{"paperId":"55e31baa3ae5f32fb5e695761892319e26dbc639","externalIds":{"DBLP":"journals/corr/abs-2206-02353","ArXiv":"2206.02353","DOI":"10.48550/arXiv.2206.02353","CorpusId":249395333},"title":"Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data"},{"paperId":"ca285c906eac9b1b31483d2bc3cddc52c90ac564","externalIds":{"DBLP":"conf/kdd/LiZZ22","ArXiv":"2206.03364","DOI":"10.1145/3534678.3539426","CorpusId":249431724},"title":"KPGT: Knowledge-Guided Pre-training of Graph Transformer for Molecular Property Prediction"},{"paperId":"de81a91bd1f44877637a4e27dce5fd914a4b24cf","externalIds":{"DBLP":"conf/cvpr/LeeCSM22","DOI":"10.1109/CVPR52688.2022.00218","CorpusId":250961853},"title":"KNN Local Attention for Image Restoration"},{"paperId":"168e32bb9b0d3abead4da6e366d204c5d9177f6f","externalIds":{"ArXiv":"2205.13891","DBLP":"conf/nips/YangHW22","DOI":"10.48550/arXiv.2205.13891","CorpusId":249152115},"title":"Transformers from an Optimization Perspective"},{"paperId":"e5a91ebf83e519f4db14e3d33a0e5e9e209243f9","externalIds":{"DBLP":"journals/corr/abs-2205-13202","ArXiv":"2205.13202","DOI":"10.1145/3571808","CorpusId":249097855},"title":"More Recent Advances in (Hyper)Graph Partitioning"},{"paperId":"277dd73bfeb5c46513ce305136b0e71fcd2a311c","externalIds":{"ArXiv":"2205.12454","DBLP":"journals/corr/abs-2205-12454","DOI":"10.48550/arXiv.2205.12454","CorpusId":249062808},"title":"Recipe for a General, Powerful, Scalable Graph Transformer"},{"paperId":"641828b8ca714a0f70ccdac17d7e9dff485877c2","externalIds":{"DBLP":"journals/corr/abs-2205-11172","ArXiv":"2205.11172","DOI":"10.48550/arXiv.2205.11172","CorpusId":248987544},"title":"How Powerful are Spectral Graph Neural Networks"},{"paperId":"72572ee0690a97202479109f6b71736722c27cd4","externalIds":{"ArXiv":"2205.10627","DBLP":"journals/corr/abs-2205-10627","DOI":"10.48550/arXiv.2205.10627","CorpusId":248986181},"title":"DProQ: A Gated-Graph Transformer for Protein Complex Structure Assessment"},{"paperId":"bedcfb163368f2d802de3e892acb34cc5a75a22d","externalIds":{"DBLP":"conf/sigir/ChenZLDTXHSC22","ArXiv":"2205.02357","DOI":"10.1145/3477495.3531992","CorpusId":248524814},"title":"Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion"},{"paperId":"3f200791d33165673740a3224a0afc5daf36f387","externalIds":{"ArXiv":"2204.13154","DBLP":"journals/corr/abs-2204-13154","DOI":"10.1007/s00521-022-07366-3","CorpusId":248427085},"title":"Attention mechanism in neural networks: where it comes and where it goes"},{"paperId":"7b3817e949625fd3e8a2398d94d9bc29b975ebb9","externalIds":{"DOI":"10.1038/s41580-022-00488-5","CorpusId":248414760,"PubMed":"35477993"},"title":"The prospects and opportunities of protein structure prediction with AI"},{"paperId":"bffd09743a549ca0bae00ffad14376204d3a7f55","externalIds":{"DBLP":"conf/www/Balsebre0CH22","DOI":"10.1145/3485447.3512026","CorpusId":248367485},"title":"Geospatial Entity Resolution"},{"paperId":"c9de8b052562e03ebe8445a14d9cb3cb08e85fd6","externalIds":{"DBLP":"journals/access/JoshiHTAS23","ArXiv":"2204.03326","DOI":"10.1109/ACCESS.2023.3234761","CorpusId":254564784},"title":"Enabling All In-Edge Deep Learning: A Literature Review"},{"paperId":"3b6ac1763fb98011c52ed9365e1afbef1c414f5b","externalIds":{"ArXiv":"2204.02656","DBLP":"journals/corr/abs-2204-02656","MAG":"3197093862","DOI":"10.1109/tnse.2021.3108974","CorpusId":239733292},"title":"CHIEF: Clustering With Higher-Order Motifs in Big Networks"},{"paperId":"a2fc77f075f666b462d9350e7576f0ba9845c61b","externalIds":{"DBLP":"journals/corr/abs-2203-16634","ArXiv":"2203.16634","DOI":"10.48550/arXiv.2203.16634","CorpusId":247839823},"title":"Transformer Language Models without Positional Encodings Still Learn Positional Information"},{"paperId":"2e6654520d8831f1721d4ec2dd1089b5d27f460f","externalIds":{"ArXiv":"2203.15876","DBLP":"journals/tkde/YuYXCLH24","DOI":"10.1109/TKDE.2023.3282907","CorpusId":247794106},"title":"Self-Supervised Learning for Recommender Systems: A Survey"},{"paperId":"23903b4c42fdbea0b7b35e3157b48d8dfd18e1a5","externalIds":{"DBLP":"conf/aaai/WangXS22","ArXiv":"2203.15350","DOI":"10.48550/arXiv.2203.15350","CorpusId":247778466},"title":"End-to-End Transformer Based Model for Image Captioning"},{"paperId":"c721752485f58a2074c119333c0adb64273193ef","externalIds":{"ArXiv":"2203.08566","DBLP":"journals/corr/abs-2203-08566","DOI":"10.1109/CVPR52688.2022.00146","CorpusId":247475790},"title":"EDTER: Edge Detection with Transformer"},{"paperId":"40887c19829666621eb5ca139ddb3f601813426e","externalIds":{"DBLP":"journals/corr/abs-2202-10688","ArXiv":"2202.10688","DOI":"10.1109/MCI.2022.3222049","CorpusId":247025723},"title":"Graph Lifelong Learning: A Survey"},{"paperId":"e58dde4b23f251314e900f56e765b4aad27bc15f","externalIds":{"ArXiv":"2202.08455","DBLP":"journals/corr/abs-2202-08455","CorpusId":246904638},"title":"Transformer for Graphs: An Overview from Architecture Perspective"},{"paperId":"820dedf46024bc0dc7221cf2782119bae6949116","externalIds":{"ArXiv":"2202.07826","DBLP":"journals/corr/abs-2202-07826","DOI":"10.1109/TKDE.2022.3149888","CorpusId":246712274},"title":"CenGCN: Centralized Convolutional Networks with Vertex Imbalance for Scale-Free Graphs"},{"paperId":"ea0e4a9778e33b7f8e7b3246d63071330950995a","externalIds":{"DBLP":"conf/icml/ChenOB22","ArXiv":"2202.03036","CorpusId":246634635},"title":"Structure-Aware Transformer for Graph Representation Learning"},{"paperId":"daaa79be8d02cd3f748b6a3465fc1f09b5068880","externalIds":{"DOI":"10.1101/2022.02.03.479055","CorpusId":246653356},"title":"Towards Effective and Generalizable Fine-tuning for Pre-trained Molecular Graph Models"},{"paperId":"b92898a28bfad42a053726c2707cc05686cd332a","externalIds":{"ArXiv":"2201.12787","CorpusId":247476434},"title":"GRPE: Relative Positional Encoding for Graph Transformer"},{"paperId":"32820e6c6b145a497c2d9f40e259e41af8ec93fa","externalIds":{"ArXiv":"2201.11460","DBLP":"journals/pami/CongYR23","DOI":"10.1109/TPAMI.2023.3268066","CorpusId":246294640,"PubMed":"37074895"},"title":"RelTR: Relation Transformer for Scene Graph Generation"},{"paperId":"d00c436974359a250f0869aa548e94324c9a62a5","externalIds":{"ArXiv":"2201.07614","DBLP":"journals/corr/abs-2201-07614","CorpusId":246035431},"title":"Uncovering More Shallow Heuristics: Probing the Natural Language Inference Capacities of Transformer-Based Pre-Trained Language Models Using Syllogistic Patterns"},{"paperId":"2d57a3f90adf3fc28f0de61fb4b7b34bccb1b92d","externalIds":{"DBLP":"journals/corr/abs-2201-07284","ArXiv":"2201.07284","DOI":"10.14778/3514061.3514067","CorpusId":246035402},"title":"TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data"},{"paperId":"4d42ba1860c84a9d2d462f4e109f4eeab8f53127","externalIds":{"ArXiv":"2112.12970","DBLP":"journals/corr/abs-2112-12970","DOI":"10.1109/CVPR52688.2022.01888","CorpusId":245502596},"title":"SGTR: End-to-end Scene Graph Generation with Transformer"},{"paperId":"5553f9508dd1056ecc20c5b1f367e9a07e2c7e81","externalIds":{"DBLP":"journals/corr/abs-2112-10762","ArXiv":"2112.10762","DOI":"10.1109/CVPR52688.2022.01102","CorpusId":245334475},"title":"StyleSwin: Transformer-based GAN for High-resolution Image Generation"},{"paperId":"489af1ae6db21c198b6efeffc3ba68313f9bd4b3","externalIds":{"DBLP":"journals/tnn/AlkendiAAJSZ24","ArXiv":"2112.09685","DOI":"10.1109/TNNLS.2022.3201830","CorpusId":245329490,"PubMed":"36107888"},"title":"Neuromorphic Camera Denoising Using Graph Neural Network-Driven Transformers"},{"paperId":"8a658ca1e914ef690e7ed62e89042d24eca9689c","externalIds":{"DBLP":"journals/tcbb/ChuNHNN23","DOI":"10.1101/2021.11.29.470386","CorpusId":244825827,"PubMed":"36107906"},"title":"Graph Transformer for drug response prediction"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","externalIds":{"DBLP":"conf/nips/BergenOB21","ArXiv":"2112.00578","CorpusId":244773350},"title":"Systematic Generalization with Edge Transformers"},{"paperId":"011470642a6758d0c2c7fb2c48564370364289be","externalIds":{"ArXiv":"2111.15143","DBLP":"journals/corr/abs-2111-15143","DOI":"10.1109/CVPR52688.2022.00384","CorpusId":244729623},"title":"HEAT: Holistic Edge Attention Transformer for Structured Reconstruction"},{"paperId":"7eb27a485408d0bbc21219b45013fdcd6e6a68ea","externalIds":{"DBLP":"conf/sdm/CongWTGXCM23","ArXiv":"2111.10447","DOI":"10.1137/1.9781611977653.ch50","CorpusId":255185867},"title":"DyFormer : A Scalable Dynamic Graph Transformer with Provable Benefits on Generalization Ability"},{"paperId":"a585828fab3ec46ad27e14adfd299953df107a47","externalIds":{"ArXiv":"2110.14416","DBLP":"journals/corr/abs-2110-14416","CorpusId":239998288},"title":"Transformers Generalize DeepSets and Can be Extended to Graphs and Hypergraphs"},{"paperId":"1ce5f097564e85813b9c9cfc7273eb090dfede24","externalIds":{"ArXiv":"2110.07347","DBLP":"journals/corr/abs-2110-07347","DOI":"10.1093/bib/bbac162","CorpusId":238857069,"PubMed":"35514186"},"title":"Improved Drug-target Interaction Prediction with Intermolecular Graph Transformer"},{"paperId":"a46b06a4b8b4deecf96a4e42cd19b4696f999e66","externalIds":{"ArXiv":"2110.02642","DBLP":"journals/corr/abs-2110-02642","CorpusId":238408395},"title":"Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy"},{"paperId":"e07093f22c1689c6a6ffeebab65db7538b18547d","externalIds":{"MAG":"3193501747","DBLP":"journals/tkde/GaoGYLW23","DOI":"10.1109/TKDE.2021.3105544","CorpusId":238700729},"title":"Higher-Order Interaction Goes Neural: A Substructure Assembling Graph Attention Network for Graph Classification"},{"paperId":"fd753314bfa805a0c831cc0a693a1de2defbc387","externalIds":{"ArXiv":"2108.03348","DBLP":"conf/kdd/HussainZS22","DOI":"10.1145/3534678.3539296","CorpusId":249375304},"title":"Global Self-Attention as a Replacement for Graph Convolution"},{"paperId":"a04018f3bf09ad29f61f63ef60973c875ffc97dc","externalIds":{"DBLP":"conf/mm/DongLXX21","ArXiv":"2108.02366","DOI":"10.1145/3474085.3475439","CorpusId":236924384},"title":"Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning"},{"paperId":"9480a398dc62f9d63e1584660fa9342775d37274","externalIds":{"DBLP":"conf/icml/WuYA21","ArXiv":"2107.09240","CorpusId":235826264},"title":"Generative Video Transformer: Can Objects be the Words?"},{"paperId":"9058d322a09bfc0c93a070f87cac8fd840e63088","externalIds":{"ArXiv":"2107.07999","DBLP":"conf/icml/ChoromanskiLCZS22","CorpusId":246430573},"title":"From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers"},{"paperId":"ddb842d2d609ddd535c4d15637e4bcd2c6d834b2","externalIds":{"ArXiv":"2106.09876","DBLP":"journals/corr/abs-2106-09876","DOI":"10.1109/TKDE.2021.3124061","CorpusId":235485388},"title":"Anomaly Detection in Dynamic Graphs via Transformer"},{"paperId":"35067cc8153b7e6270797fffaefc5c9cefdfe515","externalIds":{"DBLP":"journals/tkde/MaWXYZSXA23","ArXiv":"2106.07178","DOI":"10.1109/TKDE.2021.3118815","CorpusId":235421996},"title":"A Comprehensive Survey on Graph Anomaly Detection With Deep Learning"},{"paperId":"a6337d9ebb0b7de84588806110157806f9c0383b","externalIds":{"ArXiv":"2106.05667","DBLP":"journals/corr/abs-2106-05667","CorpusId":235390675},"title":"GraphiT: Encoding Graph Structure in Transformers"},{"paperId":"1f47d68fe87d0b1317b34a71f98548df3f5da5ff","externalIds":{"PubMedCentral":"8192505","DOI":"10.1038/s41467-021-23720-w","CorpusId":235403425,"PubMed":"34112777"},"title":"Algebraic graph-assisted bidirectional transformers for molecular property prediction"},{"paperId":"d8d2e574965fe733eb1416e03df2b5c2914fc530","externalIds":{"DBLP":"journals/corr/abs-2106-04554","ArXiv":"2106.04554","DOI":"10.1016/j.aiopen.2022.10.001","CorpusId":235368340},"title":"A Survey of Transformers"},{"paperId":"cdc86a4a9cd487eecd722f55da0b004d62e7ffeb","externalIds":{"DBLP":"journals/corr/abs-2106-04113","ArXiv":"2106.04113","CorpusId":235368311},"title":"Self-supervised Graph-level Representation Learning with Local and Global Structure"},{"paperId":"5863d7b35ea317c19f707376978ef1cc53e3534c","externalIds":{"DBLP":"conf/nips/KreuzerBHLT21","ArXiv":"2106.03893","CorpusId":235368041},"title":"Rethinking Graph Transformers with Spectral Attention"},{"paperId":"705d28ab55d65663ec5783302941e8eed2b99084","externalIds":{"ArXiv":"2109.05346","DBLP":"journals/corr/abs-2109-05346","DOI":"10.1109/CVPRW53098.2021.00244","CorpusId":235691730},"title":"BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation"},{"paperId":"ab30672c8c5e4787f6a5985f26a8f281f0db2fb8","externalIds":{"DBLP":"journals/corr/abs-2105-14491","ArXiv":"2105.14491","CorpusId":235254358},"title":"How Attentive are Graph Attention Networks?"},{"paperId":"61cce75554a6d1bb802f26758c3b0ba97de6918d","externalIds":{"DBLP":"journals/corr/abs-2105-04037","ArXiv":"2105.04037","DOI":"10.1007/978-3-030-75762-5_41","CorpusId":234337529},"title":"Graph Attention Networks with Positional Embeddings"},{"paperId":"6709d5583f658f589ae6a2184805933aceb18849","externalIds":{"DBLP":"conf/nips/ChuTWZRWXS21","ArXiv":"2104.13840","CorpusId":234364557},"title":"Twins: Revisiting the Design of Spatial Attention in Vision Transformers"},{"paperId":"2d9ae4c167510ed78803735fc57ea67c3cc55a35","externalIds":{"DBLP":"journals/corr/abs-2104-10157","ArXiv":"2104.10157","CorpusId":233307257},"title":"VideoGPT: Video Generation using VQ-VAE and Transformers"},{"paperId":"db46b0de44c5113c47f0ec5392eb91d0726497bf","externalIds":{"ACL":"2021.emnlp-main.236","DBLP":"conf/emnlp/ChenTBCCF21","ArXiv":"2104.08698","DOI":"10.18653/v1/2021.emnlp-main.236","CorpusId":243728757},"title":"A Simple and Effective Positional Encoding for Transformers"},{"paperId":"95f5870b18d5f894e4f6ec8490d1a39e0963e79e","externalIds":{"DBLP":"journals/corr/abs-2104-03466","ArXiv":"2104.03466","DOI":"10.1109/JIOT.2021.3100509","CorpusId":233181653},"title":"Learning Graph Structures With Transformer for Multivariate Time-Series Anomaly Detection in IoT"},{"paperId":"a56bf7ee9a56d8f84079684339a953c2df9ce76b","externalIds":{"MAG":"3146366485","DBLP":"journals/ijon/NiuZY21","DOI":"10.1016/J.NEUCOM.2021.03.091","CorpusId":233562906},"title":"A review on the attention mechanism of deep learning"},{"paperId":"57fbaf35321b2c4c4c0cc2b63e72bfb9c5d5d9c9","externalIds":{"DBLP":"journals/tai/00010YAWP021","ArXiv":"2105.00696","DOI":"10.1109/TAI.2021.3076021","CorpusId":233481068},"title":"Graph Learning: A Survey"},{"paperId":"40f4d7fe800810288a80f84cdb357a8f4c28e880","externalIds":{"DBLP":"conf/iccv/HeoYHCCO21","ArXiv":"2103.16302","DOI":"10.1109/ICCV48922.2021.01172","CorpusId":232417451},"title":"Rethinking Spatial Dimensions of Vision Transformers"},{"paperId":"3cbe314cc5407a6c3249815b5173f22ea15173c2","externalIds":{"ArXiv":"2103.15358","DBLP":"journals/corr/abs-2103-15358","DOI":"10.1109/ICCV48922.2021.00299","CorpusId":232404731},"title":"Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"},{"paperId":"cbb8faf1051d858b4b0426742fdbbd0f104833ea","externalIds":{"MAG":"3042185737","DBLP":"journals/eswa/El-KassasSRM21","DOI":"10.1016/j.eswa.2020.113679","CorpusId":224955327},"title":"Automatic text summarization: A comprehensive survey"},{"paperId":"0ae67202f0584afccefa770865d14a46655d2975","externalIds":{"DBLP":"conf/nips/HanXWGXW21","ArXiv":"2103.00112","CorpusId":232076027},"title":"Transformer in Transformer"},{"paperId":"9eea59c34f139f3d2153226c8cf026e975622074","externalIds":{"ArXiv":"2102.01818","DBLP":"journals/corr/abs-2102-01818","ACL":"2021.eacl-main.113","DOI":"10.18653/v1/2021.eacl-main.113","CorpusId":231786600},"title":"Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation"},{"paperId":"849b88ddc8f8cabc6d4246479b275a1ee65d0647","externalIds":{"MAG":"3113177135","DBLP":"journals/corr/abs-2012-09699","ArXiv":"2012.09699","CorpusId":229298019},"title":"A Generalization of Transformer Networks to Graphs"},{"paperId":"a11828bb8b2e5f1644360567f0e46d20de342ad6","externalIds":{"MAG":"3108202858","DBLP":"journals/tbd/WangBSFYY23","ArXiv":"2011.14867","DOI":"10.1109/TBDATA.2022.3177455","CorpusId":227229005},"title":"A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources"},{"paperId":"3af8a493cf756f9fe72623204a11e378a9cd71a5","externalIds":{"ArXiv":"2011.14203","DBLP":"conf/micro/TambeHPJYDSWR0W21","DOI":"10.1145/3466752.3480095","CorpusId":237421361},"title":"EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"},{"paperId":"3443efc855cebd17d1512d1a703b6e9ee2e4da8b","externalIds":{"ArXiv":"2011.02260","MAG":"3097300053","DBLP":"journals/corr/abs-2011-02260","DOI":"10.1145/3535101","CorpusId":226246289},"title":"Graph Neural Networks in Recommender Systems: A Survey"},{"paperId":"e1d082562981a9f51649c60663aa484ee623dbb0","externalIds":{"DBLP":"journals/access/EngelBD21","ArXiv":"2011.00931","DOI":"10.1109/ACCESS.2021.3116304","CorpusId":226227046},"title":"Point Transformer"},{"paperId":"76c124786ccf4263e6403a15a8e350ac28be4e65","externalIds":{"MAG":"3102419180","ArXiv":"2010.13902","DBLP":"journals/corr/abs-2010-13902","CorpusId":225076220},"title":"Graph Contrastive Learning with Augmentations"},{"paperId":"95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452","externalIds":{"DBLP":"journals/corr/abs-2010-09885","ArXiv":"2010.09885","MAG":"3093934881","CorpusId":224803102},"title":"ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction"},{"paperId":"c0f709acf38eb27702b0fbce1215db0ebaa2de2b","externalIds":{"ArXiv":"2010.05315","DBLP":"journals/corr/abs-2010-05315","MAG":"3092852584","CorpusId":222290917},"title":"SMYRF: Efficient Attention using Asymmetric Clustering"},{"paperId":"ec7bac39655b5ea177e76f7a80afd02658154027","externalIds":{"ArXiv":"2010.03009","DBLP":"conf/aaai/AhmadPC21","MAG":"3091998909","DOI":"10.1609/aaai.v35i14.17478","CorpusId":222177188},"title":"GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","externalIds":{"ArXiv":"2009.06732","DBLP":"journals/csur/TayDBM23","MAG":"3085139254","DOI":"10.1145/3530811","CorpusId":221702858},"title":"Efficient Transformers: A Survey"},{"paperId":"96629a28ffe1b474b1d33173d1921b5346a1eeaf","externalIds":{"PubMedCentral":"7495825","MAG":"3085429933","DBLP":"journals/bmcbi/PengLS20","DOI":"10.1186/s12859-020-03677-1","CorpusId":221767654,"PubMed":"32938374"},"title":"A learning-based method for drug-target interaction prediction based on feature representation learning and deep neural network"},{"paperId":"ee0b27c32603a352ddebe27f98f89fc8d7795cb6","externalIds":{"MAG":"3090968664","DBLP":"conf/ijcnn/He0R20","DOI":"10.1109/IJCNN48605.2020.9207273","CorpusId":221658240},"title":"TimeSAN: A Time-Modulated Self-Attentive Network for Next Point-of-Interest Recommendation"},{"paperId":"6f68e1bb253925d8431588555d3010419f322e04","externalIds":{"DBLP":"conf/icml/KatharopoulosV020","MAG":"3037798801","ArXiv":"2006.16236","CorpusId":220250819},"title":"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"},{"paperId":"04faf433934486c41d082e8d75ccfe5dc2f69fef","externalIds":{"DBLP":"conf/kdd/HuDWCS20","MAG":"3037208489","ArXiv":"2006.15437","DOI":"10.1145/3394486.3403237","CorpusId":220250007},"title":"GPT-GNN: Generative Pre-Training of Graph Neural Networks"},{"paperId":"cf1eb488136995d76b5e64410fffa63ed7236702","externalIds":{"DBLP":"journals/corr/abs-2006-13561","MAG":"3037989546","ACL":"2020.acl-main.589","ArXiv":"2006.13561","DOI":"10.18653/v1/2020.acl-main.589","CorpusId":220041944},"title":"Differentiable Window for Dynamic Local Attention"},{"paperId":"a9a4e8e631890a14257539948e1813b5214c60dd","externalIds":{"DBLP":"conf/nips/RongBXX0HH20","MAG":"3101620381","CorpusId":226191736},"title":"Self-Supervised Graph Transformer on Large-Scale Molecular Data"},{"paperId":"965652c0e426c5b42d7218d7429025be7ac542bf","externalIds":{"MAG":"3035649237","ArXiv":"2006.07739","DBLP":"journals/corr/abs-2006-07739","CorpusId":219687460},"title":"DeeperGCN: All You Need to Train Deeper GCNs"},{"paperId":"d39f7f66c0cde20c83197e0a16caf3f496842a06","externalIds":{"MAG":"3034077950","DOI":"10.1007/978-3-030-40245-7_10","CorpusId":219903765},"title":"Message Passing Neural Networks"},{"paperId":"4fb0a181676a5200bc6e53dea1b770613c164aab","externalIds":{"MAG":"3026476702","DBLP":"conf/acl/LiXLWWD20","ACL":"2020.acl-main.555","ArXiv":"2005.10043","DOI":"10.18653/v1/2020.acl-main.555","CorpusId":218718706},"title":"Leveraging Graph to Improve Abstractive Multi-Document Summarization"},{"paperId":"a93b62bdc8b8e64aec7f7900d0193358134f1677","externalIds":{"ArXiv":"2005.08225","DBLP":"conf/ijcai/LiuX0ZHPNYY20","MAG":"3103796199","DOI":"10.24963/IJCAI.2020/693","CorpusId":218674573},"title":"Deep Learning for Community Detection: Progress, Challenges and Opportunities"},{"paperId":"d365f9c805d59788f9ae5ad36fee69f9abd8d3c7","externalIds":{"DBLP":"journals/tnn/ChenWZ24","ArXiv":"2004.06015","MAG":"3015927585","DOI":"10.1109/TNNLS.2023.3264519","CorpusId":215745291,"PubMed":"37093721"},"title":"Toward Subgraph-Guided Knowledge Graph Question Generation With Graph Neural Networks"},{"paperId":"e401f6b5118279afb7304ab07d18581d286155de","externalIds":{"MAG":"3035750285","DBLP":"journals/corr/abs-2004-01389","ArXiv":"2004.01389","DOI":"10.1109/CVPR42600.2020.01151","CorpusId":214795201},"title":"LiDAR-Based Online 3D Video Object Detection With Graph-Based Message Passing and Spatiotemporal Transformer Attention"},{"paperId":"e6601482896afffb3b602469491724135b16facf","externalIds":{"ArXiv":"2008.03639","MAG":"3106006733","DBLP":"journals/corr/abs-2008-03639","DOI":"10.1109/TETCI.2019.2952908","CorpusId":214058761},"title":"Random Walks: A Review of Algorithms and Applications"},{"paperId":"7d5f30684cab3602f359d7c6928abc5b0bfbc248","externalIds":{"DBLP":"journals/access/GongMTTZDCBLL20","MAG":"3006271038","DOI":"10.1109/ACCESS.2020.2972751","CorpusId":211244107},"title":"Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","externalIds":{"DBLP":"journals/corr/abs-2001-04451","MAG":"2994673210","ArXiv":"2001.04451","CorpusId":209315300},"title":"Reformer: The Efficient Transformer"},{"paperId":"4c40aaa43b8edc4a4212207487bd74b32c23eb14","externalIds":{"DBLP":"journals/tgrs/MouZ20","MAG":"2977002487","DOI":"10.1109/TGRS.2019.2933609","CorpusId":204186525},"title":"Learning to Pay Attention on Spectral Domain: A Spectral Attention Module-Based Convolutional Network for Hyperspectral Image Classification"},{"paperId":"2e3a86c4b8f6883b371f718eb0a35857a6bf9b95","externalIds":{"DBLP":"journals/nn/BacciuEMP20","MAG":"2997785591","ArXiv":"1912.12693","DOI":"10.1016/J.NEUNET.2020.06.006","CorpusId":209516216,"PubMed":"32559609"},"title":"A Gentle Introduction to Deep Learning for Graphs"},{"paperId":"e63457e11357dba2a887a8968aed809121ef8cf3","externalIds":{"MAG":"3038823641","DOI":"10.1016/j.ddtec.2020.05.001","CorpusId":225598288,"PubMed":"33386091"},"title":"Molecular property prediction: recent trends in the era of artificial intelligence."},{"paperId":"bb98bc96e02396d199fc899287d9b84393c86e79","externalIds":{"ArXiv":"1911.07470","DBLP":"journals/corr/abs-1911-07470","MAG":"2998702685","DOI":"10.1609/AAAI.V34I05.6243","CorpusId":208138227},"title":"Graph Transformer for Graph-to-Sequence Learning"},{"paperId":"aa63ac11aa9dcaa9edd4c88db18bec87e0834328","externalIds":{"ArXiv":"1911.06455","DBLP":"conf/nips/YunJKKK19","MAG":"2970066309","CorpusId":202763464},"title":"Graph Transformer Networks"},{"paperId":"37a23c43ddf09ea97b82b38e2827a2229cfae545","externalIds":{"DBLP":"conf/nips/ShivQ19","MAG":"2971270287","CorpusId":202784970},"title":"Novel positional encodings to enable tree-based transformers"},{"paperId":"bc10f074708aa95fb072fee68bd5ff0336652168","externalIds":{"MAG":"2979845147","ArXiv":"2008.07097","DBLP":"journals/tkde/LiuXWXKTK21","DOI":"10.1109/TKDE.2019.2946825","CorpusId":208091288},"title":"Shifu2: A Network Representation Learning Based Model for Advisor-Advisee Relationship Mining"},{"paperId":"5951d1e1a850a333519caaeee68e3b1070e2eb7a","externalIds":{"ArXiv":"1909.11855","DBLP":"conf/www/NguyenNP22","DOI":"10.1145/3487553.3524258","CorpusId":225065036},"title":"Universal Graph Transformer Self-Attention Networks"},{"paperId":"cde440afaf3670c1b54d4e2b32724287449a0018","externalIds":{"MAG":"2971865858","ArXiv":"1909.00958","DBLP":"journals/corr/abs-1909-00958","DOI":"10.1017/ATSIP.2020.13","CorpusId":202541524},"title":"Graph representation learning: a survey"},{"paperId":"0a6a9e6d4e3efd7c69357769305b70097281655f","externalIds":{"ArXiv":"1907.10903","MAG":"2999269676","DBLP":"conf/iclr/RongHXH20","CorpusId":212859361},"title":"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification"},{"paperId":"01712a91243b66da522132de96d2ff6dfb8bc512","externalIds":{"MAG":"3003634687","ArXiv":"1907.09815","DBLP":"journals/tnn/GuoXT23","DOI":"10.1109/TNNLS.2021.3104937","CorpusId":214136195,"PubMed":"34428156"},"title":"Bilinear Graph Networks for Visual Question Answering"},{"paperId":"5fa7239e9b852661deb3d90c88c0fd170d4b2f3c","externalIds":{"MAG":"2949218973","DOI":"10.1038/s41586-019-1315-z","CorpusId":195064093,"PubMed":"31217586"},"title":"Large-scale chemical–genetics yields new M. tuberculosis inhibitor classes"},{"paperId":"c3229debfda1b015c88404cf98f1074237d80809","externalIds":{"MAG":"2965570621","DBLP":"conf/ijcai/ShangMXS19","ArXiv":"1906.00346","DOI":"10.24963/ijcai.2019/825","CorpusId":173990821},"title":"Pre-training of Graph Augmented Transformers for Medication Recommendation"},{"paperId":"ee8b1603c79a4f9c3bdc0d6633b595aa93ff3a0f","externalIds":{"ArXiv":"1904.08082","DBLP":"conf/icml/LeeLK19","MAG":"2963175980","CorpusId":119314157},"title":"Self-Attention Graph Pooling"},{"paperId":"ecce0a7b0f9b2bfbc6ca2c99805bddd53178ac35","externalIds":{"ArXiv":"1904.05880","MAG":"2935694273","DBLP":"journals/corr/abs-1904-05880","DOI":"10.1109/CVPR.2019.00214","CorpusId":119115657},"title":"Factor Graph Attention"},{"paperId":"cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022","externalIds":{"MAG":"2954922414","DBLP":"conf/naacl/Koncel-Kedziorski19","ArXiv":"1904.02342","ACL":"N19-1238","DOI":"10.18653/v1/N19-1238","CorpusId":102354588},"title":"Text Generation from Knowledge Graphs with Graph Transformers"},{"paperId":"30b38ca8151bbd5a5ff45bce94297d1248ff58b5","externalIds":{"MAG":"2904900486","DBLP":"journals/corr/abs-1812-04202","ArXiv":"1812.04202","DOI":"10.1109/tkde.2020.2981333","CorpusId":54559476},"title":"Deep Learning on Graphs: A Survey"},{"paperId":"7e27d44e3fac723ccb703e0a83b22711bd42efe8","externalIds":{"MAG":"2896348597","ArXiv":"1810.04020","DBLP":"journals/csur/HossainSSL19","DOI":"10.1145/3295748","CorpusId":52947736},"title":"A Comprehensive Survey of Deep Learning for Image Captioning"},{"paperId":"62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9","externalIds":{"MAG":"2950468517","ArXiv":"1810.00826","DBLP":"journals/corr/abs-1810-00826","CorpusId":52895589},"title":"How Powerful are Graph Neural Networks?"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","externalIds":{"MAG":"2963925437","DBLP":"journals/corr/abs-1803-02155","ACL":"N18-2074","ArXiv":"1803.02155","DOI":"10.18653/v1/N18-2074","CorpusId":3725815},"title":"Self-Attention with Relative Position Representations"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","externalIds":{"ArXiv":"1802.05751","DBLP":"conf/icml/ParmarVUKSKT18","MAG":"2950739196","CorpusId":3353110},"title":"Image Transformer"},{"paperId":"33998aff64ce51df8dee45989cdca4b6b1329ec4","externalIds":{"DBLP":"journals/corr/abs-1710-10903","ArXiv":"1710.10903","MAG":"2766453196","DOI":"10.17863/CAM.48429","CorpusId":3292002},"title":"Graph Attention Networks"},{"paperId":"006906b6bbe5c1f378cde9fd86de1ce9e6b131da","externalIds":{"MAG":"2963224980","DBLP":"journals/tkde/CaiZC18","ArXiv":"1709.07604","DOI":"10.1109/TKDE.2018.2807452","CorpusId":13999578},"title":"A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications"},{"paperId":"e76edb86f270c3a77ed9f5a1e1b305461f36f96f","externalIds":{"MAG":"2951910147","DBLP":"conf/cvpr/Tulyakov0YK18","ArXiv":"1707.04993","DOI":"10.1109/CVPR.2018.00165","CorpusId":4475365},"title":"MoCoGAN: Decomposing Motion and Content for Video Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","externalIds":{"DBLP":"journals/corr/VaswaniSPUJGKP17","MAG":"2963403868","ArXiv":"1706.03762","CorpusId":13756489},"title":"Attention is All you Need"},{"paperId":"6b7d6e6416343b2a122f8416e69059ce919026ef","externalIds":{"DBLP":"conf/nips/HamiltonYL17","MAG":"2952779545","ArXiv":"1706.02216","CorpusId":4755450},"title":"Inductive Representation Learning on Large Graphs"},{"paperId":"50233ac68c2aa139fabe71e2844c2734adf39a3d","externalIds":{"DBLP":"journals/tcsv/PengHZ18","MAG":"2606965845","ArXiv":"1704.02223","DOI":"10.1109/TCSVT.2017.2705068","CorpusId":15514398},"title":"An Overview of Cross-Media Retrieval: Concepts, Methodologies, Benchmarks, and Challenges"},{"paperId":"15705833ae94afcece8680fff6f8309d35fdb1bd","externalIds":{"MAG":"2950678578","ArXiv":"1610.09500","DBLP":"journals/tkde/LinWLG20","DOI":"10.1109/TKDE.2019.2898191","CorpusId":6254075},"title":"Efficient Entity Resolution on Heterogeneous Records"},{"paperId":"df86d2a8c217776786bac9019d8b20029e4c0dd5","externalIds":{"MAG":"2070232376","DBLP":"journals/siamsc/KarypisK98","DOI":"10.1137/S1064827595287997","CorpusId":3628209},"title":"A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs"},{"paperId":"8a08a5d59aa590441f606ca834ddb1e449a0b316","externalIds":{"DBLP":"journals/tmm/ZhangPYHMC24","DOI":"10.1109/TMM.2023.3283879","CorpusId":259777127},"title":"End-to-End Video Scene Graph Generation With Temporal Propagation Transformer"},{"paperId":"683f1e4b9b229134d20fa8ffe64340608e7703fd","externalIds":{"DBLP":"journals/tmm/MoX24","DOI":"10.1109/TMM.2023.3269219","CorpusId":258287827},"title":"BSTG-Trans: A Bayesian Spatial-Temporal Graph Transformer for Long-Term Pose Forecasting"},{"paperId":"0426f82f0f3498479c753d4da5bd8252cab249dd","externalIds":{"DBLP":"journals/corr/abs-2306-11307","DOI":"10.48550/arXiv.2306.11307","CorpusId":260887717},"title":"Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method"},{"paperId":"269bf3ae6fb3a10ea8f2583677695edfba7e1891","externalIds":{"DBLP":"conf/iclr/2023","CorpusId":259298831},"title":"The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023"},{"paperId":"2c78cd0b02e8ec639cddf3cd79fe4f42d307718c","externalIds":{"DBLP":"conf/ksem/YangLSL23","DOI":"10.1007/978-3-031-40292-0_19","CorpusId":260888367},"title":"Recent Progress on Text Summarisation Based on BERT and GPT"},{"paperId":"5602516aa9654b17174377d3f63b7e3ca636fee5","externalIds":{"DBLP":"conf/engage/PepeLC22","DOI":"10.1007/978-3-031-30923-6_7","CorpusId":258564007},"title":"Using a Graph Transformer Network to Predict 3D Coordinates of Proteins via Geometric Algebra Modelling"},{"paperId":"2fee933a42433b807159566baefeeaa5c75bbd06","externalIds":{"DBLP":"conf/nips/MorenoWNDR22","CorpusId":258509674},"title":"Kernel Multimodal Continuous Attention"},{"paperId":"1705c81a406592d8117653c0257645624ac03ece","externalIds":{"DBLP":"conf/dasfaa/DouSNKSCY22","DOI":"10.1007/978-3-031-00129-1_4","CorpusId":248435703},"title":"Empowering Transformer with Hybrid Matching Knowledge for Entity Matching"},{"paperId":"60a6b17f28e88f17e58f60923d98674358dbd0e4","externalIds":{"DBLP":"journals/access/YeKSSW22","DOI":"10.1109/access.2022.3191784","CorpusId":250654689},"title":"A Comprehensive Survey of Graph Neural Networks for Knowledge Graphs"},{"paperId":"20333c34f892c8e0c2f4e6c37295a8b43ef35c02","externalIds":{"DBLP":"conf/emnlp/Peng0ZJ22","ACL":"2022.emnlp-main.210","DOI":"10.18653/v1/2022.emnlp-main.210","CorpusId":256460973},"title":"Rethinking Positional Encoding in Tree Transformer for Code Representation"},{"paperId":"06c5beed59a16a5c9816795e985b33900f84be62","externalIds":{"DBLP":"conf/icann/WangCC21","DOI":"10.1007/978-3-030-86362-3_21","CorpusId":237506286},"title":"EGAT: Edge-Featured Graph Attention Network"},{"paperId":"acf87283fa8ae426f1a4987b345b401bf2913f61","externalIds":{"DBLP":"conf/nips/YingCLZKHSL21","CorpusId":265104899},"title":"Do Transformers Really Perform Badly for Graph Representation?"},{"paperId":"a37e55b6bb39b50a31ac47100fb2f7ce10cc725b","externalIds":{"CorpusId":235641598},"title":"Supplementary File: Image Super-Resolution with Non-Local Sparse Attention"},{"paperId":"b548e2e6181f055a6668ceabe18a6b8b5f5c9ec6","externalIds":{"DBLP":"conf/ecir/HeFYLADTAZCCBV21","DOI":"10.1007/978-3-030-72240-1_71","CorpusId":271645867},"title":"ChEMU 2021: Reaction Reference Resolution and Anaphora Resolution in Chemical Patents"},{"paperId":"81a4fd3004df0eb05d6c1cef96ad33d5407820df","externalIds":{"DBLP":"journals/tnn/WuPCLZY21","MAG":"2907492528","ArXiv":"1901.00596","DOI":"10.1109/TNNLS.2020.2978386","CorpusId":57375753,"PubMed":"32217482"},"title":"A Comprehensive Survey on Graph Neural Networks"},{"paperId":"4be8e86de6ecd38d0b5204476d296fcc70004568","externalIds":{"DBLP":"reference/ml/Aggarwal17","DOI":"10.1007/978-1-4899-7687-1_348","CorpusId":42723902},"title":"Graph Clustering"}]}