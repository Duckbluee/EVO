{"abstract":"Autism spectrum disorder is one of the prevalent developmental disabilities, affecting 1%-4% children. Identification of children with ASD is the start of proper treatment, and electroencephalogram (EEG) can be helpful as an important neuroimaging tool. Deep learning models showed surprising performance and are expected to solve the problem, but training the model might be impractical due to the limited size of dataset. In this study, we provided a solution for self-supervised EEG pretraining, making the vision transformer (ViT) become available for the learn of internal EEG feature from large dataset, benefiting from the temporal-spectral-spatial localization and long-range correlation within the data structure. The model was pretrained using more than one billion EEG samples collected from more than 4500 participants, and then transferred for the identification of ASD children. The experiment results showed that the pretrained model can achieve an accuracy of 91.3%, and outperformed the model obtained by supervised learning. We demonstrated the big deep learning models can be well trained with the help of universal temporal-spectral-spatial data structure, and the representation can be efficiently extracted using self-supervised learning."}