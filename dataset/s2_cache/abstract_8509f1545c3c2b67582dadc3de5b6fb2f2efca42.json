{"abstract":"In the last recent years, there's a huge amount of data available on the internet, and is generated very rapidly. It is very difficult for human beings to analyze and extract useful information from huge data especially when the text is large in size and longer documents which increases the time to process and analyze the data, further it also increases the time taken to summarize it. To address this issue automatic text summarization is used. Text summarization is defined as creating a short, accurate, and fluent summary of a longer document. It summarizes the larger text without any human intervention. This paper will provide a mechanism where it does the text summarization quickly and effectively even for large data. We can use this model to summarize and extract important information from a large document or text based on our input. Here a concept of the Deep Learning model is used for text summarization which is called BART (Bidirectional and Auto- Regressive Transformer). it consists of both an encoder and decoder. The encoder and decoder are merged to form the BART algorithm. BERT is trained with a huge amount of unlabeled data to achieve the state of art results. The use of an attention mechanism in each layer of BERT makes the model much more popular as it highlights the important features of the input data. BERT performs the masked language modeling with the help of its several bidirectional transformer layers and predicts the missing values. On the other hand, decoder is used to predict the next token in a sentence. Merging both the encoder and decoder will form the BART model. Here we will compare the BART model with BERT, T5, and Roberta."}