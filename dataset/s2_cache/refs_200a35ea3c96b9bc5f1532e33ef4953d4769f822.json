{"references":[{"paperId":"74bfbbb7307a7af2686043ea97ab8b34cb062ba8","externalIds":{"ArXiv":"2310.00704","DBLP":"journals/corr/abs-2310-00704","DOI":"10.48550/arXiv.2310.00704","CorpusId":263334347},"title":"UniAudio: An Audio Foundation Model Toward Universal Audio Generation"},{"paperId":"045937ead1c3366c2d54e854d097731d426e969a","externalIds":{"DBLP":"conf/interspeech/Bai0TKS24","ArXiv":"2309.10740","DOI":"10.21437/interspeech.2024-1333","CorpusId":262054649},"title":"ConsistencyTTA: Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation"},{"paperId":"db72305f709bb73e8d641ef2d71bca7c7fc950fc","externalIds":{"DBLP":"conf/icassp/YuanLLHP024","ArXiv":"2309.08051","DOI":"10.1109/ICASSP48485.2024.10447898","CorpusId":262012867},"title":"Retrieval-Augmented Text-to-Audio Generation"},{"paperId":"763221f613adcc2f1f306fd76ab1912fcb50255b","externalIds":{"ArXiv":"2308.06873","DBLP":"journals/taslp/WangTCKECTLLY24","DOI":"10.1109/TASLP.2024.3419418","CorpusId":260887708},"title":"SpeechX: Neural Codec Language Model as a Versatile Speech Transformer"},{"paperId":"33de773be1733347a01cb07a5bb1b6cdfa956a47","externalIds":{"DBLP":"journals/corr/abs-2308-05734","ArXiv":"2308.05734","DOI":"10.1109/TASLP.2024.3399607","CorpusId":260775781},"title":"AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining"},{"paperId":"880bf72f9e2c6a99a759d3be702046e57ce2b423","externalIds":{"DBLP":"journals/corr/abs-2308-04729","ArXiv":"2308.04729","DOI":"10.1109/CAI59869.2024.00146","CorpusId":260735578},"title":"JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models"},{"paperId":"464edfd902f652d3ab6a25dbb6d9fa47cc3246a9","externalIds":{"ArXiv":"2308.01546","DBLP":"journals/corr/abs-2308-01546","DOI":"10.1109/ICASSP48485.2024.10447265","CorpusId":260438807},"title":"MusicLDM: Enhancing Novelty in text-to-music Generation Using Beat-Synchronous mixup Strategies"},{"paperId":"df6b38c4f89949891147dd19b059d2b179579ec3","externalIds":{"DBLP":"journals/spl/WangCXTW23","ArXiv":"2306.10521","DOI":"10.1109/LSP.2023.3308474","CorpusId":259204041},"title":"LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models"},{"paperId":"4cc8e18f5eece0b0d8e1abcb8ee10fb33680fbb2","externalIds":{"ArXiv":"2306.05284","DBLP":"conf/nips/CopetKGRKSAD23","DOI":"10.48550/arXiv.2306.05284","CorpusId":259108357},"title":"Simple and Controllable Music Generation"},{"paperId":"a559acac0e84319d62cefd564a5eecbf9d566ec4","externalIds":{"ArXiv":"2306.00110","DBLP":"journals/corr/abs-2306-00110","DOI":"10.48550/arXiv.2306.00110","CorpusId":258999849},"title":"MuseCoco: Generating Symbolic Music from Text"},{"paperId":"83d4b22d803ae856cf6b308482bd504fa151d39e","externalIds":{"ArXiv":"2305.18474","DBLP":"journals/corr/abs-2305-18474","DOI":"10.48550/arXiv.2305.18474","CorpusId":258968091},"title":"Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation"},{"paperId":"900509527851a4b694ecb873e96b44712a012931","externalIds":{"DBLP":"conf/icassp/WuGMR23","ArXiv":"2305.16608","DOI":"10.1109/ICASSP49357.2023.10096509","CorpusId":258532137},"title":"Audiodec: An Open-Source Streaming High-Fidelity Neural Audio Codec"},{"paperId":"38b93d01e08ab19ada89a40051a1ed4309fbe834","externalIds":{"DBLP":"conf/nips/LamT0YFTJXMSCW023","ArXiv":"2305.15719","DOI":"10.48550/arXiv.2305.15719","CorpusId":258887792},"title":"Efficient Neural Music Generation"},{"paperId":"42e726e2ea5bbb946001947d1a5b31ccc6b7aef9","externalIds":{"DBLP":"journals/corr/abs-2305-16107","ArXiv":"2305.16107","DOI":"10.48550/arXiv.2305.16107","CorpusId":258888211},"title":"VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation"},{"paperId":"2b237f1c6d40fe82b181a1a615b341f8089f5112","externalIds":{"DBLP":"journals/corr/abs-2305-12903","ArXiv":"2305.12903","DOI":"10.48550/arXiv.2305.12903","CorpusId":258833185},"title":"DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment"},{"paperId":"90831247f414c50820067ae8c2d4acdbc6c68746","externalIds":{"DBLP":"journals/corr/abs-2305-02765","ArXiv":"2305.02765","DOI":"10.48550/arXiv.2305.02765","CorpusId":258479750},"title":"HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec"},{"paperId":"3f1e6586b5a4d2f357618f36ab0cbd336e92ae17","externalIds":{"DBLP":"journals/inffus/MehrishMBMP23","ArXiv":"2305.00359","DOI":"10.1016/j.inffus.2023.101869","CorpusId":258426420},"title":"A review of deep learning techniques for speech processing"},{"paperId":"12454696085d66beaeb6cd43857de982a8445824","externalIds":{"ArXiv":"2303.11607","DBLP":"journals/corr/abs-2303-11607","DOI":"10.48550/arXiv.2303.11607","CorpusId":257636830},"title":"Transformers in Speech Processing: A Survey"},{"paperId":"c40ec51ddd4145402bd95eeb3ce6977778d87881","externalIds":{"ArXiv":"2303.03926","DBLP":"journals/corr/abs-2303-03926","DOI":"10.48550/arXiv.2303.03926","CorpusId":257378493},"title":"Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling"},{"paperId":"5aff097d33b281a51b129f600b239c4c54ddaa94","externalIds":{"ArXiv":"2302.04456","DBLP":"journals/corr/abs-2302-04456","DOI":"10.48550/arXiv.2302.04456","CorpusId":256697471},"title":"ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models"},{"paperId":"02540ae926814f4b7972d3fa4dd33932fdc4b58b","externalIds":{"ArXiv":"2302.03917","DBLP":"journals/corr/abs-2302-03917","DOI":"10.48550/arXiv.2302.03917","CorpusId":256662408},"title":"Noise2Music: Text-conditioned Music Generation with Diffusion Models"},{"paperId":"071d021ef9dcd83686d7b12bec2e13283b2f048f","externalIds":{"ArXiv":"2302.03540","DBLP":"journals/corr/abs-2302-03540","DOI":"10.1162/tacl_a_00618","CorpusId":256627687},"title":"Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision"},{"paperId":"6d1433f3342fbee85ad1e2809e62734aec5c3853","externalIds":{"DBLP":"journals/corr/abs-2301-12661","ArXiv":"2301.12661","DOI":"10.48550/arXiv.2301.12661","CorpusId":256390046},"title":"Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"},{"paperId":"c5eb040156adf687d7b9a97b3b16f6f243a1a514","externalIds":{"ArXiv":"2301.11757","CorpusId":264439679},"title":"Mo\\^usai: Text-to-Music Generation with Long-Context Latent Diffusion"},{"paperId":"c2f91f35df893714418cc29096083dce0b441229","externalIds":{"ArXiv":"2301.02111","DBLP":"journals/corr/abs-2301-02111","DOI":"10.1109/TASLPRO.2025.3530270","CorpusId":255440307},"title":"Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"},{"paperId":"14099f55331eaac6a3be6c0ccf00e317e4fb0014","externalIds":{"DBLP":"journals/corr/abs-2212-04559","ArXiv":"2212.04559","DOI":"10.1109/ICASSP49357.2023.10095710","CorpusId":254535560},"title":"Speechlmscore: Evaluating Speech Generation Using Speech Language Model"},{"paperId":"b8aac9e0a0e9afb6d41dfdf912d36de04f3a0d50","externalIds":{"DBLP":"conf/icassp/KangMH23","ArXiv":"2211.09383","DOI":"10.1109/ICASSP49357.2023.10095515","CorpusId":253581601},"title":"Grad-StyleSpeech: Any-Speaker Adaptive Text-to-Speech Synthesis with Diffusion Models"},{"paperId":"a070666da8a25c563b4fa1bc00311ed2960bf22f","externalIds":{"ArXiv":"2211.03089","DBLP":"journals/corr/abs-2211-03089","DOI":"10.1109/ICASSP49357.2023.10096023","CorpusId":253384594},"title":"I Hear Your True Colors: Image Guided Audio Generation"},{"paperId":"c4494280e557ae1a5d4c34c854c4350f4ea3c912","externalIds":{"DBLP":"journals/corr/abs-2210-10349","ArXiv":"2210.10349","DOI":"10.48550/arXiv.2210.10349","CorpusId":252993148},"title":"Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation"},{"paperId":"8c870bef01a4fbb20f60722ffc2f6bee3870b18b","externalIds":{"DBLP":"journals/corr/abs-2209-03143","ArXiv":"2209.03143","DOI":"10.1109/TASLP.2023.3288409","CorpusId":252111134},"title":"AudioLM: A Language Modeling Approach to Audio Generation"},{"paperId":"c036f75da24ba64a583e0b6d41c5b792347bffa6","externalIds":{"ArXiv":"2207.09983","DBLP":"journals/taslp/YangYWWWZY23","DOI":"10.1109/TASLP.2023.3268730","CorpusId":250698823},"title":"Diffsound: Discrete Diffusion Model for Text-to-Sound Generation"},{"paperId":"b0a48fcb93c42085ab92289f9216d47e61c34b9f","externalIds":{"ArXiv":"2207.04646","DBLP":"journals/corr/abs-2207-04646","DOI":"10.48550/arXiv.2207.04646","CorpusId":250425685},"title":"DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders"},{"paperId":"d8e6ee480b6b3371bd9385ed05b8b915a87362b7","externalIds":{"DBLP":"conf/ismir/HawthorneSRZGME22","ArXiv":"2206.05408","DOI":"10.48550/arXiv.2206.05408","CorpusId":249626501},"title":"Multi-instrument Music Synthesis with Spectrogram Diffusion"},{"paperId":"764c9b5da16278cbbb903743b6dc110d2ad4a5e5","externalIds":{"ArXiv":"2205.15370","DBLP":"journals/corr/abs-2205-15370","DOI":"10.48550/arXiv.2205.15370","CorpusId":249209915},"title":"Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data"},{"paperId":"d6572f2250337975fc4a54abfcdf08b4829b2cfc","externalIds":{"ArXiv":"2204.02967","DBLP":"journals/corr/abs-2204-02967","DOI":"10.48550/arXiv.2204.02967","CorpusId":247996514},"title":"Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation"},{"paperId":"12070fcac88f42d8ad67f7a3ead60e552b7f05af","externalIds":{"DBLP":"journals/corr/abs-2204-00604","ArXiv":"2204.00604","DOI":"10.48550/arXiv.2204.00604","CorpusId":247922422},"title":"Quantized GAN for Complex Music Generation from Dance Videos"},{"paperId":"5a29401272d58c20bcd95728f646c186f62b6c48","externalIds":{"DBLP":"journals/tacl/NguyenKCAHETASM23","ArXiv":"2203.16502","ACL":"2023.tacl-1.15","DOI":"10.1162/tacl_a_00545","CorpusId":247792843},"title":"Generative Spoken Dialogue Language Modeling"},{"paperId":"36131ad0b37dd1c759cb48bc316799ac5095d76b","externalIds":{"DBLP":"journals/tcsv/LiuLC22","MAG":"3163658555","DOI":"10.1109/TCSVT.2021.3079897","CorpusId":236733045},"title":"Towards an End-to-End Visual-to-Raw-Audio Generation With GAN"},{"paperId":"12809bcb734beafeb47876f42e7b438e27fe99fe","externalIds":{"DBLP":"journals/corr/abs-2202-07765","ArXiv":"2202.07765","CorpusId":246867214},"title":"General-purpose, long-context autoregressive modeling with Perceiver AR"},{"paperId":"5c333f11431d1f0d04ced62b712c8d05ebac0891","externalIds":{"DBLP":"journals/corr/abs-2202-05256","ArXiv":"2202.05256","DOI":"10.1109/ICASSP43922.2022.9746901","CorpusId":246706245},"title":"Conditional Diffusion Probabilistic Model for Speech Enhancement"},{"paperId":"f2f9c2b09b6f6005c96e1968b8069102e49cafef","externalIds":{"DBLP":"conf/icml/KimKY22","ArXiv":"2111.11755","CorpusId":246430592},"title":"Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance"},{"paperId":"581ac1e3568b8d632c2cdbef3bf91acb803b78f2","externalIds":{"ArXiv":"2111.05011","DBLP":"journals/corr/abs-2111-05011","CorpusId":243860746},"title":"RAVE: A variational autoencoder for fast and high-quality neural audio synthesis"},{"paperId":"f09b4ab3f10885fa9b881aa7e7772e62ad77de6b","externalIds":{"ArXiv":"2201.02490","DBLP":"conf/aiccsa/NatsiouO21","DOI":"10.1109/AICCSA53542.2021.9686838","CorpusId":245827795},"title":"Audio representations for deep learning in sound synthesis: A review"},{"paperId":"f9fff0d57ea37ec2a3a47bdbb8ced605e43f6b87","externalIds":{"ArXiv":"2111.08380","DBLP":"journals/corr/abs-2111-08380","DOI":"10.1145/3474085.3475195","CorpusId":239011657},"title":"Video Background Music Generation with Controllable Music Transformer"},{"paperId":"1067c44e473b6998f89e13f0d4c0de730def43f0","externalIds":{"DBLP":"journals/corr/abs-2110-07205","ACL":"2022.acl-long.393","ArXiv":"2110.07205","DOI":"10.18653/v1/2022.acl-long.393","CorpusId":238856828},"title":"SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing"},{"paperId":"fd3d85900c0693c069425a0e461667cbff59c685","externalIds":{"ArXiv":"2109.06441","DBLP":"journals/corr/abs-2109-06441","DOI":"10.1145/3503161.3548084","CorpusId":237502954},"title":"Structure-Enhanced Pop Music Generation via Harmony-Aware Learning"},{"paperId":"e50029c6ae14f11ed6212fbaa929a000a6d38219","externalIds":{"DBLP":"journals/corr/abs-2108-00443","ArXiv":"2108.00443","CorpusId":236772630},"title":"A Survey on Audio Synthesis and Audio-Visual Multimodal Processing"},{"paperId":"553b74de8cb7ebca42a686e2a3a2d6aae170946e","externalIds":{"ArXiv":"2107.11876","DBLP":"journals/corr/abs-2107-11876","CorpusId":236428748},"title":"A Study on Speech Enhancement Based on Diffusion Probabilistic Model"},{"paperId":"59a0ef2d3bccebb544a2df4ad0453d49cc8e731f","externalIds":{"DBLP":"journals/taslp/ZeghidourLOST22","ArXiv":"2107.03312","DOI":"10.1109/taslp.2021.3129994","CorpusId":236149944},"title":"SoundStream: An End-to-End Neural Audio Codec"},{"paperId":"27e5acd4042ec83d2c53d90bd1fd1e7d10e611c6","externalIds":{"ArXiv":"2106.16036","DBLP":"journals/corr/abs-2106-16036","DOI":"10.23919/DAFx51585.2021.9768298","CorpusId":235683315},"title":"A Generative Model for Raw Audio Using Transformer Architectures"},{"paperId":"4384ff4ac7459d3045ff660b1772c975512701d9","externalIds":{"DBLP":"journals/corr/abs-2106-15561","ArXiv":"2106.15561","CorpusId":235669930},"title":"A Survey on Neural Speech Synthesis"},{"paperId":"e38f1ce53fd04e62dfbf5a09e8ba28989395fc85","externalIds":{"DBLP":"journals/corr/abs-2106-06426","ArXiv":"2106.06426","CorpusId":235417485},"title":"Catch-A-Waveform: Learning to Generate Audio from a Single Short Example"},{"paperId":"2e32cde6e080f990873638f2e113767a6a19c824","externalIds":{"DBLP":"journals/corr/abs-2105-06337","ArXiv":"2105.06337","CorpusId":234483016},"title":"Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech"},{"paperId":"31c34698f3917af07113657795e40df336e7a1c8","externalIds":{"ArXiv":"2105.03070","MAG":"3161940574","DBLP":"journals/corr/abs-2105-03070","CorpusId":234095572},"title":"SpeechNet: A Universal Modularized Model for Speech Processing Tasks"},{"paperId":"fe92f3f7ceec008118842d42b578dc25bcba63f9","externalIds":{"ArXiv":"2105.02446","DBLP":"conf/aaai/Liu00CZ22","DOI":"10.1609/aaai.v36i10.21350","CorpusId":235262772},"title":"DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism"},{"paperId":"844cd260a3ca9de92fa1217c146d8cda2e0c10c0","externalIds":{"ArXiv":"2104.01409","DBLP":"journals/corr/abs-2104-01409","DOI":"10.21437/interspeech.2021-469","CorpusId":233025015},"title":"Diff-TTS: A Denoising Diffusion Model for Text-to-Speech"},{"paperId":"4c055698c68b21c4e8f25a5c1b3439f9f7f3be62","externalIds":{"DBLP":"conf/iclr/Chen0LLQZL21","ArXiv":"2103.00993","CorpusId":232075892},"title":"AdaSpeech: Adaptive Text to Speech for Custom Voice"},{"paperId":"34bf13e58c7226d615afead0c0f679432502940e","externalIds":{"MAG":"3087665158","DBLP":"conf/iclr/KongPHZC21","ArXiv":"2009.09761","CorpusId":221818900},"title":"DiffWave: A Versatile Diffusion Model for Audio Synthesis"},{"paperId":"7726349b6f2d5104a14964bbb1a444b42ca554c0","externalIds":{"MAG":"3095095816","DBLP":"journals/corr/abs-2009-02095","ArXiv":"2009.02095","DOI":"10.21437/interspeech.2020-1563","CorpusId":221507521},"title":"SEANet: A Multi-modal Speech Enhancement Network"},{"paperId":"5e5ffb13182094a2d131ffb2996ed21980b1c1b0","externalIds":{"MAG":"3049186735","ArXiv":"2008.06867","DBLP":"journals/corr/abs-2008-06867","DOI":"10.21437/interspeech.2020-1226","CorpusId":221139316},"title":"Audio Dequantization for High Fidelity Audio Generation in Flow-based Neural Vocoder"},{"paperId":"d6741241efb9ffd933df974b43d7109c72238371","externalIds":{"ArXiv":"2008.06048","DBLP":"journals/corr/abs-2008-06048","MAG":"3049272330","CorpusId":221218888},"title":"MMM : Exploring Conditional Multi-Track Music Generation with the Transformer"},{"paperId":"348c6bc43028f4b08f8a5e085477b10f4d01e515","externalIds":{"DBLP":"journals/corr/abs-2007-15474","ArXiv":"2007.15474","MAG":"3045523713","CorpusId":220870781},"title":"Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling"},{"paperId":"d0ed8a26aa689992bd0e870df9e428987ea2a4e2","externalIds":{"DBLP":"journals/corr/abs-2008-00820","MAG":"3046757598","ArXiv":"2008.00820","DOI":"10.1109/TIP.2020.3009820","CorpusId":220936390,"PubMed":"32746241"},"title":"Generating Visually Aligned Sound From Videos"},{"paperId":"25292353e1e87a7d013386d3aa4898586af2e018","externalIds":{"ArXiv":"2006.14348","MAG":"3103837916","DBLP":"conf/nips/SuLS20","CorpusId":220055934},"title":"Audeo: Audio Generation for a Silent Performance Video"},{"paperId":"2e1107db4731714230b25efa9f7a8787e6e115d8","externalIds":{"DBLP":"conf/icassp/Lancucki21","MAG":"3034949308","ArXiv":"2006.06873","DOI":"10.1109/ICASSP39728.2021.9413889","CorpusId":219635877},"title":"Fastpitch: Parallel Text-to-Speech with Pitch Prediction"},{"paperId":"d4752d70b2794248a748f3f53a73960c2f70dd03","externalIds":{"MAG":"3034581551","ArXiv":"2006.06426","DBLP":"journals/corr/abs-2006-06426","DOI":"10.1007/978-3-030-72116-9_22","CorpusId":219573542},"title":"Deep generative models for musical audio synthesis"},{"paperId":"b193d2284799e47a4fcd2b3dee75fcebce1c7382","externalIds":{"DBLP":"journals/corr/abs-2006-04598","ArXiv":"2006.04598","MAG":"3033752673","CorpusId":219531449},"title":"WaveNODE: A Continuous Normalizing Flow for Speech Synthesis"},{"paperId":"c035cf0e1231eb196968d7255ab55827e932ec7a","externalIds":{"MAG":"3033913438","DBLP":"journals/corr/abs-2006-03575","ArXiv":"2006.03575","CorpusId":219401642},"title":"End-to-End Adversarial Text-to-Speech"},{"paperId":"b19d580ba43d8033ecb485115bcbe21ca4fda157","externalIds":{"DBLP":"journals/access/HaqueRS20","MAG":"3109897136","ArXiv":"2006.00877","DOI":"10.1109/ACCESS.2020.3040797","CorpusId":219177435},"title":"High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder"},{"paperId":"2ed0d4931d89528bd53482a0b6587ebcbba6d096","externalIds":{"DBLP":"journals/corr/abs-2005-11129","MAG":"3026874504","ArXiv":"2005.11129","CorpusId":218862956},"title":"Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search"},{"paperId":"46a34e4239adfe222bdad453821372904b5d0b40","externalIds":{"DBLP":"conf/interspeech/LiuCYY20","ArXiv":"2005.08526","MAG":"3096725451","DOI":"10.21437/Interspeech.2020-1137","CorpusId":218674026},"title":"Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization"},{"paperId":"0170fc76e934ee643f869df18fb617d5357e8b4e","externalIds":{"MAG":"3025165719","DBLP":"conf/interspeech/GulatiQCPZYHWZW20","ArXiv":"2005.08100","DOI":"10.21437/interspeech.2020-3015","CorpusId":218674528},"title":"Conformer: Convolution-augmented Transformer for Speech Recognition"},{"paperId":"2899d16c4c956ad1850d42e9c6ba50d1f14b4e7a","externalIds":{"DBLP":"journals/corr/abs-2005-07799","MAG":"3025793647","ArXiv":"2005.07799","DOI":"10.21437/interspeech.2020-2123","CorpusId":218674539},"title":"JDI-T: Jointly trained Duration Informed Transformer for Text-To-Speech without Explicit Alignment"},{"paperId":"cc22c4e54c0dd381a2ff22881d4fb570cf3761e8","externalIds":{"MAG":"2992005611","DBLP":"journals/pami/KobyzevPB21","DOI":"10.1109/TPAMI.2020.2992934","CorpusId":208910764,"PubMed":"32396070"},"title":"Normalizing Flows: An Introduction and Review of Current Methods"},{"paperId":"67dea28495cab71703993d0d52ca4733b9a66077","externalIds":{"DBLP":"journals/corr/abs-2005-00341","ArXiv":"2005.00341","MAG":"3021164770","CorpusId":218470180},"title":"Jukebox: A Generative Model for Music"},{"paperId":"39bb6a13c34e2301373ecd35f5f4d31c8ff6e763","externalIds":{"MAG":"2997540646","DBLP":"conf/aaai/LiLW0ZL20","DOI":"10.1609/AAAI.V34I05.6337","CorpusId":214161830},"title":"RobuTrans: A Robust Transformer-Based Text-to-Speech Model"},{"paperId":"ce510c6cfeac703706460680e977c54554840830","externalIds":{"MAG":"3092879656","DBLP":"conf/mm/HuangY20","DOI":"10.1145/3394171.3413671","CorpusId":220919638},"title":"Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions"},{"paperId":"65d53938a12c77e7920b8eb3a49df249c978ba3f","externalIds":{"DBLP":"journals/taslp/KongCIWWP20","MAG":"3094550259","ArXiv":"1912.10211","DOI":"10.1109/TASLP.2020.3030497","CorpusId":209444382},"title":"PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition"},{"paperId":"37e52ff4714c7a08900b518127e438a195b84611","externalIds":{"DBLP":"journals/corr/abs-1910-06711","ArXiv":"1910.06711","MAG":"2980709326","CorpusId":202777813},"title":"MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis"},{"paperId":"85381e0e89e98ea1ed8bd3dfa533d911263757df","externalIds":{"DBLP":"journals/corr/abs-1909-11646","MAG":"2996286887","ArXiv":"1909.11646","CorpusId":202749904},"title":"High Fidelity Speech Synthesis with Adversarial Networks"},{"paperId":"24a70db0bbb5f486126477e32a6a44ab917a4b11","externalIds":{"DBLP":"journals/corr/abs-1907-04868","MAG":"2959020461","ArXiv":"1907.04868","CorpusId":195886341},"title":"LakhNES: Improving Multi-instrumental Music Generation with Cross-domain Pre-training"},{"paperId":"329b84a919bfd1771be5bd14fa81e7b3f74cc961","externalIds":{"MAG":"2948978827","DBLP":"journals/corr/abs-1906-02691","ArXiv":"1906.02691","DOI":"10.1561/2200000056","CorpusId":174802445},"title":"An Introduction to Variational Autoencoders"},{"paperId":"50970f392a76ced3054703a21d581377f1cc1086","externalIds":{"ArXiv":"1905.11449","DBLP":"conf/interspeech/TjandraS0S0019","MAG":"2972374322","DOI":"10.21437/interspeech.2019-3232","CorpusId":167217584},"title":"VQVAE Unsupervised Unit Discovery and Multi-scale Code2Spec Inverter for Zerospeech Challenge 2019"},{"paperId":"018df854ede0b93b8efc34ba72e57e6b4698e883","externalIds":{"MAG":"2936900244","DBLP":"conf/icassp/LeeLHH19","DOI":"10.1109/ICASSP.2019.8682513","CorpusId":145997862},"title":"Audio Feature Generation for Missing Modality Problem in Video Action Recognition"},{"paperId":"3b978703968c2e3f8a41b0d34f870bfc2228677f","externalIds":{"DBLP":"conf/interspeech/NeekharaDPDM19","MAG":"2973203693","ArXiv":"1904.07944","DOI":"10.21437/interspeech.2019-3099","CorpusId":118705627},"title":"Expediting TTS Synthesis with Adversarial Vocoding"},{"paperId":"2f6e03d60bcd38c76811463eb653e0d5012b9480","externalIds":{"DBLP":"conf/interspeech/YamamotoSK19","MAG":"2938410884","ArXiv":"1904.04472","DOI":"10.21437/interspeech.2019-1965","CorpusId":104291809},"title":"Probability density distillation with generative adversarial networks for high-quality parallel waveform generation"},{"paperId":"8e15ce5a1d38d3d17287e9e191a5f47a50ec4771","externalIds":{"DBLP":"journals/corr/abs-1905-00078","ArXiv":"1905.00078","MAG":"2931364255","DOI":"10.1109/JSTSP.2019.2908700","CorpusId":132199224},"title":"Deep Learning for Audio Signal Processing"},{"paperId":"3db2fb5789a48d3a584b586cc24ef0a3e9bdd8cf","externalIds":{"ArXiv":"1902.04072","DBLP":"conf/icml/MarafiotiPHM19","MAG":"2952521611","CorpusId":60441249},"title":"Adversarial Generation of Time-Frequency Features with application in audio synthesis"},{"paperId":"54dd66fbc59fbdfbd4cb29b966287e0b6982b703","externalIds":{"MAG":"2909606161","DOI":"10.5120/IJCA2019918334","CorpusId":68057907},"title":"Audio Enhancement and Synthesis using Generative Adversarial Networks: A Survey"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","externalIds":{"ArXiv":"1901.02860","DBLP":"conf/acl/DaiYYCLS19","MAG":"2964110616","ACL":"P19-1285","DOI":"10.18653/v1/P19-1285","CorpusId":57759363},"title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"7e9960952cac06593500ef91aad8aa2facdf339f","externalIds":{"MAG":"2951815739","ArXiv":"1811.02155","DBLP":"journals/corr/abs-1811-02155","CorpusId":53249313},"title":"FloWaveNet : A Generative Flow for Raw Audio"},{"paperId":"220d47d9b2872f0f0c33084fb4425424d91fa0ac","externalIds":{"MAG":"2949580977","DBLP":"conf/icassp/PrengerVC19","ArXiv":"1811.00002","DOI":"10.1109/ICASSP.2019.8683143","CorpusId":53145796},"title":"Waveglow: A Flow-based Generative Network for Speech Synthesis"},{"paperId":"9f2dd5cc190fc713f1339fca838a5537931744f8","externalIds":{"MAG":"2951418500","DBLP":"conf/aaai/Li0LZL19","DOI":"10.1609/AAAI.V33I01.33016706","CorpusId":59413863},"title":"Neural Speech Synthesis with Transformer Network"},{"paperId":"5e3a59695261f03aa3f09a8a5ac6166fb63e0a2e","externalIds":{"MAG":"2950942634","DBLP":"conf/iclr/PingPC19","ArXiv":"1807.07281","CorpusId":49882757},"title":"ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech"},{"paperId":"32bfa6418f4908f3c5eff08d76bcf33a90ce2a9c","externalIds":{"MAG":"2811079561","ArXiv":"1806.10474","DBLP":"conf/nips/DielemanOS18","CorpusId":49473209},"title":"The challenge of realistic music generation: modelling raw audio at scale"},{"paperId":"cb3e87412d2fa52441e40bff3db2135dec9de3b9","externalIds":{"MAG":"2788357188","DBLP":"conf/nips/ArikCPPZ18","ArXiv":"1802.06006","CorpusId":4013011},"title":"Neural Voice Cloning with a Few Samples"},{"paperId":"e834460d2cec8c24d7ade9364f68d2debcf994e4","externalIds":{"ArXiv":"1711.08102","DBLP":"conf/aaai/HaoZG18","MAG":"2952668408","DOI":"10.1609/aaai.v32i1.12329","CorpusId":9188911},"title":"CMCGAN: A Uniform Framework for Cross-Modal Visual-Audio Mutual Generation"},{"paperId":"3b70a53e40dc414a8845070c905ce30ba2f12c6e","externalIds":{"DBLP":"conf/eusipco/ChoiFSC18","MAG":"2964285380","ArXiv":"1709.01922","DOI":"10.23919/EUSIPCO.2018.8553106","CorpusId":27763034},"title":"A Comparison of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging"},{"paperId":"6a70f88b77a88db4102af25aadd66952944ac5d7","externalIds":{"MAG":"2752134738","DBLP":"journals/corr/abs-1709-01620","ArXiv":"1709.01620","CorpusId":12211154},"title":"Deep Learning Techniques for Music Generation - A Survey"},{"paperId":"36b3865f944c74c6d782c26dfe7be04ef9664a67","externalIds":{"ArXiv":"1709.01703","MAG":"2963908889","DBLP":"conf/interspeech/MichelsantiT17","DOI":"10.21437/Interspeech.2017-1620","CorpusId":11049683},"title":"Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification"},{"paperId":"27e8965cc9c166e9afee46e611039f0ce8263e51","externalIds":{"MAG":"2611160234","DBLP":"conf/mm/ChenSDX17","ArXiv":"1704.08292","DOI":"10.1145/3126686.3126723","CorpusId":1407794},"title":"Deep Cross-Modal Audio-Visual Generation"},{"paperId":"1fa6ba95b8383fad600bcbd6033c6eec73296381","externalIds":{"MAG":"2746068898","DBLP":"journals/corr/YangCY17","ArXiv":"1703.10847","CorpusId":2002865},"title":"MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation"},{"paperId":"f8d43ff00585c53f65eb04e15477113c2d2b758b","externalIds":{"MAG":"2950527469","DBLP":"journals/corr/PascualBS17","ArXiv":"1703.09452","DOI":"10.21437/Interspeech.2017-1428","CorpusId":12054873},"title":"SEGAN: Speech Enhancement Generative Adversarial Network"},{"paperId":"f01efc7e1515243b8b80a3b2230b6bc6f6247ef3","externalIds":{"DBLP":"journals/corr/Mogren16","MAG":"2559110679","ArXiv":"1611.09904","CorpusId":259842},"title":"C-RNN-GAN: Continuous recurrent neural networks with adversarial training"},{"paperId":"74ff6d48f9c62e937023106629d27ef2d2ddf8bc","externalIds":{"ArXiv":"1611.04076","MAG":"2949496494","DBLP":"conf/iccv/MaoLXLWS17","DOI":"10.1109/ICCV.2017.304","CorpusId":206771128},"title":"Least Squares Generative Adversarial Networks"},{"paperId":"2a20ed3ddcd134dc9222c92ba82055c098d03170","externalIds":{"ArXiv":"1611.03477","DBLP":"conf/iclr/ChuUF17","MAG":"2953100410","CorpusId":9651443},"title":"Song From PI: A Musically Plausible Network for Pop Music Generation"},{"paperId":"e221e2c2ca8bd74a7b818406c8a2a342760e7d65","externalIds":{"MAG":"2953331651","DBLP":"conf/iclr/MehriKGKJSCB17","ArXiv":"1612.07837","CorpusId":14254027},"title":"SampleRNN: An Unconditional End-to-End Neural Audio Generation Model"},{"paperId":"59d8c68de09da69a608ceb149f40114f5538c5b1","externalIds":{"DBLP":"conf/icassp/HersheyCEGJMPPS17","ArXiv":"1609.09430","MAG":"2526050071","DOI":"10.1109/ICASSP.2017.7952132","CorpusId":8810481},"title":"CNN architectures for large-scale audio classification"},{"paperId":"df0402517a7338ae28bc54acaac400de6b456a46","externalIds":{"MAG":"2519091744","ArXiv":"1609.03499","DBLP":"journals/corr/OordDZSVGKSK16","CorpusId":6254678},"title":"WaveNet: A Generative Model for Raw Audio"},{"paperId":"64e316c8125742e02e7b8bb5094af3c17b96735e","externalIds":{"DBLP":"conf/interspeech/WangXX16","MAG":"2515943672","DOI":"10.21437/Interspeech.2016-134","CorpusId":35521543},"title":"First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention"},{"paperId":"ae72084f3d76acf3e9605fdbe7203ef77c0080c0","externalIds":{"MAG":"2494654097","DBLP":"conf/interspeech/LiZ16","DOI":"10.21437/Interspeech.2016-172","CorpusId":7797713},"title":"Multi-Language Multi-Speaker Acoustic Modeling for LSTM-RNN Based Statistical Parametric Speech Synthesis"},{"paperId":"4aaeb6ab0aacfefd9e2f89966584b95b66a93177","externalIds":{"MAG":"2949286682","DBLP":"journals/corr/PrusaBS16","ArXiv":"1609.00291","DOI":"10.1109/TASLP.2017.2678166","CorpusId":440045},"title":"A Noniterative Method for Reconstruction of Phase From STFT Magnitude"},{"paperId":"1e3f83362854a26e26c8ee2672923b08ee558271","externalIds":{"DBLP":"conf/icassp/WangXX16","MAG":"2399853303","DOI":"10.1109/ICASSP.2016.7472733","CorpusId":9676766},"title":"Gating recurrent mixture density networks for acoustic modeling in statistical parametric speech synthesis"},{"paperId":"629380a24c3fc7c26e96415f5cf8d6496b2d1c38","externalIds":{"DBLP":"conf/icassp/TokudaZ16","MAG":"2338186431","DOI":"10.1109/ICASSP.2016.7472757","CorpusId":6445111},"title":"Directly modeling voiced and unvoiced components in speech waveforms by neural networks"},{"paperId":"584f997151cf68cb9a6f3aff088fc7dc3a723faf","externalIds":{"DBLP":"conf/icdsp/BeauregardHW15","MAG":"1548018871","DOI":"10.1109/ICDSP.2015.7251907","CorpusId":14396435},"title":"Single Pass Spectrogram Inversion"},{"paperId":"dc96e329dc7b9d29aa8b2f1a44e8928b6977456a","externalIds":{"MAG":"1916501714","DBLP":"conf/interspeech/YaoZ15","ArXiv":"1506.00196","DOI":"10.21437/Interspeech.2015-134","CorpusId":5665974},"title":"Sequence-to-sequence neural net models for grapheme-to-phoneme conversion"},{"paperId":"5a48fed28e6224855289d4a581b22f559b3da196","externalIds":{"DBLP":"conf/icassp/ZenS15","MAG":"1576227399","DOI":"10.1109/ICASSP.2015.7178816","CorpusId":1463476},"title":"Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis"},{"paperId":"5ffe28da9dd002b60a8c7bc0bf5fba14f6e6a750","externalIds":{"DBLP":"conf/icassp/RaoPSB15","MAG":"1593247906","DOI":"10.1109/ICASSP.2015.7178767","CorpusId":9770841},"title":"Grapheme-to-phoneme conversion using Long Short-Term Memory recurrent neural networks"},{"paperId":"7bf053b97e0cce85aa47f6597cd5ec116333c4f8","externalIds":{"MAG":"1527535554","DBLP":"conf/icassp/TokudaZ15","DOI":"10.1109/ICASSP.2015.7178765","CorpusId":16805640},"title":"Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis"},{"paperId":"846996945fc33abebdcbeb92fe2fe88afb92c47e","externalIds":{"MAG":"1492383498","DBLP":"conf/icassp/FanQSH15","DOI":"10.1109/ICASSP.2015.7178817","CorpusId":16559231},"title":"Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis"},{"paperId":"849c9a0b3c76c7e65e3e5f0bdfd56921731ea043","externalIds":{"MAG":"2240622083","DBLP":"journals/spm/LingKZSSQMD15","DOI":"10.1109/MSP.2014.2359987","CorpusId":17058256},"title":"Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends"},{"paperId":"8517b1a43a74fca82b549bcc9bd907bea2b1e4e4","externalIds":{"MAG":"2063105057","DBLP":"journals/ieicet/NakamuraHNT14","DOI":"10.1587/TRANSINF.E97.D.1438","CorpusId":35127305},"title":"Integration of Spectral Feature Extraction and Modeling for HMM-Based Speech Synthesis"},{"paperId":"7bac34a642edd63d84ab862dd8481fb5c00a6de5","externalIds":{"DBLP":"conf/icassp/ZenS14","MAG":"1990505856","DOI":"10.1109/ICASSP.2014.6854321","CorpusId":14238135},"title":"Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis"},{"paperId":"2aaa97f8582bf56b5adab25acf02ab9cc9ce5b11","externalIds":{"DBLP":"journals/taslp/LingDY13","MAG":"2020024436","DOI":"10.1109/TASL.2013.2269291","CorpusId":16501738},"title":"Modeling Spectral Envelopes Using Restricted Boltzmann Machines and Deep Belief Networks for Statistical Parametric Speech Synthesis"},{"paperId":"3839aae244763195f1437e7c00d2c6c8f5e3b549","externalIds":{"MAG":"2103498773","DBLP":"journals/jair/FernandezV13","ArXiv":"1402.0585","DOI":"10.1613/jair.3908","CorpusId":8822245},"title":"AI Methods in Algorithmic Composition: A Comprehensive Survey"},{"paperId":"0ecde39276771f1e2463f098a78c1daf9d244547","externalIds":{"MAG":"1571950845","DBLP":"conf/ssw/LuKW13","CorpusId":6558003},"title":"Combining a vector space representation of linguistic context with a deep neural network for text-to-speech synthesis"},{"paperId":"20969e35837d93b6e4eb52d75c8713abc6069f4a","externalIds":{"MAG":"2102003408","DBLP":"conf/icassp/ZeSS13","DOI":"10.1109/ICASSP.2013.6639215","CorpusId":16664621},"title":"Statistical parametric speech synthesis using deep neural networks"},{"paperId":"07c43a3ff15f2104022f2b1ca8ec4128a930b414","externalIds":{"MAG":"1819710477","DBLP":"conf/icml/Boulanger-LewandowskiBV12","ArXiv":"1206.6392","CorpusId":175089},"title":"Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription"},{"paperId":"1d43e2f9ca0656b9caaf9823c29951b9ab366dc9","externalIds":{"DBLP":"books/sp/21/PeetersR21","DOI":"10.1007/978-3-030-74478-6_10","CorpusId":232215918},"title":"Deep Learning for Audio and Music"},{"paperId":"dec93166b31ba2ed2a51ff3f578e128edfd4dcbf","externalIds":{"MAG":"2568428461","DBLP":"books/lib/Benesty08","DOI":"10.1121/1.3203918","CorpusId":29011535},"title":"Springer handbook of speech processing"},{"paperId":"317d2b6e97b878e77f8aad964575bcaadddb83cf","externalIds":{"DBLP":"journals/speech/ZenTB09","MAG":"2129142580","DOI":"10.1016/j.specom.2009.04.004","CorpusId":3232238},"title":"Statistical Parametric Speech Synthesis"},{"paperId":"8c2fcaaf3153779ec3e838b416cd6e6d7feecdb9","externalIds":{"MAG":"160196362","DBLP":"conf/evoW/WorthS05","DOI":"10.1007/978-3-540-32003-6_56","CorpusId":7296027},"title":"Growing Music: Musical Interpretations of L-Systems"},{"paperId":"6fbd89f915ae5fd06e8e105d30132372b5433c0e","externalIds":{"MAG":"2020166869","DBLP":"conf/mm/LavrenkoP03","DOI":"10.1145/957013.957041","CorpusId":120851},"title":"Polyphonic music modeling with random fields"},{"paperId":"c135eede17f30b901275057553d37c07c0b90198","externalIds":{"MAG":"134527144","CorpusId":8155744},"title":"A First Look at Music Composition using LSTM Recurrent Neural Networks"},{"paperId":"3d606dac6dbd9fbb9229538f2f3c150c663da0cb","externalIds":{"MAG":"2753779906","DOI":"10.1162/096112100570602","CorpusId":287936},"title":"Vox Populi: An Interactive Evolutionary System for Algorithmic Music Composition"},{"paperId":"518c3867f4272fd46fc5e8ecf1eb4f6af517c698","externalIds":{"MAG":"2049686551","DBLP":"journals/speech/KawaharaMC99","DOI":"10.1016/S0167-6393(98)00085-5","CorpusId":18151124},"title":"Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds"},{"paperId":"d8cc6ab83d2a171bc054cd37e441e54fe07b0b22","externalIds":{"MAG":"1882624840","DBLP":"journals/corr/cs-NE-9811031","ArXiv":"cs/9811031","CorpusId":607570},"title":"Speech Synthesis with Neural Networks"},{"paperId":"3a069c56e05db4dcdbc8072d1795568443f3b439","externalIds":{"DBLP":"conf/interspeech/KaraaliCGM97","MAG":"2953037075","ArXiv":"cs/9811032","DOI":"10.21437/Eurospeech.1997-209","CorpusId":1267},"title":"Text-to-speech conversion with neural networks: a recurrent TDNN approach"},{"paperId":"7feecfdabbdc2c7babc7e0d148ecfa8ba0b6ccfe","externalIds":{"MAG":"2568536086","CorpusId":63719729},"title":"Speech Synthesis by Rule."},{"paperId":"1dd0140d51e870a713340ae30734c8438b03d1a3","externalIds":{"MAG":"2150658333","DBLP":"conf/icassp/HuntB96","DOI":"10.1109/ICASSP.1996.541110","CorpusId":14621185},"title":"Unit selection in a concatenative speech synthesis system using a large speech database"},{"paperId":"149208d30dec28c30272519ecf8190f89db60fe7","externalIds":{"MAG":"2128009939","DOI":"10.1109/SIPNN.1994.344901","CorpusId":62409073},"title":"A hybrid neural network/rule based architecture for diphone speech synthesis"},{"paperId":"432902293c8d1cb2eb132116df17bd9ad64a79e0","externalIds":{"MAG":"2100167690","DBLP":"conf/icnn/WeijtersT93","DOI":"10.1109/ICNN.1993.298824","CorpusId":62527041},"title":"Speech synthesis with artificial neural networks"},{"paperId":"dc6141a9fbbff988aca757d8f3656c87ead45b9c","externalIds":{"MAG":"66734461","CorpusId":59695883},"title":"Cybernetic composer: an overview"},{"paperId":"267db38c45234d7fd0030ce68785d4889aec10e7","externalIds":{"MAG":"2115040572","DBLP":"conf/icassp/Sagisaka88","DOI":"10.1109/ICASSP.1988.196677","CorpusId":62244548},"title":"Speech synthesis by rule using an optimal selection of non-uniform synthesis units"},{"paperId":"68ccad4a17ab2de4b43527c2951e5c9f9315742d","externalIds":{"MAG":"2320263333","DOI":"10.2307/3679940","CorpusId":62303236},"title":"Nonlinear Maps as Generators of Musical Design"},{"paperId":"cb38b1d203e7cbd2a5bbb5be659987016dbd8243","externalIds":{"DBLP":"conf/icassp/Imai83","MAG":"2096980176","DOI":"10.1109/ICASSP.1983.1172250","CorpusId":44946414},"title":"Cepstral analysis synthesis on the mel frequency scale"},{"paperId":"14bc876fae55faf5669beb01667a4f3bd324a4f1","externalIds":{"DBLP":"conf/icassp/GriffinL83","MAG":"2584505851","DOI":"10.1109/ICASSP.1983.1172092","CorpusId":53067},"title":"Signal estimation from modified short-time Fourier transform"},{"paperId":"cca6bdb49d2460dee5a1a0a0e0b672f59694560a","externalIds":{"MAG":"2092623750","DOI":"10.2307/3679879","CorpusId":62652181},"title":"Compositional Applications of Stochastic Processes"},{"paperId":"7ade23b45061e094da2d2c121c006a039651ee98","externalIds":{"MAG":"2097549272","DOI":"10.1109/TASSP.1976.1162847","CorpusId":86617776},"title":"Structure of a phonological rule component for a synthesis-by-rule program"},{"paperId":"b37579163dbc207ce6c60a92bfe24b6082f15168","externalIds":{"MAG":"2045388414","DOI":"10.1109/TAU.1968.1161948","CorpusId":62195231},"title":"Terminal analog synthesis of continuous speech using the diphone method of segment assembly"},{"paperId":"72d02ec28f5f4422ad55e8135a3c578b460654ad","externalIds":{"DBLP":"journals/access/ZamanSDU23","DOI":"10.1109/ACCESS.2023.3318015","CorpusId":262179755},"title":"A Survey of Audio Classification Using Deep Learning"},{"paperId":"23ef3b7133befebac889025b640b470bf853905a","externalIds":{"DBLP":"journals/tmm/BaoS23","DOI":"10.1109/TMM.2022.3163543","CorpusId":247898772},"title":"Generating Music With Emotions"},{"paperId":"e4ee8e5d39a9b80a2b76af5b7f4de67dc5a0b8bb","externalIds":{"DBLP":"journals/taslp/HaqueRLHCBS21","DOI":"10.1109/TASLP.2021.3098764","CorpusId":237001605},"title":"Guided Generative Adversarial Neural Network for Representation Learning and Audio Generation Using Fewer Labelled Audio Data"},{"paperId":"2022ea7ad4b00a307559ae0be1d0ae2237ad7466","externalIds":{"ACL":"2020.nlp4musa-1.11","CorpusId":227216994},"title":"BUTTER: A Representation Learning Framework for Bi-directional Music-Sentence Retrieval and Generation"},{"paperId":"4f8d648c52edf74e41b0996128aa536e13cc7e82","externalIds":{"DBLP":"journals/ijsc/HaoZM16","DOI":"10.1142/S1793351X16500045","CorpusId":1779661},"title":"Deep Learning"},{"paperId":"3342bf86c262bc35d3c54732e828e6f2b2ede1af","externalIds":{"MAG":"2004453603","DBLP":"journals/taslp/DecorsiereSMD15","DOI":"10.1109/TASLP.2014.2367821","CorpusId":7882863},"title":"Inversion of Auditory Spectrograms, Traditional Spectrograms, and Other Envelope Representations"},{"paperId":"c217905bc98f00af747e8e9d5f6b79fb89a90886","externalIds":{"MAG":"2294797155","DBLP":"conf/interspeech/FanQXS14","DOI":"10.21437/Interspeech.2014-443","CorpusId":18649557},"title":"TTS synthesis with bidirectional LSTM based recurrent neural networks"},{"paperId":"a068a7dd9a99132db37a355b6fba0bae2372d2fc","externalIds":{"MAG":"2161315604","CorpusId":55404764},"title":"Improving algorithmic music composition with machine learning"},{"paperId":"af22c776f78aa9781d05e5c9021459b8e6272516","externalIds":{"MAG":"2112213875","DOI":"10.1109/IJCNN.2001.938515","CorpusId":1905580},"title":"Creating melodies with evolving recurrent neural networks"},{"paperId":"505f9e5e4ea3711707ad0fe862401ef49215cb12","externalIds":{"MAG":"1600722501","CorpusId":8037054},"title":"Simultaneous Modeling of Spectrum, Pitch and Duration in HMM-Based Speech Synthesis"},{"paperId":"0d8bb76012371d534ae8fcfb779e570ea851c487","externalIds":{"MAG":"149236175","DBLP":"conf/icmc/Biles94","CorpusId":583394},"title":"GenJam: A Genetic Algorithm for Generating Jazz Solos"},{"paperId":"69a8e331ea493894066d003ab3e5987d30527b06","externalIds":{"DBLP":"journals/connection/Mozer94","MAG":"2067516917","DOI":"10.1080/09540099408915726","CorpusId":1675838},"title":"Neural Network Music Composition by Prediction: Exploring the Benefits of Psychoacoustic Constraints and Multi-scale Processing"},{"paperId":"e22fd93fdd6e4979e031c91f47d8c7bf482cf87e","externalIds":{"MAG":"2070240110","DOI":"10.1080/09298219108570576","CorpusId":62760831},"title":"Computational generation and study of Jazz music"},{"paperId":"81b1a6c54e0b91115e2b45a6ecbf49e2d8b356cd","externalIds":{"MAG":"158008515","DOI":"10.7551/mitpress/4804.003.0020","CorpusId":53749396},"title":"A nonheuristic automatic composing method"},{"paperId":"8e2ffaa8c59eb47e7e536d577a525a31786255e7","externalIds":{"DOI":"10.1109/ijcnn.1989.118552","CorpusId":17336214},"title":"A self-learning musical grammar, or 'associative memory of the second kind'"},{"paperId":"3d39cd4652fad243776dec1e00ba3430e7ef4d90","externalIds":{"MAG":"2044781497","DOI":"10.1121/1.1907654","CorpusId":222488379},"title":"Minimal Rules for Synthesizing Speech"},{"paperId":"f9bfd5462e98a9483372dfa03f42f1ca4f61fb12","externalIds":{"MAG":"852161132","CorpusId":106785356},"title":"High Fidelity-What Is It?"},{"paperId":"6ad64ff39a6084408f41b4cfd5a84de2cf687e3a","externalIds":{"MAG":"1989395836","DOI":"10.1109/T-AIEE.1947.5059525","CorpusId":51644190},"title":"Pulse code modulation"}]}