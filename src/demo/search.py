#!/usr/bin/env python3
"""
Citation Graph Builder v2.0
ä»JSON taxonomyæ–‡ä»¶æ„å»ºCitation Graphï¼Œä½¿ç”¨Semantic Scholar APIè·å–è®ºæ–‡ä¿¡æ¯ã€‚
è¾“å‡ºPyTorch Geometricæ ¼å¼çš„å›¾æ•°æ®ã€‚

åŠŸèƒ½ç‰¹æ€§:
- æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒä¸­æ–­åä»ä¸Šæ¬¡ä½ç½®ç»§ç»­
- è¿›åº¦ä¿å­˜ï¼šæ¯å¤„ç†10ç¯‡è®ºæ–‡è‡ªåŠ¨ä¿å­˜
- æ™ºèƒ½é‡è¯•ï¼šç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•
- æ ‡é¢˜æ¸…æ´—ï¼šå¤„ç†OCRå¯¼è‡´çš„æ ‡é¢˜é—®é¢˜
- åŒè¾“å‡ºæ ¼å¼ï¼šåŒæ—¶è¾“å‡ºPyG .ptæ–‡ä»¶å’ŒJSONæ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
    python build_citation_graph_v2.py input.json output.json
    
    # ä»æ–­ç‚¹ç»§ç»­
    python build_citation_graph_v2.py input.json output.json --resume
"""

import json
import re
import time
import urllib.parse
import urllib.request
import argparse
import os
from typing import Optional, Dict, List, Set, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime

# ============== é…ç½® ==============

CONFIG = {
    'api_delay': 0.3,           # APIè¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼ŒSemantic Scholarå…è´¹é™åˆ¶çº¦100æ¬¡/5åˆ†é’Ÿ
    'max_retries': 3,           # æœ€å¤§é‡è¯•æ¬¡æ•°
    'save_interval': 10,        # æ¯å¤„ç†Nç¯‡è®ºæ–‡ä¿å­˜ä¸€æ¬¡è¿›åº¦
    'match_threshold': 0.4,     # æ ‡é¢˜åŒ¹é…é˜ˆå€¼ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
    'timeout': 30,              # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
}

# ============== æ•°æ®ç»“æ„ ==============

@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    ref_num: int                # -1 è¡¨ç¤º root paper
    title: str                  # åŸå§‹æ ‡é¢˜
    title_cleaned: str          # æ¸…æ´—åçš„æ ‡é¢˜
    year: str                   # å‘è¡¨å¹´ä»½
    abstract: Optional[str] = None
    semantic_scholar_id: Optional[str] = None
    references: List[str] = field(default_factory=list)  # è¯¥è®ºæ–‡å¼•ç”¨çš„å…¶ä»–è®ºæ–‡title
    found: bool = False
    search_attempted: bool = False  # æ˜¯å¦å·²å°è¯•æœç´¢ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰

    def to_dict(self) -> dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, d: dict) -> 'Paper':
        return cls(**d)

# ============== æ ‡é¢˜æ¸…æ´— ==============

def clean_title(title: str) -> str:
    """
    æ¸…æ´—è®ºæ–‡æ ‡é¢˜ï¼Œå¤„ç†OCRé—®é¢˜
    """
    if not title:
        return ""
    
    # ç§»é™¤å°¾éƒ¨æ ‡ç‚¹
    title = title.rstrip('.,;:')
    
    # åœ¨å°å†™å­—æ¯åè·Ÿå¤§å†™å­—æ¯çš„åœ°æ–¹æ’å…¥ç©ºæ ¼
    title = re.sub(r'([a-z])([A-Z])', r'\1 \2', title)
    
    # åœ¨é€—å·/å¥å·/å†’å·/åˆ†å·åé¢å¦‚æœç´§è·Ÿå­—æ¯ï¼Œæ·»åŠ ç©ºæ ¼
    title = re.sub(r'([,.:;])([a-zA-Z])', r'\1 \2', title)
    
    # æ ‡å‡†åŒ–å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
    title = re.sub(r'\s+', ' ', title)
    
    return title.strip()

def normalize_title_for_matching(title: str) -> str:
    """
    å°†æ ‡é¢˜è§„èŒƒåŒ–ç”¨äºåŒ¹é…æ¯”è¾ƒ
    """
    title = title.lower()
    title = re.sub(r'[^a-z0-9\s]', '', title)
    title = re.sub(r'\s+', ' ', title)
    return title.strip()

def calculate_title_similarity(title1: str, title2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦ï¼ˆæ”¹è¿›çš„Jaccard + å­åºåˆ—åŒ¹é…ï¼‰
    """
    norm1 = normalize_title_for_matching(title1)
    norm2 = normalize_title_for_matching(title2)
    
    words1 = set(norm1.split())
    words2 = set(norm2.split())
    
    if not words1 or not words2:
        return 0.0
    
    # Jaccardç›¸ä¼¼åº¦
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    jaccard = intersection / union if union > 0 else 0
    
    # æ£€æŸ¥æ˜¯å¦ä¸€ä¸ªæ˜¯å¦ä¸€ä¸ªçš„å­é›†ï¼ˆå¤„ç†ç¼©å†™æ ‡é¢˜ï¼‰
    subset_bonus = 0
    if words1.issubset(words2) or words2.issubset(words1):
        subset_bonus = 0.2
    
    return min(1.0, jaccard + subset_bonus)

# ============== Semantic Scholar API ==============

BASE_URL = "https://api.semanticscholar.org/graph/v1"

def make_api_request(url: str, max_retries: int = None) -> Optional[dict]:
    """
    å‘é€APIè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    if max_retries is None:
        max_retries = CONFIG['max_retries']
    
    for attempt in range(max_retries):
        try:
            req = urllib.request.Request(url)
            req.add_header('User-Agent', 'CitationGraphBuilder/2.0 (Academic Research)')
            
            with urllib.request.urlopen(req, timeout=CONFIG['timeout']) as response:
                return json.loads(response.read().decode('utf-8'))
                
        except urllib.error.HTTPError as e:
            if e.code == 429:  # Rate limit
                wait_time = min(2 ** (attempt + 1), 60)  # æœ€å¤šç­‰60ç§’
                print(f"      â³ Rate limited, waiting {wait_time}s...")
                time.sleep(wait_time)
            elif e.code == 404:
                return None
            else:
                print(f"      âš ï¸  HTTP Error {e.code}: {e.reason}")
                if attempt < max_retries - 1:
                    time.sleep(2)
        except urllib.error.URLError as e:
            print(f"      âš ï¸  Network error: {e.reason}")
            if attempt < max_retries - 1:
                time.sleep(2)
        except Exception as e:
            print(f"      âš ï¸  Error: {e}")
            if attempt < max_retries - 1:
                time.sleep(1)
    
    return None

def search_paper_by_title(title: str) -> Optional[dict]:
    """
    ä½¿ç”¨Semantic Scholar APIæŒ‰æ ‡é¢˜æœç´¢è®ºæ–‡
    """
    search_query = clean_title(title)
    encoded_query = urllib.parse.quote(search_query)
    
    # æœç´¢æ—¶è·å–æ›´å¤šå€™é€‰ç»“æœ
    url = f"{BASE_URL}/paper/search?query={encoded_query}&limit=5&fields=title,abstract,year,paperId"
    
    data = make_api_request(url)
    
    if not data or not data.get('data'):
        return None
    
    # æ‰¾æœ€ä½³åŒ¹é…
    best_match = None
    best_score = 0
    
    for paper in data['data']:
        score = calculate_title_similarity(search_query, paper.get('title', ''))
        if score > best_score:
            best_score = score
            best_match = paper
    
    # åªæœ‰åˆ†æ•°è¶³å¤Ÿé«˜æ‰è¿”å›
    if best_match and best_score >= CONFIG['match_threshold']:
        best_match['_match_score'] = best_score
        return best_match
    
    # å¦‚æœæ²¡æœ‰å¥½çš„åŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€ä¸ªç»“æœä½†æ ‡è®°ä¸ºä½ç½®ä¿¡åº¦
    if data['data']:
        result = data['data'][0]
        result['_match_score'] = best_score
        result['_low_confidence'] = True
        return result
    
    return None

def get_paper_references(paper_id: str) -> List[dict]:
    """
    è·å–è®ºæ–‡çš„referencesåˆ—è¡¨
    """
    url = f"{BASE_URL}/paper/{paper_id}?fields=references.title,references.paperId"
    
    data = make_api_request(url)
    
    if data and data.get('references'):
        return [ref for ref in data['references'] if ref and ref.get('title')]
    
    return []

# ============== æ ¸å¿ƒé€»è¾‘ ==============

def load_taxonomy(filepath: str) -> Tuple[Paper, List[Paper], Set[str]]:
    """
    åŠ è½½taxonomy JSONæ–‡ä»¶
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Root paper
    root_info = data['root_paper']
    root_paper = Paper(
        ref_num=-1,
        title=root_info['title'],
        title_cleaned=clean_title(root_info['title']),
        year=str(root_info.get('year', ''))
    )
    
    # Reference papers (å»é‡)
    papers_dict: Dict[int, Paper] = {}
    
    def extract_papers(node):
        if 'papers' in node:
            for p in node['papers']:
                ref_num = p['ref_num']
                if ref_num not in papers_dict:
                    papers_dict[ref_num] = Paper(
                        ref_num=ref_num,
                        title=p['title'],
                        title_cleaned=clean_title(p['title']),
                        year=str(p.get('year', ''))
                    )
        if 'children' in node:
            for child in node['children']:
                extract_papers(child)
    
    for section in data['taxonomy']:
        extract_papers(section)
    
    # æŒ‰ref_numæ’åº
    reference_papers = [papers_dict[k] for k in sorted(papers_dict.keys())]
    
    # åˆ›å»ºnormalized titlesé›†åˆ
    all_titles = {normalize_title_for_matching(root_paper.title_cleaned)}
    for p in reference_papers:
        all_titles.add(normalize_title_for_matching(p.title_cleaned))
    
    return root_paper, reference_papers, all_titles

def fetch_paper_info(paper: Paper, all_titles: Set[str]) -> bool:
    """
    è·å–å•ç¯‡è®ºæ–‡çš„ä¿¡æ¯
    """
    # æœç´¢è®ºæ–‡
    result = search_paper_by_title(paper.title_cleaned)
    paper.search_attempted = True
    
    if result:
        paper.found = True
        paper.semantic_scholar_id = result.get('paperId')
        paper.abstract = result.get('abstract') or ''
        
        # è·å–references
        if paper.semantic_scholar_id:
            refs = get_paper_references(paper.semantic_scholar_id)
            for ref in refs:
                ref_title = ref.get('title', '')
                ref_normalized = normalize_title_for_matching(ref_title)
                if ref_normalized in all_titles:
                    paper.references.append(ref_title)
        
        confidence = "âœ“" if not result.get('_low_confidence') else "~"
        print(f"    {confidence} Found (score={result.get('_match_score', 0):.2f}), "
              f"abstract={len(paper.abstract)}chars, refs_in_set={len(paper.references)}")
        return True
    else:
        print(f"    âœ— Not found")
        return False

def build_edges(root_paper: Paper, reference_papers: List[Paper]) -> List[Tuple[int, int]]:
    """
    æ„å»ºè¾¹åˆ—è¡¨ï¼ˆæ— å‘è¾¹ï¼‰
    èŠ‚ç‚¹ç´¢å¼•: 0 = root_paper, 1~N = reference_papers
    """
    # title -> index æ˜ å°„
    title_to_idx = {normalize_title_for_matching(root_paper.title_cleaned): 0}
    for i, p in enumerate(reference_papers):
        title_to_idx[normalize_title_for_matching(p.title_cleaned)] = i + 1
    
    edges = set()
    
    # Root -> all references
    for i in range(len(reference_papers)):
        edges.add((0, i + 1))
    
    # Referenceä¹‹é—´çš„å¼•ç”¨å…³ç³»
    all_papers = [root_paper] + reference_papers
    for i, paper in enumerate(all_papers):
        for ref_title in paper.references:
            ref_normalized = normalize_title_for_matching(ref_title)
            if ref_normalized in title_to_idx:
                j = title_to_idx[ref_normalized]
                if i != j:
                    edge = (min(i, j), max(i, j))
                    edges.add(edge)
    
    return sorted(list(edges))

# ============== ä¿å­˜/åŠ è½½è¿›åº¦ ==============

def save_progress(root_paper: Paper, reference_papers: List[Paper], 
                  progress_file: str):
    """ä¿å­˜å½“å‰è¿›åº¦"""
    data = {
        'timestamp': datetime.now().isoformat(),
        'root_paper': root_paper.to_dict(),
        'reference_papers': [p.to_dict() for p in reference_papers]
    }
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"  ğŸ“ Progress saved to {progress_file}")

def load_progress(progress_file: str) -> Tuple[Paper, List[Paper]]:
    """åŠ è½½ä¹‹å‰çš„è¿›åº¦"""
    with open(progress_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    root_paper = Paper.from_dict(data['root_paper'])
    reference_papers = [Paper.from_dict(p) for p in data['reference_papers']]
    
    return root_paper, reference_papers

def save_final_output(root_paper: Paper, reference_papers: List[Paper],
                      edges: List[Tuple[int, int]], output_path: str):
    """
    ä¿å­˜æœ€ç»ˆè¾“å‡ºï¼ˆPyGæ ¼å¼ + JSONæ ¼å¼ï¼‰
    """
    all_papers = [root_paper] + reference_papers
    
    # å‡†å¤‡èŠ‚ç‚¹æ•°æ®
    nodes_data = []
    for i, p in enumerate(all_papers):
        nodes_data.append({
            'idx': i,
            'ref_num': p.ref_num,
            'title': p.title,
            'title_cleaned': p.title_cleaned,
            'year': p.year,
            'abstract': p.abstract or '',
            'found': p.found,
            'semantic_scholar_id': p.semantic_scholar_id,
            'references_in_set': p.references
        })
    
    # å°è¯•ä¿å­˜PyGæ ¼å¼
    pt_saved = False
    try:
        import torch
        from torch_geometric.data import Data
        
        # æ„å»ºåŒå‘edge_index
        if edges:
            edge_list = []
            for (i, j) in edges:
                edge_list.append([i, j])
                edge_list.append([j, i])
            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
        else:
            edge_index = torch.empty((2, 0), dtype=torch.long)
        
        # åˆ›å»ºDataå¯¹è±¡
        pyg_data = Data(
            edge_index=edge_index,
            num_nodes=len(all_papers)
        )
        
        # æ·»åŠ èŠ‚ç‚¹å±æ€§
        pyg_data.years = torch.tensor([int(p.year) if p.year.isdigit() else 0 for p in all_papers])
        pyg_data.found_mask = torch.tensor([p.found for p in all_papers])
        
        pt_path = output_path.replace('.json', '.pt')
        torch.save(pyg_data, pt_path)
        print(f"  âœ… Saved PyTorch Geometric data: {pt_path}")
        pt_saved = True
        
    except ImportError:
        print("  âš ï¸  PyTorch Geometric not installed, skipping .pt output")
        print("     Install with: pip install torch torch_geometric")
    
    # ä¿å­˜JSONæ ¼å¼
    output_data = {
        'metadata': {
            'created_at': datetime.now().isoformat(),
            'root_paper_title': root_paper.title,
            'pyg_file_saved': pt_saved
        },
        'nodes': nodes_data,
        'edges': edges,
        'statistics': {
            'num_nodes': len(all_papers),
            'num_edges': len(edges),
            'num_undirected_edges': len(edges) * 2,  # PyGä¸­çš„å®é™…è¾¹æ•°
            'total_papers': len(all_papers),
            'found_papers': sum(1 for p in all_papers if p.found),
            'papers_with_abstract': sum(1 for p in all_papers if p.abstract),
            'root_reference_edges': len(reference_papers),
            'inter_reference_edges': len(edges) - len(reference_papers)
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    print(f"  âœ… Saved JSON data: {output_path}")

# ============== ä¸»å‡½æ•° ==============

def main():
    parser = argparse.ArgumentParser(
        description='Build citation graph from taxonomy JSON',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python build_citation_graph_v2.py gnn1_taxonomy.json citation_graph.json
    python build_citation_graph_v2.py input.json output.json --resume
    python build_citation_graph_v2.py input.json output.json --delay 0.5
        """
    )
    parser.add_argument('input', help='Input taxonomy JSON file')
    parser.add_argument('output', help='Output file path (will create .json and .pt)')
    parser.add_argument('--resume', action='store_true', help='Resume from previous progress')
    parser.add_argument('--delay', type=float, default=CONFIG['api_delay'],
                        help=f'Delay between API calls in seconds (default: {CONFIG["api_delay"]})')
    
    args = parser.parse_args()
    
    CONFIG['api_delay'] = args.delay
    progress_file = args.output.replace('.json', '_progress.json')
    
    print("=" * 70)
    print("Citation Graph Builder v2.0")
    print("=" * 70)
    print(f"  Input:  {args.input}")
    print(f"  Output: {args.output}")
    print(f"  API delay: {CONFIG['api_delay']}s")
    print("=" * 70)
    
    # 1. åŠ è½½æ•°æ®
    print("\n[1/4] Loading taxonomy...")
    
    if args.resume and os.path.exists(progress_file):
        print(f"  ğŸ“‚ Resuming from {progress_file}")
        root_paper, reference_papers = load_progress(progress_file)
        # é‡å»ºall_titles
        all_titles = {normalize_title_for_matching(root_paper.title_cleaned)}
        for p in reference_papers:
            all_titles.add(normalize_title_for_matching(p.title_cleaned))
        
        # ç»Ÿè®¡å·²å®Œæˆçš„æ•°é‡
        completed = sum(1 for p in [root_paper] + reference_papers if p.search_attempted)
        print(f"  Loaded progress: {completed}/{len(reference_papers)+1} papers already processed")
    else:
        root_paper, reference_papers, all_titles = load_taxonomy(args.input)
    
    print(f"  Total papers: {len(reference_papers) + 1} (1 root + {len(reference_papers)} references)")
    
    # 2. è·å–è®ºæ–‡ä¿¡æ¯
    print(f"\n[2/4] Fetching paper information from Semantic Scholar...")
    
    all_papers = [root_paper] + reference_papers
    total = len(all_papers)
    found_count = 0
    
    for i, paper in enumerate(all_papers):
        if paper.search_attempted:
            if paper.found:
                found_count += 1
            continue
        
        paper_type = "root" if paper.ref_num == -1 else f"ref #{paper.ref_num}"
        print(f"\n  [{i+1}/{total}] ({paper_type}) {paper.title_cleaned[:55]}...")
        
        if fetch_paper_info(paper, all_titles):
            found_count += 1
        
        # å®šæœŸä¿å­˜è¿›åº¦
        if (i + 1) % CONFIG['save_interval'] == 0:
            save_progress(root_paper, reference_papers, progress_file)
        
        time.sleep(CONFIG['api_delay'])
    
    print(f"\n  ğŸ“Š Summary: Found {found_count}/{total} papers ({100*found_count/total:.1f}%)")
    
    # ä¿å­˜æœ€ç»ˆè¿›åº¦
    save_progress(root_paper, reference_papers, progress_file)
    
    # 3. æ„å»ºè¾¹
    print("\n[3/4] Building citation edges...")
    edges = build_edges(root_paper, reference_papers)
    
    root_edges = len(reference_papers)
    inter_edges = len(edges) - root_edges
    print(f"  Total edges: {len(edges)}")
    print(f"    - Root â†’ References: {root_edges}")
    print(f"    - Inter-reference:   {inter_edges}")
    
    # 4. ä¿å­˜ç»“æœ
    print("\n[4/4] Saving final output...")
    save_final_output(root_paper, reference_papers, edges, args.output)
    
    # æ¸…ç†è¿›åº¦æ–‡ä»¶
    if os.path.exists(progress_file):
        os.remove(progress_file)
        print(f"  ğŸ—‘ï¸  Removed progress file")
    
    print("\n" + "=" * 70)
    print("âœ… Done!")
    print("=" * 70)
    
    # æ‰“å°ä½¿ç”¨æç¤º
    print(f"""
ğŸ“– Output files:
   - {args.output} (JSON with full text data)
   - {args.output.replace('.json', '.pt')} (PyTorch Geometric format)

ğŸ“ To load in PyTorch Geometric:
   import torch
   from torch_geometric.data import Data
   
   # Load graph structure
   data = torch.load('{args.output.replace('.json', '.pt')}')
   
   # Load text data
   import json
   with open('{args.output}') as f:
       text_data = json.load(f)
   
   # Access nodes
   for node in text_data['nodes']:
       print(node['title'], node['abstract'][:100])
""")

if __name__ == "__main__":
    main()